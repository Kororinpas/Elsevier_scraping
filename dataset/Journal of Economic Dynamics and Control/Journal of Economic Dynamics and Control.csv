name,institution,publish_date,doi,cite,abstract,introduction,Title,Url,Time,Year,Type,Unnamed: 0
"Leong Soon Heng,Urga Giovanni","Finance Department, ESCP Business School, 527 Finchley Road, London NW3 7BG, UK,Centre for Econometric Analysis, Faculty of Finance, Bayes Business School (formerly Cass), City, University of London, UK","Received 23 February 2023, Revised 6 June 2023, Accepted 12 June 2023, Available online 14 June 2023.",https://doi.org/10.1016/j.jedc.2023.104694,Cited by (0),"We propose an asymptotic N(0,1) inferential strategy to test for volatility spillover between markets consisting of multiple sectors. First, we use nonparametric kernel method to derive test statistics that assign flexible weight to each lag order and are able to check a growing number of lags as the sample size increases. Second, we propose a practical multivariate volatility modeling approach — which enjoys estimation consistency and simplicity — to facilitate higher dimensional spillover testing. Simulations show the reasonable finite sample performance of the proposed econometric strategy in a relatively large system. An empirical application highlights the merits of the proposed approach.","Volatility is undoubtedly one of the most informative risk indicators in financial economics as it is fundamentally related to, among others, market liquidity risk (Garbade and Silber, 1979), the interaction between informed and strategic traders (Admati and Pfleiderer, 1988), the rate of information flow to the market (Ross, 1989), revealed private information (Stoll and Whaley, 1990), and the degree of connection between markets as market participants infer information from each other (Ellington, 2022, King, Wadhwani, 1990). Consequently, a statistical tool that detects volatility spillover between markets provides important decision making information about hedging and managing volatility risk in the commodity market (Sévi, 2014), the energy market (Fianu et al., 2022), as well as the foreign exchange market (Barunik et al., 2016).====Very often, academics and practitioners concern with testing volatility spillover between markets consisting of multiple major sectors. For instance, Bekiros et al. (2017) consider testing volatility spillover between US commodity market (characterized by the energy, metal and agricultural sectors) and US equity market (characterized by 10 major sectors such as financial, health, and technology). In this aspect, analyzing the overall spillover effect between the commodity and stock markets based on a weighted commodity index and a weighted equity index is inadequate because the univariate analysis does not take into account covariances between the individual sectors within each market. As highlighted in our simulation study, covariances between individual sectors can play a nontrivial role in driving overall spillover. In this paper, we propose a new econometric strategy for testing volatility spillover between markets that are characterized by multiple sectors. We begin by generalizing the univariate hypothesis of Hong (2001) to the multivariate setup. We follow the author to define volatility spillover using the notion of Granger, 1969, Granger, 1980 causality in variance, in that there is volatility spillover from ==== to ==== if any of the past variances of ==== has explanatory power over the current variance of ====.==== Therefore, the terms ==== and ==== may be used interchangeably. We derive testable statistics for our hypotheses using normalized cross-spectra and we develop the asymptotic theory. Our test statistics possess several appealing features. First, the computation of our test statistics is relatively straightforward because it requires only the estimation of standardized residuals which are the main event variables. Unlike most existing parameter restriction tests which estimate all series in a global model, our procedures allow the event variables of ==== and ==== to be estimated separately. Second, our tests check a large number of lag orders ==== as the sample size ==== increases. In fact, we allow ==== to grow with ==== at a proper rate to ensure power against a broad class of alternatives such as delayed spillover effect. Third, our frequency domain kernel-based procedure allows flexible weighting of the cross-spectrum at each lag order. We show that the conventional Granger-type regression procedure can be viewed as a special case of our approach when the Truncated kernel is used. Both tests assign equal weight to each lag. Instead, we propose to use downward weighting kernels to enhance the power of our tests because empirical stylized facts suggest that market participants discount past information and thus spillover effect is expected to decay over time. Indeed, simulation evidence shows that our downward weighting tests can check a large number of lags without losing significant power when compared with an equally weighted test.====Due to the practical limitations in estimating volatility as its dimension increases, the paper proposes a modeling approach that works coherently with the spillover test. The proposed model takes the form of a modified Bollerslev (1990) variance-covariance structure, where we specify the elements in the diagonal matrix as ARCH==== processes to minimize the risk of autocorrelated residuals resulting from model inadequacy (see, e.g., Li and Mak, 1994). This will in turn contaminate the resulting test statistics by invalidating its asymptotic property. Indeed, our supplementary empirical study confirms that serial correlation induced by an inadequate GARCH gives misleading inferential result. In this aspect, the proposed long ARCH==== process — which is sometimes referred to as a “nonparametric” approach — is more appealing than conditional variance models with a prespecified order. Regarding model estimation, we show that least-squares is feasible and consistent. The proposed nonparametric covariance modeling and least-squares estimation (NC-LS) approach is simple and complements existing literature. For instance, the proposed estimation method enjoys numerical efficiency as we find that it requires only about 5% of the computing time of QMLE in a computational study provided in the supplementary document. The speed advantage effectively makes bootstrap feasible to give a more accurate finite sample performance for the volatility spillover test. Besides, the proposed method allows consistent element-wise estimation of the volatility model that is not limited by certain distributional assumptions (see, e.g., Boudt et al., 2019, p. 217).====Our econometric strategy proceeds in two stages. First, we estimate the event variables by fitting the observed data using our proposed NC-LS approach. Second, we compute our test statistics to draw inference about volatility spillover. Throughout our econometric strategy, we need not perform numerical integration nor optimization. An extensive Monte Carlo study shows that our inferential strategy provides reliable finite sample inference even in the higher dimension up to the case of 10 series while the simulation evidence in most other papers is limited to 2–3 series. We further provide a consistent bootstrap test whose finite sample size is found to converge at a faster rate. We apply our inferential strategy in a multivariate study in which we investigate the distortion in volatility spillover relations between the North America (NA) market and the UK market before and after the Brexit referendum. For a broader study, we also examine the spillover effect on the European Union (EU) market which is represented by eight major economies. First, based on a series of diagnostic examinations we find that the proposed NC-LS approach shows adequacy in modeling and fitting the collected empirical data. Next, using the properly fitted data we apply the proposed multivariate test to examine volatility spillover. Our main findings show that compared with the pre-Brexit sample, the average spillover from UK to NA diminishes in the post-Brexit period. By contrast, we find that the spillover effect from EU to NA is relatively delayed before Brexit but the nexus becomes more immediate after Brexit. Because market indices are driven by the collective actions of the respective market participants, we could infer that on average — in contrast to the pre-Brexit sample — the NA participants appear not to react to the UK market but are following more closely the EU market in the post-Brexit period.====The remainder of this paper is organized as follows. In Section 2, we derive test statistics for the hypotheses of interest and we provide their asymptotic properties. Section 3 presents the nonparametric volatility model and its asymptotic validity. The finite sample performance of our econometric strategy is reported in Section 4 using a Monte Carlo study. In Section 5, we apply the proposed inferential strategy to empirically examine volatility spillover between the North American and European equity markets. Section 6 concludes. Mathematical derivations and proofs are relegated to the Appendices. Additional simulation and empirical results are collected in a supplementary document. Throughout the paper, ==== and ==== denote convergences in distribution and probability, respectively. The Euclidean norm is denoted by ====. Unless otherwise indicated, all limits are taken as the sample size ====.",A practical multivariate approach to testing volatility spillover,https://www.sciencedirect.com/science/article/pii/S0165188923001008,Available online 14 June 2023,2023,Research Article,0.0
"Skavysh Vladimir,Priazhkina Sofia,Guala Diego,Bromley Thomas R.","Bank of Canada, 234 Wellington Street, Ottawa, ON, K1A 0G9, Canada,Xanadu, Toronto, ON, M5G 2C8, Canada","Received 28 July 2022, Revised 11 May 2023, Accepted 26 May 2023, Available online 1 June 2023, Version of Record 16 June 2023.",https://doi.org/10.1016/j.jedc.2023.104680,Cited by (0),. We discuss potential computational gains of QMC versus classical computing systems and present a few innovations in benchmarking QMC.,"Monte Carlo methods are a group of algorithms that harness randomness to perform a number of computational tasks (Kalos and Whitlock, 2009).==== This approach is common in various sciences that use statistics, data sampling, and mathematical complexity. When major improvements in computers began to occur, economics was among the first of the social sciences to take advantage of Monte Carlo techniques. However, many numerical economic applications are still not feasible today given classical computational limitations. The primary purpose of this paper is to explore whether the Monte Carlo methodology can be improved with the usage of quantum technology in the context of economic applications and to determine when a quantum advantage can be achieved for these problems. Being pioneers in quantum applications in economics, we also aim to lay down the basic foundation of quantum programming so that economists can benefit from future technological innovations.====The intuition and challenges of Monte Carlo simulations can be grasped by considering a simple problem: calculation of an expectation of a random variable. Simulations of this kind are simple to perform—samples are generated according to the underlying probability distribution, the random variable is then evaluated, and an average is taken. Clearly, the error of the approximation depends on the number of samples generated, resulting in a trade-off between accuracy and time. The trade-off persists even when the sampling is done in parallel on a CPU/GPU cluster due to the computational cost, costs of purchasing hardware and software, and maintenance costs. In practical applications, this trade-off can lead to the Monte Carlo estimation becoming the main bottleneck in a workflow.====Quantum computations may become a remedy for these problems. Quantum mechanics provides a new class of algorithms that can potentially outperform their classical (non-quantum) counterparts (Nielsen and Chuang, 2010). These algorithms must be executed on quantum hardware and require development of new technologies to control for excessive noise in computations, referred to as fault-tolerance. One algorithm of interest is the so-called quantum Monte Carlo (QMC) algorithm (Montanaro, 2015, Rebentrost, Gupt, Bromley, 2018, Xu, Daley, Givi, Somma, 2018), which has the potential to estimate an expectation value with a quadratic speedup (in query complexity) compared to the classical Monte Carlo.==== The QMC algorithm adopts established techniques from quantum computing, including amplitude estimation (Brassard et al., 2000), which we use in this paper.==== Implementing QMC requires encoding the problem as a combination of unitary operators (complex matrices that preserve the inner product). These unitaries must be then decomposed into elementary quantum gates—analogs of non-quantum Boolean logical functions—for QMC to be compatible with hardware. Importantly, such decomposition must be efficient for the quadratic speedup to persist. Hence, a major focus has been on identifying efficient encodings of the Monte Carlo problem.====In recent years, the most relevant use-cases developed alongside the QMC algorithm have involved solving problems in finance, including options pricing by Rebentrost et al. (2018); Stamatopoulos et al. (2020) and Chakrabarti et al. (2021) and risk analysis by Woerner and Egger (2019) and Egger et al. (2020). However, little work has been carried out in the larger field of economics. More generally, applications of quantum computing to economics have been extremely scarce. Among the few examples of quantum-driven economics known to us are Hull et al. (2020) with the general discussion of quantum algorithms in the context of economics and specifically quantum money; Orús et al. (2019) on modeling contagion in financial networks; Alaminos et al. (2021) on deep learning techniques for GDP forecasting; McMahon et al. (2022) on improving the efficiency of a large value transfer payments system; Fernández-Villaverde and Hull (2022) on dynamic programming using quantum annealer; and Priazhkina et al. (2023) on studying cryptocurrency adoption in networks. This leaves many unexplored opportunities for economists to apply quantum computing in their research and policy.====In this work, we focus on two policy-relevant applications in the context of QMC: stress-testing and general equilibrium macro-modeling. For the stress testing, we develop a model where banks experience severe credit losses and engage in fire sales. The credit losses are random and determined by the stress scenario. This setup resembles a typical exercise performed by regulators and central banks to assess financial stability. The goal of the exercise is to evaluate overall capital losses that banks experience following a multi-year stress scenario. We encode this problem onto the quantum circuit and compare the quantum solution with the theoretical prediction.====For the macro-modeling problem, we solve the stochastic neoclassical investment model using machine (deep) learning. The model is intended to capture relationships between major macroeconomic variables: consumption, capital, production, and productivity shocks. A central part of the deep learning approach is the Monte Carlo estimation of stochastic gradients as in the recent work of Maliar et al. (2021). We substitute the QMC algorithm for this part and perform a fair comparison between quantum and classical algorithms by decomposing the problem into elementary gates and calculating the physical runtime. This is the first paper, as far as we are aware, comparing physical runtimes between the two algorithms. We find that QMC can be advantageous over sizable high-performance computing (HPC) clusters. In reaching the computational error of ====, QMC reaches the solution 5.6 times faster than a strong HPC cluster that an average researcher should be able to rent. If a lower computational error were desired, QMC’s advantage would only grow.====We then explore this macroeconomic deep learning application farther by demonstrating QMC’s quadratic speedup on a more realistic model. The model contains multiple random variables and is solved with a deep neural network.====In addition to exploring economics problems as a use case for quantum algorithms, one of the key contributions of this work is to provide a detailed investigation into the resources needed to perform QMC. Our investigation requires an efficient decomposition of the algorithm into hardware-compatible quantum gates. This can be done using existing results in the literature that show how to decompose the unitaries that encode the probability distribution Grover and Rudolph (2002); Herbert (2021a); Zoufal et al. (2019) and random variable of the estimation problem Herbert (2021b); Rebentrost et al. (2018); Stamatopoulos et al. (2020); Vedral et al. (1996); Woerner and Egger (2019). We build upon the state-of-the-art by optimizing a variational quantum circuit with the objective of closely approximating the target probability distribution, using similar methods to those outlined in Ref. Chakrabarti et al. (2021). Furthermore, we show how a controlled version of the quantum oracle, ====, can be designed without a controlled unitary ====, which encodes the classical problem onto the quantum computer.====By combining our variational method for encoding the probability distribution with the decomposition of a linear random variable provided by Egger et al. (2020) and Stamatopoulos et al. (2020), we use the PennyLane software library to simulate the QMC algorithm and monitor its performance and resource requirements as the number of qubits are scaled. The gate depth can be mapped to an algorithmic runtime by assuming each gate is performed within a maximum time, allowing us to compare the runtimes of the classical and quantum Monte Carlo algorithms—an important exercise when gauging the potential for quantum advantage.====We begin in Section 2 by giving an overview of Monte Carlo applications in economics. In Section 3, we briefly discuss quantum computation concepts and notation for economists unfamiliar with the subject. We also describe the current state-of-the-art in quantum computing hardware and how soon it might be possible to implement the QMC algorithm for realistic problems on a real quantum device. In Section 4, we provide a mathematical formulation of the quantum Monte Carlo algorithm and describe how it can be performed on a quantum computer. This section is more technical and can be skipped by readers less interested in the details of the algorithm. Section 5 introduces two economics applications of QMC: (a) stress testing of banks and (b) solving dynamic stochastic general equilibrium (DSGE) models with deep learning. In the latter, we consider two models: a simple one and a more realistic model. In Section 6, we benchmark the QMC solution of the simple DSGE model against the classical Monte Carlo solution. In Section 7, we describe the challenges of QMC and how current and future work is likely to address these challenges. We conclude the paper in Section 8. Many mathematical details of the QMC algorithm, how QMC can be decomposed into hardware-compatible gates, and our code implementing the QMC algorithm are all provided in the Appendices.",Quantum Monte Carlo for Economics: Stress Testing and Macroeconomic Deep Learning,https://www.sciencedirect.com/science/article/pii/S0165188923000866,Available online 1 June 2023,2023,Research Article,1.0
"Kaufmann Mattheo,Schiereck Dirk","Department of Business Administration, Economics and Law, Technical University of Darmstadt","Received 21 November 2022, Revised 26 March 2023, Accepted 24 May 2023, Available online 26 May 2023, Version of Record 6 June 2023.",https://doi.org/10.1016/j.jedc.2023.104673,Cited by (0),"We investigate the effect of corporate innovation on mergers and acquisitions (M&A). Using a sample of 786 public-to-public transactions in the U.S. technology sector, we show that acquirers are willing to pay higher premiums for more innovative target firms. This effect is amplified by the acquirer's own level of innovativeness as more innovative acquirers are willing to pay higher premiums for innovative targets than non-innovative acquirers. We further document significant strategic reactions of rival firms. In the aftermath of the M&A, all acquirer rivals increase their R&D spending but the effect is more pronounced for innovative rivals than for non-innovative ones. Innovative acquirer rivals are also more likely to acquire a technology firm in the aftermath of their competitor's M&A announcement than their non-innovative peers. The similarity between acquirers and their rivals shrinks in the post-acquisition period, which may be caused by rival firms extending the breadth of their technological search in response to the acquisition.","The success of leading technology firms is commonly attributed to their innovative capabilities, operating within an industry that relies heavily on innovation to foster growth. However, once at the top it is difficult to stay there, as evidenced by various former technology conglomerates that either let their lead slip away (e.g., Yahoo, Nokia, MySpace) or had to go out of business completely (e.g., Compaq, Blockbuster, Kodak) because they failed to keep up with product innovations in their respective fields. As Glazer and Weiss (1993) note, knowledge in the technology industry depreciates rapidly, forcing successful companies to constantly innovate in order to stay competitive. While leading technology firms typically possess material inhouse innovation capabilities, most of them also rely heavily on acquiring innovation externally through corporate mergers & acquisitions (M&A) to defend their market leadership positions. Lukas et al. (2019) argue that serial acquisitions are more likely in sectors with high levels of uncertainty and Hackbarth and Miao (2012) show that mergers happen more often in industries with a high exposure to industry-wide shocks, both of which may particularly be the case for the innovation-reliant technology industry, further highlighting the importance of M&A for technology firms. Karim and Mitchell (2000) show that firms participating in acquisitions change their product lines substantially more than non-participants, indicating that M&A activity is a key mechanism in which firms tend to innovate and update their business models. Salesforce founder and CEO Marc Bernioff commented on the role of inorganic innovation in the context of Salesforce's acquisition of Tableau in 2019:====This study examines the relationship between corporate innovation and M&A activity in the U.S. technology industry. Specifically, we concentrate on the ability of technology firms to maintain or even extend their position in the market through the means of innovation-driven M&A and use premiums paid as well as stock market reactions to measure how investors evaluate these transactions. While multiple studies investigate corporate innovation in the context of M&A, most of them focus on the impact on post-acquisition innovation performance and are less concerned with premiums and stock market reactions (Desyllas & Hughes, 2010; Gantumur & Stephan, 2012; Valentini, 2012). Wu and Chung (2019) and Kim et al. (2021) are among the few who investigate the role of innovation in determining takeover premiums and abnormal announcement stock returns. However, their results are diverging, as Kim et al. (2021) report a negative relationship between target firm innovativeness and takeover premiums while Wu and Chung (2019) find a positive relationship, leaving room for further clarification. We also extend the focus and investigate corporate innovation in the context of M&A and its strategic effects on rival firms and the competitive landscape. Various studies investigate the impact of competition more generally on innovation (Aghion et al., 2005; Bloom et al., 2013; McGahan & Silverman, 2006; Spulber, 2013), but few in the context of M&A. Valentini (2016) explores this topic focusing on the effect that M&A has on the breadth of technological search of rival firms. However, he does not investigate the level of innovativeness of the rival firm itself in this context. This study is, to the best of our knowledge, the first study that investigates the role that rival innovativeness plays in determining the behavior of said rivals in response to their competitor's M&A announcement.====Using a sample of 786 mergers and acquisitions of public target firms from the U.S. technology industry conducted between 2000 and 2019, we show that acquirers are paying higher premiums for innovative targets and that innovative target firms achieve higher abnormal stock returns than non-innovative target firms, suggesting that innovation represents a source of synergies and value creation in the context of M&A. All else being equal, our baseline regression results indicate that a one-standard-deviation increase in one of our three innovation measures increases takeover premiums by between 1.8 and 3.3 percentage points, corresponding to an average premium increase between USD 31 and USD 58 million. Our results further show that innovative acquirers are able to pay higher premiums for innovative target firms than non-innovative acquirers and that these innovative acquirers do not experience negative announcement stock returns when doing so. This suggests that innovative firms are able to outbid non-innovative firms in technology-motivated M&A processes without experiencing adversarial effects, thus describing a self-reinforcing dynamic as innovative acquirers may become more and more innovative by acquiring innovative target firms. We further use a sample of 1,968 target rivals as well as 1,890 acquirer rivals to document that the M&A announcement has not only significant implications for the acquirer and the target, but also on rival firms. Their reactions, however, differ based on their own level of innovation. We show that innovative acquirer rivals increase their R&D spending more heavily than their non-innovative peers following the M&A announcement. Our results further suggest that innovative acquirer rivals are also more likely to acquire a target firm from a technology-intensive industry. This highlights that the self-reinforcing dynamic described above induces acquirer rivals to search for innovation-driven M&A opportunities themselves in order not to fall behind. Target rivals experience significantly positive abnormal stock returns in response to the announcement, a finding that is in line with the acquisition probability hypothesis, albeit their reaction does not differ based on their own level of innovativeness.====This study contributes to the extant literature in two ways. First, we contribute to the literature on the relationship between innovation and M&A. We document a positive link between the innovativeness of the target firm and takeover premiums/target stock returns. This finding may help to clarify the diverging results on the role of innovation on M&A from related studies (Kim et al., 2021; Wu & Chung, 2019). In this context, we also extend the literature by showing that premiums and target stock returns are amplified by the innovativeness of the acquirer firm and that innovative acquirers do not experience negative announcement stock returns when paying these higher premiums. This finding helps to further our understanding of the underlying competitive dynamics that lead to the concentration of innovative capabilities in a few, large technology giants as they are able to outbid non-innovative competitors when acquiring innovative target firms. Second, we contribute to the literature on the strategic effects of corporate innovation on competition in general and rival firms in particular. While multiple scholars investigate the impact of innovation in the context of competition more generally (Aghion et al., 2005; Bloom et al., 2013; McGahan & Silverman, 2006; Spulber, 2013), these studies typically investigate it in relation to product market competition and not in the context of M&A. Valentini (2016) is the first to investigate this question, focusing on the change in technological scope of innovation efforts following a competitor's M&A announcement. He does not, however, investigate the role that the innovativeness of rival firm's play in determining the innovation behavior after the competitor's M&A announcement. We extend this strand of the literature by investigating how the strategic post-M&A response of rival firms differs based on their innovativeness. We document that innovative acquirer rivals react differently to their competitor's M&A announcement than their non-innovative peers. Innovative acquirer rivals increase their R&D spending more heavily than their non-innovative peers, suggesting that the pressure exerted on rivals from innovation-driven M&A depend on rival firms' innovativeness. Innovative acquirer rivals are also more likely to acquire a target firm from a technology-intensive industries in the years after their competitor's announcement than non-innovative rivals. We also find that for both innovative and non-innovative acquirer rival firms, the similarity to the acquirer decreases in the years following the announcement. Finally, we find no evidence that ==== rival firms' generally positive stock price reactions to the competitor M&A announcement differ based on their own level of innovativeness.====The remainder of the paper is structured as follows. Section 2 provides an overview over the relevant literature and develops our main hypotheses. Section 3 presents the sample construction as well as descriptive sample statistics. Section 4 outlines the empirical approach and discusses our results. Section 5 concludes.",Acquiring for innovation: Evidence from the U.S. technology industry,https://www.sciencedirect.com/science/article/pii/S0165188923000799,26 May 2023,2023,Research Article,2.0
"Zhang Gongqiu,Li Lingfei","School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China,Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Hong Kong SAR","Received 22 May 2022, Revised 16 December 2022, Accepted 18 May 2023, Available online 23 May 2023, Version of Record 1 June 2023.",https://doi.org/10.1016/j.jedc.2023.104669,Cited by (1),". We compute its ==== and the distribution of the maximum drawdown. We prove convergence of our method for general Markov models and provide sharp estimate of the convergence rate for a general class of jump-diffusion models. We apply our method to price and hedge maximum drawdown options and demonstrate its accuracy and efficiency through various numerical experiments. In addition, we can apply our method to calculate the Calmar ratio for investment analysis, and quantify the contributions of assets to the drawdown risk of a portfolio when the assets follow multivariate exponential Lévy models.","Sharp price falls are not uncommon in financial markets, and a key concept in measuring the severity of price falls is drawdown. For a stochastic process, drawdown shows the drop from the running maximum to the current state, and maximum drawdown further marks the worst retreat from the peak. Mathematically, for a process ====, the running maximum, drawdown and maximum drawdown are defined as follows:====The risk caused by drawdown is an important concern in financial markets. Figure 1 shows the price of Bitcoin as an example of highly volatile assets. The data is characterized by a frenzied rally with the peak price notched on December 16, 2017, followed by a catastrophic collapse that halved the price in around one month. The drawdown process touched zero on several days before December 16, when a new price maximum was set, but rose sharply soon after and continued to climb as the price slipped. In general, investment in any highly volatile assets like Bitcoin is exposed to substantial drawdown risk. To account for drawdown risk in investment decisions, a popular reward-to-risk ratio known as the Calmar ratio measures risk by the expected value of the maximum drawdown in the investment horizon (Magdon-Ismail, Atiya, 2004, Schuhmacher, Eling, 2011). Quantifying drawdown risk is also important in other applications, such as insurance and water reservoir management (Landriault et al., 2017b).====This paper addresses important probability questions related to drawdown and maximum drawdown for Markov processes with a view toward financial applications. Our contributions are summarized as follows.====First, we propose a novel computationally efficient method based on continuous-time Markov chain (CTMC) approximation for solving the first passage problem of drawdown for a variety of models, including general one-dimensional (1D) time-homogeneous and time-inhomogeneous Markov models, regime-switching models, and stochastic volatility models. The existing literature focuses on analyzing drawdown in rather specific 1D time-homogeneous Markov models. One can certainly use methods like finite difference for general models, but our experiment indicates that finite difference can be quite inefficient for the drawdown problem. To the best of our knowledge, our paper is the first one to deal with 1D time-inhomogenous Markov models, regime-switching models, and stochastic volatility models for the drawdown problem.====Second, we provide rigorous convergence analysis of our method. In particular, we obtain sharp estimate of the convergence rate for the Laplace transform of the first passage time of drawdown under a general class of jump-diffusions with finite jump activity. Analyzing the convergence rate of CTMC approximation in financial applications can be challenging and many papers only estimate the convergence rate numerically.====Third, we introduce a new idea to approximate 1D time-inhomogeneous Markov processes. The standard approach is to approximate the original process by a time-inhomogeneous Markov chain (see Mijatović and Pistorius, 2010), for which it is still inconvenient to solve the drawdown problem. In our approach, we embed time into the state space as an additional state variable and consider the space-time process, which is a two-dimensional (2D) time-homogeneous Markov process. We approximate it by a 2D CTMC where the time variable evolves as a 1D CTMC (but it can only move right at a transition). The 2D CTMC is equivalent to a regime-switching Markov chain with the time variable serving as the regime. Our construction makes the drawdown analysis tractable, which can also be applied to other stochastic problems.====Last but not least, we contribute efficient algorithms to financial applications. The main application considered in our study is pricing and hedging drawdown derivatives. A range of drawdown derivatives and insurance contracts exist as tools to hedge drawdown risk (Vecer, 2006, Vecer, 2007, Zhang, Leung, Hadjiliadis, 2013). We are interested in call options and digital call options written on the maximum drawdown over the option’s lifetime. We study how to price and hedge these maximum drawdown derivatives from the perspective of a derivative seller. Employing Gauss quadrature, we reduce the option pricing problem to computing a series of first passage probabilities of the drawdown, for which we apply our CTMC approximation algorithm. Comparison with finite difference shows that our method is more efficient. Based on the pricing algorithm, we develop a dynamic hedging scheme to allow sellers of drawdown options to effectively control their risk exposures. We also mention the applicability of our method to another two applications in this paper. We can apply it to calculate the risk measure in the Calmar ratio, which is a special maximum drawdown call option with zero strike and pricing done under the physical measure. Thus, our option pricing algorithm applies. Furthermore, we can utilize our method to analyze the drawdown risk of a portfolio. If the assets in the portfolio follow a multivariate exponential Lévy model, we can quantity the contribution of each constituent to the drawdown risk of the portfolio using our method via sensitivity analysis.",A general method for analysis and valuation of drawdown risk,https://www.sciencedirect.com/science/article/pii/S0165188923000751,23 May 2023,2023,Research Article,3.0
"Hjertstrand Per,Swofford James L.,Whitney Gerald A.","Research Institute of Industrial Economics,Department of Economics and Finance, University of South Alabama,Department of Economics and Finance, University of New Orleans","Received 8 August 2022, Revised 4 May 2023, Accepted 20 May 2023, Available online 21 May 2023, Version of Record 27 May 2023.",https://doi.org/10.1016/j.jedc.2023.104671,Cited by (0), and financial/monetary assets. We propose computationally attractive nonparametric revealed preference procedures to test the models using observed data on prices and quantities. An empirical application shows that it is important to account for incomplete adjustment in consumer demand models of durable consumption goods.,"That decision makers cannot instantaneously adjust to their optimal equilibrium demand for goods and assets has been recognized ever since Böhm-Bawerk (1888). In an equilibrium growth model, Kydland and Prescott (1982) argue theoretically and establish empirically that incomplete adjustment plays a vital role when modelling the aggregate economy. Swofford and Whitney (1988), Fleissig and Swofford (1996), Jones et al. (2005), Elger et al. (2008) and Jha and Longjam (2006) are examples of studies showing that various microeconomic decisions including those on capital, monetary and financial assets, and consumer durables are characterized by incomplete adjustment. Koyck (1954) and Almon (1965) developed distributed lag models to allow for incomplete adjustment in regression analysis. In light of this literature, incomplete adjustment can arise because of a number of factors, such as habit persistence (formation), adjustment costs, information asymmetries, the formation of expectations, or a combination of reasons.====Given the nature of durable goods and in keeping with the empirical evidence on durable goods in previous studies it seems natural to assume that it is primarily durable goods and services which provide a stream of utility over periods that may cause incomplete adjustment. However, this does not preclude nondurable goods and services to also cause incomplete adjustment. For example, consumers facing a price level shock may take more than one period to adjust their money balances.====In this paper, we propose models of weak separability and utility maximization that allow for incomplete adjustment. We show how these models can be implemented and tested empirically using consumer choice data. Allowing for incomplete adjustment in consumer demand models may have important implications in several fields of economics. For example, it is common by assumption to omit durable goods in empirical demand analysis, implying that consumption of nondurable goods and services in these models are independent of how much the consumer spends on durables. In an empirical application, we find that this may be an invalid assumption using detailed panel data over Spanish households.====Utility maximization is a core concept in economics and an underlying assumption for all rational choice theory. Utility maximization is a necessary condition for weak separability, which is another key concept in economics. A group of goods is said to be weakly separable from all other goods if the quantities demanded of the other goods are independent of the marginal rates of substitution between any pair of goods in the separable group. This implies that the quantities demanded of the goods inside the separable block depend solely on the prices of the goods that form the block. In contrast, if a group of goods were not weakly separable, then it would be possible for the prices of all goods outside the block to enter into the demand functions for the group of goods being considered.====Utility maximization and weak separability can be tested using either parametric consumer demand models or nonparametric revealed preference methods.==== As argued by Varian (1982, p.945) “[the parametric] procedure will be satisfactory only when the postulated parametric forms are good approximations to the ‘true’ demand functions. Since this hypothesis is not directly testable, it must be taken on faith.” Thus, given that some functional form must be imposed in the parametric approach, any test of weak separability (utility maximization) may be regarded as a joint test of the hypothesis of weak separability (utility maximization) and the hypothesis that correct functional form and error structures have been employed.====Instead, we implement our models of weak separability and utility maximization with incomplete adjustment using nonparametric revealed preference methods. These methods build upon the empirical revealed preference theory for utility maximization and weak separability proposed by Afriat (1967, 1969) and Varian (1983). These methods do not require any ==== assumptions regarding functional form and there is no parameter estimation. Thus, the hypothesis of weak separability (utility maximization) is not confounded in any subsidiary hypothesis from estimation. The main drawback of these methods is that they are, contrary to the parametric approach, non-stochastic and are therefore not tests in the “statistical sense”.====The nonparametric revealed preference approach offers a natural way of allowing for incomplete adjustment since it can be modelled as a constraint on a good or a constraint placed upon expenditures on a particular subset of goods (as we do here). Since the nature of the constraint need not be specified in the revealed preference approach, no particular adjustment process needs to be specified a priori. In contrast, although incomplete adjustment in parametric models allows for additional flexibility, it must rest on some kind of parametric assumption as to how incomplete adjustment enters the estimating equations.====Swofford and Whitney (1994) introduced a model of incomplete adjustment assuming that all goods causing incomplete adjustment (i.e., durables) exclusively form the weakly separable block of goods. Consequently, this assumption implies that all remaining goods (not causing incomplete adjustment, i.e., nondurables) exclusively form the nonseparable block of goods. However, this is a strong assumption since it does not allow to test for weak separability on hypothesized structures where durables and nondurables simultaneously enter the separable and nonseparable blocks of goods. In contrast, our models of incomplete adjustment relax this assumption and does not place any restriction on how durables and nondurables enter the separable and nonseparable blocks.==== This allows to test for weak separability with incomplete adjustment on any hypothesized structure.====Varian (1983) and Fleissig and Whitney (2003) proposed and implemented computationally efficient nonparametric revealed preference procedures for weak separability that are sufficient but not necessary.==== Cherchye, Demuynck, De Rock and Hjertstrand (2015) proposed and implemented a computationally efficient mixed integer linear programming procedure for weak separability that is both necessary and sufficient. However, none of these procedures allow for incomplete adjustment.====Swofford and Whitney (1994) derived necessary and sufficient revealed preference restrictions for their restricted model of incomplete adjustment which come in the form of a set of nonlinear inequalities. They then propose a procedure to test for weak separability based on solving a nonlinear optimization problem where the nonlinear revealed preference restrictions act as constraints. There are ==== nonlinear constraints in this problem, where ==== denotes the number of observations, implying that the number of nonlinear constraints grow quadratically with the number of observations. Hence, this can be a computationally difficult problem in practice even for data sets with rather few observations. Moreover, this nonlinear problem only gives locally optimal solutions of the minimal amount of incomplete adjustment required to rationalize the data (given that the problem has a feasible solution). However, since these may not be globally optimal there may exist other feasible solutions which produce lower levels of incomplete adjustment. Hence, any solution to Swofford and Whitney's (1994) nonlinear procedure gives an upper bound on the minimal amount of incomplete adjustment required to rationalize the data.====We derive necessary and sufficient revealed preference restrictions for our model of incomplete adjustment. Using insights from Cherchye et al. (2015), we show how these restrictions can be reformulated as a set of linear inequalities, allowing the restrictions to act as linear constraints in an optimization program. Moreover, we show how the minimal amount of incomplete adjustment can be calculated by minimizing a quadratic objective function. Combining this objective function with the linear constraints implied by the revealed preference restrictions give a mixed integer quadratic programming (MIQP) problem which solves for the minimal amount of incomplete adjustment such that the data can be rationalized by weak separability and incomplete adjustment. The problem is mixed integer because some variables in the problem only take binary values.====Compared to Cherchye et al.’s (2015) procedure with complete adjustment, our procedure is only marginally more computationally involved since it contains ==== additional parameters and is based on solving a quadratic objective function as opposed to a linear objective in Cherchye et al. (2015). In fact, Cherchye et al.’s (2015) problem and our problem share the same computational complexity since both belong to the np-complete class of problems.====Our procedure based on the solving a MIQP problem is computationally simpler to solve than Swofford and Whitney's (1994) nonlinear problem since it consists of linear constraints, and as such, can be applied to larger data sets. From a theoretical point of view, the key motivation for adopting an integer programming approach is that this is a widely accepted and well-known approach to handle np-complete problems.====As previously mentioned, in our empirical application, we address the question of whether nondurable goods and services can be modelled independently from durable goods and services. This is a common assumption in consumer demand modelling, and made to simplify the analysis since a consumer derives utility from the flow of services that the durables provide over time, but is usually never tested prior to the analysis. Thus, modelling durables is inheritably difficult without including some form of adjustment process in the model.====If expenditure on nondurables is independent of the amount consumed of durable goods and services, then it effectively imposes the restriction that nondurables are weakly separable from durables. Thus, the methods and models introduced in here allow us to test this restriction. For this purpose, we use the Encuesta Continua de Presupuestos Familiares (abbreviated ECPF) which is a panel survey data set ranging from 1985-1997 over disaggregated Spanish household expenditures on 25 durable and nondurable consumption goods and services (We use data on 1,585 households). This data set is ideal for our purposes because it is the only consumer survey data set that has a panel structure containing exhaustive and complete information on household expenditures over more than four quarters. Since this allow us to apply our test for weak separability with incomplete adjustment on data from each individual household, we avoid making any assumption of a representative agent. Moreover, by modelling every household individually, we avoid making any preference homogeneity assumption between households, which means that we can effectively account for any form of heterogeneity between households (even any form of unobserved heterogeneity). Thus, using the models and methods introduced here enables us to optimally exploit the panel structure of the ECPF.====We find that none of the households satisfy weak separability with complete adjustment, while only about 8% of the households satisfy weak separability with incomplete adjustment. This indicates that the assumption that nondurables are weakly separable from durables is questionable. Hence, this suggests that models of consumption behavior should contain nondurables as well as durable goods and services and that these categories should be modelled in common since consumption on durables affect optimal expenditure on nondurables.====The paper is organized as follows. The next section introduces our theoretical models of weak separability and utility maximization with incomplete adjustment. In a parametric example, we also illustrate the effects of incomplete adjustment on expenditure and optimal choices. Section 3 derives nonparametric revealed preference restrictions for our models and discusses implementation issues and the computational complexity of the models. Using a parametric example, we also show the ability of our procedures to detect habit formation. Section 4 contains our empirical application and Section 5 concludes.====We include an online appendix, where we first recall the standard weak separability model with complete adjustment (Appendix A). Then, we provide a proof of our main theoretical result (Appendix B), show that our models are empirically refutable (Appendix C), and provide a generalization of our results to homothetic weak separability and homothetic utility maximization (Appendix D). Appendix E provides an additional application of our models and methods, where we derive economically valid monetary aggregates using data on financial and monetary assets, leisure and consumption expenditures from Hjertstrand et al. (2016).",Testing for Weak Separability and Utility Maximization with Incomplete Adjustment,https://www.sciencedirect.com/science/article/pii/S0165188923000775,21 May 2023,2023,Research Article,4.0
He Zhongzhi (Lawrence),"Goodman School of Business, Brock University, St. Catharines, Ontario L2S 3A1, Canada","Received 23 November 2022, Revised 17 March 2023, Accepted 18 May 2023, Available online 19 May 2023, Version of Record 26 May 2023.",https://doi.org/10.1016/j.jedc.2023.104670,Cited by (0),"This paper formulates a game-theoretic reinforcement learning model based on the stochastic gradient method whereby players start from their initial circumstances with dispersed information, using the expected gradient to update choice propensities, and converge to the predicted equilibrium of belief-based models. Gradient-based reinforcement learning (G-RL) entails a model-free simulation method to estimate the gradient of expected payoff with respect to choice propensities in repeated games. As the gradient points to the steepest direction towards discovering steady-state equilibrium, G-RL provides a theoretical justification for a probability-weighed time-varying updating rule that optimally balances the trade-off between reinforcing past successful strategies (‘exploitation’) and exploring other strategies (‘exploration’) in choosing actions. The effectiveness and stability of G-RL are demonstrated in a simulated call market, where both the actual effect and the foregone effect are simultaneously updated during market equilibration. In contrast, the failure of payoff-based reinforcement learning (P-RL) is due to its constant-sensitivity updating rule, which causes an imbalance between exploitation and exploration in complex environments.","Reinforcement learning (RL) is one of the mainstream learning models to describe human behavior in game theory studies. RL uses a set of heuristic, adaptive rules that require no information about other players nor the structure of the game. The rules are said to be payoff-based or completely uncoupled (Nax et al., 2016), because they depend solely on the pattern of a player’s received payoffs to make choices. The empirical success of payoff-based RL (P-RL hereafter) is prominently documented by Roth and Erev (1995) and Erev and Rapoport (1998) that simple reinforcement rules can explain and predict players’ strategic behavior surprisingly well across a wide range of market experiments. Furthermore, the quick convergence of P-RL to Nash equilibrium in market-entry games is so puzzling as to be regarded as a ‘magic’ (Erev and Rapoport, 1998). Despite its remarkable success, why simple reinforcement rules work well and what underlies the magic of quick convergence remain a mystery. In particular, why is there no need to model the beliefs of opponent behavior (i.e., model free) nor assume high rationality in the course of market equilibration?==== In the meantime, however, there has been counter evidence that payoff-based reinforcement rules fail to converge in more complex market institutions such as the call market (e.g., Camerer, Hsia, Ho, 2002, Pouget, 2007).====To shed new light on these issues, this paper formulates a gradient-based reinforcement learning (G-RL hereafter) model based on the stochastic gradient method in machine learning theories (Bottou et al., 2018). In contrast to P-RL that directly updates choice propensities with received payoffs, G-RL updates choice propensities with the gradient of expected payoff with respect to choice propensities at each round of repeated games. It is shown that G-RL converges to the long-run equilibrium results predicted by belief-based models under weak conditions, and actual behavior coincides with the best-response strategies when equilibrium arises. Furthermore, G-RL entails a model-free simulation method to estimate the expected gradient and provides a theoretical refinement of the updating rule. Throughout the equilibrating process, the gradient-based updating rule maintains a balanced trade-off between reinforcing past successful strategies (‘exploitation’) and exploring other underexposed strategies (‘exploration’) in choosing actions.====The main difference between P-RL and G-RL lies in their updating rule for adjusting choice propensities upon receiving a risky payoff. P-RL adjusts the propensity of the chosen action by multiplying the received payoff with a constant sensitivity parameter ====. Such an updating rule works well in simple and low-uncertainty environments, where equilibrium strategies are easily identifiable, and past successful strategies that are frequently reinforced happen to be optimal. The payoff-based updating rule facilitates quick convergence to equilibrium, as documented in market institutions such as market-entry games (Erev and Rapoport, 1998) and Walrasian tatonnement (Pouget, 2007a). In these settings, where the game structure is relatively simple and cognitive difficulty or strategic uncertainty is relatively low, P-RL works well and converges fast because there is no need to conduct an extensive search for other strategies.====However, P-RL fails in complex market institutions such as the call market, where a great deal of trial and error is required to explore all possible strategies to discover the optimal ones. This is because the payoff-based updating rule strengthens past successful but suboptimal strategies at a constant rate, creating a self-reinforcing feedback loop that makes P-RL get stuck in suboptimal strategies and unable to explore other strategies. To address this problem, the learning literature has proposed various search rules for exploration.==== Compared to these ad hoc undirected search rules, the gradient-based rule is theoretically motivated to guide the search in the steepest direction towards discovering equilibrium.====The gradient-based updating rule differs from the payoff-based constant updating in that it adjusts the propensities of both the chosen action (actual effect) and each foregone choice (foregone effect) simultaneously in a time-varying fashion. This is accomplished by multiplying the received payoff with probability-weighted sensitivities. Specifically, the propensity of the chosen action ==== is strengthened with the payoff multiplied by ====, where ==== is the choice probability of action ====; in the meantime, the propensity of each foregone choice ==== is weakened with the same payoff multiplied by ====, due to the constraint that probabilities must sum to one.====Time-vary sensitivities allow an initially successful strategy to experience a larger increase in its propensity (i.e., when ==== is low, ==== is high) following a positive payoff in the early stage of the game, resulting in a larger increase in its choice probability. This encourages exploiting successful strategies before the emergence of a dominant strategy. However, this positive feedback loop cannot continue indefinitely like in P-RL. As ==== grows larger, strategy ==== becomes less sensitive to the received payoff, resulting in a slower increase in its choice probability. This encourages exploring other strategies even when a dominant strategy has emerged, thereby striking a balance between ==== choice ==== and ==== other choice ====. When the gradient approaches zero (==== and ====), no further updating is required, and the game arrives at equilibrium. Therefore, G-RL rectifies the constant-sensitivity updating rule of P-RL by using probability-weighted time-varying sensitivities to achieve a balanced trade-off between exploitation and exploration. This rectification leads to a smooth and stable convergence to the equilibrium results predicted by belief-based models.====Above theoretical properties and convergence results of G-RL, and performance comparison between P-RL and G-RL are demonstrated in a simulated call market of Pouget (2007a), where informed traders and uninformed traders seek both common value and private value of a risky asset. Pouget, 2007, Pouget, 2007 document that neither human subjects nor payoff-based reinforcement rules are able to discover the fully revealing rational-expectations equilibrium (REE) that uniquely exists in the call market. Our simulation attributes the failure of P-RL to its imbalance between exploitation and exploration of its updating rule. Specifically, in the early stage of the game, successful out-of-equilibrium strategies are reinforced more and more often, causing their choice probabilities to quickly saturate to 1.0 and other strategies to get extinguished. In contrast, G-RL avoids the trap of strategic uncertainty such that suboptimal strategies are eventually dominated by equilibrium strategies after a prolonged period of balancing exploitation and exploration in choosing actions. Furthermore, G-RL exhibits the theoretical property of smooth convergence to REE, whereas P-RL shows vastly different out-of-equilibrium results depending randomly on which initially successful strategies tend to be reinforced more often. Finally, our statistical analysis demonstrates that simultaneous updating of both the actual effect and the foregone effect is essential to the stability and robustness of G-RL, making it a theoretically sound and empirically effective model for learning equilibrium in complex and dynamic environments with strategic interactions.",A Gradient-based reinforcement learning model of market equilibration,https://www.sciencedirect.com/science/article/pii/S0165188923000763,19 May 2023,2023,Research Article,5.0
"Bonciani Dario,Oh Joonseok","Bank of England, Threadneedle Street, London EC2R 8AH, United Kingdom,Chair of Macroeconomics, School of Business and Economics, Freie Universität Berlin, Boltzmannstrasse 20, Berlin 14195, Germany","Received 21 February 2022, Revised 14 April 2023, Accepted 2 May 2023, Available online 4 May 2023, Version of Record 23 May 2023.",https://doi.org/10.1016/j.jedc.2023.104668,Cited by (0),This paper revisits the ,"Since the Global Financial Crisis in 2008, nominal interest rates in the U.S. and the Euro Area have approached the zero lower bound (ZLB) and warranted unconventional monetary policies. Eggertsson and Krugman (2012) point out that at the ZLB, in a standard New Keynesian (NK) model, higher price flexibility is destabilising and raises the volatility of output in response to adverse demand shocks (====). The reason is that in a liquidity trap, negative demand shocks can lead to a significant fall in inflation expectations and a deflationary spiral. As the nominal policy rate is stuck at zero, the decline in inflation expectations increases the real rate, which causes a sharp drop in real activity. Under flexible prices, the drop in prices and expectations becomes substantially larger than under sticky prices, which amplifies these adverse effects.====One key determinant of this result is the monetary policy rule. The previous literature has studied the paradox of flexibility using models in which the policy rate is given by a truncated Taylor rule without an interest rate smoothing term. In this paper, we show, using a simple three-equation NK model, that smoothing the shadow rate, i.e., the hypothetical policy rate that would prevail in the absence of a ZLB constraint, can substantially mitigate the effects of adverse demand shocks at the ZLB.==== First, when we consider a Taylor-type rule where the notional interest rate is adjusted smoothly, demand shocks lead to milder output losses. Second, we do not find any paradox of flexibility. In other words, we find that when prices are more flexible, adverse demand shocks have a smaller impact on the output gap. To explain these results, we conduct both an analytical exercise, based on a two-period version of the model, as well as a numerical one using the infinite-period model.====The reason why an inertial monetary policy produces the results above is that smoothing the shadow rate introduces history dependence in the policy reaction function. As a result, at the ZLB, a fall in the shadow rate due to a negative demand shock implies that the future actual policy rate will be relatively low compared to that in the absence of shadow rate inertia. In our numerical results, we show that such an inertial policy rule implies that the central bank will keep the rate at the ZLB longer than the recession induced by the negative demand shock. Introducing the lagged shadow rate into the policy rule is one way to characterise such a “lower-for-longer” policy, which Billi and Galí (2020) define as a form of ====.==== When prices are flexible, the shadow rate falls significantly more, given the initial slump in inflation, which reinforces the forward guidance channel, thereby avoiding deflationary spirals and sharply increasing inflation expectations. As the actual policy rate is at its lower bound, the rise in inflation expectations causes a significant fall in the real rate, which mitigates the contraction in the output gap. When this channel is absent, i.e., in the absence of shadow rate inertia, the central bank in the simple NK model is not able to credibly counteract the effects of an adverse demand shock while in a liquidity trap. Therefore greater price flexibility leads to sharper declines in inflation, inflation expectations, and the output gap compared to a model with more rigid prices. Moreover, we find that the inertial policy can correct the paradox of flexibility even if we mitigate the forward guidance puzzle, following McKay et al. (2017) and Gabaix (2020). Finally, we show that when the central bank smooths the actual rate rather than the shadow rate, its ability to sustain demand and manage expectations is significantly impaired during a liquidity trap. This type of inertial policy, in fact, misses the forward guidance channel mentioned above and, for this reason, cannot correct the paradox of flexibility.",Monetary policy inertia and the paradox of flexibility,https://www.sciencedirect.com/science/article/pii/S016518892300074X,4 May 2023,2023,Research Article,6.0
"Caravello Tomas E.,Psaradakis Zacharias,Sola Martin","Department of Economics, Massachusetts Institute of Technology, U.S.A.,Department of Economics, Mathematics and Statistics, Birkbeck, University of London, U.K.,Department of Economics, Universidad Torcuato Di Tella, Figueroa Alcorta 7350, C1428 Buenos Aires, Argentina","Received 25 August 2022, Revised 27 April 2023, Accepted 28 April 2023, Available online 1 May 2023, Version of Record 13 May 2023.",https://doi.org/10.1016/j.jedc.2023.104666,Cited by (0), are considered. An empirical example focusing on monthly U.S. data on real stock prices and real dividends is also discussed.,"Empirical detection of rational bubbles has been an active area of research for a long time. This is not very surprising since the possibility of substantial price changes occurring independently of movements in the underlying market fundamentals is of considerable interest from both a theoretical and a policy-making point of view. As a result, various approaches to detecting explosiveness associated with rational bubbles have been proposed in the literature – Gürkaynak (2008) and Homm and Breitung (2012) provide useful overviews and comparisons. The focus in this paper is on the popular and influential procedures introduced in Phillips et al. (2011) (PWY henceforth) and later generalized by Phillips et al. (2015a) (PSY henceforth), which rely on right-tailed unit-root tests based on autoregressive specifications which may exhibit explosive behavior over a subset of the data. The PWY and PSY test statistics are suprema of conventional Augmented Dickey–Fuller (ADF) statistics based on suitably constructed subsamples. These statistics are used both to test the unit-root hypothesis against an explosive alternative and to estimate the beginning and end dates of explosive episodes.==== The procedures have been applied to a wide variety of time series, including stock prices (PWY, PSY, Homm, Breitung, 2012, Monschang, Wilfling, 2021, Phillips, Shi, 2020, Phillips, Shi, Yu, 2014), commodity prices (Gutierrez, 2013, Phillips, Yu, 2011), (Etienne, Irwin, Garcia, 2014, Fantazzini, 2016, Harvey, Leybourne, Sollis, Taylor, 2016, Long, Li, Li, 2016), house prices (Greenaway-McGrevy, Phillips, 2016, Hu, Oxley, 2018, Pavlidis, Yusupova, Paya, Peel, Martínez-García, Mack, Grossman, 2016, Phillips, Yu, 2011), and cryptocurrency prices (Bouri, Shahzad, Roubaud, 2019, Cheung, Roca, Su, 2015, Corbet, Lucey, Yarovaya, 2018, Hafner, 2020).====The objective of this paper is to highlight and discuss two potential difficulties encountered in the application of the PWY and PSY recursive techniques for testing for the presence of explosive episodes and for identifying their origination and termination dates, both of which can lead to over-detection of explosive behavior. These issues have not attracted the deserved attention either in the theoretical literature, much of which has focused on the properties of the testing and date-stamping procedures in the presence of periodically collapsing bubbles, or in the empirical literature in which the procedures have been applied to real-world data.====The first issue relates to the specification of the data-generating process (DGP) under the null hypothesis being tested by means of the PWY and PSY supremum-ADF statistics. Following PSY, much of the literature maintains that, under the null hypothesis, the time series of interest behaves like a unit-root (i.e., integrated of order one) process with a local-to-zero drift that is dominated by the stochastic-trend component and vanishes as the length of the series grows beyond all bounds. We argue that the assumption of a weak, vanishing drift is far from innocuous and deviations from this assumption can result in severe over-rejection of the true unit-root hypothesis in favor of an explosive alternative. The effects of possible misspecification of the drift are quantified using Monte Carlo experiments under two different scenarios about the drift characteristics of the data, namely that of a stochastic drift that evolves as a covariance-stationary process and of a fixed drift the magnitude of which reflects estimates obtained from real-world time series (such as real stock prices). We also argue that using asymptotic critical values obtained under a DGP with dominant, non-vanishing drift is not without difficulties either since, unless the drift is substantial, right-tailed unit-root tests tend to under-reject for the sample sizes that are common in applications. Bootstrap-assisted versions of the PWY and PSY testing procedures, allowing for a drift comparable to that found in the observed data, are shown to offer an asymptotically valid – but only partially effective (in finitely-sized samples) – way of dealing with such difficulties. We consider a bootstrap-based technique to calibrate the significance level of tests, thus ensuring that the probability of erroneous rejections of the unit-root hypothesis is as close as possible to the desired target value.====The second issue relates to the dating of the origination and termination dates of multiple explosive episodes based on the PSY crossing principle. The latter amounts to comparing a sequence of recursively constructed ADF statistics to corresponding critical values (thresholds) obtained under a unit-root DGP. We argue that the standard practice of using conventional levels of significance for these critical values fails to guarantee control of the probability of detecting one or more explosive episodes when none are present in the data. Even though the origination and termination dates of (mildly) explosive episodes are consistently estimable, as long as the significance level associated with the relevant critical values approaches zero as the sample size becomes infinitely large, the undesirable tendency of the crossing rule to exaggerate the presence of explosiveness is an issue of concern when fixed significance levels are used. We demonstrate that the use of 5%-level critical values, for instance, in the PSY date-stamping procedure results in at least one explosive episode being detected in more than 70% of artificial samples from a unit-root DGP, the problem becoming more pronounced as the sample size increases. Such findings offer a partial explanation for the relatively large number of apparent explosive episodes identified in applied work that uses the PWY and PSY dating algorithms, and highlight the need for appropriate adjustments to be made in order to control the probability of false detections of explosiveness. We consider how this may be achieved by using a bootstrap-based technique to calibrate the significance level for the relevant critical values.====The importance of these practical considerations and the need for careful application of the PWY and PSY recursive techniques is further highlighted in our empirical study. Using monthly time series of U.S. real stock prices and real dividends for the period 1927:3–2020:6, we demonstrate that results relating to the detection of possible explosive episodes, and to the estimation of their origination and termination dates, can vary considerably depending on how the PSY testing and date-stamping procedures are implemented.====The remainder of the paper is organized as follows. Section 2 gives a brief description of the PWY and PSY testing and date-stamping recursive procedures. Sections 3 and 4 explore the properties of the PWY and PSY test procedures under data-generating mechanisms that involve a stochastic drift component and a non-vanishing drift of fixed magnitude, respectively. Section 5 examines some properties of the PSY dating algorithm in the absence of explosive episodes in the data. Section 6 contains an analysis of the properties of U.S. stock prices and dividends. Section 7 summarizes and concludes.",Rational bubbles: Too many to be true?,https://www.sciencedirect.com/science/article/pii/S0165188923000726,1 May 2023,2023,Research Article,7.0
"Gibson John,Heutel Garth","Department of Economics and Finance and Hunt Institute for Global Competitiveness, University of Texas at El Paso, United States,Georgia State University and NBER, United States","Received 26 April 2022, Revised 10 April 2023, Accepted 26 April 2023, Available online 29 April 2023, Version of Record 9 May 2023.",https://doi.org/10.1016/j.jedc.2023.104665,Cited by (0),". With both policies present, the efficient outcome can be achieved. When one policy is constrained or absent, we solve for the second best. The absence of a vacancy policy to address the congestion externalities substantially affects the value of the emissions tax, both in steady state and over the business cycle.","The effects of environmental policy on unemployment are at the center of the public debate on environmental policy – many argue that these policies are “job-killing regulations.” Since the Great Recession, the importance of business cycles has come to the forefront, highlighting the impact of cycles on unemployment and the environment. Standard neoclassical economic theory dictates that the inefficiencies caused by externalities can be eliminated through Pigouvian pricing, e.g. pollution taxes. Search-and-matching theories of unemployment involve externalities too, where both workers looking for jobs and employers looking to hire create congestion in the labor market. This labor market friction can manifest along the extensive margin – unemployment – or the intensive margin – hours worked. The interaction between multiple market failures – the congestion externalities from labor market search and the production externalities from pollution – can have important policy implications. The efficient Pigouvian response to a negative pollution externality may change when labor market congestion externalities are present, modifying both the intensive and extensive margin for labor. Furthermore, the efficient level of these policies may vary over the business cycle.====The purpose of this paper is to study the efficiency effects of policy in an economy with unemployment, pollution, and business cycles. We begin with an analytical model of policy in the presence of pollution externalities and labor market search frictions. We then develop a dynamic stochastic general equilibrium (DSGE) model where fluctuations are driven by productivity shocks, as in the real business cycle (RBC) model. We add two features to the standard RBC model: a pollution externality and labor market search frictions that create unemployment. The pollution externality can be reduced through investment in a stock of abatement capital. The search frictions can affect both the extensive margin of employment and the intensive margin of hours worked. We allow for two policy options: an emissions tax and a tax or subsidy on job vacancy creation. We calibrate the model to the U.S. economy and carbon dioxide emissions and numerically solve it. We simulate both the first-best policy responses, where the government dynamically optimizes both policy tools, and second-best policies, where one policy is constrained.====A growing literature incorporates environmental policy in RBC models by modeling pollution as a byproduct of production that negatively affects productivity.==== While labor is included as an input in most of these models, none include labor market search frictions or involuntary unemployment. A second literature incorporates involuntary unemployment via Diamond-Mortensen-Pissarides (DMP) labor market search frictions into RBC models.==== ==== This literature does not consider pollution or pollution policy. A third literature studies the unemployment effects of environmental policy using DMP labor market search frictions, though without RBC fluctuations.==== Our model combines all three of these literatures. Our RBC model merges the pollution-generating production process of Heutel (2012) with the DMP search model of Atolia et al. (2018), thus adding cyclical fluctuations to a pollution and labor market search model like Aubert and Chiroleu-Assouline (2019).====Fewer papers consider the policy implications of the externalities generated by the DMP search model. Shi and Wen (1999) and Lu (2019) both consider various policies in addressing these externalities, including the minimum wage, unemployment insurance, and vacancy subsidies. In our paper, we focus on vacancy taxes or subsidies. Shi and Wen (1999) find that this is the most efficient policy, and we also find that this policy can bring about the first best. We consider dynamically-optimal policies, where the policies can vary with the business cycle; this is modeled in neither Shi and Wen (1999) nor Lu (2019).==== An enormous literature studies optimal environmental policy, and some of that literature considers the interactions between pollution externalities and other market failures. For example, Jaffe et al. (2005) model the interaction between pollution externalities and innovation externalities, and Kennedy (1994) models the interaction between pollution externalities and market power. We know of no other paper that considers the policy implications of the interaction between a pollution externality and congestion externalities arising from labor market search frictions.====Our analytical model provides intuitions on the relationship between these multiple market failures and efficient Pigouvian pricing, which aids the interpretation of the simulation results from our numerical model. The pollution externality is a standard negative externality, internalized by a Pigouvian tax on the source of the externality (in the model, production). Labor search creates two offsetting congestion externalities. Each unemployed worker searching for employment reduces the probability of a match for all other unemployed workers but increases the probability of a match for all hiring firms. Each firm posting a job vacancy reduces the probability of a match for all other hiring firms but increases the probability of a match for all unemployed workers. Neither the worker nor the firm considers its impact on others’ matching probabilities. The congestion externality caused by the unemployed worker leads to an inefficiently low level of employment, and the congestion externality caused by the hiring firm leads to an inefficiently high level of employment.====These two congestion externalities perfectly offset each other under a certain condition, the well-known “Hosios condition” (Hosios, 1990). If this condition is not met, then the inefficiency from the congestion externalities can be eliminated through a Pigouvian price on hiring. The efficient price instrument is either a subsidy to hiring or a tax on hiring, depending on whether equilibrium employment is inefficiently high or inefficiently low. We present closed-form expressions for the pair of policy instruments (production tax and hiring tax or subsidy) that induces efficiency. We also demonstrate the interaction between the two sets of externalities and the two policy instruments. For example, the magnitude of the pollution externality affects the level of the hiring tax or subsidy (which targets the congestion externalities).====Our calibrated DSGE model features both an extensive margin (employment vs. unemployment) and an intensive margin (hours worked) in the labor market, compared to previous models in the literature that examine only the extensive margin. By including the intensive margin, we are able to consider how variation in hours worked over the business cycle impacts other labor market dynamics within our model economy. Ultimately, we find that the inclusion of the hours worked margin leads to some undesirable outcomes that are a poor fit to key real-world moments. Furthermore, the relationship between the planner’s problem solution and the decentralized unregulated solution is puzzling in that the level of the (negative) pollution externality is higher for the planner. This is because with the inclusion of the hours margin, the congestion externalities from the search frictions, which lead to too much production and too much pollution, overwhelm the negative pollution externality. For these reasons, we drop the hours worked margin when conducting policy analysis. This demonstrates an important caveat in the application of the DMP model to policy analysis that has not yet been examined.====The calibrated model that we use for policy analysis finds that the unregulated decentralized equilibrium has higher pollution than the efficient outcome, as expected. Employment is also higher in the decentralized equilibrium, because the calibrated parameters in the wage-setting mechanism favor firms, resulting in a greater congestion externality from the firms than from the workers. Unemployment in the decentralized equilibrium is ==== relative to the efficient outcome, and thus a tax on vacancy creation (rather than a subsidy to it) increases efficiency. This direction of the inefficiency (too little unemployment rather than too much) depends on the values of the two parameters crucial to the Hosios condition – the wage bargaining parameter and the matching function elasticity – which we demonstrate in sensitivity analysis. Unemployment is ==== only on grounds of efficiency, not equity, and we show that moving to the efficient outcome disproportionately burdens workers relative to firms.====When both policy instruments – a vacancy tax and an emissions tax – are present, the efficient outcome can be achieved; this is the standard Pigouvian result. Our main results arise when one of the taxes is assumed to be unavailable. We solve for the second-best value of the remaining tax, which must be adjusted to “pick up the slack” and address both sets of externalities. Most importantly, when the vacancy tax is unavailable, the second-best level of the emissions tax is approximately 60 times greater than its first-best value at steady state. Furthermore, the cyclical properties of the two taxes differ, with the second-best emissions tax being significantly more volatile than the first-best emissions tax. Given that vacancy taxes are rarely if ever implemented, these results point to a potentially enormous policy implication for the design of emissions taxes.====The paper proceeds as follows. In Section 2, we present a simple analytical model in which we do not consider business cycles but show through analysis of the steady-state equilibrium the main intuitive findings. Then in Section 3 we present the full DSGE model. In Section 4 we provide a discussion of our calibration strategy. In Section 5 we present our results, and in Section 6 we conclude.",Pollution and labor market search externalities over the business cycle,https://www.sciencedirect.com/science/article/pii/S0165188923000714,29 April 2023,2023,Research Article,8.0
Ahn Hie Joo,"Federal Reserve Board of Governors, 20th Street and Constitution Avenue NW, Washington, DC 20551, USA","Received 28 January 2022, Revised 9 April 2023, Accepted 24 April 2023, Available online 26 April 2023, Version of Record 10 May 2023.",https://doi.org/10.1016/j.jedc.2023.104664,Cited by (0),"This paper develops a new method to estimate the trend unemployment rate in which the duration profile of unemployment hazards is taken into account. The duration profile is characterized by three time-varying elements—====, and ====. The model also recovers the duration component of trend unemployment rate and the trend mean duration of unemployment. The estimated trend unemployment rate declines by about 3 percentage points between 1980 and 2019. Its short-term duration component decreases, whereas the long-term component increases, resulting in a rise in the trend mean duration of unemployment. The opposite trends in duration components suggest falling frictional unemployment but rising structural unemployment.","Estimating the natural rate of unemployment (NRU) has become challenging in the past few decades because the signal from price inflation on the unemployment-rate gap—the difference between the unemployment rate and the NRU—has weakened. Therefore, as an alternative measure of the NRU, economists have paid attention to the trend unemployment rate implied by the trend components of labor market flows or socioeconomic changes in the labor force (e.g., Crump, Eusepi, Giannoni, Sahin, 2019, Hornstein, Kudlyak, 2019, Tasci, 2012). Since the 1990s, however, the mean duration of unemployment has trended up, while the unemployment rate has moved down steadily, indicating changes in the functioning of labor markets. Nevertheless, none of the previous studies have considered the changing distribution of unemployment duration in estimating the trend unemployment rate.==== This paper develops a new method to estimate the trend unemployment rate that accounts for secular changes in the distribution of unemployment duration and the duration dependence in unemployment hazards (Heckman and Singer, 1984).====The key innovation of the proposed method is that it takes signals from low-frequency variations in the duration distribution as well as flows into and out of unemployment for the identification of the trend unemployment rate. To facilitate this identification scheme, I develop a new model of unemployment dynamics in which the probability of unemployed individuals to stay unemployed next month—====—varies over the duration of unemployment.==== This feature is necessary to match the observed distribution of unemployment duration and is also designed to capture the duration dependence of unemployment hazards.==== I model the duration profile of unemployment-continuation probability flexibly yet parsimoniously with three elements—====, and ====—inspired by the modeling technique for the term structure of interest rates by Nelson and Siegel (1985). The duration profile is allowed to change over time and also to exhibit nonlinearity over the duration of unemployment, which may be important features of the duration dependence of unemployment hazards (Kroft et al., 2013).====Why is it important to consider the duration dependence of unemployment-continuation probability in estimating the trend unemployment rate? It is well known that an unemployed individual becomes more likely to stay unemployed longer, because employers may discriminate against the long-term unemployed or unemployed workers may reduce their job-search effort as a result of the discouragement from unsuccessful job searches, among other reasons.==== Theoretically, the duration dependence can be a channel through which cyclical joblessness has lasting effects and hence becomes structural unemployment, referred to as the hysteresis effect (Ball, 2009). To illustrate, a cyclically low job-finding rate of the newly unemployed propagates through to the job-finding rates of the unemployed with longer durations by moving the unemployed individuals along the duration profile of unemployment-continuation probability. In this way, what is originally a cyclical phenomenon has long-lasting effects on unemployment, potentially raising the NRU. Relatedly, Blanchard and Katz (1992) claim that the long-term unemployed either lose skills or become less effective in their job searches and thus their bargaining power decreases, leading to higher long-term unemployment and the NRU.==== Despite the importance, no statistical model of the NRU or trend unemployment rate has explicitly considered the duration dependence. This paper is the first one that develops such a statistical model.====I postulate the duration profile of unemployment-continuation probability with three elements characterized by the Laguerre function: ====, and ====. The Laguerre function is used to describe the temporal dependence of a variable and is adopted in the Nelson–Siegel model of the term structure of interest rates (Nelson and Siegel, 1985). I call this specification ====. This new model is incorporated into the nonlinear state space model of unemployment dynamics by Ahn and Hamilton (2020) to infer the parameters constituting the duration structure of unemployment hazards as well as the inflows to unemployment from the changing distribution of unemployment duration.==== All the model parameters are dynamic latent variables that flexibly vary over time. I then extract the trend component of each latent variable using a Bayesian trend-cycle decomposition in order to recover a trend in the unemployment rate.==== I name the estimated trend unemployment rate the ====, or ====. This model also recovers trends in the duration distribution within the ====, a feature that is distinguished from the previous studies. As a result, the model produces new estimates such as the duration components of trend unemployment rate and the trend mean duration of unemployment.====The novel method uncovers new facts on the trend unemployment rate for 1980–2019. The ==== decreases over time—from about 7 percent in the 1980s to around 4 percent in 2019. The downward trend is mainly led by the short-term duration component. Meanwhile, the long-term component with duration longer than six months rises during the 2000s, driving up the trend mean duration of unemployment. These opposite trends in duration components indicate rising structural unemployment but falling frictional unemployment.==== This observation suggests that the attribute of the trend unemployment rate has changed over time.====The pace of change in the ==== is importantly identified from low-frequency variations in the distribution of unemployment duration. Specifically, the ==== stays largely flat or edges up during the 2000s after trending down in the previous decades, and then declines during the 2010s. The stalled downtrend in the 2000s is driven by the opposite movements in short-term and long-term components of ==== that largely offset each other.==== After the Great Recession, however, the ==== resumes its decline led by both duration components. The ==== ends 2019 at the lowest level since 1980. This anatomy illustrates that secular changes in the duration distribution helps to identify the evolution of the unemployment trend.====Lastly, the ==== and the accompanied duration estimates may serve as comprehensive benchmarks for the evaluation of labor market conditions. After the Great Recession, the trend mean duration of unemployment remains elevated close to the highest level since 1980, a stark contrast to the ==== reaching its lowest level. As a result, the two measures suggest the different magnitudes of labor market slack. The unemployment rate stays below the ==== from 2016 onward, while the mean duration of unemployment continues to stay above the trend mean duration and closes the gap only in 2018. The former indicates that the slack disappears from 2016, while the latter suggests that the slack remains until 2018. This divergence alludes to increased disparity in labor market outcomes: The reemployment prospect of some job seekers may have remained lower than that of others, despite the overall improvement in unemployment incidence. In this context, the trend mean duration of unemployment may provide information about disparities in job-search outcomes. All told, the proposed trend estimates can be useful for the comprehensive assessment of labor market slack.====The rest of this paper is composed as follows. Section 2 introduces a new model of unemployment dynamics with the duration structure of unemployment hazards. Section 3 reports the parameter estimates of nonlinear state space model. Section 4 extracts the trend components of factor estimates and recovers the ====. Section 5 discusses the robustness of ====. Section 6 concludes the paper.",Duration structure of unemployment hazards and the trend unemployment rate,https://www.sciencedirect.com/science/article/pii/S0165188923000702,26 April 2023,2023,Research Article,9.0
"Kuhla Kilian,Willner Sven N,Otto Christian,Levermann Anders","Potsdam Institute for Climate Impact Research, Potsdam, Germany,Institute of Physics, Potsdam University, Potsdam, Germany,Columbia University New York, New York, USA","Received 2 February 2022, Revised 4 April 2023, Accepted 24 April 2023, Available online 25 April 2023, Version of Record 10 May 2023.",https://doi.org/10.1016/j.jedc.2023.104663,Cited by (0),", we find annual median export volume to increase in all trade blocs due to a decrease of export prices, but substantial regional differences emerge. Further we show that resilience of export to typhoon-induced perturbations increase in China, ASEAN, East Asia, and Europe within the first 16 years of this century. We trace this back to a rise of the inter-connectivity of these trade blocs to their foreign trade partners.","Since the end of World War II, international trade has grown immensely (Federico and Tena-Junguito, 2017) and even the COVID-19 pandemic has only shortly interrupted this trend (WTO, 2021). With the increase of global exports, the exposition of the global economy to supply chain risks has increased. To date, up to 90% of the national exposition to insecurities in water, energy, and land resources derive from international trade (Taherzadeh et al., 2021).====Over 80% of the world trade volume is transported on sea (Sirimanne et al., 2019), rendering disruptions of maritime trade routes a substantial threat to global supply chain security. This was impressively demonstrated by the blockage of the Suez-Canal by the container vessel ”Evergreen” in Spring 2021. This resulted in substantial delivery delays and associated production shortfalls, especially in Europe and Asia (cbc, 2021). Besides straits and canals (Martínez-Zarzoso and Bensassi, 2013), ports are major bottlenecks of the maritime trade network (Wendler-Bosco and Nicholson, 2020). In addition to socioeconomic factors such as accidents (Hassanzadeh, 2013) or congestion during loading and unloading (Gidado, 2015, Loh, Zhou, Thai, Wong, Yuen, 2017), storm surges and strong winds from extreme weather are a major source of transport delays in ports (Verschuur et al., 2020).====The increase in the concentration of carbon dioxide in the atmosphere yields in energy accumulating in the oceans (Cheng, Trenberth, Fasullo, Boyer, Abraham, Zhu, 2017, Church, White, Konikow, Domingues, Cogley, Rignot, Gregory, van den Broeke, Monaghan, Velicogna, 2011). The emergence of tropical cyclones such as hurricanes and typhoons follows a complicated mechanism that is subject to energy constraints and shear wind strength. The amount of energy that is accumulating within a tropical cyclone that has already developed is physically constraint by the amount of energy available in the upper ocean layers. Thus, under future warming, though the number of tropical cyclones as a whole is difficult to predict, it is clear that the number of strong typhoons and hurricanes will statistically increase under future warming (Balaguru, Foltz, Leung, Emanuel, 2016, Emanuel, 2005, Emanuel, 2013). Intensifying international trade volume, which is at risk due to transport perturbations by typhoons motivates an investigation of the economic responses of these transportation interruptions.====Linking major Asian exporters such as China and Japan to their European and American trade partners, the West Pacific basin is of predominant importance for international maritime shipping. For instance, the maritime trade of the world’s largest (national) exporter (wor, 2020) — China — relies exclusively on West Pacific shipping routes through the South China and East China Sea. During June and December, maritime transport in the region is put at risk by typhoons. Strong winds, high swells, and poor visibility of these events can cause transport delays due to port closures and re-direction of transport vessels (Brouer, Dirksen, Pisinger, Plum, Vaaben, 2013, Wang, Li, Liu, Zhang, Zou, Cheng, 2014).====In this study we extend the agent-based global supply chain model ==== (Otto et al., 2017) by a geographic network of maritime trade routes. We employ the model to study the economic repercussions of transport disruptions of recurrent typhoons. Immediate responses of trading partners to transportation perturbations caused by typhoons are referred to as direct impacts. Further, we compute and analyze ==== or ==== impacts, which arise due to economic ripple propagation along supply chains, e.g, through price fluctuation or demand shifts.====Including the shocks by the typhoons of 2000–2020 in the model, we compute that average export volumes increase in all major trade blocs due to a decrease of export prices. However, substantial regional differences occur. In all but one trade bloc the decrease in export prices is overcompensated by the increase in export volume resulting in an increase of the value of exported goods; in China, as the most strongly affected trade bloc, a moderate increase in export volume is overcompensated by the decline in export prices. We also study how the topography of the underlying trade network affects the resilience of international trade to typhoon strikes. Using exports as a measure for international trade, we find that its resilience to typhoon-induced transportation perturbation has increased over the period 2000–2015 especially in China, ASEAN, East Asia, and Europe. We explain these gains in resilience by an increase of the inter-connectivity of these trade blocs to foreign trade partners within the study period.====This paper is organized as follows. We first review the related literature in Sec. 2, before presenting our modeling approach in Sec. 3. In Sec. 4 we present our modeling results and before discussing them in Sec. 5.",Resilience of international trade to typhoon-related supply disruptions,https://www.sciencedirect.com/science/article/pii/S0165188923000696,25 April 2023,2023,Research Article,10.0
"Alamgir Farzana,Cotoc Johnny,Johri Alok","Department of Economics, McMaster University, 1280 Main St. West, Hamilton, Ontario L8S4M4, Canada","Received 22 November 2022, Revised 10 April 2023, Accepted 20 April 2023, Available online 23 April 2023, Version of Record 1 May 2023.",https://doi.org/10.1016/j.jedc.2023.104662,Cited by (0),"Sovereign spreads and the level of bureaucratic diversion of government spending vary widely across emerging economies and are correlated with each other. We build a ==== model where the government is constrained to use corrupt bureaucrats to deliver public goods and services in order to explain these facts. The diversion policy parameters are estimated using data on public resources and monitoring efficiency and used to calibrate the model. We use data on the average gift needed to be given to win public contracts in a country as a measure of bureaucratic diversion because it allows us to quantify diversion of public resources whereas tax evasion is hard to measure. We tie down the efficiency level to the Rule of Law index. We show that economies with low monitoring efficiency display higher diversion levels and higher default risk (and spreads) than those with higher efficiency. These results emerge because defaults reduce diversion levels and this benefit from default is higher for low monitoring efficiency economies, which encourages default.","Emerging nations are well known to be plagued with corrupt officials who divert resources from government projects for their own use. The World Bank Enterprise Survey collects data from several countries on the value of “gifts” that must be given to officials as a percent of the value of a public contract in order to win that contract. We will refer to the average value of this percentage as the bribe rate of that country. Interestingly, the bribe rate shows remarkable variation across nations, from a low of 0.1 percent to a high of 9 percent.====Emerging economies also pay widely differing interest rates on their sovereign debt when these rates are averaged over many years. Figure 1 reports the average spread of sovereign bonds for various nations contained in the EMBI Global Index over U.S. ten year treasuries averaged between 1995 and 2015. Across nations, once fluctuations are averaged out, we see dramatic variation in the sovereign spread level. This has important welfare implications for the citizens of these emerging nations because higher interest costs limit the ability of their governments to use international markets to provide government goods and services, especially when domestic sources of revenue fall.====We begin by showing that countries with high bribe rates are also countries with high spreads. Theory suggests that since higher debt levels relative to domestic revenue sources come with higher default risk, international lenders will offer loan terms in which higher indebtedness and interest rates go together. As a result, we must condition our correlation between spreads and bribe rates with the external debt of each nation as a percentage of government revenue. We find that a nation with a bribe rate that is one percent higher, tends to pays roughly 33 basis points more in interest costs, holding indebtedness constant. In the next section we will show that this positive conditional correlation is remarkably insensitive to the inclusion of various other domestic and global factors, including the level of government indebtedness as well as detrended government revenue.====Before discussing the model used to explain this correlation, it may be worth emphasizing a few things about our empirical choices. Other studies have shown that spreads are positively correlated with corruption perception measures but a problem with these measures is that they do not quantify the extent of stealing from the public purse. Moreover, they do not help to distinguish between various possible types of corruption, some of which may be more or less relevant for the government budget. After all, international lenders are unlikely to care about corruption unless it affects default risk through the budget. A high corruption index could reflect pervasive tax evasion or it could reflect diversion of resources from public projects (as is the case with the bribe rate) but it could also reflect other forms of corruption which do not directly affect the government budget constraint.==== In order to calibrate our model, therefore, we found the bribe rate to be quite useful (relative to generalized perception of corruption indexes) because it allows us to measure bureaucratic diversion from government spending. In our empirical work we can control for unmeasured objects such as tax evasion by working with actual revenue collected by the government as opposed to GDP which is more conventionally used in spread regressions. As a result, the correlation between spreads and bribe rates uncovered in our work cannot be explained away by tax leakages that reduce the ability of the government to repay its debt. For consistency with the empirical work, we also switch off any sources of revenue distortion in the model either due to tax evasion, or reduction in the tax base but we expect that if this were allowed it would generate even stronger results.====In order to explain the correlation between bribe rates and spreads, we build a sovereign default model with long-term external debt where the government is constrained to use corrupt bureaucrats to deliver public goods and services. In our quantitative sovereign default model, both international lenders and the sovereign government are aware that despite monitoring, a time-varying fraction of the resources allocated to public spending will be diverted by bureaucrats creating a wedge between public spending and the level of public services actually available in the economy. In choosing their optimal debt levels and default policy, governments of different economies take the optimal diversion policy of bureaucrats into account. This diversion policy depends on the efficiency with which bureaucrats can be monitored which varies across economies and underpins the positive correlation between default risk and bureaucratic diversion levels. We tie down the monitoring efficiency level to the Rule of Law index and estimate the diversion policy function using our assembled emerging economy data that combines fiscal information with the bribe rate and the Rule of Law index and use the estimated parameters to calibrate our quantitative model. The estimates imply that the amount of diversion is increasing in the size of government resources and decreasing in the Rule of Law index.====We use the model to show that artificial economies with low monitoring efficiency display high diversion as well as high default risk and spreads. Running the same regression on our artificial data as the international data, we find that spreads are positively associated with the bribe (gift) rate and with debt levels. These results emerge because defaults come with the ==== of reducing bureaucratic diversion and this benefit is shown to be inversely related to monitoring efficiency. Note that the usual mechanism found in sovereign default models is also present here - defaults provide the government with fiscal room to provide services since foreign obligations are not met. The model also implies that economies with better monitoring efficiency will borrow more than their counterparts with lower efficiency. We show that this is also a feature of our international data.====Next, we show that if the bureaucratic diversion policy did not display pro-cyclicality (with respect to incoming government resources) and instead was solely a function of monitoring efficiency of the nation then only a constant amount of resources would be diverted from government spending. As a result, the incremental benefit of default in the form of lower diversion would be eliminated and the model would be unable to deliver the empirical correlation between spreads and bribe rates i.e., economies that differ in diversion levels would have roughly the same equilibrium default frequency and spread. This exercise is important because it shines light on a common prior about the model – default risk is higher in economies with more corruption because the government has fewer resources. So the bigger the diversion, the higher the default risk. While technically correct, this effect is too small to account for significant variation in spreads across countries. The larger quantitative effect built into our model is more subtle. Default risk increases if the value of defaulting relative to the value of repaying debt goes up. As explained above, this relative value is a function of the diversion policy of bureaucrats. Since diversion falls in default, the wedge between government expenditure and government services is reduced, making default more attractive. If the same amount of resources were diverted in default as under repayment, this additional gain from default would be absent and variation in monitoring efficiency would have little impact on spreads.",The bribe rate and long run differences in sovereign borrowing costs,https://www.sciencedirect.com/science/article/pii/S0165188923000684,23 April 2023,2023,Research Article,11.0
Qian Wei,"School of Economics, Shanghai University of Finance and Economics, 111 Wuchuan Road, Shanghai 200433, China","Received 12 April 2020, Revised 31 August 2022, Accepted 3 April 2023, Available online 5 April 2023, Version of Record 21 April 2023.",https://doi.org/10.1016/j.jedc.2023.104652,Cited by (1),This paper investigates the relationship between individuals’ ,"Housing is an important component of household wealth and is also an important form of collateral for households to secure loans. On average, homeowners are ninety times as wealthy (in terms of net worth) and take on eight times as much debt as renters (Census Bureau, 2014). The importance of housing implies that house prices may play a key role in people's consumption decisions. Previous research (Cooper, 2013; Mian et al., 2013; Kaplan et al., 2020a; Aladangady, 2017) has documented a large marginal propensity to consume out of housing wealth, particularly among credit-constrained households. However, the existing empirical literature has focused predominantly on the impact of realized house price changes, paying less attention to people's expectations of future house prices. Yet economic theories predict that individuals’ consumption at a point in time are determined not only by realized outcomes but also by what they anticipate to happen in future years. This suggests that expectations of future house prices, not just current house prices, are likely to contribute to individuals’ spending behavior.====The objective of this paper is to provide empirical evidence on the relationship between individuals’ house price expectations and their spending intentions. The analysis is based on microdata from the New York Fed Survey of Consumer Expectations and the Michigan Survey of Consumers. Subjective data have been increasingly used to analyze the effect of individual expectations,==== and the advantages of using survey data are at least threefold. First, survey data provide us with information about expectations of individuals, and these expectations data allow researchers to estimate the effect of house price expectations for actual decision makers. Second, there is a substantial amount of variation across individual expectations that can be used to identify parameter of interest. Also, some surveys employ a rotating-panel design that let researchers control for unobserved individual fixed effects. Finally, using the available demographics in the surveys, I can investigate the presence of heterogeneity in the spending response to house price expectations, which helps explain the mechanisms through which house price expectations affect spending behavior.====The Survey of Consumer Expectations provides quantitative measures of individuals’ house price expectations and their expected changes in total household spending. I find that, in the cross section, individuals’ nationwide house price expectations are significantly correlated with their intended household spending, even after controlling for a large number of variables including expected income growth. Specifically, a one percentage point increase in one-year-ahead nationwide house price expectations is associated with an approximate 0.09 percentage point increase in annual intended household spending growth. Exploiting the rotating panel structure of the Survey of Consumer Expectations, I show that this relationship is not driven by unobserved time-invariant individual characteristics.====I complement the results from the Survey of Consumer Expectations with additional evidence from the Michigan Survey of Consumers. Although the expectation measures in the Michigan Survey are mostly qualitative, it complements the analysis because it features local house price expectations. Using an ordered probit model, I show that people with higher community-level house price expectations also demonstrate greater willingness to spend on durables such as major household items, vehicles, and houses.====I explore the mechanisms underlying the main results by studying the heterogeneous responses to house price expectations in the cross section of the Survey of Consumer Expectations. The results suggest that the relationship between house price expectations and intended household spending is stronger among respondents who are more likely to be borrowing constrained. Specifically, I find that house price expectations and intended household spending covary more strongly for low-income households, for people who report that their credit conditions have worsened during the past 12 months, and for people under the age of 40. Together, the results are consistent with house price expectations affecting spending behavior through a borrowing collateral channel: respondents increase their household spending in anticipation of a relaxation in their borrowing constraints.====In addition, uncertainty has a positive effect on the spending response to house price expectations. Specifically, I find that people who are more uncertain about future house price growth respond more to house price expectations. This may be because defaultable mortgages allow people to discount the risk of price drops and take advantage of the potential upside when volatility rises. Consistent with this explanation, I find that uncertainty increases the spending response to house price expectations for those who are likely to default on mortgage payments.====This paper is motivated by the theoretical literature that emphasizes the role of house price expectations around the Great Recession. Kaplan et al. (2020b) argue that when we take into account the rental market and long-term defaultable mortgages, changes in income and credit supply have a limited effect on aggregate house prices. Instead, house price expectations have been the principal drivers of the fluctuations in house prices around the Great Recession. In addition, Brueckner et al. (2012, 2016) argue that the relaxation of underwriting standards is a consequence of favorable house price expectations. Their model suggests that when house prices are expected to appreciate, lenders become less concerned about default and therefore more eager to make subprime loans and sell alternative mortgage products. In this paper, I provide empirical evidence which endorses the view that house price expectations have a significant influence on people's economic decisions.====In terms of methodology, I follow a large literature which attempts to identify the effect of expectations using survey data. For example, Bachmann et al. (2015) and Duca et al. (2021) estimate the relationship between inflation expectations and consumer spending using respectively the Michigan Survey and the EU Consumer Survey. Gennaioli et al. (2016) show that CFOs’ expectations of earnings growth predict firms’ actual investment using the Duke University CFO Survey. Due to data limitations, most analyses of expectations use only cross-sectional variation. I exploit the New York Fed Survey's rotating-panel structure and use the within-individual variation to measure the effect of house price expectations. This specification allows me to identify the linkage between house price expectations and household spending more accurately through controlling for time-invariant individual characteristics.====This paper also contributes to the growing literature studying the effect of house price expectations on individual behavior, such as consumption decisions (Lambertini et al., 2013; Bover, 2015; Qian, 2022), mortgage leverage choices (Stefani, 2017; Bailey et al., 2019) and investment decisions (Armona et al., 2019). For example, Bover (2015) shows that pessimism about future house prices predicts a low probability of buying durables using data from a Spanish survey. In a companion paper Qian (2022), I conduct a survey-based experiment and find that higher house price expectations lead to higher intended household spending. This paper provides additional evidence from the field survey data on the positive relationship between individuals’ house price expectations and their spending intentions utilizing both cross-sectional and within-individual variations.====The remainder of the paper is organized as follows. Section 2 describes the survey data. Section 3 discusses the basic econometric frameworks for the empirical investigation. Section 4 shows the main results. Section 5 explores the mechanisms that explain the influence of house price expectations on households’ spending decisions. Section 6 considers the role of uncertainty in the relationship between house price expectations and intended household spending. Section 7 tests the robustness of the baseline results. Section 8 concludes.",House price expectations and household consumption,https://www.sciencedirect.com/science/article/pii/S0165188923000581,5 April 2023,2023,Research Article,12.0
"Shintani Mototsugu,Ueda Kozo","Faculty of Economics, The University of Tokyo, Tokyo, 113-0033, Japan,School of Political Science and Economics, Waseda University, Tokyo, 169-8050, Japan","Received 23 August 2022, Revised 31 January 2023, Accepted 3 April 2023, Available online 5 April 2023, Version of Record 12 April 2023.",https://doi.org/10.1016/j.jedc.2023.104653,Cited by (0),"Coibion and Gorodnichenko (2015) provide a useful framework to test the null hypothesis of full-information rational expectations against two popular classes of information rigidities, sticky information (SI) and noisy information (NI). However, the observational equivalence of SI and NI in average forecast errors gives no power in the test for one against the other. We identify the source of information rigidities by estimating the equations for the average forecast errors and variance of forecasts. The results show the importance of both SI and NI, but favor a type of NI in which agents can quickly learn the underlying state.","Coibion and Gorodnichenko (2015) introduce a useful regression framework to test the null hypothesis of full-information rational expectations against the alternative hypothesis of rational expectations in the presence of information rigidities. In particular, their proposed test, which is based on a regression of average forecast errors on the average forecast revision, has power against two popular classes of information rigidities frequently employed in the macroeconomic literature: sticky information (SI) and noisy information (NI). Once deviation from the full-information model is confirmed by data using Coibion and Gorodnichenko’s regression, it is natural to examine which one of the two classes of information rigidities is more appropriate in describing the actual expectation formation process in the next step. However, the observational equivalence of SI and NI in their regression makes it impossible to construct a testing procedure to distinguish between the two. In other words, a test for the null hypothesis of NI has no power against an alternative of SI, and vice versa. Furthermore, there is potentially another type of NI different from the one assumed in Coibion and Gorodnichenko (2015). Since desirable policy prescriptions can depend on the sources of information rigidities, we wish to distinguish between SI and NI and between different types of NI.====In this paper, we discuss the identification issue of the two sources of information rigidities in Coibion and Gorodnichenko’s regression framework. To clarify the issue, we first construct a simple hybrid model of SI and NI and show the implications of the model for the cross-sectional average of forecast errors. Second, we show the implications of the hybrid model for the cross-sectional variance of forecasts. We then shut down one of the two sources of information rigidity to consider the conditions for distinguishing SI from NI and/or NI from SI.====The three main outcomes from our analytical exercise are as follows. First, the identification of the source of information rigidity based solely on the cross-sectional average of forecast errors crucially depends on the speed of learning in NI models. Specifically, if an underlying state becomes common knowledge at the end of each period, as in the case of Lucas et al. (1972), Angeletos and La’O (2009), and Crucini et al. (2015), among others, additional terms appearing in Coibion and Gorodnichenko’s regression enable the identification of NI and SI. This result is in contrast with the observational equivalence of SI and NI in their regression when realizations are never revealed to agents in the NI model, as in the case of Woodford (2003) and Coibion, Gorodnichenko, 2012, Coibion, Gorodnichenko, 2015, among others. Second, as appropriately pointed out by Coibion and Gorodnichenko (2012), the dependence of disagreements among agents on aggregate shocks is a useful feature in distinguishing NI from SI. We further derive a simple analytical form for the cross-sectional variance that is useful in identifying SI from NI, irrespective of the specification of NI. Accordingly, testing the null hypothesis of NI based on cross-sectional variance has a power against an alternative of SI, and vice versa. Third, we point out that the joint estimation of the two equations, one for average forecast errors and the other for cross-sectional variance, is helpful, not only from the perspective of identification, but also because of the efficiency in estimating the structural parameters.====Basing our analysis on these considerations, we revisit the empirical findings of Coibion and Gorodnichenko (2015) by using the same dataset on inflation forecasts from the U.S. Survey of Professional Forecasters (SPF).==== The results of our empirical analyses are summarized as follows. First, from the single-equation estimation for average forecast errors, we find that the null hypothesis of pure SI is not rejected against the alternative of pure (Lucas et al., 1972)-type NI. Second, from the single-equation estimation for cross-sectional variance, we find that a time variation in variance is at odds with pure NI, while the non-zero constant of variance is inconsistent with pure SI. The null hypothesis of pure NI is therefore rejected against the alternative of pure SI, while the null hypothesis of pure SI is also rejected against the alternative of pure NI. This outcome suggests that there is room for additional forms of information rigidities to fully explain the data. Third, from the joint estimation of the two equations for pure models of information rigidities, the nonnested test suggests that the null hypothesis of the pure SI model is not rejected against both types of pure NI models, while the null hypotheses of the two types of pure NI models are significantly rejected against the alternative of other pure models of information rigidities. Fourth, from the joint estimation of the two equations for hybrid models of information rigidities, we find a nonnegligible degree of information stickiness, but pure SI outperforms hybrid models if we allow an ==== constant for cross-sectional variance. At the same time, while the information noise is relatively small, the formulation of Lucas et al. (1972) better fits the data than that of Woodford (2003) for the NI part of the hybrid model.====The two most closely related papers, other than (Coibion, Gorodnichenko, 2012, Coibion, Gorodnichenko, 2015), are (Andrade and LeBihan, 2013) and (Hur and Kim, 2016), that also propose using cross-sectional variance to distinguish NI and SI. Specifically, Andrade and LeBihan (2013) consider a hybrid model, although they do not derive simple analytical forms as we do. Hur and Kim (2016) derive analytical expressions for the evolutions of cross-sectional variance in SI and NI models, although they do not consider a hybrid model or use those to test the presence of SI versus NI. The novelties of our study are that the analytical expressions are derived for the hybrid model and that some of these expressions allow us to distinguish among the different type (i.e. Lucas-type) of NI. Further, we conduct rigorous empirical analyses to identify NI and SI and estimate the degree of information rigidities.====The remainder of this study is structured as follows. We discuss the main implications of our model of information rigidities on the average forecast errors and cross-sectional variance of forecasts in Sections 2 and 3, respectively. The joint estimation of the average and variance equations for pure models of information rigidities is conducted in Section 4, followed by the joint estimation for hybrid models of information rigidities in Section 5. Section 6 concludes our discussion.",Identifying the source of information rigidities in the expectations formation process,https://www.sciencedirect.com/science/article/pii/S0165188923000593,5 April 2023,2023,Research Article,13.0
"Dosi Giovanni,Lamperti Francesco,Mazzucato Mariana,Napoletano Mauro,Roventini Andrea","Institute of Economics and EMbeDS, Scuola Superiore Sant’Anna, Pisa, Italy,RFF-CMCC European Institute on Economics and the Environment, Milan, Italy,Institute for Public Purpose and Policy, University College London London, UK,Université Côte D’Azur, CNRS, GREDEG, France,Sciences Po, OFCE, France,Scuola Superiore Sant’Anna, Pisa, Italy","Received 6 August 2021, Revised 16 December 2022, Accepted 26 March 2023, Available online 30 March 2023, Version of Record 10 May 2023.",https://doi.org/10.1016/j.jedc.2023.104650,Cited by (0),"We study the impact of alternative innovation policies on the short- and long-run performance of the economy, as well as on public finances, extending the ==== effects on growth dynamics. For the same size of public resources allocated to market-based interventions, “Mission” innovation policies deliver significantly better aggregate performance if the government is patient enough and willing to bear the intrinsic risks related to innovative activities.","In this paper, we extend the ==== agent-based model (Dosi et al., 2010) to assess the impact of different innovation policies on the short- and long-run performance of the economy, as well as on the public budget.====The stagnating aftermaths of the Great Recession and, more recently, of the COVID-19 pandemics, call for public policies able to restore robust economic growth. Such crises also exacerbated the pre-existing productivity slowdown experienced by most developed economies. This implies that government should introduce policies to influence the pace of innovation and technological change, which are the major drivers of long-run economic growth. The Next Generation EU program released by the European Commission goes explicitly in this direction. However, in our view, the contemporary discourse on innovation policies has been far too narrow, quite disjoint from their implications for the economic and social future of our societies. In fact, it is remarkable that, in the past, some of the most important “innovation policies” were not called as such. The Manhattan Project, the Apollo Program, Nixon’s “war on cancer” were not discussed, if at all, as “policies” but as major societal objectives, well shielded from the narrow concerns of economists’ cost-benefit analyses. On the contrary, nowadays, innovation policies - except for war-related innovations and pandemic emergencies - have to pass through the dire straits of ==== criteria. However, even on these narrower grounds, we shall show, innovation policies are well worth.====Innovation policies (written large, and meant to include science and technology policies) broadly refer to the design of a variety of instruments aimed at generating new knowledge, new products and more efficient production techniques (within an enormous literature, see from Bush et al., 1945 to Criscuolo, Gonne, Kitazawa, Lalanne, 2020, Edler, Fagerberg, 2017, Freeman, Soete, 1997). Depending on the type and scope of the policy tools employed, innovation policy might require more or less extensive involvement of the public sector in the economy. A broad distinction is between ==== and ==== innovation policies (Dosi, 1988, Dosi, Nelson, 2010, Mazzucato, Semieniuk, 2017). Indirect policies tend to be “market-friendly” as they provide monetary incentives to firms to improve their innovative performance (e.g. R&D subsidies) or to speed-up their technological renewal (e.g., investment tax discounts). In an influential debate at the OECD in the early 80s, they were called “diffusion-oriented” policies (Ergas, 1987). Differently, direct innovation policies imply an active of role of the public sector in shaping the rates and directions of innovative activities, which means - to paraphrase Nelson (1962) - shaping technological landscape and search regimes, taking risks that private businesses are not willing to sustain, and pursuing path-breaking technological developments. Direct innovation policies respond to Freeman (1987) plea for policies creating systems and institutions able to nurture the generation and diffusion of new knowledge across the economy, the creation of new industries and markets and - ultimately - to fuel economic growth. These policies may certainly be facilitated by an ==== (Mazzucato, 2013) that takes the lead and directly invests in the search for novel technological opportunities (possibly directed to specific missions; see also Mazzucato, 2018a and Mazzucato, 2021).====The ability of alternative innovation policies to spur innovation, crowd in private investment and deliver sustained long-run growth is highly debated. Notwithstanding a large body of studies evaluating single policies (see Becker, 2015, for a survey), systematic comparisons of policy designs are scarce in the literature (Grilli et al., 2018), especially from a macroeconomic perspective (Di Comite and Kancs, 2015). A recent review by Bloom et al. (2019) discusses pros and cons of various instruments, suggesting a trade-off between the short run, where tax incentives and subsides are effective in stimulating innovation, and long run outcomes, which would benefit from systemic investments in universities and education. However, Bloom and co-authors overlook (or dismiss) ====, based on the argument that the effects of these policies are hard to be identified econometrically. In addition, those policies, it is suggested, lack an economic rationale - of course in terms of the conventional economic theory, according to which were it not for market failures and externalities, one better leave the market and the search for innovations to itself.====In this work, we shall indeed show the robust rational of direct policies in complex evolving economies. We extend the ==== (K+S) macroeconomic agent-based model (Dosi et al., 2010) to systematically compare the impact of direct and indirect innovation policies on economic performance via their stimulus to incremental and radical innovation, while accounting for their impact on the public budget.==== In that, the paper also contributes to the literature about modelling of R&D, innovation activities and their impacts on the macroeconomy, integrating the representation of technological change, its sources and consequences within an agent-based perspective (for germane contributions see Caiani, Russo, Gallegati, 2019, Dawid, Gemkow, Harting, Kabus, Neugart, Wersching, 2008, Dosi, Roventini, Russo, 2019, Fagiolo, Giachini, Roventini, 2020, Gräbner, Hornykewycz, 2022, Lorentz, Ciarli, Savona, Valente, 2016, Russo, Catalano, Gaffeo, Gallegati, Napoletano, 2007, the survey in Dawid, 2006 and the recent critical review by Aistleitner et al. (2021), wherein multiple modeling approaches are discussed). Indeed, we believe that a first-order systematic comparison between “Entrepreneurial State”-like policies and price-based R&D incentives is missing in the literature linking macroeconomic dynamics and technical change, and it would be better carried out abstracting from choice of the particular sectors and missions to target (which is highly arbitrary and possibly affected by political considerations), though keeping vivid the spirit of mission orientation (Mazzucato, 2013).====The K+S model is composed of two vertically-related sectors, wherein heterogeneous firms strive to develop new technologies and locally interact by exchanging capital-goods in a market with imperfect information. This is the Schumpeterian engine of the model: new machine tools are discovered and diffuse within the economy both via imitation activities of competing capital-good producers and via investment by consumption-good firms. Firm investment depends on firm demand expectations, as well as on their financial conditions and it constitutes, together with worker consumption and public expenditures, the Keynesian soul of the model. Aggregate demand dynamics in the model affects not only business cycles, but also the pace of technological change (see e.g. Dosi et al., 2016). The K+S model is therefore able to go beyond the traditional separation between “coordination” and “change” in economics (Dosi and Virgillito, 2017).====Indeed, the K+S family of models represents flexible environments which can be used as virtual laboratories for policy experiments to investigate a variety of policy interventions and perform counter-factual analyses. We examine four stylized innovation policy regimes and their possible combinations, namely (i) R&D subsidies to capital-good firms; (ii) tax-discounts on consumption-good firms’ investments; (iii) the creation of a public research-oriented capital-good firm; (iv) the institution of a National Research Laboratory which tries to discover radical innovations that enlarge the set of technological opportunities available in the economy. The first two experiments mimic indirect innovation policies, while the latter pair captures key features of direct or “Entrepreneurial-State” policies, leaving aside the decision of which mission(s) to target. Finally, we consider a benchmark scenario where the public resources is used to support private consumption instead of innovation policies.====Simulation results show remarkable differences across innovation policy regimes. First, all innovation policies spur productivity and GDP growth, but to different degrees, while this is not the case for transfers to households. Second, the impact of direct innovation policies is larger vis-à-vis indirect ones and entails effects of ==== (Cerra, Fatás, Saxena, Dosi, Pereira, Roventini, Virgillito, 2018) putting GDP on higher growth trajectories. However, Entrepreneurial-State policies are risky: their positive impact tend to show up on longer time horizons as compared with indirect interventions, and they can fail to discover new technologies. Nonetheless, extensive Monte Carlo analyses show that, on average, direct innovation policies deliver higher productivity and GDP growth, while being less expensive in terms of net public resources, compared to “indirect” forms of intervention. The impact of Entrepreneurial-State interventions is stronger when they combine the presence a public firm with a National Research Laboratory. Conversely, indirect monetary incentives tend to be associated with some redundancy - that is transfer of resources to firms with little effect on the intensity of search. Finally, all innovation policies we consider crowd in private R&D investment (in line with Moretti et al., 2019 and Pallante et al., 2020), although direct interventions provide, again, the most bang for their buck. Accordingly, our results suggest that the type of tools utilised by a mission-oriented Entrepreneurial State (Mazzucato, 2013, Mazzucato, 2018, Mazzucato, 2021) are also more effective at meeting uncontroversial innovation policy goals of productivity and growth gains.====To sum up, our results indicate that innovation policies are highly effective. In particular, when public resources are concentrated on clear missions and Entrepreneurial-State interventions, they appear to deliver large gains in economic performance compared to policies based on monetary incentives. This should be taken into account by policy makers when designing vast policy plans such as the Next Generation EU to jump-start growth in economies hardly hit by the COVID-19 crisis.====The rest of the paper is organized as follows. Section 2 provides a critical overview of the literature on innovation policies. In Section 3, the K+S model is introduced. The empirical validation of the model is performed in Section 4. In Section 5, we present the results of innovation policy experiments. Finally, Section 6 concludes the paper.",Mission-oriented policies and the “Entrepreneurial State” at work: An agent-based exploration,https://www.sciencedirect.com/science/article/pii/S0165188923000568,30 March 2023,2023,Research Article,14.0
Werner Maximilian,"Universität Zürich, Chair of Quantitative Business Administration, Plattenstrasse 14, CH - 8032 Zurich, Switzerland","Received 18 February 2021, Revised 13 December 2022, Accepted 1 February 2023, Available online 24 March 2023, Version of Record 13 April 2023.",https://doi.org/10.1016/j.jedc.2023.104609,Cited by (0),What drives nonlinearities in modern ,"Simplifying occasionally binding constraints (OBCs) yields the advantage that standard methods remain applicable. Treating an OBC as if it were just always binding is popular in the macro-finance literature, even though OBCs are often the centerpieces of these models. In this paper, we investigate the global nonlinearities arising from an OBC as well as the losses in (quantitative) economics associated with this simplification in the Kiyotaki and Moore (2019) RBC framework. The key findings are that steady state–related concepts of macroeconomic analysis as first-order moments or impulse responses are, at least qualitatively, quite resilient with regard to the respective solution concept. Away from the steady state, however, the global impact of potentially large or consecutive shocks unfolds heavy distortions in tail distributions and thus in individual policy functions.==== The key mechanism is the dynamic interplay of the liquidity constraint, asset prices, and investment behavior, which induces a local discontinuity and globally shifts asset prices via anticipation effects.====In Kiyotaki and Moore (2019; KM henceforth), the joint interaction of financial shocks and financial constraints is modeled in the context of asset market liquidity. Aggregate investment as well as the overall trading volume in the equity market must jointly satisfy a liquidity constraint. The tightness of this constraint depends on the aggregate state, which particularly includes separate shocks to the “re-salability” of equity. The KM liquidity framework has been adopted in numerous studies that, for instance, discuss liquidity shocks as a potential source of business cycle fluctuations and/or their contributions to asset pricing puzzles (e.g., Ajello, 2016, Bigio, Schneider, 2017, Nezafat, Slavík, Shi, 2015, and Cui and Radde 2020). Furthermore, it also constitutes the foundation of the pioneering paper of Del Negro et al. (2017), who analyze massive injections of liquid assets as an instrument of unconventional monetary policy in times of financial crisis. While the majority of papers, including Del Negro et al. (2017), restrict their analysis to a constraint that is assumed to be globally binding, the present paper focuses on the impact of a liquidity constraint that is binding only occasionally.====Therein, we contribute to the literature by advancing the equity-only approaches of Bigio and Schneider (2017) and Nezafat and Slavík (2015) to the two-asset structure underlying the open-market operations considered in Del Negro et al. (2017). In contrast to the equity-only approaches, the OBC in this framework operates through a discontinuity in capital accumulation and triggers global nonlinearities. To capture the latter, we solve the model twice, using two global and nonlinear algorithms: while the ==== (OB) correctly solves for the OBC, the ==== (AB) instead implicitly assumes that it binds permanently. Since both methods are identical except for the technical treatment of the constraint, this approach isolates the global nonlinearities directly stemming from the OBC.====As a matter of course, the nature of this approach allows for two forms of readings. Economically, we investigate the global impact of states in which the aggregate demand for liquidity is occasionally satiated and the market value of liquidity is driven by expectations only. Technically, the analysis follows Fernández-Villaverde et al. (2015) and Guerrieri and Iacoviello (2015) in investigating both the role of nonlinearities and anticipation effects jointly implied by the respective methodological treatment of the constraint and their final impact on equilibrium dynamics and endogenous distributions. In both forms of readings: The global feedback rule from the OBC back to anything else. This is why we apply global methods.====The mechanism at work is as follows: An inappropriate treatment of the constraint deploys a violation of the complementary slackness conditions in certain parts of the state space. As a result, the equity price associated with the correct treatment remains flat for higher levels of capital, while the equity price of the simplified solution exhibits a sharp decline, due the multiplier, which is allowed to become negative. Via anticipation effects, this error translates into other parts of the state space, distorts decision rules, and affects capital accumulation in particular. Given these differences in capital accumulation, the two economies are centered around distinct ==== (see Coeurdacier et al., 2011) and long-run means and are, thus, located in regions where the constraints differ with respect to their tightness and their propagation of exogenous shocks. Furthermore, correlations and higher-order moments are also affected, when relations enforced by a binding constraint abruptly disentangle.====In Section 1.1 we review the literature. In Section 2 we discuss the model. Section 3 contains the technical details of the calibration and solution algorithms. In Section 4 we present our results regarding policies, risky steady states, and impulse responses. The last part of our results consists of two simulations that reveal the final impact of the OBC on the endogenous distribution. Section 5 concludes.",Occasionally binding liquidity constraints and macroeconomic dynamics,https://www.sciencedirect.com/science/article/pii/S0165188923000155,24 March 2023,2023,Research Article,15.0
"Lou Youcheng,Yang Yaqing","Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Beijing 100190, China,University of Chinese Academy of Sciences, Beijing 100049, China","Received 25 October 2022, Revised 1 February 2023, Accepted 19 March 2023, Available online 22 March 2023, Version of Record 2 April 2023.",https://doi.org/10.1016/j.jedc.2023.104643,Cited by (0),"This study proposes a tractable imperfectly competitive economy where traders are socially connected with each other via an information network. We investigate the impact of information linkages on market equilibrium outcomes. In the linear-quadratic-normal framework, we first establish the existence and uniqueness of symmetric linear ","There is substantial evidence suggesting that professional fund managers transmit investment ideas socially, either through direct word-of-mouth communication of information (Hong et al., 2005) or sharing of profitable ideas (Crawford, Gray, Kern, 2017, Pool, Stoffman, Yonker, 2015), or indirect access to information through the same channels (e.g., local TV stations, newsletters, advisory services, etc.) Shiller and Pound (1989) also established that institutional investors’ portfolio choices are driven in part by interpersonal communication.==== While these works focus on the effect of social interactions on investment decisions, some important unanswered questions on the effect of social interactions on the market aggregate outcomes remain. For example, do information linkages improve market efficiency? Do information linkages make traders better off? Do information linkages enhance market liquidity? This study addresses these questions.====Some related research examines the influence of information networks on market equilibrium outcomes under the framework of REE, for example, Ozsoylev and Walden (2011), Han and Yang (2013) and Walden (2019). However, these often consider large, perfectly competitive economies to obtain closed-form solutions. Very few studies consider imperfectly competitive economies that capture the significant price impact by large traders. An exception is Colla and Antonio (2010), who mainly limit their investigation to numerical analysis due to the complexity of the combination of information networks, price impact, and endogenous prices. There is a lack of quantitative analysis on the effects of information networks on equilibrium variables for imperfectly competitive markets. This study contributes to the literature by filling this gap.====We consider a tractable finite-agent economy, which is in essence similar to Kyle (1989) with imperfect competition. We quantitatively analyze the effect of information linkages among investors on trading behavior and market quality parameters, such as market efficiency, price volatility, liquidity, trading volume, and so on. In the economy, there is a single risky asset in zero net supply. There are finitely many speculators (professional fund managers) who compete in demand schedules and have linear-quadratic preferences over their holdings of the risky asset. There are also noise traders, who trade for non-speculative motives, for instance, hedging or liquidity needs, to prevent the price from being fully revealed. Speculators are strategic in the sense that they realize that their demands have an impact on the asset price and take such an effect into account when choosing optimal demand schedules. Based on the market-clearing condition, the equilibrium price is set by an auctioneer who aggregates the demand schedules from all speculators and the noise demand.====A key feature of our model is that speculators are informationally connected and their information sets are overlapped. For example, two geographically close speculators exchange information through word-of-mouth communication (Hong et al., 2005) or acquire their information through the same local newspaper or TV station (Colla and Antonio, 2010). Following the network in Colla and Antonio (2010) and also for mathematical tractability, we assume that the information network is a ====-cyclical graph (which is modellingly equivalent to a ====-regular graph); more specifically, the signal available at any speculator’s location can be observed by ==== clockwise neighbors and ==== counterclockwise neighbors of any given speculator, where ==== is an odd integer. We refer to ==== as ====, which plays a similar role as the parameter of “network connectedness” in Han and Yang (2013) and Ozsoylev and Walden (2011). A higher number of information linkages means that there are more common information sources among speculators and the degree of information overlap is higher.====We first establish the existence and uniqueness of symmetric linear Bayesian Nash equilibria. For the case where speculators are risk neutral, we theoretically demonstrate that price impact, belief disagreement, return volatility, and trading profits strictly decrease with the number of information linkages, while liquidity, signal sensitivity, and price volatility strictly increase with the number of information linkages. We then numerically show that the above implications of information linkages on market equilibrium outcomes also hold for the case where speculators are risk neutral but with a quadratic holding cost. Intuitively, when the level of information linkages increases, speculators trade on more similar signals, which reduces speculators’ belief disagreement, and the trading volume and trading profits due to a disappearance of the potential trading opportunities. An increase of the level of information linkages increases the competition between speculators on the information, and then reduces each speculator’s monopolistic power. An increase of the level of information linkages also induces speculators to be more likely to trade in the same direction so that the price becomes more volatile. As an average of speculators’ beliefs which are more accurate to predict the asset payoff, the price gets closer to the asset payoff so that the return volatility decreases. Furthermore, as the level of information linkages increases, speculators’ beliefs depend less on the price so that their demands are potentially more elastic. In addition, a decrease of the price impact further increases the price elasticity of demand. As a result, the demand of one more unit of the risky asset by noise traders will move the price less, that is, the market liquidity improves.====Additionally, we demonstrate that the equilibrium price reveals more than one-half the private precision of speculators, and almost all that when the number of speculators is large and the number of information linkages takes the maximum value, as noise trading vanishes or as speculators become risk neutral. This differs from the classical result in Kyle (1989) without information linkages. Intuitively, when there are more information linkages, speculators trade more aggressively on their information so that speculators’ information is incorporated into prices more efficiently. We also consider two extensions of the baseline model to show the robustness of the implications of information linkages on market equilibrium outcomes for more general information networks and imperfectly shared signals between speculators.====The rest of this paper is organized as follows. In the next section, we discuss related literature. We introduce the model in Section 3 and characterize the equilibrium in Section 4. In Section 5, we discuss how the distance between two speculators affect correlated trading and how the information linkages affect market efficiency. In Section 6, we analyze the impact of the information linkages on market equilibrium outcomes including liquidity, price impact, return volatility, trading volume, and trading profits, and so on, for linear utility and linear utility with a quadratic holding cost, respectively. Section 7 presents some empirical predictions. Finally, concluding remarks are presented in Section 8. The proofs of all propositions and the two extensions can be found in the Appendix.",Information linkages in a financial market with imperfect competition,https://www.sciencedirect.com/science/article/pii/S0165188923000490,22 March 2023,2023,Research Article,16.0
"Kim Kijin,Kim Soyoung,Lee Donghyun,Park Cyn-Young","Economic Research and Regional Cooperation Department at the Asian Development Bank (ADB), Philippines,Department of Economics, Seoul National University, Republic of Korea,Future Strategy Development Department, Korea Development Bank, Republic of Korea,Economic Research and Regional Cooperation Department, ADB, Philippines","Received 10 October 2022, Revised 26 January 2023, Accepted 17 March 2023, Available online 20 March 2023, Version of Record 1 April 2023.",https://doi.org/10.1016/j.jedc.2023.104642,Cited by (0),"This paper investigates the dynamic impact of social distancing policy on coronavirus disease (COVID-19) infection control, mobility of people, and consumption expenditures in the Republic of Korea. We employ structural and threshold vector autoregressive (VAR) models using big-data-driven mobility data, credit card expenditure, and a social distancing index. We find that the social distancing policy significantly reduces the spread of COVID-19, but there exists a significant, growing trade-off between infection control and economic activity over time. When the level of stringency in social distancing is already high, its marginal effect on mobility is estimated to be smaller than when social distancing stringency is low. The effect of social distancing also becomes secondary after vaccination. Increased vaccination is found to significantly reduce the critical cases while it increases visitors and consumption expenditures. The results also show that the effect of social distancing policy on mobility reduction is strongest among the population of age under 20 and the weakest among the population of age over 60.","The coronavirus disease (COVID-19) pandemic presented unprecedented challenges to public health and economic systems around the world. During the early stage of the pandemic,==== and in the absence of vaccines, governments around the world combined physical distancing policies with stay-at-home orders, business and school closures, and restrictions on travel and mobility to reduce the virus transmission. Since then, social distancing policies have continued as an essential part of the overall strategy to contain the pandemic in many countries.====The Republic of Korea (ROK) was one of the first countries to be hit. The country witnessed a sharp rise of the confirmed cases and declared Daegu and Cheongdo—where the highly concentrated local transmissions were detected and announced—as special management zones in February 2020. While avoiding a nationwide lockdown, the ROK introduced strict social distancing policies to control the virus spread. Following the successful control of this first outbreak, the government continued to manage the recurrent waves by strategically loosening and tightening social distancing measures, without forcing the whole economy to pause.====The ROK's outbreak control was considered broadly successful at least until the end of 2021. In December 2020, Bloomberg ranked the ROK as the fourth efficient health care systems based on its Health-Efficiency Index, next to Singapore; Hong Kong, China; the People's Republic of China (PRC); and Taipei,China, in the pandemic response (====). The ROK was one of the few countries that implemented strict border requirements instead of border closures. With these measures, international travel was still possible, although inbound travelers were subject to strict testing and quarantine rules for infection prevention and control (====).====Central to the ROK's pandemic management was its social distancing policy, implemented through nationwide health campaigns which began in late February 2020. In early May 2020, the government introduced a more systematic program, wherein it classified social distancing approaches into three: Levels 1, 2, and 3. This was further specified into a 5-Level program (Levels 1, 1.5, 2, 2.5, and 3), implemented on 7 November 2020. After more than six months, the government simplified the program (to Levels 1, 2, 3, and 4) effective 9 July 2021.====Before COVID-19 vaccines became available, social distancing was among the few options to effectively reduce the virus transmission. However, it also disrupted social and economic activities by restricting social gathering and people's mobility. This increased both the economic cost and resistance to the policy. Social distancing programs have evolved, reflecting the trade-off between infection control and economic activity. Yet, amid growing social distancing fatigue, their effectiveness has been increasingly questioned. COVID-19 is unlikely to be the last pandemic. Lessons learned today would be important in managing future health crises. In this regard, understanding the dynamic relationship between social distancing measures and their impacts on COVID-19 containment and economic activity is critical for effective policy preparedness and responses.====Literature lags due to lack of data on large scale pandemic cases and social distancing practices. This paper aims to analyze comprehensively the effectiveness and economic cost of social distancing policy during the pandemic's long duration. Existing studies tend to focus on one-sided effect of social distancing policy such as its containment effect or economic impact during a particular period, most often less than 1 year. We look at both how effectively social distancing contain the virus spread and how the effects of the measures on people's mobility and consumption spending change over recurrent pandemic waves.====Recent studies note that social distancing's effectiveness depends on various factors, particularly public behavior and economic cost. Public acceptance of the policy would wane if the economic cost of following social distancing rules were to increase and/or if restrictions were to be drawn out over a long time. More than two years into the pandemic, the effectiveness of the policy is naturally affected by social distancing fatigue and the economic cost. We contribute to the existing literature by empirically examining (i) dynamic responses of infection and economic activity (proxied by people's mobility and consumption expenditure) in recurrent pandemic waves over nearly two years, (ii) the effect of social distancing policy on infection control and mobility by different age groups, and (iii) vaccination effects on severe disease cases and economic activity.==== We also compute weekly averages from daily data to utilize the mobility data measured by age group to capture any policy impact variation by age.====We investigate the effect of social distancing policy on both infection control and economic cost using a structural vector autoregressive (SVAR) model and a threshold vector autoregressive (TVAR) model. The estimated ====These empirical methods are useful and essential to capture dynamic relationships among social distancing policy, infections, and economic impacts. Indeed, several recent studies used VAR models to measure the health and economic impacts of mobility restrictions, including ====. While we use VAR models to measure the impact of containment policy on health and economic outcomes, our paper are different from the earlier studies in terms of data and methodological approaches. First, we use high frequency and granular data such as daily social distancing index, daily mobility and daily credit card usage that specifically cover the geographically confined area, Seoul Metropolitan Area, to help us avoid major specification issues, while ==== assessed the economic impacts of mobility policy using quarterly GDP for Belgium at the national level. Second, ==== adopted a two-stage approach to address data and endogeneity issues in assessing causality. They estimated reproduction numbers, new cases, and death based on an epidemiological model and then applied a SVAR model using those estimates along with actual data for mobility, policy stringency, and economic sentiments to measure the impact of nonpharmaceutical policy interventions and trade-offs between virus containment and economic activities. While this methodology allows better generalization and application of their models where data are incomplete, the estimated variables from an epidemiological model will be subject to the accuracy of model specifications. Third, ==== estimated a health policy equation from the SVAR model where the policy stringency level is contemporaneously affected by all of the other variables in the model assuming a positive relationship between them. Instead, we use a recursive SVAR assuming that a health policy authority mainly uses the number of confirmed cases in determining the level of social distancing policy. Fourth, we adopt a threshold vector autoregressive (TVAR) model in addition to a SVAR model to capture asymmetric responses by regime.====Social distancing policy endogenously responds to the state of the pandemic in an economy, measured by the level of social infections. That is, when the social infection level, proxied by the number of confirmed people, increases, the government tightens social distancing policy. Then, people's mobility and economic activity (for example, consumption spending) decline. In such a case, the declines in mobility and economic activity can be in one part due to the increase in the infection level and in another due to the tightening of the social distancing policy. To precisely infer the effect of social distancing policy only, it is important to extract the exogenous changes in social distancing policy. SVAR models have been useful for identifying exogenous changes in policies such as monetary and fiscal policy. We also employ a SVAR model to identify exogenous shocks to social distancing policy and to infer its sole effects.====In general, social distancing policy, the social infection level, people's mobility, and economic activity interact each other dynamically. The Korean government in its public communication indicated that it considers both the social infection level and ==== when making decisions on the level of stringency and detail measures in social distancing policy. Following its decision, the new level of social distancing stringency would influence mobility and hence infection levels. People's mobility and economic activity would be also influenced by social distancing stringency and social infection levels. Such interactions would occur recursively, while the intensity of such recursive interactions change over time. In this regard, the SVAR model can properly capture dynamic interactions among these variables under consideration.====We also employ a TVAR model that allows non-linearity in responses. Effects of social distancing policy may not be uniform over time, subject to the status of the economy and varying initial conditions such as pandemic duration, existing social distancing stringency, and other factors affecting public behavior. Loosening and tightening actions may also have a different effect. The TVAR is appropriate to account for such non-linear and asymmetric responses.====The results show that a tightening of social distancing policy significantly reduces the number of confirmed cases and reproduction rates (R0; the average number of people infected by one person). It also decreases mobility and consumption spending, and so increases economic cost. In the recurrent pandemic waves in the ROK, the responses in the number of confirmed cases have weakened while the responses in mobility and credit card spending have strengthened. This indicates that a significant and growing trade-off is experienced between infection control and economic activity. We also find that when social distancing policy is intensified, the marginal effect of a unit increase in social distancing on mobility is smaller than when measures are less restrictive. Using the weekly data with age group analysis, we find the mobility reduction is strongest among the young population of age under 20 and weakest among the old population of age over 60.====We also add analysis of the effect of vaccination programs on health and mobility during 10 March 2021 to 14 October 2021. We find that vaccination significantly contributes to a reduction in severe disease cases caused by COVID-19 while visitors and expenditure increase as vaccination rates improve. These new findings add especially to the literature on the COVID-19 vaccination, which has been limited to investigating the health effect of vaccinations rather than their economic impact.====The next section reviews the literature on non-pharmaceutical interventions and vaccinations during health crises, along with their effectiveness. Sections 3 and 4 discuss data on high-frequency visitor data and empirical modelling strategies. Section 5 then presents estimation results on the effects of social distancing policy and vaccination on COVID-19 infections, mobility, and economic activity. Section 6 concludes with policy implications derived from the estimation results.",Impacts of social distancing policy and vaccination during the COVID-19 pandemic in the Republic of Korea,https://www.sciencedirect.com/science/article/pii/S0165188923000489,20 March 2023,2023,Research Article,17.0
"Corato Luca Di,Maoz Yishay D.","Università Ca' Foscari Venezia, Italy,The Open University of Israel, Israel","Received 1 September 2022, Revised 13 March 2023, Accepted 16 March 2023, Available online 18 March 2023, Version of Record 1 April 2023.",https://doi.org/10.1016/j.jedc.2023.104640,Cited by (0),In a competitive ,"Weitzman (1974) has studied the dilemma concerning which policy tool to use – taxation or a cap on quantity – when maximizing welfare in a context where production entails a negative externality. His seminal work has led to decades of intense study of this dilemma within the fields of public economics and environmental economics. Koenig (1985), in which the adverse externality is due to the entry of foreign firms that negatively affects the domestic industry, Spulber (1985), who assumes that market entries increase pollution, and Anderson (1993), where the development of new properties harms residents by reducing open space, are a few examples for studies dealing with this dilemma and the wide range of topics to which it is relevant.====Weitzman's model was static, and his main result was that a cap on quantity performs better than a tax if the marginal benefit curve is steeper than the marginal cost curve, otherwise a tax does better. As per the survey by Tang et al. (2019), much of the subsequent research has remained within a static framework, and in particular, very few studies have explored this issue under the rather realistic assumptions of stochastic profitability, irreversibility of the investment, and flexibility in choosing the investment timing. The literature on the industry equilibrium under these conditions is vast, and Dixit and Pindyck (1994) presents much of it. So far, the only study analyzing Weitzman's “tax vs. cap” dilemma within the typical framework of this literature is Baldursson and von der Fehr (2004) who study the efficacy of price and quantity controls in a setup where the investment in abatement is irreversible for some firms in the industry and reversible for others. Their main finding repeats Weitzman's result, as they conclude that the relative slope of the cost curve with respect to the slope of the benefit curve determines whether tax or cap is the better policy. Yet, their modeling does not follow the standard lines of the literature on investment under uncertainty and, in particular, the uncertainty in their model springs from the unorthodox assumption that the number of the firms adopting a reversible abatement technology is stochastic.====Other studies which add adverse externalities to the typical framework of the investment under uncertainty literature are few and they focus either on taxes or on a cap policy, or possibly analyze the equilibrium under both policies but not comparing the two.==== Thus, for example, Jou and Lee (2008) who consider a real estate market assuming that newly developed properties, by reducing open space, have an external cost, have focused on tackling this externality with a tax policy. Similarly, Lee and Jou (2007) show how the regulator can correct the negative externality by imposing a density ceiling control. Di Corato and Maoz (2019) search for the optimal cap on private firms’ entries in markets where production has adverse externalities and the output price is stochastic, but do not consider the possibility of a tax policy.====To fill this void, in this paper, we set up a model analyzing the industry equilibrium under perfect competition in a dynamic setup where market demand is stochastic and entry is irreversible. Production generates an external cost for Society, which is assumed increasing and convex in the industry output. We then consider the following two polar policy instruments for regulating the negative externality: (i) a quantity control exerted by introducing a cap on the industry output and then rationing market entries, (ii) a price control exerted by imposing an output tax. We characterize the industry equilibrium under each policy and try to find which of the two policy tools leads to greater welfare.====Our main findings are as follows. In the case of a cap policy, we find that optimality leads to on an internal welfare-maximizing cap level. Rather intuitively, this level is the quantity at which the marginal market surplus is equal to the marginal social cost, i.e., the sum of private and external costs. If the current market quantity is still below this level, then it is optimal to set this level as a cap on market quantity and allow the market to expand towards it over time, based on the strategic entry considerations of the firms. If, on the other hand, the current market quantity is already above this welfare-maximizing cap level, then, due to irreversibility, the market cannot revert to this welfare-maximizing cap level, and it is optimal to set the cap at the current level, i.e., to immediately ban any further entry. We also find that the welfare-maximizing cap level is increasing in the level of market uncertainty, which implies that greater profit uncertainty makes the policy maker allow more entries. This result is based on the same effect by which the uncertainty premium counterbalances the external cost as described in Di Corato and Maoz (2019).====The result that the optimal policy in this case is based on an internal welfare-maximizing cap level is novel because the only other study searching for an optimal cap within a competitive environment with profit uncertainty and investment irreversibility is Di Corato and Maoz (2019) which reaches a different result. Specifically, they assume that the external cost is a linear function of the quantity produced, and therefore reach the result that it optimal to have either no cap at all, if the uncertainty is high enough, and otherwise to set the cap at the current market quantity. In contrast, in this study we assume that the external cost is a convex function of market quantity, as the empirical literature about pollution damages often suggests, and therefore reach the result of an internal welfare-maximizing cap level.====In the case of a tax policy, we show that the output tax can be viewed as an additional cost of production for the private firm whose impact can be studied using the model by Leahy (1993). In his model, the price threshold triggering market entries is increasing in the cost of production, therefore, the introduction of an output tax, by raising the entry threshold, delays market entries with respect to the scenario where the industry is not regulated. This is because the output price, in its random evolution, needs more time (in expected terms) before hitting eventually a higher threshold. We then determine the tax rate maximizing welfare and find that it must be set equal to the marginal external cost associated with the industry output supplied at each time point. This implies that further market entries become less and less likely as the industry output increases since the higher the tax burden, the higher the entry threshold.====Finally, when comparing the cap and the tax policies, a relevant trade-off emerges. With the cap, the industry output is bounded but the cap does not affect its temporal evolution with respect to the scenario where the industry is not regulated. In contrast, with the output tax, there is no limit to market entries but the tax affects the temporal evolution of the industry output by delaying market entries.====We then show that a first-best outcome can be achieved only by adopting a tax policy and that, in this respect, the ability to affect the entry timing is crucial. In fact, by setting the tax rate equal to the marginal external cost associated with the industry output supplied at each time point, the externality is fully internalized by the firms and consequently entries occur only when the exogenous stochastic shifts in market demand yield an associated gain in terms of market surplus covering the marginal social cost of an additional unit of the good. In contrast, a cap policy may only be considered as a second-best alternative since, in the presence of a cap, market entries occur at a socially suboptimal time. In fact, when the cap is not binding, firms keep entering the market using the same strategy that would be followed in the absence of regulation while, when the cap is binding, market entries do not occur at all, even when they would be beneficial from a welfare perspective.====The main reason for the difference between our result of tax superiority and Weitzman's result springs from the dynamic setting that we portray, in contrast to the static analysis provided by Weitzman (1974). Due to that, while the uncertainty that policy makers face in Weitzman's model is about the current situation, in our model they have a perfect view of current situation and the uncertainty they face is about its future development. Thus, at each point in time, the policy makers in our model can fit the best tax rate for the current situation. With a cap policy, this is not possible because, by the very nature of this policy tool, the cap level is assumed to be fixed credibly over a sufficiently long time period. In that sense, the dynamic setting gives an advantage to the tax policy.====To shed more light on the role of the dynamic modelling in making the tax policy better than the cap policy, we study, in the final section of this article, a case in which the tax rate is constant as in Weitzman (1974) and much of the related literature. We have taken the resemblance to the static Weitzman's model to the extreme by assuming also that the tax is levied immediately at time 0. This Weitzman-like modeling of the tax policy has indeed led to a result that resembles Weitzman result that the cap policy may be the better one if, as a function of the industry output, the external cost curve is sufficiently steeper than the benefit (market surplus) function. Yet, the analysis of this case within a dynamic framework allows us to show the time inconsistency of such a policy. Moreover, it takes very little deviating from this extreme case to make the tax policy doing, once again, better than the cap policy in terms of welfare maximization. We demonstrate it with allowing the government the freedom to choose the time at which the tax is imposed. We show that, once again, this makes the tax policy dominate the cap policy regardless of the specifics of the benefit and cost functions.====As a side product of that analysis, we also develop a dynamic version of the measure for the relative steepness of the external cost function with respect to that of the benefit curve. The need for that arises because in a static model this measure is based on the slopes at the single equilibrium point while in our dynamic model the equilibrium moves from one point to another as firms endogenously enter the market over time.====The paper remainder is as follows. In Section 2, we present our model set-up. In Section 3, we determine the industry equilibrium under no policy intervention. In Section 4, we introduce the two policy instruments for externality control and determine the optimal entry strategy under each policy. We determine the optimal cap and the optimal tax rate, compare the two policies and discuss our findings. In Section 5, we provide some final remarks and conclude.",Externality control and endogenous market structure under uncertainty: The price vs. quantity dilemma,https://www.sciencedirect.com/science/article/pii/S0165188923000465,18 March 2023,2023,Research Article,18.0
"Hagenhoff Tim,Lustenhouwer Joep","Deutsche Bundesbank, Wilhelm-Epstein-Str. 14, Frankfurt am Main 60431, Germany,Heidelberg University, Bergheimer Str. 58, Heidelberg 69115, Germany","Received 20 October 2021, Revised 6 March 2023, Accepted 8 March 2023, Available online 11 March 2023, Version of Record 21 March 2023.",https://doi.org/10.1016/j.jedc.2023.104638,Cited by (0),"We estimate a simple reduced-form model of expectation formation with three distinct deviations from full-information rational expectations that combines underreaction with overreaction to new information. In particular, forecasts are sticky, extrapolate the most recent news about the current period, and depend on the lagged consensus forecast about the period being forecast. We find that all three biases are present in the Survey of Professional Forecasters as well as in the Livingston Survey, and that their magnitudes depend on the forecasting horizon. We also stress the point that using the past consensus forecast to form expectations is a reasonable thing to do if a forecaster is not able to come up with full-information rational expectations all by herself. Finally, we show that forecasters that have more sticky expectations generally tend to rely less on the lagged consensus forecast but extrapolate news about the current period more.","How are macroeconomic forecasts formed? There is ample evidence that forecasters do not make fully efficient forecasts and have systematic biases.==== That is, real-world forecasts deviate (at least to some extent) from the full-information rational expectations (FIRE) benchmark. Moreover, these deviations from FIRE matter for economic developments, as a link between individual expectations and economic decisions has been established (Burke, Ozdagli, 2014, Coibion, Gorodnichenko, Ropele, 2020, Dräger, Nghiem, 2021, D’Acunto, Hoang, Weber, 2022, Roth, Wohlfart, 2020). But the question of how expectations exactly deviate from FIRE is not easily answered. Answering this question is, however, crucial for judging the validity of macroeconomic models and their policy implications, for two reasons. First of all, because most macroeconomic models rely on the assumption of FIRE, and secondly, because empirical evidence for models that deviate from FIRE are often not fully convincing, especially when one considers data of individual forecasters.====The existing literature that proposes deviations from FIRE has mainly focused on single deviations from FIRE that consider either underreaction or overreaction to new information.==== For example, Coibion and Gorodnichenko (2015) consider models of information frictions to explain underreaction to new information in aggregate survey data, whereas Bordalo et al. (2020), instead, consider diagnostic expectations as a single deviation from FIRE to explain overreaction of individual forecasts. At the same time, evidence on overreaction and underreaction in survey data remain mixed with, e.g., Fuhrer (2018) finding sluggish expectation updating also at the individual level.====In this paper, we estimate a linear, reduced-form model of expectations that combines ==== deviations from FIRE. We use survey data of professional forecasters==== and make a quantitative inference of the relative importance of the different deviations from FIRE. Our findings indicate that three simple deviations from FIRE that have been discussed in recent literature are simultaneously present. This suggests that theories for expectation formation with single deviations from FIRE may not sufficiently explain real-world expectations and that policy advice based on models with such theories may not be robust.====The first two deviations from FIRE that we combine are stickiness in expectations and the extrapolation of recent news about the ==== period. A combination of stickiness (underreaction) and extrapolation (overreaction) can explain why different studies have found conflicting evidence as to whether forecasters underreact or overreact to news. If individual forecasters have sticky expectations but also extrapolate recent news about the current period, then one can indeed find either underreaction or overreaction. This may then depend on the methodology and the data used. In our framework, net overreaction can arise when a particular piece of news has a large impact on the current period. In that case, overreaction due to extrapolation is particularly strong. On the other hand, if a particular piece of news mainly concerns future periods, extrapolation of news about the current period is weaker and net underreaction arises due to stickiness in expectations.====The third deviation from FIRE that we consider is that forecasters may partly base their forecast on the most recently observed consensus forecast about the period that is being forecast.==== Such a bias can be interpreted as meaning that individual forecasters do not have the skills or the time to efficiently use all available pieces of information to come up with the FIRE forecast all by themselves. Therefore, they instead only partly base their forecast on new information and form only part of the FIRE forecast. For the other part, they make use of the “wisdom of the crowd” and turn to an easy-to-use piece of information: the most recently observed consensus forecast.====We find statistically significant estimates for all three deviations from FIRE simultaneously. Our results further indicate that all three biases make a relevant contribution (in terms of ====) to explaining the data and that the full model including all three biases has the best out-of-sample predictive performance. Moreover, larger forecasting horizons turn out to imply more sticky expectations. Interestingly, we show that 20–50% of the forecasters in the survey would have been better off in terms of forecasting performance if they had abandoned all attempts to construct their own forecast and instead blindly submitted the most recently observed consensus forecast in every period. Basing a forecast on the lagged consensus can, hence, be seen as a reasonable thing to do.====In addition to estimating our model and assessing the relative importance of each of the three biases, we also test a restriction on the coefficients of an over-identified econometric specification. This restriction is in most cases not rejected. Finally, we find substantial heterogeneity among forecasters when estimating our model for each forecaster individually. We show that forecasters that exhibit more sticky expectations tend to rely less on the lagged consensus forecast but extrapolate news about the current period more compared to forecasters whose expectations are less sticky.====Many alternative models of expectation formation that deviate from FIRE have been proposed. A considerable body of literature assumes that expectations are formed in a purely backward-looking manner, either by all or by a fraction of agents in the economy. A disadvantage of modeling expectations as purely backward-looking is that there can be little to no role for news and announcements about the future. Backward-looking expectations hence cannot meaningfully contribute to, e.g., the question of whether forecasts overreact or underreact to news. Moreover, recent empirical evidence favors alternative deviations from FIRE over backward-looking expectations (Fuhrer, Landier, Ma, Thesmar, 2019), which is consistent with our results.====Other deviations from FIRE include models with sticky information or noisy information and rational inattention (Mackowiak, Wiederholt, 2009, Mankiw, Reis, 2002, Sims, 2003, Woodford, 2003). Moreover, Gabaix (2020) proposes a framework of sparsity to model deviations from FIRE. These theoretical frameworks may offer partial explanations for, e.g., ==== expectations are sticky. These explanations by themselves are, however, not always in line with survey data of ==== expectations (Bordalo, Gennaioli, Ma, Shleifer, 2020, Fuhrer). This is why we turn to a formulation of a model with higher-level individual biases. This enables us to combine overreaction and underreaction to news in a simple manner. Such a reduced-form model of expectation formation can, in future research, be used to inform and guide the development of more sophisticated theories of expectation formation or as an ad-hoc building block in macroeconomic models.====The rest of the paper is organized as follows. In Section 2, we outline our reduced-form model of expectation formation. In Section 3, we derive two empirical formulations of the model, and our main estimation results are presented in Section 4. Next, we discuss several extensions that contribute to the interpretation and robustness of results in Section 5. Section 6 discusses the role of the lagged consensus forecast and Section 7 concludes.","The role of stickiness, extrapolation and past consensus forecasts in macroeconomic expectations",https://www.sciencedirect.com/science/article/pii/S0165188923000441,11 March 2023,2023,Research Article,19.0
"Chen Jian,Tang Guohao,Yao Jiaquan,Zhou Guofu","School of Economics & the Paula and Gregory Chow Institute for Studies in Economics, Xiamen University, 361005, China,College of Finance and Statistics, Hunan University, Changsha, 410082, China,School of Management, Jinan University, 510632, China,Olin School of Business, Washington University in St. Louis, St. Louis, MO 63130, United States","Received 10 October 2022, Revised 17 January 2023, Accepted 6 March 2023, Available online 11 March 2023, Version of Record 20 March 2023.",https://doi.org/10.1016/j.jedc.2023.104636,Cited by (0),"We propose an employee sentiment index, complementing investor sentiment and manager sentiment indices, and find that high employee sentiment predicts low monthly (weekly) market returns significantly both in- and out-of-sample. The predictability can also deliver sizable economic gains for mean-variance investors in asset allocation. The impact of employee sentiment is found stronger among employees who work in the headquarters state and are less experienced. The economic driving force of the predictability is unique: high employee sentiment leads to high contemporaneous wage growth due to immobility, which subsequently results in lower firm cash flow and lower stock returns.","Who cares the most about the valuation of stocks? Investors, managers, and employees are seemingly the three most important groups. Hence, their sentiments are of the utmost interest in understanding how sentiment can drive stock prices away from their fundamental value. The important role of investor sentiment is well understood since Baker and Wurgler (2006)’s pioneering work on providing a measurable investor sentiment index (Barberis, Thaler, 2003, De Long, Shleifer, Summers, Waldmann, 1990, Fuster, Laibson, Mendel, 2010, Hirshleifer, 2001, see also). However, there is a lack of research on employee sentiment studies due to data availability. Edmans (2011) is an important exception who uncovers the relation between employee satisfaction and firm abnormal stock returns, and Edmans et al. (2021) show further it is robust over time and around the world. However, the number of firms, which is 100 in the U.S., is limited in their studies. Recently, it is possible to measure employee sentiment much more broadly with data from Glassdoor. Green et al. (2019) and Sheng (2021) use this data and find that employee satisfaction can significantly predict returns in the cross-section.==== However, how employee sentiment affects the aggregate stock market remains unknown.====In this study, we provide the first aggregate employee sentiment index and investigate its predictability for the entire stock market. If employees’ behavior can only influence individual firms and stock returns in the cross-section, its role is limited to the broad scope of finance. However, if it can affect the entire market, its role increases significantly. As Cochrane (2008) emphasizes, the market risk premium has a profound impact on asset pricing, corporate finance, and the entire economy, and market efficiency, and so the market predictability is one of the central issues in finance.====To predict the market, we aggregate firm-level employee sentiment into the aggregate employee sentiment index, which is similar to the widely used investor sentiment index of Baker and Wurgler (2006), and analogous to the manager sentiment index of Jiang et al. (2019). Following Green et al. (2019) and Sheng (2021), among others, we obtain one-to-five star overall ratings at firm level based on crowdsourced employer reviews, which are responses by employees for overall employer quality as well as ratings for several dimensions of employee satisfaction. For each firm, we define the monthly employee sentiment as the difference between the percentage of positive reviews (five and four stars) and that of negative reviews (one and two stars) within each month. Thereafter, we compute the equal-weighted firm-level employee sentiment across firms as our market-level measure, consistent with aggregating signals cross firms in the predictability literature (see, e.g., Rapach and Zhou, 2022, for a review of the literature).====Our employee sentiment index has two unique features related to the investor and manager sentiment measures. In contrast to the former, which is market information-based, our index is a source of firms’ inside information. Employees routinely observe nonpublic value-relevant information and thereafter might hold overly optimistic or pessimistic beliefs by extrapolating this information. In contrast to the latter, our index is based on the daily reviews posted by crowdsourced employees and can reflect the sentiment inside firms more timely than the manager sentiment, which is based on quarterly or annual financial disclosures.====Empirically, we find that the employee sentiment index has significant in-sample predictive power on the stock market for the sample period from June 2008 to December 2020. Specifically, a high employee sentiment predicts a subsequent low return significantly. The ==== of the regression of one-month ahead market excess return on employee sentiment is 6.06%, with a slope of ====% which is statistically significant at the 5% level. The negative predictive power is consistent with the overreaction prediction of sentiment theories. Moreover, we find that the predictability remains significant after controlling for alternative sentiment measures: the investor sentiment of Baker and Wurgler (2006), the aligned investor sentiment proposed by Huang et al. (2015), the manager sentiment suggested by Jiang et al. (2019), the FEARS investor sentiment index constructed by Da et al (2015), and the University of Michigan Consumer Sentiment Index. In addition, we also compare the predictive power of employee sentiment with the common economic variables used by Goyal and Welch (2008) and find that employee sentiment maintains strong predictability after controlling for them. Overall, we find that employee sentiment is distinct from extant sentiment measures and common economic predictors, and has strong predictive power for the stock market return.====We find that the employee sentiment predicts future market returns ====. This seems puzzling as Green et al. (2019) and Sheng (2021), among others, find that high employee sentiment is associated with future high firm returns. In fact, there is no contradiction. The reason is that their studies are about cross-sectional predictability. When they compare different firms at the same time, they control for the market effect. In contrast, we focus on market-level predictability. This entails time series return predictability, which may not necessarily work in the same direction as the cross section predictability. Indeed, this is the case for many predictors, such as the implied cost of capital (Li et al., 2013) and investor sentiment, in which the two types of predictability differ in signs. A simple way to see this is to assume that the sentiment is up for 50% firms, and down for another 50%, but the sum of the up ones is greater than the sum of the down ones. Then the sentiment index is up. However, the positive returns from the optimistic firms can be lower than the negative returns from the pessimistic firms, resulting in a negative return for the market.====The strong return predictability of employee sentiment also exists out-of-sample, which has become a critical assessment of predictability ever since Goyal and Welch (2008), because in-sample predictability can be unreliable due to parameter instability. Following the predictability literature, we evaluate Campbell and Thompson (2008)’s out-of-sample ==== statistic, and find that the employee sentiment index still delivers statistically significant ==== of 3.93% for the out-of-sample period from January 2013 to December 2020.====There is also significant economic value of the predictability of the employee sentiment index. Because of the significant positive ====, a mean-variance investor who allocates funds monthly between the market and risk-free assets can earn investment gains if the investor uses return forecasts based on the employee sentiment index rather than using the historical average return. Indeed, the annualized certainty equivalent return (CER) gain is 3.59% if the investor has a risk aversion degree of 3. Moreover, the return forecasts of employee sentiment generate a large annualized Sharpe ratio of 0.75, while the market has a Sharpe ratio of only 0.45.====Our findings are robust in quite a few dimensions. First, the predictability exists in both high- and low-sentiment periods, but it is stronger in the high-sentiment period, consistent with Huang et al. (2015); Shen et al. (2017), and Jiang et al. (2019). Second, we match the state of the “work location” submitted by employees with that of the firms’ headquarters from Compustat, and then identify the in-state reviews (reviews from employees who work in the same state as the firm’s headquarters) and out-of-state reviews (reviews from employees who work in different states from the firm’s headquarters). After excluding the out-of-state reviews from our data sample, we reconstruct the employee sentiment index and find that its return predictability becomes even stronger. This suggests that the sentiment of employees who work geographically close to the headquarters has a more powerful predictability for market returns. The possible interpretation is that because of the advantage of geographical location, these employees may have easier access to non-public firm information and accordingly form biased beliefs after extrapolating non-public firm performance relative to employees who work far away from firm headquarters. Third, we find that excluding reviews from employees who work for less than one year may slightly impair the predictive ability. Because inexperienced junior employees are more likely to overreact to firm news and have more expectation errors than senior employees do, their sentiment may be stronger power to predict the stock market. Fourth, we use an alternative method of neural network method, shown by Gu et al. (2020) as the best machine learning approach among all they studied, to predict stock returns firm by firm, and then aggregate the results into the market. Again, we find that the predictability is statistically significant. Lastly, we investigate also the predictability at the weekly frequency. Employees may extrapolate firms’ information over recent intervals and post reviews on the Glassdoor. Thus, employee sentiment is likely to contain predictive power at a higher frequency, for example, the weekly level to capture the timely information. We do find such evidence that weekly employee sentiment predicts the market negatively and significantly. This predictability remains strong out-of-sample.====We explore further possible underlying economic mechanisms for the negative predictability of employee sentiment in the market. We conjecture that employees are subject to the extrapolative bias suggested by Greenwood and Shleifer (2014), Barberis et al. (2015), and Hirshleifer et al. (2015). Employees may extrapolate the superior firm performance into the future and expect that firms will continue to perform well. We find supporting evidence that employee sentiment is correlated with the past two-month cumulative return positively. If employees’ beliefs reflect the true firm performance, we would observe that the stock price continues to increase, otherwise there exists a price reversal subsequently. Our results show that the stock return decreases after high employee sentiment, indicating that employees may hold biased expectations.====A possible market outcome of this extrapolative bias is increased labor hiring costs, such as wage growth. Holding optimistic beliefs about future firm economic conditions, employees are unlikely to separate from their current jobs because they expect higher income and career development opportunities in the future. Consequently, hiring new job candidates becomes difficult. However, recent superior firm performances may also raise manager sentiment (Jiang et al., 2019). Firms tend to increase their real investment (Baker, 2009, see, for example) and labor demand. The conflict between high labor demand and low job turnover can increase firms’ hiring costs. Indeed, we find evidence to support this argument. First, we use job vacancy postings as a proxy for labor demand and find that vacancy posting is less influenced by employee sentiment, but is significantly and positively associated with manager sentiment. Second, the labor quitting rate, measuring employees’ voluntary separations from their current jobs, is low when employee sentiment is high. Conversely, manager sentiment has a minimal impact on it. Third, we find evidence that high employee sentiment is indeed correlated with contemporaneous wage growth, even after controlling for manager sentiment, asset growth, and earnings growth. Fourth, current high wage growth proceeds a subsequent low return. Summarily, our results suggest that hiring cost is a likely channel through which employee sentiment has real effects; sentiment-driven cost slows down firms’ future fundamentals and therefore leads to a price reversal subsequently.====There are likely other mechanisms through which employee sentiment affects a firm’s expected returns. First, Sheng (2021) finds evidence that outside investors might consider employees’ opinions about firm fundamentals and trade on them. Second, employees are likely to invest in their own firms because they have superior information about their employers (Coval and Moskowitz, 1999). Additionally, employees may also hold too much of their company stocks due to extrapolative bias (Benartzi, 2001); thus, their sentiment can influence their firms’ market prices through their holdings.====In summary, our study makes three contributions. First, we contribute to the literature on market sentiment by providing an aggregated employee sentiment index, which has been missing in previous studies on the three components of sentiment. Our index is similar to the investor sentiment index of Baker and Wurgler (2006) and the manager sentiment of Jiang et al. (2019), but it has the greatest predictive power on the market return. Our study complements the cross-sectional studies by Edmans (2011); Green et al. (2019), and Sheng (2021), showing that employee sentiment is much more important than previously thought. This is because it has market-wide implications on the firm cost of capital and expected returns on various managed portfolios and investment strategies. In other words, employee sentiment matters in macro-finance and in the equilibrium to determine the market risk premia.====Second, we contribute to the literature by using crowdsourced data from social media. Chen et al. (2014) show that investor opinions transmitted through social media can predict future stock returns and earnings surprises. Using customer product reviews on Amazon, Huang (2018) finds evidence that consumer opinions contain forecasting information for stock returns. Green et al. (2019) use the changes in average ratings from Glassdoor and find that they predict stock returns in the cross-section. Da and Huang (2020) investigate how the wisdom of crowds could be harnessed to provide better earnings forecast consensus. Da et al (2021) examine the firm-level extrapolation by using novel data from a crowdsourcing platform for ranking stocks. Agrawal et al. (2021) explore the unknown information contained in rank-and-file labor flows. We show that crowdsourced data play an important role even at the market level.====The last but not the least, our paper adds to the literature on labor finance. A growing studies has shown that rank-and-file employees are important determinants of stock returns. Eisfeldt and Papanikolaou (2013) study organizational capital embedded in specialized labor input; Belo et AL (2014) focus on labor adjustment costs; Donangelo (2014) focuses on labor mobility; Favilukis and Lin (2016) consider labor operating leverage arising from rigid wages; Kuehn et al (2017) explore the impact of labor search frictions; Donangelo et al. (2019) studies how firm-level labor share affects the cross-section of returns; Liu (2021) studies the role of vacancy rates on stock returns. We show how the employee sentiment affects the valuation of stock market.====The remainder of the paper is organized as follows: Section 2 describes the data and variables used in this study. Section 3 provides the empirical forecasting results. Section 4 explores the economic sources of predictability. Section 5 shows the results of robust check. Section 6 concludes.",Employee sentiment and stock returns,https://www.sciencedirect.com/science/article/pii/S0165188923000428,11 March 2023,2023,Research Article,20.0
Chini Emilio Zanetti,"University of Bergamo, Department of Economics, Via dei Caniana, 2 - 24127, Bergamo, Italy","Received 20 August 2022, Revised 21 December 2022, Accepted 3 March 2023, Available online 10 March 2023, Version of Record 20 March 2023.",https://doi.org/10.1016/j.jedc.2023.104632,Cited by (0),"We answer positively to this question by using Maximum Lq-Likelihood (or Deformed Likelihood) estimator. This is based on a parameter which measures the aggregate quote of judgment in the forecasting (game-based) system formed by three players—Forecaster, Policy Maker and Reality. For the first time in econometric literature, we apply this estimator to a dynamic system and derive a robust version of the Kalman Filter—the Deformed Kalman Filter (DKF). The evidence from U.S. data suggests that the judgmental dynamics exists and is correlated (but not coincident) with the phases of the Business Cycle. Furthermore its knowledge improves in-sample as well as out-of-sample estimation.","The difficulty in observing and correctly interpreting a phenomenon is a non-trivial aspect of any scientific knowledge. When the Italian composer formulated his statement, these capabilities—and their understanding—were a privilege for a restricted élite. Thus, despite its humor, the statement should be considered as a (partial) truth. The growing pervasiveness of information technology and the rapid acceleration and development of statistical tools for data processing since the mid 20th Century made the incentive for exploiting the capability of data science superior to the incentive to think of new approaches to Economics, for the first time in Social Science history. Thus the Nobel Laureate’s statement might be considered less paroxysmal than suggested by a superficial interpretation.====The bias in the survey of the U.S. economy’s professional forecasters is an example of mismatching among observation and interpretation in contemporary Macroeconomics. Figure 1 illustrates the case of 1-quarter-ahead forecasts of the U.S. Real GDP (RGDP, henceforth): namely, panel (a) considers the bias across time—the spread among realizations and Greenbook estimates====—while panel (b) investigates the same bias in a cross-sectional perspective—that is, the spread among the realizations and the 1-quarter ahead estimates for individuals corresponding to the 75th and 25th percentiles of the distribution of survey respondents (denoted RGDP-P75 and RGDP-P25, respectively). The spread among data and Greenbook forecasts is pervasive both in time and in cross-sectional dimension, even if considering the difference among the 75th and 25th percentile’ survey respondents as a centered measure to make our investigation robust to extremely optimistic/pessimistic forecasters.====The existence of the experts’ forecasting bias may not constitute a challenge if an optimal prediction would convey estimates ‘near enough’ to consensus forecasts. In fact, the problem of the optimal prediction in a dynamic regression framework is well-known since the introduction of Kalman (1960)’s recursive algorithm known as ‘the Kalman filter’ (KF, henceforth). Nevertheless, the evidence displayed in Fig. 2 also contradicts this hypothesis: the Greenbook forecasts are structurally over the optimum and below the realizations (in absolute values). In other words, the optimal forecasts lie in a very strict band around zero, below (above) both the data for positive (negative) values and consensus estimate. Moreover, the existence of large outliers is ignored and, in some cases, poorly addressed: for example, the 2020 Pandemic shock in RGDP growth is interpreted as a (dramatic) increase from 0 to 5% approximately, while the corresponding observation lies at a -30% level. Thus, the U.S. macroforecasters were effectively more accurate than an agnostic—and uninformed—econometrician.====One may be tempted to cover this lack of standard signal extraction methods by complicating the model assumed as a data generating process around which optimal forecasting exercise is applied. For example, the data may be assumed to follow a nonlinear and/or time-varying parameter evolution in several of its moments; or to be driven by some unobserved factors to be estimated using large datasets; or a combination of these features. The econometric literature is considerable and in a mature state, see Durbin and Koopman (2012) for an exhaustive introduction. However, using more complex econometric modelling, while improving our forecasting exercise, would ==== explain why U.S. professional forecasters diverge from realization of the RGDP measurements. Explaining this bias and using this information to improve our forecasting capability is the aim of this paper.====According to the literature, economic agents (and thus, professional forecasters) face some sort of limitation in their data processing capabilities and other types of costs arising from interaction with other agents (for example, reputation). These (unobserved) costs, in turn, may easily influence the agents’ utility maximization and their expectations—and consequently, their forecasts based on standard, rational expectation-based econometric models. In particular, limited attention and imperfect utility maximization justify the existence of outliers—that considerably complicate the problem of signal extraction—in forecasting. In this regard, a number of (apparent) expectation-formation biases have been documented, see Coibion, Gorodnichenko, 2012, Coibion, Gorodnichenko, 2015 and Bordalo et al. (2020) ====.====The identification of the ultimate source of these biases is an open issue. Its exploration requires (i) a theory that explains the genesis of the macroforecasters’ misbehavior and its effect on the business cycle; (ii) an estimation method that allows economists to capture the dynamics of such misbehavior. While there is a growing literature on the former issue, the latter is still at an early stage. In other words, we know that economic agents face with behavioral aspects that justify the forecasting bias but there is still not a recognized measure of aggregate distortion caused by agents’ behavior. In this paper we consider macroforecater’s mis-behavior an epiphenomenon of the ====, defined by Svensson (2005, p. 2) as ==== and represented, by the same Author, as a Central Bank’s estimate of arbitrary stochastic factor added to the econometric model equations.====Hence our research question: ==== Our contribution is represented by the definition of a novel robust methodology to extrapolate judgment empirically. Namely, we propose the dynamically judgmental system (DJS) as a general characterization of the judgmental dynamics via state-space modelling and introduce a direct estimation method of the amount of judgment via signal extraction techniques. In our framework the (aggregate) amount of judgment in the system is parametrized by a functional of the likelihood—named “====”, or “====”—of the model to be estimated. The Deformed Likelihood estimator is characterized by a parameter that defines the degree of deformation of the logarithmic transform of the likelihood function due to the presence of additive outliers, hence providing a direct, robust measure of the degree of deformation due to judgment in the whole forecasting system. The DJS allow us to estimate the dynamics of this deformation via a peculiar version of KF, named “Deformed Kalman Filter” (DKF).====Our simulation exercise demonstrates that the DJS-DKF has good general properties in terms of accuracy in small samples and that the distribution of the deformation parameter is very well approximated by a Gaussian distribution. Finally, we apply our methodology to the Survey of Professional Forecasters of the Federal Reserve Bank (FED-SPF) with focus on the forecast of RGDP. The empirical evidence leads us to the conclusion that the judgmental dynamics varies considerably according to the ownership of the dataset and is not perfectly coincident with recession dating. These findings open new perspectives on beliefs formation in macroeconomic forecasting and related economic theory.====Section 2 allocates our contribution in the scientific debate; Section 3 describes the theoretical issues to set-up the DJS; Section 4 illustrates the robust statistical methodology to be adopted in the new framework; the results of the application on real U.S. data are illustrated in 5 Application, 6 Conclusions concludes; finally, an Appendix provides mathematical proofs, while a separate Supplement reports the results of some Monte Carlo simulations and additional results.",Can we estimate macroforecasters’ mis-behavior?,https://www.sciencedirect.com/science/article/pii/S0165188923000386,10 March 2023,2023,Research Article,21.0
"Huang Wenli,Liu Wenqiong,Lu Lei,Mu Congming","China Academy of Financial Research, Zhejiang University of Finance and Economics, Hangzhou, China,School of Economics and Management, Huzhou University, Huzhou, China,Asper School of Business, University of Manitoba, 181 Freedman Crescent, Winnipeg, MB, R3T 5V4, Canada,College of Finance and Statistics, Hunan University, Changsha, China","Received 17 October 2022, Revised 16 January 2023, Accepted 6 March 2023, Available online 9 March 2023, Version of Record 17 March 2023.",https://doi.org/10.1016/j.jedc.2023.104637,Cited by (0),We introduce learning into the hedge fund managers’ risk choice problem with imperfect information. We find that with a constant but unobserved expected ,"In response to macroeconomic conditions or business cycles, hedge fund managers are generally free to change their trading strategies, leverage, and allocations to different asset classes. A growing body of literature finds evidence of the large deleveraging process that took place in the hedge fund sector during the 2008 financial crisis and the COVID-19 pandemic (Ang, Gorovyy, Van Inwegen, 2011, Ben-David, Franzoni, Moussawi, 2012, Kruttli, Monin, Petrasek, Watugala, 2021). On the other hand, hedge fund managers not only process information about future asset values and use that information to invest in high-valued assets, they also actively learn from what they have experienced.==== This stylized fact raises the question of how hedge fund managers shift investment strategies if they actively learn over business cycles and/or macroeconomic fluctuations.====By assuming imperfect information about financial market states captured by a two-state Markov regime switching process, we introduce a learning process into hedge fund managers’ risk choice problem and study how fund managers’ active learning process influences their risk taking and compensation. Specifically, we introduce large uncertainties captured by a two-state Markov regime switching process in the hedge fund industry to study the optimal leverage policy and manager compensation with learning amid imperfect information about market states. Our model builds on the dynamic frameworks by Goetzmann et al. (2003), henceforth GIR, and Lan et al. (2013), henceforth LWY, by adding partial information about the market states captured by a two-state Markov regime switching process. With general market swings, it becomes much more difficult for managers to estimate the expected return of investment. The manager with a prior belief in his investment return learns about expected returns from the realized asset value process, revising beliefs as new information arrives. The five main building blocks of the model are (1) an unknown expected return on investment with a possibly stochastic description by a two-state Markov regime-switching process, (2) the manager’s Bayesian learning process, (3) management fees as a fixed ratio of asset value and performance fees as a high-water mark (HWM) incentive mechanism, (4) a risk strategy and leverage constraint, and (5) liquidation either exogenously or endogenously and the manager’s outside option as a lower fraction of total fees at the HWM. The manager chooses optimal dynamic leverage strategies and readjusts asset allocations dynamically to maximize the expected present value of total fees.====We solve the manager’s decision rules using dynamic programming and finally obtain the manager’s value function and optimal leverage based on two state variables ====, the ratio of asset value ==== to the HWM ====, and the belief ==== that the expected return on investment is high. To investigate how fund managers’ active learning process influences fund risk taking and compensation in this extended model, we consider two scenarios about the expected return on investment. In the first scenario denoted by ====, the hedge fund manager does not know the expected return on investment, which is constant. In the second case denoted by ====, the expected return on investment is stochastic, and the hedge fund manager does not have full information about it.====With an unknown and constant expected return, the learning process may increase the level of leverage and the manager’s total fees relative to the case of complete information. This outcome, to some extent, theoretically verifies the presence of the learning process in the hedge fund industry documented by Racicot and Théoret (2016). As ==== increases with a fixed ====, the learning process delivers the information of good fund performance and thus increases the belief in a high expected return. Consequently, the manager would increase risk investments to collect more fees. Keeping ====, for example, ====, at the liquidation boundary, the leverage and the manager’s total fees are higher than in the complete information case when ====. At this point, the volatility of belief updating reaches a maximum, and the fund is near liquidation; the manager has an incentive to increase the leverage and take a gamble. As ==== increases to 1 or decreases to 0, the difference vanishes since there is no uncertainty.====In the case with an unknown and stochastic expected return on investment, the manager chooses a higher leverage level than in the case with complete information when the fund is close to liquidation. However, as the performance of hedge funds improves, the manager chooses to decrease the leverage lever more than when there is full information about market states, leading to flatter leverage. Specifically, as ==== increases, the leverage is relatively flatter relative to the complete information case. The mean reversion property of the belief process is expected to decline on average when ==== is relatively high, which dampens the belief about a high return, so the leverage reverses. The drift and volatility of belief learning jointly increase the gap between the complete information and partial information cases. In the ==== case, the expected change in the manager’s belief is no longer zero. In other words, incorporating learning about the state switch into the belief updating leads to changes in the belief being locally predictable. However, the belief updating process is a martingale in ====, and hence, the manager’s learning about returns becomes more volatile. Note that the leverage within the hedge fund is positively related to the market beta of a hedge fund; a flatter leverage implies a smaller dispersion of hedge fund market betas. Therefore, learning about states corresponding to unknown and stochastic expected returns on investment reduces the dispersion of hedge fund market betas. This outcome explains why the cross-sectional dispersion of hedge fund strategies’ market betas is high during the Asian crisis and at the start of the tech bubble episode, but the decrease in cross-sectional dispersion is severe during the subprime crisis (Racicot and Théoret, 2016).====We also consider the effects of transition intensity ==== on total compensation and leverage decisions. With respect to ====, keeping ==== constant and increasing ==== would result in higher compensation and leverage as the transition probability increases from a low return to a high return. In contrast, keeping ==== constant and increasing ==== would result in lower compensation and leverage. When the fund is near liquidation and holding ==== constant, the leverage choice is similar to that above. However, more interestingly, the leverage choice reverses when the fund performs well. In fact, different transition intensities reflect the duration of time from one state to another and represent different market conditions. Increasing ==== and keeping ==== constant means that the probability of shifting from a low state to a high state increases, so the market condition will turn out to be good. At this point, the manager choose to decrease leverage, making the leverage flatter with increases in ====. The above results reflect that procyclicality is significantly weakened, which further strengthens the empirical finding of Racicot and Théoret (2016).====Finally, many parameters, such as the volatility rate ====, wedge ====, leverage constraint ==== and liquidation boundary ====, have a substantial impact on the optimal leverage and present value of the manager’s fees. Our model shows increasing volatility ==== and a liquidation boundary ====; decreasing the fraction of the manager’s outside option ==== and leverage constraint ==== could lead to a lower leverage level. Moreover, increasing the wedge ==== makes the leverage choice flatter as ==== increases. The larger the wedge is, the more volatile the belief updating is. On the other hand, a larger wedge also means more volatility in the market. Although the fund operates well, the manager worries about a future bad state and thus chooses less risky strategies. Again, these results verify the empirical finding that procyclicality has declined since a learning process is at play.====Our paper contributes to the literature on the leverage of hedge funds. GIR (2003) provide the first quantitative intertemporal evaluation framework for hedge fund management fees and performance fees given a HWM. Panageas and Westerfield (2009) study fund managers’ leverage investment management and test the hypothesis of no management fees and no liquidation boundary for the fund. They find that managers place constant and bounded weights on risky assets and the remainder on riskless assets and act as investors with CRRA utility. LWY (2013) develop a unified dynamic model to analyze hedge fund leverage policy and value fund management compensation contracts. The results show that the manager dynamically trades off the benefit and risk of leverage to maximize the present value of fees. Hedge fund managers may dynamically adjust leverage levels to adapt different hedge fund strategies (Getmansky, Lee, Lo, 2015, Schneeweis, Martin, Kazemi, Karavas, 2005).==== Unlike these papers, we consider the case with incomplete information about hedge fund returns.====Our paper has a close relation to the literature on partial information. Xia (2001) analyzes the effects of uncertainty about the predictability of stock returns on optimal dynamic portfolio choice. Dynamic learning leads to a rich set of relations between the optimal choice and investment horizon and substantial market timing elements in the optimal hedge demand. In an individual’s optimal consumption-saving and portfolio choice problem, estimation risk arising from the agent’s leaning process generates additional precautionary savings demand (Wang, 2009). Collin-Dufresne et al. (2016) show that parameter learning generates long-lasting, quantitatively significant additional macroeconomic risks that help explain standard asset pricing puzzles. Additionally, with real options, learning about the unobserved mean appreciation rate would lead to a significant loss in the implied value of the option to invest (Yang and Yang, 2012). In contrast, we introduce imperfect information and parameter learning into the hedge fund industry.====This paper is also related to a body of literature about learning over business cycles or macroeconomic fluctuations. Cyclical learning is widely used in macroeconomics to generate asymmetric business cycle dynamics. For instance, Chalkley and Lee (1998) explain the asymmetric movement of economic time series over business cycles by considering learning and imperfect information about the economy’s state. Similarly, the explanation for asymmetric movements in macroeconomic aggregates in Van Nieuwerburgh and Veldkamp (2006) relies on procyclical learning as a consequence of higher precision signals in booms than in recessions. Recently, Angeletos et al. (2020) study the policy implications of the endogeneity of information about the state of the economy by learning over business cycles. In addition, learning over business cycles or economic conditions is also taken into account by asset management literature.==== Bollen and Whaley (2009) employ an optimal change-point regression that allows risk exposures to shift to accommodate hedge fund managers’ responses to changing market conditions and arbitrage opportunities. Kacperczyk et al. (2016) develop an attention allocation model that uses the state of the business cycle to predict information choices, which in turn replicate the observed patterns in investments and returns of actively managed U.S. mutual funds. Differing from these papers, we focus on how fund managers’ active learning influences their risk taking with the HWM contract by assuming incomplete information about the economy’s state.====The remainder of this paper is organized as follows. Section 2 describes the model setup, Section 3 solves the model and Section 4 presents the numerical analysis. An extension is provided in Sections 5 and 6 concludes the paper.",Hedge funds trading strategies and leverage,https://www.sciencedirect.com/science/article/pii/S016518892300043X,9 March 2023,2023,Research Article,22.0
"Ding Jing,Jiang Lei,Liu Xiaohui,Peng Liang","School of Economics and Management, Tongji University, Shanghai, 200092, China,School of Economics and Management, Tsinghua University, Beijing, 100084, China; Ambassador Crawford College of Business and Entrepreneurship, Kent State University, Kent, OH, 44240, USA.,School of Statistics, and Key Laboratory of Data Science in Finance and Economics, Jiangxi University of Finance and Economics, Nanchang, 330013, China,Department of Risk Management and Insurance, Georgia State University, Atlanta, GA, 30303, USA","Received 19 January 2022, Revised 22 January 2023, Accepted 6 March 2023, Available online 9 March 2023, Version of Record 25 March 2023.",https://doi.org/10.1016/j.jedc.2023.104635,Cited by (0),"When using daily mutual fund returns to study market timing ability, heavy tails and ==== significantly challenge the existing methods. We propose a weighted nonparametric measure and test for market timing. The test finds different results from the traditional parametric inference concerning timing. By examining the holding characteristics of the funds with different levels of timing ability, we find that funds with positive timing ability hold stocks with lower trading frictions. We find evidence of a tradeoff between market timing ability and stock picking skill after excluding funds with zero timing ability, which is robust to different benchmark models.","Whether mutual funds possess timing ability has been debated since Treynor and Mazuy (1966), Henriksson and Merton (1981) published seminal papers. Applying parametric measures (i.e., a coefficient related to market volatility in a factor model) to monthly fund returns, researchers find that, on average, U.S. equity funds have negative market timing ability; see, for example, Chang and Lewellen (1984), Henriksson (1984), Grinblatt and Titman (1988), Becker et al. (1999). Jiang (2003) finds similar results using ratios of change in fund return to that in market return or change in residual of modeling fund returns to that of modeling market returns. The test in Jiang (2003) assumes independent ratios, which requires modeling and estimating both fund returns and market returns.====Since most well-diversified equity funds hold and trade hundreds of stocks, we expect that mutual funds will time the market and trade very frequently in response to continuous information, time-varying stock market liquidity, and daily capital flow.==== Thus, compared with monthly data, daily fund returns should better capture the timing ability of mutual funds. Goetzmann et al. (2000) find that estimating market timing ability based on monthly returns results in a downward bias when funds time the market daily. Bollen and Busse (2001) indicate that using daily fund returns provides better statistical power when testing for the absence of timing ability than using monthly fund returns, even if mutual funds time market at a monthly frequency. Over time, more and more researchers have reached the consensus that, because of the dynamic trading effect, using daily fund returns to evaluate fund timing ability should be as accurate as, if not more accurate than, using monthly returns. For example, Chance and Hemler (2001), Bollen and Busse (2005), Mamaysky et al. (2008) all estimate fund managers’ timing ability based on daily fund returns and a factor model with a coefficient for market timing measure.==== Recently, Back et al. (2018) use daily fund returns and find a negative association between nonparametrically measured market timing ability==== and stock picking skill measured by alpha (see Fama and French 2010). However, these papers do not explicitly consider the stylized facts of mutual fund daily returns, such as heavy tails and heteroscedasticity, making the conventional test fail as the limit is not normal. Breen et al. (1986) show the importance of heteroscedasticity in parametrically inferring and evaluating the timing ability of mutual fund managers and find that heteroscedasticity affects the power of the timing test.====By estimating the tail indexes of daily returns of U.S. equity funds, we find that empirically many funds do not have enough finite moments, which confirms heavy tails in those funds. Hence, the coskewness (a nonparametric market timing measure) in Back et al. (2018) may not be well-defined (i.e., finite), or its estimator has a nonnormal limit. This paper first refines their measure and proposes nonparametric market timing measures for individual funds requiring fewer finite moments. We also develop robust and efficient methods to test whether a fund has zero timing ability based on our measure. To precisely capture heteroscedasticity and volatility persistence of fund daily returns and to consider autocorrelation of daily risk factors, we assume that the factor model for fund daily excess return has GARCH errors (see Bollerslev, 1986, Engle, 1982), and each risk factor follows an ARMA-GARCH process.==== This assumption is more realistic and different from most previous empirical studies of mutual funds, such as Carhart (1997), in which researchers use the pure white noise assumption.====We define a weighted nonparametric market timing measure to reduce the heavy tail effect. Since the weighted and unweighted measures have the same sign, using either one leads to the same negative, zero, and positive timing classification. Therefore, the critical issue is to quantify the estimation uncertainty. Uncertainty quantification becomes challenging when using the unweighted measure without enough finite moments as the limit is nonnormal and standard techniques provide an incorrect estimation. We derive the asymptotic normality of the proposed weighted measure estimator. We use a random weighted bootstrap method to avoid estimating the complicated asymptotic variance due to GARCH errors for daily returns, the ARMA-GARCH models for factors, and the weighted inference. We also provide an extension to generalized GARCH errors in a remark.====Applying the proposed test for the absence of market timing ability to all actively managed U.S. equity funds using daily fund returns from September 1, 1998 to December 31, 2018, we find that out of 534 funds with positive Treynor-Mazuy timing ability based on the traditional parametric method, 344 of them have zero or perverse timing ability using our weighted nonparametric method. The traditional parametric method identifies 1837 funds with zero Treynor-Mazuy timing ability, while the weighted nonparametric method identifies 2426 funds with zero timing, and 1522 of them overlap. These significant differences may come from the fact that the traditional least squares estimate and ====-test do not take the heavy tail and heteroscedasticity into account, or the parametric model is misspecified.====What kind of funds is likely to show timing ability? After grouping funds into three categories of positive, zero, and negative nonparametric market timing ability, we find that different groups hold stocks with different characteristics. For example, funds with significantly positive ability hold big stocks and stocks with lower values of Amihud ratio and volatility of liquidity based on dollar trading volume. They also hold stocks with fewer zero trading days than funds with negative timing ability. Therefore, funds with timing ability tend to have stocks with lower trading frictions (according to the classification in Hou et al. 2015). The results hold even after controlling for spurious timing bias. One potential explanation for this phenomenon is that, when facing the dynamics of market returns, only funds holding liquid stocks time the market and show significantly positive timing ability. In contrast, other funds are constrained by the high trading costs and give up their opportunity to time the market. Another reason could be that fund managers with timing ability intentionally hold stocks with low trading frictions because they expect to time the market and generate high turnover in the future.====Given our statistically identified distinction between funds with zero and nonzero timing ability, we reexamine the association between fund alpha and market timing for each fund. Back et al. (2018) find a tradeoff between alpha and timing, which may be driven by the negative effect of seeking alpha on coskewness. Kacperczyk et al. (2014) provide empirical evidence that market timing and stock picking may appear separately during different market conditions. Both papers include funds with zero market timing ability in their sample, which may blur the association. Motivated by Hansen (2005), Hansen et al. (2011), we reexamine the association by excluding funds with zero timing ability and find that the negative association between market timing ability and stock picking skill is more substantial. The tradeoff is robust to the selection of the factor models. Finally, different from stock pricking skill, alpha, we find that mutual funds with significantly positive timing ability based on our weighted nonparametric measure provide a higher net return to the investors in the long run than poor timers, which is consistent with the expectation that market timing ability of fund managers is valuable to investors. However, we can not see such a pattern based on the traditional parametric approach.====Our paper is related to but different from Back et al. (2018). First, our unweighted nonparametric Treynor-Mazuy market timing measure is the same as the coskewness measure in Back et al. (2018). Back et al. (2018) find a tradeoff between stock picking skill and market timing ability for the one-factor model using daily data. However, they do not derive a test for zero coskewness and can not exclude funds with insignificant market timing ability from their samples when analyzing the tradeoff. Our paper develops such a test and finds evidence of the tradeoff after excluding funds with zero market timing ability, regardless of the factor model selected. Second, this paper finds that the coskewness in Back et al. (2018) is ill-defined for funds without enough finite moments and that its estimator has a nonnormal limit for those funds. Therefore, this paper further defines a weighted nonparametric market timing measure and develops a test for zero timing ability for daily returns with fewer finite moments.====Our paper is also related to the literature measuring mutual fund market timing ability: the standard Treynor-Mazuy model and Henriksson-Merton model test the nonlinear relation between fund returns and contemporaneous market returns. However, the nonlinear relation can also be induced by reasons other than active market timing ability. Possible reasons are the spurious timing from holding stocks with option-like features (Jagannathan and Korajczyk, 1986) and the artificial timing caused by funds changing their beta in response to the previous market return. Our paper uses daily returns to overcome the dynamic trading effect and solves underestimating timing ability using monthly fund returns when funds are daily timers (Bollen, Busse, 2001, Goetzmann, Ingersoll, Ivković, 2000). Our developed method is robust against heavy tails and heteroscedasticity of fund daily returns.====We organize the paper as follows. Section 2 presents the definitions of the nonparametric market timing measure and its weighted version, the model, and methods for testing the absence of market timing ability. Sections 3 and 4 are our simulation study and empirical analysis of U.S. equity funds. Section 5 concludes. The choice of weights, regularity conditions, and theoretical proofs are in Appendices A, B, and C, respectively.",Nonparametric tests for market timing ability using daily mutual fund returns,https://www.sciencedirect.com/science/article/pii/S0165188923000416,9 March 2023,2023,Research Article,23.0
"Kolasa Marcin,Wesołowski Grzegorz","SGH Warsaw School of Economics and International Monetary Fund. Al. Niepodleglosci 162, Warszawa 02-554, Poland,University of Warsaw","Received 3 June 2022, Revised 1 March 2023, Accepted 3 March 2023, Available online 7 March 2023, Version of Record 17 March 2023.",https://doi.org/10.1016/j.jedc.2023.104631,Cited by (0),", as well as a significant loss of price competitiveness. We next develop a quantitative ==== between borrowers and savers.","It is widely acknowledged that large capital inflows may drive domestic financial cycle (DFC), contributing to economic and financial instability, and seriously constraining monetary policy, especially in emerging market (EM) economies, see e.g. BIS (2012), Reinhart and Reinhart (2009) or Rey (2015). It is also well-documented that quantitative easing (QE) introduced by the US Federal Reserve in response to the Great Recession of 2007-2009 was the crucial factor in steering the global financial cycle (GFC), that resulted in a massive wave of cross-border capital flows and impacted asset prices worldwide. From the perspective of EM economies, a distinctive feature of this episode was that, unlike before, the post-crisis capital inflows were not driven by recipient countries’ economic fundamentals (like better growth prospects, financial liberalization etc.) or conventional monetary policy adjustments abroad, but were instead a byproduct of unconventional (and possibly distortionary) actions taken by foreign central banks. Many EM countries, notably in Latin America (LA), implemented various measures aimed at counteracting the impact of the resulting global financial cycle, such as currency interventions and macroprudential policy tightening.====This paper investigates the tensions between GFC and DFC created by the US QE for central banks in EM economies, as well as additional non-standard measures that may help resolve them. On the one hand, in the GFC context, exchange rate appreciation caused by capital inflows is conducive to a severe loss in international competitiveness, calling for monetary accommodation. As argued by Rajan (2016), unlike in the case of standard monetary easing abroad, the demand switching effect through the exchange rate induced by large-scale asset purchases (LSAP) abroad is likely to dominate the effect of easier financial conditions on domestic demand in EM economies. Data from LA countries are consistent with this view. As can be seen in Figure 1, the current account balance clearly deteriorated in this region during the time of US QE, and this was despite GDP growth being similar to the pre-crisis period. On the other hand, in the DFC context, QE eases credit conditions, which may justify some monetary contraction. The special feature of asset purchase programs is that, unlike conventional monetary easing, they target the longer part of the yield curve, and hence are more important for the costs of long-term financing, such as housing loans. The transmission is likely to be particularly strong when the banking sector exposure to government bond holdings is large, which is clearly the case in most LA countries.==== Indeed, after QE was introduced in the US, credit to households (Figure 2), which consists mainly of mortgage debt, and house prices (Figure 3) continued to soar in this region. During the same period, household indebtedness in the US was on a decline while house price growth was only moderate.====To verify the presence of these dilemmas more formally, we estimate the series of surprise changes in large-scale asset purchases by the Fed by following the approach in Swanson (2021), and run local projections as in Jordà (2005) on a panel of LA economies. We find that, despite measures undertaken by these countries to curb capital inflows, QE in the US led to a persistent appreciation of LA currencies as well as growth in credit to households and house prices, which however did not result in a significant expansion in output. We interpret this evidence as indicating that US QE creates tensions between macroeconomic and financial stability that cannot be easily resolved by using standard monetary policy alone. We next develop a quantitative macroeconomic framework that features such tensions, and use it to show that they can be mitigated by an appropriate selection of policy tools, including those that were adopted by LA economies. The model object is a small open economy, linked financially to the rest of the world by trade in long-term bonds, which are in turn imperfect substitutes of short-term bonds. As shown by Kolasa and Wesołowski (2020), such a form of international asset market segmentation is strongly supported by the data for EM economies, and generates realistic international capital flows, comovement in the term premia and exchange rate adjustments in response to QE in developed countries. Another important feature of our model is the presence of banks, operating similarly as in Gertler and Karadi (2013) and Kirchner and Wijnbergen (2016), and collateral constraints in the housing market in the spirit of Iacoviello (2005). The role of banks is to intermediate funds between savers and the government and private borrowers. They provide transmission from long-term bond prices to the cost of credit, which is additionally amplified by endogenous responses of house prices.====The model, calibrated to LA data, successfully replicates a number of empirical findings concerning both domestic and global financial cycles triggered by US QE. It generates a large inflow of capital to EM economies, loss of international competitiveness, as well as a boom in house prices and mortgage loans. Two key mechanisms are at play. One is the no-arbitrage condition associated with international trade in long-term bonds, which postulates equalization of returns on domestic and foreign bonds. Since QE abroad generates a deep fall in foreign long-term rates, which is not matched by an equal drop in domestic rates under standard monetary accommodation, local currency strongly appreciates, consistently with evidence from local projections. The side effect is a sharp deterioration of small economy’s international competitiveness. The second mechanism operates through internal arbitrage between bank assets. An increase in bond prices depresses the cost of loans, leading to a boom in credit and asset prices, also in line with econometric evidence.====The described scenario poses challenges for policy in the EM economy, calling for the use of non-standard measures that could complement conventional monetary policy. We consider three types of measures discussed in the literature, and used in LA economies in response to US QE: foreign exchange (FX) interventions, loan-to-value (LTV) cap on domestic borrowing, and capital controls in form of taxes on long-term bond returns earned by non-residents. According to our model simulations, an attempt to avoid the loss in international competitiveness by using FX interventions necessarily leads to a stronger drop in local long-term rates, which amplifies the domestic financial cycle. Constraining the latter can be achieved by applying domestic macroprudential instruments such as tightening of the LTV ratio. However, when used alone, this policy is neither very efficient in reducing capital inflow nor in limiting exchange rate appreciation, and hence it does not address the problem of international competitiveness. A policy that successfully deals with both macroeconomic and financial stability, and hence provides an alternative to coordinated FX interventions and LTV tightening, is to impose capital controls. In fact, this type of non-standard measure turns out to be clearly preferred from a macro-financial loss function perspective. Moreover, and in contrast to other interventions, it helps alleviate wealth redistribution between borrowers and savers that is caused by foreign QE.====Our paper is related to several strands of the literature, and in particular to positive and normative analyses of international capital flows, with a special emphasis on those arising from QE. Among studies with a positive flavor, empirical papers clearly dominate, see e.g. Fratzscher et al. (2018), Ahmed and Zlate (2014), Neely (2015), Lim and Mohapatra (2016), Tillmann (2016) and Bhattarai et al. (2021). The empirical part of our study adds to this line of research by using panel local projections rather than VARs, and by explicitly controlling for non-QE monetary shocks in the US. Importantly, we estimate the effects on GDP, credit to households and house prices, arguing that the latter two are key elements of the transmission of US QE shocks to domestic credit cycle, especially when banks hold large stocks of government bonds. However, our main contribution to the positive literature on international capital flows is to offer a quantitative theoretical framework that, by incorporating both cross-border and internal borrowing, helps understand the tensions between macroeconomic and financial stability faced by many EM as a result of LSAP programs conducted by foreign monetary authorities.==== In this way, we also highlight the importance of QE in large economies for the financial cycle in other countries.====On the normative side, we contribute to the recent literature and policy debate on optimal policy responses to international capital flows.==== Unlike other papers in this line of research, we focus on a specific type of capital flows, namely those induced by foreign QE, and on their impact on the domestic credit and asset price cycle. This has important consequences for the effectiveness of the considered policy reactions. The existing theoretical papers on FX interventions suggest that they may improve macroeconomic performance (see e.g. Cavallino, 2019, Fanelli, Straub, 2021, Mertens, Hassan, 2017) by alleviating financial market imperfections as in Gabaix and Maggiori (2015).====There is also large literature stressing the benefits of countercyclical LTV adjustments for smoothing credit booms driven by asymmetric productivity, news, terms-of-trade, housing market or global liquidity shocks (Bianchi, 2011, Bianchi, Liu, Mendoza, 2016, Bielecki, Brzoza-Brzezina, Kolasa, Makarski, 2019). These predictions are usually confirmed by empirical studies, see e.g. Fratzscher et al. (2019) or Gambacorta and Murcia (2019), even though less optimistic or skeptical views also exist, especially for currency interventions (e.g. Chamon et al., 2019). Our key insight is that, when faced with capital inflow induced by QE abroad, using either FX interventions or LTV policy is not enough, and they need to be combined to address the relevant imbalances. As shown by Ghosh et al. (2017), this is exactly what emerging market economies do when faced with capital inflows.====Our analysis also underlines high effectiveness of capital controls, as already stressed by a line of theoretical papers that do not consider QE as the driving force (e.g. Korinek, 2011, Ostry, Ghosh, Chamon, Qureshi, 2012, Davis, Presno, 2017), also confirming their favorable role for the credit cycle, as empirically tested by Forbes et al. (2015). According to our paper, if capital controls are available, they are sufficient to address the impact of both GFC and DFC arising from asset purchases by foreign central banks. This result contrasts with Korinek and Sandri (2016), who find an additional role for macroprudential regulation of credit during contractionary exchange rate depreciations, and the difference exactly reflects the special feature of QE-induced capital inflows, namely their harmful effect on international competitiveness of recipient countries.====The rest of this paper is structured as follows. In Section two we discuss empirical evidence based on local projections. Section three describes the model and Section four its calibration. In Section five we present our baseline scenario that shows how QE in the US generates a financial cycle in EM economies. Section six discusses the transmission of possible non-standard policy measures. Section seven evaluates the usefulness of these additional instruments as complements to standard monetary policy. Section eight concludes.",Quantitative easing in the US and financial cycles in emerging markets,https://www.sciencedirect.com/science/article/pii/S0165188923000374,7 March 2023,2023,Research Article,24.0
"Herwartz Helmut,Wang Shu","Chair of Econometrics, University of Göttingen, Humboldtallee 3, Göttingen D-37073, Germany","Received 19 October 2022, Revised 23 February 2023, Accepted 1 March 2023, Available online 5 March 2023, Version of Record 27 May 2023.",https://doi.org/10.1016/j.jedc.2023.104630,Cited by (0),"The median and median target estimates in sign-restricted SVARs are driven by a highly informative prior for the set-identified structural parameters. This paper proposes an approach for point elicitation by minimizing the evidence against the null hypothesis of independence with respect to the orthogonalized residuals implied by the identified set. Finite sample properties of the estimator are studied in a Monte Carlo experiment. As an empirical illustration, we analyze ","Following the pioneering work of Canova and De Nicolo (2002), Faust (1998) and Uhlig (2005), sign restrictions have become a relevant approach to identification in structural vector autoregressions (SVARs, VARs). Compared with the use of conventional zero restrictions (Blanchard, Quah, 1989, Sims, 1980), identification by means of sign restrictions relies on much weaker assumptions, which often align with a broad class of theoretical models.==== However, imposing sign restrictions produces a set of structural models that equally align with both identifying assumptions and observations, which complicates the inferential analysis of structural objects of interest. Given the point-identified reduced form parameter, all structural models implied by the identified set have the same likelihood and thus are indistinguishable from a frequentist perspective. Owing to this, conditional Bayesian priors for the structural parameter remain unrevised in the view of new data and will continue to influence the posterior even in the asymptotic case. Therefore, results from (standard) Bayesian posterior inference about the structural parameter must be interpreted with caution.====Alternative approaches to estimation and inference in sign-restricted SVAR models have been recently developed in the literature (e.g., Gafarov, Meier, Montiel Olea, 2018, Giacomini, Kitagawa, 2021, Granziera, Moon, Schorfheide, 2018).==== While the provision of confidence sets is of key importance in statistical inference, a natural complement of representative intervals is the elicitation of a particular point estimator of structural model parameters (incl. IRFs). Because many sign restrictions have only weak identification power, the resulting credible intervals are often too wide to deliver meaningful economic implications. Therefore, in many empirical applications, researchers typically report certain point estimates with the purpose of communicating the core implications of the identified set conveniently. However, problems related to commonly-reported posterior median estimates have been powerfully underlined in the recent literature (see e.g., Baumeister and Hamilton, 2015).====In this paper, we show that the identification power of sign restrictions crucially depends on the correlation of the reduced form residuals, whereas consistent point estimation is especially essential when the employed restrictions are conditionally weak for the structural objects of interest. However, because the conditional priors for set-identified parameters are not updated through the likelihood function (Moon, Schorfheide, 2012, Poirier, 1998), both the median and the median target estimators may be driven by uncontrolled prior information. Thus, conclusions based upon these point estimates could be misleading and should be treated with caution.==== As an alternative, we propose a non-parametric way to elicit point estimators of structural model parameters by exploiting additional statistical information of the data. Specifically, we take advantage of a developing body of literature (e.g., Gouriéroux, Monfort, Renne, 2017, Herwartz, 2019, Keweloh, 2021, Lanne, Luoto, 2021, Moneta, Entner, Hoyer, Coad, 2013) that suggests tools of independent component analysis (ICA) as a means of identification in structural models. Explicitly, by assuming statistical independence of the structural shocks, a ‘most plausible’  choice of the structural parameter is obtained from maximizing the ====-value of a non-parametric independence test based on the distance covariance (dCov, Székely et al., 2007) as suggested by Matteson and Tsay (2017). Accordingly, we refer to this eliciting approach as sign-restricted Hodges–Lehman (SRHL) estimation (Dufour, 1990, Hodges, Lehmann, 1963). From an economic perspective and depending on the matter of interest, the assumption of mutual independence could appear quite strong. In this regard, the SRHL approach may serve as a complementary tool, which allows for testing the stronger assumptions (i.e., mutual independence) conditional on sign restrictions that are often consensual and, hence, weak. Such a practice could provide valuable information on the statistical properties of the shocks – which might reassure the selected economic framework – or on the higher-order dependence patterns of orthogonal shocks.====We also develop a less restrictive framework for achieving (partial) identification that only relies on group-wise independence (instead of mutual independence). Since shocks within the same group are allowed to be dependent, this can be considered as a weaker form of independence, as called for by Montiel Olea et al. (2022). Unlike purely data-driven identification methods, SRHL does not suffer from local identification under mild conditions and delivers outcomes that are truly structural by construction in terms of economic interpretations as given by the employed sign restrictions. In addition to the non-parametric method of Matteson and Tsay (2017), the proposed framework for point elicitation can be easily modified to incorporate alternative independence criteria. We explicitly address two of them: The pseudo maximum likelihood estimation (PML) (Gouriéroux et al., 2017) and the generalized method of moments (GMM) approach of Lanne and Luoto (2021). Owing to its robustness in various circumstances as shown in previous simulation studies (Herwartz, Maxand, 2020, Herwartz, Rohloff, Wang, 2022, Matteson, Tsay, 2017), we consider the non-parametric approach based on dCov statistics as a flexible framework for testing independence, when the distribution of the structural innovations is unknown. We provide simulation-based evidence on the performance of the suggested SRHL estimator in both small and large samples and highlight its consistency and robustness under various distributional scenarios featuring super-Gaussian, nearly Gaussian and sub-Gaussian shocks. Combining statistical independence with set-identifying sign restrictions has also been advocated by Drautzburg and Wright (2021). Unlike the approach developed in this paper that aims for a point elicitation, Drautzburg and Wright (2021) focus on the refinement of the identified set. While the interval estimation suggested by these authors is conducted by taking the intersection between the confidence interval obtained by sign restrictions and the region, for which the null hypothesis of independence cannot be rejected at a certain significance level, the point estimator developed in this study is obtained by minimizing the evidence against the null hypothesis conditional on the sign restrictions.====We employ SRHL estimation to reconsider the empirical assessment of monetary policy (MP) effects within the rational bubble model of equity valuation (Galí, 2014). The adoption of (weak and consensual) sign restrictions with further narrative resolution holds particular merit for this purpose, since the recursive identification scheme employed in Galí and Gambetti (2015) has been considered as crucial for detecting rational bubbles (see e.g., Paul, 2020). Without imposing timing restrictions, the proposed coupling of set-identification and point estimation allows the data to speak in favor of results that are similar to those in Galí and Gambetti (2015). In specific, an unexpected monetary tightening invokes a significant decline in dividends such that the fundamental component of the stock price exhibits a negative short-run response that tapers off. However, the response of the bubble component is positive and tends to dominate the long-term responses of asset prices. The core findings remain largely robust when we relax the independence assumption by allowing for dependence among non-MP shocks and by employing a GMM test that accounts for the possible prevalence of stochastic volatility. Going beyond this benchmark study, we underline the informational content of the detected MP shocks by highlighting their strong linear correlations with MP measures that have been retrieved in a theoretically and conceptually distinct context. Among others, the strongest correlation is detected with reference to the innovations in the MP reaction function in the DSGE model estimated by Smets and Wouters (2007).====The remainder of this paper is organized as follows. Section 2 introduces the sign-restricted SVAR models and discusses in detail the proposed approach for point elicitation. Monte Carlo evidence in Section 3 sheds light on the finite sample properties of the estimator. We employ the suggested approach in Section 4 to the work of Galí and Gambetti (2015) on the link between asset valuation and monetary policy in the US. Section 5 concludes. The online Appendix provides additional theoretical analysis and simulation evidence on point elicitation and implicit priors in sign-restricted SVARs (Appendices A, B and D) and further empirical results for US monetary policy and rational bubbles (Appendix C).",Point estimation in sign-restricted SVARs based on independence criteria with an application to rational bubbles,https://www.sciencedirect.com/science/article/pii/S0165188923000362,5 March 2023,2023,Research Article,25.0
"Chang Juin-Jen,Kuo Chun-Hung,Lin Hsieh-Yu,Yang Shu-Chun S.","Institute of Economics, Academia Sinica, Taiwan,Department of Economics, National Tsing Hua University, Taiwan,Department of Economics, Tunghai University, Taiwan","Received 21 November 2022, Revised 24 February 2023, Accepted 24 February 2023, Available online 1 March 2023, Version of Record 27 May 2023.",https://doi.org/10.1016/j.jedc.2023.104622,Cited by (0),"Since the mid-1980s, U.S. ","Share buybacks reached historic highs among S&P 500 companies in 2021. Amid rising energy prices and profits during the Russia-Ukraine war, major oil and gas companies’ buybacks surged while their capital expenditures remained mostly flat despite record second-quarter profits in 2022 (Kimani, Kinery, 2022). Rising buybacks have been blamed for reducing investment, jeopardizing innovation and long-term growth (Lazonick, 2014, Lazonick, Sakinc, Hopkins, Nguyen, Vu, Yin, 2021, Palladino, 2018). Some studies, however, highlight their merits in correcting for firm under-valuation (Autore et al., 2019; D’Mello and Shroff, 2000; Peyer and Vermaelen, 2009) and decreasing capital misallocation (De la O, 2020). Missing from these discussions is the influence of buybacks on the macroeconomic effects of corporate tax cuts.====We investigate whether buybacks, in fact, undermine the macroeconomic effects of corporate tax cuts. Following the Tax Cuts and Jobs Act (the 2017 Jobs Act), which decreased the corporate tax rate from 35 to 21 percent, the investment response appears muted (see Fig. 1). Meanwhile, buybacks in 2018 rose by 88 percent from the 2017 level (Standard & Poor’s, 2022). Some argue that corporations spent the tax windfall on buying back shares rather than investing (Egan, 2018, Tung, Milani, 2018), although the literature has mixed evidence on whether buybacks decrease investment in general (Fried, Wang, 2018, Gruber, Kamin, 2017, Turco, 2018).====Over a longer horizon, U.S. corporate buybacks and investment have opposite trends (see Fig. 2).==== After the adoption of Rule 10b-18 in 1982, which provides share issuers a safe harbor from manipulation liability under the Securities and Exchange Act (the 1934 Act), buybacks began to increase. The downward trend in investment, on the other hand, has numerous explanations, such as increasing ownership concentration in certain industries and declining competitiveness and productivity in the U.S. traded goods sector (see, e.g., Alexander, Eberly, 2018, Covarrubias, Gutiérrez, Philippon, 2019, Gutiérrez and Philippon, 2017a, Gutiérrez, Philippon, 2017b, International Monetary Fund, 2015, Stewart, Atkinson, 2013). One explanation is related to corporate governance and agency issues in buyback decisions. As buybacks often increase share prices, which can enrich corporate executives, Gutiérrez and Philippon (2018) find that buybacks crowd out investment in non-competitive sectors. We abstract from governance and agency issues to focus on how buyback activity influences the macroeconomic effects of corporate tax cuts.====We first estimate U.S. federal corporate tax cut effects before and after the mid-1980s to see if share buybacks, GDP, and investment respond differently to corporate tax cuts. Following Mertens and Rvan’s (2013) methodology, which embeds Romer and Romer’s (2010) narrative series for corporate income taxes in structural vector autoregression (SVAR) estimation, we find that corporate tax cuts became less expansionary in GDP and investment, but more stimulative in buybacks in the latter sample.====We then build a dynamic general equilibrium model with corporate financial allocations to study corporate tax cut effects with share buybacks. The baseline model features a stock market and has two types of agents—shareholders and non-shareholders. Simulations for a permanent corporate tax cut of the magnitude of the 2017 Jobs Act show that corporate tax cuts increase profits, raising both investment and buybacks. The optimal buyback response is, nonetheless, much smaller than the actual response following the 2017 Jobs Act. While both types of agents enjoy higher disposable income from the corporate tax cut, most of the income increases accrue to shareholders. Whether non-shareholders’ consumption increases depends on the financing mechanism: when government fiscal adjustments reduce transfers to both types of agents, non-shareholders’ consumption falls below the path without the tax cut.====We also simulate an alternative model without a stock market, as in a standard real business cycle model, to illustrate the role of buybacks in affecting corporate tax cut effects. Although the 1934 Act does not prohibit buybacks, it prohibits share price manipulation. Because buybacks often raise share prices, corporations generally limited buybacks (Grullon and Michaely, 2002). We find that a corporate tax cut produces more positive investment and output responses in the alternative model without buybacks than in the baseline model with buybacks. For a permanent 5.2-percentage-point reduction in the corporate tax rate, approximately the size of the 2017 Jobs Act, the two-year (long-run) cumulative output and investment multipliers are ==== and ==== (==== and ====) in the alternative model, versus ==== and ==== (==== and ====) in the baseline model.==== This suggests that buybacks render tax cuts less expansionary in raising investment and output, especially in the long run.====Since the average corporate tax changes in the SVAR estimation are transitory, we also simulate the baseline and alternative models under a transitory corporate tax cut. Given a certain degree of shock persistence, it remains the case that the baseline model with buybacks generates less expansionary corporate tax cuts than the alternative model without. With an AR(1) coefficient of 0.95 in the corporate tax process, the 10-year cumulative output and investment multipliers are ==== and ==== with buybacks, versus ==== and ==== without buybacks. Also, simulations show that the degree of persistence matters for the corporate tax cut effects: a more persistent corporate tax cut generates more positive effects on output and investment.====Our main analysis relies on a non-distorting financing mechanism by reducing transfers to shareholders. In reality, fiscal adjustments to finance tax cuts are distorting. In the sensitivity analysis, we simulate fiscal adjustment via reductions in transfers to both shareholders and non-shareholders or in government consumption. It remains the case that corporate tax cuts generate more positive output and investment responses in the model without buybacks than in the model with. Corporate tax cuts, however, become less expansionary under distorting financing mechanisms. In particular, if non-shareholders’ transfers are decreased to stabilize government debt, they might not benefit from a corporate tax cut with higher consumption, as found in the baseline model.====Our paper is one of the few that includes corporate finance decisions in a macroeconomic model. Among the few, Chen et al. (2017) focus on the evolution of corporate savings and investment in the global economy, and De la O (2020) studies how Rule 10b-18 affects financial and capital allocations.==== Our paper is also related to the literature that emphasizes the supply-side effects of income taxes (e.g., Chamley, 1986, Judd, 1985, Judd, 1987, Leeper, Yang, 2008, McGrattan, 1994), though they ignore corporate finance decisions. Lastly, various papers study the weak investment effects of the 2017 Jobs Act, blaming economic uncertainty and rising market power (Kopp, Leigh, Mursula, Tambunlertchai, 2019, Krugman, 2018), elevated government debt (Fotiou et al., 2020), and a reduced investment subsidy (Occhino, 2019, Occhino, 2020). Our analysis suggests that considerable buybacks weakened the investment effects of the 2017 Jobs Act.",Share buybacks and corporate tax cuts,https://www.sciencedirect.com/science/article/pii/S0165188923000283,1 March 2023,2023,Research Article,26.0
Guthrie Graeme,"School of Economics and Finance, Victoria University of Wellington, PO Box 600, Wellington, New Zealand","Received 10 September 2022, Revised 17 February 2023, Accepted 21 February 2023, Available online 24 February 2023, Version of Record 27 May 2023.",https://doi.org/10.1016/j.jedc.2023.104621,Cited by (0),Enormous public investment will occur as communities adapt to climate change. Much of this investment will be irreversible and the future benefits are uncertain. The ,"Adapting to climate change will require substantial investment of public funds over the coming decades, creating enormous potential for inefficient decisions involving the timing, scale, and location of investment. Modern decision-support tools can potentially improve the efficiency of this investment. In particular, typical climate-change adaptation projects have the characteristics needed for real options analysis to be important: future benefits are uncertain; reversing investment is costly, if it is possible at all; and decisions can be spread over time as uncertainty is resolved. This paper presents a new real options framework for analysing investment decisions related to climate-change adaptation and demonstrates its application using the example of a decision-maker tasked with upgrading an urban stormwater system.====The paper makes three main contributions. First, it introduces a framework that incorporates climatic and economic volatility in a single model. Its main features are the way it models uncertainty about future climate change, how this uncertainty evolves over time, and how this injects volatility into investment payoffs. Second, the paper constructs an optimal investment policy for an adaptation problem in which the capacity of infrastructure to cope with extreme weather events is a continuous variable and the decision-maker decides how quickly to increase capacity. The value of investment delay options is economically significant, but the relative importance of climatic and economic volatility varies over time. Third, the paper shows that—for the parameterisation used here—almost all the net benefits of investment can be captured if investment timing is decided using a simple alternative to “full” real options analysis. A decision-maker using this alternative rule invests if and only if the present value of the project’s benefits is greater than or equal to the sum of the investment expenditure and a specific approximation of the value of delaying investment.====Turning to the first contribution, there is considerable uncertainty surrounding future climatic conditions (Pindyck, 2021). Climate change happens slowly, we do not know how much ==== and other greenhouse gases will be emitted over the coming decades, and we are uncertain how the climate will change as a result. This uncertainty will eventually fall due to scientific progress, statistical evidence, and observing weather patterns (van der Pol et al., 2017b). The arrival of new climate information will inject volatility into forecasts, but meaningful learning about some important aspects of climate change will take 20–50 years to occur (Lee, Haran, Keller, 2017, Urban, Holden, Edwards, Sriver, Keller, 2014).==== Intra-regime variability means that learning about some aspects of climate change will be even slower (Kopp et al., 2014).==== The framework introduced in this paper models this uncertainty by assuming that the outcome of climate change will match one of two possible scenarios. The decision-maker observes a stream of information that she uses to continuously update her subjective beliefs regarding the probability of each scenario holding. This injects volatility into the payoff from delaying investment in an adaptation project. The arrival of new climate information is slow at first, so this source of volatility is initially small, but accelerates as the two climate scenarios diverge.====The use of Bayesian decision theory has a long history in environmental decision-making. In a paper preceding the real options literature, Davis et al. (1972) consider the option to delay choosing the capacity of a system of dikes in order to gather more river-flow data. Delaying investment allows the decision-maker to update her beliefs regarding the distribution of peak annual river flows used to calculate the expected costs of flood damage. A small but growing literature adopts a similar approach to issues surrounding climate change. For example, when van der Pol et al. (2014) analyse flood-protection investment, they assume uncertainty about the rate of water-level increase is completely resolved at a single future date, whereas van der Pol et al. (2017a) assume uncertainty is reduced, but not eliminated, at a single future date. De Bruin and Ansink (2011) assume uncertainty about the impact of climate change is eliminated in two steps, whereas Guthrie (2019) assumes uncertainty falls gradually over time. In these papers, climate-induced volatility comes from changes in ==== about some unobservable aspect of climate change. That is, volatility comes from the future arrival of new information about the extent of climate change. This is a key feature of the approach adopted here.====The paper’s second contribution is the construction of an optimal policy for increasing an asset’s capacity to cope with extreme weather events. The investment benefits depend on the distribution of the severity of these weather events, which varies as more climate information arrives. Initially, climatic uncertainty falls slowly, so most of the volatility in the investment benefits comes from fluctuations in an economic factor. However, climatic uncertainty eventually begins to fall, which increases the volatility of the investment benefits. Thus, the value of the option to delay investment varies over time, depending on the arrival-rate of new climate information. The delay option’s value contains a second time-dependent component, reflecting the trajectories of key climate variables. This induces relatively fast growth in benefits in the short term, which slows as the climate approaches its new equilibrium. The optimal investment policy can be expressed in several forms, including a threshold for the benefit–cost ratio. This threshold exceeds one by a substantial margin, reflecting the delay option’s value. The threshold is initially high, especially when the decision-maker believes climate change is going to be relatively severe, but falls over time due to the declining growth rate in benefits. This is offset by the faster arrival of new climate information, which increases the investment threshold.====These results originate from the stochastic properties of the investment payoff, which ultimately stem from the model of climate uncertainty. The paper thus contributes to the part of the real options literature that investigates non-standard stochastic structures. Most models assume one or more exogenous state variables evolve according to geometric Brownian motion, but some adopt different structures, especially when considering climate-change adaptation. For example, Truong et al. (2018) assume bushfires occur with a stochastic Poisson intensity. Guthrie (2019) assumes the decision-maker learns about the underlying climate scenario by observing the frequency of extreme weather events, which injects volatility into the investment payoff. Most recently, Lee and Zhao (2021) combine a Brownian motion process with extreme events, represented by Poisson jumps with a hyper-exponential jump size distribution.====The paper’s third contribution is to show that most investment benefits can be captured using a simple alternative to “full” real options analysis. This is important because many practitioners regard real options analysis as complex and resource-intensive (Dawson, Hunt, Shaw, Gehrels, 2018, Wreford, Dittrich, van der Pol, 2020). It is not widely used, particularly for relatively small projects. This is serious because much adaptation spending will fund relatively small projects under the jurisdiction of authorities with limited analytical resources. The importance of the flexibility in these projects is too costly to ignore. Decision-makers need approaches that are simple enough to be useful for evaluating small- and medium-scale adaptation decisions, yet retain a degree of economic rigour, as recommended in recent reviews of decision-support tools for adaptation assessment (Watkiss, Hunt, Blyth, Dyszynski, 2015, Wreford, Dittrich, van der Pol, 2020). This paper provides one such approach.====Optimal investment involves comparing the present value of the benefits and the total cost, where the latter equals the sum of the investment expenditure and the present value of the real option to wait. The alternative rule involves replacing the “fully optimal” value of the delay option with its value assuming investment is delayed until the best ==== future date. This is simpler to calculate than the full option value because all it requires is one standard cost–benefit calculation for each possible future investment date. The approximate option value is the maximum of these values. A decision-maker using this alternative rule invests if and only if the present value of the project’s benefits is greater than or equal to the sum of the investment expenditure and the approximate option value. Equivalently, she invests if no fixed future investment date implies a greater net present value than investing immediately.====Under the alternative policy, investment is delayed significantly past the date when the present value of the benefits from investment equals the required expenditure. Nevertheless, it still occurs earlier than under the welfare-maximising investment policy. For the next few decades at least, the acceleration in investment is moderate. The calculation underlying the alternative investment policy assumes the decision-maker delays the investment, but not the decision when to invest—that is made now. This only introduces substantial errors if there is a significant risk of new information arriving that would cause the decision-maker to regret the investment decision that is implicit in the approximate option value. The prospect of such bad news is low, at least for the foreseeable future. Due to the economic factor’s relatively low volatility and—for the foreseeable future—the low volatility of the decision-maker’s beliefs regarding the magnitude of climate change, there is little chance that any subsequent information will make the net present value of investing negative. That is, if the decision-maker uses the alternative rule, she delays investment so long that there is little chance she will subsequently regret investing. It is not surprising that the added value from delaying the ==== is so small.====For at least the first 40 years, the welfare losses from adopting the alternative rule are small. This rule performs worse if the economic factor has a lower expected growth rate or higher volatility. These situations increase the likelihood that past investments will subsequently be stranded, which increases the value of delaying the investment ====. If there is more noise in the climatic signal, and thus less volatility in the decision-maker’s beliefs, the probability of post-investment stranding is smaller and the alternative rule’s performance improves. The effect is insignificant in the short term, when the climate scenarios are similar. However, for longer horizons, there is a noticeable improvement in the rule’s performance when the climatic signal is noisier. Although the rule performs reasonably well over a wide range of parameter values, there will likely still be situations where it results in a significant reduction in welfare. In particular, if the economic factor is volatile enough and grows slowly enough, and if there is enough noise in the climatic signal, then there is still a place for “full” real options analysis.====This paper’s final contribution is therefore to the subset of the real options literature that tries to link the technique to real-world decision-making. Some papers in this literature attempt to derive forms of real options analysis that are simple enough to be useful in practice. For example, Copeland, Antikarov, 2003, Copeland, Antikarov, 2005 have developed an approach that is popular among some corporate-finance practitioners of real options analysis. The remaining papers in this literature attempt to interpret pragmatic decision-making techniques as tractable approaches that capture the value of the real options embedded in projects. For example, McDonald (2000) and Malchow-Møller and Thorsen (2005) evaluate various rules of thumb from a real options perspective, whereas Boyle and Guthrie (2006) and Wambach (2000) concentrate on the payback method.====The rest of the paper is organised as follows. Section 2 presents the model set-up and Section 3 develops the underlying theory. This has two components: a description of how beliefs about climate change evolve over time, in Section 3.1, and a description of how investment policies are evaluated, in Section 3.2. The paper continues in Section 4, which investigates these optimal policies, and Section 5, which compares the performance of these policies with a simpler alternative. Finally, Section 6 offers some concluding remarks.",Optimal adaptation to uncertain climate change,https://www.sciencedirect.com/science/article/pii/S0165188923000271,24 February 2023,2023,Research Article,27.0
"Fontanelli Luca,Guerini Mattia,Napoletano Mauro","Université Côte d’Azur, CNRS, GREDEG, France,University of Brescia, Italy,Institute of Economics, Scuola Superiore Sant’Anna, Italy,Sciences Po OFCE, France,Fondazione ENI Enrico Mattei (FEEM), Italy","Received 7 August 2022, Revised 30 December 2022, Accepted 17 February 2023, Available online 21 February 2023, Version of Record 6 March 2023.",https://doi.org/10.1016/j.jedc.2023.104619,Cited by (0)," dynamics. The model features two countries populated by firms heterogeneous in their productivity and size. Market selection is driven by a ==== and the tightness of the market selection process allow us to draw interesting policy insights on concentration, volatility and the dynamics of international leadership in both export shares and aggregate productivity.","This work proposes a new model of intra-industry trade wherein imperfect market selection and idiosyncratic firm learning are the key ingredients for the joint explanation of firm, industry and export flow dynamics.====Since the seminal contribution of Melitz (2003), fixed entry cost have been the most important mechansisms in explaining export patterns. Our approach is complementary to such view and builds upon a recent stream of literature that attempts to explain international trade patterns by means of simple stochastic processes (e.g. the “balls and bins” model proposed in Armenter and Koren, 2014). While acknowledging that the presence of entry barriers is an empirically relevant force that shapes trade patterns (see also Guerini et al., 2021), we here focus on two other mechanisms that have been highlighted by the empirical literature, but less so by the theoretical one. The first one is the presence of imperfection in the market selection process. Several works in the industrial dynamics literature unveiled the presence of wide and persistent productivity differentials among firms (see e.g. Bartelsman, Dhrymes, 1998, Doms, Bartelsman, 2000), which are significant also when controlling for the firms export status. In addition, notwithstanding the existence of an export productivity premium (Bernard, Jensen, 1995, Bernard, Jensen, 1999, Bernard, Jensen, Redding, Schott, 2007), the productivity distributions of exporters and not-exporters partially overlap (Bernard, Eaton, Jensen, Kortum, 2003, Grazzi, Mathew, Moschella, 2021, Impullitti, Irarrazabal, Opromolla, 2013, Mayer, Ottaviano, 2008, Melitz, Trefler, 2012). This implies that high-productivity non-exporters co-exist with low-productivity exporters, and that market selection in foreign markets is imperfect.==== The second mechanism is idiosyncratic firm learning. Several empirical findings suggest that aggregate productivity growth is only residually explained by the reallocation of market shares to best firms, but it is affected mostly by idiosyncratic firm learning (see e.g., Bottazzi, Dosi, Jacoby, Secchi, Tamagni, 2010, Dosi, Moschella, Pugliese, Tamagni, 2015, Foster, Haltiwanger, Krizan, 2001, among the others). Furthermore, while the level of productivity contributes to determine firm performance in international markets (see e.g. Bernard, Jensen, 1999, Melitz, 2003), also innovation activities and investments in R&D by firms are positively linked to export (Dosi, Grazzi, Moschella, 2015, Grazzi, Mathew, Moschella, 2021). Therefore, also the idiosyncratic learning by firms is crucial for explaining firm-level export patterns. Taking stock of this evidence, in this paper we study the joint effects that these two forces exert on firm and industry dynamics as well as on trade patterns.====We contribute to the literature by building a model of intra-industry trade characterized by the presence of dynamic increasing returns to market selection. In line with traditional intra-industry trade models (see Krugman, 1980, Melitz, 2003), our model features two countries that are initially symmetric. Moreover, market selection of incumbent firms in each country is driven by a two-steps ====.==== In the first step, pairs of firms are randomly drawn with a probability proportional to their size. In the second step, the selected pair of firms compete for customer demand on the basis of their productivity levels. The customer demand is then reallocated within the pair from the least productive firm to the most productive one. The micro-foundation of firm selection according to the above pairwise sampling rule captures the presence of consumers’ imperfect knowledge in the product markets, similarly to the well-established models of imperfect competition with random encounters among customers (see e.g. Bils, 1989, Greenwald, Stiglitz, 2005, Phelps, Winter, 1970, Rotemberg, Woodford, 1991). Furthermore, the assumption that the probability of competing for a customer is an increasing function of market size captures the presence of dynamic increasing returns to market selection (see Arthur, 1989, Dosi, Kaniovski, 1994, Dosi, Moneta, Stepanova, 2019), and proxies the fact that larger firms have better distribution channels and are more likely to be noticed by customers. At the same time, they are also more exposed to competition by other firms. Notice that we label the above returns as “dynamic” to emphasize the fact that they are an inherent feature of the competitive process of market share reallocation across firms. They do not arise instead from the presence of fixed costs as in other trade models (e.g. Melitz, 2003). Finally, competition over the productivity domain captures the fact that relatively more productive firms are able to charge a lower price and to attract a higher number of consumers, thereby increasing their market shares (Dosi, Marsili, Orsenigo, Salvatore, 1995, Dosi, Pereira, Virgillito, 2017, Melitz, 2003).====The first applications of Pólya urns in economics date back to the seminal works by Herbert Simon on the firm size distribution (Ijiri, Simon, 1975, Ijiri, Simon, 1977, Simon, 1955). Since then, Pólya urn processes have been employed in several domains of economics, and in particular in the analysis of technical change (Arthur, Ermoliev, Kaniovski, 1987, Arthur, 1989, Dosi, Ermoliev, Kaniovski, 1994, Dosi, Moneta, Stepanova, 2019) and industry dynamics (Bottazzi, Dosi, Fagiolo, Secchi, 2007, Bottazzi, Secchi, 2006, Fu, Pammolli, Buldyrev, Riccaboni, Matia, Yamasaki, Stanley, 2005, Riccaboni et al., 2008).==== However, to the best of our knowledge, so far there is no application of Pólya urns to the analysis of international trade dynamics.====In the above mentioned framework, when the distribution of firm productivity levels is static, the market selection asymptotically generates either a national or an international monopoly (depending on the presence of iceberg costs). This is a result recalling the Fisher’s fundamental theorem of natural selection (see Fisher, 1930). However, when idiosyncratic learning and when entry and exit of firms take place, results are more interesting. By means of extensive Monte Carlo simulations, we show that the above extended version of the model is able to ==== reproduce the most important stylised facts related to international trade and industry dynamics. We also use this extended version of the model to carry out comparative dynamics exercises, which lead us to draw some interesting policy insights.====The increasing importance of international competition, witnessed by the fall of trade barriers over the last decades, has indeed increased the awareness about the significant impact of trade flows on industry and firm dynamics. On the one hand, recent studies have highlighted that market concentration (di Giovanni et al., 2011) and volatility (Ćede, Chiriacescu, Harasztosi, Lalinsky, Meriküll, 2018, di Giovanni, Levchenko, 2009, Vannoorenberghe, 2012) may be positively affected by trade openness. Furthermore, the co-existence in real-world economies of positive trends in industry concentration (witnessed in several Western countries, see Bajgar, Berlingieri, Calligaris, Criscuolo, Timmis, 2019, Bajgar, Criscuolo, Timmis, 2021) and of decreasing trade barriers and stricter competition policies in the past decades constitutes a policy puzzle which is at odds with the traditional wisdom stating that increases in competitive pressure lead to lower concentration.==== In particular, we show that trade openness and the intensity of the market selection positively affect firm and industry volatility and different measures of market concentration (domestic and export concentration) by means of a winner-take-all type of dynamics. On the other hand, the international leadership of a country in a specific sector may suffer from episodes of catching up by new players (see Abramovitz, 1986, Lee, Malerba, 2017). We show that the model generates persistent country-level asymmetries in export volume and aggregate productivity, as well as catching up dynamics, whose realizations are affected by both trade openness and the intensity of market selection. Higher trade openness has a twofold effect. On the one side, it eases the technological catching-up of laggard economies. On the other side, higher trade openness also generates higher industry concentration. This limits the ability of laggards to catch-up, making international leadership less contestable. More intense market selection, instead, implies more catch-up processes both in terms of productivity and export shares.====Overall, the above results indicate that our intra-industry trade model provides a fairly accurate description of how the interplay between cumulative learning and market selection shape the most interesting stylized facts concerning international trade, industry dynamics and firm dynamics. Moreover, our model also generates further predictions about the effects of trade openness and of market selection that are worth to investigate in future empirical research.====The remainder of the paper is organized as follows. In Section 2 we provide a review of the empirical and theoretical literature in trade and industry dynamics related to our work. In Section 3 we describe the model, together with some asymptotic properties of the ==== under a static productivity distribution assumption. Section 4 presents the model results using extensive Monte Carlo simulations, and discusses their economic implications. Section 5 concludes.",International trade and technological competition in markets with dynamic increasing returns,https://www.sciencedirect.com/science/article/pii/S0165188923000258,21 February 2023,2023,Research Article,28.0
"Chiah Mardy,Long Huaigang,Zaremba Adam,Umar Zaghum","Newcastle Business School, The University of Newcastle, Newcastle, New South Wales, Australia,School of Finance, Zhejiang University of Finance and Economics, 18 Xueyuan Street, Hangzhou, Zhejiang 310018, China,The New Type Key Think Tank of Zhejiang Province “China Research Institute of Regulation and Public Policy”, Zhejiang University of Finance and Economics, Hangzhou City, Zhejiang Prov 310018, China,Montpellier Business School, 2300 Avenue des Moulins, 34185, Montpellier CEDEX 4, France,Department of Investment and Financial Markets, Institute of Finance, Poznan University of Economics and Business, al. Niepodległości 10, Poznań 61-875, Poland,College of Business, Zayed University, P.O. Box 144534, Abu Dhabi, United Arab Emirates,South Ural State University, Lenin Prospect 76, Chelyabinsk, 454080, Russian Federation","Received 15 March 2022, Revised 20 August 2022, Accepted 13 February 2023, Available online 15 February 2023, Version of Record 26 February 2023.",https://doi.org/10.1016/j.jedc.2023.104618,Cited by (0),"Using the change in the real effective exchange rate (REER) to reflect trade competitiveness, we examine its role in the cross-section of global ====. The changes in REER negatively affect stock market returns. The REER effect is robust after controlling for known risk factors and market characteristics. Furthermore, it remains pervasive across different periods and subsamples. Our findings support the conventional wisdom that appreciating currency harms trade values, consequently dampening a firm's stock market performance.","Trade is an important aspect driving the economic growth of a country. Countries have benefited from faster growth, higher living standards, and new economic opportunities with decades of globalization and increasing trade. Conventional wisdom suggests that depreciating the value of a currency helps a country to be more trade competitive globally because the prices of goods and services become relatively cheaper. The higher export values, henceforth, support economic growth and company profitability. This relation is documented not only aggregately in country-level analysis (Chou, 2000; Marquez & Schindler, 2007; Chit et al., 2010; Nouira et al., 2011; Zhang & Ouyang, 2018) but also at the firm level (Cheung & Sengupta, 2013; Li et al., 2015). The downside of this practice is that imports become more expensive. However, this consequence also boosts economic growth by encouraging consumers to seek domestic alternatives. As such, a lower relative national currency is likely to improve future expected cash flows of companies through two channels: higher export values or higher domestic consumption. Abundant evidence shows that exchange rates play an important role in export and import dynamics.==== However, underexplored research is its effect on equity markets, given that public listed companies are now competing globally.==== Specifically, does a country's currency-induced trade competitiveness affect the cross-section of international equity returns?====With these backdrops, this study evaluates the implications of a country's currency-implied trade competitiveness on its equity returns. To achieve this aim, we use 57 country indices spanning from 1969 to 2021. In particular, we examine whether the cross-sectional differences in the real effective exchange rate (====) are priced as a global equity premium. ==== is defined as the weighted average of a country's currency in relation to an index or basket of other major currencies. The weights in computing ==== are established based on the relative trade balance of a country's currency against that of each country in the index. As such, ==== arguably reflects a country's trade competitiveness against its major trade partners well. Countries with an increase in ==== are likely to be less competitive in the global trade market, especially against their major trade partners and vice versa. Based on this channel, we hypothesize a negative relation between change in ==== (====) and subsequent stock returns in the global equity markets due to the close link between the strength of a currency and trade competitiveness. This will thus affect the aggregate future profitability of companies listed on the index. In an efficient market, the implications of ==== on stock returns should be understood immediately and unbiasedly by investors. Therefore, examining the lead-lag relation between ==== and subsequent stock returns provides fruitful insights into market efficiency in a global context.====Our main findings are as follows. Forming quintile country portfolios based on ====, we find a negative relation between ==== and subsequent stock return. To be specific, an equal-weighted hedge portfolio taking a long position in the high ==== portfolio and a short position in the low ==== countries generates a negative average return. Adjusting for risks only has a trivial influence on the abnormal returns. We use a repertoire of asset pricing models to evaluate the abnormal returns of the hedge portfolio, and the corresponding monthly alphas range from -0.62% to -0.52%. We also consider value-weighted portfolio returns, and the inferences remain unchanged: the hedge portfolio consistently produces a negative monthly return. This suggests that our findings are not driven by extremely small nations in the case of equal-weighting or large countries when value-weighting.====The ==== effect may be a manifestation of other known country-level equity premiums. Therefore, to address this concern, we perform a multivariate analysis accounting for a battery of alternative return predictors. We conduct sequential double sorts and Fama-MacBeth regressions in this exercise. We control for the effects of size, value, short-term reversal, intermediate momentum, long-term contrarian, idiosyncratic volatility, beta, skewness, seasonalities, and sovereign credit risk. The double-sorts analysis confirms that the high-minus-low ==== portfolio persistently generates negative and statistically significant monthly excess and abnormal returns. This finding is also not specific to the weighting schemes (equal- or value-weighted). Unlike the double-sorting procedure, which only accounts for one additional variable at a time, Fama-MacBeth regressions can control for multiple variables jointly. The negative relation between ==== and subsequent stock returns remains strong in this setting. The slopes on ==== in the regressions are consistently negative and significant, even after simultaneously controlling for all the other predictors of stock returns used in this study. We conclude that ==== contains novel and independent incremental information about the future cross-sectional returns.====After documenting a robustly negative relation between ==== and stock index returns, we proceed with further analyses to better understand the dynamics of this phenomenon. We begin by exploring different subperiods and market conditions. Past studies document that equity returns may be asymmetrically affected by significant events in the market events or business cycles (Antoniou et al., 2007; Maio & Santa-Clara, 2017; Chan et al., 2020). A simple partition of the period into two even halves, or the 20th and 21st centuries, does not alter the negative relation between ==== and stock returns. The associated hedge portfolio continues to generate significant abnormal returns. The pervasiveness of this effect, even during more recent periods, further suggests that it remains unexploited and does not vanish, unlike many other anomalies (McLean & Pontiff, 2016).====In addition, we evaluate a battery of alternative methodological choices to mitigate the concern of data mining. For instance, Hou et al. (2020) suggest that many stock market anomalies fail to hold up to currently acceptable standards for empirical finance. Whilst our overall approach is conventional, we proceed to examine whether our main findings are caused by a specific methodological choice. These alternatives include additional portfolio weighting schemes, the use of quartile and sextile portfolios in place of quintiles, the exclusion of the largest or smallest countries, and modifications to the estimation period for ====. The negative ====-return relation remains statistically strong across all these settings. These analyses lend further support that this anomaly is genuinely present in our large country samples. In relation to cross-sectional regressions, we also reestimate them with panel regressions advocated by Petersen (2009) and Thompson (2011). The negative coefficients of the ==== effect continue to be prevalent. Given that our returns are denominated in U.S. dollars, the main findings take the view of a U.S. investor who converts the global investment returns back to U.S. dollars. We further validate that the negative association between ==== and subsequent stock returns remains strong when using local currency-denominated returns.====Equity anomalies are documented to be influenced by various market conditions. For example, a good run in the equity markets reflects optimism, hence, a higher likelihood of overpricing.==== To evaluate the impact of different market states on the ==== effect, we separate the time-series return of the hedge portfolio into high and low subperiods based on a large panel of controls for market conditions. The negative raw and abnormal returns remain prevalent, thus demonstrating that the ==== effect is not sample or time-specific. It also shows that unlike stock market anomalies or a subset of stocks affected by mood or investor sentiment (Birru, 2018), the ==== effect is not sensitive to significant market fluctuations due to macroeconomic shifts. Therefore, the finding is consistent with our conjecture that changes in ==== affect the future profitability of a country due to its importance in global trade competitiveness.====Lastly, we examine whether cross-country heterogeneity affects the profitability of the ==== hedge portfolio. For example, past studies show that predictable stock return patterns may be more prevalent among the subsamples that mispricing is more difficult to exploit (Shleifer & Vishny, 1997; Brav et al., 2010; Chu et al., 2020). We partition our country sample into high and low subgroups based on several country-specific characteristics and examine the monthly stock returns of the ==== hedge portfolio in these subgroups. These characteristics can be grouped into three broad categories, namely, shareholder protection and accounting quality, law and development, and cultural traits. The ==== effect remains statistically and economically significant in these cross-sectional subgroups. This result demonstrates that the effect of currency-driven trade competitiveness on asset prices cannot be subsumed by other characteristics with different underlying mechanisms, such as limits to arbitrage associated with small non-tradeable markets. Interestingly, we find that the ==== effect is stronger among countries with low masculinity, individualism, and indulgence cultural traits. These cultural dimensions are found to be associated with innovation and hence bilateral trade competitiveness (Prim et al., 2017; Espig et al., 2021). Consequently, our findings indicate that countries with these cultural traits are less trade competitive due to lower innovation. Further reduction in trade advantage via the exchange rate channel adversely affects the stock markets of these countries to a larger degree.====Our study contributes to the literature in the following manner. First, we document a return predicting variable—in the form of currency-driven trade competitiveness—that helps to explain the cross-section of country equity risk premia. Hence, we add to the mounting evidence on the predictability of the cross-section of country index returns (e.g., Bali & Cakici, 2010; Balvers et al., 2000; Avramov et al., 2012; Asness et al., 2013; Frazzini & Pedersen, 2014; Pitkäjärvi et al., 2020; Zaremba et al., 2020; Baltussen et al., 2021). Second, our article also relates to the voluminous literature on the interplay between trade competitiveness, foreign exchange, and stock markets (e.g., Jorion, 1990, 1991; Bartov & Bodnar, 1994; Griffin & Stulz, 2001; Phylaktis & Ravazzolo, 2005; Bartram & Bodnar, 2012; Koijen & Yogo, 2020; Karolyi & Wu, 2021a, 2021b). For academics, it enhances our understanding of the drivers of stock returns at the global level. Our findings also have important implications for practitioners. Given that we rely on liquid broad country-level indices in our analysis, the portfolios are tradeable, unlike stock market anomalies documented at the firm level (especially in the subsection of small stocks). For policymakers, our study helps to understand the implications of exchange rates on the aggregate profitability of firms and their subsequent stock returns. Our findings reflect the global trade dynamics. For instance, the Australian stock market is dominated by the mining industry that is heavily reliant on exports such as iron ore.==== Many commodities, including iron ore, are traded in US dollars in world markets; therefore, the exchange rate movement of AUD/USD is crucial in driving the Australian dollar earnings of these companies (Allday, 2022).==== Our findings explicitly demonstrate the connections among exchange rate movements, trade competitiveness, and subsequent stock returns.====This paper proceeds as follows. Section 2 outlines the data employed, key variable definitions, and empirical designs. Section 3 presents the main findings. Section 4 explores additional analyses that complement our main findings. Lastly, Section 5 concludes the paper.",Trade competitiveness and the aggregate returns in global stock markets,https://www.sciencedirect.com/science/article/pii/S0165188923000246,15 February 2023,2023,Research Article,29.0
"de Castro Luciano,Galvao Antonio F.,Muchon Andre","Department of Economics, University of Iowa, United States,Department of Economics, Michigan State University, United States,Möbius Capital and IMPA, United States","Received 15 September 2022, Revised 12 December 2022, Accepted 9 February 2023, Available online 11 February 2023, Version of Record 24 February 2023.",https://doi.org/10.1016/j.jedc.2023.104617,Cited by (0),"-quantile utilities, for ","The dynamic programming framework has been extensively used in economic modeling because it is sufficiently rich to deal with most problems involving sequential decisions over time and under uncertainty.==== Recently, de Castro and Galvao (2019) proposed a new infinite-horizon dynamic model for an economic agent, who, when selecting among uncertain alternatives, chooses the one with the highest ====-quantile of the stream of future utilities for a fixed ====, instead of the standard expected utility. This quantile preference model is tractable, simple to interpret, and allows for separation between risk aversion and elasticity of intertemporal substitution. Moreover, some quantile dynamic models have closed form solutions which are not available for their expectation counterpart.==== The quantile model also possesses all the desirable standard properties of recursive models. In particular, it is dynamically consistent, and the principle of optimality holds. Moreover, the corresponding dynamic problem yields a value function, via a fixed point argument, the value function is monotone, concave, and differentiable, and it is also possible to derive the corresponding Euler equation. There is a growing literature on economic models using quantile preferences, see, e.g., among others, Manski (1988), Chambers, 2007, Chambers, 2009, Bhattacharya (2009), Rostek (2010), Giovannetti (2013), Long et al. (2021), Baruník and Čech (2021), He et al. (2021), de Castro et al. (2022a), and Baruník and Nevrla (2022).====Solving dynamic stochastic optimization problems numerically is very important in practice. There is a large literature discussing numerical solutions for dynamic economic models under uncertainty. For reviews of methods for numerically solving intertemporal economic models, see, among others, Taylor and Uhlig (1990), Rust, 1996, Rust, 2008, Gaspar and Judd (1997), Judd (1998), Marimon and Scott (1999), Santos (1999), Christiano and Fisher (2000), Miranda and Fackler (2002), Adda and Cooper (2003), Aruoba et al. (2006), Stachursky (2009), Den Haan (2010), Kollmann et al. (2011), Ljungqvist and Sargent (2012), Miao (2013), and Maliar and Maliar (2014). Most of the available existing methods work with the expected utility case. Nevertheless, quantiles are nonlinear operators, which precludes the use of some simplifications that are only valid for expectations. Recently, numerical computation using recursive preferences together with non-linearity has received attention – see, e.g., Caldara et al. (2012) and van Binsbergen et al. (2012) for applications of Epstein, Zin, 1989, Epstein, Zin, 1991 preferences to DSGE models. Solving these models numerically may be a challenge.====This paper extends the literature in two fronts. First, we extend existing theoretical results on the intertemporal quantile model in an important dimension that is useful to applied work. We allow the model to have a finite-horizon instead of being restricted to infinite-horizons. We show that, in the finite case, the sequence of corresponding value-functions is well-defined. Finite-horizon problems are often encountered in making life-cycle planning decisions on optimal consumption, savings, portfolio choice, etc. This is a significant extension since it allows for applications of many more realistic models.====Second, we suggest numerical methods for solving the dynamic stochastic programming for quantile recursive models based on value function iterations. The numerical solution provides a link between the theory for quantile dynamic programming and empirical analyses of dynamic optimization problems. The need for numerical tools arises from the fact that, in general, dynamic programming problems do not possess tractable closed form solutions. Hence, numerical techniques must be used to approximate their solutions. In particular, we suggest a procedure based on value function iterations where the main difference with standard methods is that one is required to compute the conditional ====-quantile of the value function, instead of the conditional average. We note that, for the standard conditional expectation case, given a particular state and conditional probabilities in the transition matrix, the conditional average is easily obtained by calculating the sample average over the states using the corresponding probabilities. For the quantile model, the calculation of the conditional ====-quantile is also simple. Given the conditional probabilities for the current shock, the state, and the choice, one orders all the possible future value function outcomes, and sums the corresponding probabilities from the lowest value to the highest, until the sum of probabilities is at least the given ====.====Numerical methods are provided for both the finite and infinity horizon cases. Both algorithms use the Bellman equation to compute the value function. For the former case, backward iterations on an initial guess are employed. In the latter case, the value function is iterated until convergence occurs. In this paper, we focus on the case that the shock is discrete with a Markov transition matrix. Nevertheless, we provide a brief discussion on an extension of the algorithm to the continuous shock case using the Tauchen (1986) finite state Markov-chain approximation.====We provide results assessing the accuracy of the proposed numerical methods and illustrating its practicality. To do so, we use a simple intertemporal consumption model where the economic agent is characterized by quantile preferences and decides on the intertemporal consumption and savings (assets to hold) over time, subject to a linear budget constraint. This model is characterized by three parameters — discount factor, elasticity of intertemporal substitution (EIS), and risk attitude. Notice that in quantile models, the risk attitude is captured by the quantile ====, while the other two parameters play a similar role to those in dynamic expected utility models. However, in the standard expected utility, the EIS is indissociable from risk aversion.==== de Castro et al. (2022b) derive explicit algebraic closed form solutions for the value and policy functions for this intertemporal consumption model.==== Hence, we are able to calculate these functions both theoretically and numerically, for different value of the parameters. We are then able to evaluate the performance of the numerical procedures by comparing the results for the numerical computation with the corresponding closed form solutions.====To compute the intertemporal consumption quantile model, numerically and theoretically, we specify an isoelastic utility function and calculate the value and asset allocation policy functions for several combinations of the parameters, as well as different probabilities in the transition matrices, using independent and identically distributed (iid) and Markov shocks. Results from these exercises show evidence that the suggested numerical methods to solve the dynamic quantile model approximate the theoretical solutions very closely, and provide a high degree of accuracy. Hence, researchers applying this solution algorithm can be confident that their quantitative answers are sound.====To illustrate the usefulness and practicality of the proposed methods, we compare numerical results for the quantile model with the standard expected utility case. We use different parametrizations that change probabilities of the shock, the risk aversion, and the EIS. For each design, we compute the asset allocation decision rule and the value function. These exercises illustrate several interesting features of the dynamic quantile model. First, we verify that both value and policy functions are monotone increasing. Second, results illustrate the flexibility and practicality of the quantile framework. The quantile model is characterized by three parameters, while the standard expected utility is only characterized by two parameters. Thus, in the quantile case, for a given discount factor, asset allocation varies with both risk attitude and EIS. When ====, asset allocation is relatively smaller for a more risk averse agent (====) compared to a less risk averse (====). On the other hand, for ====, the volume of assets bought for the next period (savings) is relatively larger for a more risk averse agent (====) compared to a less risk averse (====). As EIS decreases, the responsiveness of the growth rate of consumption to the real interest rate decreases. If consumption is less responsive to changes, savings is more responsive. Thus, the asset allocation increases for all quantiles, but it increases relatively more for a more risk averse agent. This illustrates the advantage of being able to separate risk and EIS, and hence evaluate effects of changing EIS on next period asset allocation decisions, while maintaining risk attitudes constant.====Finally, we simulate the intertemporal consumption model. Applied economists often characterize the behavior of the model through statistics from simulated paths of the economy. We simulate the model for 10,000 periods and compute density functions of the allocation of asset savings for the next period for different combinations of parameters. Results show large heterogeneity in the distributions across different risk attitudes and EIS.====The remainder of the paper is structured as follows. Section 2 reviews the dynamic quantile model. Section 3 presents the general infinite-horizon model, and its numerical algorithm for computation. Section 4 studies the finite-horizon quantile model, and its corresponding numerical algorithm for computation. Section 5 provides numerical results using the intertemporal consumption model as an example. We assess the numerical performance of the computational algorithms, compare results with the standard expected utility model, and provide simulation results. Finally, Section 6 concludes.",Numerical Solution of Dynamic Quantile Models,https://www.sciencedirect.com/science/article/pii/S0165188923000234,11 February 2023,2023,Research Article,30.0
Fu Bowen,"Center for Economics, Finance, and Management Studies, Hunan University, Changsha, China","Received 9 April 2022, Revised 17 January 2023, Accepted 17 January 2023, Available online 20 January 2023, Version of Record 27 January 2023.",https://doi.org/10.1016/j.jedc.2023.104606,Cited by (1),"The trend real interest rate is important for ==== decision making and understanding the secular decline in ====. Many papers have estimated it. However, the uncertainty surrounding these estimates is substantial. Using the US data, we construct a new measure of the trend real interest rate in a data-rich environment using a large time-varying local mean ","The trend real interest rate can be considered as an empirical proxy for the low-frequency component of the equilibrium real interest rate in DSGE models (see, e.g., Del Negro et al., 2017). Thus, the trend real interest rate plays an important role in monetary policy decision making and understanding the secular decline in interest rates. Given the importance of the trend real interest rate, many papers have estimated it. Some papers use a multivariate unobserved components model embedded with the Phillips curve and the investment-saving (IS) curve to estimate it (see, e.g., Sébastien, Jean-Stéphane, Sarah, Jean-Paul, 2018, Holston, Laubach, Williams, 2017, Kiley, 2020, Laubach, Williams, 2003, Lewis, Vazquez-Grande, 2019). Another strand of the literature uses a variety of flexible time series models to estimate the trend real interest rate (see, e.g., Del Negro, Giannone, Giannoni, Tambalotti, 2017, Del Negro, Giannone, Giannoni, Tambalotti, 2019, Hamilton, Harris, Hatzius, West, 2016, Johannsen, Mertens, 2021, Lubik, Matthes, 2015, Morley, Tran, Wong, 2022).====Despite the wide variety of approaches, these estimates of the trend real interest rate tend to have large error bands. For example, Beyer and Wieland (2019) shows that after the Great Recession, the 95% confidence interval of Laubach and Williams’s (2003) estimates ranges from ==== to ====.==== Large uncertainty around the trend real interest rate more likely leads to policymakers misperceptions of the level of the equilibrium real interest rate. Ajello et al. (2021) show that these misperceptions can carry substantial economic costs in terms of unemployment and inflation.====We contribute to the literature of estimating the trend real interest rate by constructing a more precise measure of the trend real interest rate in a data-rich environment. First, we construct the real interest rate as the nominal effective federal funds rate minus the backward PCE inflation expectation, which is proxied by the four-quarter moving average of core personal consumption expenditures (PCE) inflation. Then, we obtain the trend real interest by estimating a large time-varying local mean Bayesian VAR with 126 variables, where the posterior median of the time-varying local mean of the real interest rate is our proposed measure. The width of 95% credible intervals of our proposed estimates varies from 0.83% to 3.35%. Also, the average the width is 1.43%.====In a multivariate trend and cycle decomposition model, linking the gap of variable of interest to gaps of relevant variables can help to reduce the uncertainty of the trend of variable of interest. For example, Basistha and Startz (2008) reduce the uncertainty around the natural unemployment rate by half via linking the unemployment gap to output gap. Thus, we develop a large time-varying local mean Bayesian VAR with common stochastic volatility which can allow for linking the real interest rate gap to other gaps in all the equations due to a full matrix structure.==== In our proposed model, a large amount of total variation of the real interest can be explained by potentially related gaps. This leads to small variance of error term in the measurement equation. The variance of error term in the measurement equation is a component of the posterior variance of the trend real interest rate. This small variance leads to low uncertainty around the trend real interest rate.====Using more data may not guarantee narrower credible intervals. Thus, we also check validity for these narrower credible intervals from the large-scale model proposed in this paper via evaluation of real-time revisions of our proposed estimates. We find that the credible intervals of the real-time revisions of our proposed estimates are also narrower than those of the real-time revisions from the small-scale and medium-scale model.====From 1982Q2 to 2010Q1, our proposed estimates of the trend real interest rate declined to near zero and became negative after 2010Q1. Our proposed estimates provide more solid evidence for a substantial decline in the equilibrium real interest rate over last two decades and are consistent with the secular stagnation hypothesis (see, e.g., Summers et al., 2016). A near-zero or even negative equilibrium real interest rate implies that the usefulness of conventional monetary policy tools could be limited and that unconventional monetary policy tools such as central bank balance sheets and forward guidance might is necessary.====This paper is closely related to the strand of literature that uses a variety of time series models to estimate the trend real interest rate. In this strand, Lubik and Matthes (2015) estimate the trend real interest rate by using a time-varying parameter VAR. Hamilton et al. (2016) measure the trend real interest rate as the 10-year moving average of the real interest rates. Del Negro, Giannone, Giannoni, Tambalotti, 2017, Del Negro, Giannone, Giannoni, Tambalotti, 2019 estimate the trend real interest rate, using a VAR with common trends. Johannsen and Mertens (2021) use a similar approach to that of Del Negro, Giannone, Giannoni, Tambalotti, 2017, Del Negro, Giannone, Giannoni, Tambalotti, 2019, modeling the effective lower bound on nominal interest rates. Morley et al. (2022) estimate the trend real interest rate by a multivariate two-stage Beveridge and Nelson decomposition approach based on Morley and Wong (2020). As the BN trend estimates from data with and without measurement error are cointegrated (not necessarily identical) with assumption of stationary measurement error, the BN approach of Morley et al. (2022) is robust to misspecification due to measurement error. The BN approach of Morley et al. (2022) also allows for considering a large set of variables that have been hypothesized to explain changes in the trend real interest rate. In particular, the trend interest rate of this paper is close in spirit and concept to the estimates of Del Negro, Giannone, Giannoni, Tambalotti, 2017, Del Negro, Giannone, Giannoni, Tambalotti, 2019, Johannsen and Mertens (2021), and Morley et al. (2022).====The remainder of this paper is organized as the follows. Section 2 introduces the econometric approach used in this paper. Specifically, we describe our large time-varying local mean Bayesian VAR, provide the priors for our estimation, and outline the sampling scheme for our estimation. Section 3 provides a data description. Section 4 presents our results. Section 5 is the conclusion.",Measuring the trend real interest rate in a data-rich environment,https://www.sciencedirect.com/science/article/pii/S016518892300012X,20 January 2023,2023,Research Article,31.0
Guo Mng,"ShanghaiTech University, Shanghai 201210, China","Received 28 August 2022, Revised 11 January 2023, Accepted 14 January 2023, Available online 18 January 2023, Version of Record 7 March 2023.",https://doi.org/10.1016/j.jedc.2023.104604,Cited by (0),The way informed traders use their privileged information affects the properties of asset prices. This paper analyzes the differences in ,"Trading by informed traders allows for the incorporation of a firm’s fundamental information into stock prices. In the real world, informed traders may use their privileged information in different ways. The competitive trading paradigm originated from Grossman and Stiglitz (1980) and the strategic trading paradigm originated from Kyle (1985) are perhaps the two most widely adopted paradigms regarding the use of information by informed traders. These two paradigms are used by the academia and practitioners to address different issues. However, it is unclear how different they are in terms of market efficiency (perhaps the most important property of speculative markets) and what mechanisms explain the differences. There are two main reasons for this. First, there are different measures of market efficiency. Return autocorrelations and price informational efficiency (price informativeness) measure different aspects of market efficiency. Return autocorrelations measure the predictability of future prices based on historical and current prices, while price informativeness measures the equilibrium level of private information incorporated into security prices (reflecting the degree of information asymmetry in equilibrium). Second, the basic assumptions underlying these two paradigms are different, which hinder their fair comparison. While investors are risk averse under the competitive trading paradigm (Goldstein, Yang, 2015, Grossman, Stiglitz, 1980, Wang, 1993), a typical assumption of the strategic-trading paradigm is that investors, particularly uninformed market makers, are risk neutral, and thus return autocorrelations are always zero (Back, 1992, Chau, Vayanos, 2008, Collin-Dufresne, Fos, 2016, Kyle, 1985, Lambert, Ostrovsky, Panov, 2018). This paper takes a first step towards addressing these two questions in a dynamic setting: What are the differences in the two measures of market efficiency between these two paradigms? What mechanisms explain these differences?====To highlight the differences in dynamic responses to a piece of fundamental information under the two paradigms, we consider a three-period parsimonious setting in which the private information observed by informed traders remains constant over time. Three types of traders, including uninformed market makers, informed traders, and a large number of liquidity traders, trade one risk-free bond and one risky stock. The liquidation value of the stock is realized and becomes public information in Period 3, and all traders liquidate their holdings and consume their terminal wealth. Before trading, informed traders observe a signal about the liquidation value of the stock, but the rest only know its distribution.==== Both informed traders and uninformed market makers are risk averse (CARA utility functions) and maximize their expected terminal utilities. Market makers are competitive; they are atomistic, so their demands do not affect price distributions. They submit demand schedules contingent on historical and current prices to accommodate the net trades from other investors. Liquidity traders are assumed to trade randomly for exogenous motives.====In the Grossman-Stiglitz-style model, informed traders are competitive. They observe both contemporaneous and historical prices and submit demand schedules. We show the existence of a unique linear equilibrium. In the Kyle-style model, informed traders collude and exert their market powers as a monopolistic trader and trade strategically. They submit market orders.====In our parsimonious setting, the aggregate risk—the product of fundamental risk (the volatility of the liquidation value), liquidity trading risk (the amount of liquidity traders’ trading), and the inverse of the economy’s risk-bearing capacity (measured by investors’ risk aversion coefficient)—is the single factor that determines price informativeness, return reversals, and market makers’ compensations for providing immediacy, as measured by their unconditional expected utilities, in both models.==== This result conforms to recent empirical evidence that suggests common factors of price informativeness and return reversals.====However, these two models exhibit significantly different properties. In the Grossman-Stiglitz-style model, informed traders only trade on their private information in Period 1. Their Period-2 trades are uncorrelated with previous Period-1 returns, their trades in each period do not predict the next-period returns, and the stock price follows a random walk process between the two trading periods.==== In contrast, in the Kyle-style model, informed traders trade smoothly on their private information in both trading periods. In Period 2, they also exploit the previous transitory pricing error in the stock price and are contrarian investors, leading to a return reversal between the two trading periods. Moreover, the markets are less efficient in the Kyle-style model, with less informative prices, stronger return reversals, and higher unconditional expected utilities of market makers.====In the Grossman-Stiglitz-style model, informed traders do not explicitly take into account the impact of their trades and are not exposed to order execution risks. Because of competition, informed traders trade on their private information as quickly as possible (in Period 1 only). In Period 2, the information contents of their trading on fundamental information and their trading against their prior positions offset each other, so their Period-2 trades are not informative about the stock’s liquidation value. Informed traders and uninformed market makers only provide liquidity to accommodate Period-2 liquidity shocks, and consequently there is no return reversal between the two trading periods.====In contrast, a self-reinforcing effect arises from the informed traders’ trading on fundamental and non-fundamental information about their prior positions in the Kyle-style model, which explains the differences between these two types of models.==== Compared to the Grossman-Stiglitz-style model, informed traders trade less aggressively on fundamental information. This in turn causes them to trade less aggressively against their prior positions and thus exploit the stale information in previous stock prices, which leads to their contrarian trading and a return reversal between the two trading periods. Exploring more the stale information in previous prices further reduces their trading on fundamental information and vice versa. We refer to this effect as the ====, which leads to a reduction in market efficiency, contrarian trading by informed traders, and in particular a return reversal between the two trading periods.====Our explanation of return reversals relies on the dampening effect and complements existing explanations.==== If market makers are risk neutral, there is no return autocorrelation in either the Grossman-Stiglitz-style model or the Kyle-style model. Thus, we show that the risk premium demanded by risk-averse market makers is a necessary, but not sufficient, condition for return reversals. However, considering risk-averse market makers, the dampening effect is a sufficient condition for return reversals. The dampening effect also explains the empirical findings of So and Wang (2014) that the return reversal during non-announcement periods is much smaller than that during announcement periods.====The results of this paper are useful in addressing some empirical questions. Compared to return autocorrelations which are readily estimated from market data, price informativeness is more difficult to estimate, because it is related to unobservable private information. Our study proposes a simple approach to quantitatively estimate price informativeness by using information from short-term return reversals (see Section 6.2 for more details). This approach complements two current methods for estimating price informativeness from publicly available information about stocks—==== (Morck, Yeung, Yu, 2000, Roll, 1988) and ==== ==== (Easley, Kiefer, O’Hara, 1996, Easley, Kiefer, O’Hara, 1997), which involve more complicated calculations.====Our paper is closely related to two papers on comparing different ways of using private information. Cespa (2008) compares two economies: one in which information is controlled by a monopolistic analyst who sells his information to competitive traders, and one in which information is controlled and used directly by a strategic insider. The market is less informative in the latter economy due to the tighter control of information flow by the insider. Our paper obtains similar results. However, there are two key differences between these two papers. First, market makers are risk neutral in Cespa (2008) but risk averse in ours. Second, in Cespa (2008), informed traders trade based on fundamental information, affecting only price informativeness. In our paper, a dampening effect arises from informed traders’ trading on fundamental and non-fundamental information about their prior positions in the Kyle-style model, affecting both price informativeness and return reversals.====Vayanos and Wang (2012) compare three dynamic models, including one with symmetric information, one with asymmetric information and competitive informed traders, and one with asymmetric information and strategic informed traders. Their focus is to examine the effects of asymmetric information and monopolistic market power on the liquidity premium. They show that asymmetric information increases expected returns whereas strategic trading (imperfect competition) can decrease expected returns. In contrast, we aim to understand their effects on different measures of market efficiency.====There is a growing literature on the Kyle-style model of risk-averse market makers. Subrahmanyam (1991) considers a one-period model and examines the effects of risk aversion on equilibrium properties, such as market efficiency and liquidity. Vayanos (2001) analyzes the optimal trading of a large investor who has private information about his endowment shocks in the presence of competitive risk-averse market makers in a stationary setting. Guo and Kyle (2018) extend Guo and Ou-Yang (2015) in a stationary setting by introducing risk-averse market makers and show with numerical calibrations that a feedback effect arising from the trading of a large informed trader on private information and liquidity trading leads to both short-term momentum and long-term reversal. Back et al. (2021) extend the Kyle model to multiple assets, general distributions, and risk-averse market makers by incorporating optimal transport theory. They show that risk-averse market makers lead to lower liquidity and short-term reversals. In contrast to these papers, we intend to compare market efficiency between the competitive and strategic trading paradigms and discover the mechanisms that explain the differences.====The remainder of the paper is organized as follows. Section 2 presents the common assumptions. Section 3 studies the baseline model under symmetric information in which liquidity traders and uninformed market makers trade. Section 4 details the Grossman-Stiglitz-style model. Section 5 details the Kyle-style model. Section 6 compares the differences of these two models in terms of market efficiency, discusses a dampening effect that explains the differences, and presents the empirical implications. Section 7 concludes the paper. All proofs and figures are presented in the Appendix.",Dampening effect and market efficiency,https://www.sciencedirect.com/science/article/pii/S0165188923000106,18 January 2023,2023,Research Article,32.0
"Zaremba Adam,Cakici Nusret,Bianchi Robert J.,Long Huaigang","Montpellier Business School, 2300, avenue des Moulins Cedex 4, Montpellier 34185, France,Department of Investment and Capital Markets, Institute of Finance, Poznan University of Economics and Business, Al. Niepodległości 10, Poznań 61-875, Poland,Nusret Cakici, Gabelli School of Business, Fordham University, 45 Columbus Avenue, Room 510, New York, NY 10023, USA,Robert Bianchi, Griffith Business School, Griffith University, 170 Kessels Road, Nathan, QLD 4111, Australia,Huaigang Long, School of Finance, Zhejiang University of Finance and Economics, 18 Xueyuan Street, Hangzhou, Zhejiang 310018, China","Received 23 June 2022, Revised 3 November 2022, Accepted 5 January 2023, Available online 7 January 2023, Version of Record 23 January 2023.",https://doi.org/10.1016/j.jedc.2023.104596,Cited by (0)," worldwide. The quintile of stock markets with the highest change in government bond yields underperforms the countries with the lowest change by 0.76% per month. The phenomenon is distinctly robust and cannot be explained by known risk factors. Furthermore, the low correlation with other return patterns paves the way for effective country allocation strategies.","A standard narrative suggests that equity values are negatively associated with interest rates.==== Accordingly, a downward shift of the yield curve translates into lower discount rates, which drive equity prices up, and vice versa.==== Lower rates may also support aggregate demand, boosting valuations through the cash flow channel. In an efficient market, these changes in the economic environment should be immediately impounded in stock prices. However, adjustments in financial markets sometimes take time—long decision processes, slow-moving capital, market frictions, market segmentation, and rational inattention pave the way for temporary mispricing (Mitchell et al., 2007; Duffie, 2010; Gromb and Vayanos, 2010; Pitkäjärvi et al., 2020; Li, 2022). If investors react slowly, the change in stock prices may be delayed. As a consequence, past interest rate changes may result in a predictable pattern in equity returns.====In this study, we examine whether the past changes in interest rates explain the cross-section of country equity returns around the world.==== To capture the broad shifts in interest rates in the economy, we measure the changes in long-term sovereign bond yields. Government bond yields are a textbook input in valuation models (e.g., Damodaran (2008, 2012)), as they encapsulate different components affecting stock prices through the discount channels: real risk-free rates, inflation expectations, and credit (sovereign) risk premiums.==== However, bond markets tend to be highly persistent and move in cycles that can last for decades. Therefore, we build a unique multi-asset dataset that spans over a century. Using bond and stock market data from sixty countries, we test whether past bond yield changes predict the cross-section of future country equity returns.====Our results uncover a novel asset pricing phenomenon: past interest rate changes (====) negatively predict future stock market returns in the cross-section. The quintile of stock markets with the highest ==== underperforms their counterparts with the lowest ==== by 0.76% per month. The effect is confirmed by various analyses, including univariate and bivariate portfolio sorts, tests of monotonic relationships, as well as cross-sectional and panel data regressions. Furthermore, the abnormal returns cannot be attributed to established risk factors and country-level anomalies, such as value, momentum, long-term reversal, idiosyncratic risk, beta, skewness, seasonality, credit risk, or market size. The ==== effect is a novel asset pricing phenomenon that provides incremental information about future stock returns. We attribute this asset pricing anomaly to the effects of market segmentation and rational investor inattention between bond and stock markets.====The return predictability by ==== is distinctly robust. It is not restricted to 10-year government bonds only but extends to a wide range of interest rate instruments, including overnight rates, bank deposit rates, interbank rates, treasury bills, and bonds of various maturities. Moreover, it survives alternative formation, holding, and skip periods, controlling for outliers, changes in breakpoints, and portfolio implementation methods. Lastly, it holds in different market states, subsamples, and subperiods, and exhibits more stability through time than other prevalent factors. In short, the ==== effect seems unlikely to be just another statistical artifact in the asset pricing literature.====Having established the baseline properties of the ==== effect, we explore the underlying sources of these returns. We begin by estimating a series of tests in order to determine whether the results are not driven by alternative explanations. Specifically, we verify whether the ====-based returns are determined by the following: (a) the manifestation of other bond-based strategies; (b) exposure to macroeconomic risks; and, (c) alternative economic mechanisms. In short, we find no supporting evidence for any of these hypotheses to explain the ====-based returns.====Other bond-based equity strategies cannot explain the ==== phenomenon. We consider the cross-asset time-series momentum by Pitkäjärvi et al. (2020) and the bond-to-equity momentum of Geczy and Samonov (2017). Neither of these strategies explains the ==== effect, but the ==== effect subsumes the bond-to-equity momentum. Furthermore, there is no major link between the ==== effect and macroeconomic risks. The abnormal returns do not vary through time or cross-sectionally along with changing economic conditions. In addition, the test of Chordia and Shivakumar (2006) reveals no impact of the ==== signal on the return components predicted by macroeconomic variables. Moreover, the ==== returns are not driven by foreign exchange rates (Hou et al., 2011), the spillover of macro momentum (Brooks, 2017), or by changes in government debt (Wisniewski and Jackson, 2021). Lastly, the return predictability remains significant when we control for bond yield levels, yield curve slope, and changes in its steepness. None of these alternative explanations are supported by our findings.====Having ruled out the possible alternative explanations, we turn to our core story of the ==== effect, that is, the delay in investor reaction caused, for example, by slow-moving capital. We perform additional tests that provide findings that support this explanation. We plot post-formation portfolio returns to reveal that they follow the typical pattern characterized as underreaction. The abnormal performance is limited to a short period of approximately twelve months following portfolio formation. Subsequently, once investment decisions are executed and capital reaches its target, the mispricing is eliminated, and no unexplained profits occur thereafter. These findings are also confirmed by cross-sectional regressions involving contemporaneous and past bond yield changes. In line with the delayed reaction narrative, the relationship is the strongest for concurrent yield changes and gradually weakens with an increasing delay. Finally, the cross-sectional and time-series variation in the ==== effect provides broad support for the underreaction explanation. This phenomenon is noticeably stronger in a market characterized by high idiosyncratic volatility, a common indicator of limits to arbitrage, which is closely linked with market integration. Furthermore, the ==== strategy profits weaken through time as global capital markets become more open, developed, and less segmented.====To isolate the potential source of underreaction from slow-moving capital, we directly examine market segmentation and rational inattention as possible explanations for the ==== effect. Greenwood et al. (2018) note that most investors tend to specialize in an asset class and there are only a few generalists with the skill or the mandate to reallocate capital across different markets. This implication suggests that market segmentation creates an environment where capital moves quickly within a single asset class but slowly between asset classes. Bekaert et al. (2011) show that market segmentation has declined over time; however, less developed markets remain significantly segmented in comparison to developed markets. To measure market segmentation, we follow Pukthuanthong and Roll (2009) by employing a principal component approach to show there is no common component or a single global risk factor that explains the variation of both bond and stock returns. This evidence of low financial market integration supports the idea that market segmentation contributes to the underreaction effect of slow-moving capital.====We also examine rational inattention as another potential source of underreaction that induces slow-moving capital. We examine sovereign credit rating events as a proxy for rational inattention that may drive the ==== effect. Li (2022) shows that corporate bond prices react slowly to company default risk and interest rate shocks in Treasury markets. We extend this line of inquiry to evaluate whether global stock index returns are also slow to respond to interest rate shocks. Our findings show that sovereign credit rating shocks capture the dynamics of rational inattention as the underlying source of slow-moving capital. Our empirical evidence shows that investors are very attentive with countries that experience changes in their sovereign credit rating. When there are changes in a sovereign's credit rating, we find no underreaction in their respective equity markets resulting in insignificant returns from the ==== effect. Conversely, countries with no change to their sovereign credit rating exhibit underreaction in their respective equity markets, resulting in significant excess returns from the ==== effect. This evidence of rational inattention reveals that investors cannot process all available information, but rather, they choose which precise information to make decisions. In our case, changes in a sovereign's credit rating are the piece of information that investors use to act upon in cross-asset allocation decisions between bond and stock markets.====Lastly, we are interested in possible practical applications of the ==== effect. The ==== strategy can be effectively combined with other popular global risk factors. The ==== portfolios display low correlations with the common market, value, and momentum strategies, which improves the overall Sharpe ratio of an investment portfolio. Furthermore, the ==== strategy is robust to alternative implementations and conservative trading cost assumptions. Finally, it does not rely on shorting or high-turnover signals, so it can be successfully deployed as a long-only portfolio with limited turnover.====This paper contributes to several strands of literature. First, we extend the empirical studies on cross-asset return predictability (Geczy and Samonov, 2017; Pitkäjärvi et al., 2020). Several studies demonstrate that variables from the corporate bond universe can predict future stock returns, or vice versa (Bektić (2019); Gebhardt et al., 2005; Haesen et al., 2017; Ho and Wang, 2018; Lin et al., 2013; Tolikas, 2018). Lee et al. (2021) use credit default swaps data to forecast equity performance. Bakshi and Panayotov (2013) and Lu and Jacobsen (2016) demonstrate how equity and commodity price movements can predict carry trade returns. Avramov et al. (2019) and Bali et al. (2020) comprehensively scrutinize the links between corporate bonds and stock return predictors. Our study extends this literature as we are the first to derive information signals from government bond yields that predict returns on national equity indexes. To our knowledge, the earlier analyses of the bond yield–equity relationship are mostly limited to single-country studies of time-series dependencies (e.g., Chan et al. 1985, Chen et al. 1986, Sweeney and Warga 1986, Pesaran et al. 2004, Ehrmann et al. 2011, Welch and Goyal 2008).====Second, from a broader perspective, we expand the range of return-predictive variables that provide incremental information on future stock index returns. The literature documents well-established predictors, such as momentum (Baltussen et al., 2021; Chan et al., 2000; Balvers and Wu, 2006; Bhojraj and Swaminathan, 2006; Asness et al., 2013), value (Asness et al., 1997, 2013; Baltussen et al., 2021), idiosyncratic risk (Bali and Cakici, 2010; Umutlu, 2015), long-run reversal (Balvers et al., 2000; Balvers and Wu, 2006), beta (Frazzini and Pedersen, 2014), cross-sectional seasonality (Keloharju et al., 2016), and financial ratios (Calice and Lin, 2021). We contribute to this literature by introducing a new predictor, the interest rate change (====) effect.====Third, we add to the literature by extracting novel asset pricing insights through extended historical datasets covering more than a century of bond and equity prices. Previous research on long datasets focused on momentum (Chabot et al., 2008; Geczy and Samonov, 2016, 2017; Goetzmann and Huang, 2018), value (Baltussen et al., 2021), trend-following (Greyserman and Kaminski, 2014; Lempérière et al., 2014; Hurst et al., 2017), low-volatility (Annaert and Mensah, 2014), long-term reversal (Spierdijk et al., 2012), beta (Fohlin and Reinhold, 2010), and calendar anomalies (Urquhart and McGroarty, 2016; Jacobsen and Zhang, 2021). Our study broadens this empirical literature by exploring bond market signals that use long-run historical data.====The remainder of the study proceeds as follows. Section 2 discusses the theoretical basis of our paper and how they relate to our main hypothesis. Section 3 presents the data and variables. Section 4 reports the baseline results of the ==== effect. Section 5 provides further robustness checks. Section 6 explores the potential explanations of the phenomenon. Section 7 focuses on the practical investment implementation of the ==== effect. Finally, Section 8 concludes the study.",Interest rate changes and the cross-section of global equity returns,https://www.sciencedirect.com/science/article/pii/S0165188923000027,7 January 2023,2023,Research Article,33.0
"Duong Huu Nhan,Kalev Petko S.,Tian Xiao","Department of Banking and Finance, Monash Business School, Monash University, Clayton Campus, Wellington Road, Clayton VIC 3800, Australia,Department of Accounting, Data Analytics, Economics and Finance, La Trobe University, Australia,Victorian Department of Treasury and Finance, Australia","Received 31 July 2022, Revised 29 November 2022, Accepted 27 December 2022, Available online 4 January 2023, Version of Record 22 January 2023.",https://doi.org/10.1016/j.jedc.2022.104592,Cited by (0),"We investigate the impact of short selling activity on price volatility in the corporate bond market. We find that bond short selling activity is positively related to volatility, trading activity, and the volume-volatility relation. During the Global Financial Crisis, when investors’ expectations tend to be more homogenous, the positive relation between short selling activity and price volatility breaks down. We further show that bond short selling is not a substitute for equity short selling and ==== trading. Overall, our study highlights the importance of bond short selling as a platform to express investors’ differences of opinion.","Short selling is an integral part of financial markets. While the information content of short selling in equity market has been extensively researched,==== the literature on short selling in bond markets is sparse (Bessembinder et al., 2020). This is surprising given that the bond market dwarfs the global equity market.==== Prior studies also offer inconclusive evidence about the information content of bond short selling. For the period between 2004 and 2007, Asquith et al. (2013) find that on average, bond short sellers do not have private information. Extending the sample period to 2011, Hendershott et al. (2020) show that bond short selling predicts bond returns and contribute to efficient bond prices, even after controlling for the effect of equity short selling. Motivated by the size, importance of the bond markets, and the lack of research on corporate bond short selling, we examine the relation between short selling and trading activity and price volatility in the corporate bond market. In addition, we consider whether this relation is independent from other conduits for investors to express their negative views such as equity short selling or put options.====The theoretical motivation for our work builds on the argument that short selling is a channel through which investors express differences of opinion. In his seminal paper, Miller (1977) argues that prices tend to reflect more optimistic views when investors have a divergence of what the fair value of a given security should be. When short selling activities are constrained, investors cannot express their negative views. This leads to security misvaluation.==== Bessembinder et al. (1996) argue that short selling is the ideal proxy for difference of opinion. Theoretical models of Harris and Raviv (1993) and Shalen (1993) postulate that difference of opinion among investors creates excessive trading volumes and induces high price volatility. Synthesizing these evidences, we postulate that there is a positive association between bond short selling and corporate bond price volatility.====An alternative hypothesis that short selling is negatively related to price volatility is also plausible. According to Diamond and Verrecchia (1987), insiders who possess negative information will take short positions. Therefore, short selling activities contain information regarding future price movements. Prior studies find that short sellers in equity markets possess private information and trading strategies based on their trades generate abnormal returns (see, among others, Boehmer et al. 2008; Diether et al. 2009; Engelberg et al. 2012).==== Hendershott et al. (2020) also find that short selling activity in the bond market is informative about future bond returns. To the extent that short selling represents informed trading in the corporate bond markets, short selling activities will facilitate price discovery by impounding more information into the price, alleviating excessive trading and price volatility. One can, therefore, hypothesize that short selling is negatively related to corporate bond price volatility.====We examine these conflicting theoretical predictions using data on corporate bond short selling over the sample period between 2007 and the end of 2016. First, we find that short selling activity, measured as the absolute value of short interest change,==== is positively and significantly related to both trading activity, measured as the number of trades, and volatility. This finding implies that short selling is used by investors to express their difference of opinion in the corporate bond market, consistent with Harris and Raviv (1993) and Shalen (1993) that investors’ heterogeneity induces excessive trading activity and volatility.====Second, we find that short selling activity reinforces the positive relation between the number of trades and volatility. Our results provide support for the theoretical predictions of Harris and Raviv (1993) and Shalen (1993) that the link between volume and volatility gets stronger when there is an increase in difference of opinion among investors. This is consistent with the conjecture that difference of opinion would be high with a heightened level of information flow, proxied by the relationship between volume and volatility. Furthermore, accounting for the impact of the GFC when investors’ expectations are likely to be homogeneous, we find the relation between short selling and volatility is alleviated significantly by the GFC.====Finally, we examine if short selling in the corporate bond market is an independent conduit for investors to express differences of opinion, apart from the common channels such as equity short selling and put option trading. Asquith et al. (2013) argue that investors who possess negative news about company would generally short the stock of the company first due to the priority of claims (the price of the stock should fall first). Hence, bond short sellers are doing so because shorting the stock of the company is not available or it is too expensive. Investors can also engage in option trading as option trading is generally viewed as relaxations of short selling constraints (see, among others, Diamond and Verrecchia 1987; Senchack and Starks 1993; Safieddine and Wilhelm 1996; Beber and Pagano 2013). Therefore, we expect to find a significant difference in the relation between short selling activity and volatility between bonds with and without corresponding equity short selling and option trading available. We do not observe evidence consistent with this expectation. Moreover, using the equity short-sale ban in the U.S market as an exogenous shock, we do not find a significant increase in the level of bond short selling activity. Overall, these results suggest that short selling in the corporate bond market is an independent platform for investors to express their different views regarding bond-specific news and information, not simply a substitute for equity short selling and option trading.====Our contributions are twofold. First, our paper contributes to the sparse literature about short selling in the corporate bond market despite the corporate bond market playing a significant role in the financial markets regarding capital financing and information dissemination. Asquith et al==== (2013) show that the short selling activity is a substantial component of corporate bond trading, constituting around 19.1 percent between 2004 and 2007. Using a comprehensive data set from a primary lender, Asquith et al. (2013) find that borrowing costs for the corporate bond are comparable to stocks and have declined over time. More importantly, the authors show that there is no evidence of short sellers having private information. In contrast, Hendershott et al. (2020) find that bond short selling forecasts future bond returns, especially for high-yield bonds and after the Lehman Brothers collapse in 2008. We extend these studies by providing empirical evidence on the effect of short selling activity on trading activity and price volatility in the corporate bond market.====Second and more importantly, we show that short selling in the corporate bond market is not just an additional tool for investors to trade negative news and information against the underlying company. On the contrary, it is an independent conduit for investors to express difference of opinion specific to bonds, independent from reasons to short the listed equity of the underlying company or to engage in put option trading. The implication of the finding is highly relevant to all investors and regulators as it highlights the importance of the short selling in the corporate bond market as an indispensable mechanism for impounding bond specific information. Therefore, short selling constraints or impediments to short selling could be harmful to price discovery and efficiency.====The rest of the paper is organised as follows. Section 2 reviews the related literature. Section 3 describes the data and specifies our econometric models. Section 4 and Section 5 report the empirical results and robustness tests respectively. Section 6 concludes the paper.","Short selling, divergence of opinion and volatility in the corporate bond market",https://www.sciencedirect.com/science/article/pii/S0165188922002950,4 January 2023,2023,Research Article,34.0
"Buhai I. Sebastian,van der Leij Marco J.","SOFI at Stockholm University, Sweden,NIPE at Minho University, Portugal,CEPREMAP, France,Congregation of the Blessed Sacrament, Belgium,University of Amsterdam, Netherlands","Received 2 February 2022, Revised 24 December 2022, Accepted 29 December 2022, Available online 30 December 2022, Version of Record 14 January 2023.",https://doi.org/10.1016/j.jedc.2022.104593,Cited by (0),We propose an equilibrium interaction model of occupational segregation and labor market ,"Most studies investigating the causes of labor market inequality agree that classical theories such as taste or statistical discrimination by employers cannot, ==== explain pay, employment, and occupational disparities between genders or races, and their remarkable persistence over time====.==== While several meritorious complementary theories have been advanced, some leading social scientists have suggested that social interactions could also be an important, yet relatively little explored channel in this context, see for instance Arrow (1998)====.====In this paper, we investigate a potential network channel leading to occupational segregation and wage inequality in the labor market, by developing and analysing an intuitive, parsimonious social interactions model====.==== We construct a four-stage model of occupational segregation between two homogeneous, exogenously given, mutually exclusive social groups (e.g., genders or races) acting in a two-job labor market. While the model stages are formally described and detailed in Section 3, we sketch them intuitively below. In the first stage each individual chooses one of two specialized educations to become a worker. In a second stage individuals randomly form ”friendship” ties with other individuals, with a tendency to form relatively more ties with members of the same social group, what is known in the literature as “====”, “====” or ”====”====.==== In a third stage individuals search for jobs, either directly or through their networks of employed friendship contacts. In the last stage workers earn a wage and spend their income on a single consumption good.====We obtain the following results. First, unsurprisingly, we show that with inbreeding homophily within social groups, a complete polarization in terms of occupations across the two groups arises as a stable equilibrium outcome. This follows from standard arguments on network effects. If a group is completely segregated and specialized in one type of job, then each individual in the group has many more job contacts if she ”sticks” to her specialization. Hence, sticking to one specialization ensures good job opportunities to group members, and these incentives stabilize segregation.====We next extend the basic model allowing for “good” and “bad” jobs, in order to analyze equilibrium wage and unemployment inequality between the two social groups. We show that with large differences in job attraction (i.e., wages at equal labor supply), the main outcome of the model is that one social group ”fully specializes” in the good job, while the other group ”mixes” over the two jobs. In this partial segregation equilibrium, the group that specializes in the good job always has a higher payoff and a lower unemployment rate====.==== Furthermore, with a sufficiently large intra-group homophily, the fully-specializing group also has a higher equilibrium employment rate ==== a higher wage rate than the ”mixing” group, thus being twice advantaged. Hence, our model is able to explain typical empirical patterns of gender, race, or ethnic labor market inequality – see, e.g., our next (sub)section reviewing known empirical facts on gender segregation patterns in the labour market. The driving force behind our result is the fact that the group that fully specializes, being homogenous occupationally, is able to create a denser job contact network than the mixing group. We emphasize at this point that we do not intend to imply that there is no more taste or statistical discrimination by employers in the labor market. On the contrary, we regard our social interaction model, classical discrimination theories, as well as other theories such as, e.g., gender-specific work amenity preferences, as ==== bits in explaining observed patterns of labor market inequality.====We finally consider whether society benefits from an integration policy, in the sense that labor inequality between the social groups would be attenuated. To this aim, we analyze a social planner’s first and second-best policy choices.==== Rather counter-intuitively, segregation is the preferred outcome in the first-best analysis and a laissez-faire policy leading to segregation shaped by individual incentives is always maximizing social welfare in the second-best case. Hence, overall employment is higher under segregation, while laissez-faire wage inequality remains sufficiently small, such that segregation is an overall socially optimal policy. We show that integration policies are justified only in the presence of additional distribution concerns, beyond individual utilities. Our social welfare analysis points out therefore some policy issues unfortunately ignored in most debates concerning anti-segregation legislature, such as the need for ====.====Our model shares similarities with the frameworks by Benabou (1993) and Mailath et al. (2000). Benabou (1993) introduces a model in which individuals choose between high and low education; the benefits of education are determined globally, but the costs are determined by local education externalities. As in our model, these local education externalities lead to segregation and inequality at the macro level. Unlike in our model, inequality in education level fully explains pay gaps in Benabou (1993), which, for instance, is at odds with the evidence on gender education gaps — see the discussion in our Section 2.1. Mailath et al. (2000) also consider a model in which workers choose between high and low (no) education, but in a setting with search and matching between firms and workers. The features of their and our segregation equilibria are similar, even though the behavioral mechanisms behind them are very different. Crucially, in their model firms may target their search to one group of workers; they show that there exists a segregation equilibrium, in which workers of only one group invest in high skills and firms target their search to this highly-skilled group. In that case, unemployment is lower for the highly-skilled group, whose wage is also higher, since lower unemployment gives them a better bargaining position vis-à-vis the firms. In both Benabou (1993) and Mailath et al. (2000) social welfare policies are ambiguous: depending on the parameters, either integration or segregation might be socially optimal. This is in stark contrast to our model, in which a segregation is always a first best optimal policy, and under reasonable assumptions also a second-best optimal policy. It suggests that ignoring the channel of homophilous job contacts may overestimate the welfare effects of integration policies. While we believe the mechanisms considered by Benabou (1993) and Mailath et al. (2000) are present as well (and so are other channels, including direct taste discrimination), we show in our paper that neither spatially local education externalities, nor targeted firm search are needed to explain the wage and employment gaps between gender or racial groups: job search through social networks combined with inbreeding homophily is already sufficient. We provide a small calibration and simulation exercise showing that our model generates wage and unemployment gaps in line with actual figures.====Significant progress has been achieved in modeling labor market phenomena by means of social networks. Existing research has for instance investigated the effect of social networks on employment, wage inequality, and labor market transitions.==== This work points out that individual performance on the labor market crucially depends on the position individuals take in the social network structure. However, these studies typically do not focus on the role that networks play in accounting for persistent patterns of occupational segregation and inequality between races, genders or ethnicities====.==== A recent exception is Bolte et al. (2022) who model and discuss how the distribution of job referrals could lead to persistent inequality and intergenerational immobility, also discussing the impact on productivity. While we do not analyse the wider impact and implications of the ==== or the intergenerational dynamic aspects, our simpler, ’reduced form’ approach enables us to focus on and highlight the essential role of the homophilous referral mechanism that relates indirect job finding via social network contacts to occupational segregation and labor market inequality in earnings and employment between social groups====.==== In particular, we use our model to explore strategic career choices in this context of homophilous job contact networks, complementing therefore the scope of Bolte et al. (2022).====The next section overviews empirical findings on labor market gaps between genders/ races, on the relevance of job contact networks, and on the extent of social group homophily. We set up our model of occupational segregation in Section 3 and discuss key results on the segregation equilibria in Section 4. Section 5 analyses social welfare optima. We summarize and conclude in Section 6.",A Social Network Analysis of Occupational Segregation,https://www.sciencedirect.com/science/article/pii/S0165188922002962,30 December 2022,2022,Research Article,35.0
"Ciola Emanuele,Turco Enrico,Gurgone Andrea,Bazzana Davide,Vergalli Sergio,Menoncin Francesco","Fondazione Eni Enrico Mattei (FEEM), Milano,Dipartimento di Economia e Management, Universitá degli Studi di Brescia, Brescia,The Complexity Lab in Economics, Department of Economics and Finance, Catholic University, Milano","Received 2 March 2022, Revised 4 December 2022, Accepted 19 December 2022, Available online 30 December 2022, Version of Record 30 December 2022.",https://doi.org/10.1016/j.jedc.2022.104589,Cited by (2)," and lower real GDP. Nevertheless, the distribution of gains and losses across sectors and agents varies significantly depending on the type of shock. Our findings suggest that policymakers should carefully consider the nature of energy shocks and the resulting distributional effects to design effective measures in response to energy crises.","Energy has always been a crucial element in economic development and has been assuming an increasingly important role in recent years. The fight against climate change (see IPCC 2021 and COP26) has pushed countries to define transition processes toward renewable resources, reducing Greenhouse Gas (GHG) emissions and imposing implementation targets, such as net-zero emissions (NZE) for 2050 in the European Union (EU). It is widely recognized that the likelihood of a low-carbon transition will crucially depend on the policymakers’ ability to spur the economic system toward cost-effective and environmentally sustainable energy sources. However, it is still unclear how long the process will take and what are the major macro-financial risks associated with it. The global decarbonization process has also involved the beginning of the coal phase-out. At the same time, the post-covid recovery has increased energy demand for all countries seeking to restart. Not least, the political crisis between the United States (US) and Russia in Ukraine has further contributed to overheating energy markets and prices. All this has profound repercussions on the level of inflation, which is reaching values not seen for decades. Moreover, according to the last World Energy Outlook (Birol, 2021), energy will assume an even-more central role in the lives of households and worldwide economic growth. People’s well-being and industrial competitiveness strongly depend on safe, secure and affordable energy. The accessibility and reliability of energy will become even more critical to all aspects of people’s lives (e.g., cooking, heating). For example, the projections on the US energy demand by the US Energy Department forecast a positive trend even if some policies will be adopted to improve energy use efficiency.====In light of this, understanding the major challenges and opportunities resulting from the energy transition requires a modelling framework capable of capturing market imbalances and out-of-equilibrium dynamics that may arise along the transition pathway. According to Hansen et al. (2019), the interactions in the energy sector, influenced by network and governance structures, require a methodological approach that can deal with complexity and non-linear dynamics. As a result, Agent-Based Models (ABM) constitute a promising approach since they introduce heterogeneous agents whose characteristics and behaviour can enhance the realism of the analysis (Delli Gatti et al., 2011).====An ABM is a computational framework representing the economic system as a complex evolving environment where autonomous interacting agents behave according to fixed or evolving rules of thumb (Tesfatsion, 2006). Agents may represent individuals, firms, groups of individuals, policymakers or even countries, depending on the problem under investigation. In the agent-based approach, the macroeconomic outcome emerges from the interactions of heterogeneous agents and does not coincide with the micro-representative agent behaviour (Bonabeau, 2002). Hence, as highlighted by Karimi and Vaez-Zadeh (2021), ABMs are an appropriate methodology to investigate the energy transition because they allow studying the (suboptimal) macro-dynamics emerging from locally optimal but uncoordinated micro-interactions. Moreover, agent-based modelling is a useful approach to test policies and better understand the responsiveness of a complex system to small or big changes in its structural parameters or behavioural rules, allowing for a better description of the system transition (Farmer, Foley, 2009, Helbing, 2012).====Despite the growing interest of the ABM literature in the role of energy in macroeconomics, little effort has been made to endogenize the energy sector, assuming, on the contrary, an infinitely elastic energy supply, thus excluding the possibility of supply shortages and energy market feedback. For this reason, we propose a new macroeconomic ABM with an endogenous energy sector, the MATRIX model – a Multi-Agent model for Transitions Risks. The model is populated by heterogeneous agents who take decisions derived from profit/utility maximization in a context characterized by limited information and bounded rationality. Further, interactions among agents occur through decentralized matching protocols built to represent real-world markets. The structure of the model is composed of: a corporate sector, characterized by energy, consumption and capital goods firms; a fossil fuel sector; a banking sector; a household sector, including workers and firms and banks’ owners; and a public sector, comprising a government and a central bank. The business sectors have interrelated input demands so that variations in prices and quantities in one industry propagate through market mechanisms to the rest of the economy. In particular, the energy sector is fully integrated into the economic system as it combines labour, capital and fossil fuels (e.g., oil or natural gas) to generate and distribute energy services that are functional to the production of consumption and capital goods.====The model is calibrated on US quarterly macroeconomic data and can replicate selected statistics from empirical time series in terms of aggregate volatility and business cycles’ properties. In particular, the sample auto- and cross-correlation functions closely match those obtained from US data between 1957 and 2019. At the same time, the model reproduces additional empirical evidence, like the positive relationship between aggregate production and energy demand or the right-skewed distribution of firms’ size and households’ wealth. Afterwards, we employ the MATRIX model to simulate the economic and distributional effects of three different energy shocks, namely: (i) an increase in the price of fossil fuel (EIP); (ii) a reduction in energy firms’ total factor productivity (EP); (iii) a negative shock to the available quantity of fossil fuel (EQ). The EIP and EP scenarios are characterized by a one-off autoregressive shock, while the EQ shock remains in place for a certain number of periods to reflect a persistent reduction in the available quantity of natural resources. Lastly, we perform a sensitivity analysis on the parameters governing energy production to assess the robustness of our findings to changes in the relative importance of natural resources.====Simulation results show that all energy shocks lead to a sudden increase in inflation and a decrease in real GDP because of rising energy prices, associated with a spike in corporate defaults and unemployment rate. Contrary to the EIP and EP scenarios, which are characterized by one-off shocks, price levels remain high and grow for the entire duration of the EQ shock. At the same time, the recovery path presents a second recession in the EP and the EQ scenarios due to a significant increase in private debt. Indeed, the prolonged energy shortage, by systematically reducing the production and accumulation of physical capital, lowers the economy’s productive capacity. Accordingly, as the shock starts to fade away, firms try to regain past production levels by re-accumulating physical capital. However, that requires additional liquidity that they must borrow from banks. Therefore, that produces a sharp increase in private debt, thus undermining firms’ financial stability and causing a wave of bankruptcies and a new recession. Lastly, whereas energy shocks entail similar effects at the aggregate level, we find that the distribution of gains and losses across sectors and among agents vary significantly depending on their type. In particular, energy firms largely benefit from the redistribution of fossil fuel rents following the spike in their price in the EIP and EQ scenarios. Conversely, the energy shortage resulting from the decrease in energy firms’ productivity is detrimental to all industries, including the energy sector. The heterogeneous effects of energy shocks across sectors are then reflected in the diversified impact on the wealth distribution of different agent groups. Because price inflation erodes real wages, two years after the shock, workers are still worse off than they would have been in its absence, especially in the EQ scenario characterized by accelerating inflation. On the contrary, energy entrepreneurs experience positive growth in financial wealth in all but the EP scenario. Finally, the post-shock recovery fuelled by credit expansion eventually boosts banks’ profits and bankers’ wealth.====The paper is structured as follows: Section 2 reviews the existing literature on energy production in macroeconomic models and energy shocks; Section 3 presents the model; Section 4 illustrates the calibration procedure based on US data showing the capability of the model to reproduce empirical regularities; Section 5 proposes some simulations results exploring the dynamics generated by the model; Section 6 focuses on the energy sector analysing the endogenous response of the aggregate economy to exogenous shocks in energy production. Section 7 concludes and states future lines of research.",Enter the MATRIX model:a Multi-Agent model for Transition Risks with application to energy shocks.,https://www.sciencedirect.com/science/article/pii/S0165188922002925,30 December 2022,2022,Research Article,36.0
de Beauffort Charles,"Economics and Research Department, National Bank of Belgium, Bd de Berlaimont 3, 1000 Brussels, Belgium","Received 12 May 2022, Revised 14 December 2022, Accepted 26 December 2022, Available online 28 December 2022, Version of Record 14 January 2023.",https://doi.org/10.1016/j.jedc.2022.104591,Cited by (0),"When the government issues long-term bonds, the optimal time-consistent fiscal and ","The recent literature on the optimal time-consistent management of a liquidity trap with government debt (Burgert, Schmidt, 2014, Eggertsson, 2006) demonstrated that a government with short-term liabilities should provide a large debt-financed fiscal stimulus in the liquidity trap. The coordinated policy is for fiscal policy to support the economy in the recession with low taxes and stimulative government spending and for monetary policy to accommodate inflation upon the exit of the zero lower bound (ZLB). More recently, Matveev (2021) showed that this optimal coordinated policy hinges on government debt being short-term. If instead, the government issues bonds of longer maturity, the prescription becomes to consolidate government debt during the liquidity trap by increasing taxes and by curtailing spending. The reason is that the long maturity of debt reduces the fiscal benefits of monetary accomodation because the yield to maturity of the long bond reacts less to changes in the nominal interest rate. Consequently, monetary policy focuses more on inflation stabilisation and fiscal policy consolidates debt to avoid inflation stemming from the cost-push effect of labor taxes. Given empirically observed long average maturity of government bonds in the US, this result raises questions about the merits of the large amount of debt accumulated during previous liquidity trap episodes.==== For example, between the first time the Feds Fund rate reached the ZLB in the last quarter of 2008 and the end of the Great Recession two quarters later, government spending and the market value of government debt have both increased by about 6%. During the same period, the average tax rate has remained remarkably constant at around 30% of income.====To reconcile those observations with the optimal time-consistent policy prescription, I enrich the model with two realistic ingredients: sluggish wage adjustments and limited participation to asset market. In my model, sluggish wage adjustments come from wage negotiations by labor unions that are subject to quadratic adjustment costs while limited asset market participation is captured by the introduction of Rule-of-Thumb (RoT) households who consume their full income every period. Numerical simulations under plausible calibration reveal that those modifications overturn the previously found consolidation of debt in favor of an optimal debt-financed fiscal expansion. The intuition is as follows.====In the representative household model with flexible wages assumed in the literature so far, labor taxes are chosen to implement a given path of government debt and inflation. As shown by Matveev (2021), this implies that optimal taxes are very volatile in response to large contractionary demand shocks. More specifically, labor taxes soar in the liquidity trap to create a cost-push effect that mitigates the inflation shortfall. Upon the exit of the lower bound, long-term government debt and taxes persistently fall below their steady state level to lower real interest rate expectations and to stimulate consumption of forward-looking households.====In reality, increasing taxes in the liquidity trap may have dire effects if it reduces the net disposable income of households that do not have the possibility to trade consumption intertemporally. In my heterogeneous households model, the policy maker trades-off this Keynesian demand effect against the conventional intertemporal substitution effect associated with Ricardian consumers. For a share of RoT households matching empirical evidence, the government curtails the rise in labor taxes and boosts government spending sufficiently to cause an accumulation of debt in the liquidity trap. I show both analytically and numerically that the resolution of this trade-off depends on the degree of nominal rigidities prevailing in the economy. For nominal wages that are rigid enough, taxes can even drop in the liquidity trap. The reason is that the net disposable income of RoT households becomes more sensitive to variations in taxes and thus the tax-cut is very effective in stimulating consumption. On the contrary, when wages are more flexible, the policy maker focuses more on the benefits of a debt consolidation for Ricardian households and less on the cost in terms of net income loss for the RoT households. This also happens logically when the prevalence of RoT households in the economy drops. In this case, government debt increases only moderately on impact before falling persistently below its pre-shock level.====From a welfare point of view, optimal policy reaches a better outcome when the economy is populated by a representative household as compared to heterogeneous households. This reflects the additional policy trade-offs that the policy maker has to address with the same number of instruments. Since taxes cannot be used as freely to smooth inflation, wage and price inflation volatility increases with heterogeneous households yielding to additional welfare losses. Importantly, this constraint on the optimal policy implies that the volatility of labor taxes in response to demand shocks falls below the one of government spendingin line with empirical evidencewhile the opposite holds in the representative household economy.====This paper is related to the voluminous literature on fiscal and monetary policy in a low-rate environment. An important strand of this literature abstracts from debt sustainability issues to focus on the stabilising role of fiscal policy. Those models typically predict that the fiscal multiplier is large (superior to one) at the lower bound (Christiano, Eichenbaum, Rebelo, 2011, Erceg, Lindé, 2014, Woodford, 2011) and optimal fiscal policy is therefore expansionary (Nakata, 2016, Schmidt, 2013).====Closer to this paper, another strand of the literature attributes a meaningful role to government debt by confining tax instruments to labor income. Eggertsson (2006), Burgert and Schmidt (2014) study the optimal time-consistent fiscal-monetary policy mix in a liquidity trap when government budget is a binding constraint. Matveev (2021) explores the role of debt maturity while keeping the assumption of flexible wages. His finding about optimal debt consolidation in the liquidity trap constitutes the main motivation of this paper (this is, generalising optimal debt accumulation to long-term debt).====Finally, my model extension with sticky wages and Rule-of-Thumb consumers is grounded in the well-established and still growing literature on heterogeneous households. The wage setting by labor unions draws from Hagedorn et al. (2019). In a model with ad-hoc policy rules, Drautzburg and Uhlig (2015) show that delayed debt consolidation increases the fiscal multiplier at the ZLB when taxes are distortionary, wages are sticky and a share of consumers are Rule-of-Thumb while in a similar model, Kaszab (2016) finds that a tax-cut can increase output in a liquidity trap. The optimal debt-financed fiscal expansion in my model is analogous to their results. Moreover, numerous papers have revisited New-Keynesian models adding Rule-of-Thumb consumers. Related to this paper, Bilbiie (2008) characterises (optimal) monetary policy with limited asset participation while Colciago (2011), Ascari et al. (2017) combine Rule-of-Thumb consumers with sticky wages as in the present model, however abstracting from fiscal policy and the ZLB constraint. Analytical results of Ascari et al. (2017) concerning the impact of wage rigidities on the demand and supply blocks of the model can be seen as complementary to those of Section 4.2.====The remainder of the paper is organised as follows. Section 2 presents the main ingredients of the heterogeneous households model. Section 3 lays out the time-consistent policy problem and derives the conditions for an optimal equilibrium. Section 4 provides analytical intuition about the main trade-offs faced by the policy maker and performs numerical experiments to demonstrate the dynamics. Section 5 concludes.",When is government debt accumulation optimal in a liquidity trap?,https://www.sciencedirect.com/science/article/pii/S0165188922002949,28 December 2022,2022,Research Article,37.0
McNeil James,"Department of Economics, Dalhousie University, 6214 University Avenue, Halifax, Nova Scotia, Canada","Received 14 January 2022, Revised 11 November 2022, Accepted 15 December 2022, Available online 17 December 2022, Version of Record 26 December 2022.",https://doi.org/10.1016/j.jedc.2022.104588,Cited by (0),"I estimate how the term structure of ====) are available at many horizons, but combining these in a consistent way is challenging because of information frictions and the number of available forecasts. I extend the unobserved components model to include additional ","The ability of monetary policy actions to influence real interest rates, and hence affect the real economy, depends crucially on the response of inflation expectations. This is so for conventional monetary policy actions, in which case real and nominal interest rates are linked by inflation expectations through the Fisher equation, as well as unconventional monetary policy actions like forward guidance, which may be able to directly influence expectations through central bank communication. The ability of central banks to put downward pressure on real interest rates by adjusting inflation expectations through unconventional monetary policy is especially important when interest rates are at their effective lower bound—a situation encountered by many central banks over the past decade—to avoid falling into a liquidity trap.====In models with rational expectations, the response to a monetary policy shock of ====-period ahead inflation expectations will match the response of actual inflation ====-periods after the shock occurs. Knowledge of how actual inflation responds to monetary policy thus implies knowledge of how expected inflation will respond to the same policy. But work by Coibion, Gorodnichenko, 2012, Coibion, Gorodnichenko, 2015 has documented that survey forecasts of inflation have properties inconsistent with full-information rational expectations and consistent with models with information frictions. A property of aggregate expectations in these models is that they are slow to respond to new information. Those authors show that this slow response is a common feature of several models with information frictions, such as the sticky information model proposed by Mankiw and Reis (2002) and the noisy information model proposed by Woodford (2003), and provide evidence that these frictions can be large, even for professional forecasters. How these information frictions affect the response of inflation expectations after changes to monetary policy is less well understood.====In this paper I estimate how US inflation expectations, taken from the ====, respond to changes in monetary policy over the period 1982–2019, while allowing for the presence of information frictions in the underlying survey data. The exercise sheds light on the transmission of monetary policy through inflation expectations, the quantitative importance of information frictions, and the interaction of the two. To do this I use maximum likelihood methods to estimate a term structure model of inflation expectations based on the unobserved components model, which provides a mapping from inflation forecasts at any horizon to two latent model variables, representing short- and long-run components of inflation expectations. To account for information frictions, I follow Mertens and Nason (2020) and allow observed survey forecasts to depart from the unobserved components model in a predictable manner, consistent with models where forecasts are slow to adjust to new information. To estimate the response of the entire term structure of inflation expectations after a monetary policy shock, I use SVAR-IV methods proposed by Stock and Watson (2012) and Mertens and Ravn (2013) along with a now standard instrumental variable based on federal funds futures data as a proxy for exogenous interest rate fluctuations.====I find significant evidence of information frictions: outdated information accounts for 9% of inflation forecasts. This estimate is statistically significant at conventional levels and falls at the lower end of the existing range of estimates. I then show that a monetary policy contraction twists the term structure of inflation expectations, raising short-run expectations while lowering long-run expectations. This response can be explained by a temporary increase in the short-run component and a decrease in the long-run component. For the first several years after a monetary policy shock, the short-run effect dominates, so that inflation expectations increase. But, in the long run, the temporary response of the short-run component fades and expectations decline because of the more persistent long-run component. A temporary monetary policy shock that raises interest rates by 100 basis points lowers long-run expectations by about 12 basis points, which suggests that expectations are not as well anchored as may be thought. Rather, long-run expectations demonstrate non-trivial variability over the sample period, about 30% of which can be explained by monetary policy actions. Of particular interest, unconventional monetary policy actions undertaken between 2008 and 2015 put sustained upward pressure on long-run expectations through the zero lower bound period. In addition, failing to account for information frictions in the survey data underestimates the contribution of monetary policy shocks to variation in long-run inflation expectations.====Several recent papers, including Beechey et al. (2011), Bauer (2015), and Diegel and Nautz (2021), have studied the response of inflation expectations to monetary policy actions while Aruoba (2020), Mertens and Nason (2020), and many others have estimated term structure models of these expectations. A contribution of this paper is to draw on methods from both of these research areas by identifying the effects of a monetary policy shock within a term structure model augmented to include observable macroeconomic series. This allows forecasts at all available horizons to be used when studying the response of expectations to a monetary policy shock. The term structure model is estimated using forecasts of inflation at 17 distinct forecast horizons, spanning one quarter to ten years ahead, all of which are incorporated in a consistent manner using the structure of the underlying statistical model. Although survey forecasts differ by forecast horizon, each can be written as a function of two latent model variables, representing short- and long-run components of expectations, which provides considerable dimensionality reduction and allows the common information in survey forecasts at all horizons to be incorporated into the analysis in a consistent manner. Because estimation uses forecasts at all horizons, the model then delivers credible estimates of the response of expectations at all horizons to a monetary policy change.====There are several reasons to be interested in the response of expectations at all horizons to a monetary policy change. First, as central banks have run up against the lower bound of nominal interest rates, they have turned their attention to ways to lower interest rates further out on the yield curve. To understand the full effects of these policies it is essential to understand the response of the entire term structure of inflation expectations, because inflation expectations make up an important component of the term structure of interest rates, as demonstrated by Bauer (2015). Second, we may expect inflation expectations at different horizons to respond differently to monetary policy shocks. The conventional view of monetary policy is that higher interest rates lower inflation after a lag of several years. The results of Romer and Romer (2004) and Gertler and Karadi (2015), for example, support this view. If expectations are consistent with actual inflation, as they are in the vast majority of theoretical models, then they should respond in the same manner. Further, if the central bank has a credible inflation target then long-run expectations should be stable at that target and not respond to short-run policy moves.====There are reasons to expect that the response of inflation expectations is inconsistent with the conventional view, both in the short run and the long run, when expectations are not full-information rational expectations. Melosi (2016) shows that, in a model where firms have incomplete information similar to the noisy information model proposed by Woodford (2003), monetary policy shocks amount to a public signal, revealing information about the monetary authority’s information set. Incomplete information thus leads to a signaling channel where contractionary monetary policy shocks signal positive demand shocks. Inflation expectations respond to these shocks with a delay, but also become disanchored and do not return to trend even five years after the shock. The signaling channel arises in this model because information is dispersed: agents receive private noisy signals about the true state of the economy so that the public signal—the monetary policy change—provides additional information about the state of the economy. If agents have full information, the public signal reveals nothing and expectations respond only to the anticipated policy effect. To properly test for evidence of such an information effect it is thus essential to account for information frictions while estimating the term structure of expectations.====Several other recent papers, including Beechey and Wright (2009), Gürkaynak et al. (2010), Bauer (2015), Nakamura and Steinsson (2018), and Diegel and Nautz (2021) have demonstrated a significant response of long-run inflation expectations to monetary policy shocks and macroeconomic news, consistent with the view that long-run inflation expectations in the United States are not well anchored. The results of this paper contribute to this literature by demonstrating that the decline of long-run inflation expectations can be explained by a decline in the long-run component of the unobserved components model. This explains the finding by Melosi (2016) that expectations do not return to trend even many years after the shock.====The rest of this paper proceeds as follows: Section 2 presents the unobserved components model. Section 3 reports estimates of the model parameters and the unobserved components. Section 4 discusses the identification strategy and shows the effects of monetary policy on inflation expectations. Section 5 shows that the main findings are not sensitive to several changes to the specification and Section 6 concludes. An Online Appendix contains a complete list of all the measurement equations in the state space model, and details of the identification method and of the sensitivity analysis.",Monetary policy and the term structure of inflation expectations with information frictions,https://www.sciencedirect.com/science/article/pii/S0165188922002913,17 December 2022,2022,Research Article,38.0
"Anton Ramona,Chenavaz Régis Y.,Paraschiv Corina","Sorbonne Université, IMJ-PRG, Paris 75005, France,Kedge Business School, Domaine de Luminy, Marseille 13009, France,Université Paris Cité, LIRAES, Paris 75006, France","Received 8 June 2022, Revised 28 November 2022, Accepted 12 December 2022, Available online 16 December 2022, Version of Record 2 January 2023.",https://doi.org/10.1016/j.jedc.2022.104586,Cited by (1),"The price-quality relationship receives attention from both practitioners and scholars. Yet, the reference price effect, a central aspect in modeling consumer behavior, is absent from the debate. This article addresses the role of the reference price dynamics in understanding the price-quality relationship. We develop an ==== model of the dynamic behavior of a firm setting price and quality investment over time, accounting for a reference price effect based on prior selling prices. We contribute to the literature on the price-quality relationship by highlighting the conditions under which better quality may drive a lower price when consumers are prone to a reference price. Analytical results show that a counterintuitive negative price-quality relationship may appear even with a linear demand function. Numerical results emphasize the importance of the initial market conditions in the negative price-quality relationship.","The relationships between price and quality, on the one hand, and between price and reference price, on the other hand, have been extensively studied in the literature. Yet, the surveys of dynamic control models of pricing, quality and reference price (Chen, Chen, 2015, Den Boer, 2015, Elmaghraby, Keskinocak, 2003) reveal the separate analysis of these two relationships. In this paper, we link these two streams of research by investigating how consumer reference price dynamics may affect price-quality relationship.====Prior theoretical research focusing on firms’ pricing decisions shows that the price exerts an informational role, with higher prices signaling better product quality (Gardner, 1971, Gavious, Lowengart, 2012, Gerstner, 1985, Jin, Kato, 2006). Yet, more recent research suggests that the price-quality relationship may also be negative, for instance, when producing higher quality leads to a lower production cost because of subventions or regulation issues (Heinsalu, 2021, Kim, Berg, 2017). The sign of the price-quality relationship also generates empirical debates, with empirical studies reporting mixed results (Olbrich, Jansen, 2014, Olbrich, Jansen, Hundt, 2017, Völckner, Hofmann, 2007, Yan, Sengupta, 2011, Zhou, Su, Bao, 2002).====The aim of this paper is to understand the dynamics of price-quality relationship, and why it may be negative when the consumer is prone to a reference price effect. Behavioral literature highlights that consumers use a reference price (i.e., a psychological price used as a benchmark to judge the current transaction) to decide whether to buy or not a product (Den Boer, Keskin, 2022, Kopalle, Rao, Assuncao, 1996, Mazumdar, Raj, Sinha, 2005, Nasiry, Popescu, 2011, Popescu, Wu, 2007, Sorger, 1988). The reference price depends on the history of product’s past prices. Therefore, it may be affected by firm’s dynamic pricing strategy. Surprisingly, prior investigations of the price-quality relationship in a dynamic setting disregarded the reference price effect. Our paper fills the gap by analyzing the price-quality relationship, while accounting for the psychological element of consumer behavior.====In this article, we study the dynamic pricing policy of a firm making investment decisions in product quality on a market with a reference price effect on consumer’s side. More precisely, consumer purchasing behavior depends not only on product price and quality, but also on a dynamic reference price. The literature on dynamic pricing dealing with price-quality relationship and that on behavioral decision-making dealing with consumer reference price both inform this research.====Our theoretical proposal builds on recent contributions investigating the price-quality relationship in a dynamic setting with the goal to provide theoretical support to a negative price-quality relationship. Chenavaz (2017) shows that better product quality may be associated with a lower product price because of a sales effect. The sales effect measures the changes in consumer behavior when confronted to greater quality sold at a lower price. If the sales effect is strong enough, then a negative price-quality may emerge where the firm charges less for better quality. Ni and Li (2019) extend the analysis by considering the advertising expense of the firm and the corresponding carry over effect on goodwill. Introducing advertising and goodwill may be sufficient to change the sign of the price-quality relationship. Vörös (2019) further shows the role of the salvage value and of the operational efficiency in the price-quality relationship. Salvage value has a great impact on the quality dynamics and operational efficiency softens quality dynamics. Independently of the above quoted research, which uses continuous time, Kim and Berg (2017) use a two period model and point to the possibility of a negative price-quality relationship based on the Schmalensee effect. In contrast to prior analyses, in this paper, we integrate the reference price in the analysis of the price-quality relationship. Our goal is to enrich prior literature with an alternative explanation to the negative price-quality relationship based on consumer psychology. The reference price represents a key element in modeling consumer behavior in dynamic settings, as highlighted by Fibich et al. (2003), Popescu and Wu (2007), and Den Boer and Keskin (2022).====Building on the above cited research, we analyze the dynamic pricing and quality investment policies of a firm facing a reference-dependent demand. To the best of our knowledge, this is the first attempt to jointly examine the consumer psychological element and the price-quality relationship. Our modeling relies on a general demand function, that is, we assume little restriction on consumer behavior. We further complete the analysis with a focus on the classical linear demand function. Our results propose new dynamic pricing and quality investment formulas. They also measure how the dynamics of the selling price relates to the dynamics of product quality and to the dynamics of the reference price. A firm disregarding the psychological element of consumer would set a too low price, thus degrading future profit. The consideration of consumer behavior offers a more comprehensive understanding of the dynamic pricing and quality policies of the firm.====The rest of this paper is organized as follows. Section 2 presents the research background. Section 3 deals with the structural model, and Section 4 with the analytical results. The parametrical model is presented in Section 5 and the numerical results in Section 6. Section 7 concludes with a discussion of our main findings.","Dynamic pricing, reference price, and price-quality relationship",https://www.sciencedirect.com/science/article/pii/S0165188922002895,16 December 2022,2022,Research Article,39.0
"Kukacka Jiri,Sacht Stephen","Czech Academy of Sciences, Institute of Information Theory and Automation, Pod Vodarenskou vezi 4, 182 00 Prague 8, Czechia,Charles University, Faculty of Social Sciences, Institute of Economic Studies, Opletalova 26, 110 00 Prague 1, Czechia,Kiel University, Department of Economics, Olshausenstrasse 40, Kiel 24118, Germany","Received 24 March 2022, Revised 2 December 2022, Accepted 6 December 2022, Available online 9 December 2022, Version of Record 17 December 2022.",https://doi.org/10.1016/j.jedc.2022.104585,Cited by (0),This paper addresses the issue of empirical validation of ,"The implications of the departures from fully rational agents have become one of the central issues in macroeconomic modeling. As suggested by Simon (1972), economic agents act under bounded rationality (BR) instead of perfectly processing all information using unlimited cognitive capacities. In most situations, human behavior follows simple decision rules known as behavioral heuristics that have proven to be suitable in the past.====The theoretical literature has long attempted to formalize deviations from rational expectations (RE) and to describe the decision making of boundedly rational heterogeneous agents. One such possible macroeconomic framework is a business cycle model with heuristics and a nonlinear switching mechanism (De Grauwe, 2010). However, this approach has not yet been satisfactorily reflected in the empirical literature, and it is often difficult to identify certain behavioral effects in the current nonlinear macroeconomic models. This has important implications regarding monetary and fiscal policy interventions because business cycle dynamics and the volatility in key economic variables like inflation might heavily depend on turnovers in the agents’ decision process under BR. For example, Cornea-Madeira et al. (2019) state that the degree of behavioral heterogeneity varies considerably over time and that monetary policy should be aware of potentially destabilizing heterogeneous expectations. Hommes et al. (2019) argue that in a behavioral environment under a flexible central bank inflation targeting regime, i.e., given a simultaneous attempt to stabilize both the output gap and the inflation rate, the volatility of the latter decreases only up to a certain point but increases after this threshold is passed. We thus propose a new econometric method that allows estimating these models, especially to identify important behavioral parameters crucial to monetary policy under BR.====As a point of departure, two very recent key contributions that we directly build on are De Grauwe and Ji (2020) and Hommes et al. (2019). De Grauwe and Ji (2020) study the effects of structural reforms regarding price flexibility and labor market rigidities in a hybrid heuristic switching New-Keynesian model (NKM), i.e., a model framework with leads and lags. The authors show that their model accounts for the empirical observation of a non-Gaussian distributed and highly persistent output gap. Hommes et al. (2019) consider a forward-looking model similar to that by De Grauwe and Ji (2020) but with a higher number of forecasting heuristics to choose from. The microfoundations of the corresponding heuristics based on experiments are presented in Anufriev and Hommes (2012). A mechanism for dynamic endogenous switching of economic agents between groups applying particular heuristics is the multinomial logistic model (Brock and Hommes, 1997). This approach has become a prominent feature of models in financial economics over the last two decades (Dieci, He, 2018, Franke, Westerhoff, 2012, Hommes, 2006, among others) and recently also in dynamic stochastic general equilibrium (DSGE) models.====Although the related econometric research has made significant progress in the last decade, using these types of models with the empirical data remains challenging. First, the choice of empirical variables is not straightforward, as standard econometric tools assume the stationarity of the input data that is often violated for macroeconomic time series. Second, because of a relatively large number of unknown parameters, some of the coefficients might not be identified. This requires new tools for parameter estimation (Del Negro, Schorfheide, Smets, Wouters, 2007, Kleibergen, Mavroeidis, 2014, Kulish, Pagan, 2017). Third, although nonlinear models can be represented in the state-space form, analytical solutions of the joint probability density seldom exist to allow for, e.g., maximum likelihood inference. Finally, because of the nonlinear structure, “a possibly non-monotonic likelihood surface ... tends to be rugged making it challenging to find a global optimum” (Lux and Zwinkels, 2018). The presented challenges and rapidly increasing operational capacity of computers have motivated simulation-based estimation methods that make empirical inference possible for analytically intractable models.====We propose a simulated maximum likelihood (SML) estimation method, which addresses the four abovementioned issues. Our main contribution lies in the modification of the univariate SML estimator and its transfer from financial econometrics (Altissimo, Mele, 2009, Kristensen, Shin, 2012, Kukacka, Barunik, 2017, Lee, Song, 2015) to macroeconomic optimization. We first pursue a simulation study to test its performance in a controlled environment. Compared to the standard maximum likelihood estimator (MLE), the SML technique numerically approximates each observation’s conditional density via a standard kernel method. The derivation of the SML estimator is then similar to the MLE. Through a kernel approximation, the method elegantly bypasses the distributional assumptions of the MLE and the Bayesian approach, or the moment selection problem of the simulated method of moments (SMM). It also leads to a smooth surface of the simulated likelihood function, which generally supports optimization search. Finally, we illustrate the first empirical application of the multivariate SML for the estimation of a heuristic switching NKM.====As an empirical novelty, we report statistically significant estimates for the intensity of choice of macroeconomic forecasters forming expectations about the future output gap and inflation rate, which is a parameter that governs the multinomial logistic switching mechanism. The intensity of choice drives the agents’ sensitivity regarding the individual heuristics, and its value is crucial for the switching process from one group to another. Note that this parameter is crucial for stability in nonlinear models with regime-switching, which gives rise to the possibility of bifurcation (Gaunersdorfer et al., 2008). This is of serious concern when these macroeconomic models are used for policy consulting. For instance, Cornea-Madeira et al. (2019) provide a summary of observations taken from the literature indicating that a unique stationary solution under RE does not necessarily prevail under BR. Instead, multiple equilibria and complex dynamics might occur when applying the multinomial logistic switching approach. Using simulations, we first show that the SML method can identify this important parameter for the intensity of choice. We then successfully identify the empirical value of the intensity of choice for the US economy, which is approximately between 1.4 and 1.5. We thus find the switching process to be less intense, i.e., with a low tendency towards coordinating all agents to adopt a dominant forecasting strategy. We also estimate all additional behavioral parameters together with structural parameters, especially the Taylor rule coefficients.====To summarize, the main contribution of this paper is the following. First and foremost, we introduce a new powerful estimation method into behavioral macroeconometrics and verify its performance for use in practice. Second, from an econometric perspective, we modify its univariate version and transfer it from financial econometrics to a multivariate macroeconomic setup. Third, methodologically, we bridge the gap between macro lab experiments and empirical estimation of behavioral macroeconomic models. Finally, from an empirical and policy-making perspective, we estimate the behavioral parameters of the US economy. In particular, we are able to reliably identify the intensity of choice for macroeconomic forecasters of output and inflation, which can be seen as crucial when conducting specific policy interventions.====This paper proceeds as follows. The next section provides an overview of the related literature. Section 3.2 describes the heuristic switching NKM. In Section 4, we introduce the SML estimation method modified for multivariate macroeconomic models. Section 5 presents a Monte Carlo simulation study of the suggested estimation framework’s performance before we discuss the empirical results in Section 6. Finally, Section 7 concludes the paper. The technical details and additional results are relegated to a regular Appendix and an ====.",Estimation of heuristic switching in behavioral macroeconomic models,https://www.sciencedirect.com/science/article/pii/S0165188922002883,9 December 2022,2022,Research Article,40.0
"Xepapadeas Anastasios,Yannacopoulos Athanasios N.","Department of International and European Economic Studies, Athens University of Economics and Business and Department of Economics, University of Bologna, 1, piazza Scaravilli 40126 Bologna Italy,Department of Statistics, Athens University of Economics and Business, Athens Greece","Received 27 July 2021, Revised 8 October 2022, Accepted 4 December 2022, Available online 7 December 2022, Version of Record 18 December 2022.",https://doi.org/10.1016/j.jedc.2022.104584,Cited by (1),"Spatiotemporal dynamics are introduced in a standard Ramsey model of optimal growth in which capital moves toward locations where the marginal productivity of capital is relatively higher and initiate a study of the effects of nonlinear capital transport in terms of a linearized analysis around steady state solution solutions.====The potential spatial heterogeneity of optimal growth, as seen from the point of view of an optimizing social planner, is examined. Our results suggest that for a high utility discount rate, the spatial capital flows induce the emergence of optimal spatial patterns, while for a low utility discount, a flat earth steady state is socially optimal. Furthermore, when spatial heterogeneities exist due to total factor productivity differences across locations, we identify conditions under which the spatial capital flows could intensify or weaken spatial ====.","Optimal growth theory, in the context of both traditional and new growth theory, has been studied in a temporal domain (e.g., Acemoglu (2009); Aghion and Howitt (1998); Barro and Sala-i Martin (2004)). The explanation of the temporal evolution of key variables – such as output or capital per capita, the capital-output or the capital-labor ratio, or the evolution of positive externalities with temporal structure – has been central to dynamic optimal growth models.====However, space and geography seems to be important when studying economic growth; Acemoglou (2008, chapter 1) points out the great inequality in income per capita and income per worker across countries, and the increase in this inequality across nations between 1960 and 2000. Xepapadeas and Yannacopoulos (2016) provide evidence suggesting that geographical (or spatial) heterogeneity of per capita GDP increased between 1980 and 2011 across 11 world regions. Despite, however, the profound importance of the combined temporal and spatial dimension in the study of economic growth, little attention has been given to incorporating space into models of optimal growth.====Spatiotemporal models of economic growth started appearing in the 2000s.==== Earlier research which has provided the main mechanism for capital flows across space can be found in Isard and Liossatos (1979). A central reason for introducing spatial aspects in growth could be the question posed by Quah (1996, p. 1053): “What one wants to know here is, what happens to the entire cross sectional distribution of economies, ==== whether a single economy is tending towards its own, individual steady state.” Answering such a question implies that the growth process should be defined in terms of the temporal evolution of the spatial distribution of capital or output when there are nontrivial interactions across locations.====This paper explores spatial growth by developing a spatial Ramsey optimal growth model in a two-dimensional spatial domain. In this domain, capital located at a certain spatial point has the tendency to move to locations where the marginal productivity of capital is higher relative to the marginal productivity in the location of origin. The assumption of the marginal-productivity-driven (MPD) capital flows was first introduced by Xepapadeas and Yannacopoulos (2016) and differs from the assumption underlying capital flows which is used in much of the recent literature on spatial growth (e.g. Boucekkine et al. (2013b); Brito (2004); Fabbri (2016)). In this literature, capital flows across locations are modeled through a trade balance approach with respect to a closed region. This approach leads to a model of classic linear local diffusion with a constant diffusion coefficient which implies that capital moves from locations of high concentration to locations of low concentration. The MPD assumption about capital flows adopted in this paper seems to be plausible but it leads, however, to a model of nonlinear local diffusion. This introduces new challenges in the solution of the optimization problem, which is required in order to study optimal growth in a spatiotemporal domain.====In this context, the present paper contributes to growth theory by: (i) extending the standard optimal growth Ramsey model with a traditional neoclassical production function exhibiting diminishing returns to capital, to a two-dimensional spatial domain in which capital flows toward locations of high marginal productivity; (ii) developing a maximum principle to the solution of dynamic optimization problems, where capital accumulation is described by a partial differential equation with nonlinear diffusion; and (iii) providing results on optimal spatiotemporal dynamics, especially regarding the endogenous emergence of spatial patterns both analytically and numerically. To the best of our knowledge the solution of the standard Ramsey problem with capital accumulation governed by nonlinear diffusion – induced by MPD flows – and the explicit identification of conditions for the endogenous emergence of optimal spatiotemporal patterns from a spatially homogenous state is new.====By solving the spatiotemporal Ramsey problem we try to discuss issues which could emerge when the spatial dimension is combined with MPD capital flows across locations in the context of a social planner’s model with spatial externalities. More specifically, we seek to explore two questions.====First, suppose that the economies located within a bounded spatial domain with symmetric production functions converge in the long run to a “flat earth” – using the terminology of Krugman (1998) and Fujita et al. (2001) – steady state in which per capita output and capital is the same across all locations. Is it possible, when MPD capital flows across locations take place, for a small perturbation of the flat earth capital-labor ratio across locations to induce spatial heterogeneity to capital and output per capita, which persists and eventually drives the economies to a “non-flat earth” steady state, or will the perturbation die out with the passage of time – in which case, capital flows will be a driver which homogenizes the economies across locations?====Second, suppose that in a flat earth economy there is a perturbation in total factor productivity (TFP) which, without MPD capital flows will eventually drive the economies to a spatially-heterogeneous steady state with respect to capital and output per capita. If, along with the TFP perturbation, capital starts flowing to locations with higher marginal productivity, will the economies be driven to a more or less spatially-heterogeneous steady state relative to the case of no MPD capital flows?====The first questions explores whether or not optimal growth when capital is seeking locations of high marginal productivity promotes spatial inequalities. The second explores whether, in a world with TFP differences across locations, which can be connected with nature-first arguments, optimal growth with capital seeking locations of high marginal productivity intensifies or weakens spatial inequalities.====Our main results, in relation to the questions posed above, are that when production functions are symmetric across locations, optimal growth with MPD capital flows could lead to a growth process in which output and capital per worker are different across locations if the utility discount rate, the share of capital in a Cobb-Douglas production function and the elasticity of marginal utility are sufficiently high. For the conventional low discount rate, low capital share and low elasticity of marginal utility, MPD capital flows will act as a homogenizing factor and reduce inequalities after a spatial perturbation of the capital per capita ratio. On the other hand, under spatial TFP differences, optimal growth under MPD capital flows could intensify or diminish spatial inequalities depending on the specific characteristics of capital flows. These issues have been studied in the literature (e.g., Boucekkine, Camacho, Fabbri, 2013, Boucekkine, Fabbri, Federico, 2016, Boucekkine, Fabbri, Federico, Gozzi, 2019; Xepapadeas and Yannacopoulos (2016)). However, we believe that by replacing the Fickian assumption about capital diffusion with the MPD assumption and by developing the appropriate maximum principle, well-founded insights regarding the endogenous emergence and the strengthening or weakening of spatial inequalities by MPD capital flows can be provided.====The rest of the paper is organized as follows. Section 2 models capital flows under the assumption that the flows are driven by marginal productivity differentials across locations and defines the spatial Ramsey model. Section 3 extends Pontryagin’s principle under nonlinear diffusion. Section 4 discusses the formation of spatial patterns in the Ramsey model and the relation between spatial heterogeneity and capital mobility, while section 5 concludes. All proofs are relegated to the Appendix.",Spatial growth theory: Optimality and spatial heterogeneity,https://www.sciencedirect.com/science/article/pii/S0165188922002871,7 December 2022,2022,Research Article,41.0
"Ali Khan M.,Zhang Zhixiang","Department of Economics, The Johns Hopkins University, Baltimore, MD 21218, USA,China Economics and Management Academy, Central University of Finance and Economics, Beijing, 100081, China","Received 15 January 2022, Revised 14 November 2022, Accepted 3 December 2022, Available online 5 December 2022, Version of Record 11 January 2023.",https://doi.org/10.1016/j.jedc.2022.104583,Cited by (0),"This paper shows that the introduction of uncertainty in the two-sector model due to Robinson-Solow-Srinivasan (RSS) fully subdues the veritable plethora of the results that have been obtained the theory of deterministic optimal growth. Rather than an “anything goes” theorem that admits optimal cyclical and chaotic trajectories for the discrete-time deterministic version, we present results on the existence, uniqueness, ==== and comparative-static properties of the steady state measure. We relate the basic intuition of our result to global games, and note that the properties of value and policy functions we identify rely on “supermodularity” and “increasing-differences property” of Veinott-Topkis-Milgrom-Shannon. While of interest in themselves, our results highlight a methodological advance in developing the theory of optimal growth without Ramsey-Euler conditions.","This paper is a contribution to a rich literature on the theory of stochastic optimal economic growth initiated in the seventies, and by now enjoying the status of a mature theory capable of calibration and empirical implementation with ongoing extension and development. However, the basic point that we articulate seems novel and goes against the grain of fundamental intuitions that have been garnered so far. It is simply that adding stochasticity to a rather straightforward extension of the Ramsey-Cass-Koopmans (RCK) can eliminate some equilibria: adding noise can remove the complexity of the optimal dynamics. In contrast to the deterministic case, the stochastic treatment “mops up” the tedious taxonomy of the several “anything goes” theorems pertaining to optimal trajectories and associated bifurcations that have been documented: only stable behavior appears in the long run in the stochastic RSS model. Unlike Merton and Mirrlees, the stochastic setting gives order and a streamlining to the theory.====In addition to this substantive point, we also see the paper making some technical advances that may be of methodological significance. To begin with, we note that the two-sector RSS model is a hybrid that naturally falls between the RCK one-sector neoclassical aggregative model and the two-sector setting of Uzawa-Srinivasan, one that dispenses with the assumption of a linear production possibility transformation and an an instant or costless labelling of a single commodity that can be used as a consumption or an investment good. The RSS set-up does this in most parsimonious fashion: it allows depreciation of machines, a Leontieff technology for the manufacture of the consumption good and precludes machines being used to produce machines. Despite its simplicity precluding the use of the Ramsey-Euler conditions, we can show the existence, uniqueness and asymptotic stability of an invariant measure, and also note some basic comparative static properties of the resulting equilibrium. We make full use of the Veinott-Topkis’ order-theoretic approach based only on ordinal relationships of economic variables, and without any reliance on Inada conditions on the technology or on the planner’s preferences, and also rely on Foster-Lyapunov (“drift”) criteria to get asymptotic properties, including locally uniform geometric rate of convergence.====The work that we present here goes against the grain of two, not entirely uncontroversial and mutually reinforcing, intuitions of economic dynamics: first, stemming from May (1976), the claim that descriptive or optimum dynamics of stochastic models are observational equivalent to simple deterministic models; and second, a criterion that we see as stemming from Merton (1975) and Stigum, 1963, Stigum, 1972 as to whether unknowns of its deterministic formulation are unbiased estimators of those in its stochastic rendering, a rather more sophisticated version of the check whether the special case of a singleton state space of the uncertainty set-up reduces to a representation with full certainty.==== Mirrlees (1965) had already asked, “granted that the stochastic specification of the model provides a more accurate description of the economy than the deterministic version, whether it makes any great difference.” What the two-sector RSS story shows is that it makes a whole lot of difference, and the question to be asked and answered is why this is so? Why do the bifurcations leading to a veritable plethora of strange attractors and dynamic outcomes of the deterministic version disappear? and what is gained and lost when they do so?====For this, the connection to the literature on global games merits discussion.==== The essential thrust of this literature, as pioneered by Carlsson and van Damme (1993) and pursued in a macroeconomic context in Morris, Shin, 1998, Morris, Shin, 2001, Morris, Shin, 2003 and their followers, sees the introduction of uncertainty in an entirely different way than the earlier literature. As Atkeson (2001) notes:====The uniqueness of the steady state measure of the stock of machines and the consumption good, and the convergence of the temporal trajectory of the measures to it allowing comparative static exercises, all with the introduction of noise, is precisely what we obtain in the results reported below. Admittedly, ours is a Ramseyian planning exercise and not a game with interacting players’ beliefs, but there is surely a connection in that a basic parameter of the deterministic RSS model, the marginal rate of transformation, ==== of capital today into capital tomorrow in a regime of no consumption constitutes a sufficient statistic for the variety of dynamic outcomes, and it is being continuously perturbed. In some sense, a planning exercise can be fruitfully viewed as a game of countable multiple-selves interacting with some imprecison.====The plan of the paper is straightforward. After a quick introduction to the model in Section 2, and a recapitulation of the results already obtained for the RSS model, we proceed in Section 3 to the principal results. After the basic analysis of the value and policy functions, we present the principal result as Theorem 1, supplemented by two corollaries and one comparative static-proposition on the monotonicity of the value function in terms of the distribution of the shock. This section also discusses the deeper implication of getting rid of some equilibria by attempting a succinct guide to the relevant RSS literature as well as that on global games. Section 4 presents the proof of the results: a sketch of the formal argumentation followed by the formal proofs.",The random two-sector RSS model: On discounted optimal growth without Ramsey-Euler conditions,https://www.sciencedirect.com/science/article/pii/S016518892200286X,5 December 2022,2022,Research Article,42.0
"Bianchi Francesco,Bianchi Giada,Song Dongho","Department of Economics, Duke, 213 Social Sciences building, Box 90097, JHU, CEPR, and NBER, Durham, NC 27708, United States,Department of Medicine, Division of Hematology, Brigham and Women’s Hospital, Harvard Medical School, United States,Carey Business School, John Hopkins University, JHU Carey, 100 International Drive, Baltimore, MD 21202, United States","Received 9 August 2022, Revised 18 November 2022, Accepted 1 December 2022, Available online 5 December 2022, Version of Record 14 December 2022.",https://doi.org/10.1016/j.jedc.2022.104581,Cited by (3),"We adopt a time series approach to investigate the historical relation between unemployment, life expectancy, and mortality rates. We fit Vector-autoregressions for the overall US population and for groups identified based on gender and race. We use our results to assess the long-run effects of the COVID-19 economic recession on mortality and life expectancy. We estimate the size of the COVID-19-related unemployment shock to be between 2 and 5 times larger than the typical unemployment shock, depending on race and gender, resulting in a significant increase in mortality rates and drop in life expectancy. We also predict that the shock will disproportionately affect African-Americans and women, over a short horizon, while the effects for white men will unfold over longer horizons. These figures translate in more than 0.8 million additional deaths over the next 15 years.","Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is the pathogenic agent of coronavirus disease 2019 (COVID-19), see ====. Initially reported as an outbreak in the province of Wuhan, China at the end of 2019, COVID-19 was recognized as a pandemic by the World Health Organization (WHO) on March 11, 2020, e.g., ====. Approximately 74 million cases and 1.6 million deaths have been reported worldwide, with over 17 million people infected and approximately 0.3 million deceased in the USA alone (updated December 16, 2020; see ==== and ====).====SARS-CoV-2 is shed by asymptomatic individuals and persists in the environment for days, implying that public health measures to halt virus spreading could be effective at reducing transmission and mortality, see ==== and ====. Universal masking, social distancing, contact tracing, and quarantine were later identified as effective tools to contain SARS-CoV-2 spreading, see ==== and ==== and ====The impact of the loss of income on psychological and physical health has been well documented in white males, see ====, ====, and ====. In epidemiological studies, unemployment at the individual level associates with decreased health and higher mortality, regardless of aggregate unemployment rate, see ====. A surge in suicide rates has been clearly observed in unemployed individuals, particularly men. Cardiovascular diseases peak in face of financial stress and preventive ontological care declines, thus contributing to excess mortality.====While the short-run trade-off between containing the COVID-19 pandemic and preserving economic activity has been extensively analyzed, there is currently no analysis regarding the long-term impact of the COVID-19-related economic recession on public health. What is more, most of the papers interested in the relation between the COVID-19 pandemic and economic activity argue, correctly, that lockdowns can save lives at the cost of reducing economic activity,==== but they do not consider the possibility that severe economic distress might also have important consequences on human well-being (==== and ==== method to assess the effects of an increase in unemployment on the life expectancy and mortality rates. The main message arising from our exercise is that the typical unemployment shock results in a significant decline in life expectancy and increase in mortality rates for the overall population. In normal times, the size of unemployment shock is around 0.79% on an annual basis and it is quite persistent. The effect of the unemployment shock on the growth rate of life expectancy and the death rate reaches its peak in the fourth year and remains elevated for a long time.====We repeat the exercise with data for different population groups identified based on race and gender to highlight substantial heterogeneity. We find that the size of the typical unemployment shock is much larger for African-Americans, with a ==== of around 1.05%, than for the White population (standard deviation around 0.74%). Specifically, African-American men typically experience the largest unemployment shocks, with a standard deviation of around 1.31%, approximately 60% larger than the typical shock experienced by White men. We find that the typical unemployment shock for White women is the smallest, around 0.63%, which is about 75% of the unemployment shock experienced by White men. Similarly, the size of a one-standard deviation unemployment shock for African-American women (0.92%) is about 70% of the typical shock experienced by African-American men, but its absolute magnitude is larger than that experienced by White men. In light of this evidence, it is perhaps not surprising that the effects of the typical unemployment shock on life expectancy and the death rate are more severe for the African-American population. However, we emphasize that this is not entirely the consequence of larger shocks, as the pattern largely persists when controlling for the size of the shock, especially for the case of life expectancy. When controlling for the size of the shock, we find that women present a relatively larger increase in death rates. This pattern is especially visible for White women, indicating that even if they generally suffer smaller shocks, they are disproportionately more affected by them.====To understand the channels behind our results, we extend the analysis to study the response of the death rates for the leading causes of death to an unemployment shock. Because of data availability, we focus on the overall population. We find that an unemployment shock leads to increases in death rates due to heart disease (the leading cause of death in the United States), stroke, influenza and pneumonia, and accidents, while we do not find a significant response of the death rate due to cancer. These results suggest that multiple channels might be at work. On the one hand, the effects on heart disease and stroke death rates indicate that access to preventive care and lifestyle might play an important role in explaining why unemployment is followed by an increase in mortality rates. The increase in pneumonia/influenza-related mortality may be also explained based on the lack of access to preventive care and the decline in a healthy life style. On the other hand, the large effect on accidents also indicates that other channels are likely to be at work. Possible explanations are that people engage in more dangerous activities, spend more time driving in a less careful way, and are more subject to domestic accidents during periods of economic distress.====We then move to use our estimates to examine the long-run effect of the COVID-19 unemployment shock on life expectancy and the age adjusted death rates across difference races and gender. Our data for life-expectancy and death rates stop in 2017. However, we have unemployment data until 2020. We thus use observations for the unemployment rate to construct an estimate of the COVID-19 unemployment shock, while treating the corresponding ones for the life expectancy and mortality rates as missing observations. As before, we adopt an identification strategy based on a Cholesky decomposition. By comparing the magnitude of the COVID-19 unemployment shock to those in the normal (non-critical) times, we can infer the severity of the COVID-19 pandemic unemployment shock.====According to our estimates, the COVID-19 unemployment shock is about 3.64 standard deviations larger than the typical shock to the unemployment rate for the overall population (about 2.90% in magnitude). We estimate that this unprecedented unemployment shock will result in a 2.43% increase in mortality rates and a 0.83% drop in life expectancy over the next 15 years. Compared to the typical unemployment shock, we find that women (both African-American and White) are disproportionately affected relative to men. Particularly for White women, the COVID-19 unemployment shock is about 4.91 standard deviation larger (about 3.10% in magnitude) than their typical shock to the unemployment rate, by far the largest in relative magnitude with respect to the typical shock. However, African-Americans still suffer the largest shocks in absolute terms (approximately 3.4% for both African-American men and African-American women). As a result, the impact of the COVID-19 unemployment shock on the death rate is large for all groups, but visibly larger for African-Americans and White women. As explained above, this is in part the result of a larger shock, but also of a larger response conditional on the size of the shock.====The long-term effects of the COVID-19 related unemployment surge on the US aggregate mortality rate have not been characterized in the literature. Thus, as a last step, we compute an estimate of the excess deaths associated with the COVID-19 unemployment shock. This corresponds to the difference between the number of deaths predicted by the model with and without the unemployment shock observed in 2020. For the overall population, the increase in the death rate following the COVID-19 pandemic implies staggering 0.84 and 1.22 million excess deaths over the next 15 and 20 years, respectively. These numbers correspond to 0.23% and 0.33% of the projected US population at the 15- and 20-year horizons, respectively. For African-Americans, we estimate 200 thousand and 290 thousand excess deaths over the next 15 and 20 years, respectively. These numbers correspond to 0.38% and 0.52% of the projected African-American population at the 15- and 20-year horizons, respectively. For Whites, we estimate 0.76 and 1.09 million excess deaths over the next 15 and 20 years, respectively. These numbers correspond to 0.28% and 0.40% of the projected White population at the 15- and 20-year horizons, respectively.====Overall, our results indicate that, based on the historical evidence, the COVID-19 pandemic might have long-lasting consequences on human health through its impact on economic activity. We interpret these results as a strong indication that policymakers should take into consideration the severe, long-run implications of such a large economic recession on people’s lives when deliberating on COVID-19 recovery and containment measures. Without any doubt, lockdowns save lives, but they also contribute to the decline in real activity that can have severe consequences on health. Policy-makers should therefore consider combining lockdowns with policy interventions meant to reduce economic distress, guarantee access to health care, and facilitate effective economic reopening under health care policies to limit SARS-CoV-19 spread.====The idea that economic activity might affect human well-being has been studied before. Contrary to what might be expected, there is no widespread agreement on the effect of economic activity on mortality rates. ====, ====, ====, ==== and ==== argue for a procyclical relation between macroeconomic activity and mortality, with death rates increasing during periods of high employment. However, in a more recent contribution, ==== finds that since 1990 the relationship has become weak or non-existent. This seems to be due to a change in the composition of the causes of deaths. Specifically, fatalities due to cardiovascular disease and, to a smaller degree, transport accidents are procyclical, whereas cancer and some external sources of death (particularly accidental poisonings) have emerged as strongly countercyclical. ==== argue that individuals who are approaching retirement when a recession hits may be particularly likely to suffer long-lasting negative consequences, such as reduced longevity. With respect to these studies, our methodological approach is quite different, given that we take a time series approach, as opposed to panel regressions. This allows for a dynamic relation between the variables of interest and for a discussion of the effects of the national business cycle that in these studies is absorbed by the time fixed effect (see ==== for an excellent discussion). We see these two approaches as complementary (====, ====, ====, ====).====The evidence for a casual link from job loss to poor health is mixed in the literature. ==== find that there is no impact of job displacement on hospitalization for stress-related diseases for men. ==== find that the cross-sectional negative relationship between unemployment and self-assessed health is not found longitudinally. On the other hand, ==== provide evidence that displaced workers experience higher rates of mortality. ==== find that job loss in the years before retirement is associated with a higher risk of cardiovascular disease and death. ==== show that cohorts coming of age during a deep recession suffer increases in mortality later in their middle age. ==== studies the potential long-run effects of large-scale unemployment during the COVID-19 crisis focusing on vulnerable job losers and labor market entrants. He finds that these losses could be substantially larger than losses in potential life years from deaths directly due to COVID-19. Our results are in line with his findings, despite the different methodological approach taken in the paper. With respect to these contributions, our approach based on aggregate data allows for the possibility that unemployment, as a proxy for the overall performance of the economy, might affect mortality rates and health through indirect channels (e.g., income, crime rates, drug abuse,...).====Our benchmark results are based on an identifying assumption that relies on unemployment not having any contemporaneous effect on mortality rates and life-expectancy. Our results on the long-term effects of unemployment on mortality and life-expectancy are qualitatively unchanged when using a different identification strategy in which shocks to unemployment are allowed to have a contemporaneous effect on the other two variables. When using this alternative identification assumption, an interesting result emerges. For some groups, an increase in unemployment leads to a contemporaneous ==== in mortality rates and to a contemporaneous ==== in life-expectancy. However, error bands for this initial effect tend to be large and the response reverts in two-three years. In the long run, mortality increases and life expectancy declines. The long-term effect dominates and our key results on the long-term cumulative effects of unemployment remain unchanged. At the same time, these results could help reconcile the mixed evidence in the existing literature discussed above. On impact, unemployment can lead to a reduction in mortality as deaths due to work-related causes or motor vehicle accidents decline, but over time economic distress takes a toll on human well-being. We consider this an interesting direction for future research.====Our results add to the body of literature that analyzes the macroeconomic consequences of COVID-19. This literature is growing exponentially and we apologize to our colleagues for being unable to cite all relevant contributions here. A few papers that rely on historical pandemic episodes to provide plausible estimates for outcomes due to COVID-19 include ====, ====, and ====. Other papers study the interaction between economic decisions (e.g., optimal policy) and epidemics, e.g., ====, ====, ====, ====, ====, ====, ====, and ====. Among the existing papers, our work is more closely related to those that examine the medium- to long-term effects of pandemics such as ====The rest of the paper is organized as follows. ==== presents the data and the methodological approach. ==== presents the historical relation between shocks to unemployment and life expectancy and mortality rates. ==== studies the implications of the historical results for the COVID-19 unemployment shock. ==== concludes.==== We denote the “log average life expectancy,” “log age-adjusted mortality rate,” and the “unemployment rate” as ====, respectively. We allow for (potentially serially correlated but mutually uncorrelated) measurement errors ==== in the level series. The joint dynamics of the growth rates of ==== and ==== and the level of ==== follow a VAR(1). Put together,====Note that we can re-express (A-1) by====where====The state space representation is====where====For ease of exposition, we collect parameters in",The long-term impact of the COVID-19 unemployment shock on life expectancy and mortality rates,https://www.sciencedirect.com/science/article/pii/S0165188922002846,5 December 2022,2022,Research Article,43.0
"Bao Te,Corgnet Brice,Hanaki Nobuyuki,Riyanto Yohanes E.,Zhu Jiahua","School of Social Sciences, Nanyang Technological University 48 Nanyang Ave, 639818, Singapore,EM Lyon Business School 23, Avenue Guy de Collongue, Lyon, France,Institute of Social and Economic Research, Osaka University 6-1 Mihogaoka,Osaka, Ibaraki 567-0047, Japan,Ma Yinchu School of Economics, Tianjin University 92 Weijin Road, Tianjin 300072, China","Received 3 June 2022, Revised 17 November 2022, Accepted 23 November 2022, Available online 25 November 2022, Version of Record 19 December 2022.",https://doi.org/10.1016/j.jedc.2022.104571,Cited by (1),We investigate how individuals use measures of apparent predictability from price charts to predict future market prices. Subjects in our experiment predict both ,"The use of charts and graphical displays has a long history in markets (Lo, Hasanhodzic, 2011, Lo, Mamaysky, Wang, 2000). In financial markets, traders who look for price patterns and trends in historical information are called “chartists” and are widely considered the main driver of fluctuations in market sentiment and the subsequent price booms and busts (Chiarella, He, 2003, Chiarella, He, Hommes, 2006, Chiarella, Iori, Perelló, 2009, Frankel, Froot, 1990, Tedeschi, Iori, Gallegati, 2012). While one can disagree with their belief in “a picture is worth a thousand words”, it is nevertheless very important to understand what specific patterns people may refer to in the charts and pictures. Under traditional finance theory, financial markets are efficient and prices follow random walks. Market participants only use information regarding fundamentals to make price forecasts, and price charts are redundant. Yet, the efficient market hypothesis is unlikely to hold at all times, as recognized by Fama himself (Fama, 2014). In practice, traders incorporate apparently-irrelevant information into their price forecasts (Anufriev, Bao, Tuinstra, 2016, Bao, Duffy, 2021, Bao, Duffy, Zhu, 2021, He, Wang, Yang, 2022). Knowing how people use charts will not only be useful to understand practitioners’ behaviors, but it will also give us insights into the circumstances that trigger market inefficiencies.====In their influential paper, Barberis et al. (1998, BSV in below) argue that individuals make predictions based on the presumed predictability of past-price dynamics, namely, they tend to overreact to continuing trends in earnings while underreacting to earnings surprises. BSV propose that investors use the number of reversals in the sequence as an indicator of a future change in the earning regime. The primary experimental support of this model is provided by Bloomfield and Hales (2002, BH thereafter) through an experiment in which participants were shown the history of realization of some random walk time series and asked to predict the direction of the next move of the series. They found that, in line with previous studies, e.g., Kahneman and Tversky (1973) and Griffin and Tversky (1992), participants do not regard random walk sequences as random even after they are told so. In addition, they find that participants tend to predict a price movement in the opposite direction to the direction in the previous period for sequences with more reversals and make a trend-following prediction for sequences with fewer reversals.====While BSV and BH generate many useful insights in understanding how people make forecasts in asset markets, there appears to be many unanswered questions that require further investigation: (1) The BH experiment was conducted in 2002 with 38 MBA students of the Johnson Graduate School of Management at Cornell University. Can the result be generalized to other samples from a different culture or background knowledge in finance? (2) The price time series in BH can only go up or down by a constant step-length, making the number of reversals the only observable pattern in the data. However, stock prices typically exhibit a much richer set of features such as autocorrelation, seasonality, retreat, and volatility. Would subjects still mainly rely on the number of reversals as the main indicator of price momentum and basis of their forecasts? (3) Random walk time series are, by definition, unpredictable, and there is no way to evaluate subjects’ forecasting accuracy in the BH experiment. However, whether the market participant can predict the direction and size of the next price movement is crucial for him or her in the stock market. Does making price forecasts like a chartist add to one’s forecasting accuracy and profit in the stock market?====Motivated by the above questions, we run an experimental study to further examine financial forecasting based on patterns in past prices. The participants of our experiment are 81 undergraduate students from Singapore.====Our experiment consists of two parts. In Part 1 (Predicting Random Walk), participants go through the same set of tasks as in Part 1 of BH. Namely, after seeing a graph of price movements generated from a “random walk” model, participants submit their belief about the likelihood of the next price movement being up. We employ a Becker-DeGroot-Marschak (BDM) incentive-compatible mechanism (Becker et al., 1964). Participants repeat the task 16 times with different graphs. In Part 2 (Stock Price Prediction), participants are asked, after seeing a graph of daily stock price movements of a randomly selected stock over one year, to make a price prediction for 30 days after the last price shown on the graph (Bao et al., 2022a). This task is similar to the task in Glaser, Iliewa, Weber, 2019, Glaser, Langer, Reynders, Weber, 2007. Participants repeat this task 20 times. The payoff in Part 2 depends on the accuracy of the forecast. A smaller prediction error leads to a higher payoff. One methodological innovation of our paper is that we introduce moving average convergence divergence (MACD, Appel, 2005), a commonly used measure for momentum in technical analysis, and examine whether there is evidence that subjects use it to predict future price movements. Instead of the reversal of the sign of price changes in BH, we use the MACD reversal: a reversal is recorded if a bullish signal is followed by a bearish signal or the other way around. To the best of our knowledge, while MACD is widely used by chartists in financial markets, our work is the first to consider it in an experiment.====In general, the goal of Part 1 is to investigate whether the findings in BH can be replicated using a subject pool from a different cultural and professional background, and we add Part 2 to examine whether subjects will refer to other patterns in stock prices. We conducted a Lo-Mackinlay variance ratio test (Lo and MacKinlay, 1988) on stock price time series used in Part 2 and confirmed that we cannot reject the null hypothesis that the underlying data generating process is a random walk. Thus, the time series in this part should be considered as unpredictable as those in Part 1.====Because the information display and task in Part 2 are similar to the situation faced by traders in financial markets, Part 2 should be associated with a higher degree of “representativeness of the situation” than Part 1 (List, 2021). To the best of our knowledge, we are the first to study how a broad range of statistical properties of stock asset prices such as autocorrelation, amplitude, and volatility impact forecasting behavior in a laboratory experiment. Moreover, our within subject design allows us to explore whether there is heterogeneity in subjects’ type in momentum-chasing behavior, i.e., whether there is a positive correlation between the same subject’s level of overreaction in Part 1 and Part 2. Our findings are as follows:====For Part 1 of the experiment, i.e., the Random Walk Prediction task, we successfully replicate BH’s findings. We observe that participants are more likely to overreact to the random walk sequences when there are fewer reversals. Additionally, participants tend to make their predictions closer to 50% for sequences with more reversals.====Our findings from the Stock Price Prediction task can be summarized as follows: first, as in the BH experiment, subjects overreact less to stock price time series with more reversals in the random walk prediction task, but not in the stock price prediction task. Second, subjects are unable to predict price movements better than pure guessing. On average, their success rate is not significantly different from 50%. Third, the number of reversals appears to be a good indicator of the difficulty of forecasting future price movement. The chance for a subject to correctly predict the direction of the price movement is lower when the number of reversals is higher. Fourth, subjects pay attention to other features of the series such as volatility.====Furthermore, we do not observe any significant relation between overreaction behavior in the random walk prediction task and the stock price prediction task. This finding suggests that the overreaction behavior is driven mainly by the characteristics in the tasks/patterns in the time series instead of the subjects personal idiosyncratic characteristics.====Our results show that BH’s result is robust to changes in subject pool, though subjects may refer to different patterns when making price forecasts in the simulated random walk series and the stock price series. As in BH, this perceived predictability does not help them make more accurate price predictions in the stock market. Overall, our findings suggest prediction in financial markets is difficult. Although individuals try to play rationally by referring to different apparent patterns in different situations, stock prices are still largely unpredictable for them.====Besides BH, our paper relates to several strands of literature. Frieder (2008) explored how individuals extrapolate past news to provide an indication of future trends. Her study showed that after viewing positive news, investors tend to be more likely to buy. Moreover, Huber et al. (2010) conduct an experiment to evaluate the behavior of investors making decisions under risk. Subjects are asked to guess the outcomes of a series of coin tosses either by themselves or by relying on a prediction provided by “experts”. The hot hand belief is observed when subjects choose to rely on experts who were successful in the past. Gamblers fallacy is observed in those subjects who rely on themselves. Specifically, the frequency of betting heads increases after streaks of tails. Rötheli (2011) examine how subjects extrapolate patterns in time series to provide expectations of stock prices and exchange rates. Loh and Warachka (2012) conclude that investor expectations are influenced by trends in prior quarterly earnings surprises. Their evidence supports the gamblers fallacy in Rabin (2002), where investors appear to underreact to trends in earnings surprises.====Our paper is also related to the literature on heterogeneous expectations and regime-switching in learning to forecast experiments (LtFEs), e.g., Marimon et al. (1993), Hommes (2013), Assenza et al. (2014), Bao and Duffy (2016), Bao et al. (2017), Colasante et al. (2017), Hanaki et al. (2018), Bao and Zong (2019), Landier et al. (2019), Giamattei et al. (2020), He and Kucinskas (2020), Bao et al. (2021b), Bordalo et al. (2020), Hommes (2021), Hommes et al. (2021), Kopányi-Peuker and Weber (2021), Mokhtarzadeh and Petersen (2021), Zhu et al. (2021) and Bao et al. (2022b). Different from BH, this literature usually does not impose an exogenous data generating process on the asset price but lets it be endogenously determined as a function of the average price forecast by the subjects. This literature usually finds that agents have difficulty learning the rational expectations equilibrium of the economy, and their trend chasing expectations can lead to persistent bubbles and crashes in asset prices. However, because asset prices are endogenously determined in LtFEs, it is difficult for researchers to study how exogenous features of the series such as number of reversals, autocorrelation, and volatility influence forecasting behavior. Besides, our paper is also related to the literature on generalized trend chasing or hot hand fallacy, e.g., Camerer (1989), Offerman and Sonnemans (2004), Yuan et al. (2014).====Last, we explain whether the prediction behavior can be explained by other properties that are broadly related to the predictability of the data, e.g., Hommes and Zhu (2014). Our results suggest that the impact of price volatility of the stock series on individuals’ prediction exhibits similarity to that of the reversals of the stock series. That is, the higher the volatility of the stock series, the less accurate the prediction is. We also find that series exhibiting higher autocorrelation coefficients are less predictable. This result differs from Anufriev et al. (2016), Anufriev et al. (2019) who find that higher levels of autocorrelation are associated with a lower prediction error. The reason why we reach a slightly different conclusion from Anufriev et al. (2016), Anufriev et al. (2019) is that we use different time series. Anufriev et al. (2016), Anufriev et al. (2019) do not use random walks but time series data from the Brock-Hommes model and the moving average of stock indices. Subjects in their experiment make predictions in many periods, and the higher autocorrelation may indeed imply that they can make more accurate predictions by extrapolating the past trend. However, subjects in our experiment only make one prediction for each time series, and the change in values in random walks are not correlated. By extrapolating the trend, the prediction accuracy will be lower when mean reversion takes place.====The remainder of this paper is organized as follows. Section 2 summarizes the experimental design. Section 3 and Section 4 present the analysis of the experimental data. Finally, Section 5 concludes.",Predicting the unpredictable: New experimental evidence on forecasting random walks,https://www.sciencedirect.com/science/article/pii/S0165188922002743,25 November 2022,2022,Research Article,44.0
"CHEN Yuanyuan,WU Qi,LI Duan","Department of Finance and Insurance, Nanjing University, Nanjing, China,School of Data Science, City University of Hong Kong, Hong Kong,Laboratory for AI-Powered Financial Technologies Limited, Hong Kong","Received 27 April 2022, Revised 26 September 2022, Accepted 23 November 2022, Available online 25 November 2022, Version of Record 13 December 2022.",https://doi.org/10.1016/j.jedc.2022.104572,Cited by (0),"We propose a counter-cyclical initial margin model for option portfolios. Our model explores the intrinsic netting within a given portfolio of European options and outputs a constant upper bound of the maximum possible loss. This feature would allow option clearinghouses and regulators to gauge the tightest margin levels that are stable. We compare our model with the scenario-based SPAN model and the sensitivity-based SIMM model in terms of the netting efficiency and the procyclical property. Using the SPX options and the ==== swaptions as examples, we quantify the minimum amount of additional margins needed to make them fully counter-cyclical. We then show how to strike a balance between risk-sensitivity and counter-cyclicality if needed by mixing our model flexibly with a prevailing risk-sensitive margin model.","This paper studies the upper bounds of the maximum loss of an option portfolio and discusses the potential of using them to set stable initial margins during the Margin Period of Risk (MPOR). Whether a derivative contract trades through an exchange or bilaterally over-the-counter (OTC), the trade needs to be cleared and settled in the subsequent one to ten days (the MPOR). The clearing counterparties require their clearing members to post collateral in the form of initial margins to fend against potential loss during the settlement period, and initial margins are typically designed to be risk sensitive. The prevailing methodologies of setting initial margins for derivatives typically gain their risk sensitivity either through historical simulation====, or through a set of pre-defined scenarios, or through the sensitivities of the product with respect to the underlying and the volatility. Examples include the Standard Portfolio Analysis of Risk (SPAN) method practiced by Chicago Merchandise of Exchange (CME)====; the System for Theoretical Analysis and Numerical Simulation (STANS) adopted by the Options Clearing Corporation (OCC)====; and the Standard Initial Margin Model (SIMM) proposed by the International Swaps and Derivatives Association (ISDA)====.====Risk-sensitive margin requirements tend to be procyclical in the sense that they can amplify shocks Murphy et al. (2014). Volatility spikes lead to margin calls on clearing members and market stress such as elevated cost of funding is likely to be corrected with the increase of volatility. Clearing members therefore need to post additional collateral precisely at the time when it becomes most difficult to raise funding. As the procyclicality in margin requirements threatens financial stability Constâncio (2016); European Systemic Risk Board (2017), global regulators propose countermeasures such as adding additional margin buffers, placing higher weights on stressed scenarios, and lengthening the historical look back window (Duffie, 2018, European Commission, 2013, Murphy, Vasios, Vause, 2016). When examining these countermeasures, a key insight pointed by Glasserman and Wu (Glasserman and Wu, 2018a) is that the buffer required to offset procyclicality corresponds to the unconditional quantile of price changes. Its magnitude depends on the tail heaviness of the unconditional distribution, which depends on the persistence and burstiness of the volatility dynamic.====For linear derivatives, such as US 10 year interest rate swap and North America Investment Grade CDX, investigations in Glasserman and Wu (2018a) show that the stable margin level could be ==== and ==== higher respectively than the average conditional or risk-sensitive margin levels computed from the 5-day ==== VaR metric. As of the first half of 2021, the outstanding notional of OTC interest rate swaps alone stands at 372.4 trillion US dollars globally, with 120.9 trillion for USD-denominated contracts and 94 trillion of EUR-denominated contracts====. Given the sheer size of linear derivatives alone, the consequences and implications of additional collateralization are already far-reaching including how loss and defaults spread across a collateralized financial network Ghamami et al. (2021), and how clearinghouses can strike the balance between higher fees and better default protection versus decreased market volume (Capponi and Cheng, 2018).====The situation is more complex for options whose payoffs are ==== functions of the underlying. Although the size of global options market is relatively smaller than that of the linear derivatives, the degree of margin procyclicality is by no means small or simple. Taking the sensitivity margin approach on the OTC swaption (options on interest rate swaps) as an example, Glasserman and Wu (Glasserman and Wu, 2018b) show that, in a separate study, although the Standard Initial Margin Model (SIMM) includes features to reduce procyclicality, sensitivity-based margin requirements are still exposed to procyclicality through the dependence of price sensitivities on current market conditions, and this indirect exposure of greeks (delta, gamma, vega, etc) to market volatility changes may not be obvious in advance. To access the magnitude of stable margin for options, one needs to characterize the unconditional tail heaviness of the price changes of options, which depends nonlinearly on the dynamics of both the price change of the underlying and the change of the implied volatility. This will likely to be probabilistically difficult even for a single option.====A large portfolio of options, however, might present room for netting among payoffs if they share the same underlying. The observation is that the option books of many retail brokers such as Robinhood and TD Ameritrade typically contain from hundreds of thousands to millions of positions from individual customer accounts, and at especially at turbulant times, a significant portion of their option books concentrate on just a few underlying names in the front expiries====. If one develops constant bounds of a portfolio’s maximum potential loss and they are indeed tight enough after netting, clearing members whose businesses are option heavy can use them to quickly assess their margin requirements at book level before clearing through OCC. This assessment would be distribution- and model-free, therefore insensitive to risk and volatility.====We therefore seek to develop a strategy-based netting algorithm==== to identify the best possible internal netting for a given option portfolio and evaluate the feasibility of using it to set margins that are high enough to reduce credit exposure yet stable enough to avoid procyclical effects. The idea of netting is not new and goes back to studies such as the tradeoff between multilateral netting across dealers versus bilateral netting across asset classes Cont and Kokholm (2014). The essence of the method developed in this paper centers around how to optimally divide a given option portfolio into pre-defined simple strategies, where the margin requirements for individual simple strategies are specified by the counterparty, to achieve maximum netting efficiency inside the portfolio. The main issues with the existing strategy-based margin models are that they restrict the option portfolio to be balanced (Section 3 discusses the detail on balanced versus unbalanced portfolios), are computationally complex, and tend to overestimate the potential loss under normal market conditions.====In the following, we present the idea and contributions of our approach, begining with the maximum possible loss. We assume individual options in a portfolio share the same maturity ==== and use ==== to denote the portfolio payoff at ====, where ==== is the underlying asset’s price at the maturity. The portfolio value at time ==== is thus ====. Let ==== be the MPOR, the portfolio loss is then the greater one between zero and the negative of the portfolio value at ====, i.e., ====, with ==== = ====, if ==== ==== 0, and ==== = 0 otherwise. Different risk-sensitive models present different estimation methodologies of this conditional expectation and different margin definitions based on the possible losses. For example, the SPAN by CME estimates the possible loss via simulating several possible scenarios of the underlying price changes or/and the volatility changes and identifies the maximum possible loss among these scenarios as the margin. The SIMM by the ISDA estimates the possible loss by approximating the change of option value via Greeks and identifies an approximation of the 99==== percentile loss as the margin. We define the maximum portfolio loss of a given option portfolio as ====. With a constant interest rate ====, it is easy to show that portfolio loss ==== at any time before maturity ==== is bounded above by ====:====Note that this maximum is a supremum since the loss approaches ==== when time ==== approaches the maturity ====, and the underlying price approaches the one corresponding to the worst case. We thus aim to eliminate the inefficient netting in terms of ====.====A key component of our approach is the notion of base offsets which contain two to six legs of pre-defined simple option strategies. We prove the proposed base offsets are sufficient to capture the hedging properties inside any balanced option portfolio. It is a combinatorial problem to search for the optimal division of a given option portfolio into offsets so that the maximum possible loss is minimized. We apply an integer programming model with base offsets to provide an exact calculation of the upper bound of the potential loss for a balanced option portfolio. Numerical studies on real historical data show that the problem can be solved in less than one second for twenty different strike prices using ILOG CPLEX 12.6. For problems of smaller size involving base offsets up to four legs, we further show that our formulation is equivalent to its linear relaxation and can be solved in polynomial time.====We then discuss the potential of using the maximum possible loss to set portfolio margins within a MPOR from the clearning member perspective. We examine two cases. The first case is the exchange-traded products such as the SPX options. The clearing counterparties such as CME typically use historical Value-at-Risk(VaR) method or the SPAN method to set portfololio margins. The second case is the OTC products such as the interest rate swaptions where the current market standard is the SIMM method proposed by ISDA. We compare the maximum possible loss prescribed by the proposed model with the margin levels set by three prevailing risk-sensitive models: historical VaR, SPAN and SIMM. As expected, the risk-sensitive models require less conservative margin levels than our estimation of the maximum possible loss. However, the difference is relatively acceptable after exploring the best possible payoff netting using base offsets. The benefit of our approach is that margin requirements set by the minimized maximum possible loss is independence of market conditions. They are therefore free from the procyclicalities inherent in risk-sensitive margin models.====If margin requirement does not need to be the most aggressive one, one can mix our approach with a risk-sensitive model in the spirit of European Commission (2013) to allow a given degree of risk sensitivity. We compare the performances of the mixture models with those of the risk-based models in terms of both margin level and procyclicality with SPX data in turbulent times. With the benchmark provided by our model, we present that the risk-sensitive margin level runs up and approaches the objective maximum possible loss when the market becomes turbulent. We can think of the difference between the risk-sensitive margin level and the supremum as the result of the optimistic sentiment, which overlooks the possible risk in good times. Furthermore, the margin run-ups happen when market participants are faced up with the existence of risk. Similar to the phenomenon pointed out by Antoniou et al. (2016) in stock markets, unsophisticated trading in risky opportunities would be more prevalent when the sentiment is optimistic, whereas market participants making such trades take less risk during pessimistic periods. Our strategy-based model would be on the alert guard against optimistic sentiment. By incorporating the risk-sensitive model with our model, we aim to reduce margin run-ups in turbulent times to stabilize the market.====This paper is related to the literature on margin requirement of derivatives and the netting efficiency. Lopez et al. (2017) propose a new methodology to estimate the margin requirement by a derivatives central counterparty (CCP), by introducing the interdependence among different market participants’ profits and losses to a risk-sensitive model. Capponi and Cheng (2018) model the decision problem faced by a profit-maximizing clearinghouse, which sets fee and margin requirements for heterogeneous participants who may default. Duffie and Zhu (2011), Cont and Kokholm (2014) and Garratt and Zimmerman (2020) investigate how the introduction of centralized netting affects the netting efficiency, since the exposures to different counterparties cannot be netted. We focus on how to do the netting based on the special properties inside the option portfolio, by one specific counterparty to calculate exposures for each individual netting set.====The rest of this paper is as follows. In Section 2, we review the literature on the strategy-based margin calculation approach for option portfolios. In Section 3, we introduce the concept of base offsets and prove that the margin reduced by the hedging property inside any balanced option portfolio can be ascribed to base offsets. We also set up our strategy-based netting model and investigate its properties in Section 3. In Section 4, we apply our model to the historical data of SPX options and swaptions to illustrate the difference between our model and the risk-sensitive models. We also incorporate these two methodologies to present the mixed margin calculation models. We conclude our paper in Section 5. All the proofs are placed in the online supplement to make the paper concise.",Counter-cyclical Margins for Option Portfolios,https://www.sciencedirect.com/science/article/pii/S0165188922002755,25 November 2022,2022,Research Article,45.0
Amundsen Alexander,"Department of Finance Canada, 90 Elgin Street, Ottawa, Ontario, K1A0G5, Canada","Received 16 March 2022, Revised 17 September 2022, Accepted 18 November 2022, Available online 20 November 2022, Version of Record 9 December 2022.",https://doi.org/10.1016/j.jedc.2022.104570,Cited by (0),"The literature has found that interaction effects exist between capital and labour in the adjustment cost function of firms, but there is no consensus on whether these costs are positive or negative. Using a dynamic firm structural model where capital is broken up into buildings and machinery & equipment, this paper finds that part of this ambiguity can be directly tied to capital heterogeneity. Firms are found to enjoy negative costs between labour and machinery & equipment, and between labour and buildings, while they suffer from positive costs between buildings and machinery & equipment. When compared to a model with only capital and labour, the interaction cost is found to be positive, highlighting how neglecting capital heterogeneity can lead to a spurious result on its sign. I evaluate the importance of these interactions by simulating three shocks: an increase in uncertainty, an increase in the ====, and an increase in the wage rate. I uncover large differences in the steady state dynamics of each input that are directly attributable to the interaction effects in the adjustment cost function.","It has been well documented that the investment dynamics at the macro and micro level differ substantially. Aggregate investment is highly pro-cyclical and relatively smooth, while firm level investment exhibits low serial correlation with a large degree of lumpiness (Caballero, 1999, Doms, Dunne, et al., 1998). This is reflected in the firm investment rate distribution, which is highly non-normal, fat right tailed, and positively skewed (Caballero, Engel, Haltiwanger, Woodford, Hall, 1995, Cooper, Haltiwanger, Power, 1999).====Spurred by these results, a number of studies have suggested that these dynamics are driven by the adjustment costs firms face when adjusting their inputs (Bloom, 2009, Cooper, Haltiwanger, 2006). These adjustment costs include: convexities, which make it increasingly expensive to invest the greater the investment rate is; fixed, which impose a constant cost irrespective of the investment size; and irreversibilities, which levy a lower resale price relative to the buying price when divesting. A fourth cost, known as interrelation, has been suggested to match an additional characteristic of the data: the degree of co-movement, or lack thereof, between inputs (Eslava et al., 2010, Letterie, Pfann, Polder, 2004, Mizobata, et al., 2015, Nilsen, Raknerud, Rybalka, Skjerpen, 2009, Sakellaris, 2004). This is formulated as an interaction term that can take on positive or negative values when the firm adjusts multiple inputs simultaneously (Asphjell, Letterie, Nilsen, Pfann, 2014, Contreras, 2006, Lapatinas, 2012, Merz, Yashiv, 2007, Mizobata, 2016, Mizobata, Toyoda, 2016, Mumtaz, Zanetti, 2015, Polder, Verick, 2004, Tang, 2022, Yashiv, 2013).====This paper investigates the role of these interrelation costs at a more disaggregated level, where capital is split up into different capital goods. I develop a three-factor dynamic structural model of firms that includes: 1) a nested CES production function with buildings, machinery & equipment (M&E), and labour; and 2) adjustment costs for each input, including interrelation costs between each input combination.====The model addresses a key issue found in the literature: the lack of consensus on the sign of the interrelation cost between capital and labour.==== Models using capital make the implicit assumption that capital is relatively homogenous, so that aggregation across different types of capital do not bias the parameter estimates. However, capital goods will differ in delivery lags, depreciation rates, indivisibilities and secondary markets, and empirical evidence shows substantial differences between buildings and M&E (Becker, Haltiwanger, Jarmin, Klimek, Wilson, 2006, Nilsen, Schiantarelli, 2003). Consequently, their adjustment costs have been found to be distinct, and severely biased when aggregated (Bontempi, Del Boca, Franzosi, Galeotti, Rota, 2004, Del Boca, Galeotti, Himmelberg, Rota, 2008, Del Boca, Galeotti, Rota, 2008, Goolsbee, Gross, 1997). This paper proposes the same issue is relevant for the estimation of the interrelation cost, which needs to be explored at a disaggregated level in order to identify the unique relationships that exist between labour and each capital good.====The paper makes three contributions. First, I present new evidence of investment dynamics and capital heterogeneity for Canada by employing empirical methods from the literature using a confidential administrative dataset of Canadian manufacturing firms from Statistics Canada. Unlike for other countries, such as in Caballero et al. (1995), Cooper et al. (1999), Nilsen and Schiantarelli (2003), Becker et al. (2006) and Cooper and Haltiwanger (2006), evidence of adjustments costs by exploring the investment dynamics of firms has not been studied for Canada. I find the investment rate distribution for each input to be non-normal, fat right tailed and positively skewed. Buildings investment has the largest standard deviation, followed by M&E and labour. Similarly, buildings has the highest frequency of near-zero investment, followed by M&E and labour. I observe few negative net investments for both capital inputs, yet for labour a substantial proportion is made up of negative changes. Ranking the investment rates over each firm’s panel, most firms concentrate their high investment periods in only a few years, relegating the remaining time to inaction. Periods of inaction and investment spikes are more likely to occur for buildings than for M&E and labour, and those dynamics for buildings essentially disappear for capital, highlighting the effect of capital aggregation. I find a common theme across rank plots and fixed effects regressions for empirical evidence of the interrelation costs. In terms of magnitudes, buildings and M&E have the strongest relationship, followed by labour and M&E. I find a weaker relationship between buildings and labour. In terms of frequency of simultaneous adjustment, the highest frequency occurs between labour and M&E, followed by buildings and labour. Buildings and M&E have the lowest frequency of simultaneous adjustment. These results taken together suggest that all three inputs face varying degrees of adjustment costs, including distinct interrelation costs between them.====Second, I develop a new three-factor dynamic structural model of firms by breaking up capital into buildings and M&E and estimating the adjustment costs of each input using Value Function Iteration and Simulated Method of Moments. A structural model with four state variables and convex, fixed, irreversible and interrelation costs has not been estimated before in the literature, partially due to the computing power needed to solve along a four-dimensional grid as the curse of dimensionality occurs. I alleviate this by employing state-of-the-art servers from Calcul Quebec and Compute Canada, taking advantage of GPU acceleration and parallelization. I find that labour has the largest convex cost, followed by M&E and buildings.==== There are no fixed costs for labour or M&E, while significant fixed costs exist for buildings.==== Irreversible costs for buildings are modest, while they are severe for M&E.==== I find three distinct estimates for the interrelation costs: positive between buildings and M&E, negative between buildings and labour, and substantially negative between M&E and labour.==== This indicates that firms face substitutability effects (positive costs) and complementarity effects (negative costs) that are separate from the CES production function. I compare the three-factor model to two models seen in the literature: 1) a model with only capital and labour, as seen in Contreras (2006), Lapatinas (2012), Asphjell et al. (2014) and Tang (2022); and 2) a model with buildings, M&E and labour where labour is fully flexible and non-convexities are excluded, as seen in Cummins and Dey (1998). I find the interrelation cost between capital and labour is estimated to be significantly positive, highlighting how neglecting capital heterogeneity can lead to a spurious result on its sign.==== For the latter model, I continue to estimate a positive interrelation cost between buildings and M&E, yet the cost is at a much lower magnitude and close to zero, implying a downward bias in the estimate of the cost if non-convexities and adjustment costs for labour are not taken into account.====Third, given the distinct estimates of the interrelation costs between buildings, M&E and labour, I evaluate their importance by simulating three shocks between a model with and without the interrelation costs, holding constant all other parameters. The shocks are specified as: 1) an increase in the variability of the demand/productivity process faced by firms, representing a period of greater uncertainty in the spirit of Bloom (2009); 2) an increase in the interest rate, representing a period of economic tightening by the central bank; and 3) an increase in the wage rate, representing a change in the relative factor price between labour and each capital good.==== I find the interrelation costs as a whole are beneficial for firms, where the negative costs between buildings and labour, and between M&E and labour, outweigh the positive costs associated with buildings and M&E. This results in firms holding greater quantities of each input in relation to the model without interrelation costs. Consequently, the costs amplify the response of firms when hit by beneficial shocks and dampen the response of firms when hit by adverse shocks. This leads to large differences in the steady state dynamics of each input. I find some evidence that interrelation costs can also lead firms to prefer the adjustment of some inputs at the expense of others. In this case, the adjustment of M&E at the expense of buildings. These dynamics highlight the importance of interaction effects in the adjustment cost function as it can affect the firm’s investment decision in response to shocks.====The remainder of the paper is organized as follows. Section 2 discusses the related literature. Section 3 describes the data. Section 4 presents the empirical analysis. Section 5 outlines the model. Section 6 presents the estimation results. Section 7 conducts the counterfactual analysis. Section 8 concludes.",Interaction effects in the adjustment cost function of firms,https://www.sciencedirect.com/science/article/pii/S0165188922002731,20 November 2022,2022,Research Article,46.0
"Olijslagers Stan,van der Ploeg Frederick,van Wijnbergen Sweder","Centraal Planning Bureau, The Hague, The Netherlands,Department of Economics, University of Oxford, UK,University of Amsterdam, CEPR and Tinbergen Institute, Netherlands,Faculty of Economics and Business, University of Amsterdam, Netherlands","Received 21 February 2022, Revised 5 August 2022, Accepted 17 November 2022, Available online 19 November 2022, Version of Record 5 December 2022.",https://doi.org/10.1016/j.jedc.2022.104569,Cited by (1),". Allowing for damages as well as a cap leads to a higher carbon price which grows more slowly. But as temperature and cumulative emissions approach their caps, the carbon price is ramped up ever more. Policy makers should expect a rising path of carbon prices.","Rising temperatures and the ensuing threat to our planet and the economy constitute the biggest market failure we know (Stern et al., 2006). One way to correct this market failure is to price carbon uniformly throughout the global economy at a price equal to the social cost of carbon (SCC) which is the expected present discounted value of all future damages resulting from emitting one ton of carbon today. In this sense, the carbon price or SCC is an asset price just like a share or house price. Another way is to ensure that any production inefficiencies in the production of renewable energies are properly internalized. Production of renewable energies is subject to Swanson’s law with any doubling of installed solar panels or windmills leading a reduction of 20 or 40 percent in the unit costs of a panel or windmill. The mechanism underlying Swanson’s law is technical progress through learning by doing. This naturally leads to the policy recommendation that renewable energies should receive a subsidy equal to the social benefit of learning or SBL which is the expected present discounted value of all future renewable cost reductions resulting from using one unit of renawable energy today.====Our aim is threefold. First, to analyse the initial level of an optimal carbon price internalizing that externality. Second, to investigate what the entire time path of carbon prices should look like and how various shocks to the economy, damages and temperature, preference structures and parameter choices influence the shape of that time path and its initial starting point. The time path of (expected) carbon prices arguably matters as much as the initial carbon price since much of the adjustment and mitigation efforts will have to take place through new investments and these depend on the trade-off between current costs and future prices. Third, to gain insight into the level and time path of the optimal subsidy for renewable energy and how this interacts with the optimal carbon price.====Pricing carbon reduces demand for carbon-intensive goods, encourages green innovation, carbon capture and sequestration, and locks up fossil fuel in the crust of the earth. This Pigouvian solution charges emissions at a price that implements the optimal policy and in this way internalizes the global warming externality (Pigou, 1920). This price can be implemented as a carbon tax with the revenue rebated in lump-sum manner to the private sector or one could adopt a Coasian approach where property rights to emit or the right to a clean planet are allocated (Coase, 1960), with subsequent trade allowed. There is a burgeoning literature on how high that tax should be today, but some argue that future prices should decline from high initial levels (e.g. Daniel et al. (2019)) while most others argue for the exact opposite. Our contribution is to shed light on these differences and offer better understanding of the determinants of the shape of the entire time path of optimal carbon prices.====The learning-by-doing externality should thus be dealt with using a separate instrument: early and direct subsidies of green energy. Given that most integrated assessment models of climate and the economy used for optimal policy analysis are based on the model of Nordhaus (2017) and are concerned with the optimal proportion of energy that is carbon-free, they cannot distinguish between a carbon price and a renewable energy subsidy. This is why the learning-by-doing subsidy and the carbon price are often lumped together with carbon prices as in Daniel et al. (2019), which then leads to an unwarranted early spike in carbon prices. This may discourage investment in clean technology by raising input costs while not raising future prices commensurately and should thus not be taken literally. We argue that appropriately targeting the second externality by a separate renewable energy subsidy separate from the carbon price unambiguously results in a rising expected time path for the latter.====In climate economics the Pigouvian price is referred to as the social cost of carbon or SCC. Imposing the Pigouvian tax as the private price of carbon actually leads to decentralized implementation of the optimal policy. But the SCC is in fact a more general concept than a Pigouvian tax as it can be evaluated along other paths than the optimal path. For example, the SCC evaluated along a business-as-usual path where global warming externalities are not internalized by private actors in the economy will be higher than along the optimal path if damages are convex enough (Olijslagers, 2021a).====Policy makers must evaluate the SCC under huge uncertainties regarding the wealth of future generations and future global warming damages resulting from emissions today. This involves difficult trade-offs between consumption today and the risks of damages from global warming to consumption and the risks of shocks to temperature in the near and distant future. For that reason we focus extensively on the nature of the stochastic processes driving these uncertainties, on whether we know their distribution or not and on the interaction with the preference structures determining society’s current attitude with respect to future uncertainty.====Our benchmark is the case where damages to aggregate production are linear in temperature. Given that recent insights in atmospheric science suggest that temperature is approximately linear in cumulative emissions (Allen, Frame, Huntingford, Jones, Lowe, Meinshausen, Meinshausen, 2009, Dietz, Venmans, 2019, Matthews, Gillett, Stott, Zickfeld, 2009, van der Ploeg, 2018), the function relating the percentage loss in aggregate production to cumulative emissions is then also approximately linear in cumulative emissions.==== Since damages are proportional to aggregate production, we can show that for this benchmark the optimal carbon price grows at the same rate of growth as the economy. We then consider step by step four generalizations of our benchmark and show how they impact the qualitative pattern of the time path of optimal carbon prices.====First, we show that if damages are a convex function of temperature as has been argued by Weitzman (2012) and Dietz and Stern (2015), the optimal carbon price will start at a higher level than in the benchmark case and will also grow faster than the economy.====Second, we confirm an earlier result by Daniel et al. (2019) that if there is gradual resolution of uncertainty in the damage ratio, there is a component of the optimal carbon price which falls over time.==== But we show that when there is sufficient growth of the economy, this component is outweighed by the growing component of the carbon price resulting from growing damages. The key insight is thus that gradual resolution of uncertainty slows down the rate of growth of the optimal carbon price but under plausible assumptions does not reverse it. Gerlagh and Liski (2018) also find that learning and resolution of uncertainty slows the rise in the optimal carbon price.====Third, we show that climatic and economic tipping points whose arrival rates increase in temperature boost the carbon price. Once a climate tipping point occurs, it will suddenly increase the sensitivity of temperature to cumulative emissions which in turn should prompt policy makers to boost carbon prices and abatement significantly right now. Immediately after the tip has occurred, climate policy is ramped up resulting in an instantaneous further upward jump in the carbon price and abatement. An economic tipping point that becomes more likely with global warming and abruptly leads to a percentage destruction of production also leads to a higher path of carbon prices and abatement ex ante. But immediately after the tip the optimal level of the carbon price and abatement jump down. Different types of tipping points thus have radically different implications. However immediately after such a downward jump, optimal carbon prices will start rising again.====Fourth, although economists usually take a conventional welfare-maximizing approach, the Intergovernmental Panel on Climate Change (IPCC) and most countries have adopted the more pragmatic approach of agreeing that policy makers aim to stay below a temperature ceiling. They will do their utmost best to keep global mean temperature well below 2 degrees Celsius and aim for 1.5 degrees Celsius. A temperature cap which bites implies that the optimal carbon price should grow at a rate equal to the risk-adjusted interest rate (cf. Gollier (2020)).==== Once allowance is made for the risk premium, this Hotelling path for the carbon price is typically faster than the rate of growth of the economy (even when the safe return is below the economic growth rate). Hence, the initial carbon price and abatement will be lower upfront but higher in the future. The intuition is as follows. Consider along the optimal abatement path the ‘investment’ in which emissions are reduced by one additional unit today and are raised by one additional unit in the future. The payoff of this investment equals the marginal abatement cost in the future. Given that there is abatement cost uncertainty, the rate of return on this investment should reflect this uncertainty. We also show that if policy makers take account of a temperature cap ==== internalize damages from global warming to aggregate production, the optimal carbon price will grow faster than if only damages are internalized but slower than if only a temperature cap is imposed.====Overall, our results suggest that in face of a wide range of risks and uncertainties and under a wide range of assumptions about society’s preferences with respect to various aspects of uncertainty, and in particular whether we know their distribution or not, ====.====Our framework of analysis is a simple endowment economy where the endowment is subject to normal economic shocks (modelled by a geometric Brownian motion) and by rare macroeconomic disasters as in Barro, 2006, Barro, 2009 and Barro and Jin (2011). Temperature is driven by cumulative emissions, and the fraction of damages lost due to global warming is a power function of temperature and is subject to stochastic shocks with a distribution that is skewed and has mean reversion as in van den Bremer and van der Ploeg (2021). Our short-cut approach to modelling gradual resolution of damage uncertainty is slow release of information. We distinguish aversion to risk from aversion to intertemporal fluctuations, so we use recursive preferences (Duffie, Epstein, 1992, Epstein, Zin, 1989, Epstein, Zin, 1991). This allows for a preference for early resolution of uncertainty when the coefficient of relative risk aversion exceeds the inverse of the elasticity of intertemporal substitution, as empirical evidence strongly suggests Epstein and Zin (1991).====Our paper is closely related to recent contributions by Lemoine (2021) and van den Bremer and van der Ploeg (2021) who also study the effect of damage ratio uncertainty and uncertainty about the economic growth rate in an endowment economy and offer analytical insights into the deterministic, precautionary, damage scaling and growth insurance determinants of the optimal SCC. Our paper differs in that we distinguish relative risk aversion from the inverse of the elasticity of intertemporal substitution and thereby allow for preferences for early resolution of uncertainty. Also we have more general forms of uncertainty and allow for skewness and declining volatility of the shocks to the damage ratio (cf. Daniel et al., 2019), the risk of rare macroeconomic disasters, and both economic and climatic tipping risk whose frequency increases with temperature. We also allow for learning-by-doing effects in mitigation and thus for the consequent need for renewable energy subsidies. Furthermore, another contribution of our study is that we analyse the effects of temperature caps under uncertainty (both with and without damages to the economy) on the time path of the optimal carbon price under uncertainty. Temperature caps are also analysed in Gollier (2020) but his two period framework precludes the analysis of intertemporal changes in the risk premium which we show to be of importance.====Our paper is also related to an extensive literature on optimal discounting under uncertainty (Gollier, 2002, Gollier, 2002, Gollier, 2008, Gollier, 2011, Gollier, 2012, Olijslagers, van Wijnbergen, Weitzman, 1998, Weitzman, 2007, Weitzman, 2009, Weitzman, 2011) and optimal climate policy under uncertainty (van den Bremer, van der Ploeg, 2021, Crost, Traeger, 2013, Crost, Traeger, 2014, Jensen, Traeger, 2014, Traeger, 2022). It also relates to a growing literature on optimal climate policy in the presence of climatic and economic tipping points (Cai, Lenton, Lontzek, 2016, Cai, Lontzek, 2019, Lemoine, Traeger, 2014, Lemoine, Traeger, 2016, van der Ploeg, de Zeeuw, 2018).====Like much of this literature, we present a simple general equilibrium asset pricing model to answer many of the questions regarding uncertainty and tipping points in this literature. Our focus is different however, in that we explicitly aim to understand the qualitative nature of the ==== of the path of optimal carbon prices and abatement. We also study temperature caps with time-varying risk premia in a continuous-time, infinite-horizon integrated assessment model of the economy and the climate. In the absence of damages from global warming to the economy, we show that the expected growth in the marginal abatement cost and the price of carbon equals the risk-free rate plus an insurance premium. Compared to Gollier (2020), we additionally consider the implementation of a temperature cap while at the same time internalizing the damages to aggregate production caused by climate change. This gives an expected growth of the carbon price that is in between the growth rate of the economy and the risk-adjusted interest rate.",On current and future carbon prices in a risky world,https://www.sciencedirect.com/science/article/pii/S016518892200272X,19 November 2022,2022,Research Article,47.0
Gu Jiadong,"Bay Area International Business School, Beijing Normal University, China","Received 22 February 2022, Revised 6 November 2022, Accepted 14 November 2022, Available online 18 November 2022, Version of Record 13 December 2022.",https://doi.org/10.1016/j.jedc.2022.104568,Cited by (0),"We study stress tests as ==== persuasion within the fundamental bank run framework. This paper shows that the optimal disclosure policy depends on the liquidation cost of the long-term asset. In particular, when the liquidation cost is low, the optimal stress test ==== discloses information about banks: it increases the likelihood of enjoying the high ====. When the liquidation cost is high, the optimal stress test ==== discloses information: it reduces the likelihood of costly bank runs. The central trade-off in stress test design is between the bank run cost and the high ====. The theory suggests regulatory policy coordination and offers insights on different stress testing experiences across countries.","Following the global financial crisis of 2008, stress tests for financial institutions have been a regular tool for regulators to promote financial stability. For example, the regulatory authorities in Europe and the U.S. have attempted to increase the transparency of financial institutions by conducting stress tests and revealing the test results to the market.==== But the welfare implication of stress tests is still ambiguous. What is the optimal design of stress tests, and what will affect the optimal design? Why are there differences in stress tests in different countries and areas?====The first motivation for our questions is the wide debates about policy coordination and the calls for a holistic approach to prudential regulations. There is an ongoing congressional debate in the U.S. on proposals to modify parts of or even abolish the substantive authorities of the Financial Stability Oversight Council (FSOC), which was initially tasked with improving coordination among different modes of regulation.==== The supporters of FSOC defend its essential role in the U.S. financial system.==== A message in Bolton et al. (2019) is that “prudential regulation should take a holistic approach, considering and setting requirements for capital, liquidity, and disclosure together and taking into account their potential interactions.”====Essentially, the policy coordination and holistic approach lie in the concern about interactions between different regulatory tools. The regulator needs to coordinate the stress test design with liquidity-related policies if they are dependent. Otherwise, it may not benefit much from the joint regulatory design or policy coordination. However, it is less known how the stress test design and the liquidity interact.====Second, the factors that affect the optimal stress test design are key to understanding the puzzling differences between the U.S. and European stress tests after the 2008 financial crisis. The U.S. stress test is more successful, and transparent in the test results. The first U.S. stress test in 2009 - The Supervisory Capital Assessment Program - publicly reported the results and was widely seen as a success in restoring market confidence. However, the European stress tests were relatively opaque and hid some bad banks. The first Europe-wide stress test results in 2009 were to remain confidential. Although test results were published in 2010 and 2011, the Irish banking system suffered bailouts four months after passing the test in 2010; Dexia Group broke up less than three months after being given a clean bill of health in 2011. Our goal is to provide a ==== rationale for the differences.====This paper studies stress tests as Bayesian persuasion (Kamenica and Gentzkow, 2011) to answer these questions. Specifically, we investigate how the asset liquidation cost and market confidence (belief) affect the optimal stress test designs. Our substantive result is that the optimal disclosure policy in terms of maximizing the ==== social welfare can be partially or fully informative, depending on the market liquidity (characterized by liquidation costs). A full disclosure policy is optimal for high market liquidity, while a partial one is optimal for low market liquidity. The main trade-off behind the optimal policy is that a more informative stress test separates good and bad banks, which prevents runs on good banks and increases the possibility of enjoying high asset returns to good banks; on the other hand, more information increases the likelihood of runs on bad banks, and a bank run is costly in terms of high liquidation costs.====Thus, the intuition relies on two forces: ==== of preventing runs on good banks (==== high asset return) and ==== of runs on bad banks.==== When the market liquidity is high, the cost of runs on banks is low. The benefits of preventing runs on good banks outweigh the cost of runs on bad banks. The regulator would perfectly separate good banks from bad ones to prevent runs on good banks. Hence, a full-information stress test is preferred.====In contrast, when the liquidity is low, the cost of runs is high and exceeds the benefit of preventing runs on good banks. The regulator would pool different types of banks so that some bad banks can survive the costly run. In this case, the partial-information stress test depends on market prior beliefs. For large priors, the economy has mostly good banks. The regulator fails some good banks with bad banks, which reduces the likelihood of runs on bad banks without hurting most good banks. For small priors, the economy has mostly bad banks. The regulator optimally passes some bad banks with good banks, so that this fraction of bad banks can survive the costly runs.====The main contribution of our work is then to identify the novel trade-off behind the optimal ==== (==== stress test design) and characterize the liquidity conditions for optimality of both fully and partially informative disclosure, and link them to policy debates and puzzling facts. The methodology contribution is to characterize a two-threshold optimal deposit contract under a flexible information structure within the bank run framework. To focus on the targeted trade-off, we follow the view of the fundamental bank run ==== coordination concern (Allen, Gale, 1998, Gorton, 1988),==== introducing an ==== about the long-term asset return to allow for stress test design, and ==== as in Cooper and Ross (1998) to characterize the market liquidity condition. The model of fundamental bank run does not impose sequential service constraint, which allows us to ==== the coordination issue among depositors==== and focus on the key trade-off between benefits and costs of the stress test.====In our three-date model, consumers are ex ante identical, risk-averse, and of early and late types after preference shocks. Competitive banks offer consumers a deposit contract in exchange for consumers’ endowment, promising the running consumers (==== consumers that withdraw at the intermediate date) either a fixed amount of consumption or an equal share of all the liquidated assets in case of default (Allen and Gale, 1998). Early consumers withdraw at the intermediate date, but late consumers can withdraw at the intermediate date or the final date. Banks invest the deposits from consumers into a safe short-term asset and a risky long-term asset whose return is stochastic and not observed until the final date. The long-term asset can be liquidated at an ==== with a cost (Cooper and Ross, 1998).====Although the long-term asset return is not realized until the final date, a test result about the asset return is publicly observed at the intermediate date according to the stress test, which is designed at the initial date after the financial crisis. To introduce stress tests into the model, we modify the perfect information about asset return at the intermediate date in Allen and Gale (1998) into an imperfect information scenario. In the model, the regulator has symmetric information about the banks’ asset return when designing the stress test modeled as ==== (Kamenica and Gentzkow, 2011). However, the regulator can deliberately manipulate the distribution of test results for the stress-tested banks in a committed way to affect market confidence.====After observing the regulator’s stress test policy, banks choose the optimal consumption contract (we use the terms “consumption contract” and “deposit contract” interchangeably). Banks repay consumers the promised consumption if it’s affordable at the intermediate date. Otherwise, banks go to ==== and are forced to liquidate all the long-term assets, then divide all the assets equally among the consumers who withdraw at the intermediate date.====To figure out the substantive results on optimal stress test designs, we investigate the problem by backward induction. First, we analyze the optimal consumption contract, given a stress test design. We figure out two ====, which divide the belief space into three intervals. In the ==== interval, the early consumers withdraw and get the promised consumption at the intermediate date, while the late consumers wait until the final date and enjoy the high expected return to the long-term asset. In the ==== interval, the promised consumption is still affordable to all the running consumers and the late consumers are indifferent between withdrawing at the intermediate date and final date. In the ==== interval, the expected return is so low that it’s not incentive compatible for the late consumers to wait until the final date while paying running consumers the promised consumption. So banks fail to repay the promised consumption and go bankrupt. The late consumers will be better off withdrawing at the intermediate date. Due to bankruptcy, welfare as a function of belief jumps down in the lower interval. At low prior beliefs, bank runs happen for sure without a stress test, which prevents the consumers from enjoying the higher returns on the long-term asset. This leads to ex ante inefficiency, and stress test design comes to play.====Second, we characterize the optimal stress test for different liquidation costs, given the consumers’ payoff under each test result (or market belief) following the standard concavification method (Kamenica and Gentzkow, 2011). When the liquidation cost is low, the cost of runs on bad banks is low so that the benefit from preventing runs on good banks outweighs the cost. Hence, the regulator perfectly separates good banks from bad ones, by complementing a fully informative stress test.====However, when the liquidation cost is high, the optimal stress test is partially informative and depends on prior beliefs of the market. For small priors, all the late consumers are pessimistic about banks’ health and they withdraw at the intermediate date without stress tests. Bank runs happen with probability 1 incurring large liquidation costs. The optimal partial-information stress test passes some bad banks with the good, so bank runs happen with a smaller probability. From the ex-ante respect, social welfare improves. Instead, if the regulator conducts a full-information stress test, social welfare can also be improved from the one without a stress test. But the welfare gain is less than that achieved by the optimal partial-information stress test. This is because the partial information stress test puts less weight on the event of a full bank run than the full-information stress test does. Given the cost of runs is high, more weight on the event of a bank run is not ex ante efficient. For large priors, the economy has mostly good banks. The optimal partial-information stress test fails some good banks with the bad to reduce the likelihood of runs on bad banks without much hurting most good banks.====As an extension, we investigate the case of panic-based runs to capture the idea of adversarial design of stress tests, ==== the regulator considers the worst scenario according to consumers’ feasible strategy. In panic-based runs, the fear that other consumers run leads all the consumers to run, which means the regulator does not achieve the partial run: Once some late consumers run, all the late consumers will run. The main insights still hold in the adversarial design, which discloses full information for high liquidity. In addition, no information disclosure can be optimal in the adversarial design when the economy has mostly good banks, because any extra information may trigger a panic-based costly run. Moreover, we discuss a few other extensions.==== The paper is related to several strands of the literature. Our model is built upon and contributes to the bank run literature, which goes back to the seminal works of Bryant (1980) and Diamond and Dybvig (1983) (DD). The main contribution to the banking literature is to characterize the optimal deposit contract under the flexible information structure. We deviate from DD in two key respects. First, we do not impose the sequential service constraint that relates consumers’ payoff in a bank run to their place in line. This relaxation excludes the essential coordination among consumers (Allen and Gale, 1998). Second, we modify the perfect information about the long-term asset return at date 0 in DD by introducing an endogenous stress test design at date 0 and a stress test result at date 1.====Second, our work within the fundamental-based run is complementary to a set of papers examining information design in a coordination setting, ====, Goldstein and Huang (2016), Ely (2017), Li et al. (2019), Basak and Zhou (2020a), Basak and Zhou (2020b), Inostroza and Pavan (2020). These works endogenize coordination and information while treating the deposit contract as given, instead, we endogenize deposit contract and information while taking coordination as redundant.==== In the coordination setup, most papers treat fundamental runs as something unavoidable and look at how information policy reduces the likelihood of bank failure (Goldstein and Huang, 2016), eliminates the coordination failure with endogenous and exogenous agents’ action orders (Basak, Zhou, 2020, Basak, Zhou, 2020) or strategic uncertainty among market participants (Inostroza and Pavan, 2020). However, fundamental runs are endogenized via deposit contracts in our framework. Stress tests can reduce the likelihood of fundamental runs because information disclosed by stress tests affects the beliefs about banks’ returns and hence banks’ deposit contracts, which further affect consumers’ withdrawal decisions. The trade-off between the benefit of preventing runs on good banks and the cost of runs on bad banks determines the optimal likelihood of fundamental runs.====Third, this paper speaks to the regulatory transparency of financial systems. Back to Hirshleifer (1971) more information destroys risk-sharing opportunities. Goldstein and Leitner (2018) (GL hereafter) study the trade-off between the Hirshleifer effect and preventing the market breakdown of information disclosure.==== The similar Hirshleifer effect arises in our work for high liquidity, but we have two main differences from GL. First, our paper analyzes a different trade-off between the Hirshleifer effect and a positive “allocation” effect, ==== more information allows higher returns by reducing runs on good banks. In particular, the full information disclosure is optimal when this allocation effect dominates. Second, our setup provides a deposit-contract microfoundation and characterizes the deposit-contract channel of the risk-sharing arrangement. Hence, the value of pooling is to reduce the ==== likelihood of runs via deposit contracts, rather than to protect against a fall in the capital in GL. Faria-e Castro et al. (2016) finds a similar monotone relationship between the informativeness of stress tests and numbers of banks’ funds in a crisis, but in the absence of the Hirshleifer effect and deposit contracts. The value of more information is from the information asymmetry between banks and investors. However, in our work, the value of information does not rely on information asymmetry and it is endogenous to the probability allocations on runs and no-runs based on the endogenous deposit contracts.====In addition, we model stress tests as Bayesian persuasion (Kamenica and Gentzkow, 2011) to characterize the flexible information disclosure, broadly related to the literature in ====, on which Kamenica (2019) and Bergemann and Morris (2019) provide good surveys. The problem we study is one of the information designs where the regulator manipulates the information.====The rest of the paper is organized as follows: Section 2 describes the model setup and solution concept. Section 3 analyzes the banks’ consumption contract problem. Section 4 investigates the regulator’s stress test design problem. Section 5 discusses the substantive implications, and Section 6 looks at some extensions. The final Section 7 concludes.",Optimal stress tests and liquidation cost,https://www.sciencedirect.com/science/article/pii/S0165188922002718,18 November 2022,2022,Research Article,48.0
Ge Shuyi,"The School of Finance, Nankai University, China","Received 16 April 2022, Revised 9 November 2022, Accepted 9 November 2022, Available online 13 November 2022, Version of Record 28 November 2022.",https://doi.org/10.1016/j.jedc.2022.104565,Cited by (0),"This paper proposes a new mutual exciting regime-switching model where crises can spread contagiously across countries. Each country has its own hidden stochastic process that determines whether it is in a normal or crisis regime. The mutual-excitation component allows interactions in the ====, and a crisis in one country could increase the likelihood of a crisis in another country. Using this new approach, I revisit the sovereign risk contagion in the euro area. I find striking shifts in market pricing functions for the sovereign bond spreads. Multi-country contagion plays a dominant role in driving such shifts, while common risk factors and country-specific fundamentals are less important.","The unfolding of the European Sovereign Debt Crisis shows that extreme events in the financial markets appear in clusters instead of in isolation. This triggers a surge of interest in the role of contagion in risk clustering. Testing for the existence of contagion and quantifying its size is important for economists and policymakers. This paper proposes a new mutual exciting regime-switching model where crises can spread contagiously across countries.====The first challenge in studying contagion concerns its definition. Financial markets are interconnected as a result of exposure to common factors and the propagation of shocks. Is contagion just the normal time interdependence? Or does it reflect a shift in the normal time interactions during periods of crisis? This paper does not aim to contribute to this debate. Rather, I adopt the definition from the seminal paper of Forbes and Rigobon (2001), which defines contagion as a significant shift in cross-market linkages after a shock to an individual country (or group of countries). Importantly, this shift in interdependence has to do with parameter instability (i.e., a structure change in normal time transmission structure). In this paper, I propose a new contagion model that accommodates different types of breakdowns in normal time cross-market interdependence.====Our second challenge is the specification of the crisis regime. Many studies implicitly assume that the crisis regime can be known ====. For example, the correlation-based test for contagion (Boyer, Gibson, Loretan, et al., 1997, Corsetti, Pericoli, Sbracia, 2005, Rigobon, 2003), the factor-model-based test for contagion (Bekaert, Harvey, 2003, Dungey, Fry, González-Hermosillo, Martin, 2002, Dungey, Fry, González-Hermosillo, Martin, 2006, Dungey, Martin, 2004), and the structural break test for contagion (Beirne, Fratzscher, 2013, Bekaert, Ehrmann, Fratzscher, Mehl, 2014) all rely on ==== identification of the crisis regime. While being very popular due to the simplicity of their implementation and interpretation, these methods suffer from several problems. First of all, they implicitly assume that the crisis is continuous so that countries enter the crisis regime and stay there for a specified period of time (usually based on an expert-identified date). However, from the empirical findings of this paper, distress is sometimes short-lived, which can occur and disappear suddenly in an on-and-off fashion. In addition, the ==== nature of these methods makes them not particularly useful for detecting early-warning signals. In the newly proposed mutual exciting regime-switching model, I relate the crisis probability to some explanatory variables and past states so that the framework allows one to monitor the risk of entering the crisis regime out-of-sample. Another strand of literature assumes the crisis regime is associated with some extreme values of the observed dependent variable. For example, Pesaran and Pick (2007) and Metiu (2012) assumes a country is in the crisis regime when its endogenous performance variable is above a pre-specified threshold value. Caporin et al. (2018), in a similar vein, associates the crisis regime with some high quantiles of the observed dependent variable. However, in many cases, crises are more complicated and cannot be simply modeled by extreme values of a performance indicator; namely, they tend to be unobserved processes governed by several mechanisms.====The third challenge we face is the drivers of the regime-switching process. There could be many reasons for a switch from a normal to a crisis regime. In particular, we are interested in the role of multi-country contagion in the regime-switching process. That is, whether the transition of a country to the crisis regime is more likely when other countries are in the crisis regime in the last period (i.e., crises spread contagiously across countries). Moreover, how to quantify the strength of contagion if it exists?====This paper proposes a new mutual exciting regime-switching model where crises can spread contagiously across countries. Each country has its own hidden stochastic process that determines whether it is in a normal or a crisis state. The country-specific process is governed by some common macroeconomic factors, country-specific fundamentals, and all countries’ past states. The key element in the model that captures the multi-country contagion is the mutual exciting regime-switching process. The idea is closely related to some recent papers by Ait-Sahalia et al. (2014) and Aït-Sahalia et al. (2015). To capture the propagation of jumps across markets, those authors model the jump intensity using the mutual exciting jump process, also known as the Hawkes process (Hawkes, 1971, Hawkes, 1971) where a jump in one market could increase the probability of future jumps elsewhere. In their spirits, I model multi-country contagion using a mutual exciting regime-switching process. Under this framework, one country being in the crisis regime increases the transition probability to the crisis regime for other countries. This is consistent with the definition of contagion from Pesaran and Pick (2007), where contagion implies that a crisis in one country increases the likelihood of a crisis in another.====I revisit the sovereign risk contagion in the euro area using the new approach. I use daily ten-year sovereign bond spreads of six euro area countries, including Greece, Ireland, Portugal, Spain, Italy (GIPSI countries), and France, from 12/02/2008 to 01/12/2011. The German government bond yields of the same maturity are used as the benchmark. I deliberately end the sample before the European Central Bank (ECB) announced the Long-Term Refinancing Operations (LTRO) to avoid clustering of switches due to the intervention. I account for several distinct sources of interdependence, including correlated shocks, exposures to common risk factors, and dynamic risk spillovers via vector autoregressive terms. Contagion is associated with a shift in the normal time interdependence after crises hit other countries. Some interesting empirical results are found.====First, sovereign bond spread pricing functions are highly regime-dependent as there are striking shifts in market pricing behaviours. When switching from the normal to the crisis regime, all countries experience a drastic increase in volatilities. Four out of the six sample countries (Spain, Portugal, Ireland, and Greece) have significant and positive jumps in the intercept. There is a break in the exposures to common risk factors. It is worth noticing that when switching to the crisis regime, apart from France, all other countries experience a drop in the exposure to the ====, which is used to measure investor sentiment and overall economic uncertainty in the European market. This might be because the risk aversion and uncertainty both started falling since the spring of 2009 while the euro area sovereign spreads began to skyrocket after 2010. This suggests that the uncertainty and sentiment factor cannot help to interpret the sharp increase in the euro area sovereign spreads during the European debt crisis. In contrast, I find that most countries are significant more exposed to two euro area credit risk factors in the crisis regime, showing strong evidence that sovereign bond spreads have become more sensitive to regional credit risk during distress. Furthermore, the linear cross-country spillovers also appear to gain more importance during the crisis regime as countries become more exposed to shocks that originated elsewhere.====Concerning the regime-switching drivers and the role of multi-country contagion, all sample countries are subject to considerable contagion effect (i.e., the probabilities of them switching to the crisis regime significantly increase if their neighbours were in crisis in the past state). In terms of economic magnitude, multi-country contagion plays a more important role than common risk factors and even country-specific fundamentals in determining their transition probabilities to the crisis regime. I conduct out-of-sample exercises and compare the model’s ability to raise early warning signals with a benchmark regime-switching model that has no contagion component. I consider three estimation periods which are all followed by risk clustering. The mutual exciting regime-switching model, as it is able to capture the contagion effect that reinforces the crisis probabilities, is better at detecting the risk of transition to crisis for sample countries. Without taking such an important amplification effect into account, the benchmark regime-switching model noticeably predicts lower out-of-sample crisis probabilities when the system is at high risk.====The rest of this paper is organised as follows: Section 2 introduces the mutual exciting regime-switching model and discusses how contagion enters the model. Section 3 shows the Bayesian estimation procedure and inference. The empirical application is presented in Section 4. Section 5 concludes.",A revisit to sovereign risk contagion in eurozone with mutual exciting regime-switching model,https://www.sciencedirect.com/science/article/pii/S0165188922002688,13 November 2022,2022,Research Article,49.0
Krivenko Pavel,"Baruch College, CUNY, 137 East 22nd Street, Office 408, New York 10010, United States","Received 25 February 2022, Revised 4 November 2022, Accepted 7 November 2022, Available online 10 November 2022, Version of Record 5 December 2022.",https://doi.org/10.1016/j.jedc.2022.104564,Cited by (0),"This paper studies a labor search model with agents who are averse to ambiguity (Knightian uncertainty). Shocks to confidence about future productivity are modeled as changes in ambiguity. Using the Survey of Professional Forecasters data, I find that confidence shocks help explain the equity premium and the stock market volatility, including their term structure. Ambiguity amplifies the response of the economy to productivity shocks, helping the model produce more realistic dynamics of unemployment, vacancies, labor market tightness, stock prices, and returns. Returns in the model are predictable with price-dividend ratios; both returns and dividends are predictable with unemployment, like in the data. Two extensions consider shocks to bargaining power and to separation rate and find similar implications of ambiguity about these shocks.","Individuals, firms, and governments make decisions under uncertainty that may be hard to quantify and may change over time. Most studies of uncertainty focus on risk: they assume the agents know the precise distribution of shocks – and they struggle to explain the equity premium and volatility puzzles. Recent studies emphasize the role of uncertainty about the distribution of shocks, or ambiguity, in the economy: from financial decisions at the micro level==== to stock market dynamics==== and the business cycle, explaining up to 70% of the fluctuations in the macro fundamentals.==== The most progress in jointly studying the business cycle and the financial market dynamics has been made using the labor search framework.====This paper studies the effects of ambiguity in a labor search model. First, it asks how the level of ambiguity and the shocks to ambiguity affect the labor market, output, stock prices and returns. Second, it evaluates the performance of the model in matching key financial and economic moments in the data. It finds that moderate ambiguity==== is enough to fit the equity premium. As compared to a productivity shock, a shock to confidence has a similar effect on labor market quantities, smaller effects on wages and output, and a larger effect on stock returns. As a result, confidence shocks amplify the effects of productivity shocks and improve the dynamics of unemployment, vacancies, tightness, stock prices and returns without introducing too much volatility of wages. Confidence shocks also make price-dividend (PD) ratio procyclical, as in the data, and returns – predictable with PD ratios (with negative coefficients increasing in magnitude with horizon – like in the data). In addition, both returns and dividends are predictable with the unemployment rate, both in the model and in the data.====Ambiguity-averse agents lack the confidence to assign probabilities to all possible events and act ==== they use the worst probability drawn from a set of multiple beliefs. A loss of confidence widens this set of beliefs. It can happen, for example, when agents receive conflicting news or when experts disagree about the future. I focus on ambiguity about the mean of the productivity shock in a textbook Diamond (1982), Mortensen (1982), and Pissarides (1985) (DMP) model. The (log) productivity follows an AR(1) process with zero-mean normal innovations. Firms and workers are not sure about the mean of these innovations and behave as if they had a set of beliefs about it, centered around zero. They evaluate the future using the lowest point from this set. This makes them behave as if they were pessimistic. A shock to confidence widens the set of beliefs, making the agents behave as if they received bad news about the future.====A shock that lowers confidence makes agents act as if they expect a negative productivity shock tomorrow. The propagation mechanism starts similarly to the one for a productivity shock: the stock price (the value of a firm with a filled vacancy) goes down, the firms post fewer vacancies, making it harder to find a job, that increases unemployment. Output falls as a result. However, it falls much less in response to a confidence shock than in response to a productivity shock. This is because the fears of low productivity are never realized, so the output falls only because of less hiring, not because each worker becomes less productive. This also makes the decline in wages smaller and helps the model keep wages from becoming too volatile even when the volatility of confidence shocks is high. Higher unemployment shifts the bargaining positions in favor of the firm, that increases dividends. The PD ratio falls sharply as a result of both lower stock prices and higher dividends. This makes the PD ratio pro-cyclical, as in the data, and unlike in the standard DMP model.====It is challenging to measure ambiguity. As a result, there is a variation of approaches in the literature. Most of them use survey evidence. One approach asks the respondents directly to provide a range of probabilities for some event.==== Another approach infers ambiguity indirectly from respondents’ choices in Ellsberg-type situations.==== An alternative approach, more common in macro literature, uses the dispersion in survey forecasts as a measure of ambiguity, e.g. the Survey of Professional Forecasters. The idea is that the agents in the model sample experts’ opinions. If the experts disagree (the dispersion is large), the agents are not confident in these forecasts and perceive high ambiguity.==== It is important that the dispersion uses the mean forecast reported by each expert, as opposed to using standard deviation around the mean.==== Izhakian (2020) develops an approach to measure ambiguity from market data in a way that allows to distinguish it from risk and from preferences, and Brenner and Izhakian (2018) apply this approach to the U.S. stock market data.====To quantify confidence shocks, I follow Ilut and Schneider (2014) in using the dispersion of the real GDP growth forecasts from the Survey of Professional Forecasters.==== Specifically, I use the inter-quartile dispersion, the difference between the 75th and the 25th quantiles. A large dispersion means that the optimistic forecasts are far above the pessimistic ones, i.e. the experts disagree more about the future growth. Confidence is counter-cyclical: Fig. 1 shows that the dispersion goes up sharply during recessions and declines during recoveries. In the model, I use the correlation between the forecast dispersion and the GDP growth to pin down the correlation between confidence and productivity shocks. Fig. 1 also shows that confidence moves together with the stock market. To check the robustness of my results, I also use an additional measure of ambiguity from Brenner and Izhakian (2018), which produces nearly identical results (see Appendix for more details).====Because of more realistic dynamics, confidence shocks make stock returns as predictable as they are in the data. As in a basic asset pricing model, the stock price is the sum of expected future dividends discounted at required returns. Hence, the PD ratio aggregates the information about the future returns and dividend growth and may potentially predict both of them. The empirical literature explored this idea extensively.==== Most papers find that returns are indeed predictable with PD ratios (and more so for longer horizons), while dividend growth is mostly not predictable. Confidence shocks help the model match the predictability of returns with the estimated coefficients on the PD ratio that are close to the data. In addition, I consider predicting returns and dividends with the unemployment rate, and I find them both predictable both in the model and in the data. The model is consistent with Liu (2019) which finds that vacancy rates negatively predict returns and positively predict cash flows.====The model with confidence shocks matches the (untargeted) term structure of the equity premium, returns and price volatility. Van Binsbergen et al. (2012) (BBK) provide evidence for higher importance of the near-term risk relative to the long-term risk. They find that means and volatilities of excess returns, Sharpe ratios, and volatilities of prices are higher for short-term assets. These facts are hard to match in models which rely on long-term risk. For instance, discount rate shocks lead to larger premiums and higher volatilities of long-term assets, that is not the case in the data. BBK show that the models with habit formation, long-run risk, and rare disasters fail to fit the declining risk premia and volatilities.==== In contrast with these models, a labor search model is designed to emphasize near-term uncertainty. Indeed, with the separation rate of 10% per quarter, an average employment tenure is 2.5 years, a job is a relatively short-term asset, and short-term cash flows are more important for stock prices and returns. This feature makes long-term risk and discount rate shocks matter less in a labor search model as compared to a standard DSGE model with capital accumulation. For example, Mukoyama (2009) shows that an extremely large variation in discount factors is needed to produce realistic labor market volatility. As a result, the literature relies on the features that amplify long-term risk, such as physical or human capital accumulation. In this paper, I show that ambiguity helps generate high risk premia and volatility without the need for adding a slow-moving state variable like capital. In addition, the means and volatilities of returns and volatilities of prices decline with horizon – like in the data. The model still cannot match the declining Sharpe ratios because the volatilities go down too fast.====Solving the model presents a computational challenge. The standard local solution methods require to find the steady state before solving for policy functions. This is possible in models with rational expectations since the agents expect the economy to stay in the steady state. In contrast, the agents in this paper expect a negative productivity shock tomorrow even when the economy is in the steady state, so they need to predict the response of the economy to this shock. For example, in order to decide how many vacancies to post, firms need to forecast the job finding rate tomorrow, that is an equilibrium object. Therefore, they need to know the equilibrium mapping. I use the solution method from Ilut et al. (2015) that allows to jointly solve for the steady state and policy functions when agents’ expectations can potentially deviate from the rational expectations assumption.====Labor search with ambiguity is a useful tool for policy analysis. Most of the literature that studies macroeconomic policies and the business cycle relies on linear methods. Linear methods remove the effects of risk such as precautionary savings and equity premia – leaving these topics under-represented in policy discussion and decision-making. The literature that studies the effects of risk uses complex nonlinear solutions that make it challenging to have many state variables, limiting the scope of policy-relevant questions that can be addressed, especially if the general equilibrium effects are of interest. In contrast, this paper uses ambiguity that affects the mean expected productivity, therefore the ambiguity has first order effects that are present in the linear approximation of the model. This feature makes it possible to use linear methods (such as Dynare) to study the effects of uncertainty without the limitations above. Since the labor search framework is becoming more widely used to jointly study the business cycle and the stock market, it is important that with ambiguity, it can be studied with linear tools, opening it to a wide range of policy questions, especially those related to the labor market in general equilibrium.====Following Shimer (2005), I consider two alternative versions of the model to study other potential driving forces of the business cycle in a search framework: bargaining power shocks and separation rate shocks. In each version, I add ambiguity about the corresponding shock. While I use survey data to pin down the correlation of ambiguity with productivity shocks, I keep ambiguity shocks iid in both alternative versions. Similarly to the model with productivity shocks, I calibrate the mean and variance of ambiguity to match the mean and variance of the excess returns. Then I ask if the resulting mean and variance make sense and study the dynamics. The main finding is that despite the differences in propagation mechanisms across the three shocks (productivity, bargaining power, separation rate), the shocks to confidence have similar implications in all three models. This is because confidence shocks affect the economy exclusively through expectations – they are bad news about the future that never come true. In all three models, pessimistic firms post fewer vacancies, while pessimistic workers accept lower wages, regardless of the reason for their pessimism. Still, different magnitudes of the effects and the differences in the propagation of other shocks lead the results to vary across the models.====In the model with bargaining power shocks, the lack of confidence about the workers’ bargaining power leads to asymmetric worst cases for firms and workers: they act as if they disagreed about the future. Indeed, workers are worried about losing their bargaining power, while firms fear an increase in the workers’ bargaining power. I also consider versions of the model in which only firms or only workers are averse to ambiguity – and I find similar results. Specifically, as compared to the model with productivity shocks, the model with bargaining power shocks generates smaller volatilities of output and wages, a little too high volatility of prices but more realistic dynamics of all other variables: unemployment, vacancies, tightness, dividends, and the PD ratio. Importantly, the model produces procyclical dividends and lower volatilities of dividends and of the PD ratio, bringing them closer to the data. To match the mean and variance of returns, the model requires relatively small mean and variance of ambiguity: one and two percent of the steady-state bargaining power, respectively.====The model with shocks to separation rate appears to be less useful for the purposes of this paper. The same calibration strategy leads to a large and volatile ambiguity. Ambiguity averse agents expect the separation rate to increase by 24% the next quarter (from 0.1 to 0.124). The standard deviation of ambiguity is 0.68: a negative confidence shock of 1 s.d. leads the agents to expect a 68% increase in the separation rate (from 0.1 to 0.168). These numbers seem large relative to the 12%-14% standard deviation of separation rates in the data.==== This model also produces countercyclical vacancies and dividends. The resulting term structure of the equity premium implies lower returns on short assets. This is because higher separation rates increase dividends by shifting the bargaining positions in favor of the firm. As a result, higher expected separation rates lead to higher expected dividends that raise prices of short-term assets, lowering their returns.====This paper is related to a large literature on search and business cycle, starting with Diamond (1982), Mortensen (1982), and Pissarides (1985). Shimer (2005) showed that the textbook DMP model fails to produce realistic dynamics of labor market quantities and started a strand of the literature seeking to resolve this puzzle. One solution is to stabilize wages in order to make profits more sensitive to shocks. Hagedorn and Manovskii (2008) offer a calibration that uses higher value of unemployment leading to more stable wages. In Petrosky-Nadeau et al. (2018), a DMP model with a high value of unemployment endogenously generates disaster dynamics leading to large volatilities, that combined with high risk aversion, result it a large equity premium. Heiberger (2020) further analyzes the mechanism behind the endogenous disasters and how it helps address the equity premium puzzle. Hall and Milgrom (2008) replace the Nash bargaining protocol with alternating offers bargaining by Binmore et al. (1986) that makes wages less sensitive to shocks. Hall (2017) extends this approach to show that with credible bargaining, shocks to discount rates have larger effect on unemployment.====Another strand of literature uses Epstein-Zin (EZ) preferences and capital accumulation to amplify the effects of shocks on the economy. Kilic and Wachter (2018) use a model with EZ preferences and a small variable risk of rare disasters to explain the volatility of both labor and stock markets. Kehoe et al. (2020) find that human capital and EZ preferences with time-varying risk lead to high risk premium and realistic dynamics. Bai and Zhang (2021) use Epstein-Zin preferences and capital accumulation and also explain the equity premium, stock market volatility, and the dynamics of labor market variables. Basu et al. (2021) focus on risk-aversion shocks and emphasize the importance of transitions between full time and part time jobs over the business cycle. In addition to heterogeneity of jobs, their model features capital accumulation and Epstein-Zin preferences. It explains 90% of the equity premium and a large fraction of business cycle comovements of output, consumption, employment, and investment.====Borovicka and Borovickova (2021) (BB21) discipline the approaches to solving the Shimer puzzle by constructing a non-parametric bound on the profits process so that this process produces large enough employment fluctuations given the discount factor inferred from the stock market data. They further decompose the variance of profits into the conditional variance and the variance of the conditional expectation and discuss the role of these two components in the literature on Shimer puzzle. For example, Hall and Milgrom (2008) and Shimer (2010) have risk-neutral agents and hence rely only on predictable fluctuations in profits (the variance of conditional expectation), while Hall (2017) has variable discount factor that makes both components matter. Next, BB21 document the puzzle that the profits in the data are not volatile enough, as compared to the bound. The authors suggest that this is because the existing measures of profits reflect the ==== profits, while firms make decisions based on the ==== profits. Next, they use a model to demonstrate one way to resolve the puzzle: financial constraints may make ==== profits more volatile relative to ==== profits. My paper complements BB21 by considering another way to resolve the same puzzle: confidence shocks make ==== profits more volatile than ==== profits.====This paper does not aim to match all the macro-, labor-, and financial moments by combining the features like a new bargaining protocol, Epstein-Zin preferences or slow-moving state variables. Instead, it contributes to the literature by showing that ambiguity offers a simple way to improve the performance of the textbook DMP model, especially when it comes to financial variables, and it gives finer details on the implications of ambiguity in three versions of the DMP model. Ambiguity alone is not enough to explain the business cycle but it is a good tool to be used along with other features to study questions related to the business cycle, the labor market, and the stock market in a tractable general equilibrium model that can easily be extended e.g. to allow for the policies that impact these markets simultaneously.====Section A.3 in Appendix offers a more detailed discussion of alternative mechanisms and runs a few exercises to compare ambiguity to time-varying volatility, discount rate shocks, and habit formation. Interestingly, when ambiguity is added in a version of the model with habit formation, I find that habit formation amplifies the effects of ambiguity on the economy. Specifically, the model with habit formation needs twice smaller ambiguity to fit the equity premium and it needs twice less volatile ambiguity to fit the volatility of excess returns.",Asset prices in a labor search model with confidence shocks,https://www.sciencedirect.com/science/article/pii/S0165188922002676,10 November 2022,2022,Research Article,50.0
"Feng Jingbing,Xu Xian,Zou Hong","School of Business, East China University of Science and Technology, China,Department of Insurance and Risk Management, Fudan University, China,Faculty of Business and Economics, University of Hong Kong, China","Received 8 November 2021, Revised 29 October 2022, Accepted 2 November 2022, Available online 3 November 2022, Version of Record 23 November 2022.",https://doi.org/10.1016/j.jedc.2022.104562,Cited by (0),"We study how the clarity of COVID-19 risk communications affects COVID-19 insurance demand using proprietary prefecture-level insurance data from China. We find that when local disclosures of COVID-19 risk contain case origin information, local purchases of COVID-19 insurance and local Internet searches for COVID-19 information increase, even after controlling for newly confirmed local cases and new deaths. Our results are robust to using the disclosure clarity of a major neighboring city. The findings suggest that providing improved knowledge about risk to individuals lead them to engage in more risk management. Our evidence contributes to the debate over how risk communication affects individuals’ risk-related behaviors.","Access to relevant risk information is key to risk management decision making, and understanding how individuals react to risk information when making behavioral choices has long been of interest to academics and policymakers alike. Some researchers find that more information (or improved knowledge) about risk leads individuals to reduce risky behaviors or improve their risk management (====; ====; ====; ====).==== Others, however, find that improved information can reduce information ambiguity==== and subjective risk, which may reduce the perceived need for risk management (====; ====; ====; ====; ====).==== While information disclosure can be an attractive public policy tool to improve consumers’ choices in many contexts, its efficacy is not widely tested in personal ==== risk management. ==== necessarily improve personal insurance decision-making. In this paper, we contribute to the debate by examining how communications regarding a nascent but rapidly expanding risk about which the public knows very little—the COVID-19 pandemic—affect individual and household insurance decisions. The COVID-19 pandemic is a particularly interesting context, as it caused a global economic recession and reduced people's income while simultaneously increasing their protection needs.====Using proprietary nationwide insurance data obtained from the developer of a leading mobile social communication app in China with a built-in online insurance platform, we examine how the clarity of public local (prefecture-city level) disclosures of COVID-19 risk (whether the disclosure contained detailed information about case origins) affected local demand for COVID-19 insurance between January 22, 2020, which was one day after China's official designation of COVID-19 as a legally quarantinable infectious disease, and March 16, 2020, when the pandemic was declared effectively under control. March 16 is also the last day for which our sample data are available.====At the beginning of the pandemic, people were aware that they risked becoming infected (the “COVID-19 infection risk”) but knew little about the probability distribution of this infection risk (i.e., they faced significant ambiguity). This means that alongside the standard quantitative disclosures regarding the number of confirmed cases, qualitative disclosures such as the identification of case origins may decrease the ambiguity of COVID-19 information. These qualitative disclosures, however, may also increase the salience of infection. Therefore, the effects of COVID-19 case origin disclosures on local demand for COVID-19 insurance are unclear ex ante. On the one hand, individuals’ perceptions of the risk levels to which they are exposed are greater for more ambiguously communicated risk (====). Ambiguity-averse people tend to make a worst-case assessment of information quality when processing ambiguous news (====) and to stay away from investments whose returns are more difficult to predict (====; ====).==== Information regarding case origins may help to reduce subjective risk and ambiguity in COVID-19 risk, allowing people to better assess the objective risk of infection; detailed case origin information may also signal to the public that the government authority can effectively trace and contain the spread of the virus. This could mitigate people's concern and lower the demand for insurance protection. A lack of information about case origins could also generate anxiety among local residents and increase subjective risk, as cases with unknown origins increase the difficulty of infection containment. This line of reasoning suggests a negative relationship between local disclosures of case origins and demand for COVID-19 insurance.====On the other hand, detailed information about COVID-19 cases may heighten individuals’ perception of risk and increase the salience of infection, both of which would increase insurance take-up (====). If qualitative information regarding case origins is available, people may perceive higher risk: detailed information about case origins can show that the virus is highly infectious, and such salience may result in overreactions from individuals (====; ====). ==== model an anxious agent who is more averse to imminent risks than to distant risks and show that such an agent may choose to buy short-term insurance coverage without much foresight (even if the insurance policy charges an expensive premium). In a pandemic setting, people may be under-informed about potential infection risks (====). ==== argue that for investors who care about both risk and ambiguity, “disclosure can actually exacerbate difficulties by making investors aware of obscure outcomes that they otherwise might not have thought possible.” Thus, it is also possible that disclosing COVID-19 case origins increases people's perceived risk and leads them to take up more COVID-19 insurance to cover this perceived risk. We test the above predictions and find that controlling for the number of newly confirmed cases and new deaths (as proxies for risk information), public disclosure of case origins results in a higher take-up of COVID-19 insurance policies, as measured by the count of new policies and premium amounts.====A potential threat to our inference is the endogeneity of local disclosures of COVID-19 case origins, which is likely to arise from the existence of omitted variables that are correlated with both local COVID-19 disclosures and COVID-19 insurance take-up. We mitigate this concern in two ways. First, we take advantage of the time-series variation of COVID-19 disclosures and estimate ==== prefecture cities, which allows us to effectively control for the effects of omitted time-invariant prefecture-level variables. Second, we show that the local take-up of COVID-19 insurance also responds to case origin disclosures coming from a major neighboring prefecture city (in provinces other than Hubei, whose prefecture cities were completely locked down to each other within the province). We find a no-result in Hubei, which shows that economic co-movements between neighboring prefectures are unlikely to be responsible for our finding in the neighboring-prefecture test. The reason for this is that while lockdowns prevent the virus from spreading between two neighboring prefectures via population migration, they should have a minimal effect on the economic co-movement between a prefecture and its major neighboring prefecture. Therefore, the positive response of COVID-19 insurance take-up to local disclosures of case origins is unlikely to be due to omitted time-variant prefecture-level variables; instead, geographical proximity to the disclosed case trajectory appears to be the most important. In another placebo test, we find that the effect of local disclosures of case origins on insurance take-up is limited to insurance policies that cover COVID-19 risk, with no such effect on other types of insurance that bear little relevance to COVID-19 exposure. Taken together, the above tests mitigate the concern that our finding is driven by the endogeneity of local disclosures of COVID-19 case origins.====Admittedly, our result is predicated on the availability of a supply of COVID-19 insurance and reflects an equilibrium of demand and supply. However, it is important to note that our result is not due to any prefecture-specific ==== efforts by insurance suppliers. We confirmed with the leading online insurance distribution platform (the data provider) that all COVID-19 insurance policies marketed through the platform were simultaneously available to all users of the popular mobile communication app nationally with the same premium rates and policy terms, which means that local prefecture governments do not have any control over the supply of local insurance. We were also informed that neither the distribution platform nor its insurance company partners engaged in location-specific promotions of this insurance, as such promotions can offset the advantages of selling insurance through the app and increase the cost to companies significantly. This enables us to carry out a cleaner study of how case origin disclosures affect insurance demand by holding insurance supply constant across different prefectures on each day. We also show that our result is not driven by the larger supply of COVID-19 insurance that was probably available in the latter part of our sample period. Overall, our result suggests that the disclosure of COVID-19 case origins heightens perceived infection risk and induces demand for protection.====To understand why COVID-19 case origin disclosures increase household take-up of COVID-19 insurance, we show that upon such disclosures, the number of Internet searches for COVID-19-related keywords increases by about 9%. The effect of case origin disclosures on COVID-19 insurance take-up also increases as the proportion of vulnerable people in the population increases, but is attenuated by people's lower level of confidence in local governments.====Our study makes three contributions to the field. First, it extends the debate in the literature over how ambiguous information affects people's risk perception and how this in turn leads to differences in personal risk management. Prior studies have focused on unsafe sexual behaviors (e.g., ====, ====, ====), child malnutrition (====), contaminated drinking water sources (====), and health behaviors (e.g., ====). However, with several exceptions (e.g., ==== on checking account overdrafts, ==== on health insurance), there is limited evidence in the context of personal financial management, particularly in relation to tackling a new pandemic risk. Financial decisions are more complex than the settings listed above, involve more limited information, require financial literacy, and are more affected by cognitive limitations and behavioral biases (====). Our study extends this line of research by examining insurance decisions and COVID-19 disclosures during the current pandemic.====; ====), but few studies have examined the role of public information. Although public information disclosure is an attractive public policy tool for improving consumers’ choices in many contexts, its efficacy is not widely tested in insurance settings (====) play a role in the interpretation of disclosed case origin information and how this interpretation feeds into insurance decisions. While our results are also consistent with the risk salience theory, data limitations (e.g., a lack of subsequent policy surrender information) preclude us from directly testing salience bias. Nevertheless, salience bias is likely to have limited economic consequences on the insured in our setting, given that COVID-19 insurance only lasts for a year and involves a moderate premium.====, ====), investors’ expectations of future growth (e.g., ====), economic anxiety (e.g., ====), household consumption (e.g., ====), labor demand (e.g., ====), and domestic violence (e.g., ====), among others. Additional studies examine how differences in political beliefs affect people's compliance with social distancing and self-isolation orders (e.g., ====) and health behaviors and policy preferences (e.g., ====). To the best of our knowledge, no prior studies have examined how individuals and households respond to COVID-19 risk via financial risk management. Using unique data from China, we fill this research gap.==== find that in the U.S., individuals tend to overestimate the mortality of COVID-19 but underestimate the non-linear nature of its transmission. They argue that information and public education may play a central role in infection containment and in managing the negative economic impact of increased economic anxiety. Our evidence complements their study by showing that increasing qualitative disclosures in COVID-19 risk communications can encourage people to use financial risk management tools to tackle economic anxiety.====The remainder of this paper is organized as follows. In ====, we introduce the institutional background. In ====, we develop our hypotheses. In ====, we describe our data and research design. We present our empirical results in ==== and conclude the paper in ====.====“Between 12am and 11:59pm of January 24, Shanghai reported 13 newly confirmed cases. So far there are 33 cumulative confirmed cases; of which, 30 cases are in a stable condition, 2 cases are in critical condition, and 1 case has been discharged from hospital after treatment. In addition, there are 72 suspected cases.”====“Till 5pm of January 27, Tieling reported two cumulative confirmed cases, both of which are imported cases. One is in Xifeng and the other is in Diaobingshan. The two cases are under isolated treatment in designated medical institutions. There is no newly confirmed case. So far there are 66 cumulative close contacts under medical observation.”====“There were 5 newly confirmed cases; two patients were healed and discharged from the hospital; cumulatively there were 97 confirmed cases, 19 discharged cases, and one death; 975 people having close contact with confirmed cases were put under medical observation. Below is information about the 5 newly confirmed cases:====“By 10pm of January 26, our city had one new imported confirmed case. The patient (resident of Yunxiao County) is a female and 37 years old. She came to Zhangzhou from Wuhan on January 19, went to a COVID-19 designated hospital on January 23 and was put under isolation treatment with a stable condition. During her stay in Zhangzhou, she has closely contacted 11 people including family members, friends, and restaurant waitress, and all of them are now under medical watch.”",Risk communication clarity and insurance demand: The case of the COVID-19 pandemic,https://www.sciencedirect.com/science/article/pii/S0165188922002652,3 November 2022,2022,Research Article,51.0
Azevedo Vitor,"Chair of Financial Management, Technical University of Kaiserslautern, Gottlieb-Daimler-Stra..e 42, 67663 Kaiserslautern, Germany","Received 17 May 2022, Revised 13 October 2022, Accepted 31 October 2022, Available online 3 November 2022, Version of Record 20 November 2022.",https://doi.org/10.1016/j.jedc.2022.104560,Cited by (0),"I estimate a theory-based behavioral momentum using analysts’ predictable (errors driven by) underreaction (APU) as a proxy for newswatchers underreaction. The results show that APU strongly predicts analysts’ errors and, more importantly, stock returns. A long-short strategy based on APU generates a value-weighted Fama-French six-factor alpha of 0.85% per month (","There is ample evidence that momentum strategies can predict the cross-section of stock returns.==== The lack of rational explanation==== for the momentum effect opens venues for researchers to put out behavioral theories seeking to bridge this gap. Among the theories, Barberis et al. (1998), as well as Daniel et al. (1998) propose models with relative agents. Although both models adequately describe the anomalies they were designed to explain, Fama (1998) argues that these models are not able to explain other anomalies.====The third influential model, proposed by Hong and Stein (1999) (henceforth HS model), has heterogeneous agents, newswatchers, and momentum traders. The newswatchers observe private signals about fundamentals to make their forecasts. However, they do not look at the current or past prices, and information is gradually diffused among them. The momentum traders only look at the trend of returns, not at the fundamentals. As the information from newswatchers diffuses gradually, their underreaction drives momentum traders’ actions, making the prices reach equilibrium more quickly. However, as momentum traders contribute to the continuation of the upward or downward price trend even after the equilibrium price is reached, the underreaction causes an overreaction.====Among the advantages of the HS model, empirical evidence shows that the information is more slowly diffused among small firms (e.g., Bernard, Thomas, 1989, Bernard, Thomas, 1990) and firms with lower analysts’ coverage (e.g., Hong et al., 2000). In addition, the heterogeneous agent model can be adapted to explain short-sale constraints (e.g., Daniel et al., 2019).====Despite those behavioral theories, studies rely on the prior performance of predictors to assess how these strategies should be measured.==== As a result, the specifications of momentum predictors could be driven by data mining and might change over time and regions. Furthermore, the momentum strategies based on prior returns can pick not only underreaction but also some other elements, such as crash risk (e.g., Barroso, Santa-Clara, 2015, Daniel, Moskowitz, 2016).====Motivated by the HS model’s prediction that newswatchers’ underreaction causes the momentum effect, I propose a behavioral theory-based momentum predictor. My assumption is that analysts’ forecasts are a good approximation of market expectations from newswatchers. This assumption is supported by empirical findings that analysts’ forecasts rely on fundamentals and underreact to the momentum effect (e.g., Engelberg, McLean, Pontiff, 2020, Jegadeesh, Kim, Krische, Lee, 2004). Chen et al. (2020) find that analysts’ revisions are correlated with the post-earnings announcement drift and, accordingly, are correlated with stock returns. Furthermore, previous studies show that analysts overweight their private information when forecasting corporate earnings (e.g., Chen and Jiang, 2006). Finally, one can expect that newswatchers closely follow analysts’ forecasts and, accordingly, have similar expectations.====I propose a monthly estimate of analysts’ predictable (errors driven by) underreaction (APU)==== based on a two-step regression.==== The first step consists of estimating pooled ordinary least squared (OLS) regressions, including 60 months (rolling window) of data of analysts’ errors on 12-month lagged explanatory variables. Motivated by Hughes et al. (2008),==== I use previous analysts’ revisions (e.g., Jegadeesh et al., 2004), prior 12-month returns (e.g., Abarbanell, 1991), and earnings surprises (e.g., Abarbanell and Bernard, 1992) as explanatory variables. In the second step, I estimate APU by multiplying the first-step regression’s coefficients by the current predictors of underreaction.====My results show that APU is a strong predictor of the cross-section of stock returns from January 1983 to December 2020. A value-weighted (10-1) long-short portfolio sorted on APU generates a subsequent excess return of 1.19% per month (====-stat = 3.51). After controlling for risk factors, I find positive and statistically significant risk-adjusted returns in all specifications. For instance, a long-short portfolio sorted on individual APU generates monthly alphas==== of 0.85% (====-stat = 3.48) after controlling for Fama and French (2018) six-factor model (FF6). In line with the HS model, which predicts that the prices are readjusted while the information is gradually diffused among newswatchers, I find that stock returns driven by APU are roughly 6.5 times higher on earnings announcement days than non-earnings announcement days.====To address possible concerns that APU is just a repacking of anomalies, I test alternative specifications and find that: 1) APU has orthogonal information to earnings and returns momentum; 2) the variable combinations used to estimate APU are not sufficient to predict returns; 3) the relation between APU and the cross-section of stock returns is robust if I use alternative estimates for analysts’ underreaction, such as estimating APU without including earnings surprises or using the underreaction variables proposed by Jegadeesh et al. (2004), and 4) analysts’ predictable errors driven by overreaction and contrarian variables show no statistically significant relation with future returns.====Finally, given the evidence that Carhart (1997)’s momentum factor (MOM) does not sufficiently explain APU, I propose a good-minus-bad (GMB) underreaction factor, estimated with double sorts of firm size and APU. I find evidence that the GMB factor subsumes MOM in spanning tests. Furthermore, the GMB factor added to Fama and French (2015) five-factor model can explain 205 (clear and likely) return predictors from Chen and Zimmermann (2022) at least as well as standard asset pricing models, such as FF6, mispricing factors (Stambaugh and Yuan, 2017), and behavioral factors (Daniel et al., 2020).====My paper makes several contributions to the literature. First, I provide a parsimonious estimate of analysts’ underreaction, which can help to explain the impact of analysts’ underreaction on stock prices. By showing empirical evidence that newswatchers’ underreaction is aligned with underreaction from analysts’ consensus estimates, I provide insight on the findings from Chordia and Shivakumar (2006) and Novy-Marx (2015) that earnings (fundamental) momentum subsumes returns momentum. My findings that an underreaction factor can explain returns and earnings momentum are evidence that returns and earnings momentum are part of the same phenomenon driven by newswatchers’ underreaction and that APU can better capture underreaction as compared to the traditional proxies for earnings and returns momentum.====Second, I contribute to the literature on the relation between analysts’ predictable errors and returns by showing that only the predictable errors driven by underreaction strongly correlate with returns. These results shed light on the findings from Hughes et al. (2008) by showing that predictable errors driven by accruals and overreaction variables are not correlated with future returns. The findings also hold for a model that estimates analysts’ predictable errors with contrarian variables from Jegadeesh et al. (2004) or with the variables used in the So (2013) model.====Third, by estimating a behavioral-theory-based momentum, I provide insights into previous studies that estimate momentum based on prior returns. My results show that APU can subsume prior 12-2 returns (e.g., Jegadeesh and Titman, 1993) and intermediate momentum (e.g., Novy-Marx, 2012) as well as other proxies that aim to capture earnings momentum (Ball, Brown, 1968, Bernard, Thomas, 1989) and anchoring (e.g., George and Hwang, 2004). Finally, the promising results of the underreaction factor in spanning tests and explaining a wide range of anomalies shed light on recent asset pricing studies (e.g., Daniel, Hirshleifer, Sun, 2020, Fama, French, 2018, Stambaugh, Yuan, 2017).====The rest of the paper is organized as follows. In Section 2, I describe the sample selection process and provide details on the APU estimation. In Section 3, I analyze whether APU can predict future analysts’ revisions and analysts’ errors. I then analyze the relation between APU and (excess) risk-adjusted returns in Section 4. In Section 5, I propose and analyze the properties of an underreaction factor, and I conclude in Section 6.",Analysts’ underreaction and momentum strategies,https://www.sciencedirect.com/science/article/pii/S0165188922002639,3 November 2022,2022,Research Article,52.0
Jiang Zhe (Jasmine),"School of Economics, the University of Sydney","Received 12 August 2021, Revised 9 October 2022, Accepted 23 October 2022, Available online 28 October 2022, Version of Record 13 November 2022.",https://doi.org/10.1016/j.jedc.2022.104554,Cited by (0),"This paper uses a two-country dynamic general equilibrium model to consider how, following a trade cost shock, multinational firms’ offshoring decision and the countries’ different factor endowments affect wage inequality between high- and low-skilled workers in the home country. Highlighting task-offshoring, heterogeneous firms, and factor proportions, the study sheds light on how offshoring shapes wage inequality along different time horizons. While the paper’s focus is the relationship between the U.S. (home country) and China (foreign country), its findings are more broadly relevant. The three main findings are these: First, both intensive and extensive margins of offshoring contribute to the widening wage gap, with the latter playing a more important role in the medium run than it does in the initial stage after the trade cost shock. Second, endogenous firm entry raises wages for both high-skilled and low-skilled workers while narrowing the wage gap between the two groups over time. Third, whether firms offshore to a foreign country like Mexico, where the supply of low-skilled laborers is moderately larger than in the home country, or to a country like China, where the supply of low-skilled laborers is vastly larger, makes scant difference to wage inequality in the long run. However, the latter is more beneficial to firm entry, product variety, and consumption.","Since the work of Autor et al. (2013) and Autor et al. (2016), much attention has been given to how increasing Chinese imports have affected the U.S. labor market. However, most of the previous empirical and quantitative studies of the impact of offshoring on the labor market are based on the predictions of steady-state trade models, in which the aggregate variables remain constant over time. This paper contributes to the literature by investigating how wage inequality between high-skilled and low-skilled workers, firms’ offshoring and entry decisions, and countries’ different factor endowments interact in a dynamic setting.==== I study this issue in the context of U.S.-China trade.====Using foreign direct investment (FDI) statistics from China’s Ministry of Commerce, Fig. 1 shows that in the initial stage of China’s opening to the world in the late 1980s, U.S. investment in China remained very low. It was not until the creation of Special Economic Zones (SEZs) in 1992 that the number of foreign-invested enterprises reached its peak. SEZs allowed foreign companies to set up factories that were tax-advantaged for both imports and exports besides granting foreign-market access that was free from government interference. Besides these policy benefits, the persistent decline in the US-China bilateral trade costs is another key factor that contributed to the continuous increase in FDI activities. As shown in Fig. 2, panel A, the trade costs between the U.S. and China saw a decrease of 28 percent from 1995 to 2006 and only went up slightly after 2006. In particular, Chinese tariffs had a sharp decline of 81 percent from 1992–2015 while U.S. tariffs declined by 48 percent (Fig. 2, panel B). Effectively, the realized FDI values continued to increase, reaching a peak in the early 2000s, roughly around the WTO post-accession period, and thereafter remaining quite high for several years. Another notable statistic shown in Fig. 3 is that since 1987 the United States has ranked third after Hong Kong and Taiwan in terms of the total number of firms with offshore operations in China. According to Antras et al. (2017), for the U.S., China is also the second largest sourcing country after Canada.====As multinational enterprises moved factories to China, China saw further increases in processing trade activity. According to Xu (2014), the share of exports by foreign invested enterprises (FIEs) over total exports increased from 12.6 percent in 1990 to 47.9 percent in 2000, and the share of processing exports over total exports rose from 40.9 percent in 1990 to 55.2 percent in 2000. The ratio of processing exports started to climb up in the mid-1980s and has exceeded 50 percent since 1995.====The surge of multinational firms’ offshoring activities in China has coincided with increased attention to expanding wage inequality in the United States. Many studies point to the role of changes in the economic environment, particularly in production technologies that favor high-skilled workers and widen wage distribution.==== A competing explanation emphasizes international trade, thought to have harmed domestic low-skilled workers with the introduction of cheaper imported goods that compete with domestically made goods. Recently, trade with China has been identified as an important driver of wage inequality in developed countries.==== Within the U.S., the policy debate on China’s integration into the global trade system–commonly referred to as the China shock – puts great emphasis on how painful the adjustment to it has been for both firms and workers.====This paper uses a two-country dynamic general equilibrium model with country-level differences in factor proportions, firm heterogeneity and endogenous firm entry to explore the relationship between offshoring and wage inequality. Heterogeneous firms’ production takes the form of a Cobb-Douglas aggregate of high-skilled and low-skilled labor, which is augmented by aggregate productivity and firm-level idiosyncratic productivity. Firms that offshore substitute home with foreign low-skilled labor in production. Offshoring is driven by cross-country differences in low-skill wages, which in turn reflect differences in factor proportions. Low-skilled labor is relatively more abundant in the foreign country that hosts offshoring than the home country. More offshoring opportunities increase the value of firms, giving rise to more firm entry.====I then use the model to map along different time horizons the channels through which offshoring cost reductions affect wages of different skill groups and the wage gap between them. The model implications are as follows. First, offshoring to a country with a more abundant low-skilled labor pool does substantially increase the wage gap between high-skilled and low-skilled workers in the home country immediately after the shock. As time passes, however, endogenous firm entry raises wage for both high-skilled and low-skilled workers while narrowing the wage gap between the two groups. This implies that the firm-entry effect of offshoring partly offsets its labor-substitution effect and mitigates its negative impact on the home low-skilled labor over time. Second, the intensive and extensive margins both operate to shape the wage gap in the home country, with the latter playing a more important role in the medium term as opposed to the immediate periods succeeding the shock. Last but not the least, the wage gap in home rises by more when offshoring is hosted by a country that is more abundant in low-skilled labor (i.e., China) than another (i.e., Mexico) on impact. However, in the medium term the difference vanishes, as firm entry in the home country rises by more in the offshoring-to-China case, thus mitigating the downward pressure on the home low-skill wage. This suggests that offshoring to the relatively low-wage country produces similar effects on the home wage gap, but is more beneficial to firm entry and product variety. In addition, I discuss the model’s implication of the recent U.S.-China war, which is the reverse of the previous trade liberalization period. The finding shows that the tariff hikes generate welfare losses for both countries, mainly due to a fall in average firm productivity, a reduction in the number of products available to consumers, and the drop in average firm profits.====To evaluate the properties of the model, I compute its unconditional second moments and compare them to the data. Furthermore, to better examine the empirical fit of the model, I perform an analysis which feeds data-disciplined into the model and compares model predictions with the data. The model disciplined with time-series data for bilateral iceberg trade cost, the tariff rates, and the TFPs predicts the time path for offshoring activities (using measures of both the share of offshoring firms and the share of offshore employment), the U.S. skill premium, and the Chinese skill premium reasonably well. Following the rise in offshoring, the U.S. skill premium increases whereas the Chinese skill premium falls. As noted, my analysis focuses specifically on the U.S. and China. However, the lessons learned offer insights useful for understanding the implications whenever offshoring takes place between countries whose relative endowment characteristics are similar to theirs.",‘Multinational Firms’ Sourcing Decisions and Wage Inequality: A Dynamic Analysis,https://www.sciencedirect.com/science/article/pii/S0165188922002573,28 October 2022,2022,Research Article,53.0
Zema Sebastiano Michele,"Institute of Economics, Scuola Superiore Sant'Anna, Piazza Martiri della Libertá Pisa, 56127, Italy","Received 30 January 2023, Accepted 31 January 2023, Available online 17 February 2023, Version of Record 17 February 2023.",https://doi.org/10.1016/j.jedc.2023.104608,Cited by (0),None,None,Corrigendum to ‘Directed acyclic graph based information shares for price discovery’ [Journal of Economic Dynamics and Control 139 (2022) 104434],https://www.sciencedirect.com/science/article/pii/S0165188923000143,17 February 2023,2023,Research Article,59.0
Wu Jieran,"Academy of Financial Research and School of Economics, Zhejiang University China","Available online 12 April 2022, Version of Record 26 July 2022.",https://doi.org/10.1016/j.jedc.2022.104400,Cited by (0),None,None,Comments on “Sentiments and real business cycles”,https://www.sciencedirect.com/science/article/pii/S0165188922001063,12 April 2022,2022,Research Article,62.0
"Dong Feng,Jia Yandong,Wang Siqing","School of Economics and Management, Tsinghua University, China,Research Bureau, People’s Bank of China","Available online 12 April 2022, Version of Record 26 July 2022.",https://doi.org/10.1016/j.jedc.2022.104393,Cited by (0),There is increasing evidence that the vibrant financial ,"Economic fluctuations are often driven by sectoral shocks (e.g., oil price shocks, housing booms) accompanied by resource reallocation across sectors. Most of the theoretical and empirical literature studying resource reallocation focuses on the reallocation of physical capital (Eisfeldt and Rampini (2006), Cui and Radde (2020), Dong et al. (2020)), and its misallocation (Hsieh and Klenow (2009), Reis (2013), Moll (2014), Boissay et al. (2016), Gopinath et al. (2017), Bleck and Liu (2018), and Dong and Xu (2020)). Relatedly, there has been a burgeoning discussion on the connection between misallocation and asset pricing (Dou et al. (2021)). However, there has been a surprising relative lack of theoretical analysis on the misallocation of human capital, or talent misallocation, with the exception of Murphy et al. (1991) and Hsieh et al. (2019). Furthermore, there is evidence that credit booms may attract excess talent into the financial sector in both developed and developing economies (Shi (2018) and Marin et al. (2017)).====As Jordà et al. (2015) and Miao and Wang (2014) emphasize, asset bubbles have often emerged following credit expansion in the past century, and the speculative bubbles in financial cycles have mainly occurred in the housing and stock markets. The Roaring Twenties bubble and the Internet bubble in the late 1990s were both based on speculation about the development of new technologies. Arbitrageurs expect to buy the asset at a low price and profit by quickly reselling it to a “greater fool”. Speculation provides high returns at high risk. Although the service sector, including the financial sector, has been growing (Buera and Kaboski (2012), Philippon and Reshef (2012), Philippon and Reshef (2013)), there have been growing concerns regarding the long-run growth impact of premature deindustrialization and the associated misallocation of human capital across sectors (Rodrik (2016), Rodrik (2017)). The returns to talent have been significantly higher in the finance sector than in other sectors, which attracts more talent into this sector, while the speculation does not contribute to production; see Kaplan and Rauh (2010), Bakija et al. (2012) and Célérier and Vallée (2019). Furthermore, Goldin and Katz (2008) also document a large increase in the fraction of Harvard undergraduates who have worked in the financial sector since 1970. Baumol (1996) shows that economic growth requires the efficient allocation of talent.====What is the impact of speculative asset bubbles on the returns of different sectors? How do bubbles affect the allocation of talent? What are the impacts of talent misallocation across sectors? To address these questions, we extend the one-sector model of Awaya et al. (2019) to two sectors and internalize the agent’s occupational choice. There are two sectors: the financial sector and the manufacturing sector. Individual agents choose to enter one sector to maximize their expected lifetime utility. The financial sector may admit endogenously speculative bubbles and is composed of the initial holder of assets, middlemen, and final users. The setup of the financial sector is based on Awaya et al. (2019). However, we highlight that speculative bubbles are not beneficial under the same condition as theirs. We further study the effect of bubbles on talent allocation across sectors. Talent can choose to enter the financial sector to become middlemen and resell assets to final users. There is uncertainty in the financial sector that the value of assets is zero with constant probability. Thus, the middlemen may fail to resell and suffer a loss. The financial market is not transparent, and the asset can only be traded through the over-the-counter (OTC) market with information asymmetry.==== Prior to trade, the final user may or may not know the asset value, and if he or she knows, he or she sends a signal to upstream middlemen. The signal is transmitted in the spirit of Rubinstein’s electronic mail game ==== Rubinstein (1989), i.e., a signal may be lost with constant probability. When the value of the asset is zero and all agents know this, those who do not receive the signal do not know whether the other middlemen know whether the final user knows the asset value. The lack of common knowledge allows for the existence of asset bubbles: that is, the middleman who does not receive the signal buys the asset and believes that he or she can sell it to the downstream middlemen for a higher price and profit at the expense of others (who would be the greater fools), although he or she knows that the asset is overvalued (greater than zero). This is the essence of the speculative trading of greater fool theory.====The key message of this paper is that the equilibrium allocation of talent in the decentralized economy is not constrained efficient, which encourages too many agents to enter the financial sector. Because of speculative bubbles, agents obtain high returns, and more agents are willing to enter this sector. There are two opposite effects of the number of middlemen, ====, on the marginal utility in the financial sector. One is the congestion effect, which is due to the assumption that the middlemen are arranged in random order. The larger ==== is, the more crowded the financial sector and the lower the expected (average) payoff of being middlemen. The other is the aggregate effect, which stems from the longer transaction chain reducing the trading risks and the aggregate expected payoff, i.e., the sum of the expected payoffs of all middlemen, increasing in ====. Overall, the congestion effect dominates. That is, an increase in the number of middlemen increases the total payoff, but as the middlemen are equally distributed, the payoff after distribution decreases. This reflects the tragedy of the commons. Moreover, these effects differ from the conventional crowd-in and crowd-out effects of asset bubbles, which are characterized in traditional macroeconomic rational bubble models that can hardly capture the emergence of speculation-driven bubbles.====Not only do the overcrowded markets in the financial sector per se generate a negative externality on the manufacturing sector, but speculation is also a source of financial fragility. The reason for this is that when agents make decisions, although they consider the risk of the asset bubble bursting, which can be compensated for through prices, they ignore the negative externality of congestion and the impact of the probability of endogenous bubbles from their entry into the financial sector, taking the number of middlemen as given. An increase in the number of middlemen in the financial sector leads to an increase in the probability that an asset bubble exists. Because, in equilibrium, an asset bubble bursts for certain, the greater the probability that a bubble exists, the larger the aggregate financial risk. Moreover, we consider two extensions, the social cost caused by financial risks and the spillover effect of the manufacturing sector, which result in a stronger negative externality and a greater talent misallocation.====The talent misallocation caused by speculative bubbles not only is harmful to the development of the manufacturing sector but also increases financial risks. Therefore, it is essential to efficiently allocate human resources across sectors. The government can adopt industrial policies to reduce negative externalities, that is, subsidizing the manufacturing sector while taxing the financial sector to enhance the attractiveness of the manufacturing sector to talent and promote the optimal allocation of talent. This measure can also be regarded as Pigouvian tax. As the talent misallocation intensifies, considering the social costs and spillover effect, higher tax rates and subsidy rates are required to maximize social welfare.==== The existing macro studies on rational asset bubbles are mostly based on standard frameworks, discussing the existence conditions of bubbles, the transition among multiple equilibria, and the welfare analysis of bubbles, among other topics (Arce and López-Salido (2011), Farhi and Tirole (2012), and Basco (2016)). Even those studies that examine the impact of a bubble burst rely on the assumption of an exogenous bursting probability or bursting process of bubbles (Kocherlakota (2009), Hirano and Yanagawa (2016)). However, as Kindleberger and Aliber (2011) summarized, there are five endogenous phases that all bubbles go through. First, in the displacement phase, technological changes or financial innovations affect the fundamental value of assets, leading to the creation of a bubble. After the bubble occurs, asset prices rise at a slower pace, followed by the boom phase. Then in the euphoria phase, speculators enter, asset prices increase explosively, and trading volume increases. This is followed by the phase of profit-taking when sophisticated investors sell assets, and the price falls as a consequence. Finally, in the panic phase, the bubble bursts, and the price falls to an unreasonably low level. Our model is well-behaved to characterize these five endogenous phases of bubbles from birth to bust.====We only highlight the papers that are most closely related to this study. The first strand of literature focuses on bubbles with asymmetric information. Different from the mainstream research that is based on the common-knowledge assumption, Allen et al. (1993) first construct an asymmetric information model of bubbles. Although all market participants may know that the price of an asset exceeds its fundamental value (the dividend of the asset), they do not all know that all other agents also know this fact. It is this higher-order uncertainty that makes it possible for rational asset bubbles to exist under certain conditions even with a finite horizon. Moreover, Abreu and Brunnermeier (2003) construct a model that is also different from traditional bubble theory. They assume that rational arbitrageurs are sequentially aware that an asset bubble occurs, and the bursting of the bubble requires a certain proportion of arbitrageurs to coordinate. In this case, even if the rational arbitrageurs know that the bubble will burst at some point, they are still willing to ride the bubble. Conlon (2004), Conlon (2015), and Liu and Conlon (2018) summarize this as the greater fool theory in a sequence of papers.==== Based on a heterogeneous agent asset pricing model, Chiarella and He (2003) analyze herding from the perspective of heterogeneous beliefs, which is extended by He et al. (2009), He and Li (2012), and Di Guilmi et al. (2014) thereafter, and Bayer et al. (2021) provides empirical causal evidence on investor contagion. Zheng (2020) combine the heterogeneous agent model with a coordinated game to study how market efficiency affects the existence and bursting of bubbles under information asymmetry. However, in existing studies, the bubbly equilibrium is not robust. Awaya et al. (2019) provide a tractable rational bubble model of the OTC market, demonstrating the uniqueness and robustness of the equilibrium. In OTC markets, trade is sequential and thereby relies on middlemen because of information asymmetry and search frictions. In particular, bubbles can occur with an endogenously increasing price path in their model. However, their model is a one-sector bubble model, which implies that speculative bubbles are always desirable under certain conditions.==== That turns out not to be true in our general multi-sector model, which is due to the congestion effect in the financial sector. Therefore, we integrate their tractable setup into a two-sector model and analyze the impact of endogenous bubbles on talent allocation across sectors. Our results show that the negative externality of congestion may cause talent misallocation.====The second strand of related literature focuses on the resource allocation effect of bubbles. In earlier studies, Tirole (1985) extends Samuelson (1958) and Diamond (1965) and finds that asset bubbles can improve the efficiency of resource allocation by crowding out inefficient investment. Furthermore, by introducing financial frictions, Farhi and Tirole (2012) and Martin and Ventura (2011), Martin and Ventura (2012) show that asset bubbles may reallocate capital. Miao and Wang (2012) and Miao and Wang (2018) apply the theory of credit-driven asset bubbles and find that bubbles make capital allocations more efficient among heterogeneous firms. Miao and Wang (2014) further study the reallocation effect on physical capital across sectors and show that the sector with asset bubbles may attract more capital. Our results are similar to theirs, although we focus on human capital and introduce information frictions. Dong et al. (2019) and Dong et al. (2021) study the effects of housing bubbles and find that the boom of the housing sector in China may crowd out capital in the real sector.====To the best of our knowledge, the talent allocation effect of bubbles has not been studied in the literature. However, the misallocation of talent hampers the efficiency of resource allocation, leading to insufficient innovation and restricting economic development. Hsieh et al. (2019) empirically study the impact of talent misallocation on economic growth caused by ethnic and gender discrimination. Murphy et al. (1991) focus on the impact of talent allocation between the public and private sectors. Kaplan and Rauh (2010) and Bakija et al. (2012) study the evolution of the earnings of individuals with very high incomes, with a particular emphasis on the financial sector. Finally, our paper is connected to the discussion of information asymmetry, disclosure and other policy implications in the literature. See He et al. (2019), Goldstein and Yang (2017), Goldstein and Yang (2019), Yang and Zeng (2019), Yang and Zhu (2020), and Mondria et al. (2021), among others.====The remainder of the paper is organized as follows. First, we provide examples to illustrate the basic intuition in Section 2. Section 3 sets up the model and provides the partial equilibrium characterizations. Section 4 studies the optimal allocation of talent. Section 5 concludes the paper. The technical proofs are available in the Online Appendix.",Speculative Bubbles and Talent Misallocation,https://www.sciencedirect.com/science/article/pii/S0165188922000999,12 April 2022,2022,Research Article,63.0
Li Tao,"Department of Economics and Finance, City University of Hong Kong, Hong Kong, SAR","Available online 12 April 2022, Version of Record 26 July 2022.",https://doi.org/10.1016/j.jedc.2022.104376,Cited by (0),None,None,"Comments on “Dynamic noisy rational expectations equilibrium with insider information: Welfare and regulation,” by Jerome Detemple, Marcel Rindisbacher and Scott Robertson",https://www.sciencedirect.com/science/article/pii/S016518892200080X,12 April 2022,2022,Research Article,64.0
"Jung Juergen,Tran Chung","Department of Economics, Towson University USA,Research School of Economics, Australian National University, Australia","Received 30 January 2022, Revised 30 March 2022, Accepted 31 March 2022, Available online 10 April 2022, Version of Record 28 April 2022.",https://doi.org/10.1016/j.jedc.2022.104374,Cited by (1),We quantitatively explore the economic effects of expanding the public and private components of the US ,"Health risk can be difficult to insure via competitive insurance markets. As pointed out in Rothschild and Stiglitz (1976), information asymmetries between buyers and sellers of private insurance plans can lead to adverse selection and leave large shares of the population uninsured. The principal-agent relationship between healthcare providers and their patents and the highly persistent nature of health risk add further complications to insuring health risk effectively through private insurance markets. These are only some of the arguments used to not only justify government regulation of private health insurance markets but also to call for the direct provision of health insurance by the government to large groups of market participants.==== As a consequence, today the government is the largest provider of health insurance in almost all developed countries. While the approaches to providing health insurance differ across OECD countries, most of them provide some form of basic universal public health insurance (UPHI). Private health insurance plays a somewhat minor role and typically provides supplementary insurance that facilitates additional access to private clinics, more luxurious care such a single-bed hospital rooms, or services that are not covered by public health insurance such as certain types of vision or dental care (e.g., Carrin and James, 2005).====The US, however, is the exception. The US health insurance system relies heavily on private, employer sponsored, health insurance to cover the working population, while public health insurance predominantly covers the high risk population of retired individuals through Medicare and low income individuals through Medicaid. The US health insurance system is a complex system that not only pools health risk but also redistributes wealth through its various financing mechanisms that tie into the progressive US income tax system. Thus far, relatively little effort has been devoted to quantitatively addressing to what extent the US system is able to reduce the exposure to health risk and how welfare is impacted by the heavy reliance on private health insurance. This paper aims to quantify the social insurance role of the US mixed health insurance system by exploring the welfare benefits or losses of adjustments to the current system by either expanding the public or private components.====We use a general equilibrium, overlapping generations model with idiosyncratic income risk and incomplete markets similar to Bewley (1986) and Aiyagari (1994), idiosyncratic exogenous health risk similar to Jeske and Kitao (2009) and Pashchenko and Porapakkarm (2013), and key elements of the US health insurance system such as private individual health insurance (IHI) plans, employer-sponsored group health insurance (GHI) plans, Medicaid, and Medicare. The fact that individuals are exposed to medical expenditure shocks induces demand for health insurance. Individuals have access to private and public health insurance plans which can partially insure their medical spending risk. In addition, they use precautionary saving and labor supply to smooth consumption spending. Health insurance choice is thus jointly determined with consumption, savings and labor supply over the lifecycle.====We calibrate the model to US data and match the lifecycle patterns of income, labor supply, and asset holdings. In addition, the model successfully reproduces the lifecycle profiles of insurance take-up rates for Medicaid, IHI and GHI. Finally, our model replicates important macroeconomic aggregates from national income accounts (NIPA). Our benchmark model therefore embeds the lifecycle structure of health risk in conjunction with income risk as observed in the data.====In the benchmark version of the model, the two types of idiosyncratic risks (wage income shocks and health shocks) are not fully insurable as individuals are borrowing constrained and markets are not complete. Meanwhile, the health insurance system is segmented and a large share of workers remains uninsured. In the model the limited insurance coverage of health risk directly impacts the effectiveness of the limited set of market instruments (i.e., household savings and adjustments to labor supply) to insure against income risk. This induces demand for more social health insurance in order to reduce consumption variance over the lifecycle. There are two channels through which social health insurance is provided in the current US health insurance system. A public health insurance channel via universal public health insurance for retirees (Medicare) and means-tested public health insurance targeting the poor (Medicaid) and a private health insurance channel that includes insurance premium regulations and subsidies via tax deductions of employer-sponsored GHI premiums. In order to quantify the social insurance role of each channel we study several alternative designs of health insurance such as the expansion of Medicare and the expansion of GHI. In these experiments, the government adjusts either a progressive income tax, a payroll tax, or a consumption tax to balance its budget.====We first focus on the effects of switching to an exclusive public health insurance system without any private health insurance options. Specifically, we expand Medicare to include all workers which effectively turns Medicare into a universal public health insurance (UPHI) program. We also remove the IHI and GHI plans from the model. This UPHI reform leads to significant reductions in labor supply and household savings due to crowding-out effects (a move from self-insurance via savings to government insurance) and tax distortions. As a result the economy produces less and households have lower income which leads to welfare losses. However, the UPHI system also eliminates adverse selection issues and improves risk sharing which both lead to welfare gains. Overall our quantitative results indicate that welfare losses caused by incentive effects dominate welfare gains due to insurance and redistribution effects, when the UPHI coinsurance rate is set at the current Medicare level.====The levels of the welfare gains or losses depend critically on generosity of the UPHI system, which is determined by the coinsurance rate.==== By lowering the coinsurance rate of UPHI the government can shift the financial burden of medical care from high risk individuals to the tax paying public. On the other hand, higher coinsurance rates require households to contribute more out-of-pocket to finance their health expenditures and leaves them more exposed to idiosyncratic health risk. Thus, the coinsurance rate provides a redistribution mechanism that can be used to support individuals who are exposed to a high degree of health risk. Yet, the level of coinsurance rate determines the relative size of the incentive and insurance effects. We find that when the UPHI coinsurance rate is set to a higher level than the current Medicare coinsurance rate, welfare gains can be achieved. We then solve for the optimal coinsurance rate that balances out the opposing effects and maximizes overall welfare outcomes. We find that the optimal design of a UPHI system financed by a progressive income tax is achieved when the coinsurance rate is set at around 54 percent. While some household types experience welfare losses from this reform, overall we measure a welfare gain of about 0.21 percent of CEV. When the same UPHI system is financed by a consumption tax, the optimal coinsurance rate is lower at 42.6 percent and the welfare gains increase to around 2.7 percent of CEV. This result indicates that if the government finances the UPHI system with a less distortive tax, it can offer more generous coverage at a lower fiscal cost and thereby achieve higher welfare gains.====The tax burden caused by the UPHI system can be further lessened by allowing private health insurance plans concurrently. We next examine a UPHI system where workers can decide to purchase IHI or GHI plans instead. We assume that premiums for IHI and GHI are set according to current US regulations as in the benchmark model. The option to purchase private health insurance is only relevant if the UPHI coinsurance rate exceeds the coinsurance rates offered by private health insurance plans. Individuals will then consider whether buying more generous private health insurance is worth the premium.==== Yet, allowing some workers to form their own insurance pools decreases the size of the government-run UPHI system, which in return results in smaller fiscal distortions from the required tax increases. The optimal UPHI coinsurance rate in this scenario is around 49 percent when the progressive income tax is used to finance the UPHI system. At this rate about a third of workers decide to stay in private GHI plans which lowers the fiscal cost of the UPHI expansion and income tax revenue increases by “only” 14 percent as opposed to 28 percent in the UPHI reform without any private insurance options. Overall the UPHI system in this environment leads to larger welfare gains of 1.5 percent of CEV. More importantly, the optimal UPHI system with private health insurance plans results in welfare gains for all income groups, which translates into higher overall welfare gains compared to the optimal UPHI system with no private options.====We then explore a reform that would strengthen the private health insurance component of the current US insurance system. In this reform we simulate a government mandate that would force all employers to offer GHI. This removes the risk of a worker being paired with an employer that does not offer GHI as is often the case under the current US system. We find that this reform leads to a large increase in the fraction of workers with GHI from 62 percent to about 91 percent. The experiment shows that mandating GHI offers to all workers and allowing premiums to be tax deductible can help reduce adverse selection. In addition, while offering GHI to all working age individuals increases the labor force participation rate, it does lower the average hours worked and the capital stock. The latter is a direct result of workers moving from self-insurance via household savings to tax deductible private health insurance plans. Overall, the reform leads to moderate average welfare gains of about 0.22 percent of CEV. This experiment demonstrates that further regulation of private health insurance markets (i.e., mandating GHI offers for all workers) can generate welfare gains and improve the social insurance role of the US health insurance system.====In our sensitivity analysis we show that tax financing instruments matter for aggregate welfare outcomes. For instance, if the government uses a consumption tax to finance the UPHI program (as opposed to income or payroll taxes), the tax induced distortions in the labor market are more moderate. This allows the government to support more generous public health insurance with a lower coinsurance rate.====Finally, as an extension we consider two additional healthcare reforms: ==== lowering the Medicare eligibility age from 65 to either 60 or 50 and ==== extending GHI to all individuals (including retirees), while removing all other forms of health insurance. We find that both reforms result in welfare gains. More specifically, the results from the first reform show that the Medicare expansion not only benefits the newly eligible workers but also younger workers. As the 60–64 age cohort leaves the private GHI pool and moves into Medicare, the remaining GHI pool is younger and has lower expected health expenditures. GHI premiums now decrease which directly benefits younger workers with GHI offers from their employers. The second reform removes public health insurance and therefore any tax distortions associated with financing Medicare and Medicaid. In addition, this reform mandates GHI offers to all workers and retirees which improves risk pooling via private insurance markets. The observed overall welfare gains from this reform are the result of the welfare gains from removing fiscal distortions being dominant over the welfare losses suffered by individuals previously covered under Medicare or Medicaid.==== Our paper contributes to a large macroeconomics and public finance literature analyzing the welfare benefits of public transfer programs in quantitative dynamic general equilibrium models (e.g., Auerbach, Kotlikoff, 1987, Hansen, Imrohoroglu, 1992, İmrohoroğlu, İmrohoroğlu, Joines, 1995, Conesa, Krueger, 1999, Fuster, Imrohoroglu, Imrohoroglu, 2007). More recently, Braun et al. (2017) assess the welfare effects of means-tested social insurance for the older population. We follow a similar modeling approach but focus on the social insurance role and welfare benefits of remodeling the US health insurance system.====There is a growing macro-health literature that studies the implications of health and healthcare reforms in the US. The most closely related papers to our study are Jeske and Kitao (2009), Pashchenko and Porapakkarm (2013), Hansen et al. (2014), Jung and Tran (2016), and Zhao (2017).==== While many of these papers cover aspects of US healthcare policy reforms in computational macro frameworks, our model is unique in that it combines detailed aspects of the US healthcare system in a model with exogenous health risk and insurance choice while focusing on a healthcare systems comparison across steady states. The closest to this paper is our own work in Jung and Tran (2016) where we analyze the long-run effects of the Affordable Care Act in the US. The current paper addresses a broader question concerning the overall welfare benefits of the US health insurance system relative to alternative health insurance designs. We demonstrate since the mixed US system fails to insure a large share of the population, welfare improving health insurance reforms are possible but depend on the trade off between positive insurance effects and negative tax distortions.====Our paper is related to the broader literature on incomplete markets macroeconomic models with heterogeneous agents as pioneered by Bewley (1986) and extended by Huggett (1993) and Aiyagari (1994). These models have been widely applied to quantify the welfare effects of public insurance for idiosyncratic income and longevity risks (e.g., Hubbard, Judd, 1987, Golosov, Tsyvinski, 2006, Heathcote, Storesletten, Violante, 2008, Conesa, Kitao, Krueger, 2009, Huggett, Parra, 2010, Krueger, Perri, 2011). This literature shows that if the ability of risk sharing in private markets is limited, then publicly provided risk sharing mechanisms can improve the allocation of risk, smooth non-medical consumption and increase welfare. In these models earning shocks are the sole source of uncertainty and they typically focus on modeling non-medical consumption variation. We introduce health risk as additional source of idiosyncratic variation into an otherwise similar Bewley framework.====Our paper is also connected to the literature on optimal insurance and government redistribution (e.g., Blomqvist, Horn, 1984, Rochet, 1991, Cremer, Pestieau, 1996) as well as the literature on mixed public-private health insurance systems (e.g., Besley, 1989, Selden, 1997, Blomqvist, Johansson, 1997, Petretto, 1999, Chetty, Saez, 2010). These studies analytically investigate the optimal structure of mixed insurance systems in terms of efficiency and equity in highly stylized models. Different from this literature we provide a quantitative analysis using a more realistic model.====The paper is structured as follows. Section 2 presents the full dynamic model. Section 3 describes our calibration strategy. Section 4 describes the main results. Section 5 concludes.",Social health insurance: A quantitative exploration,https://www.sciencedirect.com/science/article/pii/S0165188922000781,10 April 2022,2022,Research Article,65.0
Xu Yang,"School of Economics & Wang Yanan Institute for Studies in Economics, Xiamen University, China","Received 7 September 2021, Revised 30 January 2022, Accepted 17 March 2022, Available online 21 March 2022, Version of Record 5 April 2022.",https://doi.org/10.1016/j.jedc.2022.104364,Cited by (1),"We develop a multi-country ==== model with structural change to investigate the factors affecting the global changes in the skill premium between 1997 and 2007. Trade and technological change increase the skill premium by inducing reallocations to skill-intensive sectors. We apply our three-sector framework to 37 countries, and the model accounts completely for countries’ trade, sales, consumption, and skill premium in terms of different types of fundamental shocks. Technological changes, both skill biased and Hicks neutral, account for most of the increases in the skill premium. The effects of Hicks-neutral total factor productivity growth act through structural change.","Over the past 20 years, both developed and developing countries have experienced a rapid pace of globalization. Many of these countries have also experienced large increases in the skill premium, measured by the wage ratio of college graduates to non-college graduates.==== Fig. 1 illustrates that around two thirds of countries experienced rising skill premium from year 1997 to 2007, including both advanced economies, such as the US and Italy, and emerging ones, such as China and Brazil. While there has been considerable research on the reasons behind the global increase in the skill premium, such as trade and skill-biased technological change, to date, there is no conclusive evidence on the relative importance of different factors.====Moreover, most studies on the effects of globalization on the skill premium focus on reallocations ==== the tradable manufacturing sector. Meanwhile, the global economy has also experienced substantial reallocation of economic activity across broad sectors, that is, structural change. There have been substantial shifts from the agricultural to the manufacturing sector in developing countries, and from the manufacturing to the services sector in developed countries. These structural changes might further increase the skill premium because skill intensity in production is the highest in services and the lowest in agriculture.====Thus, we examine the effects of different factors on the skill premium in the presence of structural change. Specifically, we address the following two questions. First, to what extent have recent changes in the fundamental shocks, such as reductions in trade costs and technological change, impacted the skill premium across countries? Second, what are the contributions from different channels of structural change in affecting the impacts of those fundamental shocks on the skill premium across developed and developing countries?====We address these questions by first developing a multi-sector, multi-country general equilibrium trade model based on (Eaton and Kortum, 2002) and (Caliendo and Parro, 2015), augmented with elements of structural change. We divide the economy into three sectors—agriculture, manufacturing, and services—and all sectors are tradable. The production factors include skilled and unskilled labor as well as intermediate inputs from each sector. Structural change, defined as endogenous changes in sales shares across sectors, is driven by nonhomothetic preferences (Kongsamut et al., 2001), demand complementarity across sectors (Ngai and Pissarides, 2007), and international trade (Uy et al., 2013).====The framework allows us to perform a structural decomposition exercise to quantify the relative importance of alternative explanations, in the spirit of the accounting approach used in (Eaton et al., 2016). Specifically, changes between any two periods can be fully explained by six types of country-specific shocks: (i) changes in the bilateral trade cost between each pair of trading partners in each sector, (ii) changes in each sector’s Hicks-neutral total factor productivity (TFP), (iii) changes in each sector’s relative efficiency of skilled and unskilled labor, (iv) changes in the supply of skilled and unskilled labor, (v) changes in trade deficits, and (vi) changes in the model residuals.====After retrieving these shocks, we conduct counterfactual exercises by feeding in each type of shock separately into the initial equilibrium to evaluate its contribution to the changes in the skill premium.====This rich framework allows for those fundamental shocks to affect the skill premium through various channels. First, it captures skill-biased technical change (SBTC) through changes in sector relative labor efficiency, and changes in relative supply of skilled labor to affect the skill premium, both of which are not related to structural change. Second, changes in trade costs and Hicks-neutral TFP affect income and relative prices across sectors, which in turn affects structural change through nonhomothetic preference and demand complementarity between sectors. Third, changes in trade costs, Hicks-neutral TFP, and relative supply of skilled labor affect international trade flows between countries, thus enabling countries to produce more (less) than what they consume so affects structural change. Last, as international trade affects relative prices across sectors, any shock that affects trade flows further influences structural change through trade-induced changes in relative prices.====We apply the quantitative framework to 37 countries from year 1997 to 2007, a period when global trade values increased by more than 50%. We retrieve data on production, factor payments, consumption, and bilateral trade from four versions of the Global Trade Analysis Project (GTAP), corresponding to year 1997, 2001, 2004, and 2007. In addition, we assemble data on the skill premium from various sources for each sample country.====We use these data to retrieve fundamental shocks based on our framework. First, the model generates a gravity equation that relates bilateral trade flows to bilateral trade costs and sectoral TFP, which we obtain using an approach similar to that of (Parro, 2013) and (Reyes-Heroles, 2016). Second, shocks to the supply of skilled and unskilled labor are adjusted for consistency with the observed skill premium. Third, shocks to the relative efficiency of skilled and unskilled labor are matched to the cost share of skilled labor in total labor payments in each sector. The changes in fundamental shocks replicate the observed sales shares, household expenditure shares, trade shares, payment shares of skilled labor, and the skill premium for 37 countries between year 1997 and 2007. By feeding each type of fundamental shock separately into the model, we quantify its effect on the skill premium. Several results emerge.====First, we find that SBTC is the most important factor that drives the increases in the skill premium. The median effects are 5.7% in developed countries and 5.0% in developing countries. There is much heterogeneity across countries, ranging from -5.1% in the ==== percentile to 24% in the ==== percentile. The reason for the fall in the skill premium is that some countries experienced decreases in the relative efficiency of skilled and unskilled labor, or technological change biased toward unskilled labor. Second, we find that changes in Hicks-neutral TFP increase the median skill premium by 2.9% in developed countries, and by 3.2% in developing countries. Third, changes in trade costs and trade deficits have little effects on the skill premium, except for some countries that have experienced trade liberalization, such as China. Fourth, changes in the relative supply of skilled labor decrease the skill premium because countries on average experienced increases in the relative supply of skilled labor.====The above results illustrate that technological changes, both skill biased and Hicks neutral, are responsible for the increases in the skill premium across countries between year 1997 and 2007. We then investigate how different channels of structural change contribute to the positive effects of changes in Hicks-neutral TFP on the skill premium.====First, we compare to a framework with Cobb–Douglas utility to quantify the contributions from nonhomothetic preference and demand complementarity to the positive effects of changes in Hicks-neutral TFP on the skill premium. We find that through those two channels changes in Hicks-neutral TFP increase the skill premium by 2.6% in developed countries, and by 4.2% in developing countries. Further decomposition shows that both nonhomothetic preference and sector complementarity each account for half of the increases in skill premium induced by the changes in Hicks-neutral TFP. Thus, the impacts from changes in Hicks-neutral TFP on the skill premium are amplified by reallocations to skill-intensive sectors, and this amplification is larger in developing countries because they experienced a larger degree of sectoral reallocations than developed countries.====Second, we apply the changes in Hicks-neutral TFP to the framework with Cobb–Douglas utility to quantify the effects on the skill premium from the international trade channel. We find that changes in Hicks-neutral TFP increase the skill premium in developed countries by 0.3%, but decrease the skill premium in developing countries by 1.3% under Cobb–Douglas utility. The reason is that developing countries experienced faster growth in manufacturing TFP than developed countries, so trade induces them to specialize more in manufacturing, which is less skill-intensive than services. Moreover, this changes in sectoral specialization affect the changes in sectoral relative prices and income, which in turn influences sectoral reallocation through demand complementarity and nonhomothetic preference. To quantify the contribution of this interaction between trade, demand complementarity, and nonhomothetic preference to the skill premium, we compare the effects of changes in Hicks-neutral TFP in the baseline to an autarky framework. We find that the effects of changes in Hicks-neutral TFP through this interaction has further increased the skill premium in developed countries by 0.15%, and decreased the skill premium in developing countries by 0.04%. The above results illustrate the importance of considering the effects of Hicks-neutral technical change in an open economy.====Our study is related to a large body of literature on the impact of international trade on the skill premium focusing on ====-manufacturing reallocations. This body of research includes labor-economics literature, which finds little effects of trade by measuring the changes in factor content of trade (Berman, Bound, Griliches, 1994, Borjas, Freeman, Katz, 1997, Katz, Murphy, 1992), and recent trade literature, which studies different mechanisms of how trade is skill biased (e.g., Burstein and Vogel (2017) on technology-skill complementarity; (Parro, 2013) and (Burstein et al., 2013) on capital-skill complementarity; (Verhoogen, 2008) on quality upgrading in Mexico; (Bustos, 2011) on skill upgrading in Argentina). Our work complements this literature by quantifying the effects of different fundamental shocks on the skill premium through ====-sector reallocations, that is, structural change. We find that changes in sectoral TFP, rather than reductions in trade costs, are the main drivers of the increases in skill premium across countries through structural change.====Another strand of literature explores the determinants of structural change. The literature finds that nonhomothetic preferences, complementarity between sectors, and international trade are important drivers of structural change (Herrendorf, Rogerson, Valentinyi, 2014, Kongsamut, Rebelo, Xie, 2001, Ngai, Pissarides, 2007, Sposi, 2019, Świecki, 2017, Uy, Yi, Zhang, 2013). We build a tractable multi-sector multi-country quantitative framework incorporating all these channels of structural change, and quantify how different shocks affect the skill premium through structural change.====More closely related studies to ours are the work by (Buera et al., 2021) and (Cravino and Sotelo, 2019). Buera et al. (2021) build a two-sector ==== economy model and show that changes in sectoral TFP through structural change can explain around 30% of the observed increases in the skill premium in advanced economies. Cravino and Sotelo (2019) build a similar general equilibrium trade model as ours to show that reductions in trade costs increase the skill premium in all countries through structural change. Our study complements the above results by illustrating the quantitative importance of Hicks-neutral technical change on the skill premium in a global economy. Relative to (Buera et al., 2021), we show that Hicks-neutral technical change alters Ricardian comparative advantage across countries so affects the skill premium through international trade. This channel increases the skill premium in developed countries but decreases it in developing countries relative to autarky. We complement (Cravino and Sotelo, 2019) by illustrating how Hicks-neutral technical change generates an interaction between international trade and demand complementarity, as any changes in relative prices due to international trade will further affect sectoral reallocations through demand complementarity. This channel reinforces the initial trade effects of Hicks-neutral technical change on the skill premium through changes in comparative advantage across countries, and further increases (decreases) the skill premium in developed (developing) countries.====The rest of the paper is organized as follows. Section 2 presents the observed patterns of skill intensity and structural change. Section 3 builds a quantitative multi-sector multi-country framework with structural change. Section 4 discusses the strategy to retrieve fundamental one-time shocks and calibrate the model parameters. Section 5 uses the framework to conduct counterfactual analysis, which investigates the effects of different fundamental shocks on the skill premium, as well as how quantitatively important are different channels of structural change. Section 6 concludes.",Structural change and the skill premium in a global economy,https://www.sciencedirect.com/science/article/pii/S0165188922000690,21 March 2022,2022,Research Article,66.0
"Castex Gonzalo,(Stanley) Cho Sang-Wook,Dechter Evgenia","School of Economics, University of New South Wales, Australia","Received 28 September 2021, Revised 7 February 2022, Accepted 15 March 2022, Available online 17 March 2022, Version of Record 29 March 2022.",https://doi.org/10.1016/j.jedc.2022.104363,Cited by (1),"We revisit the capital-skill complementarity hypothesis and examine whether and under what conditions this mechanism can explain the developments in wage inequality and labor share in the 1963–2016 period. Krusell, Ohanian, Ríos-Rull and Violante (2000) show that a model with capital-skill complementarity mechanism matches the data well and can account for changes in wage inequality in the 1963–1992 period. We show that applying the model to the 1963–2016 period delivers a good fit for the skill premium; however, it does not match the declining pattern in labor share in the last two decades. We modify the model to allow for a flexible technology structure and show that the degree of capital-skill complementarity is declining over time. The augmented model with time-varying capital-skill complementarity suggests explanations for the declining labor share.","The idea of “capital-skill complementarity” was introduced by Griliches (1969), after producing evidence that capital and skilled labor are relatively more complementary than capital and unskilled labor. Krusell, Ohanian, Ríos-Rull and Violante (2000) (hereafter, KORV) develop a theoretical and empirical foundation for the capital-skill complementarity hypothesis. KORV propose an aggregate production technology general enough to accommodate a broad pattern of substitutability and complementarity within factors. KORV find that equipment-specific technological change can explain the patterns of skill premium and income inequality between 1963–1992; they show that a rise in the skill premium from the early 1980s was due to the increased investment in equipment capital, leading to an increase in the relative demand for skilled labor. The capital-skill complementarity mechanism has become a common feature of the neoclassical production technology with KORV’s estimated parameters widely adopted in a number of studies.====Since 1992, production processes, workplace environment and labor markets have all changed substantially. It is reasonable to consider that the degree of complementarity between capital and skill varies with market structure and with composition of technology and its maturity. Using data for 1963–2016 period, we show that the original KORV model underperforms in matching an important trend, the decline in labor share. In fact, using the estimates for the original 1963–1992 period, the model counterfactually predicts an increasing labor share. We extend the KORV model by allowing for a more flexible structure of technology and examine the changes in capital-skill complementarity mechanism over time. We do so by introducing time variation in all parameters; guided by estimation results, we narrow down the set of the relevant time-varying parameters.====First, we estimate the model for 1963–2016, considering the role of intellectual property products (IPP) in capital equipment. Using the simulated pseudo MLE methodology, we estimate the parameters by minimizing the distance between data and model generated series of labor income share and wage bill ratio, as well as the arbitrage between ex-ante rates of return to capital equipment and capital structures. Our estimation results using the 1963–1992 period closely replicate the original KORV results. For the 1963–2016 period, the model provides a good match for the wage bill ratio and no-arbitrage condition but it is not successful in matching the declining pattern in labor share after 2000; nevertheless, the model provides a good fit for the skill premium. Comparing the estimates for 1963–1992 and 1963–2016 periods, we record an increase in the elasticity of substitution between unskilled labor and capital equipment, from 1.73 to 1.88; and an increase in the elasticity of substitution between skilled labor and capital equipment, from 0.72 to 0.81.====Second, we analyze the dynamics of the capital-skill complementarity mechanism. To do so, we repeat the estimation for each of the 30-year period rolling-windows within the 1963–2016 period, such that the first rolling window captures exactly the same time period as in KORV. The elasticity of substitution between capital equipment and skilled labor exhibits an increasing trend when more recent years are included in the sample. The remaining parameters of the model, including the elasticity of substitution between unskilled labor and capital equipment, are fairly constant across the rolling windows, especially for periods that do not include the last five years of the sample.====Third, guided by these findings, we re-estimate the model for the 1963–2016 period, allowing for the parameter governing the elasticity between equipment capital and skilled labor to be time dependent. We estimate a substantial decline in capital-skill complementarity. Incorporating the time-varying complementarity channel, the “augmented” KORV model provides a good match for the data moments and produces a declining labor share, something the original KORV model does not generate. The overall pattern of skill premium produced by the model is consistent with the data; however, in the more recent years the model generates a decline in skill premium, while it is fairly flat in the data.====Further, a decomposition exercise confirms that the capital-skill complementarity effect is still the dominant force in driving the skill premium patterns in the last half-century. Our growth accounting decomposition for the augmented KORV model shows that more than half of the variation in the skill premium can be attributed to the capital-skill complementarity channel with capital deepening effect driving the increased demand for skilled workers. In comparison to the benchmark KORV model, the contribution of the capital-skill complementarity channel is reduced and mostly offset by the diminishing degree of complementarity between capital and skilled labor.====Skill premium has grown substantially between 1963–2016; after a decline in the 1970s, the skill premium rose rapidly in the 1980s, followed by a slowdown in the 1990s, it was essentially unchanged after 2005. The original KORV study argues that capital-skill complementarity and changes in factor inputs can explain most of the variation in the skill premium between 1963–1992. Katz and Murphy (1992) and Goldin and Katz (2008) show that before the 2000s, the changes can be attributed to technological advances that benefited the skilled workers and to the relatively slow growth of skilled labor. As noted by Autor et al. (2020b), the developments after 2000 and in particular the flattening of the skill premium are not explained by the Katz and Murphy (1992) and Goldin and Katz (2008) model; they suggest that to explain the variation in skill premium after 2000, the model needs to be adjusted.====Without directly targeting the skill premium, the augmented model generates patterns that are generally consistent with the data. However, from 2005, the model predicts a decline in skill premium and not flattening. On the other hand, the benchmark model cannot match the labor share but produces a closer fit for the skill premium after 2005. These observations suggest that the augmented model overpredicts the slowdown in skill premium. The rolling windows estimates show that the most significant variation is observed for the elasticity between capital equipment and skilled labor; the augmented model adjusts the KORV specification accounting for this observation. This modification does not take into account changes in rolling windows estimates occurring in the later years of the sample. Notably, the elasticity between capital equipment and unskilled labor is also increasing in the last 5 years of the sample and can potentially explain the understated skill premium. Rolling windows observations point to structural changes in the US labor market in the recent years and call for further adjustments to the KORV model.====We estimate a decline in capital-skill complementarity, which implies that in the more recent decades, a decrease in the price of capital equipment leads to a smaller increase in the demand for skilled labor. This finding is consistent with the theoretical implications derived by Greenwood and Yorukoglu (1997) and Beaudry, Green and Sand (2016). Greenwood and Yorukoglu (1997) argue that a technological advancement leads to a transitory increase in the demand for skilled labor; when technology matures, the demand for skill returns to its original steady state level. Beaudry et al. (2016) show that in the early 2000s, due to technology maturity and its widespread adoption, the demand for cognitive tasks with high educational skill underwent a reversal. Valletta (2016) adopts the Beaudry et al. (2016) mechanism to offer a potential explanation to the flattening skill premium. Balleer and Van Rens (2013) argue that starting in the mid-1990s, technological improvements in capital substituted skilled workers more than unskilled workers.====We find that the KORV model with the declining capital-skill complementarity can produce a close match for the declining labor share after 2000s. The decline in labor share is widely documented, starting with Elsby et al. (2013) and Karabarbounis and Neiman (2014), followed by many others who propose a range of explanations for this phenomenon. Many theories explain the decline in labor share emphasizing the replacement of local workers with other inputs of production. Within this subset of studies, our findings relate to those by Karabarbounis and Neiman (2014), who attribute a large portion of the decline in the labor share to the decline in the relative price of capital. We show that the aggregate elasticity of substitution between capital and labor is increasing when capital-skill complementarity is declining. Our findings also relate to those reported in Koh et al. (2020), who argue that the capitalization of intellectual property products (IPP) can explain most of the decline in labor share. We explore the role of IPP in the KORV model and find that some of the decline in capital-skill complementarity can be explained by the increasing share of IPP capital. The emerging literature on concentration in product markets provides another explanation for the decline in labor share. Kehrig and Vincent (2021) and Autor et al. (2020a) study the reallocation of activity across heterogeneous firms toward those with low and declining labor shares. They argue that the drop in labor share is due to the growing power of low labor share superstar firms. Further research needed to examine the differences in capital-skill complementarity across firms.====A number of recent papers study the relationship between replacement of routine occupations by machines and the decline in the labor share, see for example Eden and Gaggl (2018) and Orak (2017). However, occupational shift is less consistent with the decline in capital-skill complementarity since skilled (college-educated) workers are less likely to occupy routine jobs (manual and cognitive), see for example Acemoglu and Autor (2011). A similar argument is made by vom Lehn (2018), who shows that the decline in the labor share in the 2000s coincides with a decline in wages paid to abstract tasks. vom Lehn (2020) incorporates the Beaudry, Green and Sand (2016) mechanism and shows that it can explain labor market dynamics, in particular the decline in labor share. Aum (2017) focuses on the impacts of software innovation and argues it could lead to a lower demand for cognitive-intensive occupations and reduce the labor income share.====There are two recent studies that focus on re-estimating the KORV substitution elasticities in extended samples: Maliar, Maliar and Tsener (2020) and Ohanian, Orak and Shen (2021). Maliar, Maliar and Tsener (2020) estimate the KORV model for the extended period and propose a methodology to project developments in skill premium in the next twenty years. Ohanian, Orak and Shen (2021) estimate the KORV model with an additional focus on input variables, using alternative measures for aggregate income. Both studies agree that the estimated original KORV model for the extended time period performs well in explaining the changes in skill premium over time; however, neither can produce the declining pattern in labor share. Our study extends the KORV framework to allow for a more flexible technology structure and explores different mechanisms that can help us understand the concurrent decline in capital-skill complementarity and labor share, and the flattening skill premium. Further, our decomposition analysis outlines the roles of different channels in explaining the developments in skill premium.====The rest of the paper is organized as follows. Section 2 describes the model and the analytical derivation of skill premium. Section 3 details the estimation strategy, followed by the description of data in Section 4. Section 5 presents our main findings and discussion followed by a decomposition exercise in Section 6. Section 7 concludes the paper.",The decline in capital-skill complementarity,https://www.sciencedirect.com/science/article/pii/S0165188922000689,17 March 2022,2022,Research Article,67.0
Ogawa Toshiaki,"Musashi University, Japan","Received 13 November 2021, Revised 28 February 2022, Accepted 13 March 2022, Available online 15 March 2022, Version of Record 8 April 2022.",https://doi.org/10.1016/j.jedc.2022.104360,Cited by (0),"I study capital requirements and their welfare implications in a dynamic general equilibrium model of ====. The model is characterized by two features. First, banks choose entry and exit, which allows the number of banks change endogenously. Second, since equity issuance is costly, banks precautionarily hold capital buffers against future liquidity shocks. I find that the optimal capital requirement that maximizes social welfare is larger than 3.5%, and the value sensitively depends on the size of utility households derive from deposit holdings. In a special case without any utility from deposit holdings, I obtain welfare implications that are consistent with Corbae and D’Erasmo (2021a).","The rationale for bank capital requirements is to mitigate moral hazard problems caused by banks and depositors due to deposit insurance and limited liability. Because of deposit insurance, depositors do not care about whether their banks default or not. In this case, banks can always raise deposits at risk-free rates, regardless of the risks they are taking. Furthermore, a combination of deposit insurance and limited liability keeps banks from internalizing the wider economic costs generated by their defaults. Increasing bank capital requirements would reduce the frequency of bank defaults by reducing their leverage, or increasing “skin-in-the-game,” and would limit those moral hazard problems.==== A reduction of bank defaults means a reduction of social losses. Increasing capital requirements, on the other hand, encourages banks to issue equity rather than collect deposits in financing their investments. This would be negative for social welfare because equity capital cannot provide households with liquidity services; only deposits can. Another reason why issuing equity may have negative implications for social welfare is that there are problems caused by informational asymmetries between equity issuers and investors.==== Consequently, capital requirements can be both positive and negative for social welfare.====In this paper, I develop a dynamic general equilibrium model of banking in order to study capital requirements and their implications for social welfare. In the model, infinitely-lived heterogeneous banks intermediate funds from households to entrepreneurs, who can hold productive capital in the economy. Banks are allowed to accumulate their own capital by both retaining past earnings and issuing equity. Banks’ only assets are loans, which are financed by deposits and equity capital. In considering the welfare implications of capital requirements, I combine two, less commonly considered but important, mechanisms as follows.====The first extension is to determine the number of banks endogenously through entry and exit decisions. This extension allows me to explore the impact of bank capital requirements on the size and competitiveness of the banking industry. One current concern for U.S. policy makers is that, as shown in Fig. 1, the rate of new bank formation declined dramatically after the financial crisis of 2008 and this has accelerated a decline in the total number of commercial banks since then. This decline could be attributable to stricter bank regulations imposed in response to the crisis.==== Potential negative impacts of increasing capital requirements on the number of banks would merit consideration by bank regulators. How new regulations on banks can affect their competitiveness and market structure is also important from a policy perspective.==== Endogenizing the number of banks makes it possible to deduce some implications of capital requirements for such concerns of policy makers. To be concrete, in my model, banks compete with each other in the loan market. Specifically, if the expected value of banking business is greater than the entry cost, new banks enter the market and compete with incumbent banks to the point where expected value falls to the entry cost à la Hopenhayn (1992). The harsher the capital requirement is, the more banks need to hold capital and resort to costly equity issuance. Therefore, their profitability, or franchise value, declines, and fewer banks enter the industry. As a result of this, the number of new entrants decreases in the industry, thereby (i) reducing the total number of banks and (ii) increasing the concentration of the industry. Taking the effects through the extensive margin into account in this way contributes to a dramatic reduction in banks’ liquidity supply to the economy.====The second extension in my model is to make banks’ equity finance costly and allow their deposit collection to be exposed to shocks. In this setting, increasing precautionary capital buffers in the present by retaining profits means a reduction in costly equity issuance in the future. This encourages infinitely-lived banks to hold more capital now than required by their regulators. In short, bank capital requirements are only occasionally binding.==== This implication is confirmed by the data. Fig. 2 shows the distribution of Tier1 capital to risk-weighted assets (RWA) ratios for U.S. commercial banks from the fourth quarter of 2010 to the third quarter of 2017.==== Following the Basel Accord, U.S. authorities categorize banks whose capital ratios are (====) as “well capitalized”. One can see that banks hold more capital than required. In addition, the size of this buffer differs by bank. Most of the previous theoretical literature assumes bank capital requirements to be always binding. Due to this assumption, increasing the requirement by a specific amount increases banks’ capital holdings by the same amount, thereby improving financial stability through an increase of “skin-in-the-game”. However, the marginal impact of elevating capital requirements on banks’ capital holdings has not been illuminated fully for the case where the requirements are not binding. My model attempts to achieve this.====My model has the following implications. The first implication is that the negative impact of bank capital requirements on social welfare sensitively depends on the size of the utility that households derive from deposit holdings. Due to the endogenously determined number of banks, increasing capital requirements contributes to reducing the number of banks and deposits supplied to the economy significantly through the extensive margin, despite the fact that deposits provide important liquidity services to households.====The second implication is that, although bank capital requirements are only occasionally binding, banks actually increase their capital holdings when the requirements are strengthened because their precautionary motives remain. This benefits financial stability by reducing bank defaults.====The third implication is that the optimal capital requirement that maximizes social welfare is higher than 3.5%, and the value critically depends on the size of utility households derive from deposit holdings. When households derive no utility from deposit holdings, social welfare monotonically increases with capital requirements, which is consistent with Admati et al. (2013) and Admati and Hellwig (2014). In this case, changing capital requirements from 4% to 8.5% yields 0.16% consumption equivalent welfare gains, which is in line with Corbae and D’Erasmo (2021a) (0.1% consumption equivalent welfare gains).====Lastly, adding aggregate shocks could impact the welfare implications when the weight of the utility that deposit holding provides to households is in the intermediate region. In this case, the two dominant forces that determine the welfare implications of capital requirements (i.e., the utility that households derive from deposit liquidity services and deadweight losses from banks’ defaults) are balanced, and the change in social welfare associated with changing the capital requirements is relatively small. In other cases (when the weight of the utility that deposit holding provides to households is small or large), one of these two forces is dominant in determining welfare implications and adding aggregate shocks is unlikely to change the implications.====I also investigate how these welfare implications change in response to changes in relevant model parameters. In line with Admati et al. (2013) and Admati and Hellwig (2014), I show that reducing the value of the parameter that determines the equity issuance cost for banks pushes up the optimal capital requirement by mitigating the reduction of deposits supplied by banks to the economy through the extensive margin. This is because reducing equity issuance cost keeps bank’s franchise value from decreasing, and encourages new banks to enter the industry, even when the requirements are harsher. One policy implication is that to maintain financial stability without damaging banks’ liquidity provision, the strengthening of capital requirements needs to be accompanied by a reduction in the cost of equity issuance for banks. Moreover, I show that increasing the value of the parameter that controls the liquidation cost of defaulted banks pushes up the optimal capital requirement. The reason is that the parameter represents the size of the negative externality that banks do not internalize due to limited liability. The larger the value is, the more beneficial are capital requirements for social welfare. Even though my model does not explicitly include an element of systemic risk (i.e., a large negative externality caused by bank defaults), increasing the parameter can be interpreted as taking into account some of the effects that systemic risk has on social welfare.====The remainder of the paper is organized as follows. Section 2 reviews the literature. Sections 3 introduces the model. Section 4 defines equilibrium. Section 5 presents a calibration of the model for the U.S. economy. Section 6 presents the quantitative analysis and checks its robustness. Section 7 extends my baseline model in two directions by changing the size of the liquidity services deposits provide and adding unexpected aggregate shocks. Section 8 discusses how differences in model settings from previous studies could affect welfare implications. Section 9 concludes.",Welfare implications of bank capital requirements under dynamic default decisions,https://www.sciencedirect.com/science/article/pii/S0165188922000653,15 March 2022,2022,Research Article,68.0
"Dessertaine Théo,Moran José,Benzaquen Michael,Bouchaud Jean-Philippe","LadHyX UMR CNRS 7646, Ecole polytechnique, Palaiseau Cedex 91128, France,Chair of Econophysics & Complex Systems, Ecole polytechnique, Palaiseau Cedex 91128, France,Mathematical Institute and Institute for New Economic Thinking at the Oxford Martin School, University of Oxford, Oxford, United Kingdom,Complexity Science Hub Vienna, Josefstädter Straße 39, A-1080, Austria,Capital Fund Management, 23 Rue de l’Université, Paris 75007, France","Received 3 February 2021, Revised 15 February 2022, Accepted 13 March 2022, Available online 15 March 2022, Version of Record 28 March 2022.",https://doi.org/10.1016/j.jedc.2022.104362,Cited by (5),"We study the conditions under which input-output networks can dynamically attain a competitive equilibrium, where markets clear and profits are zero. We endow a classical firm network model with minimal dynamical rules that reduce supply/demand imbalances and excess profits. We show that the time needed to reach equilibrium diverges to infinity as the system approaches an instability point beyond which the Hawkins-Simons condition is violated and competitive equilibrium is no longer admissible. We argue that such slow dynamics is a source of excess volatility, through accumulation and amplification of exogenous shocks. Factoring in essential physical constraints absent in our minimal model, such as causality or inventory management, we then propose a dynamically consistent model that displays a rich variety of phenomena. Competitive equilibrium can only be reached after some time and within some restricted region of parameter space, outside of which one observes spontaneous periodic and chaotic dynamics, reminiscent of real business cycles. This suggests an alternative explanation of excess volatility in terms of purely endogenous fluctuations. ==== to scale and increased perishability of goods are found to ease convergence towards equilibrium.",None,Out-of-equilibrium dynamics and excess volatility in firm networks,https://www.sciencedirect.com/science/article/pii/S0165188922000677,15 March 2022,2022,Research Article,69.0
Basso Henrique S.,"Banco de España, Spain","Received 12 January 2021, Revised 9 March 2022, Accepted 13 March 2022, Available online 14 March 2022, Version of Record 7 April 2022.",https://doi.org/10.1016/j.jedc.2022.104361,Cited by (0),"Imperfect information aggregation in secondary markets of credit has significant consequences for economic cycles. As banks put more weight on mark-to-market gains, they find it optimal to refrain from revealing information about adverse shocks. Consequently, default risk is mispriced, and loan volumes, and thus investment, are not appropriately reduced. Overinvesment lowers the price of capital, leading households to increase consumption without decreasing labour supply, generating a boom. Due to mispricing, banks subsequently face bigger losses and capital depletion. Output then decreases sharply due to credit supply shortages. These instances of market dysfunction are crucial in amplifying credit cycles.","Since the Great Recession, credit cycles have been at the forefront of the debate in policymaking and academia. Several features have been identified as important drivers behind the fluctuations observed in credit and consequently in economic activity. The most prominent involve problems of asymmetric information and adverse selection and the presence of non-linearities due to credit constraints. This work presents a novel mechanism generating amplified economic and credit cycles based on imperfect information aggregation in credit markets and mispricing.====Asset holdings of financial intermediaries have grown considerably since the early 1990s (Adrian and Shin, 2010). The share of assets allocated to the trading book, which is mark-to-market, has also increased substantially (Study on Mark-To-Market Accounting - SEC-US, 2008). Finally, bankers’ compensation has been heavily skewed towards short-term payoff (Bolton, Mehran, Shapiro, 2015, Fahlenbrach, Stulz, 2011). We incorporate these features, augmenting the banking sector in a macroeconomic model of credit frictions with risk shocks. We show that the combination of heterogenous information across bankers, secondary markets of credit, and short-term bias in payoffs provide incentives for incomplete information aggregation in credit markets, in which case credit assets become mispriced. Furthermore, we find that these instances of market dysfunction and mispricing generate initially a boom, and subsequently a prolonged recession, increasing macroeconomic volatility and amplifying credit cycles. Mispricing may therefore contribute in shaping financial cycles (Borio, 2014).====We build on the standard macroeconomic model of credit frictions with risk shocks (Christiano et al., 2014). Entrepreneurs must borrow from banks to fund investment projects. Loan contracts are a function of the degree of riskiness of entrepreneurs’ projects or the dispersion of the distribution of entrepreneurs returns, which is the only aggregate exogenous stochastic variable in the model. The key novelty of our framework is the introduction of a more realistic banking sector in which (====) bankers initiate every period with a set of loans in the balance sheet (bank asset holdings) and put a greater weight on current mark-to-market gains relative to future profits, (====) bankers differ regarding their information on the expected degree of riskiness of entrepreneur projects, as only a random subset of bankers get a signal on riskiness (bankers who receive a signal are informed and the ones who do not, are uninformed), and (====) bankers interact in a secondary market of credit through signalling games where by determining the new valuation of loans, an economy wide posterior view on the degree of riskiness emerges.====The key decision for an informed banker is whether to reveal its signal to uninformed bankers or avoid doing so. Informed bankers are all identical and set the same strategy in a series of signalling games between each informed banker and the collective of uninformed bankers, which then determines the equilibrium in secondary markets. On the one hand, if informed bankers fail to reveal adverse signals to uninformed ones by refraining from selling off credit assets, the equilibrium in credit markets is such that the mark-to-market value of assets in the balance sheet are preserved. However, by doing so informed bankers forgo gains from trading while exploiting informational advantages and as information does not become public, the valuation of new credit instruments does not appropriately reflect the risks undertaken - credit markets malfunction. As a result, the banking sector fails to set credit spreads that match the expected default rates, potentially increasing future losses. On the other hand, attempting to go short in the secondary market and revealing the signal leads to lower mark-to-market valuation of asset holdings. Nonetheless, informed bankers make trading profits and information is fully incorporated into loan rates. The banking sector sets credit spreads on new loans appropriately, avoiding future losses. Therefore, informed bankers effectively face a trade-off between the current mark-to-market valuation of asset holdings and their future profits from trading and newly issued loans.====The bigger the size of banks’ balance sheets and the greater the short-term bias in the banker’s payoff, the more likely it is that, after an adverse signal, informed bankers favour mark-to-market gains on current asset holdings to the detriment of future profits. Thus, in a series of signalling games informed bankers avoid revealing the signal and the equilibrium in secondary markets only partially reflects new information. As the adverse shock is effectively overlooked, markets remain bullish on entrepreneurs projects, failing to adjust funding conditions. Credit spreads are set relatively low, and total loans/investment relatively high based on the underlying risk, benefiting entrepreneurs. As a result of this overinvesment, the price of capital falls, decreasing the funds needed for households to save in physical capital. In turn, this boosts consumption without depressing labour supply, and ultimately, production increases in the current period. Subsequently, banks face bigger losses resulting in a significant decrease in banking capital, compromising their ability to fund new investment going forward. Output thereafter decreases sharply due to credit supply shortages. This boom and bust characterisation matches closely to what we observe during banking crises. Although defaults occur after an unanticipated averse shock, without mispricing they are unable to generate volatile macroeconomic outcomes. Banks are more protected and credit market stability is guaranteed. Hence, the added mechanism creating credit market dysfunctions incorporated here, relative to standard models of credit frictions (e.g. Bernanke, Gertler, Gilchrist, 1999, Christiano, Motto, Rostagno, 2014), is crucial in amplifying credit cycles.====The main element that drives economic fluctuations after imperfect or partial information revelation is the mispricing of risk. Contrary to Akerlof and Shiller (2009), who focus on ‘animal spirits’ (or behaviour biases), mispricing in our setting results from instances where information is not fully reflected into prices as bankers react to their payoff incentives.==== Do we observe instances in which market prices do not fully reflect all available information? A cross market comparison of prices shows that agents may fail to require the correct compensation for the risk undertaken. Coval et al. (2009) show that the returns on credit default swaps on indexes and put options on these indexes, both of which reflect similar risk profiles, were significantly different. Comparatively, the differences of the lead bank’s internal valuation of syndicated loans and the price paid by investors reported by Ivashina and Sun (2011) suggest that not all information on the quality of borrowers reaches the auction for these loans. The results presented in these contributions indicate that prices of instruments used in the funding of investment (through securitization or syndication) may not internalize all available information.====Employing a macro general equilibrium model allows us to associate instances of market dysfunction with the main features of the economy. We observe that bankers are more likely to refrain from going short in credit markets and revealing adverse shocks when the volume of trading relative to the size of balance sheet is small and when banking profits are more procyclical. In both of these cases short-term mark-to-market valuation gains are boosted relative to future period losses and gains from trade. We find the opposite result for procyclical leverage ratios. Although under partial revelation the valuation of gains from primary market activity are boosted by procyclical leverage, actual mark-to-market valuations, due to the interplay between future leverage and the current price of capital, are not. Short-term gains are thus restricted and consequently, more procyclical leverage ratios increase the incentive to reveal information. Nonetheless, we find that in economies with higher average/steady state levels of leverage episodes of imperfect information diffusion are more likely to occur, as balance sheets are bigger.====Since the Great Recession several papers build macroeconomic models with a financial sector to analyse financial crises. The main papers in the literature exploit how asymmetric information and/or moral hazard problems may generate excessive and correlated risk taking (Farhi and Tirole, 2012), liquidity problems (Allen et al., 2009), market freezes (Boissay et al., 2016), collateral crises (Gorton and Ordonez, 2014), or endogenous risk due to non-linearities (Brunnermeier and Sannikov, 2014). Our framework differs inasmuch as we attempt to motivate the crisis based on mispricing of risk due to imperfect information revelation instead of negative shocks to banks’ net worth or other market externalities, which can be complementary to the mechanism proposed here. Mispricing in our framework generates credit supply restrictions due to banking capital depletion. Financial crisis driven by credit supply restrictions related to balance sheet and liquidity issues have been confirmed empirically by Ivashina and Scharfstein (2010) and Cornett et al. (2011a). We provide a theoretical framework that generates these restrictions endogenously through the mispricing of credit assets. Finally, credit induced boom and busts are also analysed empirically by Di Maggio and Kermani (2017). Although they focus on changes in regulation as the driving force behind credit fluctuations, they confirm the role of positive credit supply shocks to riskier borrowers in generating periods of greater economic activity but subsequently also leading to higher loan delinquencies.====The key mechanism generating mispricing is closely related to the contributions on price information revelation in markets following Grossman and Stiglitz (1980). The closest contribution to ours, within this set of models, is Dasgupta and Prat (2008), who show that career concerns generate information inefficiency in markets. We alter the framework to focus on a payoff structure biased towards mark-to-market valuation of current asset holdings in a secondary market to highlight how partial information revelation occurs.==== As such our work also relates to the CEO/traders compensation literature. In our framework payoff structures skewed towards short-term payments are found to generate information aggregation problems and mispricing of risk, while in that literature they are found to generate excessive risk taking (see Bolton et al., 2015 for executive compensation and risk taking and Kleinlercher et al., 2014 for laboratory evidence on the importance of trader bonuses for asset pricing and risk taking).====The remainder of the paper is structured as follows. Section 2 focuses on the model structure, presenting the features of our macroeconomic model and the equilibrium conditions in the secondary market of credit. Section 3 discusses the key aspects of our mechanism, looking at the bankers trade-off behind the imperfect information revelation and mispricing of credit. The calibration and computation method are discussed in Section 4. The main results on the feedbacks between imperfect information aggregation in markets and economic activity are presented in Section 5. Section 6 concludes.","Asset holdings, information aggregation in secondary markets and credit cycles",https://www.sciencedirect.com/science/article/pii/S0165188922000665,14 March 2022,2022,Research Article,70.0
"Karamysheva Madina,Skrobotov Anton","NRU Higher School of Economics, Moscow, 119100, Pokrovsky Boulevard 11, office number S-639, Russian Federation,Russian Presidential Academy of National Economy and Public Administration, Russian Federation,St Petersburg University (Center for Econometrics and Business Analytics), Russian Federation","Received 12 January 2021, Revised 4 March 2022, Accepted 9 March 2022, Available online 12 March 2022, Version of Record 31 March 2022.",https://doi.org/10.1016/j.jedc.2022.104358,Cited by (3),"This paper is devoted to fiscal shock identification based on the assumption of non-Gaussianity of the errors, which can be easily tested. We use additional co-kurtosis conditions in GMM estimation of the AB-model to estimate the dynamic effects of fiscal shocks and find fiscal multipliers in the U.S. economy. Our approach results in higher ==== multipliers on average relative to Blanchard and Perotti (2002) and Leeper et al. (2013). Testing the restrictions, we are not able to reject them in the ==== model. Once we control for fiscal foresight, we can reject restrictions both individually and altogether. Finally, comparing ","For several decades both researchers in academia and policymakers are struggling in estimating the fiscal multipliers. There is no consensus in the literature on the size of the multipliers and sometimes even on the sign of the multipliers. Some studies focus on tax multipliers, others on expenditure multipliers. The range of the estimates varies a lot as can be seen from the surveys (see, for example, Ramey (2016), Favero and Karamysheva, 2017, Castelnuovo and Lim (2019)).==== A central question in evaluating fiscal policy is related to the identification of fiscal policy shocks. Often identification is based on competing for economic theories, while sometimes additional ==== restrictions are required. Unfortunately, there might be no over-identifying information that can help test both theories and those ==== restrictions.====In this paper, we propose to use the statistical identification method based on non-Gaussian errors to identify fiscal shocks. We adopt the method proposed by Lanne and Luoto (2021), extend it to AB-model, and apply it to the U.S. economy to estimate the dynamic effects of fiscal shocks and find fiscal multipliers. Statistical-based identification has several advantages relative to the standard economic-based identification techniques, based on setting restrictions. First, our approach is restriction-free; second, it allows us to test the restrictions existing in the literature; third, it does not require any assumptions on the structural model form except non-Gaussianity in errors, which can be easily tested.====We apply statistical identification based on non-Gaussian errors to very parsimonious models from the literature. We focus on Blanchard and Perotti (2002) and Leeper et al. (2013) for the sake of clarity and ease of exposition. We find that the output effects of exogenous tax increases are negative and that the maximum multiplier is 1.9 in the three-variables BP baseline specification. While the output effects of exogenous spending increase are positive with the maximum multiplier of about 1.1. Once we control for the fiscal foresight using implicit tax rate our maximum tax multiplier falls below unity.====Caldara and Kamps (2008) show that the variety of the results in the literature can be explained by the different identification approaches. Caldara and Kamps (2017) find that the difference might be due to the various assumptions on tax and expenditure elasticities. Our approach has both statistical and economic contributions to the literature. First, it allows us to estimate the elasticities and avoid timing restrictions. Moreover, it allows testing restrictions assumed by the literature. In Blanchard and Perotti (2002) specification, we are not able to reject their restrictions on the individual level, but we do reject them jointly. In the Leeper et al. (2013) model we are able to reject Blanchard and Perotti (2002) restrictions both on an individual level and all together. Finally, we compare the elasticity of revenue to output to other elasticities found in the literature, and we are not able to reject the one of Caldara and Kamps (2017). So as a main economic contribution, we found that once we use the statistical-based identification we confirm Blanchard and Perotti (2002) results, but we reject Leeper et al. (2013) results. Put it differently, with our statistical-based identification technique, augmenting the model with fiscal foresight variable, does not change Blanchard and Perotti (2002) results.====Putting ourselves into the literature perspective, first, we contribute to the papers focusing on estimating dynamic fiscal multipliers using different identification techniques. One of the most popular identification techniques is based on recursive and short-run restrictions (see Blanchard and Perotti (2002)). Another way of setting restrictions is long-run restriction (see, for example, Blanchard and Quah (1988)), although the set of problems arise with this method as well (for example, additional assumption about unit root, or behaviour of restrictions in small samples, which is addressed in Francis et al. (2014)). An alternative method used in the literature is the sign restrictions identification based on restricting the signs of the responses of other variables (excluding the variable of interest) in order for shocks to have meaningful signs (see Mountford and Uhlig (2009)). However, recent contribution by Arias et al. (2018) suggested that the standard technique of sign restriction has problems. An alternative identification approach is a narrative method based on construction of series of shocks based on historical and legislative documents (budget, presidential speeches, CBO reports, etc.) to identify the motivation of the action and respective quantity.==== Alesina et al. (2015), Alesina et al. (2019) suggest to use fiscal plans instead of shocks, as plans take into account both intertemporal and intratemporal correlation of the plans components. Mertens and Ravn (2014a), Stock and Watson (2018) recommended to use external proxy variables that are correlated with the structural shocks (relevance) but uncorrelated with other structural shocks (exogeneity) in Proxy SVAR (IV-SVAR) models. Caldara and Kamps (2017) suggest identification scheme based on Proxy SVAR with non-fiscal instruments. A potential identification problem of policy foresight as highlighted by Leeper et al. (2012) and Leeper et al. (2013) might cause a non-fundamental MA representation.====All methods considered above are based on so-called economic identification, which means that some economic motivation is needed to identify all parameters of interest. The alternative type of identification is the so-called statistical identification. The statistical identification does not imply any restrictions on the parameters of the model but assumes only additional violations of the standard assumptions of the innovation errors (e.g., heteroskedasticity, non-normality). These violations are natural enough and can be easily tested. Moreover, these violations deliver additional information and allow us to identify all parameters of interest in the SVAR model.====The most popular type of statistical identification is the identification through heteroskedasticity. This identification assumes structural breaks in the variance (see Lanne and Lütkepohl (2008); Lewis (2021); Rigobon (2003)), breaks in distribution of the errors (see Lanne et al. (2010)), conditionally heteroskedastic errors (see Luetkepohl and Milunovich (2016)), Markov switching errors (see Lanne et al. (2010), Herwartz and Lütkepohl (2014)), smooth transition in variances (see Lütkepohl and Netšunajev (2017b)). Lütkepohl and Netšunajev (2017a) review and compare different volatility models, and Lütkepohl and Schlaak (2018) propose to use information criteria for choosing across different volatility models. There are a few potential drawbacks of identification through the heteroskedasticity approach. First, estimation is challenging for large models or a large number of volatility states. Moreover, it is hard to choose the proper model in the absence of prior knowledge on the structure of volatility changes.====An alternative type of statistical identification approach is based on non-Gaussianity of errors. Within this type, one might distinguish mainly to sub-streams of the literature. The first one, so-called independent component analysis literature (ICA hereafter) assumes that the structural shocks are stochastically independent, and not just uncorrelated. Moreover, this approach does not allow for heteroskedasticity. See, for example, Hyvärinen et al. (2010), Moneta et al. (2013), Herwartz (2015), Lanne et al. (2017), Gouriéroux et al. (2017). Kilian and Lütkepohl (2017) emphasize as the main drawback of this approach the independence assumption. This assumption may be problematic, because independent shocks, in general, cannot be obtained by a linear transformation of non-Gaussian reduced-form residuals. The second approach was proposed by Lanne and Luoto (2021). Differently from ICA literature, Lanne and Luoto (2021) assume shocks to be only mutually uncorrelated and not necessarily independent. Additionally, a number of co-kurtosis restrictions are used. This approach allows for various forms of conditional heteroskedasticity because the assumption of independence common in ICA literature is relaxed. Moreover, thanks to the relaxing of independence assumption, there is no need for the explicit specification of error distribution, which makes the approach to be robust to misspecification of the error distribution. Lanne and Luoto (2021) used only B-model type.==== Their method is based on the generalized method of moments (GMM) with additional informative symmetric and asymmetric co-kurtosis conditions for GMM estimation. So second and third contributions of our paper are an extension of the B-model proposed by Lanne and Luoto (2021) to the AB-model case and an application of this identification strategy to fiscal policy.====To the best of our knowledge, only two papers use identification through heteroskedasticity for investigating the dynamic effects of fiscal policy shocks. Fritsche et al. (2020) exploit time-variation in the volatility of U.S. time series via Markov switching in heteroskedasticity approach (MS-SVAR) following Herwartz and Lütkepohl (2014). Fritsche et al. (2020) estimate dynamic effects of government spending shocks. Meanwhile, Lewis (2021) developed a new identification technique based on heteroskedasticity that does not rely on a particular parametric model. This new identification technique is based on the autocovariance structure of second moments of the residuals implied by an arbitrary time-varying volatility of any form.====The main difference between the approach taken in our paper and Fritsche et al. (2020), Lewis (2021), is that we do not make any assumptions about heteroskedasticity of the errors, although we need an assumption of non-normality of the errors. In practice, both assumptions might be valid. If so, both methods should work. On the other hand, in the presence of non-Gaussianity in errors and the absence of heteroskedasticity, our approach should work while the other should not. So we see our paper as a complement to other papers that uses different statistical-based identification techniques.====During the revision process, we learn about one more paper that uses statistical-based identification with application to fiscal policy. Moneta and Pallante (2020) analyze different ICA estimators proposed for SVAR identification. One of the methods they use is the pseudo maximum likelihood (PML hereafter) derived by Gouriéroux et al. (2017). Interestingly, Lanne and Luoto (2021) simulation results suggest that the performance of the GMM estimator is comparable with PML, and outperform the recursive PML estimator.==== Both our paper and Moneta and Pallante (2020) paper start with the assumption of non-Gaussian errors. However, differently from Moneta and Pallante (2020), we do not require the structural shocks to be stochastically independent. Moreover, our approach allows for conditional heteroskedasticity.====The paper is organized as follows. Section 2 describes the statistical model, economic model, moment conditions, which are used for GMM-based identification, and also discusses local identification. The data and the estimated model are placed in Section 3. In Section 3, we also obtain impulse response functions and corresponding fiscal multipliers with their comparison to other papers. Section 4 concludes.",Do we reject restrictions identifying fiscal shocks? identification based on non-Gaussian innovations,https://www.sciencedirect.com/science/article/pii/S016518892200063X,12 March 2022,2022,Research Article,71.0
"Curtis Chadwick,Garín Julio,Lester Robert","Department of Economics, Robins School of Business, University of Richmond, 1 UR Dr., Richmond, VA 23173, USA,Claremont McKenna College, USA,Colby College, USA","Received 17 September 2021, Revised 7 March 2022, Accepted 9 March 2022, Available online 11 March 2022, Version of Record 24 March 2022.",https://doi.org/10.1016/j.jedc.2022.104357,Cited by (1),We document how lifetime utility varies by demographic groups in the US and how these differences have evolved since the start of the 21st century. Using the equivalent variation as our measure of welfare we find that the ,"The facts of cross-sectional inequality across a number of dimensions are well established. Consumption, leisure, and life expectancy vary substantially between and within narrowly defined demographic groups. In this paper we aim to answer the following question: Accounting for these well-developed metrics of well-being, how much does the welfare of individuals belonging to various racial, educational, and gender groups compare to the welfare of the average American?====When looking at the data, it is not always clear how the measurable inputs to well-being translate to overall welfare. For instance, those groups with high consumption generally have lower leisure. Moreover, despite being higher within genders, the overall correlation between life expectancy and consumption is less than 0.5. Thus, inferring economic well-being from the underlying inputs requires a way of ranking competing bundles of consumption, leisure, and life expectancy, i.e. a utility function. It is the combination of data with a lifetime utility function that allows us to make welfare comparisons across groups and over time.====Following Jones and Klenow (2016) (JK henceforth), we use the equivalent variation – the amount of consumption you must compensate or tax the average American to be indifferent between living life as themselves or an average member of a specific demographic group – as our measure of welfare. Whereas JK study economic welfare across countries, we focus on the distribution of welfare within the United States. A person’s utility within a period depends on consumption and leisure. Each person also receives a set of group-specific mortality rates over their life cycle which gives rise to different life expectancies across demographic groups. Thus, welfare dispersion is driven by differences in average consumption, average leisure, and mortality rates.====We uncover a number of facts. First, we find that welfare is higher for women than men, highest for Asians and lowest for Blacks, and increasing in educational attainment. While men spend more time on market work than women on average, we show that the welfare “gender gap” persists even when classifying child care and home production as non-leisure activities. Second, the standard deviation of economic welfare is comparable to the standard deviation of income at midlife and more than double the standard deviation of consumption. There is also a wide range, with college-educated Asian women having more than three times the welfare of Non-Hispanic White and Black men without a college degree. Third, differences in life expectancy and consumption drive the welfare results with differences in leisure playing a more minor role. Fourth, the correlation between welfare and income is about 0.64, but rises to more than 0.9 within each gender. Finally, economic welfare for the average American grew by about 30 percent cumulatively, or two percent annually, between the first five years of this century and 2013–2017. Welfare growth is especially high for Black men and for individuals with a college degree. There is a correlation of about ====0.26 between initial welfare levels and subsequent growth, implying a limited degree of welfare convergence.====Although average welfare has increased since the start of the century, welfare growth due to changes in life expectancy is the smallest for less than college-educated White men and women without a college degree. To further study this finding, and motivated by the increase in “deaths of despair” since the turn of the century documented by Case and Deaton (2017), we calculate the welfare gains (and the distribution of those gains) from eliminating the rise in deaths caused by suicide, drug overdose, liver failure or cirrhosis since 2000–2004. We find that Whites without college degrees would be willing to sacrifice the most consumption to eliminate deaths of despair. Less educated White men, the hardest hit group, would be willing to give up about seven percent of lifetime consumption to return to the early 2000s deaths of despair mortality rates. This finding is consistent with Case and Deaton (2017) who find that the increasing prevalence of deaths of despair is enough to decrease the life expectancies of non-Hispanic Whites without a college education, while college-educated Whites and members of other groups, irrespective of education, have continued to see improvements in life expectancy.====Our paper relates to a long-standing literature attempting to create plausible proxies of economic welfare and then using those proxies to understand the distribution of welfare across countries and how it has changed over time within countries. An entire NBER volume in 1973 was devoted to studying how economic and social performance has evolved in the US (Moss, 1973). Anticipating future work in the subject, Nordhaus and Tobin (1971) build a measure of economic welfare that includes consumption, time use, and urban (dis)amenities.==== More recently, JK build a measure of economic welfare that includes consumption, leisure, life expectancy, and inequality. They show that cross-country welfare is strongly correlated with GDP per capita and that welfare has been growing faster than income per person on average. They note that the high correlation in the cross section hides some discrepancies between welfare and income. Falcettoni and Nygaard (2020) use the JK utility function to study how economic welfare is distributed across states in the US. Like us, they find that there is significant dispersion in welfare. While we share a common framework with Falcettoni and Nygaard (2020), we focus on different units on observation – with ours being demographic groups and theirs being states. Brouillette et al. (2021) quantify welfare differences between White and Black Americans. Like us, they find that welfare for the average Black American is lower than the average White American, but that this difference is closing over time. Unlike Brouillette et al. (2021), we disaggregate the results by education and show that the Black-White welfare gap is higher for the college educated.====Our paper is also related to a voluminous literature studying various measures of economic inequality in the US. Trends in income inequality have been extensively studied and are summarized in Goldin and Katz (2008) and Acemoglu and Autor (2011). The general consensus is that there exists a sizable college wage premium that has grown since the late 70s; real wages have fallen slightly over the same time period for workers without a post-secondary education, and that the increase in inequality has been concentrated in the top relative to the median rather than the median relative to the bottom. While cross-sectional consumption inequality is unequivocally lower than income inequality, the trend in consumption inequality is more controversial. Krueger and Perri (2006) show that the large increase in wage income inequality in the last quarter of the 20th century was not accompanied by nearly as much of an increase in consumption inequality. This conclusion has been challenged by Aguiar and Bils (2015) among others who argue that the Consumer Expenditure Survey (CE) is subject to substantial measurement error and that once one accounts for measurement error, consumption inequality has risen about as much as income inequality. In our baseline, we use the CE interview survey and consider some alternative definitions of consumption. These adjustments do not much change the quantitative results.====In addition to consumption and income, a number of recent papers document trends in time use and inequality in life expectancy. Aguiar, Hurst, 2007, Aguiar, Hurst, 2009 show that less educated individuals enjoy more leisure than highly educated individuals and this gap has expanded over time. Chetty et al. (2016) show that income and life expectancy are highly correlated and that differences in life expectancy by income group expanded between 2001 and 2014. Case and Deaton (2017) discuss the rising mortality rates for middle age White non-Hispanic men and women between 1999 and 2013, attributing much of the increased mortality rates to “deaths of despair.” Novosad et al. (2020) show that these increasing mortality rates are concentrated among the lowest ten percent of the education distribution. Our paper unifies the differences in cross-group consumption, time use, and mortality rates into a comprehensive welfare measure.====Finally, while our paper is about measurement, there is a large literature investigating the causes and consequences of rising income inequality in structural models with Heathcote et al. (2010b) being one example. Beyond some speculation, we are entirely silent on the economic mechanisms that give rise to the distribution of welfare. The quantified welfare differences that we present in this paper can be used in structural models that speak to specific mechanisms and evaluate welfare effects of particular policies. In that regard, one of our goals is to present a way of looking at the data that will be useful for structural modeling.","Working, consuming, and dying: Quantifying the diversity in the american experience",https://www.sciencedirect.com/science/article/pii/S0165188922000628,11 March 2022,2022,Research Article,72.0
Mineyama Tomohide,"Economist, International Monetary Fund 1900 Pennsylvania Ave NW, Washington, DC 20431, United States","Received 31 October 2020, Revised 25 February 2022, Accepted 1 March 2022, Available online 11 March 2022, Version of Record 29 April 2022.",https://doi.org/10.1016/j.jedc.2022.104350,Cited by (0),"In this paper, I study the optimal ","There has been a long-lasting debate regarding the optimal inflation rate. It is one of the most fundamental questions in monetary economics in that it involves the costs and benefits of inflation. One of the first studies to address the question is Friedman (1969), who argues that the inflation rate should be negative so as to minimize the opportunity cost of holding money. In the wake of the New Keynesian theory around the 1990s, a widely accepted view is that zero inflation is optimal in a cashless economy because both inflation and deflation generate welfare losses through relative price dispersion (e.g., King and Wolman, 1999).====In contrast to the early literature that supports the inflation rate at or below zero, many central banks in both advanced and emerging economies have adopted positive inflation targets. Schmitt-Grohé and Uribe (2010) point out that the observed positive inflation targets are puzzling in the light of the traditional views of monetary theories. Echoing their study, recent literature has explored potential sources of a positive optimal inflation rate.====Among others, a prominent factor that could justify a positive optimal inflation rate is downward nominal wage rigidity (DNWR). Numerous studies report that nominal wages are more downwardly rigid than upwardly (e.g., Bewley, 1999, Card, Hyslop, 1997, and more recently Grigsby et al., 2021). In this regard, Tobin (1972) claims that positive inflation facilitates real wage declines upon a contractionary shock even if nominal wages are downwardly rigid, mitigating the adverse effect of DNWR. However, the magnitude of such “grease the wheels” effect is still controversial. For example, Kim, Ruge-Murcia, 2009, Kim, Ruge-Murcia, 2019 (KRM henceforth) build a representative agent New Keynesian model with DNWR and find that the optimal inflation rate in the model economy is positive but close to zero.====In this paper, I demonstrate that the implications of DNWR for the optimal inflation rate are substantially altered once worker heterogeneity is taken into account. Specifically, I develop a sticky price model in which workers are heterogeneous in labor productivity, and wage changes are subject to asymmetric adjustment costs that potentially capture lumpy and asymmetric wage adjustments observed in the data. The specification nests the cases of absolute DNWR and fully flexible wages. When calibrated to U.S. micro wage data, the model suggests a much larger cost for downward wage adjustments than upward ones, which implies the presence of DNWR.====Regarding the cost of inflation, I adopt the staggered price setting of Calvo (1983) with non-zero steady-state inflation. In doing so, the model explicitly considers firms’ pricing behavior under different levels of the inflation rate and its welfare consequences.====Under the baseline calibration of the heterogeneous agent (HA) model, the optimal inflation rate is 2.3 percent per year. Notably, the optimal inflation rate is substantially higher than in a representative agent (RA) model that is often used by previous studies.==== The main reason behind the higher optimal inflation rate in the HA model is that individual workers seek to adjust their wages in response to idiosyncratic shocks as well as to aggregate conditions. Consequently, the lack of wage adjustments due to DNWR leads to a sizable welfare loss through an inefficient cross-sectional allocation of labor, whereas such cross-sectional implications of DNWR are abstracted in a RA model. In other words, the benefits of a positive inflation rate become larger in the presence of worker heterogeneity. Various sensitivity analyses verify that the difference between the HA and RA models remains considerable under alternative calibration. At the same time, the analyses uncover key determinants of the optimal inflation rate such as the trend productivity growth and aggregate volatility.====This paper is related to a long line of literature on the optimal inflation rate.==== In particular, a number of studies have explored the benefits of positive inflation after the Global Financial Crisis when many advanced economies suffered a severe downturn in a low-inflation environment. Some of these studies investigate the consequences of DNWR in a RA framework (e.g., Abo-Zaid, 2013, Carlsson, Westermark, 2016, Fahr, Smets, 2010, Kim, Ruge-Murcia, 2009, Kim, Ruge-Murcia, 2019). Other studies explore the ZLB (e.g., Andrade, Galí, Bihan, Matheron, 2019, Coibion, Gorodnichenko, Wieland, 2012); trends in relative prices (e.g., Ikeda, 2015, Wolman, 2011); and biases in measuring the inflation rate (e.g., Schmitt-Grohé and Uribe, 2012). On the other hand, previous studies also point out that shifting the steady-state inflation rate into a positive territory generates a considerable welfare loss through nominal price rigidity (e.g., Coibion et al., 2012). In fact, most of the studies mentioned above concluded that a moderate inflation rate or an inflation rate close to zero is optimal as a consequence of the trade-off between the costs and benefits of inflation.====The optimal inflation rate in a heterogeneous agent model is a relatively new research field. Previous studies address various dimensions of heterogeneity, including firms’ productivity growth (Adam and Weber, 2019), firms’ price setting (Blanco, 2021), and households’ asset holdings (Menna and Tirelli, 2017). For instance, Adam and Weber (2019) show that trend inflation affects the relative price among new and old firms whose productivity grows at different rates and estimate the optimal inflation rate for the U.S. economy between 1 to 3 percent per year. I contribute to this line of the literature by investigating the role of worker heterogeneity in the labor market.====As for the literature on DNWR, several studies, including Akerlof et al. (1996), Benigno and Ricci (2011), and Daly and Hobijn (2014), consider some sort of worker heterogeneity in an economy with DNWR, though quantitative investigation for the optimal inflation rate is not provided.==== An exception is Fagan and Messina (2009) (Fagan and Messina, 2009 henceforth), who build a stationary model with worker heterogeneity and found the optimal inflation rate for the U.S. economy is 5 percent per year. Compared to Fagan and Messina (2009), the novelty of this paper is twofold. First, I generalize the specification for individual wage adjustments. While Fagan and Messina (2009) use asymmetric fixed costs as a form of wage adjustment cost, I consider both fixed and linear components of adjustment cost. I also incorporate job changes, in which wages change frequently. Note that the estimation of the generalized model is attained thanks to the detailed micro data that recently became available. The generalized specification better explains individual wage adjustments observed in the data. More importantly, the optimal inflation rate is found to stay in a moderate positive territory—lower than the rate found by Fagan and Messina (2009). The result is understandable as Fagan and Messina (2009)’s specification of fixed cost, along with the assumption of workers infinitely staying at the same jobs, requires a significant shift in the inflation rate to facilitate wage adjustments. In contrast, the generalization of this paper accommodates various ways of wage adjustments (e.g., relatively small wage changes, wage adjustment through job changes). Second, I take the aggregate dynamics into account, whereas Fagan and Messina (2009) consider an stationary environment without aggregate shocks. While the presence of aggregate shocks affects both the costs and benefits of inflation, a quantitative analysis suggests that it enlarges the welfare loss arising from price rigidity more. Consequently, the optimal inflation rate is reduced compared to a case without aggregate shocks, contributing to this paper’s lower optimal inflation rate than Fagan and Messina (2009).====The remainder of the paper is organized as follows. Section 2 develops the model, and Section 3 describes the computation method and calibration procedure. Section 4 investigates the optimal inflation rate in the calibrated model. Section 5 offers various sensitivity analyses. Section 6 concludes.",Revisiting the optimal inflation rate with downward nominal wage rigidity: The role of heterogeneity,https://www.sciencedirect.com/science/article/pii/S0165188922000550,11 March 2022,2022,Research Article,73.0
"Nygaard Vegard M.,Sørensen Bent E.,Wang Fan","Department of Economics, University of Houston, Houston, TX, USA,Centre for Economic Policy Research, London, UK","Received 7 June 2021, Revised 20 December 2021, Accepted 3 March 2022, Available online 11 March 2022, Version of Record 31 March 2022.",https://doi.org/10.1016/j.jedc.2022.104352,Cited by (1),"A planner allocates discrete transfers of size ==== to ==== and has CES preferences over the resulting outcomes, ","A planner desires to allocate a number ==== of discrete units of a resource to heterogeneous groups ====, with resulting outcomes ==== assumed to increase with ==== at non-increasing rates. Each group is typically identified by a vector of covariates, but a group can be a single individual. The planner has a constraint on the total number of transfers, faces limits on how many units can be transferred to each group, and evaluates the distribution of outcomes using a CES index over group outcomes. We derive a closed-form solution for optimally allocating a fixed budget in such situations.====More formally, we consider the following optimization problem====subject to an aggregate resource constraint. Here, ==== are group-specific lower and upper bounds on allocations, ==== is a weight that allows for certain groups to be prioritized by the planner, and ==== parameterizes the strength of the planner’s aversion to inequality in outcomes. We show that for given inputs to the algorithm (i.e., calculated increments in ====), there exists a closed-form solution to this problem which takes the form of an ==== that lists the order in which each increment should be allocated until aggregate resources are exhausted. The optimal allocation queue is a sorted list of these increments, where successive values in the queue can involve the same or different groups.====We illustrate our method by applying it to study the optimal allocation of “stimulus checks” (or “support checks”) from the U.S. government to groups of households defined by covariates such as income and civic status. The two applications that we consider are the Economic Stimulus Act of 2008—where households received stimulus checks in the form of tax rebates in an effort to stimulate the economy in the wake of the Great Recession—and the Coronavirus, Aid, Relief, and Economic Security (CARES) Act of 2020/American Rescue Plan (ARP) Act of 2021—where the government issued stimulus checks in the form of cash transfers as part of an effort to insure households against the economic fallout brought about by the COVID-19 pandemic. We compare actual and optimal allocations under alternative household limits on tax rebates and check amounts. Motivated by the actual policies, we assume that the government in 2008 focused on stimulating aggregate consumption, whereas the government in 2020–2021 focused on the lifetime utility, or welfare, of check recipients.====To apply our framework and solve for the optimal allocations, we need inputs from a consumer model. For the 2008 tax rebates, we need to know the effect of increments in the rebates on each group’s consumption, and for the 2021 cash transfers, we need to know the effect of increments in the transfers on each group’s lifetime utility. We derive these inputs by formulating a life-cycle consumption-savings model where consumers are ex-ante heterogeneous in marital status, educational attainment, number of children, and rate of time discounting. Consumers face idiosyncratic shocks to income and fertility over their life-cycle. Unemployment probabilities and unemployment insurance are calibrated to the relevant crisis-periods, and the marginal utility of consumption is assumed to be temporarily reduced during the COVID-19 pandemic due to the lockdown. We use the model to predict the impact of each $100 increment in tax rebates or check amounts for different household types: single or married, having 0–4 children under age 18, of different ages and income-levels.====A key feature of our approach is to break the problem into two steps. First, approximate the allocations by an integer number of increments (in our application, $100 increments), and use data, regressions, or a model (in our application, the life-cycle model applied to a typical member of a group) to compute each group’s gains (as evaluated by the planner) from each allocation increment within the given limits. Second, based on the matrix of gains for each potential increment, sort those gains across both increments and groups in a descending sequence and optimally allocate from the beginning till the budget is exhausted.====While we focus on the optimal allocation of tax rebates and checks, our solution method can be applied in several settings where a government or agency needs to allocate limited resources among heterogeneous recipients. Resources may come in discrete units or in continuous units for which discrete approximations are reasonable (as in our examples), and there may be bounds on how much the recipients can and/or must receive. Examples are job training programs for displaced workers and nutritional aids for children who are at risk of undernourishment. Moreover, the framework easily allows for different planner objectives: in our examples, maximizing aggregate consumption and maximizing a CES index over the welfare of recipients. The inputs required by the algorithm can be output from structural models, regressions, experiments, or data. The ==== efficiently solves for the optimal allocation queue along with aggregate outcome statistics for any value of planner inequality-aversion for given inputs.====Our algorithm requires non-increasing impacts of increments, which may rule out some applications, for example, consumer models with threshold wealth needed for acquiring a durable good, and it does not allow for direct spillover effects between groups, which precludes it from being used to evaluate the benefits of, for example, vaccinations which protect recipients as well as those around them.",Optimal allocations to heterogeneous agents with an application to stimulus checks,https://www.sciencedirect.com/science/article/pii/S0165188922000574,11 March 2022,2022,Research Article,74.0
"Dierkes Maik,Krupski Jan,Schroen Sebastian","Leibniz University Hannover, Institute of Banking and Finance, Koenigsworther Platz 1, Hannover DE-30167, Germany","Received 1 September 2021, Revised 14 January 2022, Accepted 7 March 2022, Available online 8 March 2022, Version of Record 2 April 2022.",https://doi.org/10.1016/j.jedc.2022.104356,Cited by (0),"We study the impact of time-varying lottery demand on first-day returns and the poor long-term performance of IPOs. Lottery demand – measured in terms of option-implied ==== weighting – is associated with significantly higher first-day returns, tantamount to higher IPO underpricing and more money left on the table. Interacting the time variation in lottery demand with cross-sectional expected skewness reveals that IPO returns are particularly driven by the interaction between market-wide lottery demand and asset-specific lottery characteristics. When expected skewness meets low lottery demand, there is virtually no effect of skewness on first-day returns. In the long run, IPOs issued in high lottery demand regimes are more likely to perform poorly for up to five years after the IPO.","We present a novel behavioral explanation for the anomalously high first-day returns of initial public offerings (IPOs) and their underperformance in the long run. Our explanation is based on time-varying demand for lottery-like investments, measured in terms of market-wide probability weighting. Our rationale is as follows: As outlined by Barberis and Huang (2008), the positively skewed returns of IPOs are attractive to investors with a preference for lottery-like returns, also referred to as lottery demand. Due to lottery demand, IPOs earn high first-day returns but perform poorly in the long run, tantamount to a correction of the initial overpricing. We account for time variation in lottery demand and find that expected lottery demand significantly predicts high first-day returns as well as a poor long-term performance. Furthermore, we disentangle the pricing effects of expected idiosyncratic skewness and lottery demand and find that skewness significantly predicts IPO returns – as documented by Green and Hwang (2012) – but only if there is a market-wide lottery demand to cater for. Our results suggest that institutional investors have a lower preference for skewed returns as they are less likely to exert buying pressure in IPOs, both in the primary and the secondary market.====We follow Dierkes (2013) and estimate Cumulative Prospect Theory’s (CPT) probability weighting parameter gamma (====) nonparametrically from S&P 500 index options. During our sample period from 1996 to 2020, we find several episodes of increased probability weighting, for example, during the run-up of the DotCom bubble in the late 1990s or the most recent surges of US stock indices in 2019 to 2020. Since lower values of gamma imply stronger probability weighting, we use gamma as an inverse proxy for expected lottery demand in the subsequent month. More specifically, gammas below one are tantamount to an overweighting of small probabilities, consistent with higher lottery demand.====We find expected lottery demand to perform well in explaining both the anomalously high first-day returns and the poor long-term performance of IPOs. With respect to first-day returns, a sample split at ==== yields a highly significant difference of 14.28 percentage points (====-value = 12.27) between periods of high and low lottery demand (26.76% versus 12.48%). The effect is strongest for firms younger than 4 years. Our baseline results extend to a regression analysis which controls for various deal, firm, and market characteristics known to affect first-day IPO returns. A one unit ==== of gamma – and thus an ==== in lottery demand – is associated with higher first-day returns of up to 13 percentage points (====-value = –9.42). Net of control variables and industry fixed-effects, this effect reduces to six percentage points but remains statistically significant at the 1%-level (====-value = –3.63).====Given our time series of gammas, we are able to disentangle the effects between skewness and lottery demand and find the interaction of these to be an important driver of IPO returns. More precisely, we revisit the findings of Green and Hwang (2012) who relate first-day returns to the expected skewness of stock returns within the IPO’s industry and also document a positive impact of skewness. Compared to Green and Hwang (2012), the performance of expected skewness is not only weaker once we include control variables, but also depends on the prevailing lottery demand regime. Interacting lottery demand and expected skewness highlights that skewness significantly predicts first-day IPO returns, but only if there is lottery demand to cater for. Without lottery demand, the relation between skewness and first-day returns is virtually flat.====Expected lottery demand also explains the poor long-term performance of IPOs. We compare long-term IPO returns to the returns of non-issuers by matching each IPO to the firm with the closest book-to-market ratio in the same size decile. Although matching firms outperform IPOs for several return horizons (1 year, 3 years, and 5 years), this outperformance is only significant for all three time horizons if the IPO took place during a high lottery demand regime. We thus conclude that IPOs are more likely to become overpriced on the first day of trading if they go public during periods of high lottery demand. Since this overpricing tends to be corrected in the long run, respective IPOs are more likely to underperform matching firms.====A closer look at the relationship between expected lottery demand and the trading behavior of institutional and individual investors furthermore suggests that our results are driven by the differential in the skewness preference between institutional and individual investors. Institutional investors in the primary market only incorporate a fraction of 12.5%–25% of expected lottery demand in their revision of the offer price, whereas the lion’s share of the market reaction occurs in the secondary market. Well in line with this finding, expected lottery demand predicts retail buying pressure on the first trading day of IPOs, while being less important for the trading behavior of institutional investors, both measured in terms of the Barber et al. (2009) herding measure. This finding likely explains why expected lottery demand particularly predicts IPO returns on the secondary market and suggests that retail investors indeed have a higher skewness preference than institutional investors. Apart from that, we perform several robustness checks and find that our baseline results hold for alternative definitions of lottery demand regimes and different sub-periods.====Our study contributes to a large and growing body of literature on the underpricing of IPOs and their subsequent long-term performance. Early studies focus on traditional explanations based on information asymmetry (Beatty, Ritter, 1986, Rock, 1986), litigation and reputation risk (Lowry, Shu, 2002, Tinic, 1988), and a changing risk composition (Ritter, 1984).==== Under the changing risk composition hypothesis, riskier IPOs are expected to be more underpriced, equivalent to higher first-day returns. Ljungqvist and Wilhelm (2003) explain the exceptionally high first-day returns in 1999–2000 by a reduced (fractional) CEO ownership and a strongly increased proportion of ‘family & friends’ shares.==== Since the latter entitle to purchase shares ==== the offer price, there is an incentive to underprice. Loughran and Ritter (2004) reject this finding together with the changing risk composition hypothesis. Instead, they propose that return differences trace back to a changing objective function of issuing companies. In the late 1990s, issuers were more willing to accept underpricing due to an increased emphasis on analyst coverage (resulting in an oligopoly of underwriters) and personal brokerage accounts (resulting in incentives to seek underpricing).====Ritter and Welch (2002) suggest a behavioral perspective in order to explain IPO underpricing. In this direction, Loughran and Ritter (2002) propose a Prospect Theory approach to explain why issuers agree to leave ‘money on the table’ (which is tantamount to underpricing). According to Prospect Theory, issuing firms focus on the ==== in wealth rather than the ==== of wealth, which is in line with Prospect Theory’s reference point dependent valuation (see Kahneman, Tversky, 1979, Tversky, Kahneman, 1992). Therefore, they will aggregate the loss from leaving money on the table and the large gain from an increased valuation of retained shares, resulting in a net profit.==== As a consequence, issuers only partially adjust offer prices to news and high demand during the book building period.==== In a more recent study, Loughran and McDonald (2013) show that IPO underpricing is also related to the tone of the S-1 form. By conducting a text mining approach, they find higher first-day returns for issuers whose S-1 form contains more words that are uncertain or negative. Loughran and McDonald (2013) conclude that their results are consistent with Prospect Theory as well.====Barberis and Huang (2008) take a closer look at the relation of IPO underpricing and probability weighting, which is one of CPT’s key preference components. Investors (on average) overweight small probabilities for large gains, resulting in a preference for lottery-like stocks with highly right-skewed returns. Barberis and Huang (2008) predict that IPOs with positively skewed returns earn higher first-day returns but perform poorly in the long run. As the study most closely related to ours, Green and Hwang (2012) provide first evidence in favor of this prediction. They find expected skewness, based on the IPO’s industry, to be positively (negatively) related to first-day returns (long-term returns).==== This result is in line with Boyer et al. (2010) who find a negative impact of idiosyncratic skewness on subsequent returns and Kumar (2009) who shows that individual investors prefer stocks with lottery-like features and underperformance is strongest for those who overweight lottery stocks the most.====We provide additional evidence in favor of behavioral explanations for IPO underpricing and extend the findings of Green and Hwang (2012) in several directions. First, we provide a cleaner test of the predictions in Barberis and Huang (2008) by directly estimating the impact of probability weighting on IPO pricing. Second, the empirical literature usually studies the asset pricing implications of time-varying skewness under constant preferences for skewness. However, there is ample evidence that skewness preferences (and thus lottery demand) vary over time. Kumar (2009) finds higher gambling demand during economic downturns, whereas Polkovnichenko and Zhao (2013) and Dierkes (2013) show that option-implied skewness preferences exhibit significant time variation. By accounting for this time variation in aggregate lottery demand, we shed further light on the otherwise puzzling episodes of high IPO underpricing documented in Loughran and Ritter (2004).==== Third, by studying the interaction between expected lottery demand and cross-sectional expected skewness, we are able to emphasize the role of aggregate preferences for skewness as the main driver of the findings in Green and Hwang (2012). High expected lottery demand strongly amplifies the otherwise flat relationship between idiosyncratic skewness and first-day returns, further highlighting the interaction between skewness preferences and asset-specific skewness as an important determinant of IPO returns.",Option-implied lottery demand and IPO returns,https://www.sciencedirect.com/science/article/pii/S0165188922000616,8 March 2022,2022,Research Article,75.0
"Aldasoro Iñaki,Hüser Anne-Caroline,Kok Christoffer","Bank for International Settlements, Switzerland,Bank of England, United Kingdom,European Central Bank, Germany","Received 8 March 2021, Revised 19 January 2022, Accepted 5 March 2022, Available online 7 March 2022, Version of Record 19 March 2022.",https://doi.org/10.1016/j.jedc.2022.104354,Cited by (5),. Direct interbank contagion is negligible in our analysis. Our findings underscore the importance of accurately estimating the price effects of fire sales.,"Banking sector interconnectedness and contagion materialize in more than one way. The most obvious channel linking the health of various banks is the direct exposures between them. However, as highlighted by developments during the Great Financial Crisis and the Covid-19 crisis, the fate of seemingly disconnected and geographically dispersed institutions can still be bound together due to the importance of indirect contagion (Clerc et al., 2016). Most important within the universe of such interconnections are exposures to the same assets (i.e. “overlapping portfolios”) and to externalities driven by fire sales (Shleifer and Vishny, 2011a). These channels are all part of the transmission of distress in the banking system. The importance of each contagion channel remains a contested issue and is ultimately an empirical question.====We provide a simple and tractable accounting-based stress-testing framework to assess loss dynamics in a banking sector connected via direct interbank market exposures and overlapping portfolios. The model allows for a straightforward incorporation of fire sale price dynamics in the spirit of Greenwood et al. (2015) and Duarte and Eisenbach (2020). We develop measures that allow for the computation of losses stemming from network exposures, as well as additional losses generated by fire sale dynamics. In this way, we provide an exercise in ====: we can account for the contribution to system losses of each contagion channel. We apply our framework to granular microfinancial euro area data, and document that interbank exposures account for only a minor share of the overall losses due to contagion. Aside from the direct effect of shocks, the bulk of distress contagion is driven by the common exposures of banks that make them vulnerable to fire sales and synchronous price dislocations. The calibration of the price impact of fire sales is a critical input to determine the extent of losses.==== Stress-testing would greatly benefit from more efforts to improve such calibrations.====We present our model in a modular way. The starting point is a basic framework focusing on direct exposures between banks, following Aldasoro and Angeloni (2015). It builds on an accounting representation of the balance sheet of the banking system, which analytically bears a close resemblance to classic input-output analysis (Leontief, 1936). We consider a banking system comprising assets and liabilities both internal and external to the interbank network, as well as deposits and equity on the liability side. We decompose losses into the initial loss due to a shock to the bank’s balance sheet, the first-round effect due to losses suffered by direct counterparties, and the higher-round amplification effects. Some studies suggest, however, that direct interbank exposures do not generate substantial losses in the banking system, unless they are coupled with additional contagion channels such as fire sales and overlapping portfolios.====We then expand the model by adding a second form of interconnectedness through overlapping portfolios. Under marking-to-market, banks investing in the same asset would be simultaneously impacted by price movements in that asset. Conditional on price movements, this introduces another, indirect, contagion channel. Within our framework, we can decompose the contribution of each channel to loss dynamics. Importantly, the framework can be tailored to data availability. In the extreme, it is flexible enough to drill down to securities-level data and account for contagion from the bottom up.====Finally, to make overlapping portfolios bite as a contagion channel, we add price dynamics due to fire sales. We do so by combining our approach with a variation of the matrix-based fire sales model of Greenwood et al. (2015) and Duarte and Eisenbach (2020). A key element in that model is the active deleveraging response of banks, whereby, following a shock, they sell assets to regain target leverage (Adrian and Shin (2014)). They sell assets for shocks of any size and from their entire asset holdings. We also assume this active leverage targeting-based reaction on the part of banks, but we posit a slightly different selling behavior. To make the stress testing more realistic, distressed banks sell assets only from the trading book.==== It indeed seems unrealistic that distressed banks would also sell assets from the banking book, since these are labeled as to be held to maturity for regulatory purposes. To the best of our knowledge, this is the first paper that takes into account the distinction between the trading book and the banking book in studying fire sales dynamics, in addition to providing a framework that allows for accounting for different channels of contagion and that can be mapped directly to real data.====We complement the vulnerability measures developed by Greenwood et al. (2015). They define aggregate vulnerability as the percentage of aggregate bank equity that would be wiped out if banks fire sell in response to a shock to asset returns. The measure only takes into account the impact of the change in prices, but not the balance sheet impact of the asset sales. We therefore build analogous measures to quantify the balance sheet impact of fire sales.====The framework is illustrated by using three confidential European Central Bank (ECB) datasets for the 26 largest euro area banking groups, using observations for the first quarter of 2016. The balance sheet data are extracted from ECB supervisory statistics. These are also used to construct overlapping portfolios of loans and derivative positions as well as to build the network of bilateral exposures. In order to construct the overlapping portfolios of securities, we make use of two micro-financial datasets, namely the European System of Central Banks’ Securities Holdings Statistics by Group and the ECB Centralised Securities Database. We distinguish between assets held in the trading and banking books and document that tradable assets make up a third of the total assets of banks in the sample. The five largest asset categories in the trading book are derivatives, long-term government bonds, loans to other financial corporations, loans to banks and long-term bonds issued by other financial corporations. Tradable assets holdings are higher for more leveraged banks.====A 5 percent shock to the price of assets held in the trading book leads to an initial loss of 30 percent of system equity and an additional loss of 1.3 percent due to fire sales spillovers.==== Deleveraging in the interbank market on its own does not lead to large balance sheet adjustments, which confirms empirically the result of the simulation studies referenced above. Only in the presence of fire sale dynamics is there a significant balance sheet change due to deleveraging: in response to a 5 percent price decline in tradable assets – which wipes out 30 percent of system equity – banks will adjust their balance sheets in the order of 5.9 times system equity in order to restore initial leverage ratios.==== In the presence of indirect contagion due to fire sales, there can be significant deleveraging in response to shocks.==== The paper is structured as follows. Section 2 reviews the related literature. Section 3 presents the structure of our stress-testing framework, which is built sequentially from direct exposures up to the inclusion of price dynamics. Section 4 presents the data as well as descriptive statistics of the banks’ balance sheets, of the network layers and of the banks’ portfolios. Section 5 present results on the empirical application of the framework to the data presented in Section 4. Concluding remarks are presented in 6.",Contagion accounting in stress-testing,https://www.sciencedirect.com/science/article/pii/S0165188922000598,7 March 2022,2022,Research Article,76.0
Shang Fei,"School of Economics, The University of Sydney, Sydney, NSW, 2006, Australia","Received 17 June 2021, Revised 25 January 2022, Accepted 6 March 2022, Available online 7 March 2022, Version of Record 21 March 2022.",https://doi.org/10.1016/j.jedc.2022.104355,Cited by (1),I examine how the level of uncertainty affects the sensitivity of the U.S Treasury yield curve to ,"Does uncertainty affect the response of the yield curve to monetary policy shocks? If so, are the different effects more evident at the shorter or longer end of the yield curve? While many studies have investigated the sensitivity of the yield curve to monetary policy shocks, less is known about the possible nonlinearities across the different states of the world, such as periods of high or low uncertainty.====In this paper, I consider a threshold factor-augmented VAR model based on the dynamic Nelson-Siegel model to study the effect of unexpected monetary policy surprises on the yield curve of U.S. Treasury bonds. Since Diebold and Li (2006), the dynamic Nelson-Siegel model has been widely used and extended to include regime shifts for yield curve modeling and forecasting, but unlike Markov switching models (Eo, Kang, 2020, Guidolin, Pedio, 2019, Xiang, Zhu, 2013) in which variations in the parameters are governed by a Markov process of the unobservable state variable, I directly link the regimes shifts with the level of uncertainty in the economy. The three factors defined in the Nelson-Siegel framework capture the majority of information related to the yield curve, and their sensitivity to monetary policy surprises under different levels of uncertainty is also examined. Furthermore, this paper estimates the responses of the entire yield curve across high and low uncertainty regimes and investigates the influence of the zero lower bound (ZLB) period (from November 2008 to the end of my sample period) on state-dependent responses of the yield curve.====My empirical approach offers three advantages related to using the information contained in the whole yield curve. First, the model is based on a Nelson and Siegel (1987) and Diebold and Li (2006) approach that provides a parsimonious model of the term structure represented by three factors: level, slope, and curvature, which possess common information of as many as 17 different maturities Treasury yields and provide a natural economic interpretation of the results. Second, a novelty of this model is to consider regime switching within a latent process, which improves the identification of the threshold in a Bayesian setting because it is not affected by noise in the monthly yield data but still preserves the fundamental dynamics of the whole yield curve. Third, yields with different maturities are linear functions of the three factors, and thus one can easily derive a spectrum of responses for the whole yield curve from the factor responses.====I use a Gibbs sampling algorithm for estimation of my heavily parameterized model. Specifically, estimation of the latent factors does not involve Kalman filtering and smoothing recursions. However, it builds upon a simple derivation of the posterior density of the latent factors conditional on the data and the parameters as in Chan and Jeliazkov (2009), including using the precision sampling method within the Gibbs sampler to estimate latent factors efficiently. For model comparison, I use the Deviance Information Criterion (DIC) particularly tailored for my model rather than Bayes factors based on the marginal likelihood, which is computationally costly, if not impossible. For a simulated data set, the algorithm proves to be efficient and precise in identifying the nonlinearity and estimating model parameters, and the DIC performs well in terms of favoring the correct model.====There are several reasons why uncertainty might affect how Treasury yields react to monetary policy. First, the literature on financial markets has considered theoretical links between financial market volatility and macroeconomic uncertainty (David, Veronesi, 2000, Veronesi, 1999). A standard view is that macroeconomic uncertainty is highly correlated with financial market volatility. In a typical financial market, participants make inferences about asset prices based on observations of macroeconomic fundamentals. When uncertainty about fundamentals is high, unexpected news causes assets to move more drastically than normal. Therefore, one might expect the bond market to exhibit a similar behavior despite its unique qualities compared to other asset markets. Second, uncertainty is related to investors’ risk aversion and thus affects how they perceive monetary policy changes. Bekaert et al. (2013) confirm a dynamic link between risk, uncertainty, and monetary policy, using a simple vector-autoregressive framework. U.S. Treasury bonds are largely traded and priced on the secondary market, and it is natural to assume that under high uncertainty (usually during recessions), restless market participants could drive bond prices to display amplified responses to unanticipated shocks. Third, an increase in uncertainty indicates that the economy is less predictable (Jurado et al., 2015), so that market participants’ ability to predict future economic states is restricted, especially after a recessionary disturbance. Information conveyed in Fed actions could receive a different weight than usual. Of course, one may also argue that investors could expect to be surprised in high uncertainty periods and react relatively less intensively after an announcement, although the results do not support this. Also, the global financial crisis (GFC) is an unusual period with exceptionally high uncertainty in the sample, and the zero lower bound introduced significant constraints on the monetary policy that could also play an essential role in the nonlinear responses of the yield curve in my sample period.====My analysis covers the period of 19902016, and it turns out uncertainty and the zero lower bound are both crucial factors in understanding possible nonlinearities in the sensitivity of the yield curve to monetary policy surprises during this time. The results show a significant difference in the responses of the entire yield curve across two regimes during the zero lower bound but an insignificant difference before the zero lower bound. Moreover, in the high uncertainty period during the zero lower bound, the longer end of the yield curve is much more affected by surprises than the short end. In contrast, during other periods, the long end is less affected by a monetary surprise, consistent with traditional views about the yield curve. To investigate the different results before and after the zero lower bound, I quantify the different effects of monetary policy surprises on the three factors driving the yield curve dynamics. The results show that the differences mainly result from the level and slope factors, and the signs of their sensitivity are reversed across the two regimes. However, there is no significant difference in the sensitivity of the level factor across the two regimes. The ZLB period clearly plays a crucial role in the analysis, and it appears to correspond to a fundamental change of the impact of the monetary policy on the yield curve.====This study is the first to my knowledge to explicitly explore the yield curve responses to monetary policy surprises under different economic states. It is also the first to relate yield curve sensitivity with uncertainty and the zero lower bound. The previous literature that adopts a regime-switching framework only indirectly links the regimes with business cycles such as recessions or expansions, although that could also be relevant to my study because it has been shown that uncertainty is usually high during recessions and vice versa. Also, I use a multivariate dynamic model instead of an event study approach, which could provide more insights into the dynamic effects. An impulse response analysis shows that the responses of the yields to a one standard deviation ”shock” in monetary policy surprises in each regime are different in pattern and magnitude.====The rest of this paper is organized as follows. Section 2 provides a short overview of the related literature. Section 3 presents the threshold factor augmented vector autoregressive model of the Treasury yields. Section 4 describes the dataset, including the core variables in the model. Section 5 reports empirical results for my proposed approach and conducts subsample analysis regarding the ZLB period. Section 6 concludes. Full details of the estimation methods and model comparison measure DIC for my model are provided in the Appendix.",The effect of uncertainty on the sensitivity of the yield curve to monetary policy surprises,https://www.sciencedirect.com/science/article/pii/S0165188922000604,7 March 2022,2022,Research Article,77.0
"de Soyres François,Gaillard Alexandre","Federal Reserve Board, United States,Toulouse School of Economics, France","Received 7 August 2021, Revised 3 March 2022, Accepted 4 March 2022, Available online 7 March 2022, Version of Record 23 March 2022.",https://doi.org/10.1016/j.jedc.2022.104353,Cited by (3),This paper revisits the relationship between trade and cross-country GDP correlation for 134 countries from 1970 to 2009. We introduce two notions of trade linkages: (i) ==== bilateral trade index and (ii) common exposure to ”third” countries capturing the role of ,"Over the past decades, both import and export flows have increased much faster than GDP for almost all countries in the world. This march toward more open economies has been accompanied by a reorganisation of the world’s production across different locations: as a share of world GDP, both trade in intermediate inputs and in final goods increased sharply, reaching in 2009 around three times the share observed in the 1970s. In valued-added terms, trade increased at an average annual growth rate of more than 5 percent during the 1990–2009 period. During the same period, the average cross-country correlation of GDP – or GDP comovement – rose from 6% to 38%.====The increase in the overall density of the world trade network suggests that complex patterns of international propagation could be at play. Over and above direct bilateral trade linkages, two countries can be increasingly connected to the same direct or indirect trade partners, implying a higher ”====” exposure. The consequences of these changes in trade patterns for the synchronization of economic activity are important because they can have implications for macroeconomic policies.==== In light of these global trends, several questions arise: are ==== trade linkages more important than common exposure through trade network? Did the rise of Global Value Chains (GVCs) have a specific effect on the correlation of GDP and its association with both direct and indirect trade flows? Did the rise in production fragmentation have the same effect across income groups? Did the sensitivity of GDP comovement to an increase in bilateral trade flows evolve over time?====Since the seminal paper by Frankel and Rose (1998), hereafter FR, a large empirical literature has studied the determinants of cross-country business cycle comovement. From a qualitative point of view, the vast majority of papers emphasize the role of bilateral international trade linkages as an important determinant of GDP comovement: an increase in trade intensity is associated with an increase in cross-country GDP correlation. From a quantitative point of view, however, the strength of the association is very heterogeneous across papers.==== Using a sample of 100 low and high income countries from 1970 to 1995, Baxter and Kouparitsas (2005) show that the association between bilateral trade intensity and GDP comovement is robust, but do not find a clear role of industrial similarity highlighted in Imbs (2004). Their point estimate implies that a doubling of bilateral trade intensity is associated with a 1.5 to 3 percentage points increase in GDP correlation. Kose and Yi (2006) use 21 OECD countries from 1970 to 2000 and show that a doubling of bilateral total trade over GDP is associated with a GDP comovement surge of about 6 percentage points. Calderon et al. (2007) use a comprehensive dataset containing 147 countries between 1960 and 1999. The authors highlight that bilateral trade intensity has a smaller impact on business cycle correlation among low-income countries than among high-income ones. When bilateral trade over GDP is multiplied by two, the associated increase in GDP comovement ranges from just 1.0 to 6.5 percentage points, respectively. Using a sample of 21 OECD countries from 1970 to 2003, Inklaar et al. (2008) employ a structural model to estimate the effect of trade intensity on output correlation. They find that doubling bilateral trade over GDP increases GDP cross-correlation between 3.5 and 9 percentage points. Based on a panel of 55 countries and 28 sectors from 1970 to 1990, Di Giovanni and Levchenko (2010) show the importance of trade in intermediate inputs. In their estimation, doubling bilateral trade intensity is associated with a 4 percentage points increase in output comovement, with an effect three times bigger for sector-pairs characterized by strong input-output links. Using panel regressions, Duval et al. (2016) use a mix of high and low countries from 1995 to 2013 and show the importance of the measurement of value-added trade.==== Finally, Liao and Santacreu (2015) use a sample of 20 OECD countries and 10 developing countries from 1980Q1 to 2009Q4 and highlight the importance of the extensive margin of trade. All told, the empirical ==== (hereafter, TC-slope) varies greatly with the sample of countries, time coverage, and constructed ==== trade indices.====On the theoretical side, the widely accepted empirical association has spurred a series of papers proposing different channels through which ==== trade linkages propagate shocks across countries. However, as shown by Kose and Yi (2006), the workhorse IRBC can explain at most 10% of the ==== between bilateral trade linkages and business cycle synchronization, leading to what they called the ==== (TCP). Since then, the literature has refined the puzzle, highlighting ingredients that could bridge the gap between the data and standard models (Johnson, 2014), Duval et al. (2016), Drozd et al. (2021) and (de Soyres and Gaillard, 2021).====Despite the important heterogeneity of the TC-slope found in the literature, there is surprisingly little systematic empirical research on the association between trade linkages on GDP correlation. Guided by a simple theoretical model, the purpose of this paper is to estimate and decompose the relationship between direct (bilateral) and indirect (network) trade linkages and GDP correlations. To this end, we estimate the TC-slope using a large sample of 134 countries from 1970 to 2009 which contains large variations in the countries’ composition of traded goods and development stages. Using constructed panel data, we control for time-invariant country-pair specific effects and time varying global effects, eliminating a potential source of omitted-variable bias resulting from unobserved heterogeneity between country-pairs and over time. We estimate the TC-slope across different income groups and unveil a series of new determinants of GDP comovement for each income group, including the different role of the content of trade flows and the presence of network effects. Moreover, we also uncover important time variations in the TC-slope, which suggests that the sensitivity of GDP correlation to changes in trade proximity is not akin to a time-invariant structural parameter but is itself a function of other elements that evolve over time. As such, in the light of the Trade-Comovement Puzzle, our insights shed light on new dimensions of trade that go beyond the accounting of direct bilateral total trade linkages in generating GDP synchronisation across countries. Our results highlight that country-specific stage of development and the evolving network structure of trade linkages are key dimensions of heterogeneity.====Our first contribution is to separate intermediate inputs from final goods trade and to investigate their specific role for GDP synchronization for high and low income countries. We first confirm and update previous findings that total trade intensity is significantly associated with higher GDP correlation among all income groups, but the association is economically much stronger in high income countries. This result echoes cross sectional estimates in Calderon et al. (2007) and Di Giovanni and Levchenko (2010), who show that the relationship between trade integration on business cycles is higher for industrial countries than for developing countries. Moreover, as shown in de Soyres and Gaillard (2021) and confirmed in this paper, trade in intermediate inputs plays a particular role in the TC-slope for OECD countries. However, this finding is complemented and nuanced here by a novel insight regarding low income countries. Using only ==== country-pair variations and controlling for several factors including changes in the similarity of industrial structure across country pairs, we show that economies at the lower end of the income distribution experience an increase in the correlation of their GDP with their trade partners when the content of their trade flows is more tilted toward final goods.====Second, guided by recent debates on the role of Global Value Chains and the systemic interdependence that can arise from worldwide input-output (I/O) linkages (see Bems et al. (2011) and others), we move beyond ==== trade linkages and construct new indices of ==== proximity for all country pairs. We argue that changes in GDP synchronization between two countries can be the result of an increased common exposure to third markets, which can happen either at the first order when two countries have similar trade partners or at the second order when countries’ direct partners have similar partners. On the whole, our results reveal that the association between GDP synchronization and both first- and second- order common exposure are strong in the data. Moreover, we show theoretically and empirically that the marginal increase in GDP correlation associated with larger trade links is itself increasing in the overall ==== of the network. As such, the amplifying effect of network density helps rationalize the wide array of TC-slopes found in the literature. Indeed, any estimate of the impact of a marginal change of trade linkages on GDP comovement depends on the overall network structure of the economy, which in turn depends on the geographical and time period considered. While previous empirical research on the association between trade and GDP correlation does not ==== state that there is only a single, immutable, TC-slope, the empirical specifications and associated quantitative exercises sometimes make such assumption ====. Our result questions this implicit assumption and emphasises that there are important variations in the TC-slope. As discussed in Section 6, this simple observation helps understand the variety of TC-slopes found in the literature. Moreover, it suggests that further theoretical and quantitative investigations of the direct and indirect role of trade linkages in propagating shocks across countries are necessary. Finally, most of our results are robust to the addition of different controls, measures and sample selection.====The rest of the paper is organized as follows. In Section 2, we propose a simple trade framework along the line of Long and Plosser (1983) and Acemoglu et al. (2012) that highlight how global GDP-comovement is related to direct and indirect trade exposure, before turning to our main empirical contribution. Section 3 presents the data and describes the variables used throughout the paper. Section 4 investigates the global TC-slope across countries. We highlight important differences across income groups and present evidence of significant time variations, which we link to the evolution of the overall density of the world trade network. In Section 5, we show that our results are robust to a variety of alternative specifications. Section 6 summarizes our findings and Section 7 concludes.",Global trade and GDP comovement,https://www.sciencedirect.com/science/article/pii/S0165188922000586,7 March 2022,2022,Research Article,78.0
"Garriga Carlos,Manuelli Rody,Sanghi Siddhartha","Federal Reserve Bank of St. Louis, USA,Washington University in St. Louis, USA","Available online 4 March 2022, Version of Record 21 June 2022.",https://doi.org/10.1016/j.jedc.2022.104351,Cited by (5),"This paper analyzes the optimal management of a pandemic (stay-at-home and vaccination policies) in a dynamic model. The optimal lockdown policies respond to the spread of the virus with significant restrictions to employment, followed by partial loosening before the peak of the epidemic. Upon the availability of a vaccine, the optimal vaccination policy has an almost bang-bang property, despite the loss of immunity of the vaccinated: vaccinate at the highest possible rate, and then rapidly converge to the steady state. The model illustrates interesting trade-offs as it implies that lower hospital capacity requires flattening the infection curve and hence a more stringent lockdown, but lower vaccination possibilities (both the likelihood of a vaccine and the vaccination rate) push the optimal lockdown policy in the opposite direction, even before the arrival of vaccine. The model implies that the “dollar value of a vaccine decreases rapidly as time passes with the reinfection rate being an important determinant of the monetary value”. The value that society assigns to averting deaths is a major driver of the ","How does the optimal response to an epidemic–both in terms of the nature of “stay-at-home” policies that restrict employment as well as the intensity of vaccination efforts–depend on the features of the economy and the epidemiological parameters? To make progress in understanding the answer to this question, we study the problem faced by a planner that can impose restrictions on employment and, when a vaccine becomes available, has to decide on the (costly) intensity at which the population can be vaccinated.====We study a standard continuous time infinite horizon model. We assume that individual preferences depend on individual consumption, and that social preferences take into account the utility loss associated with deaths. In addition, we model the impact of a virus using a standard SIRS model, and we constrain the policy space taking into account limitations imposed by the existing public health infrastructure. Another important dimension of the model is that a fraction of individuals that have been vaccinated lose immunity. That requires additional rounds of vaccination for individuals previously vaccinated.====We assume that at the beginning of the epidemic–what we label Phase I– the only policy available to the planner is a stylized version of a “stay-at-home” policy that, simultaneously, restricts employment and lowers the rate of transmission of the epidemic. Phase I ends when a vaccine becomes available and the economy enters Phase II. We view the arrival of a vaccine as a random event and take the probability distribution as exogenous. At this point, the planner has a second tool to control the epidemic: the speed at which the population can be vaccinated, which we also view as requiring resources. This less-than-instantaneous ability to vaccinate the population is a novel feature of our model and one that has significant effects on the optimal policy, even before the vaccine becomes available.====On the theoretical side, we show that the model has a steady state. In the case where the reinfection rate is low (essentially the case in which the economy is dealing with an epidemic and not an endemic problem), we show that along a path in which a vaccine or a treatment never becomes available (Phase I)–although optimal policies take into account that the probability is positive–the epidemiological variables converge to the ==== as those of another economy that has access to a vaccine/treatment. This implies that the economic value of a vaccine decreases over time. To the extent that the private value of a vaccine moves with the social value, the model predicts that fewer resources will be allocated by the private sector to finding a vaccine as the epidemic progresses.====We study a quantitative version of the model. We consider a variety of scenarios to capture the uncertainty associated with the true value of epidemiological parameters, the effective availability of health care resources in the case of an epidemic, and the differential case fatality rates associated with situations in which hospital capacity is exceeded. Our findings about optimal policies (both employment and vaccination) are very sensitive to assumptions about the appropriate value of the relevant parameters (epidemiological as well as public health infrastructure).====We find that in the initial phase (Phase I) it is generally optimal to impose large restrictions on employment in the initial weeks of the epidemic. Depending on epidemiological parameters, the restrictions are slowly removed. In all cases employment is increased ==== the epidemic peaks. We also find that the lower the probability of a vaccine or the lower the speed at which the population can be vaccinated, the ====-restrictive are the employment policies. The reason for this is standard: If there is no tool to fight the virus there is no need to flatten the curve as the same number of individuals will perish. The only force that counterbalances this result is the assumption that the case fatality rate increases when hospital capacity is exceeded. In this case, flattening the curve can result in fewer deaths. Thus, the optimal policy has to balance these different factors that have opposing effects.====The details of how a vaccine interacts with other policies is novel and interesting as arrival of a vaccine (Phase II) ==== imply–depending on the state of the epidemic–that all restrictions on employment should be lifted. Moreover, availability of a vaccine may result in an ==== in the spread of the epidemic. This last somewhat counterintuitive result can be easily explained: Availability of a vaccine increases the rate at which the susceptible population shrinks and this reduces the future contagion rate. This implies that the cost of the epidemic in terms of future deaths and consumption decreases (less future contagion) and that, consequently, the marginal cost in terms of current output should decrease as well. This last step requires a liberalization (more contact among individuals) that, in turn, pushes up the contagion rate. Our results imply that for a developed country like the U.S., it is in general optimal to vaccinate at the highest possible rate (institutionally determined) when the vaccine first becomes available. The optimal vaccination policy rapidly converges to its long-run value (which is non-zero in the case of reinfections). Thus, the optimal vaccination policy has almost a bang-bang quality.====The model implies that for a large range of plausible scenarios the social value of a vaccine decreases rapidly as time goes by. In our quantitative model (that allows for the endemic nature of the virus) the dollar value of a vaccine decreases by about 60% after one year. The specific dollar value depends critically on the reinfection rate and the social value of life.====As mentioned before, we study a fairly large range of plausible scenarios. Even though we find our quantitative results useful we are fully aware that their ====. At this point there is ==== about many of the key parameters, both those corresponding to the economic model as well as those implicit in the epidemiological mode. Our findings suggest that the optimal policy is ==== to the specific parameterization. It is not clear to us what to conclude from this other than showing the importance of acquiring information (for example, adopting a large program of random testing====). We also view our results as providing policymakers with a framework of reference to study worst-case scenarios.====Our work falls within the large and growing macro literature that emphasizes the trade-off between managing the epidemic by controlling the spread of the infection and economic outcomes. There is a large (and growing) number of papers that use optimal control techniques to explore the management of an epidemic.==== This paper is closest to the recent work of Alvarez et al. (2020), Acemoglu et al. (2020) and Gonzalez-Eiras and Niepelt (2020). The major difference from the other planner models is that we take a different approach to modeling the effect of the availability of a vaccine, and this allows us to evaluate the consequences of different arrival times. This permits us to discuss how the optimal policy and the value of a vaccine depends on both the epidemiological variables as well as the implicit value of life. We also formalize the role of hospital capacity, explicitly allowing for reinfections due to loss of immunity, both of which are key determinants of optimal policy. Even though our formulation emphasizes a control approach, in Appendix D we sketch a simple model that allows for a behavioral component and captures individual response to the risks associated with infection (see, for e.g., Bisin and Gottardi, 2021 for a model with externalities).====Section 2 presents the model and Section 3 discusses some theoretical results. Section 4 presents our quantitative findings. Section 5 briefly discusses ongoing work on extensions, and section 6 offers some preliminary concluding comments.","Optimal management of an epidemic: Lockdown, vaccine and value of life",https://www.sciencedirect.com/science/article/pii/S0165188922000562,4 March 2022,2022,Research Article,79.0
"Cho Daeha,Kim Kwang Hwan","Department of Economics, University of Melbourne, Australia,School of Economics, Yonsei University, Republic of Korea","Received 14 September 2021, Revised 17 January 2022, Accepted 26 February 2022, Available online 28 February 2022, Version of Record 9 March 2022.",https://doi.org/10.1016/j.jedc.2022.104348,Cited by (0),"We measure the extent of inefficient fluctuations in the relative price of investment in the US using an estimated two-sector ====. In the presence of these fluctuations, inefficiencies in the economy are summarized by the variation in the following policy objects: the relative price gap, sectoral output gaps, and sectoral price and wage ====. The welfare-maximizing monetary authority cannot reduce the volatility of the relative price gaps due to nominal rigidity in both sectors. However, it can manage trade-offs among the other policy objects, both across and within sectors. Identifying the nature of shocks that drive the relative price of investment is important for central banks in deciding the optimal weights on sectoral price ==== under the composite inflation targeting rule. This is because the property of trade-offs depends on whether shocks alter the efficient relative price or not.","There is growing interest in the extent to which the relative price of investment plays a role in aggregate fluctuations.==== Some existing studies assume that fluctuations in the relative price of investment are efficient in the sense that the relative price is determined under perfect competition (and flexible prices) in both the consumption and investment sectors. In particular, these studies attribute all of the observed fluctuations in the relative price of investment to exogenous changes in investment-specific technology (IST) by imposing a strict restriction on the period-by-period relative price of investment to equal the inverse of the IST level (Fisher, 2006, Greenwood, Hercowitz, Krusell, 2000).====Recent studies, however, suggest empirical evidence that the fluctuations in the relative price of investment are inefficient. As for microeconomic evidence, Nakamura and Steinsson (2008) and Vermeulen et al. (2012) point to substantial price stickiness in several categories of investment goods. As for macroeconomic evidence, Ikeda (2015) and Moura (2018) find a high degree of stickiness in both the consumption and investment sectors in an estimated two-sector model. Moura (2018) furthermore finds that the sectoral price markup shocks drive the majority of variation in the relative price of investment. Nominal rigidities in the prices of consumption and investment goods and the sectoral price markup shocks drive a wedge between the IST and the observed relative price of investment. This wedge causes the economy to move away from its efficient frontier, calling for policy intervention. Given the empirical relevance of the distorted relative price of investment, the following questions arise: What are the inefficiencies implied by the variation in the relative price of investment? How should monetary policy respond in the face of its variation? Normative implications of the distorted relative price of investment have been largely under-explored in the literature.====To this end, we construct and estimate a two-sector New Keynesian model with consumption and investment-goods producing sectors. In our model, the relative price of investment fluctuates for the following reasons. First, we allow for limited inter-sectoral factor mobility. In the presence of reallocation frictions in production factors, in addition to the IST shock, economy-wide disturbances such as the total factor productivity (TFP), marginal efficiency of investment (MEI), preference, and monetary policy shocks also affect the efficient relative price of investment, which is the relative price that would prevail under perfect competition and flexible prices in both sectors.==== This is because limited inter-sectoral factor mobility leads to factor prices and thus nominal marginal costs to respond differently across sectors in response to economy-wide shocks. Second, we allow for heterogeneity in the degree of price and nominal wage stickiness across sectors. Because of these nominal rigidities, the observed relative price of the investment deviates from its efficient counterpart in response to the shocks that change the efficient relative price of investment. Third, we introduce sectoral markup shocks. Unlike the IST and economy-wide shocks, sectoral markup shocks create a wedge between the observed relative price of investment and its efficient counterpart without affecting the latter.====Using our estimated model, we uncover the inefficient component of the relative price movements by measuring the distance between the actual and efficient levels of the relative price. We define this distance as the ==== and find its variation sizable. Much of the variation in the relative price gap is driven by the IST, MEI, and sectoral price markup shocks. In addition to the relative price gap, there are two other sources of inefficiencies in our estimated model. One is the large variation in the ====, which measures the degree of inefficient output fluctuations in a particular sector. This sectoral output gap is defined as the difference between actual and efficient output, which is the output level that would be observed under perfect competition and flexible prices in that sector. The other is the variation in sectoral price and wage inflation, given a significant degree of sectoral price and wage stickiness in our estimated model. Hence, inefficiencies in our economy are summarized by the variation in the relative price gap, sectoral output gaps, and sectoral price and wage inflation.====To evaluate the extent to which the optimizing monetary authority stabilizes these policy objects, we compute the evolution of the counterfactual economy under the Ramsey optimal monetary policy, which is the interest rate path that maximizes the representative household utility. We find that the optimal policy cannot reduce the relative price gap due to nominal rigidity in both sectors. Intuitively, it is difficult for the welfare-maximizing monetary authority to correct the four nominal distortions that affect the relative price gaps, namely sticky prices and wages in each sector, with only one policy instrument. However, the optimal policy can manage trade-offs among the other remaining variables. Compared to the estimated policy, the variation of the output gap and price inflation in the investment sector is reduced under the optimal policy, whereas their variation in the consumption sector is increased. This indicates the presence of a trade-off across sectors. Moreover, within each sector, wage inflation is much more stabilized than the output gap and price inflation under the optimal policy. This implies a trade-off among policy objects within a sector.====The type of trade-offs faced by the monetary authority depends on whether shocks alter the efficient relative price or not. Conditional on shocks that affect the efficient relative price, such as the IST and MEI shocks, only a trade-off occurs across sectors. Specifically, the Ramsey planner reduces the volatility of all objects in the investment sector at the cost of increased volatility of some objects in the consumption sector. Circumstances are different for shocks that change the relative price gap without affecting the efficient relative price. For example, the investment-sector price markup shock imposes a trade-off among policy objects within the investment sector as well as across sectors. In particular, the Ramsey planner wishes to reduce the variation in the output gap and wage inflation in the investment sector. However, she can do so when she allows for a larger variation of price inflation in the investment sector and a larger variation of the output gap and price and wage inflation in the consumption sector.====The dependence of trade-offs on shocks illustrates the importance of identifying the nature of shocks that drive the relative price of investment in deciding the optimal weight on sectoral inflation under the composite inflation targeting policy. If central banks believe that the relative price of investment is only driven by sectoral productivity shocks and ignore the empirically relevant sectoral price markup shocks, the optimal weight on investment price inflation would be overstated. This is because, under sectoral price markup shocks, there is a trade-off within the investment sector, which is absent under sectoral productivity shock. Accordingly, assigning the weight on investment price inflation that would be optimal only in the presence of sectoral productivity shocks leads to highly volatile output gaps and wage inflation in the investment sector and thus large welfare losses.====Previous papers have also investigated the optimal conduct of monetary policy in a multi-sector or multi-country model. Important works include Aoki (2001); Barsky et al. (2016); Benigno (2004); Erceg and Levin (2006), and Basu and De Leo (2019). These papers emphasize the presence of a trade-off when relative prices are distorted in a setting that abstracts from several essential elements that determine the relative price gaps. First, they only consider a subset of shocks that affect the efficient relative price. Second, they exclude one or more of the following factors: sticky prices, sticky wages, and sectoral markup shocks. In contrast, we provide an empirical measure of the extent to which relative prices are distorted using an estimated model that embeds many elements that these papers do not consider.====This paper is also related to the literature on the estimation of two-sector DSGE models with consumption- and investment-goods producing sectors (Ikeda, 2015, Katayama, Kim, 2018, Moura, 2018). Moura (2018) focuses on positive analysis, exploring the empirical relevance of sticky relative price of investment and its implications on the sources of economic fluctuations. In contrast, we focus on the normative implication of the distorted relative price of investment. Ikeda (2015) derives the optimal steady state consumption sector inflation rate. However, we keep the steady state inflation fixed and characterize the allocations under the Ramsey optimal monetary policy. Katayama and Kim (2018a) study sectoral and aggregate comovement issues using a model with flexible prices and wages.====The rest of our paper is organized as follows. Section 2 presents our two-sector New Keynesian model. In Section 3, we define the relative price gap and investigate its determinants analytically. Section 4 describes the data and the estimation procedure. In Section 5, we compare the dynamics of policy objects under the estimated and optimal monetary policy. Section 6 studies the allocations conditional on each shock under the estimated policy and then provide intuitions on how the Ramsey planner changes these allocations. Section 7 discusses the practical implications of our results for targeting sectoral inflation. Finally, the conclusion is laid out in Section 8.",Inefficient relative price fluctuations,https://www.sciencedirect.com/science/article/pii/S0165188922000537,28 February 2022,2022,Research Article,80.0
"Feng Xu,Lütkebohmert Eva,Xiao Yajun","College of Management & Economics, Tianjin University, 92 Weijin Road, Tianjin, 300072, China,Department of Quantitative Finance, Institute for Economic Research, University of Freiburg, Rempartstr. 16, Freiburg i. Br. 79098, Germany,International Business School Suzhou (IBSS), Xi'an Jiaotong-Liverpool University, No.8 Chongwen Road, Suzhou, 215123, China","Received 23 July 2021, Revised 23 February 2022, Accepted 25 February 2022, Available online 26 February 2022, Version of Record 20 March 2022.",https://doi.org/10.1016/j.jedc.2022.104346,Cited by (3),Shadow financing through off-balance sheet ,"Wealth management products (WMPs) in China have become the second most important funding source next to deposit funding since 2009. They represent short-term financing instruments where the funding raised through WMPs is either actively or passively managed by the issuing retail banks, while non-bank financial institutions are used as ‘channels’ so that WMPs can be recorded off-balance sheet. WMPs typically have maturities of less than one year and are invested in long-term illiquid assets. Thus, similar to shadow money in Moreira and Savov (2017)====, WMPs are run-prone which potentially exposes a banking system to financial instability (see Schroth et al. (2014), Covitz et al. (2013), and Gorton and Metrick (2012)). How and to what extent WMP funding affects the stability of the Chinese retail banking system has not been examined yet.====In this paper, we close this gap by quantifying the economic magnitude of the effect of WMPs on banking stability when retail banks compete for deposit and WMP funding. Therefore, we extend the deposit competition model of Egan et al. (2017) to build a quantitative model of the Chinese banking system. Retail banks (or ==== for short) supply consumers with traditional deposits and with WMPs. The funds raised through WMPs are channelled to non-bank financial institutions (==== for short) such as trust companies, securities companies, and insurance companies. Demand for deposits, which are insured by the government, depends on the interest rates offered by the banks and on consumer preferences. The off-balance sheet WMPs are contractually uninsured. We assume that the demand for WMPs depends not only on interest rates and consumer preferences but also on the banks’ default risk.==== Banks obtain the full profit on the invested deposits, while they split the profits on the co-invested WMPs with the paired shadow bank according to the intermediary contract. As in Leland and Toft (1996), banks maximize equity value by setting interest rates on deposits and WMPs and by making default decisions. This gives rise to three nonlinear equations describing the optimal behavior. As demand for WMPs is sensitive to banks’ default probabilities, a feedback loop between demand for WMPs and bank distress arises. More specifically, an increase in a bank’s default probability reduces the demand for WMPs and hence lowers the bank’s profitability which, in turn, increases a bank’s default probability. The feedback mechanism characterized by the nonlinear optimality conditions gives rise to multiple equilibria, some of which are associated with large welfare losses and high default risk.====Our extension of the model in Egan et al. (2017) accounts for several stylized facts of the Chinese retail banking sector and the WMP market. First, we model the assets underlying deposits and the assets underlying WMPs separately as they typically have different risk-return profiles (see Dang et al. (2015)). Second, banks contract with shadow banks, co-manage the assets underlying WMPs, and split the rent earned on the assets. When banks have a higher bargaining power and retain a larger fraction of the rent, this provides further incentives for banks to raise WMP funding when the assets underlying WMPs perform well. This, however, increases banks’ default risk if the realized returns are low and the rents become negative. Third, banks need to hold capital reserves against deposits but not against WMPs. Such an asymmetric capital requirement allows us to investigate the impact of regulatory arbitrage opportunities on banking stability. Finally, we can incorporate the cost of rolling over WMPs which can quickly exhaust equity and increase banks’ default risk. These features amplify the feedback mechanism and hence are important for understanding the effect of WMPs on the stability of the banking system.====We estimate and calibrate our model to the Chinese banking system using deposit and WMP data over the period 2008–2017. We estimate the demand elasticity parameters for WMPs and deposits using a difference-in-difference specification as in Egan et al. (2017) and Berry et al. (1995). The estimated market share of WMPs significantly declines with bank default probability, whereas that of deposits is left unaffected. This confirms the assumption that WMP investors are sensitive to bank distress. We then calibrate the supply-side parameters using two first order conditions with respect to deposit and WMP rates as well as the default condition. Our endogenous approach can determine the expected returns on the unobserved off-balance sheet assets underlying WMPs, which are of great interest and have not been reported in the literature before. The calibrated annualized expected returns on the assets underlying deposits and WMPs range between 3.5–6% and 6.5–11%, respectively, across all of the banks in the sample period. The return differential shows that deposits and WMPs indeed finance different types of assets.====We compare the observed banking system with the alternative (unrealized) equilibria of interest rates, default probabilities, and market shares under the same bank fundamental values as those calibrated to the data for Q3/2015. We rank these equilibria according to the change in welfare relative to the welfare level of the observed state, and categorize them as good equilibria, which are associated with low probabilities of default and welfare gains, and bad equilibria, which are associated with high probabilities of default and welfare losses.====Our results for the baseline model reveal several interesting facts. First, we find that the observed banking system is comparable to the best alternative equilibrium, and that equilibria are highly negatively skewed. Bad equilibria deviate substantially from the observed banking system, since they have a much higher average default probability and significantly higher welfare losses. Therefore, we conclude that the seemingly stable Chinese banking system is in fact highly fragile due to the feedback introduced by the run-prone WMPs. Second, bad equilibria are asymmetric with some banks being in particular distress, while their competitors are not. The distressed banks raise WMP rates and deposit rates and thereby effectively push their competitors out of the insured deposit market. This creates equilibria that are substantially worse than the observed one. Third, the equilibria for the non-systemically important banks are associated with welfare losses with the same order of magnitude as those for the systemically important banks. This challenges the view that the non-big 4 banks in China are less important to the stability of the banking system.==== Fourth, among many bad equilibria, we find that the state-owned banks have an advantage on the deposit market, while the joint stock banks are more successful in raising shadow funding due to their aggressive strategy. Lastly, when extending our baseline model to account for rollover risk in the WMP market, we observe devastating runs on WMPs which produce bad equilibria with even higher welfare losses and average default probabilities than those observed in the model without rollover costs.====In addition, we study the risk transmission channel when banks have to internalize losses from the asset market underlying deposits and/or WMPs, and its impact on banking stability. We find that a negative shock to the assets underlying WMPs has a stronger impact on the distribution of the equilibria than a negative shock to the assets underlying deposits. In particular, we observe that the distribution of the default probability becomes more positively skewed after such a shock, whereas the welfare distribution becomes more negatively skewed. This indicates that the shadow funding instruments such as WMPs render the Chinese banking system highly vulnerable.====In an Online Appendix, we investigate the impact of banks’ bargaining power on the equilibria and observe that the fragility of the banking system increases when retail banks have a higher participation in creating shadow money. Moreover, we investigate the impact of interest rate ceilings applied to deposits on financial fragility and find that the rate cap can remove the extremely bad equilibria but some equilibria with substantial welfare losses remain. The intuition is that, although banks’ deposit rates are capped, some banks with high default probability aggressively increase their WMP rates, which results in high welfare losses. Finally, we study whether imposing capital requirements on WMPs can effectively stabilize the financial system, and we find that such a policy has only a moderate effect. Capital requirements need to be as high as 30% to produce a stable banking system.====Our study is closely related to the literature on banking competition and financial stability. We employ the banking competition framework of Egan et al. (2017) but extend it to account for different assets underlying deposits and WMPs, linear risk sharing between retail and shadow banks, and asymmetric capital requirements. Shu (2017) considers a model in which a representative shadow bank and a representative retail bank compete for deposit funding. However, his model does not consider shadow funding. To the best of our knowledge, our study is the first to model how a pool of banks simultaneously compete for deposit and shadow (WMP) funding, and to examine how the retail banks’ participation in intermediating shadow credit affects financial stability.====Furthermore, our paper contributes to the large literature on regulatory arbitrage and financial fragility associated with shadow funding. For instance, Gorton et al. (2010), Acharya et al. (2013b), and Harris et al. (2014) point out that the shadow banking sector played a key role in the financial crisis of 2007–2009 and document that regulatory arbitrage is a main driver for the prosperity of shadow banking. In our baseline model, the absence of capital requirements for WMPs provides incentives for banks to maintain a high demand for WMPs across all equilibria. We further show that WMP investors expect less compensation for risk when their belief in the implicit guarantee becomes stronger. Acharya et al. (2013a), Allen et al. (2017), Hachem and Song (2016), Acharya et al. (2019), Chen et al. (2018), Chen et al. (2020), and Allen et al. (2019) find that monetary policy interventions and regulatory arbitrage significantly contributed to the growth of the shadow banking sector in China and India. They further show that banks’ stock price volatilities and interbank lending costs increase with the rates offered on the shadow instruments. Our paper complements the existing results by analysing the stability of the Chinese banking system when retails banks intermediate shadow credit.====The remainder of the paper is structured as follows. In Section 2, we provide a brief overview of WMPs in China. In Section 3, we present the model setup and the equilibrium solution. Section 4 describes the data. The empirical results are discussed in Section 5, while Section 6 concludes.","Wealth management products, banking competition, and stability: Evidence from China",https://www.sciencedirect.com/science/article/pii/S0165188922000513,26 February 2022,2022,Research Article,81.0
Karamysheva Madina,"National Research University Higher School of Economics, Moscow, Pokrovsky Boulevard 11, Russian Federation","Received 5 July 2021, Revised 19 January 2022, Accepted 25 February 2022, Available online 26 February 2022, Version of Record 19 March 2022.",https://doi.org/10.1016/j.jedc.2022.104347,Cited by (0),", uncertainty, or financial markets can explain the heterogeneous effects of fiscal adjustment plans. I find that the financial market and macro uncertainty channels are the most important ones.","The global pandemic has prompted a worldwide fiscal response. Many advanced economies have announced additional fiscal stimulus packages. The IMF’s Fiscal Monitor Report==== estimates that the announced fiscal measures account for 12 percent of global GDP. Countries with limited fiscal space would need to reduce their fiscal deficit. With an already high level of public debt and a growing deficit, the composition of fiscal adjustment will become central to forbear the recovery disruption.====According to various studies, fiscal consolidation based mainly on tax hikes has a more recessionary impact on economic growth than that based on expenditure cuts.==== However, much less is known about the reasons for such a difference in output effects. Studying the channels of the heterogeneous output effects of fiscal adjustment plans is important for both academics and policy makers since it helps to tighten the link between theory and data. In this paper, I analyze possible explanations for the heterogeneous effects of fiscal adjustment plans on economic activity.====The major contribution of the paper is to decompose the channels through which fiscal consolidations affect economic activity in order to understand the differences between tax versus spending consolidations.====To achieve this goal, I first reclassify and extend a U.S. fiscal plan database using quarterly data for the 1978–2014 period. Fiscal consolidation is usually implemented through a set of multi-year actions. These actions generate interactions between spending and revenue components, as well as between expected and unexpected components. In this setup, the expected components refer to parts of the plan announced in previous years and implemented at time ==== and the parts announced at time ==== to be implemented in the future. Unexpected components are actions announced upon implementation at time ====.====Next, I incorporate fiscal plans into a vector autoregression model to track the dynamics and interdependencies between the variables of interest, to capture expectational effects, and, most importantly, to explain the heterogeneity of the output effects of fiscal adjustments.====I consider four impact mechanisms through which a fiscal consolidation can affect economic activity.==== The first is reducing or increasing the amount of distortion in the economy.==== The second is inducing a response of monetary policy that affects output. The third is changing the amount of uncertainty, which affects output. The fourth is inducing a reaction in the financial market.====The first mechanism captures the direct effect of fiscal adjustment plans on output, while the others capture indirect effects. The direct effect can be measured simply by projecting the output growth on the current and past values of the exogenous fiscal adjustments.==== To measure the indirect effects, one needs to consider the other endogenous variables, such as monetary policy, uncertainty, and financial market variables.====There are two key reasons for the lack of empirical evidence explaining the difference between the output effects of fiscal adjustment plans: the data and the econometric models. Ramey (2016) points out the importance of dynamics, general equilibrium effects, and expectations for macroeconomic questions. To explain the heterogeneous output effects of fiscal adjustment plans, one needs to take into account expectations, since the effect of anticipated and unanticipated components of the fiscal plan can be different. Interdependencies between the variables of interest are crucial for defining the transmission channel. To derive the dynamic effect of fiscal consolidation on the variable of interest, the literature has thus far focused on a truncated moving average (see, for example, Romer and Romer (2010), Guajardo et al. (2014), Alesina et al. (2015)) and on a local projection originally developed by Jordà (2005).==== Neither method captures the dynamics between the variables. The vector autoregression model with exogenous fiscal plans allows us to take into account the interdependencies between the variables and to close the channels to measure how much of the difference in output effects is due to a particular channel. Although Plagborg-Møller and Wolf (2021) prove the equivalence of LP and VAR, they focus on impulse response estimates, so it does not follow immediately from their analysis that the equivalence extends to counterfactual scenario analysis. To the best of my knowledge, there is no paper showing equivalence with the ultimate goal of a counterfactual experiment. Thus, thinking in terms of a VAR makes more sense since this is an exercise that depends on dynamically modeling a full system of endogenous variables.====The main empirical findings of the paper are as follows. The estimation of the baseline specification confirms that there is a heterogeneous response of economic activity, depending on the policy. In particular, GDP and employment decrease after using a tax-based fiscal adjustment plan (TB plan hereafter) and increase with an expenditure-based plan (EB plan hereafter). There is substantial heterogeneity in the responses of monetary policy, uncertainty, and financial market proxies.====With a TB plan, uncertainty goes up, and financial markets react negatively, with a large drop in the S&P 500 index and an increase in corporate bond spread. Under an EB plan, reaction of those variables is the opposite. Moreover, there is a loose monetary policy in the TB case and a tight policy in the EB case. Of these competing channels, only financial markets and uncertainty can explain the heterogeneity in the responses of macroeconomic aggregates to TB and EB fiscal adjustment plans.====To see how much of the effect is due to a particular channel, I use the methodology of the counterfactual experiment that isolates different indirect effects. Without an indirect monetary policy channel, the economy’s response is similar to that in the baseline specification. In contrast, without the indirect financial market channel, the recessionary effect of the TB policy is much smaller, while without uncertainty channels, the expansionary effect of the EB policy is smaller. These results suggest that both the financial markets and uncertainty are essential in explaining the heterogeneous effects of fiscal adjustment plans. More precisely, financial markets can explain about 100%, and macro uncertainty can explain about 36%, of the total difference in the response of GDP, while they explain 78% and 31%, respectively, of the total difference in the response of employment. The most persuasive evidence comes from the stock market, and this is in line with Gomes et al. (2012), Croce et al. (2012a), and Belo and Yu (2013). These papers investigate the relations between fiscal policy and financial markets.====This paper fits within a growing strand of literature that examines the transmission mechanism of policy shocks into economic activity (e.g. Bekaert et al. (2013), Wong (2015), Aastveit et al. (2017)). Most relevant is work by Bachmann and Sims (2012), Jalil (2016), Alloza (2017), Mumtaz and Theodoridis (2020), and Beetsma et al. (2021)), who investigate propagation channels of fiscal policy shocks on the real economy. Relative to their work, my contribution deviates by considering both the expenditure and tax components of fiscal policy and by considering several propagation channels simultaneously: monetary policy, uncertainty, and financial markets are all endogenous.==== As argued above, the use of a counterfactual with several channels included in the system is important because it allows me to disentangle the role of each channel.====The closest papers in terms of identification are those identifying fiscal shocks and fiscal adjustment plans using the narrative approach (e.g. Romer and Romer (2009), Devries et al. (2011), Alesina et al. (2015)).====The rest of the paper is organized as follows: Section 2 describes the methodology and the data. Sections 3 and 4 report baseline and counterfactual estimation and results. Section 5 provides some intuition for channels, and Section 6 describes the robustness checks. Section 7 concludes.",How do fiscal adjustments work? An empirical investigation,https://www.sciencedirect.com/science/article/pii/S0165188922000525,26 February 2022,2022,Research Article,82.0
"Müller Tobias,Christoffel Kai,Mazelis Falk,Montes-Galdón Carlos","Boston College, Department of Economics, United States,European Central Bank, Directorate General Economics, Germany,European Central Bank, Directorate General Monetary Policy, Germany","Received 17 October 2020, Revised 16 February 2022, Accepted 22 February 2022, Available online 26 February 2022, Version of Record 13 March 2022.",https://doi.org/10.1016/j.jedc.2022.104336,Cited by (0),Forward guidance operates via the expectations formation process of the agents in the economy. In standard quantitative ,"With nominal interest rates approaching the lower bound, standard monetary policy tools become less effective and forward guidance is seen as a pivotal instrument for central banks to provide accommodation. Acknowledging the significant role that expectations of future monetary policy developments play for the economy, central banks aim to engage transparently with the public to promote a better understanding of the central bank’s reaction function and future actions. For instance, Coenen et al. (2017) underline the importance of transparent communication as a non-standard monetary policy tool and highlight the central bank’s capability to influence expectations of economic agents via precise and committed signals.====New Keynesian DSGE models are particularly well equipped to study forward guidance policies as they incorporate forward-looking agents. Forward-looking agents rationalise and anticipate statements regarding expected future policy decisions contained in monetary policy announcements, and therefore adjust their behaviour based on those expectations. By changing their consumption and investment decisions today, a credibly announced future expansionary monetary policy brings about notable reactions of macroeconomic variables in the present. However, standard DSGE models predict implausibly large reactions to forward guidance. Del Negro et al. (2012) refer to this phenomenon as the ====. Moreover, when the length of the forward guidance horizon is extended further into the future, the responses result in an explosive behaviour in standard models with forward-looking agents.====Recent research has focused on modifying existing models by introducing micro-founded or reduced-form mechanisms to mitigate the forward guidance puzzle. In contrast, our approach to moderate the forward guidance puzzle relies on an empirical method without interfering with the structural equations of the model. For that purpose we address the core channel of the forward guidance mechanism in DSGE models – the formation of the model-implied expectations. Drawing a quantitatively reasonable conclusion is especially crucial in models used in policy institutions. We therefore choose the New Area-Wide Model (NAWM), an estimated micro-founded open-economy model of the euro area and one of the main DSGE models of the European Central Bank (ECB) (see Christoffel et al., 2008).====Although researchers have developed a better understanding of the pass-through of forward guidance policies in standard monetary models, little attention has been given to the question of whether these models adequately reflect the agents’ expectations. We argue that the model’s expectations formation process is crucial for the way in which announced future monetary shocks impact the economy. Aiming for a better fit between the expected variables in the model and actual observed forecast data, we provide the model with additional information on how market participants form their expectations and assess the macroeconomic outlook. We use information from survey data on real GDP growth and on inflation as well as the term structure of interest rates. The estimation of the model parameters based on measured macroeconomic forecasts gives us model-implied expectations which are more in line with actual empirical data. Disciplining the model expectations in this way, yields more sticky, less volatile smoothed variables which is reflected in the values of the estimated parameters. As a result, the responses of the variables to a forward guidance news shock are reduced to quantitatively reasonable amounts. Furthermore, the explosive behaviour of the model for extended forward guidance horizons disappears.====The concept to mitigate the effects of forward guidance with forecast data on key macroeconomic variables in the model takes up on the idea of Campbell et al. (2016). Campbell et al. already give a hint on how to use forecast data in an estimated general equilibrium model to make it more suitable for studying forward guidance. However, in their work, they trace the muted power of forward guidance back to three modifications of their benchmark model: a discount factor in the consumption Euler equation, Jaimovich–Rebelo (JR) preferences and an enriched data set in the estimation of the model, including expected federal fund rates and survey expectations on inflation. Our framework demonstrates that the excessive response of the model to forward guidance can be adjusted to more plausible levels by solely focusing on the last of the three adjustments. For that purpose, we trim the model-implied expectations towards the information given by macroeconomic forecasts on expected rates of output growth, inflation and the interest rate.====It is important to stress that our approach does not rely on modifications to key structural equations in the New Keynesian model. However, besides augmenting the data set for the estimation of the model, we introduce a four quarters ahead forward guidance news shock in the Taylor rule. While the inclusion of an additional shock is not strictly necessary to enhance the expectations formation process, this formulation of the Taylor rule helps to reconcile the model expectations and the corresponding observed yield curve data. In addition, the forward guidance shock allows us to distinguish between the effects of standard monetary policy and information about future monetary policy intentions. We otherwise keep the main structure of the model unchanged as described in Christoffel et al. (2008), which we henceforth refer to as the baseline model. Following Del Negro and Eusepi (2011), we show as a further result that survey expectations and the term structure of interest rates also contain information about the economy beyond the time series used in the baseline model, which improves the general forecast performance of the model. Finally, our results indicate that there is no empirical evidence for a discount parameter in the consumption Euler equation. To underline the robustness of our results, we conduct a series of estimations using a wide range of alternative forecasting time series and specifications in the measurement equations as well as further sensitivity analyses.====The remainder of the paper is structured as follows. In Section 2, we demonstrate the forward guidance puzzle in the NAWM and outline our approach to target the expectations formation process of the model. Section 3 provides information about the model while focusing on the adjustments to append the forecast observables in the estimation, the forward guidance news shock in the Taylor rule and the discount parameter in the Euler equation. Section 4 presents the employed Bayesian estimation techniques, summarises the estimation results and describes the model selection process for our preferred specification. Also, we present the estimation results for the discount factor in the consumption Euler condition. Section 5 illustrates the disciplined expectations formation process and the advancements in the study of forward guidance. In Section 6, we evaluate the model performance of our preferred specification and assess the forecast accuracy using cross-validation on empirical results. We end with robustness checks and sensitivity analyses. Section 7 concludes.",Disciplining expectations and the forward guidance puzzle,https://www.sciencedirect.com/science/article/pii/S0165188922000410,26 February 2022,2022,Research Article,83.0
"Hu Yuan,Lindquist W. Brent,Rachev Svetlozar T.,Shirvani Abootaleb,Fabozzi Frank J.","Department of Mathematics & Statistics, Texas Tech University, Lubbock, TX 79409-1042, USA,Department of Applied and Computational Mathematics and Statistics, University of Notre Dame, Notre Dame, IN, 46556, USA,Finance Group, EDHEC Business School, 393/400 Promenade des Anglais-BP3116, CEDEX 3, 06202 Nice, France","Received 15 June 2021, Revised 15 February 2022, Accepted 24 February 2022, Available online 26 February 2022, Version of Record 8 March 2022.",https://doi.org/10.1016/j.jedc.2022.104345,Cited by (0)," in both the natural and risk-neutral world. We construct implied surfaces for the parameters determining the GJR tree. Motivated by Merton’s pricing tree incorporating transaction costs, we extend the GJR pricing model to include a hedging cost. We demonstrate ways to fit the GJR pricing model to a market driver that influences the price dynamics of the underlying asset. We supplement our findings with numerical examples.","Pricing trees determined by a Markov chain have been studied in a number of academic papers.==== These works either begin directly in the risk-neutral world or result in a formulation that is not arbitrage-free.==== When a Markov chain pricing tree is constructed directly in the risk-neutral world, it is not clear what discrete pricing model in the natural world would evolve to such a risk-neutral Markov chain pricing model in accordance with dynamic asset pricing theory.==== It is essential that an option pricing model be defined first in the natural world. If the option pricing model is placed directly in the risk-neutral world, and calibrated with market option data, no option market dislocation or option mispricing can be revealed. Option pricing models which do not start with modeling the underlying assets in the natural world and then, via risk-neutral valuation based on Black-Scholes-Merton dynamic pricing theory, pass to the risk-neutral world are meaningless, if not dangerous, in practical applications. Such pricing models are generally used to predict what the option traders ==== the correct option prices are and not what option prices are ====. It is imperative that opinion prices be aligned with reliable spot market models.====To illustrate this claim, suppose that a trader would like to determine if potential mispricing in the option market exists due to a market bubble that the trader suspects will burst.==== The trader’s option pricing model, when calibrated to option market data, should recover, uniquely, the spot price dynamics (Ross, 2015) of the underlying asset (Kim et al. (2016), Hu et al. (2020a)). If that recovered spot price process does not conform with market data on the asset spot price, this could be a trading signal that there is potential option mispricing.====To demonstrate the issue quantitatively, Hu et al. (2020b) developed a binomial pricing model in the natural world which allows for variable trading times. Via risk-neutral valuation, their model is then passed to the risk-neutral world. As a result the model retains the quantitative link between the risk-neutral and natural world probabilities, a link missing in the Cox et al. (1979) and Jarrow and Rudd, 2012, Chapters V and VI) models which are placed directly in the risk-neutral world. Significantly, the risk-neutral models of both Cox et al. and Jarrow and Rudd are special cases of the Hu et al. model. Consideration of these special cases in the Hu et al. formulation thus provides the connection between the risk-neutral and natural world probabilities missing from the original Cox et al. and Jarrow and Rudd models. Using 26 1/2 years of daily SPDR S&P 500 ETF (SPY) data, and a moving window of length one year (==== trading days), the authors estimated (Figure 2 in Hu et al.): the natural-world probability ==== of upward daily log-return using the proportion estimate====the upper and lower limits of the 95% confidence interval for this proportion estimate; and the natural-world probabilities ==== and ==== associated, respectively, with the Cox et al. and Jarrow and Rudd models. Over the time period Q1-1994 through Q2-2020, ==== generally underestimated ==== by ==== while ==== generally overestimated ==== by 5% to 10%. While ==== remained within the 95% confidence interval of ==== over 25 1/2 years, ==== exceeded the upper bound at times. Consistent, daily overestimations of 5% to 10% over long periods (e.g. Q1-2000 through Q2-2003, Q1-2008 through Q2-2009) were observed between ==== and ====, illustrating the particular danger of relying on a option pricing model that is developed in the risk-neutral world, unconnected with the natural world.====In this work, we extend the progress of Hu et al. (2020b) by starting with a higher-order-moment-capturing pricing tree formulation in the natural world and transforming it into the pricing tree in the risk-neutral world while preserving market completeness. Our paper is close in spirit to Kijima and Yoshida (1993), where an option pricing model with Markov chain stochastic volatility was introduced and the continuous-time limiting price process determined as a subordinated geometric Brownian motion (GBM).==== While the discrete- and continuous-time market models are incomplete in the paper by Kijima and Yoshida, in our paper we deal with complete market models, both in the discrete- and continuous-time settings. Specifically we extend the Jarrow and Rudd (JR) binomial pricing model (Jarrow and Rudd, 2012, Hull (2012, p. 442), Kim, Stoyanov, Rachev, Fabozzi, 2016, Kim, Stoyanov, Rachev, Fabozzi, 2019, Hu, Shirvani, Lindquist, Fabozzi, Rachev, 2020b, Hu, Shirvani, Stoyanov, Kim, Fabozzi, Rachev, 2020a) with an additional parameter determining the skewness and excess kurtosis of the underlying asset return distribution.==== We refer to our extended model as the ==== (GJR) ====.====In this GJR pricing model, the embedded Markov chain driving the discrete underlying price process is a skew random walk which, in the limit, becomes a skew Brownian motion (SBM) after the necessary scalar normalization.==== In discrete time, the distributional mapping between the GJR price dynamics of the underlying asset and its risk-neutral dynamics is one-to-one.==== The GJR pricing model allows us to study option pricing in a Markov chain market model with transaction costs. Our market model has a relatively simple parametric form and is easily calibrated to option data, as illustrated by numerical examples.====Our paper proceeds as follows. In Section 2, based upon a discrete skew random walk ====, we introduce a bivariate process ====, each component of which has piecewise linear trajectories. Paralleling the use of the Donsker-Prokhorov invariance principle==== in the Cox-Ross-Rubinstein (Cox et al., 1979) and JR pricing trees, we utilize the Cherny-Shiryaev-Yor invariance principle (CSYIP) (Cherny et al., 2003) to relate the continuous-time convergence of this bivariate process to SBM. We proceed by first developing the discrete time GJR pricing tree using only the process ====. In addition to the mean return ==== and standard deviation ====, the GJR tree includes an additional parameter ==== which governs the skewness and kurtosis of the price process in the natural world. As long as the GJR tree is based upon a finite time interval ====, regardless of how small, the pricing process is that of geometric SBM, retaining the skew properties of the model.==== Using daily closing price data for the SPDR S&P 500 ETF Trust fund (SPY), we demonstrate how to estimate the governing parameters ====, ==== and ====.====In Section 3, we develop option pricing for the single-component GJR pricing model and explicitly determine the risk-neutral probabilities, which depend on the moment-governing parameters ====, ==== and ==== as well as the risk-free rate ====. Using SPY price and call option data, we estimate the implied ====, ==== and ==== surfaces.====Motivated by Mertons binomial tree model with transaction costs (Merton, 1990, Chapter 14), in Section 4 we extend the single-component GJR pricing tree model to include a hedging transaction cost term. This introduces a two parameter transaction cost function ====. Again using the SPY option data, we estimate option transaction costs, the implied transaction cost surfaces ==== and ====, as well as the impact transaction costs have on the implied ====, ==== and ==== surfaces.====In Section 5, we explore possibilities for fitting the skew random walk ==== in the single-component GJR model to a market driver that affects the price dynamics of the underlying asset. Using stock of the Microsoft Corporation (MSFT) as the asset, we illustrate both an endogenous and exogenous approach. In the endogenous approach we consider the daily sign changes of the asset itself as the source of the skew random walk. To illustrate the exogenous approach, we assume the asset returns are determined by Fama-French five-factor loading values (Fama and French, 2015).====In Section 6, we extend the GJR pricing tree to include both components ==== and ==== of the bivariate process. We derive option pricing for this full model, explicitly compute the risk-neutral probability, and estimate the implied volatility surface. Conclusions are presented in Section 7.",Market complete option valuation using a Jarrow-Rudd pricing tree with skewness and kurtosis,https://www.sciencedirect.com/science/article/pii/S0165188922000501,26 February 2022,2022,Research Article,84.0
Woo Jinhee,"Department of Economics, Soongsil University, 369 Sangdo-ro, Dongjak-gu, Seoul 06978, Republic of Korea","Received 5 November 2021, Revised 20 February 2022, Accepted 21 February 2022, Available online 24 February 2022, Version of Record 7 March 2022.",https://doi.org/10.1016/j.jedc.2022.104335,Cited by (1)," of the observed variations in the entrants’ productivity. Notably, several testable implications of the model economy are consistent with the properties of the U.S. data.","Lee and Mukoyama (2015a) and Lee and Mukoyama (2018) highlight the asymmetry between the cyclical plant entry and exit behavior in the U.S. manufacturing sector from empirical and theoretical perspective. This study is mainly a follow-up of Lee and Mukoyama (2015a) and Lee and Mukoyama (2018). Specifically, this study provides new empirical facts on the possibility that entrants overestimate variations in expected value of entry along the business cycles. Subsequently, based on the motivating observations, we construct a quantitative industry equilibrium model where entrant plants have inferior information compared to incumbent plants.====Lee and Mukoyama (2015a) has shown that, in the U.S. manufacturing sector, the entry rates are strongly procyclical while the exit rates are acyclical, based on the Annual Survey of Manufacturers. Meanwhile, the productivity and employment size of entrants varies significantly along the business cycle (idiosyncratic productivity and employment size of entrants improves during recessions). Using Business Dynamics Statistics (BDS), this study confirms that, the establishment entry rate is procyclical, and the exit rate acyclical, in the U.S. manufacturing sector. This study also provides the following new facts. First, in the U.S. manufacturing sector, the establishment exit rate positively lags business cycle indicator (Tian, 2018 documented this fact at the firm level in the U.S. non-farm private sector). Second, whereas the “job destruction rate from exiting plants” is acyclical in the aggregate manufacturing level, it is significantly countercyclical in the two-digit manufacturing level. Third, in the U.S. manufacturing sector, plants that entered the market during boom periods are more likely to exit the market within three years after entry than cohorts that entered the market during recession.====Lee and Mukoyama (2018)’s model economy, which is a business cycle extension of Hopenhayn (1993) model of industry dynamics with cyclial entry cost, can account for the observed asymmetric cyclicality of entry and exit rates and cyclical patterns of size and productivity of entrants documented by Lee and Mukoyama (2015a). Our industry equilibrium model where entrant plants face information friction generates the entry-exit margin asymmetry jointly with aforementioned three new empirical facts documented by this study.====Given that pure sectoral shock might be immune to general equilibrium forces, our finding that employment size weighted exit rate of the two-digit manufacturing sector level is countercyclical, differently from the aggregate manufacturing sector implies general equilibrium forces can be a source of acyclical exit behavior of plants at the aggregate level. Potential entrants’ incentive to enter the market and incumbents’ incentive to stay in the market are determined by variations in expected flow of profits reflecting general equilibrium forces associated with business cycles. Consequently, the observation that general equilibrium forces only mute exit margins seems puzzling.====Our empirical finding that cohort of plants that entered during booms are more likely to exit the market quickly relative to plants that entered the market during recessions implies that potential entrants might overestimate their expected profit from entry during booms and underestimate profit from entry during recessions. A possible explanation for the observation that general equilibrium forces only mute exit margins is that potential entrants underestimate variations in factor price associated with business cycles. Thus, the study focuses on the imperfect information of potential entrants as a possible explanation for the observation.====This study primarily constructs a quantitative industry equilibrium model that explains the asymmetric cyclical plant entry and exit behavior along business cycles at the aggregate level, given motivating observations. Assume that incumbent plants face persistent aggregate and idiosyncratic productivity shocks. Potential entrants must know the current aggregate productivity and their idiosyncratic productivity upon entry to precisely form an expected profit from their entry. However, they can observe only the sum of aggregate productivity and their potential idiosyncratic productivity. Accordingly, they must solve a signal extraction problem to estimate the aggregate productivity and potential idiosyncratic productivity separately. A consequence of this information structure is that they will underestimate variations in aggregate productivity as long as the idiosyncratic shock is more volatile than the aggregate shock, and potential entrants are aware of it.====This situation distorts the accurate formulation of the expected value from the entry by the potential entrants, underestimating the variations in future labor and capital costs. For example, when the economy booms, potential entrants do not immediately realize that aggregate productivity has improved. Instead, they believe that their potential idiosyncratic productivity has improved. Given the persistence of aggregate productivity shocks, these entrants underestimate the next period’s aggregate productivity. Consequently, they underestimate the next period’s equilibrium wage and price of capital. They ultimately overestimate the expected value from entry when the positive shock hits. The converse logic applies for negative shocks, amplifying the response along the entry margin to the aggregate productivity shocks.====This overshooting along the entry margin, given information friction, mutes the exit margin response to aggregate shocks. Incumbent plants that observe the aggregate productivity directly can accurately forecast the persistent equilibrium factor price dynamics. Hence, the positive effect of the aggregate productivity shock on marginal incumbent plants’ expected operating profit is cancelled by the amplified response of the equilibrium factor prices.====The model economy, driven by aggregate total factor productivity (TFP) shocks, generates a realistic cyclical entry and exit behavior. The model economy generates ==== of observed variations in the entry rate and ==== of observed variations in the entrants’ productivity. The model economy also generates cross-correlation of exit rate with cyclical indicator consistently observed from data: the contemporaneous correlation coefficient with the cyclical indicator is 0.05 (====0.02 in the data). Further, the two-period lagged correlation coefficient with the cyclical indicator is 0.66 (0.46 in the data).====This study is closely related to recent studies that extend the Hopenhayn (1992) industrial equilibrium model by adding business cycle fluctuations to the competitive equilibrium. Clementi and Palazzo (2016), Clementi et al. (2015), Lee and Mukoyama (2018), and Samaniego (2008) develop variants of this model to investigate the entry and exit behavior of the business cycle, and plant dynamics.====Samaniego (2008) argues that a disciplined household with a standard risk aversion value implies an entry and exit rate much less volatile than the magnitude in the data. In the model economy by Clementi and Palazzo (2016), general equilibrium forces emerge from only the iso-elastic labor supply function. In this scenario, the entry rate is volatile as observed in the data, while the exit rate becomes too volatile relative to the data.====Lee and Mukoyama (2018) assume a constant marginal cost of building a new plant, inducing the entry margin to fully absorb variations in the expected profit associated with business cycle fluctuations. As observed in the data, their model economy can thus generate a strongly procyclical entry rate and acyclical exit rate. However, given the free entry condition, the characteristic of entrants does not move along the business cycle in their baseline model economy without cyclical entry cost. To jointly generate cyclicality of entrants’ properties together with cyclical patterns of the entry and exit rate, they introduced a particular combination of the cyclicality of entry costs.====Given that Lee and Mukoyama (2018) has shown that industrial equilibrium model with free entry condition can generate a strongly procyclical entry rate and acyclical exit rate, explaining the asymmetry between entry rate and exit rate using the information friction of potential entrants itself does not add value to the literature. The main takeaway of this study is showing that the information friction of potential entrants can be a possible source of generating the cyclicality of entrants’ properties.====The rest of this paper is organized as follows. Section 2 recaps the cyclical properties of plant entry and exit in the manufacturing sector using data from the BDS and provides motivating observations. Section 3, the model economy is described formally. Section 4 discusses the calibration. Section 5 discusses the quantitative results and provides insight into the model mechanism through impulse response exercises. It also examines the implications of the model for cyclical entry and exit behavior. Moreover, the study tests the implications of model economy regarding the motivating observations. Section 6 concludes the study.",The cyclicality of entry and exit: The role of imperfect information,https://www.sciencedirect.com/science/article/pii/S0165188922000409,24 February 2022,2022,Research Article,85.0
"Ramadiah Amanah,Fricke Daniel,Caccioli Fabio","Financial Network Analytics Ltd, United Kingdom,University College London, Department of Computer Science, United Kingdom,Deutsche Bundesbank, Directorate General Markets, Germany,London School of Economics and Political Science, Systemic Risk Centre, United Kingdom,London Mathematical Laboratory, United Kingdom","Received 26 June 2020, Revised 9 February 2022, Accepted 12 February 2022, Available online 22 February 2022, Version of Record 28 February 2022.",https://doi.org/10.1016/j.jedc.2022.104333,Cited by (2),"Macroprudential stress tests generate a wide range of stress outcomes, depending on the chosen input parameters. Building on the concept of reverse stress tests, we embrace this parameter sensitivity in a backtesting exercise. We generalize an otherwise standard model of price-mediated contagion by interpolating between different liquidation dynamics among banks (leverage targeting vs. threshold dynamics). We then test the capability of this model to match actual bank non-/defaults in the United States for the years 2008–10, where we treat the underlying liquidation dynamics as another free input parameter. While the model performance depends on the type of shock being imposed, we find that all liquidation dynamics we consider can explain to some extent (in particular better than a random benchmark) the pattern of defaults observed during the subprime crisis. We identify the region in the parameter space where a specific dynamic leads to the best fit of the data, and in the most relevant regime (illiquid asset markets and small initial shocks) leverage targeting turns out to provide the most accurate results. We also show how the results depend on the initial shock level, the market impact parameter, on the number of asset liquidation rounds, and the chosen liquidation functions.","The interconnectedness of the financial system can be an important source of systemic risk (Haldane and May, 2011). Ever since the global financial crisis of 2008-09, macroprudential stress tests (or network models of systemic risk) have become very prominent tools to uncover potential vulnerabilities in the financial system (Glasserman and Young, 2016). The typical approach, as illustrated in Fig. 1, consists of feeding a given stress test model that contains the relevant contagion mechanism(s) with the relevant inputs (such as a stress scenario and certain model parameters) to obtain an estimated stress outcome under the chosen conditions. It is well-known, however, that stress test outcomes are often sensitive both to the chosen modelling framework and the chosen inputs (e.g., Niepmann, Stebunovs, 2018, Siemsen, Vilsmeier, 2018). The fact that stress test models can generate a wide range of outcomes as a function of the chosen model parameters is, implicitly or explicitly, sometimes brought forward to question the validity of such modelling exercises. Therefore, researchers follow standard practice and spend substantial efforts on analysing the parameter dependence of their stress test results.====In this paper, we take a different perspective on macroprudential stress tests and embrace this parameter sensitivity in a so-called reverse stress testing exercise (Grigat and Caccioli, 2017). The fundamental idea is that we take a given stress test model and fix the stress outcome (i.e., the model output). This allows us to ask how well the chosen model is able to match this outcome for a broad range of inputs. Which input combinations generate the stress outcome reasonably well? We acknowledge that the answers to these questions will be application-specific, so we provide a general framework that can be adjusted for each application.====As an illustration, in this paper, we take a given bank fire-sale model with common asset holdings and try to match the stress outcome of actual defaults and non-defaults of US banks during the global financial crisis of 2008-11. We vary the relevant inputs and feed the model using only pre-crisis balance sheet data (from 2007-Q4) to find which inputs replicate the stress outcome ’best’. Hence, our reverse stress testing approach takes the form of a backtesting exercise. Such an exercise is important because, generally speaking, while stress tests are not a prediction tool, they may be used as early-warning indicators and it is crucial to calibrate models in a way that they can capture dynamics that were observed during relatively recent crisis period. Notably, while we take the stress test model as given, our framework could also be used to identify models that fare rather poorly. In cases where a model cannot match a given stress outcome under any input combinations, the researcher would have to search for a more adequate modelling approach.====Much of the literature has focused on the development of contagion analyses in financial networks, but with the exception of Huang et al. (2013), relatively little work has been devoted to empirically testing the capability of any given stress test model to match actual bank defaults (and survivals) during crisis periods. Broadly speaking, the literature can be differentiated into three broad categories in terms of the contagion channels (Baranova, Coen, Lowe, Noss, Silvestri, et al., 2017, Bardoscia, Caccioli, Perotti, Vivaldo, Caldarelli, et al., 2016, Battiston, Puliga, Kaushik, Tasca, Caldarelli, et al., 2012, Caccioli, Shrestha, Moore, Farmer, et al., 2014, Cont, Schaanning, 2017, Duarte, Eisenbach, 2015; Eisenberg and (NOE), 2001; Elsinger, Lehar, Summer, 2006, Fricke, Fricke, et al., 2020, Greenwood, Landier, Thesmar, 2015, Huang, Vodenska, Havlin, Stanley, et al., 2013): (i) direct contagion in interbank borrowing/lending networks, (ii) indirect price-mediated contagion due to common asset holdings (overlapping portfolios), and (iii) information and liquidity contagion (Aymanns, Farmer, Kleinnijenhuis, Wetzer, et al., 2018, Battiston, Martinez-Jaramillo, 2018).====Here we look at the second contagion channel, which is often found to be the most relevant one (Caccioli, Shrestha, Moore, Farmer, et al., 2014, Glasserman, Young, 2015). This contagion mechanism is based on the idea of fire sales in asset markets: when a leveraged bank faces a loss, it may need to liquidate (part of) its assets. The corresponding market impact decreases the prices of the liquidated assets further, which creates a vicious circle where banks may need to sell even more assets in a falling market. Two extreme types of banks’ liquidation dynamics have been proposed in the literature, namely the model of Caccioli et al. (2014) and Huang et al. (2013) and the model of Greenwood et al. (2015): in the former, banks are assumed to sell their assets only after they have defaulted (====). In the latter, banks are assumed to sell their assets whenever their leverage ratio is off-target (====).====In order to acknowledge that the actual liquidation behaviour of banks might lie somewhere in-between, we generalize an otherwise standard fire sales model by introducing a one-parameter (====) family of non-linear liquidation functions that interpolates between these extremes. These functions determine the volume of assets that a bank liquidates in response to an initial loss. Intuitively, modest (large) values of ==== can be interpreted as the tendency of banks to follow leverage targeting (threshold) dynamics. On the modelling side, the main novelty of our paper is that we can treat ==== simply as another input parameter.====We then use this generalized model to match actual bank defaults/non-defaults for a range of liquidation dynamics and other input parameters. One of our main goals is to identify the values of ==== that perform best in terms of accuracy (in a backtesting sense). Following Huang et al. (2013), we use U.S. commercial bank balance sheet data for the last quarter of 2007 and apply a variety of shock scenarios to assess whether the different models manage to accurately match the actual defaults that occurred during the years 2008-10 based on the list of bank failures published by the Federal Deposit Insurance Corporation (FDIC). In line with previous work, we find that the stress test model is able to generate a broad range of stress outcomes and that the performance of the stress test model strongly depends on the type of initial shock being imposed. While systematic shocks tend to yield relatively poor results, idiosyncratic shocks can yield much better results. However, the model performance strongly depends on which asset class is being shocked initially. Our approach allows to compare the performance when imposing different shock scenarios and helps identifying ex-post those asset classes that appear most relevant.====For our specific application, our main results can be summarized as follows:====We also show that allowing for asset class-specific market impact parameters can improve the model performance, while accounting for multiple rounds of asset liquidation can affect the performance of the model. In particular, we show that considering only the first round of asset liquidations appears most accurate for a model with small ==== (banks act as leverage targeters), while accounting for multiple rounds of asset liquidations provides better results for larger ==== (banks only liquidate in case of default). Lastly, we show that our main results are robust with respect to the liquidation functions. To this end, we consider a leverage constraint on asset liquidations as proposed in Cont and Schaanning (2017) where bank do not react to small deviations from their target leverage ratios, but rather when the leverage ratio exceeds the maximum leverage constraint allowed by prudential regulation.====In addition to the above-cited papers, our paper mainly contributes to the following streams of the literature: first, our generalization of existing stress test models captures a wide range of banks’ asset liquidation behaviour in response to some initial shock. In this sense, our work is analogous to Bardoscia et al. (2016) who analyse counterparty risk within interbank networks. Second, our paper adds to the literature on backtesting risk models. While backtesting microprudential risk models is now common practice among market practitioners (e.g., Cavestany, Rodríguez, 2015, Daníelsson, 2011, Philippon, Pessarossi, Camara, 2017), relatively little attention has been devoted to the case of macroprudential stress tests. Our methodology allows us to compare the performance of different models and thus to identify the most accurate stress test model, given some exogenous parameters. This is similar to the approach of Huang et al. (2013), who test the performance of the threshold model. In our work, we use the same methodology but allow for different liquidation dynamics and different combinations of the initial shock/market liquidity parameters. Lastly, our paper is also related to the literature on reverse stress testing (e.g., Grigat and Caccioli, 2017). Fundamentally, reverse stress testing is concerned with identifying scenarios that would lead to a certain stress testing outcome. In our analysis, the outcomes that we wish to match are individual bank defaults/non-defaults. In other words, in our backtesting exercise, we know exactly which banks defaulted and which banks did not default over the sample period of interest and we seek to find the stress test model (and the corresponding parameters) that jointly matches each bank’s final state as closely as possible.====The remainder of this paper is structured as follows: in Section 2 we explain our framework and present the underlying stress test model. Section 3 provides details on the data sets being used in the paper. Section 4 contains the application and Section 5 concludes.",Backtesting macroprudential stress tests,https://www.sciencedirect.com/science/article/pii/S0165188922000380,22 February 2022,2022,Research Article,86.0
Khan Abhimanyu,Shiv Nadar University,"Received 23 September 2021, Revised 11 January 2022, Accepted 8 February 2022, Available online 11 February 2022, Version of Record 9 March 2022.",https://doi.org/10.1016/j.jedc.2022.104332,Cited by (2),"I examine the effect of decision-making processes on the dynamics of bargaining over a fixed pie by comparing the share received when individuals are subject to reference-dependent preferences, loss-aversion, and probability-weighting, to the share they would receive on choosing by maximising expected utility instead. I show that: ==== reference-dependent preferences are unambiguously advantageous, ==== loss-aversion does not have any effect, and ","In situations of risk and uncertainty, preferences over possible outcomes, and perceptions of their likelihood, jointly determine decision-making. In the expected utility framework, an individual’s preferences (represented by a utility function which specifies the utility obtained from each outcome) along with the probabilities of the occurrence of outcomes come together when the individual chooses the action that yields the highest expected utility. While this framework is founded on a-priori reasonable axioms, and is analytically tractable, its descriptive validity has often been called into question, and prompted the development of alternatives to expected utility.==== Cumulative prospect theory (Tversky and Kahneman, 1992), arguably the most comprehensive empirically-founded alternative, emphasises: ==== reference-dependent preferences: the value of an outcome is given by the gain/loss that it represents about a reference-point, and individuals are risk-averse (risk-seeking) in the gains (losses) domain, ==== loss-aversion: a loss of a particular magnitude weighs more heavily than a gain of the same magnitude, and ==== probability-weighting: individuals weight the probability of occurrence of each outcome, with low (high) probabilities receiving more (less) weight than the actual probability. According to cumulative prospect theory, an individual chooses the action that generates the highest probability-weighted value.====In this context, it fundamental to understand how decision-making processes influence outcomes. I consider one of the most common situations of strategic uncertainty – bargaining over the division of a pie – and analyse how the dynamics of bargaining evolves when the individuals’ decision-making process is described by cumulative prospect theory rather than the expected utility framework, and what is the individual, and joint, effect of the above-mentioned components of cumulative prospect theory.====I use a dynamic model of bargaining similar to Young (1993b) and Naidu, Hwang, and Bowles (2010). There are two finite populations namely, ==== and ====. In every time period, each individual is randomly matched to an individual from the other population to bargain over a pie of fixed size. The randomly matched individuals claim a share of the pie for themselves, and receive their respective demands only if the sum of the two claims does not exceed unity. In order to choose his claim, each individual draws a random sample from the demands made by the other population’s individuals, and associates the probability with which his randomly chosen co-player will make a particular demand with the relative frequency of that demand in his sample. Hence, if an individual claims a particular share, say ====, then the probability of receiving it is the relative frequency with which demands not exceeding ==== appear in his sample. Thus, in case of the expected utility (cumulative prospect theory) framework, each feasible claim generates a lottery (prospect), and an individual claims the share that generates the most desirable lottery (prospect). The trade-off is that demanding a higher share increases the risk of not obtaining it due to a higher probability of incompatibility with his co-player’s claim. An individual’s decision-making process weighs this trade-off, and thereby determines his actions, and, consequently, the bargaining outcome.====I show that this unperturbed dynamic process of bargaining eventually leads to a convention where all the individuals in one population demand and obtain an identical share ==== of the pie while all the individuals in the other population demand and obtain ==== of the pie. This convention may, in general, depend on the exogenously specified initial conditions (read, initial profile of claims). In order to identify the stochastically stable convention, I perturb this bargaining process by introducing the possibility of experimentations in the decision-making process of the individual. I follow Naidu et al. (2010), and assume that these experimentations are ‘intentional’ in that individuals experiment “in the direction that could benefit them if sufficiently many others did the same; thus the population driving transitions are the ones that stand to gain” (pp. 32 Naidu et al., 2010).==== This contrasts with the more standard model of uniform experimentation (see, for instance, Young, 1993a and Young, 1993b) wherein individuals may experiment with any strategy, and not necessarily with the strategies from which they may stand to gain.====When all individuals in a particular population have the same increasing and concave utility function, and individuals in both populations choose by maximising expected utility for each period in isolation, then the pie is divided according to the Nash bargaining solution (Naidu et al., 2010).==== I maintain this as the bench-mark, and first analyse the individual impact, and then the joint impact, of the building blocks of prospect theory. To this end, I first incorporate reference-dependent preferences, loss-aversion, and probability-weighting into the decision-making of the population ==== individuals. In each case, population ==== individuals choose by maximising expected utility. I compare the share received by the population ==== individuals to the share they receive in the benchmark situation (where they also choose by maximising expected utility).====I find that individuals obtain a higher share of the pie when their decision-making process involves only reference-dependent preferences (but not loss-aversion or probability-weighting). The intuition is that with reference-dependent preferences, the trade-off referred to earlier is framed in the losses domain, and the risk-seeking behaviour in this domain causes them to be more aggressive in their bargaining posture; this results in obtainment of a higher share of the pie.====Secondly, incorporating loss-aversion in the reference-dependent preferences does not have any marginal effect on the bargaining outcome beyond the effect of reference-dependence. (Individuals cannot be loss-averse without having reference-dependent preferences.) This is due to the linear scaling that is used to represent loss-aversion in cumulative prospect theory, along with the fact that it is the domain of losses that is relevant for decision-making in this bargaining game.====Thirdly, when decision-making involves only probability-weighting, then individuals obtain a lower share of the pie. This is because probability-weighting over-emphasises the co-players’ claims for a higher share, and induces the probability-weighting individuals to compromise with a lower share to avoid the possibility of obtaining nothing.====Naturally, when these three features come together so that cumulative prospect theory appropriately describes decision-making, it is the relative strength of the advantage conferred by reference-dependence and the disadvantage imposed by probability-weighting that determines whether these individuals receive a higher share, and I obtain a precise condition that expresses this tradeoff.====This paper is closely related to three different strands of literature: one explores the connections between evolutionary game theory and cooperative game theory, the second studies the relationship between evolutionary dynamics and solutions to coordination games and bargaining games, while the third analyses how behavioural biases influence the outcome of bargaining games. I present a brief review of each of these in turn, as they pertain to the current paper (see Newton, 2018 for an exhaustive survey), and then discuss the contribution of my paper in light of the existing body of work.====In a seminal paper, Nash (1950) studies the bargaining problem as a cooperative game, and the proposed solution – referred to as the Nash bargaining solution – belongs to the core. Similar to the manner in which Nash (1953) initiated the ‘Nash program’ to provide non-cooperative foundations to cooperative game theory, Young (1993b) was instrumental in founding the literature to provide evolutionary foundations to cooperative game theory by obtaining the Nash bargaining solution as the stochastically stable state in an evolutionary model of bargaining where individuals claim a share of the pie by playing (perturbed) best-responses to demands made in the past. Agastya (1997) extends this to multi-player bargaining and shows that, under some conditions, an allocation in the core is obtained. Agastya (1999) examines the stochastically stable allocations in the core when the individuals’ responses are perturbed with uniform errors. Arnold and Schwalbe (2002) and Rozen (2013) show that the result in Agastya (1997) holds even when the strategy space of the individuals includes choice of the coalition as well. Newton (2012b) demonstrates that with the possibility of coalitions of individuals jointly best-responding to the history of the game, convergence to a core allocation obtains under weaker conditions than in Agastya (1997), and that the stochastically stable allocation maximises a Rawlsian social welfare function. Nax (2018) and Sawa (2019) analyse aspiration-based dynamics and logit responses, respectively, and show that the convergence to a core allocation result generalises to dynamics other than the best-response dynamic.====The bargaining game is related to coordination games in which the principal diagonal of the payoff matrix – which correspond to the coordination outcomes – describes the efficient outcomes of the game. Important questions relate to how the stochastically stable outcome is affected by the nature of the off-diagonal mis-coordination outcomes (for instance, the contract game in Young (1998) heavily penalises any form of mis-coordination whereas certain types of mis-coordination – given by situations where the sums of claims made by the individuals is less than unity – is not as costly in the bargaining game), the specification of the unperturbed dynamics, and the manner in which this dynamic is perturbed. When individuals play best-responses to the past history of the game that are occasionally perturbed by uniform errors, then Young (1993b) and Young (1998) show that the Nash bargaining solution is the stochastically stable outcome in bargaining games but the Kalai and Smorodinsky (1975) solution is obtained in contract games. However, when the errors are intentional (as described earlier), or coalitions of individuals may form, then Naidu et al. (2010) and Newton (2012a), respectively, recover the Nash bargaining solution as the stochastically stable outcome in contract games. On the other hand, while Hwang et al. (2018) consider logit dynamics in the contract game and show that the emergence of the Egalitarian bargaining solution (Kalai, 1977) when errors are intentional, and a logit bargaining solution when the errors are uniform, Hwang and Rey-Bellet (2021) demonstrate the stochastic stability of the Nash bargaining solution in the bargaining game under logit dynamics with uniform errors but not under the logit dynamics with intentional errors.====In the context of analysing the effect of (behavioural) biases on the outcome of bargaining, Brito et al. (1977), Thomson (1981), and Gupta and Livne (1988), analyse the effect of reference points in the axiomatic approach to bargaining by Nash (1950) while Gimpel (2007) explores how the preference of ‘negotiators’ change in multi-attribute negotiations. Compte and Jehiel (2003) and Li (2007) obtain that reference dependent preferences can lead to delay and gradualism along the equilibrium path in the bargaining game. Hyndman (2011) examines how the share received by individuals in the bargaining game depends on the process of reference point adjustment. The examination of the consequences of reference points is enriched by the incorporation of loss-aversion in Shalev (2002) and Driesen et al. (2012), endogenous determination of reference points by Vartiainen (2007), and endogenous reference points along with loss-aversion by Karagözoğlu and Keskin (2018).====The paper most closely related is Sawa (2021), which studies a two-stage bargaining game between loss-averse individuals when the reference point is an exogenously specified outside option, and individuals either exit with the outside option in the first stage of the game or proceed with bargaining over the pie in the second stage otherwise.==== In Sawa (2021), a modified version of the Nash bargaining solution, namely the ‘prospect theory’ bargaining solution, emerges as the stochastically stable outcome. The qualitative findings are that reference-dependent preferences may actually result in a lower share of the pie, and loss-aversion always decreases the share of the pie. The current paper, apart from also focussing on probability-weighting thereby making the effect of cumulative prospect theory and its components on the bargaining outcome more comprehensive, differs in two critical aspects. Firstly, the reference point is determined endogenously and evolves along with the play of the game, and secondly, individuals do not have the option of exiting with an outside option. As a result, I obtain that reference-dependence of the preference is beneficial, while loss-aversion is neutral, to the cause of the individual.====In this paper, I explore the effect of a descriptively sound decision-making process, namely cumulative prospect theory, on the bargaining outcome. In comparison to the papers (cited above) that explore the connections between cooperative solutions and evolutionary game theory, or the impact of the type of dynamics on the outcome of bargaining and coordination games, it is not surprising that a different decision-making process should lead to a different prediction than what is predicted by say, best-response dynamics or logit dynamics. Rather, the contribution of this paper is to first delineate the effect of each of the individual components of prospect theory, then examine their combined influence on bargaining, and finally explore the impact of the parameters of cumulative prospect theory (such as the curvature of the value function, the degree of loss-aversion, and the extent of probability-weighting). This comprehensive examination stands in contrast to the existing literature which generally focusses on the influence of one particular component of prospect theory.====Finally, I note that there is a related stream of research which argues that features such as reference dependence (see, Friedman, 1989), the endowment effect (see, Huck et al., 2005), and probability-weighting or distortions in perception of likelihood (see, Herold, Netzer, 2015, Steiner, Stewart, 2016, and Gossner and Steiner, 2018) may emerge as evolutionary second-best or constrained optimum. This stream is also connected to the literature on indirect evolution. Starting with Guth and Yaari (1992) and Guth (1995), indirect evolution studies preferences that can evolve when the individuals’ actions are chosen by maximising subjective payoffs (that are derived from the preferences) whereas fitness and evolution depend on objective payoffs. In this paper, I do not deal with the question of whether behaviour of the type predicted by cumulative prospect theory can emerge out of an evolutionary process; I rely on the extensive empirical research on cumulative prospect theory for support of its appropriateness as a model of decision making (see Barberis, 2013 for a survey), and examine its implication for the outcome of a particularly relevant and pervasive strategic situation, namely the bargaining game. The restriction to a two-player bargaining game facilitates a tractable analysis; in future research, it would be instructive to the study a multi-player bargaining games where coalitions of ‘behaviourally-biased’ individuals may jointly respond in a coordinated manner – a research question that lies in the intersection of this paper and Newton (2012a).",Expected utility versus cumulative prospect theory in an evolutionary model of bargaining,https://www.sciencedirect.com/science/article/pii/S0165188922000379,11 February 2022,2022,Research Article,87.0
Xu Jing,"School of Finance, Renmin University of China, 59 Zhongguancun Street, Haidian District Beijing, 100872, China","Received 11 June 2021, Revised 3 January 2022, Accepted 8 February 2022, Available online 11 February 2022, Version of Record 19 February 2022.",https://doi.org/10.1016/j.jedc.2022.104331,Cited by (0),We examine optimal effort choice in a competition model where the agents are averse to low relative status and to exerting excessive effort. The game has a unique pure strategy ,"Competition is a fundamental theme of modern societies. In order to acquire scarce resources (e.g., entrance to elite schools, position in high-paying jobs, publication in top academic journals, etc.), people usually need to compete against others intensively, which may require considerable amount of effort. The literature has empirically documented the effects of competition on individuals’ effort choices.==== Nonetheless, theoretical studies on this topic are relatively limited.====This paper examines optimal effort choices of a group of agents who care about their future statuses. Here, status refers to certain attribute (e.g., performance, ability, skill, achievement, etc.) of the agents. In the model, we assume that an agent’s derived utility may depend on her status relative to other agents’ statuses. This implies the existence of competition between these agents, and we use the sensitivities of utilities with respect to relative statuses to measure the strength of the agents’ competition incentives. We assume the agents are averse to low relative statuses, and we term it ====. Endowed with an initial status, each agent can exert costly effort, which results in disutility, to improve her future status. Hence, the agents need to simultaneously trade off self-improvement, effort cost, and relative status concern.====The incentive to compete gives rise to a game between the agents. Assuming complete information for all agents, we first show that a unique pure strategy Nash equilibrium exists for a fairly large class of utility and disutility functions. Then, we explore properties of the equilibrium in greater detail.====We begin with analyzing a case where the agents are homogeneous. In this case, a stronger competition incentive will unambiguously drive up the effort levels and reduce ==== utilities in equilibrium.==== Therefore, competition can result in inefficiency from the agents’ perspectives. The intuition behind this finding is as follows. Since the agents exhibit left-behind aversion, when they are ==== homogeneous, none of them will accept to be a final loser. They will choose their effort levels such that for each agent, the marginal utility gain from avoiding being a loser exactly equals the marginal effort cost. Since a greater competition incentive implies a heavier utility loss incurred by a loser, it also implies a higher effort cost that is deemed acceptable by the agents. Therefore, a higher effort level is chosen in equilibrium.====Next, we examine a case with heterogeneous agents, which is the main focus of this study. We provide a set of theoretical and numerical analyses. The results can shed light on some competition-related empirical findings documented in the literature, which we explain in detail below.==== First, we examine a case where the agents are heterogeneous in initial status only. We find that in equilibrium, agents with lower initial statuses (initial losers) will exert more effort to improve their statuses, trying to catch up with agents with higher initial statuses (initial winners); however, initial losers’ final statuses will still be lower than that of initial winners. This result suggests that the initial losers in this case cannot move up in rank ====. Instead, they need to change other characteristics, such as increasing effort efficiency or reducing effort aversion.==== In our model, if a subgroup of agents change their effort choices (e.g., due to changes in effort aversion levels or competition incentives), then these agents’ final statuses are likely to change, which will affect the remaining agents’ relative statuses and utilities. Thus, we expect (and verify) that the remaining agents will also change their effort choices to re-establish their optimal status improvement-effort cost tradeoffs. Put differently, changes in effort choices can have a spillover effect. Furthermore, we show that the left-behind aversion is a critical driver of this effect. This result can help explain the empirical finding that individuals’ effort choices in workplace appear to be influenced by their peers’ performances.==== As we previously argued, the effort-competition incentive relationship is monotonically increasing in the case with homogeneous agents. When agents are heterogeneous in initial status or effort efficiency, however, we find that the optimal effort choices of advantageous agents (i.e., agents with higher initial statuses or effort efficiencies) may locally decrease in competition incentive. The intuition is as follows. When agents are heterogeneous in initial status or effort efficiency, a stronger competition incentive has two competing effects: on one hand, it induces the advantageous agents to lever on their advantages, giving them incentive to exert less effort to reduce effort cost; on the other hand, it induces stronger left-behind aversion, motivating the advantageous agents to exert more effort to avoid turning into losers in the end. These two competing effects can lead to a non-monotonic relationship between the advantageous agents’ effort choices and competition incentive.==== For the disadvantageous agents, the first effect is absent; as a result, their effort choices monotonically increase in competition incentive.==== When the agents are heterogeneous in certain aspects (e.g., initial status or effort efficiency), a natural question is: should the advantageous agents take the advantages and actively compete against the disadvantageous agents? Intuitively, engaging in a competition has two competing effects on an advantageous agent’s ==== utility. On one hand, it has the positive effect that she could hopefully derive a higher utility level by exploiting her advantages to become a final winner. On the other hand, it has the adverse effect that she may incur heavier disutility due to the extra effort induced by competition. Thus, whether an advantageous agent should engage in competition depends on the tradeoff of these two effects. We find that if the magnitude of advantage is large, then (i) engaging in competition with moderate intensity can make her better off; and (ii) engaging in excessively intensive competition will make her worse off. On the other hand, when the magnitude of advantage is only marginal, engaging in competition may always make her worse off. These results indicate that excessive competition is costly, and are consistent with Chung and Lee (2017) who show that intensive competition can cause inefficient educational effort.====Using samples from an urban Chinese textile firm, Kato and Shu (2016) empirically document that heterogeneity in social identity (rural identity vs. urban identity) is an important determinant of competition between individuals in their sample. Specifically, they find that only employees with urban identities tend to compete against those with rural identities. To the extent that difference in social identity may lead to perceived difference in status, ability, or other characteristics, this finding can be explained by our theoretical result that only sufficiently advantageous agents should seek to compete.====We also examine some extensions of the baseline model. For example, using a model with limited effort capacities, we show that constraining some agents’ effort choices can lower other agents’ effort choices in equilibrium; in a probabilistic choice model in which the agents can choose both their effort levels and success probabilities, we find that agents with lower initial statuses or stronger competition incentives tend to choose higher success probabilities; in a model with peer comparison effect, we find that purposefully choosing peers with similar statuses to compare with will also affect agents’ effort choices; in a model with multiple generations and bounded rationality, we show that the effort choices monotonically converge to the equilibrium effort choices in the baseline model if the agents are homogeneous across generations.====Then, we examine two variants of our baseline model. In the first variant, we consider a competition model with option-like payoffs (prizes), which implies that winners are rewarded but losers are not penalized. If the agents are heterogeneous only in initial status,==== agents with sufficiently low initial statuses (lower than a critical threshold level) will choose zero effort. Moreover, we find that the critical threshold level increases in prize size, implying that a larger prize size will deter the disadvantageous agents’ incentives to exert effort in the competition. The intuition is as follows. A larger prize size will motivate agents with high initial statuses to exert more effort to ensure that they can win the prize, which makes it more difficult to be final winners ====. Thus, more agents whose initial statuses are not high enough will voluntarily choose zero effort to avoid effort costs. This prediction is consistent with the experimental finding of Bedard and Fischer (2019) that a larger prize size can reduce effort levels chosen by students who believe they are relatively low in the ability distribution.====In the second variant, we consider a multi-period competition model and examine its implications for the effectiveness of different assessment methods used in educational programs. Specifically, we assume the final score an agent obtains is a weighted average of interim evaluations, which are based on relative status (i.e., performance), made at different stages of the program. Assuming the agents are risk-neutral and have quadratic effort cost functions, we show that (i) if the evaluator aims to induce maximal effort and improvements, then she should allocate all evaluation weight to the last period (i.e., implementing a terminal assessment method); and (ii) if instead, the evaluator’s objective is to maximize effort-adjusted utilities, then she should allocate all weight to the first period (i.e., choosing an initial assessment method). These findings can shed light on the (non)effectiveness of continuous assessment (CA) method, which is a popular educational evaluation method implemented in many developing countries. CA method proposes to evaluate a student’s progress throughout a prescribed course, and is often used as an alternative to the final examination system. However, the effectiveness of the CA method is questioned by several empirical studies. For example, by conducting a randomized evaluation of CA method in the state of Haryana, Berry et al. (2020) find that CA method had no significant effects on test scores (see also De Lisle, 2016). These empirical findings can be explained by our theoretical finding that neither the students’ academic improvements nor their effort-adjusted utilities are maximized by the CA method. Instead, we argue that the CA method may help balance the students’ academic improvement and effort-adjusted utilities.====Lastly, we consider two dynamic competition problems in continuous time and present numerical solutions. The first problem is a goal-reaching competition, in which an agent aims to make her status reach a target level earlier than her opponent does. In this problem, the agents choose high effort levels when the final outcome is mostly uncertain. The second problem is a competition model with status concerns, in which the agents’ relative statuses at the final time directly affect the shape of their utility functions. In this problem, the agents choose high effort levels when their current statuses are around the convex kinks of their utility functions. These two applications demonstrate the applicability of our main idea in different economic settings.====The remainder of this paper is organized as follows. In Section 2, we briefly review other related studies. In Section 3, we introduce our baseline framework. In Section 4, we present a set of theoretical and numerical analyses of the baseline model and its various extensions. In Section 5, we examine alternative competition models and their implications. In Section 6, we summarize the empirical relevance of our theoretical results. In Section 7, we conclude. All the proofs are relegated to the Appendix.",Competition and equilibrium effort choice,https://www.sciencedirect.com/science/article/pii/S0165188922000367,11 February 2022,2022,Research Article,88.0
"Görtz Christoph,Yeromonahos Mallory","University of Birmingham, Department of Economics, Birmingham B152TT, UK,University of Westminster, School of Organisations, Economy and Society, NW15LS London, UK","Received 18 January 2021, Revised 4 February 2022, Accepted 7 February 2022, Available online 9 February 2022, Version of Record 19 February 2022.",https://doi.org/10.1016/j.jedc.2022.104330,Cited by (2),"A large literature suggests that the expected equity risk premium is countercyclical. Using a variety of different measures for this risk premium, we document that it also exhibits growth asymmetry, i.e. the risk premium rises sharply in recessions and declines much more gradually during the following recoveries. We show that a model with recursive preferences, in which agents cannot perfectly observe the state of current productivity, can generate the observed asymmetry in the risk premium. Key for this result are endogenous fluctuations in uncertainty which induce procyclical variations in agent’s nowcast accuracy. In addition to matching moments of the risk premium, the model is also successful in generating the growth asymmetry in ==== aggregates observed in the data, and in matching the cyclical relation between quantities and the risk premium.","The most recent U.S. investment and housing booms that ended abruptly in 2001 and 2007 have been associated with highly optimistic beliefs about profitability. In the former case, beliefs about profitability were linked to information technology and in the latter to house price gains. Both booms were associated with times of low uncertainty and saw long hikes in stock markets. Adjustments of beliefs about profitability resulted in sharp recessions, heightened uncertainty, and strong corrections in stock markets (see e.g. Beaudry and Portier, 2006 and Shiller, 2007). In this situation, investors were less willing to bear financial risk and, for a given level of stock market risk, they required a higher compensation to hold stocks instead of a risk free short-term asset. Indeed, this equity risk premium increased sharply at the brink of both recessions, very much in contrast to the slow and gradual decline that could be observed during the preceding booms. This growth asymmetry shows in positively skewed distributions of risk premium growth — skewness is 0.64 and 1.00, respectively over these two business cycles.====There is a large body of work in the finance literature on risk premia, which for example provides substantial empirical evidence that the equity risk premium varies over time and is countercyclical.==== A recent and growing literature jointly studies both the behavior of risk premia and macroeconomic dynamics. We contribute to this body of work which has not considered the important asymmetric feature of risk premia. We first document the degree of asymmetry in the data and then develop a structural model with endogenous countercyclical variations in uncertainty that is consistent with this feature in the data. To the best of our knowledge this is the first paper that tackles this issue.====We start by computing statistics on growth asymmetry for a variety of expected equity risk premium measures that have been found relevant in the literature. We document for the post-WWII U.S. economy that growth rates of all risk premium measures exhibit positive skewness. In particular, we construct risk premium measures using models based on the historical mean of realized stock market returns in excess of Treasury bond yields, and models based on predictive time series regressions of equity returns on selected fundamentals. We further employ direct risk premium measures based on responses of Chief Financial Officers recorded in the Duke CFO Global Business Outlook Survey. The broad support for growth asymmetry is remarkable given the substantial diversity in assumptions underlying the various employed risk premium measures.====We then develop a structural model that is consistent with the empirically observed growth asymmetry in the risk premium. We build on the framework in Van Nieuwerburgh and Veldkamp (2006) who introduce a Bayesian learning mechanism into an otherwise standard RBC model. We extend the framework with capital adjustment costs and preferences of the Epstein and Zin (1989) type. This extension allows disentangling relative risk aversion and the elasticity of intertemporal substitution and gives rise to a risk premium. Our empirical measures for expected risk premia incorporate information about future returns, so that their value can differ from realized ex-post risk premia. In the model, deviations from fundamentals are possible, because agents need to form nowcasts about the state of total factor productivity (TFP). Nowcasting is required as the otherwise standard Cobb-Douglas type production function includes an additive noise term and neither TFP nor the noise term can be observed separately at the time decisions about production inputs are made.====The key mechanism to generate growth asymmetry in the risk premium are endogenous changes in the degree of uncertainty about the state of productivity, which in turn induce procyclical variations in agent’s nowcast precision. TFP follows a two-state Markov process and agents employ a Bayesian learning technology to form a nowcast about the state of current productivity. As in Van Nieuwerburgh and Veldkamp (2006), intensified use of production inputs amplifies the signal on the state of productivity relative to the noise and results in endogenous procyclical variations of the signal-to-noise ratio. When production inputs are high, agent’s nowcasts are relatively accurate as uncertainty about the state of productivity is low. The peak of the cycle, can be observed relatively precisely. The following period is accompanied with a substantial decline in nowcast precision which leads to a sharp increase in the risk premium. The reduced use of production inputs during the following recession leads to a lower signal-to-noise ratio and a situation of heightened uncertainty about the state of productivity. For this reason, agent’s nowcasting accuracy increases only slowly during the following recovery. The associated gradual decline in uncertainty comes along with a slow and gradual decline in the risk premium.====The model is calibrated to match nowcast precision in the Survey of Professional Forecasters (SPF). Overall, it is successful in generating the observed growth asymmetry in the risk premium and in matching the risk premium’s relation with macroeconomic activity in the data. The model captures the empirically observed positive skewness in the risk premium, while a framework without the endogenous variation in nowcast accuracy does not imply a skewness substantially different from zero. The model’s ability to generate growth asymmetry rests on the procyclicality of nowcast precision and the associated variations in uncertainty. This mechanism finds strong support in the data. Firstly, the median absolute nowcast error for real GDP growth from the SPF varies countercyclically and is notably heightened when the economy contracts. We also find this absolute median nowcast error is particularly high at times when the risk premium rises strongly. Secondly, we employ the dispersion in nowcasts for GDP growth from the SPF as a proxy for uncertainty to provide further corroborative evidence for the model mechanism. We report uncertainty varies countercyclically — in line with evidence in the literature, see e.g. Bloom (2014) — and is notably heightened at times when the economy contracts and when risk premia are high. Endogenous procyclical variations in agent’s nowcasting precision are crucial also for the model’s ability to generate the well known stylized business cycle fact of negatively skewed growth rates in macroeconomic aggregates — i.e. expansions in economic activity are long and gradual while recessions are sharp and short. In addition to the skewness statistics, it is notable that our model is also successful in matching the countercyclical movements of risk premia observed in the data.====Our paper is related to several strands of the literature. There is a large body of work in the empirical finance literature on risk premia. Yet, existing theoretical work in finance has mostly been confined to endowment economies that do not consider feedback between time-varying risk premia and macroeconomic aggregates.==== On the other hand, most standard macroeconomic models do not include a meaningful role for the risk premium. Our work links to a growing literature that jointly studies the behavior of macroeconomic aggregates and risk premia in bond or equity markets (e.g. Jermann, 1998 and Kaltenbrunner and Lochstoer, 2010). Gilchrist and Zakrajšek (2012) empirically document a close link between increases in the excess bond premium and a deterioration of macroeconomic conditions. Gourio (2013) develops a macroeconomic framework driven by variations in disaster risk that reproduces key features of corporate bond risk premia — such as their countercyclicality — and studies their implications for business cycles. Campbell et al. (2019) show how macroeconomic dynamics drive risk premia in bond and equity markets and Corradi et al. (2013) find that the level and volatility of fluctuations in the stock market are largely explained by business cycle factors. Bekaert et al. (2009) highlight the role of uncertainty for the countercyclical volatility of asset returns. We contribute to this literature by explaining the growth asymmetry in risk premia and macroeconomic aggregates.====Our paper is also related to work that highlights the importance of beliefs about current and future TFP for fluctuations in macroeconomic and financial aggregates (e.g. Barsky, Sims, 2011, Beaudry, Portier, 2006, Cascaldi-Garcia, Vukotic, 2019, Pinter, Theodoridis, Yates, 2017). Görtz et al. (2021) document a close link between changes in expectations about future TFP, stock prices and risk premia. Risk premia incorporate expectations about future stock market returns and as such, they can differ from ex-post realizations. In our model, this can be the case as agents need to form nowcasts to learn about the current state of productivity. Milani (2011) highlights the relevance of expectations and learning for output fluctuations. He relaxes the rational expectations assumption to allow for agent’s learning in a New Keynesian framework and estimates the model using forecast data from the SPF. Enders et al. (2017) compute GDP nowcast errors based on the SPF and show that these are sizable and play a non-negligible role, accounting for up to 15% of output fluctuations. Cascaldi-Garcia and Galvao (2020) disentangle the effects of beliefs about future technology and uncertainty shocks. Johri and Karimzada (2020) document that fluctuations in firms’ learning from production activity is an important source for fluctuation in macroeconomic aggregates over the business cycle.====While the above literature typically does not consider asymmetries, in our framework agents have to solve a signal extraction problem with time varying parameters to explain growth asymmetries in the data. In this respect, our work links closely to a literature that considers an asymmetric speed of learning and time variation in uncertainty (e.g. Boldrin, Levine, 2001, Fajgelbaum, Schaal, Taschereau-Dumouchel, 2017, Veldkamp, 2005). It is important to note that learning at a constant speed is not sufficient to resemble the empirically observed asymmetry — the beginning of an expansion and a contraction would be observed with the same precision which triggers in the model a response of risk premia of equal size (in absolute terms). The key for the model to generate asymmetry in risk premia is that the speed of learning varies procyclically. Our mechanism for endogenous variations in the signal-to-noise ratio is closely related to Van Nieuwerburgh and Veldkamp (2006) and Ordoñez (2013) who employ it to explain steepness asymmetry in macroeconomic aggregates observed at business cycle frequencies. A similar mechanism is used in Saijo (2017), where agents learn about the efficiency of investments in an environment where uncertainty varies endogenously and has adverse effects on economic activity. Common across this literature is that endogenous variations in uncertainty imply state dependencies in the strength of agents’ responses to shocks. While also our model relies on such a type of mechanism, the studies above use it to explain empirical facts related to macroeconomic aggregates. We add to this literature by studying asymmetries in risk premia.====A procyclical speed of learning is not the only model mechanism that can facilitate matching the growth asymmetry of risk premia in the data. In principle, this would be possible for any mechanism that generates countercyclical risk premia, and that implies a state dependent response of risk premia so that these rise faster at the onset of a recession than they decline during the following expansion.==== This holds for example for models with occasionally binding constraints of the type in He and Krishnamurthy (2013). He and Krishnamurthy (2013) focus on the sharp increase in risk premia during asset market crisis. In their model, the level of financial intermediaries’ equity capital is important in driving growth asymmetries in risk premia: when equity capital is particularly low, as for example during an asset market crisis, losses within the financial sector have significant effects on risk premia and will trigger a surge in the latter. Yet, when equity capital is high, as during an expansion, losses have no effect on risk premia and they decline more gradually. While the particular focus of He and Krishnamurthy (2013) is on times of severe frictions in financial markets and substantial reductions in bank equity, this mechanism would be less prevalent for the business cycles that do not coincide with a financial crisis. We instead rely on procyclical variations in agent’s learning, which we find — consistent with the literature on varying signal-to-noise ratios discussed above — to be a feature over the entire sample.====The remainder of the paper is structured as follows. Section 2 provides an overview about the data. In Section 3 we provide details on the estimation of risk premium measures and document their growth asymmetry. Section 4 describes the model and Section 5 the calibration and computational details. Section 6 discusses the model mechanism that gives rise to asymmetries and results from simulations. Section 7 concludes.","Asymmetries in risk premia, macroeconomic uncertainty and business cycles",https://www.sciencedirect.com/science/article/pii/S0165188922000355,9 February 2022,2022,Research Article,89.0
"Bajaj Ayushi,Damodaran Nikhil","Monash University, 900 Dandenong Road, Caulfield East, VIC 3145, Australia,O.P. Jindal Global University, Sonipat Narela Road, Sonipat, Haryana 131001, India","Received 11 October 2021, Revised 1 February 2022, Accepted 4 February 2022, Available online 7 February 2022, Version of Record 26 February 2022.",https://doi.org/10.1016/j.jedc.2022.104329,Cited by (3),". We analyze India’s sudden demonetization of 86% of the cash in circulation with new notes gradually being replaced over the next several months. The welfare cost of the liquidity shock was equivalent to 1% of consumption owing to the slow remonetization process. Even though all consumers experienced a decline in welfare, its extent varied depending on the degree of cash dependence and the ability to switch to non-cash payments. The middle consumption deciles were disproportionately affected.","Consumer payment methods worldwide have undergone fundamental changes over time, with the most recent being a shift away from cash, to other electronic or digital means, such as debit cards and digital wallets. The use of these instruments involves trade-offs which influence a consumer’s payment choice. For consumers, carrying several small bills of cash is cumbersome while larger bills are more susceptible to counterfeiting. On the other hand, non-cash digital payments are usually devoid of such costs. However, using them requires a bank account in the least, which involves usage fees. Further, anonymous cash payments facilitate tax evasion whereas a shift towards digital payment methods generates a paper trail of tractable transactions. These factors together influence aggregate outcomes including the size of the shadow economy. It also creates a payments divide based on access and adoption, which has distinct distributional consequences.====In this paper, we model these features of payment instruments using a tractable monetary framework based on Lagos and Wright (2005) and Rocheteau and Wright (2005). We also include preference heterogeneity and taxation to characterize equilibrium regimes based on consumer’s choice of means of payments. This allows us to examine the consequences of this choice on consumption distribution, aggregate output, welfare and the shadow economy. If consumers transact in cash, they economize on their money holdings because they face a marginal carrying cost. However, the alternative of making non-cash payments is limited by usage costs, higher effective taxation and infrastructural constraints.====We apply this framework to analyze and quantify the heterogeneous impact of a unique monetary episode in India which led to a payments system shock. On November 8, 2016 the Government of India unexpectedly demonetized the two largest denomination bills comprising 86% of the existing currency in circulation, effective at midnight. Replacement of the demonetized currency with new notes took time and effort, imposing a significant strain on the payments system over the next several months. This large liquidity shock occurred in an otherwise stable macroeconomic environment and led to an immediate fall in aggregate output and welfare, as consumers were unable to undertake routine cash transactions. This aggregate impact has been analyzed and quantified in recent studies of this monetary episode using alternative approaches including Chodorow-Reich et al. (2020). In this paper, we delve into the mechanisms behind its distributional consequences on heterogeneous consumers by explicitly modeling their payment choices. We find that the aggregate welfare impact of the slow remonetization is comparable to that of 21.9% inflation. However, unlike inflation which most adversely impacts the top consumption deciles in the model, this monetary shock is felt most severely by the middle deciles.====Our framework has three key features that makes it particularly amenable to analyze this monetary episode. First, we model money as a means of payment with explicit micro-foundations i.e. money helps alleviate limited commitment and lack of double coincidence of wants. We employ a tractable environment where money is essential i.e. its presence makes superior allocations possible. In addition, we assume that money can be held in two forms. The first form is cash, which itself is available in two denominations involving a trade-off between carrying cost and counterfeiting – each low denomination bill is less susceptible to be counterfeited but it is cumbersome to carry many small bills. The other is a non-cash digital form of payment which is easy to carry and cannot be counterfeited but it incurs a usage cost, independent of transaction size. For instance, consumers need a bank account with fixed operational fees to access instruments such as debit cards.====The second key feature of our model is heterogeneity on consumer preferences. We find that it is typically not worthwhile to invest in non-cash payments for consumers with a lower level of consumption and they end up using cash. For instance in India, rural monthly consumption averaged Rs. 1430 as opposed to urban at Rs. 2630 for 2011-12. Thus, the model would suggest that rural consumers undertake more cash transactions as their optimal response. This is corroborated by the fact that rural bank deposits comprise only about 10% of the total bank deposits in India as reported by the Reserve Bank of India, while the rural share of Net Domestic Product is about 47% as reported in the National Accounts Statistics for the same period. Modeling consumption distribution via preference heterogeneity is a tractable way to capture such differences in payment methods and relative cash dependence, which helps analyze the differential impact of an aggregate demonetization shock.====Third, we assume that the sales tax levied by the government cannot be perfectly enforced. This allows us to analyze the link between payment choice, tax enforcement and the tax-evading shadow economy or a parallel black economy. Gomis-Porqueras et al. (2014) also presents a monetary model where a shadow economy emerges endogenously by assuming that cash transactions are not subject to a sales tax but all non-cash ones can be perfectly enforced. We allow cash transactions to have a (weakly) positive and lower probability of enforcement than non-cash ones. In Appendix B, we endogenously determine the probability of successfully enforcing taxes. These three features of the model as outlined above, make it well-suited to analyze this monetary episode, as the model explicitly addresses why cash matters and what determines its choice over non-cash payments.====We calibrate the model parameters to match key features of the Indian economy and determine the welfare impact of consumers suddenly finding their high denomination bills to be unacceptable for transactions. We then ascertain the aggregate and disaggregate impact of the slow and costly remonetization that followed by adjusting the cost of payments to match the pace of currency replacement. For our exercise, we do not vary the rate of redistribution of new notes across households as in Chodorow-Reich et al. (2020). Instead we study the response of heterogeneous consumers to the same liquidity shock based on their ability to switch to alternative payment methods. To measure its impact, we calculate the welfare cost of the policy shock. Our measure of the cost asks how much consumers would be willing to give up in terms of total consumption in order to go back to the ease of pre-demonetization payment systems. This cost was 1.0% of consumption owing to the slow and costly remonetization process as captured by the drop in M1, which remained around 25% below its level prior to the shock till January 2017. In contrast, the welfare cost of 10% inflation (as compared to 0% inflation) is 0.3% of consumption and that of 21.9% inflation is 1.0%.====One the objectives of demonetization in India was to combat tax evasion and reduce the size of the black or shadow economy as discussed in Lahiri (2020). We find that there was a temporary reduction in the size of the shadow economy in response to slow remonetization as it fell from 18.6% to 16.7% (i.e. output generated on which tax is evaded as a percent of output on which tax is not evaded). This happened on account of two factors, namely a switch to non-cash means of payments on which enforcement is higher and a reduction in output on which tax is evaded i.e. cash dependent output. However, inflation also acts as a tax on shadow economies and has a much smaller welfare cost. We find that a 10% inflation also leads to a decline in the size of the shadow economy to 13.5%.====In terms of household level impact, we find that the payments system shock led to a decline in household welfare for every group in every region but the magnitude of this fall varied. The impact was felt most by consumers with a high cash dependence who were unable to switch to non-cash means i.e. those in the middle consumption deciles. Consumers in the top deciles were less affected because they either previously used non-cash means of payments or later found it worthwhile to transition away from cash following the shock. In contrast, inflation affects households carrying higher money balances the most i.e. ones in the top consumption deciles. In reality however, inflation may act as a regressive consumption tax because the richer households usually hold a higher fraction of their wealth in illiquid assets and tend to undertake more credit transactions. See for example Erosa and Ventura (2002). Since we do not model illiquid wealth and credit transactions, the higher consumption deciles use more money balances, thereby being the most affected by inflation. In fact, in a model with illiquid wealth and access to credit, the differential impact of demonetization and slow remonetization might be even more stark as the richer households would have access to these other hedging instruments.",Consumer payment choice and the heterogeneous impact of India’s demonetization,https://www.sciencedirect.com/science/article/pii/S0165188922000343,7 February 2022,2022,Research Article,90.0
"Hevia Constantino,Macera Manuel,Neumeyer Pablo Andrés","Universidad Torcuato Di Tella, Argentina,Growth Lab, Harvard Kennedy School of Government, USA","Available online 5 February 2022, Version of Record 21 June 2022.",https://doi.org/10.1016/j.jedc.2022.104328,Cited by (3),We document the heterogeneous effect of Covid-19 on health and economic outcomes across socioeconomic strata in Bogotá. We assess its distributional impact and evaluate policy counterfactuals in a heterogeneous agent quantitative dynamic general equilibrium model intertwined with a behavioral epidemiological model.,"The goal of this paper is to understand the distributional and ==== consequences of Covid-19 in an unequal society. We document the epidemiological and economic disparities associated with SARS-CoV-2 in Bogotá. Then we build an integrated macroeconomic and epidemiological model with heterogeneous agents, which we use to interpret the data and simulate policy counterfactuals.==== We document the epidemiological and economic disparities associated with the spread of SARS-CoV-2 among these two groups in the city of Bogotá, where people in the low SES account for 89% of the population.====Our main epidemiological data source is the CoVida project, a large-scale epidemiological surveillance study that was conducted for nine months in Bogotá (====). Almost 60,000 people were tested for the virus (with an RT-PCR test) and responded to a questionnaire on their socioeconomic and demographic characteristics. A problem in studying the Covid-19 pandemic is that official data on infection are often unreliable because the data fail to detect most asymptomatic cases. The CoVida data are highly valuable in that they provide reliable estimates of infections and the personal background of those infected. We find that by February 2021, 54% of Bogotá’s population was infected by the virus, the prevalence ratio between the two groups was two, and the fatality rate of the low SES was 50% higher than for the high SES. The reliable estimate of the prevalence rates allows us to estimate infection fatality rates by SES. We find that, although infection fatality rates conditional on age are higher for the more vulnerable group, the unconditional infection fatality rates for the adult population are similar across the two groups because of age composition effects.====We record the economic impact of the Covid-19 epidemic, drawing on official data on economic activity and labor market indicators and on a private tracker of private consumption. Economic activity in Bogotá fell by 25% in April 2020, recovered sluggishly, and was 5% below trend in April 2021. Private consumption dropped on impact and recovered faster, exhibiting a similar pattern for people in the two social groups, with the caveat that consumption in the high SES was smoother. Employment fell by 30% and hours worked by more than 50% on impact, and hours recovered to a level that was 15% below trend. In the second quarter of 2020, during the shutdown, capital income was 30% below trend and labor income was 10% below trend. Thus, to the extent that people in the high SES hold most of the capital stock, their income losses would be higher.==== and ====. When the epidemic breaks out, agents decide their optimal exposure to the risk of contagion, and there is a mutual feedback between the resulting health dynamics and aggregate economic outcomes (wages, employment, consumption, asset and capital accumulation).====We follow ==== in assuming that exposure to contagion in the workplace and consumption venues is proportional to labor hours and consumption expenditures. We also assume that people adjust their social behavior not associated with these activities by wearing masks, socially distancing from loved ones, meeting outdoors, and so on. We model decisions balancing the risk of infection and the utility cost of this change in social behavior, reinterpreting the model in ====. A key difference between ====’s ==== model and ours is that we assume that when people are not working, they are consuming or engaging in other social interactions. Thus, reducing the labor supply to avoid infections is more effective if it is complemented with costly adjustments to non-work activities.====We find that in the competitive laissez-faire equilibrium the welfare cost of the epidemic outbreak is equivalent to a three-year reduction in consumption of 4.3% for the high SES and 4.9% for the low SES. These welfare costs stem from the expected cost of death, a strong reaction in social interactions, and modest changes in consumption and labor.====As Bogotá’s economic performance is inconsistent with the laissez-faire equilibrium, we consider, as our benchmark case, an economy with two policy interventions: a shutdown calibrated to track Bogotá’s output pattern and lump-sum transfers calibrated to those received by the city’s poor residents. In this case, the model is very good at predicting the epidemiological and economic patterns in the data. The welfare cost borne by high-SES agents becomes 11.4% of steady-state consumption for three years, and the one for the low-SES agents is equivalent to a three-year fall in consumption of 8.7%. Most of the fall in welfare stems from the inefficiency of the shutdown, which is very costly from an economic perspective and has little epidemiological benefit. The shutdown hurts the rich more than the poor, via the capital income channel, and the redistributive transfer further hurts high-SES individuals. To smooth consumption at the peak of the lockdown, low-SES agents sell some of their capital, paying an adjustment cost, and later repurchase it, again paying an adjustment cost.====In our model, redistributive lump-sum transfers naturally hurt the high-SES agents that pay for them to benefit the recipients. Macroeconomically, transfers have the expected effect of reducing aggregate labor and increasing consumption. Investment rises as a result of our assumption that low-SES agents do not have access to credit markets, so they choose to save the transitory income from the transfers by accumulating capital.====We exploit the richness of the CoVida epidemiological data, in combination with economic data, to internally calibrate through the simulated method of moments key epidemiological parameters and the preference parameters tied to the cost of social distancing. The rest of the parameters are exactly identified and chosen to match moments that correspond to the disease-free steady state.====We find that the basic reproductive number for Bogotá was 2.3 and, through a combination of endogenous behavior and economic restrictions, fell to 1.2 at the peak of the epidemic.==== Our internally calibrated epidemiological transmission rates imply that people in the low socioeconomic group are 41% more vulnerable to infection than those in the high SES. They are more vulnerable mainly because, while the transmission rates from people in the high SES to people in both groups are similar, people in the lower SES transmit the virus to somebody in their own group 95% more relative to transmission to their richer peers.====The model estimates that, at the peak of the pandemic, Bogotá’s inhabitants reduced their social interactions with a high risk of transmitting the virus, outside workplaces and shopping venues, by about 60%. At that point, the ==== of consumption with respect to social distancing along an indifference curve was close to 4%. People would pay 0.04% of their weekly consumption to reduce social distancing by 1%.==== and ====, ====, and ====. In addition to several modeling choices described above and in the text, the most distinguishing feature of our work is that we discipline the epidemiological features of the model with more reliable data on infections and fatalities. Our model delivers as a by-product the private value of a vaccine for each socioeconomic group. However, we do not compute the social planner’s allocation, so our approach does not take into account the externalities associated with vaccines, as in ==== and ====.====The rest of the paper is structured as follows. We document the epidemiological and economic impact of Covid-19 in Bogotá in ====, present the model in ====, the calibration strategy in ====, the simulation results in ====, the sensitivity analysis in section 6, and conclusions in ====.",Covid-19 in unequal societies,https://www.sciencedirect.com/science/article/pii/S0165188922000331,5 February 2022,2022,Research Article,91.0
Azzimonti Marina,"Stony Brook University and NBER, USA","Available online 2 February 2022, Version of Record 21 June 2022.",https://doi.org/10.1016/j.jedc.2022.104317,Cited by (0),None,"Boppart et al. (2021) present a macro SIR model with endogenous behavioral responses, in which individuals can adjust consumption and labor in sectors or activities that are perceived to be more risky (e.g. restaurants). I refer to these behavioral responses as the ‘fear factor’ effect. Because agents respond to aggregate infection risk, there is an interaction between the level of economic activity (the ECON part of the model) and the spread of infections (the EPI part of the model). Higher risk induces agents to substitute away from ‘social leisure’ (e.g. in-person leisure activities or consumption near other agents).====The authors show that the degree of heterogeneity among agents, and in particular their age, is relevant in designing optimal lockdown and vaccination policies. A key finding is that due to the presence of externalities, a decentralized vaccination strategy yields sub-optimal results. Households would prefer their older members to receive a vaccine first, because these are the ones subject to higher risk of severe illness or death. A planner, instead, would vaccinate younger members of the household before vaccinating older ones. Why? Because young agents do not internalize how their choices affect the decisions of older individuals in the population as a whole. As a result, young agents engage in too much economic activity, spreading the disease faster, whereas older individuals reduce their probability of infection by staying inside their homes. By vaccinating young individuals first, society is able to reduce the number of infections and deaths, while at the same time permitting older agents to increase their consumption of social leisure. Notice that this result would not arise in an environment that does not feature ‘fear factor’ effects.====In this note, I briefly describe their model in Section 2, and contrast some of their key assumptions with empirical evidence in Section 3. In Section 4, I discuss how taking into consideration the stability of human relationships can affect their results, drawing insights from network models. Section 5 concludes.","Comment on “Integrated epi-econ assessment of vaccination,” by Boppart, Harmenberg, Krusell, and Olsson",https://www.sciencedirect.com/science/article/pii/S0165188922000227,2 February 2022,2022,Research Article,92.0
"Panovska Irina,Ramamurthy Srikanth","School of Economic, Political and Policy Sciences, University of Texas at Dallas, Richarson, TX 75080, United States,IMF and Loyola University Maryland, United States","Received 23 May 2021, Revised 27 January 2022, Accepted 31 January 2022, Available online 2 February 2022, Version of Record 12 February 2022.",https://doi.org/10.1016/j.jedc.2022.104327,Cited by (1),We incorporate adaptive learning-based ,"Beyond the overt co-movement between inflation and output, to what extent does inflation provide a meaningful signal about the output gap? Does this information change based on different assumptions about how inflation expectations are formed? In this paper we analyze the role of inflation expectations and their signals about the output gap based on an Unobserved Components (UC) model where inflation expectations are formed using an adaptive learning algorithm. The questions about the link between output and inflation reflect some of the central tenets of economics: that output can be separated into a trend component and a cyclical component, that cyclical fluctuations in output affect fluctuations in inflation through a Phillips Curve type relationship, and that movements in inflation and inflation expectations are informative about the movements in real activity and economic slack, that is, the output gap.====The literature has taken different approaches when studying these questions, depending on whether the objective of a study was to identify structural parameters or to obtain information about the output gap. The vast literature that studies the New Keynesian Phillips curve (NKPC), inter alia, Berardi and Galimberti (2017); Gali (2003); Galí et al. (2005); Nason and Smith (2008), typically takes the measure(s) of slack as known observed quantities by using marginal costs, and detrended output or unemployment based on some specific, usually linear, trend-cycle decomposition. Broadly, this strand of the literature focuses on the structural parameters that drive inflation.====The question that goes in the opposite direction is similarly relevant both from an academic and from a policy point of view: if the output gap is a latent variable that is not observed by the researcher, do inflation and inflation expectations provide information about the output gap? Recent developments in the output trend-cycle decomposition literature note that there is substantial uncertainty when it comes to the cyclical component of output (Hamilton, 2018, Kamber, Morley, Wong, 2018) and that conventional univariate detrending methods can send very different signals about the size and even the sign of the output gap. The rise of the New Keynesian Phillips Curve provided a unified framework to model the joint dynamics of inflation and output in a more formal framework. While early versions of the purely forward looking NKPC struggled to match the data, its many subsequent incarnations, particularly the hybrid NKPC (hereafter HNKPC) with both a forward and backward looking component, strive to improve upon the empirical fit of the relationship between output and inflation that is observed in practice. Similarly, most of the models that have attempted to extract the output gap by using inflation expectations have resorted to proxying inflation expectations by using surveys or financial market based expectations.==== In this study we model inflation expectations in a HNKPC setup following an adaptive learning process, similar to the approach in Berardi and Galimberti (2017) but, unlike in that paper, we treat the output gap as a latent variable. This allows us to estimate both the inflation expectations and the output gap from the data while still retaining endogenous inflation expectations. A secondary benefit of our approach is that it allows one to consider other measures of inflation for which survey measures of expectations either do not exist or are available only for a short sample (for example core measures that try to strip out the effects of supply shocks).====Our modeling approach is motivated by two distinct findings in the literature. The first stems from empirical evidence that tilts in favor of adaptive learning compared to Rational Expectations (RE). For instance, Milani (2020) demonstrates this in an inflation targeting framework, Gelain et al. (2019) in a Dynamic Stochastic General Equilibrium (DSGE) framework; and Berardi and Galimberti (2014) demonstrate the superior performance from a forecast standpoint. Secondly, Coibion and Gorodnichenko (2015) show that errors from survey expectations are predictable in some cases and that the null hypothesis of full information rational expectations is commonly rejected for survey measures - in particular in the period during and following the Global Financial Crisis (GFC). They indicate that these commonly observed rejections of the null of full information rational expectations most likely reflect deviations from full-information rather than departures from rational expectations. Similarly, Hasenzagl et al. (2020) show that there is a persistent deviation from rationality in the consumer survey of inflation expectations, mostly explained by oil price fluctuations.====Our approach is more readily compared to the model in Basistha and Nelson (2007) (BN2007, henceforth) than a learning-based DSGE model ((Milani (2007), for example). For one, except for the use of survey expectations, our models are, at least in principle, closely aligned together. Secondly, there are no additional exogenous processes in our model that commonly appear in GE models (for example, the interest rate rule). This deliberate choice of parsimony allows us to focus exclusively on the time variation in inflation dynamics owing to the learning algorithm and extract the feedback from inflation expectations to analyze output cycles.====Several interesting and novel findings for the data sample 1960Q1 to 2019Q3 are worth highlighting. First, the implied output gap from our model not only has a smaller amplitude compared to that from survey expectations, but also departs from it significantly in the post-1984 period, particularly during the turning points around business cycles. Specifically, our estimated gap indicates that the last three recessions were at least partially driven by drops in the trend component of output. Thus, with regard to the recent debate on changes to potential output following the Great Recession, we find that the initial drop in output is driven by a decline in potential, followed by an adjustment to this initial negative shock. These findings are robust even with trend breaks to the permanent component of output, when using alternative measures of inflation, and when relaxing the assumption of neutrality. Moreover, these findings are not driven by a breakdown in the HNKPC: we find a relatively flat but significant Phillips curve coefficient. Many of our results from the learning model are comparable to the purely-reduced form estimates from Morley et al. (2003) and those from Sinclair (2009), where trend movements drive most of fluctuations in output during expansions. Thus, the adaptive learning based HNKPC-UC model provides some structural support for the estimates from their econometric models.====Second, while learning based inflation forecasts largely shadow survey expectations in the pre-Volcker era, they avoid the persistent overshooting of the latter during the initial stages of the financial crisis. Given the reduced form inflation dynamics in our model, this finding resonates with Stock and Watson (2007). Our model also has good out-of-sample performance when it comes to forecasting inflation, compared to a model that includes survey expectations or to a bivariate UC model that does not incorporate expectations.====The rest of this paper is organized as follows. In the following section we provide a brief overview of the literature. In Section 3 we present our learning-based NKPC-UC model. Section 4 discusses the results. Comparative insights pertaining to alternative specifications, alternative measures of inflation, and comparisons with other models are provided in Section 5. Brief concluding remarks are presented in Section 6.",Decomposing the output gap with inflation learning,https://www.sciencedirect.com/science/article/pii/S016518892200032X,2 February 2022,2022,Research Article,93.0
"Chen Zilin,Chu Liya,Liang Dawei,Tu Jun","School of Finance, Southwestern University of Finance and Economics, Chengdu 611130, China,East China University of Science and Technology, Shanghai 200237, China,Research Institute of Economics and Management, Southwestern University of Finance and Economics, Chengdu 611130, China,Lee Kong Chian School of Business, Singapore Management University, 178899, Singapore","Received 24 November 2021, Revised 21 January 2022, Accepted 31 January 2022, Available online 1 February 2022, Version of Record 9 February 2022.",https://doi.org/10.1016/j.jedc.2022.104325,Cited by (2),"Using a novel data set to identify geographic peer firms based on the locations of both firms’ headquarters and material subsidiaries, we show that returns on geographical peers have strong ","Geographic locations are essential for pricing assets. Stock returns on firms that operate in the same headquarters state tend to move together (Pirinsky and Wang, 2006). Parsons et al. (2020) show that the return comovement of firms headquartered in the same state extends to a predictable lead-lag effect. The underlying assumption in these two papers is that all economic activities of one specific firm occur in a single place. In the real world, though, many public firms do not operate in only one location. A public firm may headquarter in a state but operate subsidiaries in other states, and so value-relevant information about the firm is also likely to be geographically dispersed (Bernile et al., 2015). Meanwhile, the information flow within geographically dispersed firms may not be as efficient as in geographically concentrated firms (Aarland, Davis, Henderson, Ono, 2007, Giroud, 2013). Investing in such firms can present challenges to investors with limited ability to process value-relevant information, and this may lead to market inefficiencies in the form of gradual incorporation of information into stock prices (Hong, Stein, 1999, Hong, Stein, 2007). We therefore hypothesize that valuable information arising from geographically distributed economic activities is slowly incorporated into stock prices.====In this study, we test this hypothesis by examining whether returns on geographic peers based on the locations of both headquarters and economically relevant subsidiaries are useful for predicting stocks returns on focal firms. Consistent with our hypothesis, we find that the stock prices for focal firms are adjusted to shocks to such geographic peers for up to four months. Focal firms whose geographic peers experience higher (lower) returns in the current month will earn higher (lower) returns in the next month. A value-weighted trading strategy that buys stocks with the highest geo-peer returns and shorts stocks with the lowest geo-peer returns earns a monthly Fama and French (2015) five-factor alpha of around 0.6%. This return predictability is robust to an extensive set of controlling variables and other cross-sectional momentum effects, and is hard to square with risk explanations. We find that the return predicative power is stronger among firms that receive less attention and that are more costly to arbitrage. Therefore, our results are consistent with slow adjustment of value-relevant information from the geographic peers to the stock prices of the focal firms.====In the empirical analysis, we obtain the locations of firms’ headquarters and economically relevant subsidiaries from Exhibit 21, a section within or attached to 10-K fillings that provides the names and locations of firms’ headquarters and significant subsidiaries, particularly material subsidiaries.==== These data enable us to connect firms using the time-varying locations of both headquarters and material subsidiaries that firms disclose in Exhibit 21, while the headquarters location variable in Compustat captures only the most recent information. We then construct a proxy for geographic information using returns of geographic peer firms that are linked through the locations of material subsidiaries or headquarters. Specifically, we construct a frequency-weighted return of state-level portfolio returns on all geo-linked firms (GPRET). These state-level portfolio returns are computed as equal-weighted averages of stock returns on the geographic peers. The frequency, which captures the economic importance of one state to the focal firm, is the number of times of that a given state is mentioned in the annual reports. By construction, this GPRET captures general good or bad news for a focal firm via aggregating all the relevant information pertaining to geographically linked firms. If investors have limited attention or are constrained in processing such information, the GPRET should predict focal firms’ stock returns.====We test the predictive power of the geographic information by forming a trading strategy. Specifically, at the end of each month, we sort firms into deciles based on GPRET. We then form a long-short portfolio that goes long on the decile portfolio with the highest GPRET and shorts on the decile portfolio with the lowest GPRET. After controlling for the Fama and French (2015) five-factor factors, we obtain a 0.45% (====) monthly abnormal return from an equal-weighted long-short portfolio, or a 0.60% (====) abnormal return from a value-weighted portfolio. We refer to this return predictability as “geographic momentum”. We further confirm the return predictability of the long-short GPRET portfolio is robust to a number of prominent factor models.====Furthermore, the GPRET effect is more pronounced than a similar geographic momentum effect documented by Parsons et al. (2020), who find that returns of peer firms that are headquartered in the same state are significantly associated with a focal firm’s future returns. If we follow Parsons et al. (2020) and define geographic peers based on the locations of headquarters only, an equal-weighted portfolio based on the returns of these peers could earn similarly significant returns as in Parsons et al. (2020), while a value-weighted portfolio could not generate any significant results. In sharp contrast, if we use the locations of material subsidiaries only to construct the GPRET, the return predictability remains significant when portfolios are value-weighted but slightly weaker than those based on the locations of both headquarters and material subsidiaries. Hence, the geographic momentum effect documented by Parsons et al. (2020) may be a special case of our more comprehensive GPRET effect.====We also conduct Fama and MacBeth (1973) regressions to confirm that the return predictive power of GPRET is not driven by other firm’s characteristics. After controlling for other well-known anomalies, including market beta, size, book-to-market ratio, medium-term momentum, short-term reversal, gross profitability, asset growth, turnover ratio, idiosyncratic volatility, and illiquidity ratio, the return predictability of GPRET remains significant. In particular, a one-standard-deviation increase in GPRET leads to 0.09% increase in a focal firm’s return, which is also economically significant. To further distinguish the GPRET effect from previous findings, we follow Moskowitz and Grinblatt (1999) and Daniel and Titman (1997) to control for effects related to industry and firm characteristics. Specifically, we adjust stock returns using the industry returns and DGTW matched portfolio’s returns in our regression. We obtain similar and significant coefficients on the GPRET in these settings, suggesting that the GPRET return predictive power is not driven by these two effects.====To ensure that our geographic momentum effect is not a rediscovery of other well-known cross-sectional momentum effects, such as industry momentum effect (Moskowitz and Grinblatt, 1999), customer and supplier industry momentum (Menzly and Ozbas, 2010), complicated firm momentum (Cohen and Lou, 2012), technology momentum (Lee et al., 2019), connected-firm momentum (Ali and Hirshleifer, 2020), common board momentum (Burt et al., 2020), and common institutional momentum (Gao et al., 2017), we also conduct Fama and MacBeth (1973) regressions controlling for these momentum effects directly. In general, controlling for these momentum effects, especially connected-firm momentum does not have any impact on our geographic momentum effect, while geographic momentum effect based on locations of headquarters are explained by connected-firm momentum effects, according to Ali and Hirshleifer (2020). The only exception is the customer momentum effect, and one possible reason for this is that the sample size becomes too small to generate meaningful results if we include customer returns in our regression.====To better understand the mechanism that underlies GPRET effect, we examine the sensitivity of GPRET effect to firm-specific characteristics associated with investor inattention and the costs of arbitrage. We find that return predictability is more pronounced among focal firms that receive less attention (i.e., firms that have lower media coverage and analyst coverage), and that are more costly to arbitrage (i.e., firms that have higher idiosyncratic volatility and higher illiquidity ratio). These results are consistent with sluggish price adjustment of information embedded in the geographic network.====We also examine stock price reactions to subsequent earnings announcements in one-day and three-day windows. This test has been widely used in previous literature to separate mispricing from risk-based explanations (e.g., Engelberg, McLean, Pontiff, 2018, La Porta, Lakonishok, Shleifer, Vishny, 1997, Sloan, 1996). The idea is straightforward. Earnings announcements help investors update expectation biases. If the predictable returns are driven by changes in potential risks, we expect that the returns are evenly affected on different trading days. In contrast, if the return predictability is due to mispricing, it will be stronger when it is closer to earnings announcement days. We observe that roughly 16% of abnormal returns of the GPRET strategy are realized in the three-day window around earnings announcement dates. This result is hard to square with the risk-based explanation.====Our paper contributes to two strands of the existing literature. First, our paper is related to studies on the relationship between geographic locations and stock returns. Pirinsky and Wang (2006) document that stock returns comove between companies that are headquartered in the same geographic area. Garcia and Norli (2012) show that truly local firms earn significantly higher returns than geographically dispersed firms. Korniotis and Kumar (2013) and Smajlbegovic (2019) find that local economic conditions or economic activities have strong predictability for local stock returns. Our paper is also closely related to Parsons et al. (2020), who document a geographic lead-lag effect using information about the locations of firms’ headquarters. However, our paper differs from their paper in several important respects. First, Parsons et al. (2020) identify firms’ locations using ZIP codes from Compustat, which contains only the most recent information but ignores changes over time, and thus their GPRET can be inaccurate if a firm changes its headquarters location. In contrast, we retrieve time-varying location information from firms’ 10-K annual reports such that we are able to identify all relevant locations accurately and in a timely way. In addition, as shown by Bernile et al. (2015), a typical U.S. public firm has economic interests in five states beyond its corporate headquarters location, while we find that our GPRET, which incorporates locations beyond firms’ headquarter locations, is able to capture more value-relevant information from the distant locations in which firms’ economically relevant subsidiaries are located. Therefore, another key innovation in our paper is that we show significant results for both equal- and value-weighted portfolios using our GPRET, while results using the headquarters-based GPRET like the one in Parsons et al. (2020) are significant only for equal-weighted portfolios.====Our paper is also related to an emerging body of literature on cross-sectional return predictability. Studies in this area depart from the rational expectations framework and have utilized behavioral explanations, combined with market frictions, to rationalize the predictability of returns (Nagel, 2013). Alongside various theoretical explanations of return predictability in this behavioral framework (Hong, Stein, 1999, Shleifer, Vishny, 1997), a growing number of studies offer empirical explanations as to how information pertaining to linked firms may be slowly incorporated into the stock prices of focal firms. Moskowitz and Grinblatt (1999) find that past industry returns are associated with future stock returns after controlling for individual stock momentum. Cohen and Frazzini (2008) find that past returns on firms’ principal customers forecasts future returns on focal firms after controlling for industry and cross-industry momentum effects. Similarly, using data on flows of goods to and from industries, Menzly and Ozbas (2010) find that past customer and supplier industry returns predict future stock returns. Cohen and Lou (2012) use operating segment data to show that single-segment firm returns can predict returns on multi-segment firms operating in the same industries. Jiang et al. (2019) find that stocks that are frequently co-mentioned in online forums tend to exhibit return comovement, and such comovement can be extended to a cross-firm return predictability. Lee et al. (2019) find a lead-lag relationship in returns between technology-linked firms. Parsons et al. (2020) find that return comovement of firms headquartered in the same state extends to a lead-lag effect. Hameed et al. (2015) document that information flows among stocks that are covered by the same analysts. Similarly, Ali and Hirshleifer (2020) find that the above momentum spillover effects are a unified phenomenon captured by the shared analyst coverage. A basic theme of these papers is that investors are subject to limited attention and therefore unable to process information about linked firms in a timely fashion. Our paper also contributes to this framework by showing that limited investor attention causes delayed stock price responses to the value-relevant information embedded in the geographic network.====The remainder of this paper is organized as follows. In Section 2 we describe the data and variables. Section 3 presents our main results. In Section 4 we explore the underlying mechanism behind the GPRET effect. In Section 5 we report results for earnings announcement returns predictions. Section 6 concludes.",Far away from home: Investors’ underreaction to geographically dispersed information,https://www.sciencedirect.com/science/article/pii/S0165188922000306,1 February 2022,2022,Research Article,94.0
"Fernández-Villaverde Jesús,Jones Charles I.","University of Pennsylvania,Stanford GSB","Available online 29 January 2022, Version of Record 21 June 2022.",https://doi.org/10.1016/j.jedc.2022.104318,Cited by (19),"We use data on deaths in New York City, Madrid, Stockholm, and other world cities as well as in various U.S. states and other regions and countries to estimate, quickly and with limited data, a standard epidemiological model of COVID-19. We allow for a time-varying contact rate in order to capture behavioral and policy-induced changes associated with social distancing. We simulate the model forward to consider possible scenarios for various countries, states, and cities, including the potential impact of ==== on re-opening.","The sudden arrival of COVID-19 in the winter of 2020 highlighted the importance of estimating a standard epidemiological model of the epidemic quickly and with limited data. In this paper, we show how to tackle this challenge. We use data on deaths in New York City, Madrid, Stockholm, and other world cities as well as in various U.S. states, countries, and regions around the world during the first half of 2020 to estimate a SIRD model of COVID-19. Relative to existing frameworks, our contributions are:====We study a standard model of COVID-19 using common tools in ====, and then we analyze its main quantitative implications in ways that resemble how economists study other dynamic models. Our exercise can help us understand where a simple SIRD model has difficulties fitting observed patterns in the data and points out avenues for improvement while maintaining the virtues of simplicity and parsimony.====In the interest of space, we will report a very short summary of our results, up to mid-May 2020. By the end of May, the first wave of the epidemic was over in many cities, regions, and countries. Later waves of the epidemic need, to be analyzed in more detail, using models with time-varying parameters, such as the one in ====, and, consequently, much more powerful econometric techniques. Nonetheless, we have an online dashboard, ====, that reports data extended until October 9, 2020 for around 100 cities, states, and countries.","Estimating and simulating a SIRD Model of COVID-19 for many countries, states, and cities",https://www.sciencedirect.com/science/article/pii/S0165188922000239,29 January 2022,2022,Research Article,95.0
Mohimont Jolan,"National Bank of Belgium and University of Namur, Belgium","Received 10 January 2020, Revised 21 January 2022, Accepted 23 January 2022, Available online 25 January 2022, Version of Record 8 February 2022.",https://doi.org/10.1016/j.jedc.2022.104316,Cited by (0),"This paper evaluates the welfare cost of business cycles and the effects of ==== in a ==== tailored to a small open emerging economy. The model generates rich business cycle fluctuations, features labor market idiosyncratic risks and accounts for imperfect financial and capital markets inclusion. In this context, households excluded from financial and capital markets experience larger costs of business cycle fluctuations due to their inability to hedge against labor market idiosyncratic risks. Different degrees of exposure to different types of risks generate divergent preferences regarding the conduct of monetary policy. While a strong response to ==== deviation from target maximizes welfare for included households, excluded households benefit the most from unemployment and wage stabilization policies.","The measure of the welfare cost of business cycles and the design of appropriate stabilization policies has received substantial attention in advanced economies. The same is not true of emerging markets. However, since works by Lucas, 1987, Lucas, 2003 and Imrohoruglu (1989), it has become clear that two crucial determinants of the welfare cost of business cycles are aggregate consumption volatility and the ability - or not - to hedge against idiosyncratic risks. In emerging markets, business cycle fluctuations tend to be stronger. Moreover, a large proportion of households are excluded from financial markets and own little to no wealth. They are therefore highly exposed to large aggregate and idiosyncratic income risks. It is therefore surprising that most papers measuring the welfare costs of business cycles and dealing with the design of monetary policies disregarded emerging markets.====This paper aims to fill this gap by evaluating the welfare cost of business cycles and the effects of monetary policies in a DSGE model tailored to a small open emerging economy. The model generates rich business cycle fluctuations, features labor market idiosyncratic risks and accounts for imperfect financial and capital markets inclusion. In this context, households excluded from financial and capital markets experience larger costs of business cycle fluctuations due to their inability to hedge against labor market idiosyncratic risks. Different degrees of exposure to different types of risks generate divergent preferences regarding the conduct of monetary policy. While a strong response to the CPI inflation rate deviation from target maximizes the welfare of included households, excluded households benefit the most from unemployment and wage inflation stabilization policies. As an illustration, the model is applied to South Africa.==== As will become clear shortly, the model is sufficiently general to fit small open commodity producers with imperfect financial markets.====The core of the model follows the small open economy (SOE) model developed by Adolfson et al. (2007) with four main extensions that are meant to take the specificities of an emerging economy into account and to introduce idiosyncratic risks. First, firms produce two different types of goods: commodities and secondary products, which capture the empirical importance of commodity price fluctuations in many emerging economies (e.g. Mendoza, 1995, Kose, 2002 and Houssa et al., 2019). Second, I introduce search and matching frictions (hereafter SAM) with staggered wage bargaining following Bodart et al. (2006), Gertler et al. (2008) and Thomas (2008). These rigidities generate idiosyncratic income risks in the labor market. Unemployment risks, sectoral wage differences (between the commodity and secondary sectors) and nominal wage rigidities generate dispersion in households’ labor incomes. Third, there are two categories of households that differ with respect to access to capital and financial markets. Households excluded from these markets own no physical capital or financial wealth and simply consume their entire labor income in every period as in Mankiw (2000). In addition, they do not trade in state-contingent asset markets and are unable to insure against idiosyncratic risks. Labor income idiosyncratic risks thus translate into consumption dispersion for this category of agents. Fourth, I explicitly model unemployed households’ revenues. In the baseline, unemployed households receive pro-cyclical social transfers as observed in many emerging economies (Michaud and Rothert, 2018). In an alternative scenario, unemployment benefits are tied to past wages, which fits the South African institutional framework described in the appendix. In another alternative case, I assume that the unemployed do not receive benefits, but rather work in an informal sector. This experiment captures the importance of this sector (e.g. Fernandez and Meza, 2015) and the low unemployment benefits coverage in some low- and middle-income countries (e.g. OECD, 2011). Within this rich framework, monetary policy is modeled as a Taylor rule with interest rate smoothing. The monetary authority responds to CPI inflation, wage inflation and unemployment rates deviation from their respective targets. This rule allows to evaluate the trade-off between stabilizing inflation and mitigating labor income idiosyncratic risks.====This paper contributes to the long debate on the magnitude of the welfare cost of business cycle fluctuations with an analysis on an emerging economy. Business cycles in emerging and developing economies display more volatility (see Agenor, McDermott, Prasad, 2000, Rand, Tarp, 2002, Neumeyer, Perri, 2005, Aguiar, Gopinath, 2007 and Male, 2011). In this context, Pallage and Robe (2003) and Houssa (2013) demonstrate that excess consumption volatility translates into a higher welfare cost. Replicating Lucas’ experiment, Pallage and Robe (2003) report a welfare cost of 0.06% of consumption for South Africa (about three times as much as in the US) with a coefficient of relative risk aversion slightly larger than in this paper. However, these studies disregarded unequal wealth distribution and households’ imperfect integration into financial markets preventing them from absorbing idiosyncratic shocks. This gap in the literature is important, because financial exclusion and unequal wealth distribution are important characteristics of emerging markets that are likely to affect the welfare cost of business cycle fluctuations in these economies. In the literature applied to the US, Krusell et al. (2009) use an heterogenous agents model (where agents differ with their degree of patience) with unemployment risks and a borrowing constraint. They find that a few impatient, poor and financially constrained households are largely exposed to business cycle fluctuations with cost as high as 4% of consumption in their baseline experiment. This paper differs in the sense that it additionally considers sectoral and staggered wage bargaining risks, and relies on a structural model whose parameters are estimated on a large set of observed variables. As argued by Otrok (2001), estimated models bring discipline to the choice of parameters and capture agents endogenous responses to shocks.====I estimate the welfare costs of business cycles for included and excluded households at 1.27% and 4.19% of their steady-state level of consumption, respectively. The welfare cost is much higher for households excluded from asset markets, especially because of their inability to insure against labor market idiosyncratic risks. Indeed, while their costs are approximately three times greater than those incurred by included households, they would be lower if they could eliminate the idiosyncratic risk. The underlying mechanism is based on the interaction between business cycle fluctuations, idiosyncratic risks in the labor market and financial markets exclusion. In this paper, business cycle fluctuations exacerbate idiosyncratic risks via three channels. First, the outcome of the wage bargain depends on the state of the business cycle, which generates wage dispersion within each sector due to the staggered wage bargaining framework.==== Second, business cycle fluctuations cause sectoral wage differences. Third, business cycle fluctuations push up the average unemployment rate and generate volatility in replacement incomes as they rise during booms and fall during recessions.==== These interactions between the business cycle and idiosyncratic risks generate more volatility in labor incomes at the household’s level. For households excluded from financial markets behaving as hand-to-mouth consumers, it translates into substantial consumption fluctuations and welfare losses.====This paper also contributes to the design of optimal simple monetary policy rules, with a focus on the trade-offs between price, wage and unemployment stabilization. Following the move towards inflation targeting, many papers studied the desirability of inflation targeting, compared to exchange rate management policies in emerging economies (e.g. Rebucci and Ghironi, 2002 and Devereux et al., 2006). However, the trade-offs between CPI inflation and wage inflation or unemployment has been largely understudied. This is an important gap in the literature, because the above-mentioned specificities of emerging markets could influence this trade-off.====I find that different degrees of exposure to labor income idiosyncratic risks generate divergent preferences regarding monetary policy. To highlight this monetary policy trade-off, I evaluate optimal simple monetary policy rules, for included and excluded households, respectively.==== While a strong response to CPI inflation deviation from target maximizes welfare for included households, excluded households would benefit the most from unemployment and wage inflation stabilization. This trade-off faced by the monetary authority is robust to changes in the value of key parameters, to different assumptions governing unemployed households’ replacement incomes and to different types of (domestic and external) shocks driving the business cycle. Regardless of the source of business cycle fluctuation considered (such as domestic labor market, TFP and cost-push shocks or commodity price shocks), excluded households always favor a more aggressive response to unemployment and wage fluctuations. It is also interesting to note that the nature of unemployed household’s replacement incomes can influence these preferences: when unemployment benefits are constant (in real term), the importance of labor income idiosyncratic risk diminishes and so does the trade-off for monetary policies, although they do not vanish completely. These results highlight that emerging economies (characterized by lower rates of financial inclusion and weaker social safety nets) are more likely to benefit from deviation from pure inflation targeting rules compared to advanced economies. Finally, the welfare gains associated with these optimal simple rules compared to a benchmark (the estimated Taylor rule followed by the monetary authority) are relatively large, which highlights the important role of monetary policies in emerging economies.====This paper’s final contribution is to consider the effect of labor income idiosyncratic risks on consumption dispersion and welfare for hand-to-mouth consumer. To the best of my knowledge, this has never been applied to a large scale DSGE model, although hand-to-mouth consumers are standard ingredients in this literature. For example, Prasad and Zhang (2015), Iyer (2016), Ascari et al. (2017) and Cugat (2019) all introduced two categories of households, including financially excluded hand-to-mouth consumers when studying (optimal) monetary policy. However, they do not consider consumption dispersion within the excluded household’s group, because they do not introduce labor income idiosyncratic risks or assume some form of risk sharing within this group. In this paper, idiosyncratic risks turn out to play an important role. This idea relates to Heterogenous Agents New-Keynesian (HANK) models, which generate rich income, wealth and consumption dispersions (e.g. Kaplan, Moll, Violante, 2018, Bayer, Luetticke, Pham-Dao, Tjaden, 2019, Gornemann, Kuester, Nakajima, 2016 and Ravn and Sterk, 2016). Yet, HANK models come at the cost of complex solution methods currently limiting the set of shocks, frictions and estimation methods that they can handle. Consequently, these models generate less refined business cycle fluctuations. In contrast, representative agent NK-DSGE models can reproduce business cycle fluctuations, due to the rich set of shocks and frictions that were gradually introduced in these models, in an effort to match aggregate fluctuations (e.g. Christiano, Eichenbaum, Evans, 2005, Smets, Wouters, 2007). However, idiosyncratic risks are left in the background as households trade in state-contingent asset markets to hedge against those risks. This paper offers a compromise between those two strands of literature, by considering the impact of idiosyncratic risks for excluded households in an estimated DSGE model with a rich set of shocks and frictions.====I find that shocks originating from the labor market (such as shocks related to the wage bargain and employment creation) disproportionately affect households excluded from financial markets, while TFP and cost-push shocks are especially costly for included households (which own the firms). In contrast, commodity price shocks largely affect both types of households. These latter results illustrate that the identification of shocks can have a large impact on these estimates. This is a clear advantage of this method compared to HANK models that typically rely on a much smaller set of calibrated shocks.====The remainder of this paper is organized as follows. Section two presents the model and section three the empirical strategy. Section four gives the results. Finally, section five concludes.",Welfare effects of business cycles and monetary policies in a small open emerging economy,https://www.sciencedirect.com/science/article/pii/S0165188922000215,25 January 2022,2022,Research Article,96.0
"Kang Junqing,Lin Shen,Xiong Xiong","Lingnan (University) College, Sun Yat-sen University, No. 135 Xingangxi Road, Haizhu District, Guangzhou 510275, China,College of Management and Economics, Tianjin University, No. 92 Weijin Road, Nankai District, Tianjin 300072, China","Received 11 October 2021, Revised 14 January 2022, Accepted 19 January 2022, Available online 20 January 2022, Version of Record 4 February 2022.",https://doi.org/10.1016/j.jedc.2022.104313,Cited by (2),"Previous studies of the U.S. market regard short-term reversal as compensation for liquidity provision. However, we find that intraday reversal has no significant dependence on stock liquidity in the Chinese market. Hence, based on a stylized framework, we propose an alternative explanation: irrational uninformed liquidity providers, who underestimate the information component in the ==== due to physiological anchoring, trade against previous price movement, which generates an opposing price pressure. The empirical results confirm this explanation of liquidity oversupply (from irrational uninformed liquidity providers). The negative correlation between previous intraday returns and future returns in the Chinese market is reversed once we extend the holding period. This indicates that reversal is a pricing error due to excessive liquidity provision from uninformed retail traders instead of a price correction from a temporary price concession due to a lack of liquidity.","Numerous studies have shown that short-term return reversal is both robust and economically significant.==== An illiquidity-based explanation that a price reversal serves as risk compensation for investors who provide liquidity has received considerable attention in the literature.==== Indeed, in the U.S. market, we find that the returns of illiquid stocks==== are more likely to reverse than those with greater liquidity. For instance, when sorting all stocks into equal-weighted portfolios based on previous intraday return (====) and market capitalization, the difference between the sequential intraday return spread (returns of the high ==== portfolio minus those of the low ==== portfolio) for the most liquid stocks and that for the most illiquid stocks is 39.9 basis points (BPs) per day, with a t-statistic equal to 15.6. Our results are therefore complementary to the findings in Avramov et al. (2006), Nagel (2012), Da et al. (2014) and suggest that liquidity shocks are particularly relevant to cross-sectional intraday return reversal.====Although the U.S. market has some of the worlds largest stock exchanges and most liquid stocks, the adequate provision of liquidity is concentrated among the largest and most liquid firms (see Fig. 3 for further details). Unlike the classical institutional investor-dominated structure in Western countries, the Chinese stock market is characterized by a majority of retail traders, most of whom trade frequently and lack sophisticated financial education backgrounds.==== However, the market liquidity for small, high turnover and illiquid stocks in the U.S., from which standard reversal strategy profits mainly derive, is lower than that in the Chinese market. Hence, this paper attempts to provide insight into the following questions: Is there intraday reversal in the Chinese market considering the above observations indicating excessive liquidity supply? What drives this intraday reversal if intraday reversal does not depend on stock liquidity?====To start the analysis, Section 2 presents a stylized framework showing how a return reversal can occur as the result of either a lack of liquidity or excessive liquidity provision (from irrational uninformed liquidity providers).==== The baseline framework provides insight into the illiquidity-based explanation and focuses on two market participants: 1) an informed speculator who demands liquidity with exogenous intensity and 2) a market maker who provides liquidity based on the aggregate order flow and his or her own risk attitude. Informed trades lead to a temporary price concession that, when absorbed by market maker, results in a reversal in price that serves as compensation for liquidity provision. In contrast, the extended stylized framework focuses on the liquidity oversupply explanation and introduces a new type of trader: irrational uninformed liquidity providers, such as retailed investors in the Chinese market, who underestimate the information component in the equilibrium price due to physiological anchoring. These additional traders irrationally trade against the previous price movement, which creates opposing price pressure at the intraday level and forms a return reversal pattern.====A natural question follows: do illiquidity-based and liquidity oversupply explanations play different roles in driving intraday return reversal? We recognize that under each explanation, the reversal plays a different role in the return dynamics, either the correction of a temporary price concession due to a lack of liquidity (illiquidity-based explanation) or a pricing error due to excessive liquidity from irrational uninformed retail traders who underreact to news (liquidity oversupply explanation). Hence, future returns should also respond differently based on these two explanations. Within the liquidity oversupply framework, we would expect the negative correlation between previous intraday returns and future returns to subsequently reverse. Instead, if the reversal is a correction of a temporary price concession from a lack of liquidity, there is no economic reason for a resilience pattern to exist after the reversal period.====Next, we combine several common datasets from the literature, described in Section 3, to test the model predictions and draw further conclusions on the forces driving intraday reversal. First, we find that a significant cross-sectional intraday reversal pattern exists in the Chinese market, i.e., subsequent intraday returns (i.e., ====) are negatively predicted by ====. For instance, at the representative timing of 10:00 a.m. each day, an equal-weighted standard reversal strategy earns a return of 44 BPs in China after controlling for a basic possible bid-ask bounce and extreme liquidity shock. As a robustness check, by applying the Fama-MacBeth (1973) regression, the intraday predictive ability of ==== persists when we control for several famous return predictors.====Second, our empirical results for the Chinese market speak against illiquidity-based explanations. We sort all stocks into portfolios based on ==== and each of three liquidity measures (market capitalization, share turnover and the illiquidity measurement of Amihud (2002)) featured in the literature. Compared to U.S. market, liquidity has a much smaller impact on intraday reversal in the Chinese market. The difference between the ==== spread for the most liquid stocks and that for the most illiquid stocks is only 10.2, 3.4 and 9.8 BPs for the three different liquidity measures. This departure mainly derives from the ==== spreads for the most liquid group. In particular, for the most liquid group, the spreads are still above 24 BPs (-25.5, -31 and -34.3 BPs, respectively), which are much higher than those in the U.S. market. These observations indicate that the automatic application of illiquidity-based explanations should be viewed with suspicion.====Third, we investigate how the return spread between ==== portfolios responds across different holding periods for holding periods of up to 20 days. Indeed, consistent with the prediction from the liquidity oversupply explanation, for the Chinese market, the negative correlation between the ==== and the future daily return is reversed after the intraday period. For instance, for equal-weighted portfolios in the Chinese market, high-==== stocks outperform low-==== stocks by 26.1 BPs on the first day after reversal, with a t-statistic equal to 8.5. Thereafter, the high-==== stocks continuously outperform the low-==== stocks from the second day until the fourth day by 4.3, 8.3, and 5.5 BPs, respectively. As a robustness check, we also provide the results for the U.S. market, which show that only a nonsignificant negative correlation between ==== and the future return exists following the intraday period. For instance, in U.S. market for equal-weighted portfolios, high-==== stocks underperform low-==== stocks by 2.2 BPs on the second day after reversal, with a t-statistic equal to -1.5. Thereafter, the high-==== stocks continuously underperform low-==== stocks for up to 10 days. These results speak for the illiquidity-based explanation.====To summarize, the paper analyzes the cross-sectional intraday return patterns between the Chinese and U.S. markets and further studies the driving forces of each of them. Several recent studies focus on a similar intraday return pattern or highlight similar results. It is worth emphasizing that the underlying mechanisms are different, thus leading to contrasting testable implications:====Ultimately, we offer a fresh perspective on intraday return reversal. In the process, we make several contributions. First, we provide the first study comparing cross-sectional intraday reversals in the U.S. and Chinese markets, offering a unique contribution to the expansive literature on intraday price patterns (Avramov et al. (2006); Bremer and Sweeney (1991); Cox and Peterson (1994); Jegadeesh (1990); Lehmann (1990)). Second, in addition to the illiquidity-based explanation, this paper proposes an alternative mechanism, that is, liquidity oversupply, thereby providing new insights into intraday reversal. While numerous studies have explored the profitability of such contrarian strategies (Avramov et al. (2006); Da et al. (2014); Nagel (2012)), a complete understanding of what drives reversal profits remains unclear. We hence shed some new light on the economic drivers of intraday reversal profits. Finally, this paper offers the novel discovery that liquidity oversupply is the driving force for intraday reversal in the Chinese market. Many aspects of the Chinese economy are shared by other developing and developed countries, and thus, the insights from papers on China may be applicable to them.====The reminder of the paper is organized as follows. Section 2 presents a stylized framework. Section 3 describes the data sources and summary statistics. Section 4 discusses our main result on intraday reversal and several possible driving elements. Section 5 concludes the paper.",What drives intraday reversal? illiquidity or liquidity oversupply?,https://www.sciencedirect.com/science/article/pii/S0165188922000185,20 January 2022,2022,Research Article,97.0
"Belongia Michael T.,Ireland Peter N.","Department of Economics, University of Mississippi, Box 1848, University MS 38677,Department of Economics, Boston College, 140 Commonwealth Avenue, Chestnut Hill MA 02467","Received 15 May 2020, Revised 7 January 2022, Accepted 16 January 2022, Available online 19 January 2022, Version of Record 29 January 2022.",https://doi.org/10.1016/j.jedc.2022.104312,Cited by (3),". The simulations also reveal that, under the same money growth rule, the US economy would have recovered more quickly from the 2007-9 recession, with a much shorter period of exceptionally low interest rates. These results suggest that money growth rules can serve as simple but useful guides for ","For the past quarter century, and perhaps longer, the Federal Reserve has conducted monetary policy by managing nominal interest rates. While today’s practice of strict federal funds rate targeting has its origins in the early 1990s, Greenspan (1997), Meulendyke (1998), and Thornton (2006) all describe Federal Reserve policy as shifting towards tighter interest rate control beginning sometime in the 1980s. Cook (1989) and Gilbert (1994) go back even further, arguing that the reserves targeting procedures used from 1979 through 1982 disguised policy actions taken to manage the funds rate instead.====Academic economists also depict Federal Reserve policy as managing interest rates. Taylor (1993) introduces his now-famous rule, which describes how the Fed adjusts its interest rate target in response to movements in the output gap and inflation. Taylor (1993) also demonstrates that the strikingly simple formula tracks actual movements in the federal funds rate remarkably well over the period from 1987 through 1992. Some variant of the Taylor rule now appears as the description of monetary policy in textbook New Keynesian models presented, for example, by Woodford (2003) and Galí (2015).====Preference for interest rate management, in both practice and theory, often is motivated with reference to Poole’s (1970) classic analysis, demonstrating that in a stochastic IS-LM model, policies targeting the nominal interest rate insulate output from the effects of money demand shocks, whereas policies targeting the money stock instead allow these shocks to contribute to macroeconomic volatility. Poole’s model holds the aggregate price level fixed, but Collard and Dellas (2005), Galí (2015), and Ireland (2000) demonstrate that these results extend to modern New Keynesian models as well, in which monetary policies calling for a constant rate of money growth lead to excess volatility in both output and inflation, compared to policies targeting interest rates instead, especially when the economy is hit by recurrent money demand shocks. Furthermore, as emphasized by Ireland (2004c) and Belongia and Ireland (2021), standard New Keynesian models feature forward-looking variants of more traditional Keynesian IS and Phillips curves that imply monetary policy affects output and inflation exclusively through its ability to influence the current and expected future path for the short-term nominal interest rate. The Taylor rule, therefore, becomes a natural benchmark for describing monetary policy in these models. And, to the extent that Federal Reserve officials also believe that monetary policy influences economic activity mainly, if not entirely, through the New Keynesian interest rate channel, it makes sense for them to focus on managing interest rates as well.====Recent events, however, prompt a reconsideration of the prevailing consensus favoring interest rate rules. First and most obviously, the extended period from 2009 through 2015, during which the Federal Reserve’s traditional federal funds rate targeting procedures were constrained by the perceived lower bound on nominal interest rates, raises the question of whether alternative policy rules focused on managing the money stock might have allowed the Fed to pursue its stabilization objectives more effectively during and after the financial crisis and Great Recession of 2007-9. Belongia and Ireland, 2017, Belongia and Ireland, 2018 and Keating et al. (2019) present empirical evidence suggesting this may have been the case, but stop sort of exploring the possibility, theoretically, in the standard New Keynesian framework. Second, the Fed’s actual response to continued weakness in output and inflation while the funds rate remained in a target range near zero went beyond three waves of large-scale asset purchases. Also required were other important changes in operating procedures, such as the introduction of interest payments on bank reserves and the establishment of a reverse repurchase agreement program through which the Fed interacted with a wide range of nonbank financial institutions. Although moving from a policy involving interest rate management to one of targeting the growth rate of a monetary aggregate might once have seemed prohibitively difficult, this recent experience shows, to the contrary, that operating procedures and institutional arrangements can be changed significantly, even on short notice, to support any major shift in policy regime. Finally, while the previous studies by Ireland (2000), Collard and Dellas (2005), and Galí (2015) all suggest that policy rules calling for ==== rates of money growth will perform poorly, relative to Taylor rules, in stabilizing output and inflation, none of these studies considers the possibility that money growth rules might work significantly better if they allowed policy to adjust to movements in the output gap and inflation in a manner similar to that of the Taylor rule.====Thus, this paper extends previous work by reconsidering money growth rules in an estimated New Keynesian model. By identifying a parsimonious rule that dictates a systematic response of money growth to changes in the output gap, it follows in the same style of research presented, for instance, in Taylor (1999) by characterizing rules that remain simple while still delivering favorable economic outcomes. And by using counterfactual simulations to assess how the US economy would have performed over a sample period running from 1983 through 2019, it illustrates the satisfactory performance of a money growth rule in both good times – the period of the Great Moderation – and bad – the Great Recession and its aftermath.====The particular variant of the New Keynesian model used here takes most of its basic features from those in Ireland, 2004b, Ireland, 2004c, Ireland, 2007, Ireland, 2011, but innovates in four distinct ways. First, it introduces real money balances into a representative household’s utility function in a manner that leaves the New Keynesian IS and Phillips curves in their standard forms, excluding the additional terms involving money growth that appear in Ireland (2004c). This ensures that the extended model retains the New Keynesian assumption that monetary policy actions have an impact on output only through their effects on the current and expected future path of the short-term nominal interest rate. The intent is to put money growth rules to a most stringent test, by excluding model features that might specifically favor stability in the money stock.====Second, the model’s money-in-the-utility function specification is also tailored to imply that the level of real balances demanded by the non-bank public remains finite even as nominal interest rates fall to zero, reflecting observations made by Ireland (2009) and Rognlie (2016) that US money demand did not explode during either episode of very low nominal interest rates following the 2001 and 2007-9 recessions. Intriguingly, as noted by Rognlie (2016), this specification implies that short-term interest rates ==== fall below zero, at least by modest amounts for short periods of time – a phenomenon that will be explored in the counterfactual experiments performed with the estimated model.==== Third, the model includes adjustment costs of real balances in its specification, following Nelson (2002) and Andrés et al., 2004, Andrés et al., 2009, all of which present evidence that New Keynesian models with money fit the data better when they allow for gradual adjustment of real balances to shocks that hit the economy.====Fourth and finally, the analysis here employs methods developed by Kulish et al. (2017) to account for periods, like that experienced in the US from 2009 through 2015, when short-term nominal interest rates were constrained by the Fed to remain near zero. According to the New Keynesian model, even after its current policy rate is lowered to zero, the central bank can use “forward guidance,” in the form of policy announcements that lengthen private agents’ expectations regarding the duration of the zero interest rate episode, to deliver additional monetary stimulus. The Bayesian estimation methods used here exploit survey data to track changes in the expected duration of the zero interest rate period and the effects these shifts in expectations have on output and inflation. Thus, with these methods, the model can be estimated over a sample running continuously from 1983 through 2019, accounting for the effects of both zero interest rates and forward guidance over the 2009-15 period as well as the effects of more traditional interest rate policy before and after. The estimated model can then be used to explore counterfactual scenarios in which the central bank systematically adjusts its target for the money growth rate under both favorable and unfavorable economic conditions.====The results from this exercise reveal that, even in a model that departs minimally from standard New Keynesian specifications and therefore offers no special role for changes in the money stock, a money growth rule nonetheless can deliver performance on par with that generated by more conventional Taylor rules for the interest rate. The counterfactual simulations show, in particular, that under a money growth rule that responds modestly but persistently to changes in the output gap, the US economy would have recovered more quickly than it actually did from the financial crisis and Great Recession without requiring a prolonged period of zero or negative interest rates. Thus, the results suggest that as Federal Reserve officials search for a new policy framework within which they can more reliably achieve their stabilization objectives in an environment with low interest rates and inflation following a series of adverse disturbances, abandoning the traditional practice of managing the federal funds rate in favor of a rule targeting the money growth rate should be added to the list of possibilities considered.",A reconsideration of money growth rules,https://www.sciencedirect.com/science/article/pii/S0165188922000173,19 January 2022,2022,Research Article,98.0
Matsuoka Tarishi,"Faculty of Urban Liberal Arts, Tokyo Metropolitan University, 1-1, Minami-Osawa, Hachioji, Tokyo 192-0397, Japan","Received 15 June 2021, Revised 6 January 2022, Accepted 10 January 2022, Available online 19 January 2022, Version of Record 29 January 2022.",https://doi.org/10.1016/j.jedc.2022.104302,Cited by (0),This paper develops a ,"Modern monetary systems include two standing facilities: a lending facility and a deposit facility. At a lending facility, financial intermediaries with high liquidity needs can borrow liquidity from the central bank at a given lending (discount) rate by using their illiquid assets as collateral, while they can earn a deposit rate by depositing idle money on their reserve accounts at the central bank. Controlling both the lending and deposit rates is becoming a popular and important tool for monetary policy. For example, under a channel (corridor) system, a central bank often implements a monetary policy by changing the width of the interest rate spread, referred to as the “corridor” and defined by the difference between the lending and deposit rates, without changing a policy target. Despite its apparent importance and popularity, the standing facilities received relatively little attention in the academic literature. In particular, the general equilibrium effects of the facilities have not been examined in detail.====The purpose of this paper is to develop a general equilibrium model of money and banking to analyze the effects of the standing facilities on the frequency of liquidity crises, a bank’s portfolio, and the prices of the assets used as collateral. My analysis is based on the works of a New Monetarist framework developed in Lagos and Wright (2005) and Rocheteau and Wright (2005). In particular, I employ a monetary model that includes banking developed by Williamson, 2012, Williamson, 2016 and Matsuoka and Watanabe (2019). My starting point is that a buyer meets a seller bilaterally in a decentralized meeting and buys consumption goods produced by a seller with money or credit. Some decentralized transactions require money due to the lack of record-keeping technology and limited commitment, but others do not. A buyer faces an idiosyncratic risk of whether they can use credit, which motivates the endogenous role of banking, as in Diamond and Dybvig (1983). The banks collect deposits from the buyers and choose a portfolio consisting of monetary reserves and illiquid real assets on behalf of their depositors. Since some buyers must use money, they withdraw money from their banks before transactions. With the aggregate risk related to the depositor’s payment methods, the banks exhaust their reserves in some states of nature where there is a large number of buyers who withdraw money, called a liquidity crisis. Under a liquidity crisis, a shortage of liquidity in the entire banking system causes consumption inequality between buyers who must use money and those who can use credit.====If the central bank sets a lending facility and opens a discount window, then the banking economy is very different. The lending policy determines a lending rate and haircuts on illiquid assets used as collateral for the loan. Then, during a liquidity crisis, banks with high liquidity needs can borrow money at the lending facility at a given lending rate by using their illiquid assets as collateral. Consequently, this policy makes a bank’s illiquid assets liquid and mitigates the consumption inequality between the buyers. A lending policy is now considered a stabilizing force in a financial system and established in most developed countries.====On the other hand, if the central bank pays an interest rate on reserves at a deposit facility, the banks have an opportunity to earn interest by depositing idle money as reserves. The facility gives the banks an incentive to accumulate reserves ex ante, and, as a result, the banking system becomes relatively liquid. In practice, controlling this deposit rate is the primary policy tool under the floor system. Changing the two policy rates and haircuts will change a bank’s portfolio choice in advance, which in turn will affect the amount of liquidity in the entire banking system, the likelihood of a liquidity crisis, the asset prices used as collateral, and a liquidity structure of assets’ yields. Considering these general equilibrium effects here is crucial to the analysis and to the contribution of this paper.====The main results of the paper are as follows. First, with scarce pledgeable assets, or equivalently, with high haircut rates on collateral assets, the collateral asset prices carry liquidity values and exceed their fundamental prices in a monetary equilibrium. Second, in this case, an increase in the lending rate decreases these asset prices and increases the bank’s reserves, leading to a low probability of a liquidity crisis. A high lending rate makes the central bank loan less attractive, so a bank raises investment in reserves and reduces investment in real assets. Third, an increase in the deposit rate increases a bank’s reserves, reduces the likelihood of a liquidity crisis, and decreases asset prices. Fourth, the different haircut rates on the different collateral assets break the rate-of-return equality, leading to a non-degenerate structure of assets’ yields. Fifth, with abundant pledgeable assets, the asset prices are determined by their fundamental values, and the rate-of-return equality among assets arises. Finally, the optimal monetary policy requires a deposit rate equal to the time rate of preference, which is a version of the Friedman rule. If a nominal interest rate is high enough, equaling the lending and the deposit rates can be optimal, implying the optimality of a floor system. The main message of this paper is that changing the interest-rate corridor has general equilibrium effects on the amount of liquidity of the entire banking system and asset prices, even without changing the policy target rate, whose effects on asset prices are the main focus in the previous literature. These general equilibrium effects have received little attention but should be considered when designing standing facilities.====This paper bears a theoretical similitude to the modern search theory of money with banking, including such papers as those by Bencivenga and Camera (2011); Berentsen et al. (2007); Ferraris, Watanabe, 2008, Ferraris, Watanabe, 2011; Gu et al. (2013); He et al. (2008); Li (2011); Monnet and Sanches (2015); Sanches, 2016, Sanches, 2018; Williamson (2012);and Matsuoka, Watanabe, 2019, Matsuoka, Watanabe. They consider the role of traditional monetary policy, such as controlling money growth rate or open market operations but do not consider the roles of standing facilities. Berentsen and Monnet (2008) develop a general equilibrium model of standing facilities and analyze the optimal interest-rate corridor in channel systems with costly collateral. In contrast, my model introduces aggregate uncertainty and focuses on the effects of standing facilities on asset prices. The subsequent studies, such as those by Martin and Monnet (2011), Berentsen et al. (2014); Chapman et al. (2011); Williamson (2016), and Berentsen et al. (2018), also consider standing facilities in a monetary equilibrium without aggregate uncertainty but do not focus on the prices of multiple assets used as collateral.====On the asset pricing side, this paper is also related to the search-matching models of money that analyze the role of assets in the exchange process, such as those by Geromichalos and Herrenbrueck (2016); Geromichalos et al. (2007); Li and Li (2013); Rocheteau and Wright (2013), and Geromichalos et al. (2016), among others. While these models focus on the effects of the inflation rate on asset prices, I examine the effects of standing facilities on asset prices in a monetary economy where banking is modeled explicitly.====Furthermore, this paper is also related to several other works that investigate the implications of discount window policy using the overlapping generations model developed by Champ et al. (1996) and Smith (2002). They include Antinolfi et al. (2001); Antinolfi and Keister (2006), and Matsuoka (2012). However, they do not consider the collateral role of illiquid assets and the interest rate on money. Some papers, such as those by Freeman and Haslag (1996); Sargent and Wallace (1985); Smith (1991), and Ghossoub and Reed (2021), study the policy of paying interest on money in overlapping generations economies but do not consider the role of a lending facility.====The remainder of the paper proceeds as follows. Section 2 describes the model environment, while Section 3 solves the banking problem with standing facilities. Section 4 illustrates the stationary monetary equilibria. Section 5 concludes, and all proofs are contained in the Appendix A.",Asset prices and standing facilities in a monetary economy,https://www.sciencedirect.com/science/article/pii/S0165188922000070,19 January 2022,2022,Research Article,99.0
"Heipertz Jonas,Mihov Ilian,Santacreu Ana Maria","Columbia University, Department of Economics. 3022 Broadway New York, NY 10027, United States,INSEAD, CEPR and ABFER, Economics Department, 1 Ayer Rajah Avenue, Singapore, 138678, Singapore,Federal Reserve Bank of St. Louis, Research Division, 1 Federal Reserve Bank Plaza, St. Louis, MO 63102, United States","Received 7 December 2020, Revised 11 January 2022, Accepted 16 January 2022, Available online 18 January 2022, Version of Record 29 January 2022.",https://doi.org/10.1016/j.jedc.2022.104311,Cited by (0),"  the paths of the nominal exchange rate and the ==== under each rule and ====  external habits in consumption, which leads to deviations from ====. These differences are larger in economies, which are very open, which are more exposed to foreign shocks, or in which domestic and foreign goods are highly substitutable.","In the aftermath of the global financial crisis, unconventional monetary policies implemented by central banks in advanced economies resulted in an increase in liquidity in the financial system. This excess liquidity was channeled towards emerging economies, as investors were “searching for yield.” That search for yield exacerbated the appreciation of non-crisis countries.====In small open economies, the exchange rate is an important element of the transmission of monetary policy (Svensson, 2000). Fluctuations in exchange rates have an effect on inflation and other economic variables and, thus, challenge the ability of monetary policy to stabilize the economy. Because they are more subject to foreign shocks, central banks in emerging economies generally prefer to keep the exchange rate under tight control. In fact, many emerging economies today follow a quasi-managed floating exchange rate regime. Despite a large body of empirical research on managed floating, the theoretical literature discussing how monetary authorities deal with exchange rates has focused on corner solutions: either the currency rate is fixed by the central bank, or the government, or it is left to be determined by market forces. First, a large number of papers evaluate the costs and benefits of fixed exchange rates (including Friedman, 1953 and Flood and Rose, 1995). A second approach to incorporating the exchange rate into discussions of monetary policy is to augment a closed-economy Taylor rule with the rate of currency depreciation. Under this approach the interest rate reacts not only to inflation and the output but also to movements in the exchange rate. For example, DePaoli (2009) derives an optimal monetary policy rule within a DSGE model and shows that by putting some weight on real exchange rate fluctuations, a central bank can achieve improvements in social welfare. Lombardo and Ravenna (2014) characterize optimal exchange rate policy. They emphasize the role of the composition of international trade on the optimal volatility of the exchange rate, and quantify the loss from an exchange rate peg relative to the Ramsey policy conditional on the composition of imports. Other recent papers show that optimal simple rules involve some type of adjustment of the nominal exchange rate (see, for example, Devereux etal., 2018).====In this paper, we evaluate the properties of intermediate exchange rate regimes by considering an alternative class of policy rules where the central bank, instead of using the interest rate, adjusts the exchange rate in response to inflation and the output. The exchange rate is adjusted by the central bank in a manner similar to how it adjusts the interest rate when using it as an operating instrument. To evaluate the benefits of this rule relative to a standard interest rate rule, we build a DSGE model where the monetary authority adjusts the exchange rate in response to deviations of inflation from a target and fluctuations in the output.====,====Understanding the costs and benefits of an exchange rate policy rule within a fully specified model is not a trivial task. The immediate reaction is that if the model features an uncovered interest parity (UIP) condition, then interest rate and exchange rate rules might generate similar outcomes. In our model, there are two reasons why the outcomes for the two rules differ. First, the actual implementation of the exchange rate rule is important. While the central bank technically can replicate any interest rate rule by moving the exchange rate today and announcing depreciation consistent with UIP, it is not the way that our rule operates. In our model, the central bank sets the exchange rate today to react to current fluctuations of inflation and output. It then announces the depreciation rate from time ==== to ====. The underlying assumption is that the central bank can commit to a particular exchange rate next period. This implies, for example, that the model may not feature the standard overshooting result, as the currency rate both today and at ==== are determined by the monetary authority. The simulations of our model suggest that this feature does generate differences between the two rules. A key factor for the exchange rate rule to be successful in reducing economic fluctuations is that the announcements of the central bank implementing the rule are credible. If the central bank is more than backed with foreign reserves and if it has built the credibility for maintaining low inflation, it will be less necessary for it to make continuous interventions and the policy will be more successful. We abstract from credibility issues in the paper and assume that the exchange rate rule can be perfectly implemented.====Furthermore, the differences between exchange rate and interest rate rules are amplified when the UIP condition fails. Indeed, Alvarez etal. (2007) argue forcefully that a key part of the impact of monetary policy on the economy goes through conditional variances of macroeconomic variables rather than conditional means. In terms of the UIP condition, their paper implies that the interest parity condition has a time-varying risk premium.==== Interest in a time-varying risk premium has been growing in recent years. Verdelhan (2010) shows how consumption models with external habit formation can generate a counter-cyclical risk premium that matches key stylized facts quite successfully. In our model, we adopt a similar approach by allowing external habit formation. To show the importance of the counter-cyclical risk premium, we report results for a first-order approximation, which wipes out the risk premium from the UIP condition, and for a third-order approximation, which preserves time variation in the risk premium.====We start by writing down a relatively standard New-Keynesian small open economy model as in Gali and Monacelli (2005) that we extend to include external habit in consumption, as in DePaoli and Zabczyk (2013). We then analyze the performance of the model under two different policy rules: a standard Taylor rule in which the monetary authority sets interest rates and an alternative monetary rule in which the monetary authority sets the depreciation rate of the nominal exchange rate. We show that if UIP holds, these rules generate quantitatively similar responses to shocks. The Taylor rule generates higher volatility of the exchange rate and other economic variables. We then introduce deviations from UIP. The goal is to analyze the performance of the two competing rules when the one-to-one relationship between exchange rates and interest rates breaks down. In this case, the differences between the two rules, in terms of the response of the economy to shocks, are amplified. The main reason is that the implementation of the monetary rule has an effect on the volatility of the risk premium through a precautionary savings motive. The Taylor rule generates larger fluctuations in inflation and the output, as the larger volatility of exchange rates increases the risk premium. The opposite is true for the exchange rate rule, as the monetary authority adjusts its path of appreciation to smooth economic fluctuations by generating a less-volatile exchange rate. In this regard, the exchange rate rule is also different from a peg, in which the monetary authority fixes the exchange rate to a specified value.====After exposing the mechanism driving the differences between the two rules, we evaluate quantitatively their performances both in terms of managing macroeconomic fluctuations and in terms of welfare. We begin by specifying a general rule in which the monetary authority reacts to fluctuations in inflation, the output and the exchange rate, with a certain degree of interest rate smoothing. We then compute for each rule the implied volatility of key economic variables. We do this for (i) a log-linearized version of our model that does not capture the existence of a time-varying risk premium and (ii) a non-linear version of our model in which we log-linearize the demand and supply conditions, while taking a third-order approximation of the equationsthat depend on the risk premium directly. In that way, our non-linear model isolates the role of a time-varying risk premium and, hence, deviations from the UIP condition. We find that, for a wide range of plausible parameters in the monetary rules, the exchange rate rule outperforms the interest rate rule in terms of inflation and output growth. It also outperforms the peg. This is true for a large combination of parameter values in the monetary rules. In a related paper, Schmitt-Grohé and Uribe (2016) study the optimal exchange rate policy when there is downward wage rigidity and compare it to the exchange rate peg. Different from our framework, they solve for the first-best allocation using an exchange rate policy. By letting the exchange rate react to fluctuations, they find that the monetary authority would like to implement large devaluations when there are recessions. An exchange rate peg, however, would be an inefficient policy.====Finally, we compare the performances of the two rules in terms of welfare. Note that we do not compute the Ramsey policy in which the social planner finds the optimal allocation and then obtains values for the monetary policy rule parameters that mimic such allocation. An alternative would be to compute the constrained Ramsey problem in which the planner is constrained to maximize the lifetime expected utility of the representative consumer in a competitive equilibrium. Instead, in this paper we compare simple monetary rules in which the central bank uses either the interest rate or the exchange rate as the instrument. More precisely, we evaluate lifetime utility for a wide range of parameter combinations in the two rules (in all the exercises, however, we keep the autoregressive smoothness parameter fixed). We compute welfare by taking a third-order approximation of the full model and of the utility function. In that way, we capture variations in the risk premium of the economy and we are able to evaluate how they impact the preferences of the central bank. Our welfare analysis is done using numerical approximations (see Collard and Juillard, 2001 and Schmitt-Grohé and Uribe, 2004).==== We find that a policy rule in which the central bank adjusts the exchange rate to react to fluctuations in output and inflation is welfare improving with respect to a monetary rule in which the central bank uses the interest rate as its instrument. This is especially the case for very open economies and for economies in which the elasticity of substitution between domestic and foreign goods is large. A further analysis shows that the key friction driving welfare differences between the two rules is external habit formation. Indeed, removing other distortions such as monopolistic competition and the trade externality does not affect our results substantially.====Our findings suggest that (i) external habits in consumption—which drives deviations from the UIP condition—, and (ii) the different implementation of the rules, generate differences in terms of business cycle fluctuations and welfare between a Taylor rule and an exchange rate rule. These two channels prevent the monetary authority from being able to mimic the properties of an exchange rate rule with a Taylor rule. Moreover, the effect of external habits in driving these differences is amplified when we allow for a higher persistence and volatility of the productivity shocks. In this case, the economy faces a larger and more volatile risk premium that gives rise to larger welfare effects as the distortion introduced by external habits gets amplified.====The optimal choice of the monetary policy instrument was studied in the classic paper of Poole (1970). Poole (1970) shows that in a deterministic environment, the choice of the instrument is irrelevant. However, when there are stochastic terms in the model, the choice of the instrument matters because it affects the strength with which shocks pass through to variables that the policymaker cares about. Our paper is mostly related to Lubik and Schorfheide (2007), who show that the exchange rate is of significant importance in setting monetary policy in some open economies. In our paper, we investigate whether we can take this one step further by showing that in cases of very open economies, the exchange rate might serve as an instrument for implementing monetary policy. One can argue that there is a continuum or a range of policy rules that start with pure interest rate rules that ignore completely exchange rate fluctuations thus allowing for significant volatility in exchange rates ceteris paribus. Then there are rules that reduce exchange rate volatility by reacting to exchange rates with different intensity as in Lubik and Schorfheide (2007). Then there are rules introduced in this paper, where the exchange rate responds to current macroeconomic conditions and the central bank sets not only the exchange rate today but also commits to a path of future appreciation/depreciation. Finally, there are fixed exchange rates and currency unions. Our paper thus paper is a study along this continuum of rules.====The rest of the paper proceeds as follows. Section 2 lays out the details of the model. Section 3 presents the mechanism of the exchange rate rule. Section 4 performs a quantitative analysis. Section 5 provides a summary of our key findings, some ideas for future research and conclusions.",Managing macroeconomic fluctuations with flexible exchange rate targeting,https://www.sciencedirect.com/science/article/pii/S0165188922000161,18 January 2022,2022,Research Article,100.0
"Gonzalez-Eiras Martín,Niepelt Dirk","University of Bologna, Piazza Scaravilli 2, 40126, Bologna,Study Center Gerzensee, University of Bern, CEPR. P.O. Box 21, Gerzensee CH-3115, Switzerland","Available online 15 January 2022, Version of Record 21 June 2022.",https://doi.org/10.1016/j.jedc.2022.104309,Cited by (2),"We investigate how politico-economic factors shaped government responses to the spread of COVID-19. Our simple framework uses epidemiological, economic and politico-economic arguments. Confronting the theory with US state level data we find strong evidence for partisanship even when we control for fundamentals including the electorate’s political views. Moreover, we detect an important role for the proximity of elections which we interpret as indicative of career concerns. Finally, we find suggestive evidence for complementarities between voluntary activity reductions and government imposed restrictions.","Confronted with the sudden spread of COVID-19 infections, governments across the world faced new tradeoffs in the early months of 2020. At their core, these tradeoffs concerned the choice between “lives” and “livelihoods”—health risks for broad segments of the population could only be contained by reducing social and economic activity. Governments responded differently to these tradeoffs, with some countries choosing a more light-handed approach and relying on individuals and firms to voluntarily adjust their behavior while others imposed harsh lockdowns and social distancing measures.====These differences reflected “fundamentals:” Not all populations were equally at risk, nor did economic activity in all regions equally depend on social interaction. Plausibly, they were also the consequence of politico-economic factors: Even with similar fundamentals, political incentives led policy makers to assess the relative costs and benefits of lives and livelihoods differently, and to act accordingly.====In this paper, we investigate how politico-economic factors shaped early-stage government responses to COVID-19 in US states. Our contribution is, first, to use epidemiological, economic and politico-economic arguments to derive a simple framework to analyze the incentives of policy makers. Second, we confront the theory with data to identify the role of political determinants of government choices.====An empirical analysis of the determinants of government choices in response to COVID-19 infections faces several challenges. Most importantly, governments operated in environments that varied along numerous dimensions of which many are unobserved or difficult to quantify. For example, cultural factors, social capital or family structures which might affect individual behavior and thus the costs and benefits of government interventions, differ across countries but are hard to measure, such that omitted variables are a natural concern.====We address this problem by focusing on a set of governments—state governments in the US—that act within relatively similar environments.==== While there are, undoubtedly, differences between how communities operate in Minnesota or Texas these differences (as well as differences between political parties and government institutions) are minor when compared with cross-country differences. For related reasons, we focus on state-government responses during a relatively short time period, namely March to June 2020. By restricting the analysis to a quarter of a year we do not have to control for factors that changed between the first and subsequent waves of infections and did so asymmetrically across states, for instance due to different capabilities to implement test-and-trace strategies.====Another challenge when trying to identify the determinants of government interventions concerns anticipation effects. When individuals expect governments to act in certain ways and change their behavior accordingly then it is difficult to distinguish between direct and indirect effects of policy. An advantage of the COVID-19 setting that we analyze is that anticipatory effects can plausibly be excluded. While epidemiologists had warned for years that sooner or later a global epidemic would strike, policy makers and the media in the US and in other countries not affected by SARS or MERS did not prioritize this threat. With memories of the Spanish Flu having faded both US state governments and the broader public were largely caught unaware when COVID-19 struck.====In ====, we document the heterogeneous policy interventions in US states and in ====, we lay out the epidemiological, economic, and politico-economic model that guides our analysis. As far as epidemiology is concerned we focus on two specifications, the canonical and the modified SIR model (====, ====). We emphasize the advantages and disadvantages of both specifications and extract their robust features, arguing that only such robust features might plausibly have guided policy choices.====In addition we emphasize various politico-economic factors which imply that the tradeoff between lives and livelihoods as perceived by a policy maker differs from the one faced by a Ramsey planner. In particular, we argue that partisanship and re-election concerns of governors in combination with the ==== between the onset of the pandemic and the election date affect the political valuation of lives vs. livelihoods. Different politico-economic theories make contrasting predictions about the election effect. When campaign contributions are a major concern theory predicts policy makers to put more emphasis on livelihoods rather than lives as the election date approaches. Less emphasis on livelihoods, in contrast, is consistent with career concerns of policy makers that want to signal competence. Our setup is agnostic about which channel dominates.====The model predicts that in addition to these political factors, fundamentals such as health care quality, the age structure of the population and its density, the ====Against this theoretical background we present the empirical analysis in ====. We aim at identifying how the political factors—party affiliation of the state governor and proximity of the next election—determine the government imposed restrictions. In line with common wisdom we find that Republican governors tended to impose shorter and less harsh lockdowns. The effect is robust and quantitatively important even when we control for the electorate’s political views, among the other fundamentals. Moreover, we find that proximity of the next election has the opposite effect: An upcoming election tended to cause stricter restrictions. We interpret this as evidence for career concerns and against the importance of campaign contributions.====Digging deeper we hypothesize that career concerns should matter most when elections are close and the governor is legally able and chooses to run for re-election. Taking this qualification into account we find indeed that the effect we identify becomes stronger. The ====We also estimate specifications in which we try to control for voluntary activity reductions, using Google mobility data just before state governments started to impose restrictions as proxy. Interestingly, we find suggestive evidence that points to complementarities between voluntary and government imposed activity reductions: Governors found it more beneficial to impose restrictions where the population voluntarily reduced activity. In contrast, we do not find convincing evidence for important roles played by state legislatures; knife-edge political races; the governor’s gender; or differences in career concerns across party lines.==== concludes and the appendix collects proofs and further discussion.====Since mid March 2020 there has been an explosion of papers focusing on the intersection of epidemiological dynamics and economic cost-benefit analysis. Early contributions include ==== and ==== or, among the much fewer works preceding the pandemic, ====.==== Our paper adopts a positive rather than normative perspective. In addition, it confronts theory with data, provides evidence for an important role of elections in disciplining policy makers, and suggests that voluntary and mandatory activity reductions may be complements.====We also contribute to the politico-economic literature on elections and career concerns according to which elections discipline politicians in office (====, ====, ====); with imperfect information about politicians’ competence, career concerns give rise to policy cycles (====); and office holders sacrifice rents before election to signal competence to voters.====Using cross-country data, ==== document that infection dynamics, and government responses to them, affect political approval ratings. For the US there is clear evidence that government imposed restrictions did reduce economic activity (e.g. ====) and the spread of COVID-19 (e.g. ====), giving rise to a (short-term) tradeoff between lives and livelihoods. ==== and ==== document industry-level differences in the ability to let employees work from home, and ==== find evidence of partisan beliefs about the personal and social risks of COVID-19, factors that we account for when analyzing the variation of policy choices across states. ==== find evidence that Democratic governors imposed more restrictive COVID-19 policy measures. Our results support this finding even when we condition on the political views among the electorate (and other fundamentals).==== provide evidence that US senators made policy concessions to increase their chances for re-election. In a similar vein, our analysis suggests that governors running for re-election chose to impose longer and more stringent lockdowns but only when they were more than half way through their terms. ==== analyze cross-country data and find that restrictions were weaker when incumbent presidents faced re-election, a result that contrasts with our finding that career concerns fostered government imposed activity restrictions. ====, p. 7) acknowledge that their cross-country results should be interpreted with caution and they argue in favour of within-country studies like ours.==== also determine start and end dates of restrictions. In the following we highlight differences in the identified start or end dates and we explain the reasons for our timing assumptions:",The political economy of early COVID-19 interventions in US states,https://www.sciencedirect.com/science/article/pii/S0165188922000148,15 January 2022,2022,Research Article,101.0
Melosi Leonardo,FRB Chicago and CEPR,"Available online 13 January 2022, Version of Record 21 June 2022.",https://doi.org/10.1016/j.jedc.2022.104307,Cited by (1),"With this paper, Eichenbaum, Rebelo, and Trabandt have made another insightful and influential contribution to the growing literature on the ==== of epidemics. Their papers are paving the way to a new fascinating research program whose objective is to develop empirically plausible ==== of epidemics. I argued that estimating synthetic COVID shocks in familiar ==== models provides a good benchmark to evaluate progress toward this goal (Ferroni et al., 2021). Furthermore, evaluating alternative containment measures and how these measures should be deployed (e.g., should containment measures be targeted to the workplaces or somewhere else?) are important matters this research agenda should address. My work with Rottner (Melosi and Rottner, 2020) contributes to developing methods allowing researchers to study contact tracing and testing in macro-epidemiological models of the type studied in Eichenbaum, Rebelo, and Trabandt’s influential works.",None,"Comments on Epidemics in the New Keynesian model by Eichenbaum, Rebelo, and Trabandt”",https://www.sciencedirect.com/science/article/pii/S0165188922000124,13 January 2022,2022,Research Article,102.0
Hur Sewon,"FRB Dallas, United States","Available online 13 January 2022, Version of Record 21 June 2022.",https://doi.org/10.1016/j.jedc.2022.104310,Cited by (0),"This comment briefly summarizes Hevia et al. (2022), who utilize a unique dataset to shed light on the ==== of the epidemiological burden for low- and high-income individuals in Bogotá. Some suggestions and ideas for future research are provided.","The COVID-19 pandemic has had a major impact on health and economic outcomes in the aggregate—claiming more than 5 million lives globally to date and causing one of the largest economic contractions ever—with these impacts heterogeneous across individuals. The paper by ==== document disparities in infections across socioeconomic status: 58 percent of low-income individuals are estimated to have been infected (as of February 2021), compared with 29 percent of high-income individuals. They also find that, within each age group, the case fatality rate is higher in the low-income group relative to the high-income group, though unconditionally it is higher in the high-income group because of its higher average age. Thus, not only are low-income individuals more likely to get infected but they are also more likely to die from the disease, conditional on infection.====Motivated by these empirical findings, they develop a two-agent macro-SIR model that is used to study the distributional consequences of COVID-19 and mitigation/transfer policies. High-income workers have access to financial markets, higher human capital, and lower infection risk, ====, compared to their low-income counterparts, who are assumed to live hand-to-mouth. Both agents derive utility from consumption (====), leisure (====), and social interaction (====, labor ====, and social interactions ====, and those of infected low-income and high-income individuals. Firms use labor and capital as inputs of production in a standard way. Shutdowns are modeled as a quantity restriction policy, ====The model is calibrated to match relevant features such as the dynamics of output and infections in Bogotà. Using the calibrated model, the paper studies the effects of shutdowns and redistributive transfer policies. The first main finding is that a shutdown reduces the welfare of both agents and has little impact on long-run health outcomes. Because the shutdown is modeled as a quantity restriction on output, it leads to a large decline in ","Comment on “COVID-19 in segmented societies” by Constantino Hevia, Manuel Macera, and Pablo Andrés Neumeyer",https://www.sciencedirect.com/science/article/pii/S016518892200015X,13 January 2022,2022,Research Article,103.0
"Brotherhood Luiz,Santos Cezar","Universitat de Barcelona, FGV EPGE & BEAT, Spain,Banco de Portugal, FGV EPGE & CEPR, Portugal","Available online 12 January 2022, Version of Record 21 June 2022.",https://doi.org/10.1016/j.jedc.2022.104303,Cited by (1),"This note discusses age-specific vaccination programs designed to curb the Covid-19 pandemic. We first provide some comments on the analysis by Glover et al. (2021b) and point directions where further research can be carried out. Additionally, we adapt the framework from Brotherhood et al. (2021) to assess the effects of different vaccination schemes when more infectious variants can emerge when more infections take place. We find that policy prescriptions crucially depend on taking individual behavioral responses into account and on whether variants can appear.","The Covid-19 pandemic has been a huge health and economic shock that hit all economies in the world starting in 2020. This shock spurred a great effort among economists to study what types of policies could be used to diminish the effects of the pandemic. Already in April/May 2020 there were papers studying the positive and normative implications of different Nonpharmaceutical Interventions (NPIs). As 2020 came to a close, the rapid development of vaccines allowed people to start being inoculated. NPIs could then be complemented by vaccination programs. Thinking about policy now meant to study the use of these different instruments jointly.====The study of NPIs together with vaccination is the subject of ====. In a world with young and old agents in which older people are more at risk of dying from Covid, the authors ask which group should be vaccinated first and how lockdowns should respond to the specific vaccination program that is chosen. The quantitative model used in the analysis is based on their previous work (====). The framework features heterogeneous agents. Old individuals do not work. The young work; some in the essential sector and others in the “luxury” sector. The difference is that, if a lockdown is in effect, it only affects those that work in the luxury sector since the essential sector must always be open. This rich heterogeneity creates important distributional effects when choosing which policy to implement.====The model in ==== is calibrated to US data from the beginning of the Covid-19 pandemic until the end of 2020. Then, from the beginning of 2021, the authors feed into the model a vaccination program aimed at mimicking the US experience, both regarding the number of vaccines and who received them. This benchmark scenario is then compared with two counterfactuals: vaccinating the young first and then the old, and vice versa. Together with each alternative vaccination scheme, a utilitarian social planner chooses the optimal strictness of a lockdown.====The results in ==== imply that the old are the most affected group by the alternative policies. Their welfare gains/losses are at least an order of magnitude greater than those experienced by the young. The utilitarian planner prefers to vaccinate the old first as this group is the most affected. Interestingly, the optimal lockdown acts to bring preferences closer. That is, if the young are vaccinated first, the planner chooses a stricter lockdown to curb the effects on the old. On the other hand, if the old are vaccinated first, the planner responds with a lighter lockdown, a policy preferred by the young since they must work.====This note contains two parts beyond this introduction. ==== comments on some modeling choices made by ==== and provides some ideas for follow-up analyses. ==== uses the framework developed in ==== to analyze different vaccination schemes in the presence of mutating variants of the novel coronavirus. The results from this last section show that incorporating individual rational behavior and the possibility of variants can change policy prescriptions. In particular, as the old endogenously protect themselves, the planner prefers to vaccinate the young first since this larger group contributes to more social encounters. The young-first vaccination program leads to less deaths overall than the old-first alternative. However, the results change if more infectious variants can arise with the stock of infected individuals. More infectious variants are particularly dangerous to the old. Hence, in the presence of variants, a lower number of deaths is achieved with the old-first vaccination scheme.",Vaccines and variants: A comment on “optimal age-based vaccination and economic mitigation policies for the second phase of the Covid-19 pandemic”,https://www.sciencedirect.com/science/article/pii/S0165188922000082,12 January 2022,2022,Research Article,104.0
"Maliar Lilia,Maliar Serguei","The Graduate Center, City University of New York, USA,CEPR and Hoover Institution, Stanford University, USA,Santa Clara University, USA","Received 19 June 2021, Revised 20 December 2021, Accepted 21 December 2021, Available online 28 December 2021, Version of Record 6 January 2022.",https://doi.org/10.1016/j.jedc.2021.104295,Cited by (5),. Our TensorFlow-based implementation of DLC is tractable in models with thousands of state variables.,"Dynamic macroeconomic models are generally built on the assumption of continuous-set choices. For example, the agent can distribute wealth in any proportion between consumption and savings or she can distribute time endowment in any proportion between work and leisure. But certain economic choices are discrete: the agent can either buy a house or not, be either employed or not, either retire or not, etc.==== The progress in modeling discrete choices is still limited because such choices are more challenging to characterize.====In the present paper, we introduce a deep learning classification (DLC) method that can be used to solve a broad class of dynamic economic models with both continuous-set and discrete-set choices. To solve for continuous-set choices, we apply a projection-style method, specifically, we parameterize decision functions with a deep neural network, and we find the coefficients of the neural network (biases and weights) to satisfy the model’s equations. Our main novelty is a classification method for constructing discrete-set choices. We define a state-contingent probability function that, for each feasible discrete choice, gives the probability that this specific choice is optimal; we parameterize the probability function with a deep neural network; and we find the network parameters to satisfy the optimality conditions for the discrete choices.====As an illustration, let us consider the image recognition problem, which us a typical classification problem in data science. For example, a machine classifies images into cats, dogs and sheep. We parameterize the probabilities of the three classes with a deep neural network. The machine is given a collection of images and is trained to minimize the cross-entropy loss (which is equivalent to maximizing the likelihood function) that ensures the correct classification of images; see Goodfellow et al. (2016) for a survey of classification methods in data science.====Our classification method for constructing discrete choices in economics is analogous to the above image-recognition analysis. For example, in a model with three indivisible labor choices, we use a deep neural network to parameterize the probabilities of being full-time employed, part-time employed and unemployed.==== The machine is given a collection of employment choices conditional on state and is trained to maximize the likelihood function that those choices are optimal. The same idea can be applied for analyzing the models with retirement, default, house purchase, etc.====However, the analogy between the image recognition and our solution technique is not exact. Image recognition is the skill that human beings instinctly know how to do, so that software is trained on a set of images that human being first identified as matching or not matching. In our application, the software is trained to do something that human beings do not instinctly know how to do, namely, to construct matched vectors of independent and corresponding dependent variables that satisfy a set of model’s equations.====The classification solution method we propose can be used to solve small-scale representative agent models. However, the power of deep learning consists in its ability to solve large-scale applications that are intractable with conventional solution methods. To illustrate these remarkable capacities of the proposed DLC method, we solve a large-scale application, specifically, a version of Krusell and Smith (1998)’s model in which the agents face indivisible labor choices.====The studied model features heterogeneous agents, incomplete markets and borrowing constraints and is computationally challenging even in the absence of discrete choices. The state space of the studied model may include thousands of state variables of heterogenous agents and is prohibitively large. To make the model tractable, Krusell and Smith (1998) construct a reduced state space of each agent by considering her own state variables and one or few aggregate moments of the wealth distribution; see Den Haan (2010) for a review of earlier techniques for reducing the state space. Furthermore, several recent papers use linearization and perturbation to simplify the analysis of equilibrium in heterogeneous-agent models, including Ahn et al. (2018); Boppart et al. (2018); Childers (2016); McKay and Reis (2016); Mertens and Judd (2017); Reiter (2010); Winberry (2018), (Bayer and Luetticke, 2020); see Reiter (2019) for a thoughtful discussion of that literature.====A distinctive feature of our DLC method is that it does not rely on moments, linearization, perturbation or any other pre-designed reduction of the state space but works with the actual state space consisting of all individual and aggregate state variables – we let deep neural network to choose how to condense large sets of state variables into much smaller sets of features. Our code is written using Google’s TensorFlow platform – deep learning software that led to many ground breaking applications in data science – and is it tractable in models with thousands of state variables.====The studied indivisible labor model was analyzed in important contributions of Chang and Kim (2007) and Chang et al. (2019). The former paper extends Krusell and Smith (1998)’s analysis to include indivisible labor choice by constructing value functions of employed and unemployed agents; their approach reduces the discrete choice problem to the analysis of two continuous-choice value functions. The latter paper offers a more simple and tractable way of modeling indivisible-labor choice by discretizing the first-order conditions of the associated divisible labor model, namely, it assumes that the labor is divisible and that the agent decides on how many hours to work but if the chosen hours fall below a certain level, the agent becomes unemployed. With that assumption, the model with discrete choices is reduced to a familiar setup with continuous labor choices and occasionally binding constraints. We go a step further and solve for entirely discrete choices by using classification techniques instead of relying on some continuous choice representations. Our decision functions tell us when the agent switches from one discrete choice to another, conditional on the individual and aggregate states.====We solve three numerical examples: a version of Krusell and Smith (1998)’s model with continuous choices (i.e., divisible labor), a version of that model with continuous-discrete choices (i.e., indivisible labor) and the corresponding representative-agent model. We find that the introduction of indivisible labor helps us correct some shortcomings of the divisible labor model in reproducing the data, in particular, the data on labor markets. One improvement relatively to empirical data is that the volatility of labor increases relatively to the output; in that respect, our findings are similar to the indivisible labor framework of (Hansen, 1985, Rogerson, 1988). Another improvement is a reduction in the correlation between labor and wages which is excessively high in the representative-agent model with divisible labor; this implication is an outcome of both the assumptions of indivisible labor and heterogeneous agents. As for distributional implications, the predictions of our heterogeneous-agent model with indivisible labor are similar to those of the models studied by Chang and Kim (2007) and Chang et al. (2019). First, the assumption of indivisible labor increases the degrees of inequality, helping to bring the model closer to the data. Furthermore, unlike in the divisible labor model, the degrees of income and wealth inequalities in the indivisible labor economics are less sensitive to variations in the coefficient of risk aversion. However, we conclude that the assumption of indivisible labor alone is not sufficient to produce empirically relevant degrees of income and wealth inequality in the model. We should emphasize that this is a common outcome of the Krusell and Smith (1998) type of model (without assuming heterogeneity in discount factors).====Our DLC method is related to recent papers on deep learning, including Duarte (2021), Villa and Valaitis (2019), Fernández-Villaverde et al. (2018), Azinovic et al. (2020), Lepetyuk et al. (2020) and especially, Maliar, Maliar, Winant, Maliar, Maliar, Winant, 2019, Maliar, Maliar, Winant, 2021. However, this literature does not analyze models with discrete choices, which is the main subject of the present paper. From the other side, there are numerous methods in econometrics for estimating discrete-choice models but these methods are limited to statistic applications; see Train (2009) for a review. An exception is an endogenous grid method with taste shocks by Iskhakov et al. (2017) that is designed to deal with discrete choices in dynamic environment; see also Iskhakov and Keane, 2021 for an application of this method for estimating a partial equilibrium model with discrete labour supply. In the context of Carroll (2005)’s analysis, these papers suggest to apply logistic smoothing to the kinks by transferring the problem into the choice probability space via the taste shocks. There is an important conceptual difference between our analysis and those papers, namely, we do not attempt to smooth the kinks but instead to accurately approximate such kinks by using the-state-of-the-art deep learning classification method.====Finally, our solution method is related to the fields of supervised, unsupervised and reinforcement learning from machine learning literature. First, nonlinear regression equations, which we estimate using artificial data, can be viewed as a generalization of canonical supervised learning; see Maliar et al. (2021) for a discussion. Second, the decision and value functions are not known in economic models, so we can also interpret our solution method as unsupervised learning; such interpretation is advocated in Azinovic et al. (2020) Third, our solution method approximates not only the decision and value functions but also the ergodic set, so it has a direct connection to reinforcement learning; see Goodfellow et al. (2016) for a review of supervised and unsupervised learning in the computer science literature including deep learning; see Sutton and Barto (2018) for a review of reinforcement learning literature, and see Powell (2008) for a review of the related field of approximate dynamic programming.====The rest of the paper is as follows: In Section 2, we set up the Krusell and Smith (1998) model with divisible labor choice; in Section 3, we solve the model with indivisible labor choice; in Section 4, we analyze the model with full- and part-time employment; in Section 5, we compare the aggregate and distributional predictions of divisible and indivisible labor models; and finally, in Section 6, we conclude.",Deep learning classification: Modeling discrete labor choice,https://www.sciencedirect.com/science/article/pii/S016518892100230X,28 December 2021,2021,Research Article,105.0
"Guerini Mattia,Harting Philipp,Napoletano Mauro","University of Brescia, Italy,Bielefeld University, Germany,GREDEG, CNRS, Université Côte d’Azur, France,Sciences Po, OFCE, France,SKEMA Business School, France,Institute of Economics, Scuola Superiore Sant’Anna, Italy","Received 13 May 2021, Revised 23 November 2021, Accepted 17 December 2021, Available online 21 December 2021, Version of Record 3 January 2022.",https://doi.org/10.1016/j.jedc.2021.104294,Cited by (1), populated by publicly owned firms. A bargaining process between firm’s stakeholders determines the optimal allocation of financial resources between real investments in R&D and financial investments in shares buybacks. We characterize the relation between governance and investment strategy and we study how different governance structures shape technical progress and competition over the industrial life cycle. Numerical simulations of a calibrated set-up of the model show that pooling together industries characterized by heterogeneous governance structures generate the well-documented inverted-U shaped relation between competition and innovation.,"Publicly traded companies gather a collection of distinct stakeholders with different and possibly diverging interests (Gillan and Starks, 2007). Within the nexus of explicit and implicit relations between stakeholders – among which are equity holders, managers, creditors, employees, suppliers and customers – a prominent role is played by the interaction of managers and equity holders (Jensen and Meckling, 1976). In fact, the former have a contractual relationship with the owners of a company through which they are in charge of the operational decisions of a firm, whereas the latter exercise the control over the management (Hill and Jones, 1992). The purpose of governance structure, defined as the collection of mechanisms, processes and relations by which corporations are controlled and operated (Shailer, 2004), is to mitigate the agency problems associated to these relationships and to reconcile the diverging interests of the stakeholders within a firm.====Many publicly traded firms are characterized by some degree of concentration in terms of share ownership. Holderness (2009) reports that 96% of U.S. public firms have at least one large stockholder.==== In many instances, such blockholders are institutional investors like banks, hedge funds, mutual funds, pension funds, and other corporations. In fact, based on a sample of 203 U.S. public companies Hotchkiss and Strickland (2003) find that, on average, institutional investors hold about 60% of shares while the 20 largest investors account for about 40%. Large stockholders, therefore, play an active role in the firms strategic decision-making and can exploit different mechanisms through which they exert their power over the company.====However, as highlighted by Sherman et al. (1998), blockholders can be rather heterogeneous in terms of different dimensions, one of them is their investment horizon. Hotchkiss and Strickland (2003) show that about 27% of shares of the average firm in their sample are held by investors with low turnover, while 11% are held by investors with high turnover. In addition, Ehrhardt and Nowak (2003) find that for former family owned corporations, 5 years after an IPO, the family shareholder still holds 60% of the shares. This, therefore, can characterize the family blockholder as a long-term investor. However, for the same companies, about 25% of shares are in free flow, traded among short-term investors with high turnover rates. Having blockholders with different investment horizons as investors within the same firm can induce principal-principal conflicts that arise in addition to the agency problems in the relation between shareholders and the management (see, e.g., Young et al., 2008).====The above structural aspects of corporate governance can have a significant impact on innovation and competition both at the firm and industry level. Several studies have indeed shown how different aspects of corporate governance may influence firm innovation. One of them is the ownership structure of firms (e.g. Kim et al., 2008). Another one is the presence of institutional investors and career concerns of managers (e.g. Aghion, Van Reenen, Zingales, 2013, Chakravarty, Grewal, 2011). A third aspect is represented by the financial performance-based remuneration system for managers (e.g. Honoré et al., 2015), which is one possibility to solve agency problems between the management and shareholders (see, e.g., Bebchuk and Fried, 2003). At the same time, those studies have not formulated a comprehensive theory describing how the interaction among different aspects of corporate governance may influence innovation by heterogeneous firms, and how the latter may in turn shape the process of competition within an industry. This is the main task we try to accomplish in this paper.====The main scope of our paper is to study how different institutional arrangements, as measured by the type of governance structure, impact upon innovation and competition in an industry populated by publicly owned firms. To fulfill this objective, we develop a formal model that explains how several aspects of governance structure of a publicly traded firm interact in determining firm’s decision about the allocation of funds either to a productivity-enhancing R&D investment or to an investment in share buybacks. On the one hand, R&D investments increase a firm’s productivity and competitiveness in the industry. They can therefore generate higher profits and thus increase dividends and managers’ bonuses. On the other hand, share buybacks increase a firm’s share price. They thus have an immediate impact on shareholders’ wealth and on the value of managers’ share-based remuneration.====We focus on buybacks as an alternative investment opportunity due to their growing importance as a corporate payout policy (see, e.g., Farre-Mensa et al., 2014). Besides using buybacks as a signal in case the management observes an undervaluation of the firm, the major reason for buyback programs discussed in the literature is to distribute excess liquidity to shareholders (Abuaf, 2012, Grullon, Michaely, 2004). In this case, firms prefer buybacks over dividends either for tax reasons or due to market expectations, where also the presence of large blockholders can influence the choice of the distribution channel (e.g. Renneboog and Trojanowski, 2011). However, there are also empirical evidences that a higher volume of share buybacks reduces real investments, where this crowding-out can be linked either to the presence of institutional investors (e.g. Gutiérrez and Philippon, 2018) or the compensation scheme of managers (e.g. Bhargava, 2013, Turco, 2018). In a theoretical contribution, Dawid et al. (2019) employ a dynamic heterogeneous agent industry model in which managers, who are paid in bonuses and shares, decide on how much of the liquidity of the firms will be invested in R&D or share buybacks. Their findings support the hypothesis that a higher share-based remuneration induces higher stock repurchases while reducing the real investments.====In our model, we assume that managers and shareholders are the relevant stakeholders for a firm’s investment decision. In addition, shareholders are divided in two different groups according to their investment horizon: short- or long-term investors. The three stakeholders participate to a tripartite bargaining process that determines firm’s optimal allocation of internal funds between R&D investments and shares buybacks. Corporate governance structure, in this context, encompasses the distribution of bargaining power among investors and the management, as well as the managerial remuneration scheme composed of bonus- and equity-based elements. The bargaining power of each stakeholder is determined by the distribution of shares among the short- and long-term oriented investors, and by the robustness of the management regarding possible interference of shareholders.====We first characterize the impact of governance structure on the investment behaviour of a single firm by employing a static and deterministic version of the model. We show that a stronger presence of short-term investors, and/or a higher share-based compensation of managers crowds out R&D investments in favour of buybacks. Although the static model already provides insights on how corporate governance impacts on innovation, it does not allow to study how the latter may alter the process of competition among heterogeneous firms and the dynamics of an industry. For this reason, we consider a dynamic and stochastic version of the model that allows us to study the effects of governance structure upon technical change and competition over the industry life cycle (Klepper, 1996). Simulations using a calibrated set-up of the above extended model demonstrate the existence of non-trivial relationships between governance structure and industry dynamics. While technical progress is positively associated with the presence of long-term investors, the link between market concentration and ownership structure is multifaceted. The relation can either be monotonically decreasing or inverted-U shaped, depending on (i) the managerial independence, (ii) the stage of the industry life-cycle, and (iii) the short- or long-term orientation of managers induced by their payment scheme.====In a final step, we study whether governance structure can play a role in explaining the relation between competition and innovation at the industry level. Empirical evidence has indeed pointed to the existence of an inverted-U shaped relation between innovation and competition (see, e.g., Aghion et al., 2005).==== We show that our model is able to generate such a relation by aggregating innovation rates and levels of market concentration generated by industries with different governance structures and at different stages of their life-cycle. This last finding thus provides a complementary view on the link between technical change and competition with respect to Aghion et al. (2005).====The paper is organized as follows. In Section 2, we provide a short review of the relevant literature about corporate governance and firm innovation and we list two stylized facts about governance structure and innovation that our paper tries to address. Section 3 introduces the static model of corporate governance and formally describes the effects of governance structure on the optimal allocation of resources. Sections 4 and 5 respectively describe the dynamic industry model and its calibration. The results of extensive simulation experiments addressing the effect of governance structure on competition and technical progress are discussed in Section 6. Section 7 provides conclusive remarks. Complementary technical details can be found in the Appendix.","Governance structure, technical change, and industry competition",https://www.sciencedirect.com/science/article/pii/S0165188921002293,21 December 2021,2021,Research Article,106.0
"Ciola Emanuele,Gaffeo Edoardo,Gallegati Mauro","Fondazione Eni Enrico Mattei, Italy,Department of Management, Universitá Politecnica delle Marche, Italy,Department of Economics and Management, Universitá degli Studi di Trento, Italy","Received 2 August 2020, Revised 2 September 2021, Accepted 12 December 2021, Available online 17 December 2021, Version of Record 28 December 2021.",https://doi.org/10.1016/j.jedc.2021.104292,Cited by (3),This paper develops and estimates a ,"Credit to non-financial businesses and households is notoriously pro-cyclical (Covas, Haan, 2011, Schularick, Taylor, 2012), a fact that in the last three decades has attracted a huge amount of theoretical work meant to explore the channels linking financial flows to real macroeconomic activity (Foglia et al., 2011). The received pre-2007 approach was centred on the idea that causality runs from changes in the real sector to movements in financial flows, whereas financial frictions impinging on the borrowers capacity amplify the macroeconomic impact of exogenous shocks to productivity or preferences (Bernanke, Gertler, 1989, Greenwald, Stiglitz, 1993, Kiyotaki, Moore, 1997). In turn, the bulk of the research emerged after the global financial crisis has offered theories pointing toward an inversion of the causality nexus. In the models surveyed in Brunnermeier et al. (2012) and Christiano and Ikeda (2011), a shrinkage in the total amount of funds channelled from lenders to borrowers derives from disruptions in financial markets due to shocks affecting either banks capital or liquidity. Recessions are therefore the outcome of a cut in spending and hiring by borrowers generated by a supply-induced credit tightening.====In this paper, we aim at providing an integrated analysis of both views, examining the combined effect of competition in the assets and liabilities sides of banks’ balance sheets on credit cycles. We take as a starting point the main reason for the existence of financial intermediaries, which is to allow (or improve) the channelling of funds from savers to borrowers. In other words, we analyse how the search for profits of banks interrelates with the twofold competitive pressure arising from their demand for credit and the supply of capital. Indeed, while maximizing their profits, financial intermediaries must balance total assets with liabilities and, at the same time, take into account the behaviour of competitors. With this in mind, the main objective of this work is to investigate how competition in the deposit and credit markets can generate endogenous cycles and how this interacts with the real economy. In particular, we show that the presence of strategic complementarity in the behaviour of banks – mainly due to the accumulation of information capital – is the main source of credit cycles, an explanation that has recently acquired renewed interest since the seminal work of Cooper and John (1988) (see, for example, Beaudry et al., 2020).====We perform this analysis by developing an Agent-Based Model (ABM) in which the flowing of funds from savers to investors is intermediated by a stream of banks competing in fully decentralized markets for deposits and loans, and agents are imperfectly aware of the economic opportunities they potentially face. In our setting, the transmission channel between the financial and the real sectors turns out to be bidirectional, while cyclical fluctuations emerge endogenously as intermediaries adjust the size of their balance sheet to the varying competitive conditions affecting the asset and liability sides, respectively. In particular, we assume that banks compete on prices by copying the strategies of the most profitable competitors, either to attract demand deposits from the households and to offer long-term – but freely severable – credit contracts to firms. As a result, business cycles emerge endogenously in the model because of strategic complementarity in the behaviour of banks. Specifically, due to information costs in locating profitable opportunities, the economy is affected by matching and allocation imperfections that co-evolve endogenously, giving rise to periods of sustained growth followed by sharp recessions. In particular, financial crises lead banks to close a large share of their credit lines, including also the most profitable ones. That, in turn, damages the long-term efficiency of capital allocation because financial institutions need time to reconstruct their portfolios of loans, producing a steady but slow recovery. In this way, our model can reproduce the implicit information capital of banks introduced by Stiglitz (2015).====While close in spirit to the stream of research dealing with equilibrium search in credit markets (Becsi, Li, Wang, 2013, Den Haan, Ramey, Watson, 2003, Diamond, 1990, Wasmer, Weil, 2004), our model is firmly rooted in the tradition of the agent-based literature exploring the emergence of macroeconomic features from the localized interactions of heterogeneous agents employing decentralized matching and bargaining protocols (Fagiolo, Dosi, Gabriele, 2004, Gaffeo, Delli Gatti, Desiderio, Gallegati, 2008, Gaffeo, Gallegati, Gostoli, 2015, Guerini, Napoletano, Roventini, 2018, Riccetti, Russo, Gallegati, 2015). This approach offers us two key advantages. First, we can extend the macroeconomic-oriented analysis of credit allocation from a static multiple equilibria framework to a dynamic one, where credit mismatches develop from feedback effects as agents interact in distinct but interrelated customer markets (Gottfries, 1991). Second, we do not need to rely on an explicit functional form to model the matching technology, but we can adopt a more realistic approach to reproduce market interactions and analyse the effects of banks behaviour on macroeconomic outcomes.====Lastly, our article contributes to the existing literature on ABMs introducing a novel estimation methodology (see Lux and Zwinkels (2018) and Fagiolo et al. (2019) for a review, and Platt (2020) for a comparison of the available methods). In particular, we combine the well-established techniques derived from the Method of Simulated Moments==== (MSM) with the newly introduced Bayesian estimators of ABMs (Delli Gatti, Grazzini, 2019, Grazzini, Richiardi, Tsionas, 2017). Specifically, we exploit the distributional properties of the MSM criterion function to generate the Bayesian posterior distributions of the structural parameters of the model. As a result, our methodology allows us to overcome the analytical and computational complexity of finding the minimum of the criterion function and provides a complete set of statistics of the model. Overall, the time paths we obtain through simulations show that the statistical features of aggregate production and interest rates are in line with those observed in real-world data. Interestingly enough, this result is obtained in a simplified framework completely abstracting from agency frictions, aggregate disturbances to primitive parameters, time-varying risk-taking due to capital regulation or institutional arrangements like a deposit guarantee or a lender of last resort authority.====The rest of the paper is organized as follows. Section 2 presents the model and provides a discussion of the initial conditions used in simulations. Section 3 introduces the estimation procedure and describes the results obtained from United States (US) data. Section 4 takes stock of simulations to highlight the existence of endogenous and co-evolving business and financial cycles. Section 5 concludes and outlines directions for further research.",Search for profits and business fluctuations: How does banks’ behaviour explain cycles?,https://www.sciencedirect.com/science/article/pii/S016518892100227X,17 December 2021,2021,Research Article,107.0
"Roger Tristan,Roger Patrick,Willinger Marc","CEREFIGE, ICN Business School, Université de Lorraine, 86 rue Sergent Blandan, 54000 Nancy, France,LaRGE Research Center, EM Strasbourg Business School, University of Strasbourg, 61 avenue de la Forêt Noire, 67085 Strasbourg Cedex, France,CEE-M, Univ Montpellier, CNRS, INRAE, SupAgro, Avenue Raymond Dugrand, CS 79606, 34 960 Montpellier Cedex 2, France","Received 2 May 2021, Revised 10 November 2021, Accepted 14 December 2021, Available online 16 December 2021, Version of Record 20 January 2022.",https://doi.org/10.1016/j.jedc.2021.104293,Cited by (0),"We show that the acuity of the Approximate Number System (ANS), a cognitive system that allows humans and many animal species to evaluate quantities without using exact calculations, is a strong predictor of subjects’ earnings in experimental markets. We measure ANS acuity with a ","Since Markowitz (1952) and Sharpe (1964), conventional finance models assume that economic agents are rational. The rationality assumption implies that agents have unlimited cognitive skills. These rational agents are able to instantaneously perform complex calculations that lead to optimal investment decisions. In practice, however, agents have limited cognitive abilities. They rely on heuristics to compensate for their cognitive limitations (Kahneman, 2011). Since heuristics involve performing approximate calculations, agents skilled at quickly performing such approximate calculations are likely to make better economic and financial decisions. This ability to perform such approximate calculations is linked to the acuity of the Approximate Number System (ANS).====The ANS is an important feature of the brain, allowing humans and many animal species to approximately evaluate and discriminate quantities without counting or using a symbolic representation of numbers (Odic and Starr, 2018 and Feigenson et al., 2004 for a review). The ability to estimate numbers (albeit imprecisely) is deeply rooted in the human brain and precedes the acquisition of formal mathematical education. Evidence of the existence of the ANS has been found in babies (Hyde, Spelke, 2011, Izard, Sann, Spelke, Streri, 2009, Libertus, Brannon, 2009, Libertus, Brannon, 2010, Lipton, Spelke, 2003, Xu, Spelke, 2000, Xu, Spelke, Goddard, 2005) and even in animals (Agrillo et al., 2008, Agrillo, Dadda, Serena, Bisazza, 2009, Gallistel, Gelman, 1992, Garland, Low, Burns, 2012, Mehlis, Thünken, Bakker, Frommen, 2015, Nieder, Dehaene, 2009, Pepperberg, 2006, Perdue, Talbot, Stone, Beran, 2012).====A popular approach for measuring the acuity of the ANS is the Number Line Estimation (NLE) task (Booth, Siegler, 2006, Siegler, Booth, 2004, Siegler, Opfer, 2003), sometimes called Symbolic-number Mapping task (SMAP). It involves a mapping between numerical and spatial representations of numbers. The task consists of locating a given number on an empty bounded number line with 0 at the left end and 100 (or 1000) at the right end. Because of its simplicity, the NLE task is suited to everyone (the literature in neuropsychology shows that even young children are able to perform the NLE task) and requires only a few minutes to be completed.====Recent empirical research on the ANS acuity (and more specifically on the NLE task) provides growing evidence of the link between ANS acuity and mathematical competence (Chen, Li, 2014, Fazio, Bailey, Thompson, Siegler, 2014, Halberda, Mazzocco, Feigenson, 2008, Schneider, Beeres, Coban, Merz, Susan Schmidt, Stricker, De Smedt, 2017, Schneider, Merz, Stricker, De Smedt, Torbeyns, Verschaffel, Luwel, 2018).==== ANS acuity also has implications for economic valuation and decision making (Peters, Bjalkebring, 2015, Peters, Slovic, Västfjäll, Mertz, 2008, Schley, Peters, 2014, Sobkow, Olszewska, Traczyk, 2020). For instance, Schley and Peters (2014) show that, when it comes to estimating the economic value of a given good, incorrect valuations mainly come from an inexact mapping of symbolic numbers onto mental magnitudes, that is, a lack of acuity of the ANS. In four decision-making problems, Sobkow et al. (2020) show that the ANS acuity is the strongest predictor of optimal choices, beyond cognitive reflection and fluid intelligence.====Today, individuals face more complicated financial choices and have greater exposure to financial markets. It is therefore paramount to understand the drivers of financial decisions. Since differences in ANS acuity are linked to mathematical abilities and decision-making, it is likely that approximate numeracy also matters for financial decisions. In this paper, we test whether approximate numeracy is a predictor of financial performance in a continuous double auction experimental market. Experimental markets are an ideal setting to test the influence of approximate numeracy on subjects’ earnings. Indeed, experimental markets are usually organized as a sequence of short trading periods lasting 2 or 3 minutes. Trading and managing a portfolio in such a setting requires to take quick buy/sell decisions (to adjust one’s orders for changes in fundamental value (FV hereafter) or to take advantage of trading opportunities arising from others’ mistakes). Taking decisions under time pressure involves the use of heuristics. We therefore expect individuals exhibiting acute approximate numeracy to perform better. We also analyze whether approximate numeracy has an influence on mispricing. We expect that the presence of subjects with high ANS acuity contributes to reducing the deviations between prices and FV.====The design of our experimental asset market is similar to the one of Smith et al. (1988). Subjects engage in two consecutive double auction markets. In contrast to the design of Smith et al. (1988), which involves dividend payments after each trading period resulting in a decreasing FV, our subjects trade a risky asset with a randomly fluctuating FV, similar to the design 4 of Stöckl et al. (2015). This kind of FV process shares more features with real financial markets since the FV is not deterministic. Specifically, at the end of each period, a cash flow is drawn from a given five-point uniform distribution. No intermediate dividends are paid, however. Instead, the randomly drawn cash flows accumulate. They are paid at the end of the market to owners of the risky asset. In addition to experimental market data, we also collected information on subjects’ characteristics. Subjects completed an exit questionnaire, similar to the one proposed by Palan (2015), where they provided demographic information and answered questions on risk aversion.====In a first analysis, we divide the population of subjects (24 sessions with 9 subjects per session) in quartiles with respect to their scores on the NLE task. We find that subjects’ earnings in the bottom quartile of NLE accuracy are 16.01% below average while earnings of subjects in the top quartile of NLE accuracy are 11.44% above average. This difference is significant at the 1% level. We then perform a multivariate analysis where we control for differences in trading activity and demographic characteristics (trading volume, order initiation decisions, background education, age, gender, risk aversion, previous experience in experimental markets). Our multivariate analysis confirms that ANS acuity, measured by the accuracy on the NLE task, successfully predicts subjects’ earnings.====In a second analysis, we provide additional insights on some of the channels through which high NLE accuracy traders obtain higher earnings. We show that these subjects issue more profitable limit orders than their low NLE accuracy counterparts and execute market orders under better conditions. Our results indicate that, overall, subjects pay a high price for immediacy. On average, market orders take place 4.23% above FV for purchases and 10.96% below FV for sales. The cost of immediacy is particularly important for low NLE accuracy traders since they are ready to pay 6.79% above the FV to acquire the asset and are willing to sell 15.69% below the FV (compared to respectively ====0.95% and 1.95% for high NLE accuracy subjects). In addition, we also find that high NLE accuracy traders are faster and better at exploiting price inefficiencies.====A third analysis links mispricing to NLE accuracy. Overall, we find that sessions in which traders score high on the NLE task exhibit lower mispricing. We show that only a few traders with high NLE accuracy are sufficient to reduce mispricing. This result is in line with high NLE accuracy traders being more efficient in dealing with limit orders that significantly deviate from FV.====Our paper makes contributions to several strands of literature. We contribute to the literature on experimental markets by uncovering a new predictor of earnings in market experiments. Previous research on what makes a “good” trader (Corgnet et al., 2018) is based on tests like the Cognitive Reflection Test (CRT) (Frederick, 2005) or Raven Progressive Matrices (RPM). However, recent papers (Skagerlund, Forsblad, Tinghög, Västfjäll, 2021, Sobkow, Olszewska, Traczyk, 2020) show that numerical competencies, and especially approximate numeracy, are strong drivers of decision making. For instance, Sobkow et al. (2020) consider five variables; three measures of numeracy (statistical, subjective and approximate numeracy), one measure of fluid intelligence, and one measure of cognitive reflection (i.e., the score on the standard 7-item CRT). They perform a horse race between models to explain performances in four types of problems: a lottery task where people select among pairs of lotteries, a decision outcome inventory test as in Bruine de Bruin et al. (2007), and two memory tests. The score on the NLE task is the only variable that is significant in predicting decision outcomes in all four types of problems. Moreover, the score on the NLE task is the most significant variable in the four regressions including the five explanatory variables (CRT, fluid intelligence, subjective, statistical, approximate numeracy).====We also contribute to the literature that links mispricing to cognitive abilities. As mentioned above, the distribution of NLE accuracy within a session has an influence on the level of mispricing in this session, even after controlling for average risk aversion and previous experience in economic experiments. Our results resonate with previous experimental studies that demonstrate a link between mispricing and cognitive abilities. Breaban and Noussair (2015) show that deviations from FV correlate negatively with session-average CRT scores. Bosch-Rosa et al. (2018), using three different indicators (CRT, Guessing Game and Race to 60), find lower mispricing in sessions composed of subjects with high cognitive abilities.","Number sense, trading decisions and mispricing: An experiment",https://www.sciencedirect.com/science/article/pii/S0165188921002281,16 December 2021,2021,Research Article,108.0
"Dawid Herbert,Muehlheusser Gerd","Bielefeld University, Department of Business Administration and Economics, and Center for Mathematical Economics, Universitätsstrasse 25, Bielefeld D-33615, Germany,University of Hamburg, Department of Economics, Von-Melle-Park 5, Hamburg D-20146, Germany and CESifo and IZA","Received 18 March 2021, Revised 12 November 2021, Accepted 23 November 2021, Available online 2 December 2021, Version of Record 20 December 2021.",https://doi.org/10.1016/j.jedc.2021.104288,Cited by (5),"We analyze the role of product liability for the emergence and development of smart products such as autonomous vehicles (AVs). We develop, and calibrate to the U.S. car market, a dynamic model where a (monopolistic) innovator chooses safety stock investments, the timing of market introduction, and the product price. Inducing higher long-term product safety through a strict (partial) liability rule reduces short-term safety investments and slows down AV market penetration. By contrast, negligence-based liability fosters initial investments without hampering long-term product safety. However, too stringent liability might forestall investments in the development of AVs and their market introduction.","The main agenda of this paper is to evaluate from a dynamic perspective the effect of product liability on crucial aspects of product innovation such as investments into product safety, the timing of market introduction and market penetration. The major innovative aspect is that the dynamic framework allows to identify a number of novel inter-temporal effects and trade-offs, and to analyze how these are affected by different liability regimes. Our analysis is motivated by the current policy discussion on suitable product liability in the context of smart products, in particular autonomous vehicles (AVs). Therefore, we perform our dynamic analysis using a calibration based on the U.S. car market. In our baseline model where liability is strict, increasing the liability share of producers is beneficial for product safety in the long-run, but leads to less safe products in the short- and intermediate run. This inter-temporal trade-off does not arise when liability is negligence-based. Under both rules, more stringent liability leads to later market introduction of the innovation.====The advent of digitization and the movement towards a ==== constitute major policy challenges, for example with respect to infrastructure investments, standardization, regulation, and legislation.==== are one important part of this development. According to Porter and Heppelmann (2015), the capabilities of smart products can be grouped in four areas: monitoring, control, optimization, and autonomy, where the latter is the focus of this article. Products exhibiting autonomy rely less and less on human decision-making, and major decisions are instead being taken by the product’s operating system. One example of particular topicality are Autonomous Vehicles (AVs), which is the prime application in this article, but similar developments are also observed for other smart products ranging from household appliances to heavy military weapons.====One alleged major benefit of AVs is that in the long run, when fully matured, they are potentially much safer than conventional vehicles in terms of their accident rate. The main reason is that decision-making by AI algorithms is potentially less error-prone compared to human drivers, who are often impaired by poor sight and slow reactions times, for example due to fatigue, distraction, alcohol or drug consumption. While (conventional) cars have become considerably safer over the last decades, car crashes are still a significant problem and create a huge social cost, so that any significant reduction in accident risks would generate potentially large social benefits.====However, these long-run benefits of AVs cannot be expected to be reaped immediately upon their market introduction. In the meantime, as long as the technology is not yet fully matured, AVs are likely to be involved in crashes in the same way as conventional cars are, potentially at even a higher rate. This is highlighted by the recent fatal crashes caused by AVs of Uber and Tesla during test drives, triggering considerable public attention.==== Under the assumption that the safety of AVs can be improved over time through (costly) R&D investment, this raises the question of ==== and with which safety level AVs should be brought to the market. For example, it is argued that upon introduction, AVs should be at least as safe as conventional cars, if not much safer (see e.g. Schellekens, 2015, p. 510).====The possibility of accidents caused by AVs which are controlled by non-human drivers raises interesting and novel aspects with respect to the issue of product liability. In particular, unlike other and less significant product innovations, in the case AVs a new type of injurer emerges: the AV’s operating system. As computer systems cannot be held legally responsible, however, the question arises who should be liable for the harm arising from accidents caused by AVs. As one main feature (and alleged benefit) of AVs is that its driver retains less control and hence becomes more like a passenger rather than the operator of the car, many argue that there should be less scope for legal responsibility of drivers, and more scope for car manufacturers and automotive software developers (see the literature discussion below). Despite the current legal uncertainty to which extent producers of AVs and their operating systems will ultimately be held liable, they are expected to face substantially higher liability costs than they currently do with conventional cars.====One crucial question is to which degree these higher liability risks affect firms’ innovation activities and other business policies, such as R&D investments (e.g. to improve the safety of AVs), the timing of market introduction of AVs and pricing. Together these decisions determine (i) when AVs are eventually launched (if at all) and with which safety level, (ii) how quickly they penetrate the market, and (iii) how long it takes until the long-term benefit of a higher safety of AVs compared to conventional cars can be expected to materialize. For example, legal scholars such as Marchant and Lindor (2012, pp. 1336), Colonna (2012, p.84) and Schellekens (2015) argue that too stringent liability for producers of AVs might considerably delay these processes. The agenda of this paper is to address these questions. Since these are genuinely dynamic issues, they call also for a dynamic perspective in the analysis.==== We consider a dynamic model of product innovation, where there already exists a market for an established product (the ==== product), and where a monopolistic innovator can introduce a new one (the ==== product). Consumers differ with respect to their valuation for the smart product. Both products are prone to accidents, which are socially harmful. At each point in time, the innovator decides on (i) how much to invest into the safety level of the smart product which reduces its accident rate, and (ii) the price at which it is sold, once it has been launched.====We analyze how the dynamically optimal decisions and the resulting state dynamics depend on the liability regime, which apportions the damage accruing from accidents with the smart product between the innovator and the consumers. More precisely, we are interested in how liability affects (i) the safety level of the smart product in the long run (steady state), (ii) the optimal dynamic paths for safety investments as well as output and pricing choices, thereby also determining when and with which safety level the smart product is launched, and (iii) overall welfare. In the main part of this article, we examine these issues in a framework of ==== for the innovator.==== The stringency of liability is determined by the fraction of the damage from AV accidents to be covered by the innovator. Additionally, we explore how alternative liability regimes, in particular ====, perform with respect to questions (i)–(iii) posed above.====We first provide an analytical characterization of the effect of the stringency of liability on the steady states of the model. The dynamic properties of the optimal paths are then examined using numerical methods, where we calibrate the model using data from the US automobile market. Considering the impact of liability on the optimal behavior of the innovator, our analysis reveals two basic trade-offs for policy makers and legislators: The first trade-off concerns the existence and optimality of an ==== steady state, in which the innovator indeed accumulates a safety stock for the smart product, and eventually introduces it to the market: As liability becomes more stringent, the safety level of the smart product in an active steady state increases. However, too stringent liability might forestall the development of the smart product altogether. In particular, the innovator might prefer to induce a ==== steady state, in which there is no investment into the safety stock of the smart product, and where it is never launched. Overall, these results are consistent with static theoretical analyses and empirical findings showing that the effect of liability on innovation activities is ex ante unclear and might go in either direction (Galasso, Luo, 2017, Galasso, Luo, 2021, Viscusi, Moore, 1993, Viscusi, Moore, 1991).====The main innovative contribution of our paper is to point attention to a second trade-off which concerns the short- and long term effects of liability: More stringent liability has the long-run benefit of inducing a higher steady state safety level of the smart product. However, the analysis of the investment dynamics reveals that it also leads to a delay in the accumulation of the safety stock in early periods. Intuitively, under more stringent liability it takes longer until the smart product is launched, quantities are smaller, and hence the market penetration occurs more slowly. This reduces the innovator’s incentives for safety investments. As a result, the units brought to the market in the short- and intermediate term exhibit a lower safety level compared to those that would have been produced under less stringent liability. For the context of AVs, our analysis suggests that these short-term costs outweigh the long-term benefit from more stringent liability. As a result, under strict (partial) liability it would be socially optimal to ==== impose additional liability costs on the producers of AVs, compared to the status quo with conventional cars only.====We also analyze two alternative liability rules: First, we consider ==== as analyzed by Hay and Spier (2005), and our findings are qualitatively very similar to the baseline setting. Second, we analyze ==== liability, where the innovator is only liable if the AV’s safety level is below a given negligence standard. We find that such a liability rule ameliorates the issue of delay in safety stock accumulation, and thereby also reduces quantity distortions relative to the social optimum. Consequently, from a welfare perspective, an appropriately designed negligence-based rule can outperform a rule with strict (partial) liability. Finally, we also consider a policy of ==== of AVs in the sense that they can only be launched once they satisfy a given minimum safety standard. We show that the induced investment patterns and market dynamics are very similar to those under negligence-based liability.====Our article is related to a sizeable literature on how (product) liability affects the incentive of firms to improve the safety of their products, either by making existing products safer or by developing new and safer products. With respect to such innovation activities, previous research has shown that on the one hand, more stringent liability might increase the incentive to develop innovations which improve product safety. But on the other hand, it might have a “chilling effect” on innovative activities, for example in the form of choosing to not develop new products (Parchomovsky, Stein, 2008, Viscusi, 1991, Viscusi, 2012), even if they are potentially superior than existing ones. From an empirical point of view, both effects seem relevant (see e.g. Galasso, Luo, 2017, Galasso, Luo, 2021, McGuire, 1988, Viscusi, Moore, 1993), and this trade-off might indeed capture the long-term effects of product liability on innovation activities and product safety. However, it is so far unexplored how product liability affects the dynamic patterns, and our framework allows to analyze in more detail the interplay of the various effects in the short- und long-term, as well as the timing of innovations.====Furthermore, our analysis points to the consumer side as an important driver of investments into the safety of AVs, even when product liability is weak. The reason is that consumers anticipate that they will have to bear a large part of the accident risk, which reduces their willingness to pay. This channel is also stressed by Polinsky and Shavell (2010) in their rather critical discussion of product liability.====Two recent articles have also addressed the issue of liability in the specific context of AVs in static settings: Shavell (2020) analyzes optimal liability rules in a world with AVs only. Our approach can hence be seen as complementary, as we focus on the dynamic process of development and market penetration of AVs, and the (potentially long) transition period in which autonomous and conventional vehicles coexist. A hybrid setting with both types of vehicles is also considered in Friedman and Talley (2019). In their model with multilateral care, in which not only producers can contribute to the reduction of accident risks, they analyze the efficiency properties of several negligence-based rules. However, they do not consider investment or innovation incentives, which are crucial aspects of our framework.====Our paper is also related to the (static) theoretical literature on minimum quality standards, where firms compete in both prices and product quality. In a seminal paper, Ronnen (1991) has shown that a minimum quality standard leads to intensified price competition which benefits consumers through higher product quality at lower prices, and it can also lead to higher welfare. Subsequent papers have studied, for example, alternative cost functions for product quality (Crampes and Hollander, 1995), non-duopoly settings (Scarpa, 1998), international competition (Boom, 1995) or quality leadership (Lehmann-Grube, 1997). Also in our setup, consumers do value product quality (car safety). However, we consider a dynamic setup and focus on the impact of product liability on investment incentives, market outcomes and welfare over time. Our analysis points to a strong similarity of negligence-based liability and minimum quality (safety) standards with respect to investment patterns and market dynamics.====We now turn to the literature relying on dynamic models of market introduction and innovation investments. Henry et al. (2019) analyze the role of different regulatory schemes (strict liability, ex-ante and ex-post regulation) on the timing of market introduction under uncertainty about the (binary) quality of a product. They examine how these rules can be used to align social and private incentives in terms of costly experimentation ex ante (which generates signals about the product quality), market introduction, and eventual withdrawal of the product from the market (in case of sufficiently bad information about its quality after is has been launched). Our approach differs from Henry et al. (2019) in at least three crucial aspects: First, we consider investments in product safety. Second, we examine the qualitative differences between short-run and long-run implications of the different regulatory schemes. Finally, in our model the market environment (in particular, the (expected) future quantities sold) is an important driver of investment incentives, whereas in their setting a single unit of the product is considered.====As the safety of smart products is a key quality parameter, the innovator’s investment in building up the safety stock is closely related to (quality-enhancing) product innovation investments. Although dynamic models of optimal innovation investments have mainly focused on process innovations (e.g. Cellini, Lambertini, 2009, Kováč, Vinogradov, Žigić, 2010), several contributions have characterized optimal product innovation investments of monopolists in different market settings (e.g. Dawid, Keoula, Kopel, Kort, 2015, Lambertini, Mantovani, 2009). Concerning the explicit consideration of the interplay between innovation investments and the timing of market introduction, our article is related to Hinloopen et al. (2013), where the focus is, however, on process innovation.==== Moreover, none of these articles considers the impact of liability. Our numerical analysis uses collocation methods for solving the Hamilton-Jacobi-Bellman equation characterizing the optimal investment behavior of the innovator. In that respect our article is related to contributions such as Doraszelski (2003), Dawid et al. (2015) and Dawid et al. (2017) where such methods have been employed for the numerical analysis of dynamic investment problems.====Last, but not least, we contribute to a current legal debate about how existing laws and other legal procedures need to be adapted in response to the new legal challenges arising in the digital economy, in particular in the context of AVs (see e.g. Colonna, 2012, Douma, Palodichuk, 2012, Galasso, Luo, 2018, Geistfeld, 2017, Gless, Silverman, Weigend, 2016, Marchant, Lindor, 2012, Schellekens, 2015, Smith, 2017, Wagner, 2018).==== To the best of our knowledge, our article is the first one to analyze these issues from a theoretical perspective in a dynamic model framework.====The remainder of the article is organized as follows: Section 2 lays out the model framework. Section 3 analyzes the different steady states of the model, and how the steady state optimally induced by the innovator depends on the stringency of product liability. Section 4 analyzes in more detail how product liability affects the dynamic paths leading into the respective steady state. In doing so, we proceed numerically and calibrate the model to the US automobile market. In Section 5 we consider negligence-based liability rules and direct safety regulation. Section 6 explores the policy implications of our findings and discusses further model extensions. The Online Appendix contains auxiliary results, proofs, details about our calibration and the numerical method as well as additional numerical results and robustness tests.","Smart products: Liability, investments in product safety, and the timing of market introduction",https://www.sciencedirect.com/science/article/pii/S0165188921002232,2 December 2021,2021,Research Article,109.0
"Gao Xuefeng,Xu Tianrun","Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong","Received 2 September 2019, Revised 28 September 2021, Accepted 18 November 2021, Available online 29 November 2021, Version of Record 13 December 2021.",https://doi.org/10.1016/j.jedc.2021.104287,Cited by (1)," takes into account of the bid-ask queue imbalance, the queue position of an order and price dynamics. We calibrate and validate the model using the historical order book data and backtesting simulations, and show that our model can perform well empirically. We also combine our model with multi-armed bandit learning to guide order cancellation decisions. We illustrate the empirical performances of various bandit algorithms and show that the Upper Confidence Bound algorithm generally performs the best.","The adoption of electronic trading systems has transformed financial markets into trading platforms with the limit order book as a dominant mechanism to match buyers and sellers. In a limit order market, participants can submit market orders, which are typically filled immediately at the best available price. They can also submit limit orders with specified price limits, and these limit orders will wait in queues for execution in a limit order book. The aim of using limit orders is to capture a portion of the bid-ask spread while minimizing the costs including the adverse selection cost. See, e.g., Parlour and Seppi (2008) for a survey of limit order markets.====In this paper, we develop an order scoring model that quantifies the performance of an individual limit order before it is filled. Order scoring here refers to allocating a resting limit order with a numerical score to proxy its expected profit. The limit order we consider is not forced to be filled by some deadline. So the profit of an order is assumed to be zero if the order is cancelled exogeneously, and otherwise it is measured by the difference of its execution price with the fair value of the asset at the time of order execution. Our objective is two fold. Firstly, to relate the order score with a set of features that describe the condition of the limit order and market including the bid-ask queue imbalance, the queue position of an order and price dynamics. Secondly, to illustrate the application of the order scoring model in order cancellation decisions.====We focus on large-tick assets, where the bid-ask spread is usually equal to the minimum value of one tick, and the minimum price variation (one tick) is large relative to the asset, see, e.g., Dayri and Rosenbaum (2015); Eisler et al. (2012); Moallemi and Yuan (2016). These assets typically are liquidly traded and have long queues of limit orders at the best quotes. We consider orders at best bid/ask quotes, and the performance of an individual order depends on the queue position of an order. In fact, in a limit order market with price-time priority, liquidity providers compete for front queue positions in order to obtain execution priority of their limit orders when trading large-tick assets (Yao and Ye, 2018). Moreover, the adverse selection cost is lower for limit orders at the front of the queue than for orders at the back of the queue which require a larger market order to execute (Glosten, 1994, Sandås, 2001).====In addition to the queue position of a limit order, another feature that is important in estimating the pre-trade performance of a limit order is the bid-ask queue imbalance. This feature measures the imbalance between the volume at the best bid and the volume at the best ask, and it can be used to predict future price movements. Indeed, there is considerable empirical evidence that this queue imbalance is an important determinant of order flows, and hence it affects the fill probability of limit orders at the best quotes, see, e.g., Cartea et al. (2018); Cebiroǧlu and Horst (2015); Cont and De Larrard (2013); Gould and Bonart (2016); Lehalle and Mounjid (2017); Lehalle and Neuman (2017); Sirignano (2019).====This paper scores limit orders at the best quotes by taking into account of these two features, as well as other important features including price dynamics. We develop a stochastic model for this purpose. A key component is the modelling and estimation of the random execution time of a limit order, which depends not only on the queue position of the order, but also on the bid-ask queue imbalance. To address this problem, we use a theoretical approach to model the execution time which is based on the first-passage time of the volume-weighted mid-price (“microprice”) to some boundary. This first-passage-time based approach to model limit order executions has been considered in the literature for its simplicity and tractability, where the boundary is often set to be the limit price at which the order is placed and the price is often chosen to be the mid-price or the transaction price in the existing studies (see, e.g., Harris (1998); Lillo (2007); Lo et al. (2002)). Different from these studies, we specifically consider the microprice, which encodes the information of the bid-ask queue-imbalance and is modelled by a discrete-time Markov chain. We also use the microprice as a proxy of the fair value of the asset in order scoring. Furthermore, we select an appropriate boundary to capture that a limit order gains better position in the queue and hence is more likely to be filled as time progresses. Together it allows us to efficiently compute and accurately estimate the order score.====Specifically, we show via Markov chain analysis that the order score satisfies an recursive equation which can be numerically solved in an efficient and accurate manner. Moreover, we estimate and validate our model using the historical order book data from NASDAQ and backtesting simulations, and we find that our model can perform well empirically. Though our model is parsimonious and stylized, it sheds some light on the effects of key features such as the bid-ask queue imbalance, the price transitions and the queue position on the pre-trade performance of a limit order. In particular, the mathematical expression of the order score allows us to study these effects explicitly. This is in contrast to fitting a parametric function of the imbalance and the queue position to the order scores. Such a parametric function may be difficult to capture the effect of the micro-price dynamics, and hence it may provide inaccurate estimation of the order score under different market regimes.====Since the limit order we consider is not required to be filled by some deadline, the set-up in this paper is more suitable for applications including market making rather than optimal execution. In particular, our order scoring model may be useful for decision problems such as when to join the best quote or when to cancel an existing order, in a broader context of electronic marekt making (Gao, Wang, 2020, Moallemi, Yuan, 2016). To illustrate the application of our model, we apply it in guiding the real-time decision for canceling an order or letting it rest in the order book. Market participants especially market makers need effective cancel strategies to help them avoid getting adversely selected and incurring a loss. For instance, a limit buy order submitted by a trader might be one of the last in the queue to be filled, and the market trades down to a lower bid price after the order is filled. This creates an immediate loss for the trader. In such a case, the trader may regret having had the bid order filled and should have cancelled the order earlier to avoid ending up with bad fills.====Using our model, we propose a score-based cancellation strategy where one cancels a limit order if the order score drops below a certain threshold. This strategy is not derived from an optimal control formulation. Rather it is a simple heuristic strategy that is interpretable and might be suitable in the environment of high-frequency trading where actions are made within seconds or milliseconds. One may use a fixed threshold value for different orders in the proposed cancellation strategy. However, we do not know the best threshold value in advance when implementating the cancellation strategy on new data. If one uses the optimal threshold computed from past data, it might yield suboptimal empirical performance of limit orders as we will see in our backtesting simulations.====Hence, to improve the performance and “learn” a good (unknown) threshold, we adjust the threshold values for different orders adaptively using algorithms for multi-armed bandits (MAB). MAB is a classic reinforcement learning problem that deals with the trade-off between exploration and exploitation (Bubeck, Cesa-Bianchi, 2012, Sutton, Barto, 2018). In our setting, we can not observe the profit of a limit order unless we choose the threshold and implement the cancellation strategy. Moreover, with a fixed threshold, the distribution of the profit associated with this threshold could change with time since market conditions can change on a daily or intraday basis. Hence, even if one has historical data, it might not cover all financial scenarios, and active information gathering or exploration might be useful. Therefore, the problem of choosing the sequence of threshold values for the order cancellation strategy is tackled by the bandit algorithms.====Specifically, we combine the order cancellation strategy with bandit learning, by applying various bandit algorithms to “learn” a good (unknown) threshold in simulated trading. The bandit algorithms we evaluate include ====greedy, Upper Confidence Bound (UCB), Gradient Bandit, Thompson Sampling and three other algorithms that are desgined for non-stationary and contextual bandits. We find that the proposed strategy can be effective in improving the order performance by cancelling orders in anticipation of adverse price movement and reducing the chance of orders being adversely filled. We also find that the strategy based on bandit learning can outperform the strategy that uses a fixed cancellation threshold. Among the various bandits algorithms considered, we find that the class of UCB algorithms generally performs the best in backtesting simulations.==== We explain the differences between our work and the related literature.====Our work is closely related to Moallemi and Yuan (2016). They developed a dynamic model for valuing a limit order based on its queue position at the best quote for large-tick assets, and found that the value of a front queue position can be on the same order of magnitude as the bid-ask spread. The model in Moallemi and Yuan (2016) assumes that the fill probability of a limit order at the best quote is independent of the bid-ask queue imbalance. We show empirically that the order score and order fill probability critically depends on the bid-ask queue imbalance and the relative queue position of an order (see Section 3). This motivates us to incorporate the bid-ask queue imbalance in order scoring, and we develop a new model to do so. In particular, since the bid-ask queue imbalance relates two sides of the order book and it affects the statistical properties of order flows in a complex way, we choose not to directly model such order flow dynamics. Instead we use a first-passage time based approach to model limit order executions which is different from Moallemi and Yuan (2016). In addition, we use the order scoring model for informed order cancellation decisions to illustrate its applications.====Our work is also related to the market microstructure literature. Theoretical microstructure models often use a zero expected profit condition to derive an equilibrium order book. See, e.g., Glosten (1994); Parlour and Seppi (2008); Sandås (2001) and the references therein. The static model in Sandås (2001) showed that the expected profit of a limit order is a function of the fundamental value and the queue position of the limit order. Relying on the model in Sandås (2001), Skouras and Farmer (2012) provided queue position valuation in a single period setting. Recently, Dahlström et al. (2017) extended the model in Sandås (2001). They showed that the expected profit of a limit order is a function of the prices and volumes at the best bid and ask prices in the limit order book, and analyzed the determinants of limit order cancellations. Compared with this line of literature, our model is a dynamic one and we use the order scoring model for making informed order cancellation decisions.====In terms of applications, the limit order placement and cancellation problems have also been studied in the literature using different approaches, see, e.g., Donnelly and Gan (2018); Lehalle and Mounjid (2017). Both Donnelly and Gan (2018) and Lehalle and Mounjid (2017) used a stochastic control approach to investigate the optimal order cancellation decisions. Our work departures from their framework in that we combine the order cancellation decision with a multi-armed bandit learning approach, and that we empirically test the score-based cancellation strategy using backtesting simulations based on real data. Moreover, one of our focuses is the development of an order scoring model that can help quantify the pre-trade performance of an order and that relates the order score with a set of attributes asscoaited with the order and market.====There are extensive studies on multi-armed bandit algorithms and their empirical evaluations. See, e.g., Bubeck and Cesa-Bianchi (2012); Kuleshov and Precup (2014); Sutton and Barto (2018); Vermorel and Mohri (2005) and the references therein for details. In the original form of the multi-armed bandit problem, a gambler must decide which arm of a ====-slot machine to pull to maximize his expected total reward in a series of trials. Each arm, when pulled, generates a random reward following an unknown distribution. To maximize the total reward, the gambler needs to acquire information about arms (exploration) while simultaneously optimizing immediate rewards (exploitation). Multi-armed bandit problems have a large number of applications including internet advertising, recommender systems, clinical trials, etc. In the context of market microstructure and algorithmic trading, bandit-style algorithms have been primarily developed for optimal allocation of orders across multiple trading venues or liquidity pools. This problem resembles a multi-armed bandit problem where each liquidity pool can be considered as 1 arm and a trader wants to maximize the total shares executed over multiple rounds of allocations. See, e.g., Agarwal et al. (2010); Ganchev et al. (2009); Laruelle et al. (2011) for details. Our work studies a different problem by scoring limit orders in a single order book and we evaluate bandit algorithms empirically by combining them with score-based order cancellation strategies.====In a broader context, our work is related to a growing body of literature on applying reinforcement learning (RL) algorithms (beyond bandit algorithms) in algorithmic trading. For instance, several RL algorithms have been applied to the optimal trade execution problem, see, e.g., Akbarzadeh et al. (2018); Nevmyvaka et al. (2006); Ning et al. (2018). The order scoring model we develop is not primarily oriented for optimal execution, because the limit order under consideration is not forced to be filled by some deadline. On the other hand, RL algorithms have also been applied for optimal market making in limit order markets, see, e.g., Chan and Shelton (2001); Lim and Gorse (2018); Spooner et al. (2018). These studies, however, do not focus on large-tick assets as we do in this paper. Moreover, our focus is to study the performance of an individual limit order and use it for informed order cancellations while these studies focus on the problem of optimal market making.====Finally, we briefly mention that there is also rich literature on Markov chain models for the dynamics of mid-prices or the limit order book, see e.g. Cartea and Jaimungal (2013); Cont et al. (2010); Stoikov (2018) and the references therein. Cartea and Jaimungal (2013) used a finite-state hidden Markov model for the tick-by-tick dynamics of mid-prices, and employed the model to develop high-frequency trading strategies. Cont et al. (2010) proposed a continuous-time Markov chain model for order book dynamics and computed conditional probabilities of various events given the state of the order book. Stoikov (2018) proposed a new definition of microprice which is given by the limit of a sequence of expected mid-prices. They showed empirically that it is a better predictor of short term prices than the mid-price or the volume-weighted mid-price.==== The rest of this paper is organized as follows. In Section 2, we introduce the ex ante performance measure of limit orders. In Section 3, we conduct simulation of artificial orders and show empirically that order scores and order fill probabilities depend on the bid-ask queue imbalance and the relative queue position of the limit order. In Section 4, we propose a Markov-chain based model for scoring limit orders. In Section 5, we estimate and validate the model using historical limit order book data. In Section 6, we apply our model to design order cancellation strategies using multi-armed bandit algorithms. Finally, Section 7 concludes. Auxiliary results, proofs and some model extensions are given in the Appendix. All statements and results are given for the limit buy orders to simplify the presentation. The case for the sell orders can be studied similarly.","Order scoring, bandit learning and order cancellations",https://www.sciencedirect.com/science/article/pii/S0165188921002220,29 November 2021,2021,Research Article,110.0
"Bruns Martin,Lütkepohl Helmut","School of Economics, University of East Anglia, Norwich Research Park, Norwich NR4 7TJ, United Kingdom,DIW Berlin and Freie Universität Berlin, Mohrenstr. 58, Berlin 10117, Germany","Received 27 May 2021, Revised 30 September 2021, Accepted 15 November 2021, Available online 29 November 2021, Version of Record 26 December 2021.",https://doi.org/10.1016/j.jedc.2021.104277,Cited by (4), tends to be superior to its competitors and to perform as well as a standard VAR estimator for sufficiently large samples.,"In structural macroeconometric analysis, local projection (LP) estimators of impulse responses, as proposed by Jordà (2005), have become increasingly popular despite some evidence that they may be inefficient in small samples if the true underlying data generating process (DGP) is a vector autoregression (VAR) (e.g., Choi, Chudik, 2019, Kilian, Kim, 2011, Meier, 2005). LP estimators are based on linear regressions only, while VAR based impulse responses are nonlinear functions of the VAR slope coefficients. Thereby LP estimators can be defended as nonparametric estimators of impulse responses (Angrist, Jordá, Kuersteiner, 2018, Stock, Watson, 2018). They are sometimes regarded as more robust to model deficiencies, which can excuse their small sample inefficiency in standard scenarios. Also, there has been evidence that, in some small sample situations, the loss in efficiency may be quite limited, depending on the choice of the VAR lag order (Brugnolini, 2017). However, based on a large number of DGPs that do not have a finite-order VAR representation, Li et al. (2021) conclude that impulse response estimates based on approximating VARs tend to have much smaller variances than LP estimates but the latter may have smaller bias in small samples. Plagborg-Møller and Wolf (2021) present general conditions for VAR and LP methods to be equivalent tools for impulse response analysis in population.====In this study we focus on a structural VAR setup where the true DGP is a finite-order VAR process and the structural shocks are linear transformations of the reduced-form errors. We also assume that an external instrument or proxy is used to estimate the impact effects of a shock and, thus, the structural parameters (see Gertler, Karadi, 2015, Mertens and Ravn, 2013, Stock, Watson, 2012). In other words, we focus on a conventional proxy VAR framework. If a suitable external instrument exists, it is also possible to use LP estimators for the corresponding structural impulse responses (e.g., Breitung, Brüggemann, 2019, Plagborg-Møller, Wolf, 2021).====The potential small sample inefficiencies of LP estimators have motivated research in modifications with better small sample properties. By now, a number of alternative LP estimators have been proposed (e.g., Breitung, Brüggemann, 2019, Plagborg-Møller, Wolf, 2017, Stock, Watson, 2018, Lusompa, 2021). The objective of this study is to review and compare the different LP estimators for proxy VAR models in our framework. We derive similarities between the different estimators and even provide conditions for some of them to be numerically equivalent. Some of these results are not apparent from the previous literature. We also compare the small sample properties of the various estimators in a Monte Carlo study.====Anticipating the results, we find that two generalized least squares (GLS) projection estimators dominate the other LP estimators in terms of root mean squared error (RMSE). A lag-augmented GLS version proposed by Breitung and Brüggemann (2019) is the best performing estimator for smaller processes and it is about as accurate in terms of RMSE as a competing LP GLS estimator proposed by Lusompa (2021) for larger VAR models. The lag-augmented GLS estimator also yields small confidence intervals which may, however, have coverage below the desired nominal coverage in small samples if they are constructed with a moving-block bootstrap. For moderately large samples, the estimator has similar properties to the standard VAR estimator if the true DGP is a finite-order VAR process, as assumed in the following.====The study is structured as follows. In the next section the proxy VAR model is presented which is the basis for the LP estimators included in our comparison. In Section 3, the alternative estimators for structural impulse responses are discussed. Section 4 reports small sample results and Section 5 concludes. A number of additional details and results are collected in the Online Supplement which accompanies this paper.",Comparison of local projection estimators for proxy vector autoregressions,https://www.sciencedirect.com/science/article/pii/S0165188921002128,29 November 2021,2021,Research Article,111.0
"Chen Cathy Yi-Hsuan,Fengler Matthias R.,Härdle Wolfgang Karl,Liu Yanchu","Adam Smith Business School, University of Glasgow, UK,University of St. Gallen, School of Economics and Political Science, St. Gallen, Switzerland,Blockchain Research Centre, Humboldt-Universität zu Berlin, Germany,Sim Kee Boon Institute for Financial Economics, Singapore Management University, Singapore,Department of Probability and Mathematical Statistics, Charles University, Prague, Czech Republic,Department of Information Management and Finance, National Yang Ming Chiao Tung University, Hsinchu, China,Lingnan College, Sun Yat-sen University, Guangzhou, China","Received 19 September 2020, Revised 22 November 2021, Accepted 24 November 2021, Available online 26 November 2021, Version of Record 16 December 2021.",https://doi.org/10.1016/j.jedc.2021.104290,Cited by (1),We investigate the informational content of a huge assortment of NASDAQ articles about a joint cross-section of S&P 500 ,"It has been established – based on large bodies of text – that written documents contribute to price discovery in equity markets by carrying informational content that extends beyond the information sets created from past observations and other traditional market factors alone (for a survey, see Loughran and McDonald, 2016, and references therein). Text documents can contribute to price discovery, e.g., if processing textual information is costly in the sense of Hong et al. (2000) or due to behavioral biases of investors; see, e.g., Tetlock (2011). On the other hand, a growing number of studies stress the role of derivatives markets for price discovery in equity markets: Dennis and Mayhew (2002), Pan and Poteshman (2006), Xing et al. (2010), Stilger et al. (2016), among others, provide evidence that option market variables offer predictive power for future stock returns. The predictive power is attributed to the idea that informed traders maximize the value of their private information about stocks by trading in the derivatives market because option leverage and the relatively smaller number of market restrictions, such as short-sell constraints in stock markets, create powerful trading incentives.====In this work, we connect both strands of literature by examining the predictive power of single-stock option data for equity markets and measures of media-expressed tone simultaneously. Our motivation is that after all, when investors form their outlook for a particular stock based on textual information, they also need to choose a marketplace (the stock market or the derivatives market) to execute their trading idea. We therefore conjecture that textual information influences the equity market and the derivatives market alike. Moreover, a trading decision may rely on public information only, or on a mixture of private and public information. Hence, we separate textual information from other information embedded in option data and ask whether this “residual information” reaches beyond what is summarized in traditional market factors ==== textual information.====To accomplish this task, we measure firm-level textual news tone from a large text corpus scraped from NASDAQ news feed channels, which covers US companies listed in the S&P500 index. We refer to our textual measure as media-expressed “tone” rather than “sentiment,” because we aim at setting it apart from the notion of sentiment as a not necessarily fact-based manifestation of emotions (Baker and Wurgler, 2007); see Section 2.2 for more details. We then explore whether trading-hour media tone is informative about three key single-stock option characteristics (OCs), namely implied volatility (i.e., ====), out-of-the-money put prices (i.e., ====), and the implied volatility skew (i.e., ====). Paralleling the findings from stock return data, we establish that both firm-level media-expressed tone as well as the cross-sectional aggregates of firm-level tone, i.e., tone indices, have a measurable impact on these OCs.====Equipped with this empirical evidence, we examine the predictive power of single-stock OCs for equity returns. In line with previous research, we find that OCs predict stock returns and remarkably that they continue to do so in the presence of tone variables, whereby the negative tone index emerges as a particularly powerful predictor variable. To study this predictive power more closely, we use the tone data along with conventional predictors to partial out publicly available information absorbed in option data. Using these orthogonalized components of OCs, we find that they still predict stock returns, which signals a substantial amount of insider information. Lastly, we check the economic significance of the statistical results and compare the profits of two long-short trading strategies. The first one – along with the extant literature – is based on OCs only, while the second one builds on the OCs orthogonalized to tone. We find that the latter strategy dominates the former in terms of Sharpe ratio, no matter which OC it is based on. In particular, the ==== residual-based strategy can obtain a Sharpe ratio of 3.93 (versus 2.87 for the ====-based one), while it is 2.23 (versus 0.24) for ====, and 1.27 (versus 0.21) for ====. Thus, we conclude that the information content of option data reaches beyond what is summarized both in traditional risk factors and media-expressed tone.====In addition, we also discover new results about the divergent informational content of trading-hour versus overnight information. All our predictive stock return regressions underline that overnight information, i.e., information collected from articles in the preceding – not overlapping – night, is more informative than the younger trading-time tone. This parallels recent findings of Boudoukh et al. (2019), who conclude that overnight news is more informative about firm fundamentals than news that is released during the trading day. In order to shed further light on our observations, we apply topic models to the two archives, the trading-time and the overnight archive, to unearth their hidden thematic content. We find that trading-time and overnight articles cover noticeably different topics with little mutual overlap. These differing thematic emphases could contribute to the distinct predictive power of the different news archives.====As regards our techniques of textual analysis, we build on a more refined tool kit than traditionally used in the extant literature. Usually, rooted in a “bag-of-words” document model, one employs a dictionary-based counting process, a so-called lexicon projection; see, e.g., Cao et al. (2002), Das and Chen (2007), Schumaker et al. (2012), Chen et al. (2014), and Zhang et al. (2016). Bommes et al. (2020), however, observe that supervised learning algorithms trained on a phrase bank realize superior classification results because they achieve a surpassing explication of the linguistic sentence structure. Following these insights, we develop a supervised learning algorithm trained on the phrase bank of Malo et al. (2014) to predict sentence-level tone, but reserve all the tone variables which we derive from a traditional lexicon projection based on the Loughran and McDonald (2011) lexicon for robustness purposes.====The outline is as follows: Section 2 describes the text corpus and briefly presents the techniques used to quantify media-expressed tone. In Section 3, we study tone, option data, and return predictability. Section 4 provides robustness checks. Section 5 concludes, with an appendix offering all details on the data and tone measurement. The archive of NASDAQ articles is accessible from the authors upon request.","Media-expressed tone, option characteristics, and stock return predictability",https://www.sciencedirect.com/science/article/pii/S0165188921002256,26 November 2021,2021,Research Article,112.0
"Quaye Enoch,Tunaru Radu","University of Bristol, School of Accounting and Finance, Bristol, BS8 1PQ, UK,University of Sussex, University of Sussex Business School, Brighton, BN1 9SL, UK","Received 22 March 2021, Revised 1 November 2021, Accepted 11 November 2021, Available online 23 November 2021, Version of Record 20 December 2021.",https://doi.org/10.1016/j.jedc.2021.104276,Cited by (2),This study compares the information on the ,"Aggregate stock market valuation models typically set equity prices equal to the present-value of their expected future dividend payments. To a large extent, such valuation principles depend on accurately forecasting realised future dividend payments but there are challenges to attain error-free future dividend forecasts.====In this paper, for the first time in the literature, we compare the implied volatility of an equity stock index with the implied volatility of dividends corresponding to the stock in that index. We obtain the implied volatility from the options on the stock index and from the options on dividend index futures, respectively, using three different computational schemes, that is the Black-Scholes formula, Black formula and the (Bakshi et al., 2003) model-free approach. Our empirical results indicate that there are significant and economically substantial differences in the implied volatilities of stock on the one side and of its dividends, on the other side. Furthermore, we demonstrate that the information in the prices of derivatives on equity and dividend indices can be used to design profitable trading strategies, demonstrating again that financial markets are not always efficient. In addition, we analyze the determinants of the differences in implied volatilities of the two assets, with GDP, consumer price index and the dollar/euro foreign exchange rate identified as playing a fundamental role. Our paper points out that for economic investigations, even when using derivatives prices, researchers should pay attention to both the equity side and the corresponding dividend side. Our results also provide evidence in favor of the excess volatility puzzle, but on a forward-looking basis, using information extracted from options markets.====Important studies on dividend and stock volatility such as (LeRoy, Porter, 1981, Shiller, 1981, West, et al., 1988) found that movements in stock-index prices are more volatile than movements in actual dividend, indicating a stock dividend excess volatility puzzle. These conclusions are conceptually based on volatility relations derived from expected present-value models using predictive regressions. This has been called the excess volatility puzzle. Other recent extensions to the Shiller present value model consider learning effects on stock price dynamics. (Jagannathan and Liu, 2019) use learning effects in a latent variable present-value model; their findings point out to some possible market-agent aversion to long-run risk and learning impact on volatility of stock-prices. Li and Yang (2013) showed that dividend volatility positively predicts future asset returns, with predictive power improving with the forecasting horizon.====Derivative markets currently trade in stand-alone dividend derivative contracts that come with direct exposure to dividend volatilities. In this regard, the relevant literature holds the view that such markets have a tendency to support rational pricing of assets in stock markets (see Brennan, 1998). Some of these stand-alone contracts are based on index dividends paid by the index constituents with maturities that extend to about a decade. Dividend derivatives are discussed in Mixon and Onur (2016) and Tunaru (2018).====Dividend swaps have emerged recently as a useful innovation to extract forward looking information for futures equity returns, (see van Binsbergen et al., 2013; Golez, 2014). A two-factor model estimated on dividend futures data only is proposed by Kragt et al. (2020) who show that the model can explain a significant part of the dynamics of observed daily EURO STOXX 50® stock market returns. The predictability in patterns of implied volatility (IV) and option prices is statistically possible and it can be attributed to learning in options markets, as shown in Guidolin and Timmermann (2003), Goncalves and Guidolin (2006), Guidolin and Timmermann (2007), Bernales and Guidolin (2014), Bernales et al. (2017).====The EURO STOXX 50® Index Dividend Futures Option contracts are settled in cash following the realized dividends paid during their settlement period, commencing a day after the third Friday in December and ending on the third Friday of December the following year. At maturity the futures value coincides with the cum-dividend index for the respective year, so the European options have in essence a payoff depending on all dividend payments made for the constituent companies in the stock-index in that year.==== Index dividend futures option contracts provide an efficient means of measuring the market participants’ ==== assessment of implied index dividend volatility. Since dividend derivatives have become stand-alone contracts, it is important to investigate the relationship between the implied dividend volatility and the implied volatility of the corresponding stock-index.====In this study, we investigate the stock and dividend volatility puzzle from a derivatives perspective. We analyse the implied volatilities of index-dividend futures (IDF, hereafter) options and stock-index option contracts. For the first time in the literature, we take advantage of a recent set of derivative instruments, the stock-index and IDF options, to investigate jointly the implied volatilities for stock-index and for index-dividend. The novelty of our approach consists in having a ==== view and being able to determine the IV associated with each side, stock-index and index-dividends, from different sets of options. In this context, we outline conditions under which the (dividend) excess volatility puzzle effects inferred from historical data are safely exchangeable with future expectations of it. We explicitly allow for variability in time-to-maturity (TTM) of option contracts and interest rates, while outlying a computational approach that allows aggregation of volatility measures under the Black-Scholes model; Black model and the (Bakshi et al., 2003) model-free approach. We empirically show that the stock and dividend volatility puzzle still exists in this new context but the discrepancy between the two volatilities changes with the horizon. Furthermore, we explore how to identify profitable trading strategies using trading signals generated from the time-varying implied volatility differences that define the dividend puzzle.====This study is organised as follows. Section 2 describes the methodology of the study. Here, we discuss the computational methods applied to investigate the dividend puzzle from option price data. Section 3, presents the data sets and discusses the main empirical results and their implications. We show further numerical investigations on the determinants of IV differences for stock and dividend futures in Section 4. In Section 5, we summarize the findings and conclude the study.",The stock implied volatility and the implied dividend volatility,https://www.sciencedirect.com/science/article/pii/S0165188921002116,23 November 2021,2021,Research Article,113.0
"Longo Luigi,Riccaboni Massimo,Rungi Armando","Laboratory for the Analysis of Complex Economic Systems, IMT School for Advanced Studies, Piazza San Francesco, 19, Lucca 55100, Italy","Received 27 July 2021, Revised 12 November 2021, Accepted 17 November 2021, Available online 20 November 2021, Version of Record 17 December 2021.",https://doi.org/10.1016/j.jedc.2021.104278,Cited by (6)," model accounting for time-variation in the mean with a Generalized Autoregressive Score (DFM-GAS). We show how our approach improves forecasts in the aftermath of the 2008-09 global financial crisis by reducing the forecast error for the one-quarter horizon. An exercise on the COVID-19 recession shows a good performance during the economic rebound. Eventually, we provide an interpretable machine learning routine based on integrated gradients to evaluate how the features of the model reflect the evolution of the business cycle.","Forecasting the future state of an economy has considerably improved since the financial crisis in 2008-09, thanks to methodological advances and an increasing availability of different and heterogeneous data sources (Bańbura, Giannone, Modugno, Reichlin, 2013, Buono, Mazzi, Kapetanios, Marcellino, Papailias, 2017). In this rapidly evolving stream of research, we propose an ensemble that combines a metrics-based Dynamic Factor Model (DFM) with a machine learning approach, which is able to build on the advantages that each method provides in different moments of the business cycle.====Specifically, our ensemble includes a DFM with a Generalized Autoregressive Score (DFM-GAS), as in Creal et al. (2013), and Recurrent Neural Networks (RNN). We show how our ensemble model outperforms a battery of alternative models by forecasting the quarterly US GDP growth rates one quarter ahead in the period 2005Q2-2020Q1. Crucially, we focus on two periods of crisis: the Great Recession in 2008–2009, and the most recent COVID-19 crisis. In the first case, we show how the combination of DFM-GAS and RNN improves accuracy by reducing the forecast error for the one quarter horizon. In the second case, we show that our approach produces good predictions during the economic rebound thanks to the neural network component.====In general, we find that the machine learning component is especially useful in times of crises, when structural breaks occur. The ensemble as a whole produces good predictions for short-term horizons, hence beating many benchmarks both metrics-based (univariate and vector autoregressive, random walk, dynamic factor model) and machine learning models (random forest, XGBoost).====Indeed, we argue, complexity arises from a change in the data generating process that happens every time an economic structural break affects the time series, because in that case what is observed in-sample has limited information on what happens in the out-of-sample window. This is typically the case of economic recessions, and that is why we use an out-of-sample window centered around the Great Recession in 2008–2009 while implementing a Chow test (Chow, 1960) that evaluates performance in times of structural breaks====.==== Eventually, we compute a fluctuation test statistic by Giacomini and Rossi (2010) to understand the RMSE evolution over time, and a test proposed by Clark and West (2007) to evaluate the significant performance gains of our ensemble model.====In the case of the COVID-19 crisis, we perform a one quarter forecast and we find that no previous model is able to predict the initial drop in the GDP growth (2020Q2), possibly because the pandemic is an exogenous event that is not reflected in any economic and financial information available before its outburst, as reflected in traditional sets of predictors used for GDP forecast. Yet, our ensemble approach allows us to better capture the economic rebound in the US, starting already in 2020Q3, thanks to our addition of a neural network component in the ensemble.====Finally, we propose to apply integrated gradients to provide an interpretation of the model and the predictive power of its features. We show how indicators of financial stress and short-term debt were relatively more important to predict the evolution of the Great Recession in 2008–2009. On the other hand, we register how claims filed by unemployed after lockdowns and perceived economic policy uncertainty built on real-time sources (Baker, Bloom, Davis, 2016, Baker, Bloom, Davis, Terry, 2020) follow well the evolution of the COVID-19 crisis. Thus, we discuss how saliency methods like integrated gradients can be powerful tools for policy-makers and scholars to follow during macroeconomic shocks that have different origins.====The rest of the work is organized as follows. Section 2 relates our work to previous literature. Section 3 describes our methodological approach, while Section 4 introduces available data. Results are presented and discussed in Section 5. Section 6 summarizes findings and discusses promising future developments.",A neural network ensemble approach for GDP forecasting,https://www.sciencedirect.com/science/article/pii/S016518892100213X,20 November 2021,2021,Research Article,114.0
"Choi Sangyup,Furceri Davide,Loungani Prakash,Shim Myungkyu","School of Economics, Yonsei University, 50 Yonsei-ro, Seodaemun-gu, Seoul 03722, South Korea,International Monetary Fund, 700 19th Street NW, Washington, DC 20431, USA","Received 21 April 2021, Revised 18 October 2021, Accepted 17 November 2021, Available online 20 November 2021, Version of Record 6 December 2021.",https://doi.org/10.1016/j.jedc.2021.104279,Cited by (7),"Can ==== anchoring foster growth? To answer this question, we use panel data on sectoral growth for 22 manufacturing ","Central bankers often assert that low and stable inflation fosters macroeconomic stability and growth. Former Fed chair Paul Volcker stated, “Inflation feeds in part on itself, so part of the job of returning to a more stable and more productive economy must be to break the grip of inflationary expectations” (Volcker, 1979). The important role of inflation expectations has led many central banks worldwide to improve the transparency of their goals, often explicitly, by adopting an inflation target (IT) and better communication with the public.====This view is underpinned by a large body of theoretical literature suggesting that inflation uncertainty makes it difficult for firms to plan (Fischer and Modigliani, 1978; Baldwin and Ruback, 1986; Huizinga, 1993). Thus, firms may reduce or delay investment when uncertainty about future prices is high.==== While it is well established that heightened uncertainty can slow down the economy's long-run growth via credit constraints (Aghion et al., 2010, 2014), this distortion can be particularly acute in the case of inflation uncertainty.==== To the extent that most financial contracts are written in nominal terms without effective hedging instruments available, inflation uncertainty can affect a firm's borrowing costs, thereby distorting optimal investment behavior.====Higher inflation uncertainty implies a higher likelihood of unexpected inflation in the future, which would arbitrarily redistribute the wealth between savers and borrowers who agree on nominal financial contracts, thereby preventing effective financial intermediation. This adverse effect is likely detrimental for firms that heavily rely on external finance (i.e., are credit-constrained).==== We formalize this intuition by building a simple model, then test its theoretical predictions using a cross-country dataset on industry growth, a country-level proxy for inflation anchoring, and several industry-level measures of credit constraints.====Several authors have tried to demonstrate the benefits of low inflation or inflation volatility in promoting growth. For example, Fischer (1993) and Barro (1996) used cross-section and panel data for a large sample of countries to show that very high inflation was detrimental to growth, after controlling for other factors, over 1960–1990. However, other authors have found it difficult to demonstrate such impacts—particularly in more recent decades, when inflation rates have been lower than in the 1970s and 1980s—or have found the evidence to be fragile. For example, Levine and Renelt (1992) concluded that inflation variables are not robustly correlated with growth using an extreme bounds analysis. Judson and Orphanides (1999) concluded that “the empirical evidence documenting the benefits of low inflation is not very persuasive.”====The main challenge in identifying a link between inflation and growth using aggregate data is that it is very difficult to control for all possible factors that are correlated with inflation (or inflation volatility) and that may, at the same time, affect growth. Moreover, to the extent to which a rise in inflation raises inflation uncertainty, which again feeds back into higher inflation, disentangling the effect of inflation level from inflation uncertainty is not a trivial task. The current paper tries to overcome this limitation by using cross-country sectoral (industry-level) data and applying a differences-in-differences (DID) strategy à la Rajan and Zingales (1998). Our theoretical prediction about which industries should benefit more from inflation anchoring (therefore reducing inflation uncertainty) is motivated by Aghion et al. (2010, 2014). Their work suggests that volatility in the economic environment is particularly harmful to growth for those firms and industries that are credit-constrained, as it pushes them toward short-term investment rather than long-term investment that boosts long-run growth.====We build a stylized model where firms borrow from banks to finance their investment, and the central bank tries to anchor inflation expectations. In our model, since the debt contract is written in nominal terms, inflation anchoring effectively lowers the nominal interest rate and the borrowing costs in the long run, thereby facilitating the provision of credit and the production of output. This positive effect on growth increases with the degree of financial frictions captured by the share of output that the firm can divert in case of default. In other words, credit constraints amplify the growth-enhancing effect of inflation anchoring.====We test this theoretical prediction by examining the sectoral output growth effect of the interaction between a country's measure of inflation anchoring and sector-specific measures of credit constraints after controlling for the unobserved country- and sector-specific characteristics. The framework is estimated for an unbalanced panel of 22 manufacturing industries in 39 advanced and emerging market economies over the period 1990–2014. As explained above, we expect and test the hypothesis that credit-constrained industries tend to achieve relatively faster growth in countries where inflation expectations are well anchored.====The advantages of a cross-industry analysis are twofold:====The main finding of our paper is that inflation anchoring fosters growth in industries that are more credit-constrained. Fig. 1 illustrates our key findings. We plot the average value-added growth of each manufacturing industry from 1990 to 2014 against the inflation de-anchoring coefficients after controlling for the initial share of each manufacturing industry.==== The left panel in Fig. 1 plots this relationship only for industries with the below-median level of external financial dependence (i.e., less credit-constrained industries), whereas the right panel plots the relationship only for industries with the above-median level of external financial dependence (i.e., more credit-constrained industries). Only in credit-constrained industries are larger de-anchoring coefficients (i.e., higher inflation uncertainty) negatively associated with average sectoral growth.====The rest of the empirical analysis aims to establish the robustness of this main finding. First, we extend the measure of credit constraints to include liquidity needs, asset tangibility, and R&D intensity, in addition to external financial dependence. These characteristics are widely used as a proxy for credit constraints at the industry level (Braun and Larrain, 2005; Raddatz, 2006; Ilyina and Samaniego, 2011; Aghion et al., 2014). While we confirm the robustness of the key finding using these alternative measures, external financial dependence appears the most robust predictor of growth differentials.====Second, we disentangle the effect of inflation anchoring from various confounding factors, including the effect of inflation level, by explicitly controlling for the interaction between the level of inflation and industry-specific measures of credit constraints. While these two channels tend to be correlated—since low inflation is often achieved by better inflation anchoring (or a low-inflation environment fosters well-anchored inflation expectations as in Ball, 1992), our findings suggest that the degree of inflation anchoring and the level of inflation do not necessarily capture the same channel. In addition, we also rule out the possibility that our findings simply reflect the effect of nominal rigidities in amplifying the adverse impact of inflation uncertainty. Subsample analyses further indicate that our findings are not driven by the inclusion of euro-area countries with a common monetary policy framework during the second half of the sample period or extreme events such as the Global Financial Crisis and its aftermath.====Third, our main results are also robust to an instrumental variables (IV) approach, using monetary policy transparency and central bank independence as instruments. To the extent that inflation anchoring is likely achieved via the central bank's credibility while industry growth differentials are unlikely driven by central bank credibility itself, instrumenting the inflation anchoring measure using several central bank credibility proxies mitigates endogeneity issues and strengthens the structural interpretation of our findings.====Lastly, our conclusion about the role of credit constraints in determining the growth-enhancing effect of inflation anchoring still holds when we estimate a panel regression using annual industry growth as a dependent variable. To this end, we estimate time-varying inflation de-anchoring coefficients using a Kalman filter and include country, industry, and time-fixed effects to strengthen identification. This finding suggests that the interaction between inflation anchoring and credit constraints is not necessarily limited to a long-run perspective and can induce growth benefits even in the short run.====Our empirical analysis contributes to three streams of literature. The first is the longstanding literature on the link between inflation and growth (e.g., Dornbusch and Frenkel, 1973; De Gregorio, 1993; Barro, 1996; Loungani and Sheets, 1997; Judson and Orphanides,1999).==== The second is the more recent literature on the role of financial frictions in amplifying the effect of uncertainty on growth (e.g., Aghion et al., 2014; Alfaro et al., 2018; Choi et al., 2018; Arellano et al., 2019). The third is the literature documenting heterogeneity across industries regarding their interaction with monetary policy (e.g., Dedola and Lippi, 2005; Peersman and Smets, 2005; Aghion et al., 2015).====The remainder of the paper is organized as follows. Section II illustrates the credit constraint channel through which inflation anchoring can affect growth. Section III discusses our DID methodology and describes various data used in the empirical analysis to test the model's theoretical predictions. Section IV presents the main results and the results from a battery of robustness exercises. Section V offers conclusions.",Inflation anchoring and growth: The role of credit constraints,https://www.sciencedirect.com/science/article/pii/S0165188921002141,20 November 2021,2021,Research Article,115.0
"Pommeret Aude,Schubert Katheline","IREGE, University Savoie Mont Blanc, 4 chemin de Bellevue, 74940 Annecy-le-Vieux, France,Paris School of Economics, University Paris 1 and CESifo, UK","Received 24 August 2020, Revised 27 August 2021, Accepted 31 October 2021, Available online 19 November 2021, Version of Record 11 December 2021.",https://doi.org/10.1016/j.jedc.2021.104273,Cited by (5),"We propose one of the first dynamic models of the optimal transition from fossil fuels to renewables in electricity generation that takes into account the variability and intermittency of renewable energy sources as well as storage. We take as an example solar energy, which is variable (no sun at night) and intermittent (few or no sun at day when there are clouds). We show that when the clouds phenomenon is not too severe, intermittency can be safely ignored and the planner just needs to take into account the deterministic variability of the renewable source. In this case, the optimal transition consists in using fossil fuels at day and night and complement them by solar electricity at day while investing to build up solar capacity; then abandoning fossils at day and keeping them for night electricity generation only; then, when solar capacity is large enough, starting to store electricity; and finally abandoning totally fossils when the carbon budget is exhausted. However, if the cloud problem is severe, intermittency matters a lot and precaution requires to start storage earlier, before fossils have been abandoned at day. We show that renewable electricity generation and storage are complement: absent storage devices, the long run solar capacity is smaller, and so is electricity consumption. We finally provide a quantitative illustration for the case of the Spanish energy transition. We show that in Spain intermittency can be safely ignored. We compute the carbon value corresponding to a 2 ====C carbon budget, the dates at which storage starts, and the path of investment in solar capacity.","The production of electricity has to be decarbonized in the near future (see IPCC (2018)), and producing energy by renewable sources seems to be the best option to do so,==== before nuclear fusion becomes eventually available. The production of electricity by renewable sources has grown rapidly in recent years. At the end of 2020, the estimated renewable energy share in global electricity production is 27%, of which 15.6% is hydropower, 5.8% wind power, and 3% solar PV IEA (2020a). However, hydropower has limited expansion possibilities, and the combined shares of solar and wind power is still very small. It is a long way to the total elimination of the 73% share of non-renewable electricity that is left. In the short/medium term, renewables cannot be deployed at a large scale to replace coal and other fossil fuels in electricity generation. Indeed, on the one hand they are still more costly on average than fossil fuels, and, on the other hand, they are non-dispatchable and are not continuously available.====Contrary to what was the dominant view a few years ago, the cost is not the main obstacle to a larger penetration of renewable sources in the electric mix. The investment cost for wind and solar plants has already been widely reduced, due to technical progress and learning effects in production and installation, and the decrease is expected to continue, until a limit lower bound is reached. For instance, according to the International Energy Agency in its ====, “With sharp cost reductions over the past decade, solar PV is consistently cheaper than new coal- or gas-fired power plants in most countries, and solar projects now offer some of the lowest cost of electricity ever seen” (IEA, 2020b).====The serious problem is posed by the fact that wind and solar energy are not continuously available: they are both variable and intermittent. We define variability as being predictable: it follows a natural pattern such as the alternation of day and night, or of seasons. We consider intermittency as being stochastic, due to unpredictable weather events like cloud formation. Sinn (2017), discussing the German energy transition, stresses that in Germany in 2014 the installed wind power production capacity was 35.92 GW and the average production 5.85 GW, that is 16.3% of the capacity; for solar the corresponding figures are 37.34 GW of capacity, and a production of 3.7 GW, that is 9.9% of the capacity. Without solutions to handle variability and intermittency, renewable sources cannot be considered as good substitutes for conventional fossil sources Joskow (2011) and Lazkano et al. (2017). However, at the moment, issues posed by variability and intermittency are not satisfactorily solved. Practical solutions are the backup of renewables by fossil fuel-fired power plants (the German double structure strategy, see Sinn, 2017), the diversification of sources associated to a dense transmission network, storage—from the classic hydro-pumped storage to new solutions like modern batteries, compressed air storage, flywheels etc., and demand side management. Heal (2016) offers an overview of these solutions and possible lines of modelling.====There is a long tradition of macro-dynamic partial equilibrium models à la Hotelling considering renewable energy as a “backstop technology”. It consists in an abundant and steady flow available with certainty, at a unit cost higher than the unit extraction cost of fossil energy. Early path-breaking papers in this tradition are Hoel and Kverndokk (1996), Tahvonen (1997), and Chakravorty et al. (2006) within the carbon budget approach. In the simplest version of this representation of the energy transition, the price of the fossil fuel, sum of the unit extraction cost and the scarcity rent, rises over time by a Hotelling effect, and the switch to the clean renewable energy occurs when this price reaches the cost of the backstop. Useful as it may have been, this representation is not accurate for the actual energy transition problem.====First, this standard representation overlooks the fact that the relevant cost to be taken into account as far as renewables are concerned is the cost of the investment in capacity rather than the variable operating cost, which is very low. Seminal papers introducing investment in capacity and adjustment costs are Tsur and Zemel (2011) and Amigues et al. (2015). Coulomb et al. (2018) also study the energy transition with capacity constraints and adjustment costs. Second, the standard Hotelling-like literature ignores the variable and intermittent nature of renewable sources, which is the main obstacle to their penetration in the electric mix. An exception is Helm and Mier (2018), who build a dynamic model with a fossil fuel, a variable renewable source and a storage technology. Their aim is to study the optimal subsidies for the renewables and the storage capacity when a carbon tax cannot be put in place. However, they do not account for intermittency.====Another strand of literature is composed of static partial equilibrium models that are not directly interested in energy transition, but focus on the design of the electric mix when intermittency is taken into account, with or without storage devices. Ambec, Crampes, 2012, Ambec, Crampes, 2019 and Helm and Mier (2019) are representative of this literature. They study in a static framework the optimal electric mix with intermittent renewable sources, and contrast it with the mix chosen by agents in a decentralized economy where the retailing price of electricity does not vary with its availability. They examine the properties of different public policies and their impacts on renewable penetration in the electric mix: carbon tax, feed-in tariffs, renewable portfolio standards, demand-side management policies.====The survey on the economics of solar electricity by Baker et al. (2013) emphasizes the lack of economic analysis of clean energy provision through renewable sources. We intend to contribute to fill this lack by putting together the two strands of the literature mentioned above, in order to make macro-dynamic models more relevant for the study of the energy transition. Indeed we believe that the energy transition is by essence a dynamic problem, which cannot be fully understood through static models. On the other hand, dynamic models are so far unable to take into account properly variability and intermittency, with the need for storage they create. Our aim is to propose a simple and tractable dynamic model incorporating these features, to be able to study the extent to which they actually constitute a serious obstacle to energy transition.====In a first step we tackle the variability issue alone. We build a stylized deterministic dynamic model of the optimal choice of the electric mix (fossil and renewable), where the fossil energy, let us say coal for the purpose of illustration, is abundant but CO====-emitting, and the renewable energy, let us say solar, is variable and clean. The originality of the model is that electricity produced when the renewable source of energy is available, and electricity produced when it is not, are considered two different goods: day-electricity and night-electricity in the case of solar energy that we use for illustration. Considering that there are two different goods allows us to take into account intra-day variability. These two goods are produced by potentially different technologies: day-electricity can be produced with coal and/or solar, whereas night-electricity can be produced with coal, or by the release of day-electricity that has been stored to that effect. Storage is costly due to the loss of energy during the restoration process. The two goods are also potentially valued differently by consumers, who, at each period of time, derive utility from the consumption of these two goods: consumers may prefer to consume electricity at day, in which case solar is at peak, or at night, in which case solar is off-peak.====We show that with a low initial solar capacity it is optimal to first use fossil fuels during night and day, then use fossil fuels during night only and finally go for no fossil fuels at all, when the carbon budget is exhausted. Investment in solar panels is not necessarily monotonous. Storage only starts when fossil fuels have been abandoned at day and the solar capacity is large enough. In the long run, we obtain that off-peak sun –a situation known as the Californian duck====– requires larger storage capacities. Finally we show in the case of the Spanish transition that a more stringent climate policy or technological improvements (more efficient solar panels or storage) speed up the transition but have no permanent effect. Moreover, a decrease in the investment cost leads to higher long run solar capacity and day and night-electricity consumptions, at lower prices.====In a second step, we introduce intermittency in addition to variability, and study the design of the power system enabling to accommodate it. With intermittency, day-electricity generation by solar power plants becomes uncertain. We consider that there is only partial generation if solar radiations are too weak due for instance to the cloud system, which occurs with a given probability.====We exhibit two very different situations. If the cloud problem is not too severe, we show that intermittency does not matter so much, rejoining the empirical result of Gowrisankaran et al. (2016). The energy transition follows the same succession of phases as under variability only. On the contrary, if there may be very few sun during the day, the transition is very different as fossil fuels are abandoned later while storage starts earlier. We show that renewable electricity generation and storage are complement: absent storage devices, the long run renewable capacity is smaller, and so is electricity consumption. We obtain quantitatively that during the transition, intermittency requires to maintain less thermal capacities, though for a longer time. In addition, once the electricity sector is decarbonized, intermittency requires a larger capacity of solar electricity-generating equipment, that is, in some sense, overcapacity.====The structure of the paper is the following. Section 2 sets up the framework, solves the model and studies the sensitivity of the solution to the main parameters in the case where variability only is taken into account. In Section 3, we introduce intermittency, examine under what circumstances it qualitatively changes the previous solution, and wonder whether in the presence of intermittency renewable capacity and storage are substitute or complements. In Section 4 we calibrate the model to the Spanish case and perform a quantitative analysis of the sensitivity of the optimal Spanish energy transition to key parameters. Section 5 concludes.",Optimal energy transition with variable and intermittent renewable electricity generation,https://www.sciencedirect.com/science/article/pii/S0165188921002086,19 November 2021,2021,Research Article,116.0
"Lin Qian,Luo Yulei,Sun Xianming","School of Economics and Management, Wuhan University, China,Faculty of Business and Economics, University of Hong Kong, Hong Kong, China,School of Finance, Zhongnan University of Economics and Law, China","Received 15 June 2021, Revised 3 October 2021, Accepted 11 November 2021, Available online 17 November 2021, Version of Record 8 December 2021.",https://doi.org/10.1016/j.jedc.2021.104275,Cited by (4),"In reality, investors are uncertain about the dynamics of the risky ==== (e.g., the expected returns and the correlation between the returns of two risky assets). Consequently, investors make robust investment decisions with special concerns on the expected returns and correlations. In this paper, we propose a hierarchical rule for robust investment between two risky assets: select the relatively safe asset first and then decide how much to invest in the relatively risky asset to hedge the ambiguity embedded in the relatively safe asset. After introducing criteria for relative riskiness and cross-hedging for investors with a constant relative risk averse (CRRA) utility, we find that a typical investor would equally invest in the two risky assets regardless of their correlation when they are indistinguishable from the riskiness perspective. Furthermore, the investor will take a long or short position on the relatively risky asset if it can work as the cross-hedging instrument due to their correlation; otherwise, it will not be traded at all. These results provide a unified explanation for the observed “under-diversification”, “home bias”, and “portfolioinertia” in financial markets from the cross-hedging point of view.","Model-based portfolio choice relies heavily on the estimation of model parameters from the historical data of risky assets such as expected returns, volatility, jump components, and correlations between risk factors. The fitted model is then used to characterize the prices and returns of the risky assets. However, investors may be confronted with ambiguity about the dynamics of prices and returns as well as estimation risk. Ambiguity-averse investors will correspondingly take robust strategies. We investigate the optimal portfolio choice of two risky assets when the expected returns and correlation of the two risky assets are uncertain. We focus on the uncertainty about expected returns and correlation because it is well-known that they are difficult to estimate. In addition, we explore the two-asset case to obtain the explicit solution for optimal investment strategies.====Portfolio choice among risky assets is not only a very realistic setting for fund management but also an important issue from a macroeconomic perspective. For example, most equity funds are required to be fully invested in risky assets (Michaud and Michaud, 2008). Many economists also argue that the global economy could be faced with a shortage of safe assets. For example, during the 2007–2009 financial crisis, many of the private safe assets, perceived as safe because they were bestowed with a AAA rating, lost their quality and then disappeared.==== As a result, the strains associated with the financial crisis quickly lead to concern about the safety of sovereign debts, which leads to a further shrinkage in the global supply of safe assets. It is a generalization of the setting with a risk-free asset in the sense that it can be reduced to the latter case by assigning zero-volatility to one of the risky assets. However, the absence of the risk-free asset may affect the performance of some portfolio rules designed for the investment setting with the risk-free asset. For example, the ==== rule can outperform various sophisticated optimal portfolio rules (DeMiguel et al., 2009). This shocking result can be turned over when a portfolio rule is designed for a set of risky assets (Kan et al., 2020), implying that an optimal portfolio over risky assets is not a trivial generalization of an optimal portfolio with a risk-free asset. From the theoretical point of view, ambiguity is omnipresent within the price dynamics of the risky assets since no one knows the exact evolution of the financial markets. Existing literature has investigated the effect of ambiguity on optimal asset allocation when one cannot accurately estimate one of the model parameters such as the expected return, volatility, or correlation (e.g. Chan, Karceski, Lakonishok, 1999, Epstein, Halevy, 2019, Epstein, Ji, 2013, Garlappi, Uppal, Wang, 2007, Jagannathan, Ma, 2003). Note that uncertainty on any one of the model parameters cannot fully capture the ambiguous dynamics of the risky assets. A natural question is what is the robust investment strategy if a fund manager has ambiguity on the driving force of randomness behind the risky assets. We propose a modeling framework to investigate this question, and provide more testable implications for robust investments.====Ambiguity on the price dynamics of risky assets is different from parameter uncertainty due to estimation error. Parameter uncertainty is referred to as the case when an investor knows the true model for the asset price while its parameters cannot be precisely estimated. This setting is formulated with stochastic models defined on a probability space equipped with a unique probability measure, meaning that the investor has complete confidence about the fundamental uncertainty behind the financial market. Aside from parameter uncertainty within such a probability framework, we consider ambiguity on the driving force of randomness behind the price dynamics of risky assets (see Epstein, Ji, 2021, Hansen, Sargent, 2015, Luo, Nie, Wang, 2021, for discussions on the interactions of model uncertainty and parameter uncertainty). In this paper, we characterize an investor’s ambiguity on the driving force of market randomness by a set ==== of probability measures defined on the canonical space ==== , the set of continuous functions representing the driving force of the market ambiguity. This set ==== of probability measures is selected such that the expected returns of risky assets and their correlations are in some intervals, respectively. In our framework, these quantities can be time-varying or random processes with bounded values. This construction method simultaneously accounts for the ambiguity induced by the expected returns and the correlation among risk factors, which is distinctive from the ambiguity induced by one of the risk factors or model parameters (e.g., Attaoui, Cao, Duan, Liu, 2021, Chen, Epstein, 2002, Epstein, Ji, 2013, Hansen, Sargent, 2008, Lin, Riedel, 2021, Liu, 2011, Luo, 2017).====Portfolio choice can be regarded as selecting some “good” assets among risky assets. The criterion for “good” assets can be up to the preference of investors. Let ==== (respectively, ====) be the expected return and volatility of the first (respectively, second) risky asset. We propose a criterion of the relative riskiness is====We show that an investor would first choose a relatively safe asset among the risky assets by using such criterion, and then consider if it is necessary to cross-hedge the ambiguity embedded in the relatively safe asset by trading the relatively risky asset. The threshold for cross-hedging is analytically derived for an investor with a CRRA utility, which leads to the conditions for trading. The cross-hedging criterion is====where ==== governs the degree of risk aversion of an investor in the CRRA utility. This criterion allows an investor to judge whether or not to invest in the relatively risky asset.====In our model, we suppose that the individual takes a stand only on bounds of returns and their correlation of the two risky assets. For any given ==== and ====, let ==== and ==== be the standard deviations (volatility) of two risky assets, respectively. The return process ==== of two risky assets takes value in a convex compact set ==== (==== and ==== are constants, and ====), and their correlation ==== takes value in ==== (==== and ==== are constants, and ====). Using the above criterion, we obtain the following results.====This paper contributes to the literature on robust portfolio choice in three folds. First, we provide a unified mechanism for explaining “under-diversification”, “home bias”, and “portfolio inertia” from the cross-hedging point of view. In the literature, “under-diversification” is referred to as a bias in individual assets or non-participation in risky assets, “home bias” is the term given to describe the fact that individuals and institutions in most countries hold only modest amounts of foreign equity, and “portfolio inertia” refers to the observation that the list of risky assets or their holdings in the optimal portfolio do not change when the Sharpe ratio of risky assets change. Only when a risky asset can be used to work as a cross-hedging instrument for the existing portfolio, will it be traded by an ambiguity-averse investor. This rationale highlights the effect of both the expected return and correlation between the risky assets in the robust portfolio choice problem. Second, a criterion for the relatively safe asset and a criterion for the cross-hedging demand are proposed to hierarchically construct a robust portfolio. The first stage is to find the relatively safe assets while the second stage is to cross-hedge the ambiguity with the other relatively risky assets. Third, we propose “====” as a rule-of-thumb for investment between two risky assets when they are indistinguishable from the riskiness perspective for an investor in the worst-case scenario, regardless of their correlations.==== This paper is related to the literature on robust portfolio choice in absence of a risk-free asset. Robust portfolio choice with parameter uncertainty has been extensively investigated in the existing literature. “Under-diversification”, “portfolio inertia”, and “home bias” are stylized empirical facts in portfolio research (see, e.g., Boyle, Garlappi, Uppal, Wang, 2012, Calvet, Campbell, Sodini, 2007, Cooper, Kaplanis, 1994, Guidolin, Liu, 2016, Mitton, Vorkink, 2007, Van Nieuwerburgh, Veldkamp, 2010). These facts can arise from ambiguity on one of the model parameters such as the expected return or correlation (see, e.g., Illeditsch, 2011, Illeditsch, Ganguli, Condie, 2021, Jiang, Liu, Tian, Zeng, Lin, Sun, Zhou, 2020, Pham, Wei, Zhou, 2021, Uppal, Wang, 2003). In a static framework for a correlation ambiguity, Jiang et al. (2020) show that an ambiguity-averse investor will exclude one of each pair of assets with a significant point estimation of correlation when they have similar risk-return characteristics. Pham et al. (2021) provide a justification for under-diversification due to ambiguity on expected returns and correlation in a mean-variance framework. Our continuous-time framework allows us to investigate a fundamental rationale behind these stylized facts from the hedging demand point of view and hierarchical method for portfolio selection.====Portfolio choice in the absence of a risk-free asset affects the performance of portfolio rule (see, e.g., Chiu, Zhou, 2011, Kan, Wang, Zhou, Lam, Xu, Yin, 2019, Zeng, Li, Li, Yao, 2016). Taking estimation risk into account, Kan et al. (2020) propose an optimal combining strategy for one-period portfolio choice, which could outperform the ==== rule. Our results coincide with this statement in the sense that it is optimal for an investor to invest equally in the risky assets for the specific market environment. In the continuous-time mean-variance framework, Chiu and Zhou (2011) and Zeng et al. (2016) highlight that the efficient frontiers in continuous-time portfolio without a risk-free asset are different from that in the one-period setting. Lam et al. (2019) show that the optimal allocation without a risk-free asset depends linearly on the current wealth while the case with a risk-free asset turns out to be independent of current wealth. The aforementioned papers on portfolio choice without a risk-free asset all take the mean-variance criterion as the objective of the portfolio optimization problem. In contrast, we work with a CRRA preference and investigate how ambiguity on the expected returns and correlation affects the robust investment strategies.====The remainder of this paper is organized as follows. Section 2 investigates the portfolio choice with two risky assets in the absence of ambiguity, focusing on the effect of the correlation between two risky assets. In Section 3, we introduce the model setup for ambiguous dynamics of the risky assets in terms of the expected returns and correlation. The robust strategy is analyzed in detail, which sheds light on the mechanism of some stylized facts from a new point of view. Section 4 provides quantitative analysis on the robust strategies and discusses the economic implications. Section 5 concludes. Proofs are given in the appendix.",Robust investment strategies with two risky assets,https://www.sciencedirect.com/science/article/pii/S0165188921002104,17 November 2021,2021,Research Article,117.0
"Davis J. Scott,Huang Kevin X.D.,Sapci Ayse","Research Department, Federal Reserve Bank of Dallas, 2200 N. Peal Street, Dallas, TX 75201 USA,Department of Economics, Vanderbilt University, VU Station B 351819, 2301 Vanderilt Place, Nashville, TN 37235 USA,Department of Economics and Finance, Jon M. Huntsman School of Business, Utah State University, 3565 Old Main Hill, Logan, UT 84322 USA","Received 18 October 2021, Revised 1 November 2021, Accepted 2 November 2021, Available online 11 November 2021, Version of Record 24 November 2021.",https://doi.org/10.1016/j.jedc.2021.104274,Cited by (5),"The collateral channel, whereby an increase in residential ==== leads to an increase in commercial property prices, loosening firm borrowing constraints and increasing firm investment, is weaker when residential and commercial real estate are imperfect substitutes. We enrich the ","This paper asks whether increases in residential property prices lead to an increase in business investment through the collateral channel. Liu et al. (2013) (LWZ hereafter) estimate a Bayesian dynamic stochastic general equilibrium (DSGE) model using U.S. aggregate data and argue that a housing demand shock is a major driver of fluctuations in output and investment. Land is used for both housing and an input into production. Land owned by entrepreneurs can serve as collateral, so a positive housing demand shock will push up the price of land, raising the value of collateral for entrepreneurs. Through this collateral channel, a housing demand shock allows entrepreneurs to increase their borrowing and investment.====In this paper, we argue that the strength of this collateral channel depends on the substitutability of residential and commercial real estate. For an increase in local residential real estate prices or a housing demand shock to affect the value of a firm’s collateral relies on the fact that residential and commercial real estate are substitutes and thus an increase in residential real estate prices is associated with an increase in commercial real estate prices.====First, we augment the Bayesian DSGE model in LWZ with a property development sector that reallocates land between residential and commercial use. When the two types of land are perfect substitutes, this sector is trivial, but when the two types of land are imperfect substitutes this sector sets the dynamic wedge between residential and commercial land prices. We then estimate a value for the elasticity of substitution between residential and commercial land in U.S. data. LWZ model the two types of land as perfect substitutes, we show that the model can better explain the same U.S. aggregate data when the elasticity of substitution between the two types of land is around 0.88 (with a 90% probability interval between 0.62 and 1.17).====Second, using this estimated Bayesian DSGE model we show how the strength of the collateral channel linking housing demand shocks to changes in aggregate investment is reduced when the two types of land are imperfect substitutes. When the two types of land are perfect substitutes, a housing demand shock explains between 30 and 40% of the variance of aggregate investment and 20 to 30% of the variance of total output. When the estimation allows the two types of land to be imperfect substitutes, a housing demand shock explains between 12 and 15% of the variance of aggregate investment and less than 10% of the variance of output. Furthermore, impulse responses comparing the perfect substitutes and imperfect substitutes versions of the model show that when the residential and commercial land are imperfect substitutes, the response of aggregate investment to a housing demand shock is significantly smaller.====Finally, to offer additional support for this finding, that the strength of the collateral channel linking movements in residential property prices to changes in investment depends on the substitutability of residential and commercial real estate, we present some corroborating evidence using firm-level data and a reduced form estimation approach. Chaney et al. (2012) empirically measure the collateral channel using U.S. firm-level data. They compare the effect of a change in local real estate prices on the investment rates of local property-owning and non-property owning firms, and they argue that a $1 increase in local real estate prices leads to a $0.06 increase in corporate investment for local property owning firms.==== We modify the original specification in Chaney et al. (2012) to interact local residential real estate prices with measures of the strength of zoning or land use restrictions. We take the strength of local zoning or land use restrictions to be a proxy for the ease at which a piece of land can be converted from residential to commercial use, and vice versa. With this extension we find that the same $1 increase in local residential real estate prices leads to a $0.04 increase in firm investment in states with the strongest zoning regulations (where legal restrictions make the two types of land poor substitutes), but an $0.08 increase in firm investment in states with the weakest zoning regulations (where the relative lack of legal restrictions makes the two types of land better substitutes).====While in many cases residential structures can’t be converted to commercial structures and vice versa, the land beneath that structure certainly can be converted from residential to commercial use if the law allows. Davis and Heathcote (2007) decompose the value of the U.S. residential housing stock into the value of structures and the value of underlying land. They show that at the business cycle frequency, the value of the underlying land is 3 times more volatile than the value of the structure, and thus the main driver of the value of real estate is not the non-substitutable structure on top, but the (potentially) substitutable land underneath.==== Davis (2009) performs a similar exercise looking at the value of underlying land by use, and argues that the price of residential land generally behaves very differently than the price of commercial land. Sirmans and Slade (2012) and Nichols et al. (2013) use transactions data to construct residential and commercial land price indices, and they show while the two exhibited many of the same properties during the run up of the housing bubble in the early 2000’s, during the peak bubble years the two land prices began to diverge and residential land prices climbed to a higher peak and had a greater fall.====The model in this paper is an extension of the collateral channel model in LWZ. Other extensions to LWZ include Liu et al. (2016) who incorporate a labor search and matching framework into the same model to explain the observed negative correlation between land prices and unemployment and the fact that housing demand shocks have a large effect on unemployment volatility. Bahaj et al. (2016) allow entrepreneurs to use the residential housing owned by the entrepreneur as collateral for business investment. In another extension of the LWZ framework, Gong et al. (2017) alter the household utility function to allow for a substitutability between consumption and leisure. This reduces the labor supply elasticity and the amplification effect of the credit constraint triggered by the housing demand shock on key macroeconomic variables is greatly reduced.====This paper will proceed as follows. The model is presented in Section 2. The details of the Bayesian estimation of this model are presented in Section 3. Variance decomposition and impulse response results are presented in Section 4. To offer support for the main findings of the model, we present the results from a reduced form estimation using firm-level data in Section 5. Finally, Section 6 concludes.",Land price dynamics and macroeconomic fluctuations with imperfect substitution in real estate markets,https://www.sciencedirect.com/science/article/pii/S0165188921002098,11 November 2021,2021,Research Article,119.0
"Chiu Jonathan,Wong Tsz-Nga","Bank of Canada, Canada,Federal Reserve Bank of Richmond, United States","Available online 12 June 2021, Version of Record 20 August 2022.",https://doi.org/10.1016/j.jedc.2021.104173,Cited by (3),"Digital platforms, such as Alibaba and Amazon, operate an online marketplace to facilitate transactions. This paper studies a platform’s business model choice between accepting cash and issuing tokens, as well as the implications for welfare, ==== is high, the platform scope is large, and the cyber risk is small. Unbacked floating tokens with zero transaction fees or interest-bearing stablecoins can implement the equilibrium business model, which is not necessarily socially optimal because the platform does not internalize its impacts on off-platform activities. The model explains why Amazon does not issue tokens, but Alipay issues tokens circulatable outside its Alibaba platforms. Regulations such as a minimum ==== can reduce welfare.","Recent years have witnessed the emergence of private digital tokens issued by social and economic platforms to facilitate transactions among users in a variety of contexts. Early digital tokens were issued by social media networks to allow platform users to conduct peer-to-peer transfers.==== In the past few years, many blockchain-based platforms have launched utility coins circulating in their distributed networks as the only acceptable means of payment.==== In China, Alibaba’s and WeChat’s digital wallets, which allow holders to accumulate redeemable, interest-bearing balances for online and point-of-sale transactions, have dominated the retail payments system. In the United States, retail giants Walmart and Amazon have also recently applied for patents related to cryptocurrencies.====Private tokens, especially those issued by major platforms with a large user base, could reshape the retail payment landscape and influence the functioning of the monetary system. The influence of private tokens has been an important theme of policy discussion.==== According to a central banks survey by Auer et al. (2020), the announcement of Facebook’s Libra in 2019 was a tipping point to urge the development of central bank digital currencies (CBDCs). The share of central banks that are likely to issue a retail CBDC in one to six years doubled in 2019 to 20%.====Within this context, our paper studies the business model choice between running a ==== and a ====, as well as its welfare and policy implications. What trade-offs does a platform face when choosing between accepting cash and issuing its own tokens? How does a token platform design features such as transaction fees, token interests and reserves held for redemption? How are the token prices determined? Is the platform’s choice socially optimal? What are the implications for payment resiliency and interoperability? What is the scope for regulation?====The key trade-off between the two business models is as follows. A cash platform makes profits by charging fees on platform transactions. However, the platform’s ability to extract consumer surplus is constrained by consumers’ cash holdings. In particular, the platform cannot charge profit-maximizing fees when consumers’ cash holdings are tight due to high inflation (or any other costs associated with using cash). By issuing tokens, a platform can insulate platform transactions from the costs associated with cash.==== Issuing tokens also provides the platform with an option to use token sales, i.e., the seigniorage, as an alternative source of revenue. Unlike cash, tokens can pay interest to the holders to boost their demand. However, tokens can lose transaction value when the platform ceases to operate (e.g., due to cyber attacks), and hence tokens, unlike cash, are subject to default risk. To support the transaction value of tokens, the platform may need to hold reserves in liquid low-yield assets, such as cash, for consumers’ redemption.====We find that the optimal platform design features zero reserves. It is because a token platform can support the transaction value of tokens by simply engineering a sufficiently high yield of tokens (e.g., fast-appreciating token prices). The platform also profits from selling tokens at fast-appreciating prices. The trade-off is that the platform has to limit the quantity of tokens sold or keep buying back tokens in order to sustain the appreciation. There are multiple ways to implement the optimal yield of tokens. For example, the platform can charge a zero transaction fee or pay interest on tokens.====Given the above trade-off, when would a platform choose to issue tokens instead of accepting cash? When inflation is not too high, using cash is optimal, especially when setting up a token system is costly. A platform chooses to issue tokens when inflation is sufficiently high (so that the cash holding is tight), when the platform is sufficiently resilient to cyber attacks (so that tokens are expected to circulate longer) and when its market share is sufficiently large (so that transaction fees are not a good way to collect revenue). Interestingly, the platform’s choice of business model is not necessarily socially optimal because it ignores the impact on consumers’ welfare. Depending on parameter values, both cash and token platforms can be suboptimally adopted. For example, the over-adoption of token platforms happens when the platform does not internalize the negative impacts of rejecting cash on non-platform consumption.====Could policy intervention improve social welfare? When the platform’s business model choice is suboptimal, a simple tax/subsidy scheme, if feasible, can induce the platform to make the optimal decision. However, conditional on the adoption of a token system, its design can still be suboptimal relative to the first-best allocation. Could imposing reserve requirements be welfare improving?==== We find that it is optimal for a token platform to minimize its reserve holdings, and hence a reserve regulation is typically welfare reducing.====Our paper contributes to the young but fast-growing literature exploring the economics of digital currencies. A large amount of research focuses on understanding the design and functioning of cryptocurrencies, such as Bitcoin. This literature can be broadly divided into three lines of research. The first line aims at understanding the pricing of cryptocurrencies, e.g., Biais et al. (2020), Choi and Rocheteau (2020), and Schilling and Uhlig (2019). The second line examines the functioning and robustness of the blockchain, e.g., Biais et al. (2019), Eyal and Sirer (2014), Saleh (2020), Chiu and Koeppl (2019), and Pagnotta (2020). The third line focuses on mining activities and the determination of fees, e.g., Huberman et al. (2017) and Easley et al. (2019).====There are important differences between general-purpose cryptocurrencies and platform-based tokens. Unlike general-purpose cryptocurrencies, platform-based tokens are mainly used for activities on the platform, and the platform controls whether or not to circulate the tokens outside the platform, subject to regulatory and technical restrictions. Also, platform-based tokens are often redeemable in national currency or platform goods.==== Therefore, the value of platform-based tokens depends on the functioning of the issuing platform as well as the issuer’s commitment to future actions, such as redemption, investment and reserve holdings. Finally, the token prices and fees are determined by a forward-looking profit-maximizing platform like a durable-goods monopoly. The monopoly power of the platform comes from its network effect.====A more closely related research area focuses on the design of utility tokens issued by platforms and the value of initial coin offerings as a mode of financing, e.g., Catalini and Gans (2018), Cong et al. (2020), Howell et al. (2020), Garratt and Van Oordt (2019), Sockin and Xiong (2020), and You and Rogoff (2019). Gans and Halaburda (2015) study the design of private digital currencies from a platform-management perspective and explain why a profit-maximizing platform may choose to limit the functionality of digital currencies. This stream of research typically focuses on partial-equilibrium analysis and thus does not study macro implications of platform-based tokens. Our paper complements this literature by highlighting the interaction between on-platform and off-platform activities in a general equilibrium setting.====Some recent studies develop general equilibrium models to investigate the macro implications of issuing central bank digital currencies, e.g., Andolfatto (2018), Barrdear and Kumhof (2016), Chiu and Wong (2014), Chiu et al. (2019), Davoodalhosseini (2018), Fernández-Villaverde et al. (2020), Keister and Sanches (2019) and Williamson (2019). Zhu and Hendry (2019) study the competition between national currency and general-purpose electronic money. To the best of our knowledge, this paper is the first one that studies the welfare and policy implications of issuing platform-based digital tokens in a general equilibrium setting.====Cyber risk and security issues have been investigated in the payment literature, for example, Kahn and Roberds (2008), and Kahn et al. (2020a) study the trade-off between security and convenience. We explore how platform security influences payment design in a general equilibrium environment.====Our model is based on Lagos and Wright (2005), which provides a tractable framework for us to study the joint determination of the transaction values of cash and tokens and the general equilibrium effects of a platform’s payment choice. See also the survey in Kahn and Roberds (2009). Recent models of payment systems building on the Lagos–Wright framework include Li (2011) and Monnet and Roberds (2008). Chiu and Wong (2015) adopt a mechanism design approach to compare the essential payment features of digital currency with cash.====The rest of the paper is organized as follows. Section 2 describes the basic environment. Sections 3 and 4 examine, respectively, a cash platform and a token platform. Section 5 studies the equilibrium business model and its welfare implications. Section 6 discusses the effects of token platform regulation. Section 7 considers an extension with endogenous interoperability and provides examples calibrated to Amazon and Alibaba. Section 8 concludes. All proofs are provided in the Appendix.","Payments on digital platforms: Resiliency, interoperability and welfare",https://www.sciencedirect.com/science/article/pii/S0165188921001081,12 June 2021,2021,Research Article,122.0
"Garratt Rodney J.,van Oordt Maarten R.C.","University of California at Santa Barbara, Santa Barbara, CA 93106, United States,Bank of Canada, Ottawa, ON K1A 0G9, Canada","Available online 12 June 2021, Version of Record 20 August 2022.",https://doi.org/10.1016/j.jedc.2021.104171,Cited by (4),"This paper examines whether initial coin offerings (ICOs) are a beneficial form of financing with desirable economic properties. We do so by examining how financing a start-up via an ICO changes the incentives of an entrepreneur to exert effort to reduce cost. An ICO can result in a better or worse alignment of the interests of the entrepreneur and the investors compared with debt and ====. Notably, an ICO can be the only form of financing that induces optimal effort, and there are projects that should not take place unless they can be financed through an ICO.","Two common methods of obtaining external funds for start-ups are debt contracts and selling an ownership share. It is well known that both methods may result in a principal-agent problem because the interests of an entrepreneur are not perfectly aligned with those of investors. Principal-agent problems may lead to suboptimal decisions by the entrepreneur after a start-up is initiated. They may also prevent valuable projects from being initiated in the first place if anticipated principal-agent problems are too severe and prevent the entrepreneur from being able to raise funds.====Initial coin offerings (or ICOs) are a relatively new mode of financing start-ups that involve the sale of coins (for example, digital tokens on a distributed ledger platform such as Ethereum) that enable the holder to access a service, obtain goods or share in the earnings of a project initiated by the entrepreneur.==== Despite confusion over what ICOs represent and how they would ultimately be viewed by regulators, ICOs surged in popularity throughout 2017 and into the second quarter of 2018 until increased regulatory pressure, instances of fraud==== and poor performance statistics led to a sharp decline in both the origination and completion of ICOs in the second half of 2018.==== While the initial hype surrounding ICOs seems to have faded, the question remains as to whether ICOs should be dismissed as a passing fad or whether they represent an innovative funding option that has the potential to solve an economic problem.====This paper focuses on ICOs where a start-up sells tokens that represent a new currency that is intended to serve as the only form of payment on a platform created by the start-up where users can trade goods and services or as the only form of payment for goods and services offered by the start-up. Examples of start-ups that raised funding in this way include proposed decentralized platforms for exchanging photographic products and services (KodakOne), music (Voise), and data storage (Filecoin).==== We examine how financing a start-up through an ICO changes the incentives of an entrepreneur relative to debt and venture capital financing. Our results show that, depending on the characteristics of the start-up, an ICO may result in a better or worse alignment of the incentives of the entrepreneur with the interests of the investors compared with conventional modes of financing. In some cases, outcomes may be strictly improved by using an ICO. These results are derived in a framework in which all funding markets are perfectly competitive and where there are no institutional or other differences between funding markets. In other words, benefits of ICO financing can arise without assuming a reduced regulatory burden, lower transaction costs, a lower discount rate or a better revelation of product quality through the “wisdom of the crowd.”====The framework we consider is one where an entrepreneur can undertake a project by making an initial investment and then must decide whether or not to exert unobservable effort to reduce costs. The project of the entrepreneur can involve either the development of a platform or the production of goods and services. The entrepreneur lacks the resources to finance the project, but can raise funds in the debt market, the venture capital market or the ICO market. When choosing to finance the project with an ICO, the entrepreneur raises funds by issuing coins in the form of digital tokens and selling some or all of these coins to external investors. The entrepreneur commits to accepting the issued coins as the only means of payment for goods and services offered by herself or others on the platform, and commits to not issuing any additional coins beyond the pre-committed amount.====In this environment, ICO financing may have an advantage over debt or equity financing. The intuition is as follows. Unobservable effort affects the costs of operating the platform, which generates a moral hazard problem. Efficiency requires that any costs savings associated with the entrepreneur’s effort accrue to the entrepreneur. This is never the case with venture capital, because outside shareholders benefit from cost reductions in proportion to their stake (e.g., [18], [18]). This is also not the case with debt financing if repayment of the debt is risky (e.g., [23], [23]). In contrast, with an ICO, the return investors receive on their token purchases is determined solely by the equilibrium exchange rate at which investors can sell their tokens to platform users. This amount depends only on the expected sales volume on the platform, and not on the cost of operating the platform unless unsuccessful effort results in a decision by the entrepreneur to abandon the platform or to default on their commitment to only allow payments using tokens.====The option to abandon the platform or to default on the commitment can distort the incentives of the entrepreneur to exert effort and can lead to inefficiencies because such a decision allows the entrepreneur to pass on some of the costs of operating the platform to the investors, therefore reducing the incentives to exert cost-saving effort. Interestingly, in cases where operating the platform is sufficiently profitable, the entrepreneur always keeps their commitment and the incentives to exert effort are the same as if financing was not required. In this situation, the ICO may dominate both debt and venture capital financing in terms of efficiency.====Our results push back against the notion that funds raised through an ICO represent “money for nothing.” The notion arises because the entrepreneur sells no ownership share of the company, nor promises any debt payment to investors, and only commits to accepting otherwise worthless tokens as a means of payment. The fallacy in the “money for nothing” notion is that the commitment to accept tokens as the sole means of payment is perceived as costless to both the owner and the users of the platform. This perception is incorrect because it ignores the fact that it forces the users and the owner of the platform to hold the tokens over the lifetime of the platform, which is costly in terms of the time value of money due to the opportunity cost of capital. This cost reduces the value of the platform. Our results show that the difference in the value of the platform with and without the commitment to accept only tokens reflects exactly the amount at which token investors may ultimately liquidate their holdings. In short, the ICO leads to a redistribution of cash flows from the project to investors and does not generate any additional value beyond that which may arise from a better alignment of incentives.====Our results also reveal an aspect of ICO financing that is not present in conventional modes of financing and may limit its effectiveness. The total amount rational investors will pay for tokens sold in the ICO is limited to only a fraction of the total expected sales revenue over the lifetime of the project. Under equilibrium pricing of tokens, the exchange rate at which investors can sell the tokens to customers is proportional to the sales revenue at a particular point in time. The only point in time when investors benefit is when they sell the tokens to customers. Once the tokens are sold, investors no longer benefit from their initial investment even though the platform continues to operate.==== By contrast, ownership of an equity stake provides investors with a share of the dividends over the entire lifetime of the project. The limit on the amount that can be raised with an ICO can undermine the moral hazard reducing benefits of ICO financing, since the amount can be less than the amount required for the initial investment even if the net present value of the project is positive. Therefore, it is possible that, despite producing better incentives to exert cost-saving effort, certain projects with a positive net present value cannot be financed with an ICO.====Our analysis provides conditions under which ICOs lead to payoffs that are equivalent to those that would arise if the entrepreneur were to enter into a sales revenue sharing contract with investors. The aforementioned limit on the amount raised through an ICO suggests that ICO financing may not be the best way to achieve sales revenue sharing. In particular, it is not necessarily better in terms of incentive structure than a simple security where the firm pledges a fraction of future sales revenues to investors. It is likely that early adopters of ICOs were in part attempting to leverage the hype surrounding crypto assets more broadly.==== Despite the limitations of ICOs, we show that there are beneficial aspects compared to debt and venture capital financing: An ICO can be the only form of financing that induces optimal effort, and there are projects that should not take place unless they can be financed through an ICO.",Entrepreneurial incentives and the role of initial coin offerings,https://www.sciencedirect.com/science/article/pii/S0165188921001068,12 June 2021,2021,Research Article,123.0
"Li Yiting,Wang Chien-Chiang","Department of Economics, National Taiwan University, No. 1, Sec. 4, Roosevelt Road, Taipei 10617, Taiwan","Available online 23 May 2021, Version of Record 20 August 2022.",https://doi.org/10.1016/j.jedc.2021.104157,Cited by (1)," of detecting fraudulent activity is low, employing fraud deterrents may not be ideal, and allowing for double spending may instead reflect an optimal equilibrium.","Cryptocurrency is a relatively new means of payment based on electronic systems that maintain a public transaction ledger in a distributed manner. Each individual is able to have his or her own copy of the public ledger, and there is no central authority placing control or restrictions on the right to manage, store, or distribute the ledger. People can freely create accounts and participate in the system, and payers update the public ledger by sending transaction messages to other participants through peer-to-peer networks. However, cryptocurrency operates in an imperfect network, such as the internet, which has uncertainty, missing information, and delays. Through this imperfect network, there is no guarantee that all participants receive the transaction messages or that participants receive the messages in the same order, and thus cryptocurrency relies on a consensus system to achieve agreement among participants regarding the order of sent messages.====This imperfectness also provides traders with opportunities for “double-spending fraud” : an attacker can make a payment to a merchant, receive the purchased good or service, and transfer the balance to another account owned by the attacker herself or another merchant. The double-spending message and the original message conflict with each other, and if the double-spending message, instead of the original one, is confirmed by the consensus system, the buyer will retract the payment, but the merchant will not receive payment for its merchandise.====We study double-spending fraud in a monetary search model with digital payment systems. In the model, buyers and sellers meet bilaterally in a decentralized market. Traders are anonymous, and there is no commitment, so unsecured credit is not feasible. There is no fiat money or physical assets that can be used as a means of payment. However, there is a digital payment system that allows traders to transfer balances by sending transaction messages through an imperfect consensus system, and if a message is confirmed by the consensus system, the message is observable by the public. The consensus system in the real world is complex and can be affected by various factors such as the imperfectness of the network and the design of the algorithm. For example, in Bitcoin, a blockchain and mining competitions are applied to achieve consensus among record makers (miners). To facilitate our analysis, we do not model details about the formation of consensus such as blockchain or mining competitions. Rather, we consider cryptocurrency as a payment system featuring double-spending as a race attack: the seller sees the original message, but the consensus system may consider the double-spending message as the true state.====To be specific, the consensus system in our model has the following features. If only the original transaction message is sent, there is no doubt that the message will be confirmed by the consensus system. However, if the buyer sends another transaction message, the double-spending message, one of the following three outcomes will occur. One, the double-spending message, rather than the original message, is confirmed by the system; we call this a “false agreement.” Two, only the original message is confirmed by the system; we call this a “correct agreement.” Three, both messages are confirmed by the system; we call this a “fork.” We capture the imperfectness of the consensus system by the probabilities that the above agreements occur under double-spending attacks. If the network is perfect, the receipt of the messages will be immediate and guaranteed, and a double-spending message will have no chance of being misidentified as the original one. The system thus will deliver a correct agreement regardless of whether the buyer double spends, and the message-sending system can support the social optimal allocation.====Double-spending may raise sellers’ concerns about not receiving payment and thus may decrease their willingness to trade. However, the properties of message sending prompt the system to impose multiple unconventional methods to mitigate the incentives of double-spending and to safeguard against fraudulent activities. First, we can impose a disutility cost associated with the sending of transaction messages; we term this ”the message cost.” Because the message cost is required to be imposed on all transaction messages, either the original or the double-spending messages, requiring a message cost generates an extra cost on double-spending and can be applied to mitigate the fraud incentives. Second, note that even if traders are anonymous, digital accounts and transactions can be traceable. Thus, although we cannot directly exclude the attacker from future trade, we can exclude accounts or coins involved in the attack as a punishment when deviations are detected.==== For example, we can require transactions to be attached with deposits in terms of currency balance. If a transaction is found to have been involved in a double-spending attack, the currency deposit will be taken from its original owner’s accounts and be excluded from future transactions. Furthermore, we can also take away the currency transfer from the receiver’s account as a punishment. In this case, the balance is equivalent to having been deleted and we term this ”currency deletion.” The above mechanisms diminish the expected gain of double-spending attacks, and if the gain of conducting double-spending is smaller than cost of doing it, the buyer will not engage in fraud.====We take a mechanism design approach to investigate the optimal design of the payment system. A cryptocurrency system in our model consists of two components: first, the consensus system, which is characterized by the probability that a correct agreement, a false agreement, and a fork occur when double-spending fraud is attempted; second, the design variables, which include the message cost, the amount of deposits, the inflation rate, the transaction subsidy, and punishments in terms of forfeiting deposits and deleting currency transfers. We consider that the consensus system is exogenously given and will not be further influenced by the design variables; and given the consensus system, we investigate the optimal design variables of the cryptocurrency system that generates the greatest social welfare.====Note that imposing a message cost generates a loss to social welfare because the sender of the original message is also required to incur the cost. Moreover, a currency deposit, although it does not directly generate a disutility, results in an additional balance to be held, and held deposits cannot be used for transactions, so this situation also generates a cost for trade. Given that imposing a message cost and currency deposit is costly, the question that results is this: Under what circumstances should we impose these costs to deter double spending, and under what circumstances should we allow double spending in equilibrium?====To facilitate the analysis, we construct two forms of mechanisms as candidates for optimal mechanisms. The first is called a “simple honest mechanism”, in which the size of the message cost and the deposits are set just high enough to deter fraud, and buyers do not double spend. The second is called a “simple double-spending mechanism”, in which no message cost or currency deposits is imposed, and traders find it optimal to double spend in equilibrium. We show that any equilibrium is dominated in terms of social welfare by an equilibrium generated by a simple honest mechanism or a simple double-spending mechanism, so the following questions remain: First, how do the features of a consensus system influence welfare generated by these simple mechanisms? Second, under what circumstances do simple honest mechanisms yield higher welfare over simple double-spending mechanisms, and vice versa?====In a simple honest mechanism, a correct agreement is the only agreement that occurs in equilibrium, and a false agreement and a fork would occur only if the buyer had deviated to double spend. Note that people who observe the consensus outcome cannot tell whether a message is an original message or a double-spending one, so a false agreement is not distinguishable from a correct agreement. However, people can distinguish a fork from a single agreement (i.e., a correct agreement or a false agreement) because a fork includes two conflicting transaction messages. As a consequence, forks can serve as signals for deviations and can be used to trigger off-equilibrium punishments. In a simple honest mechanism, when a fork occurs, the currency transferred in the message is excluded from future circulation, and attackers’ payoffs from forks are thus eliminated; moreover, the size of the message cost and the currency deposits can be set just high enough to offset their payoffs from false agreements. As the probability of false agreements becomes sufficiently small, the cost of preventing fraud will be substantially reduced, and honest mechanisms can then support the social optimal allocation.====Forks can also be applied to trigger the forfeiting of deposits in the mechanism. For a given probability that a false agreement occurs under double-spending, if the probability that forks occur is greater, the amount of the currency deposit required to deter fraud can be smaller. In contrast, the message cost materializes at the moment that transaction messages are sent but does not utilize forks as a trigger. Thus, if the probability that forks occur under double-spending is sufficiently high, imposing a currency deposit will be more efficient than imposing a message cost to prevent double-spending, and vice versa.====Different from simple honest mechanisms, simple double-spending mechanisms allow double-spending fraud, and forks can occur in equilibrium. Therefore, a greater probability that a fork occurs, if crowding out the occurrence of correct agreements, will decrease the seller’s expected payoff. This in turn decreases the trade volume and the efficiency of double-spending mechanisms. Thus, if the frequency that forks occur is sufficiently high, honest mechanisms can generate greater social welfare than that generated by double-spending mechanisms. In contrast, if forks rarely occur, the cost of preventing fraud will be high, and mechanisms that allow for double-spending but save the cost of preventing the fraud would generate greater social welfare than mechanisms that prevent double-spending fraud.",A search-theoretic model of double-spending fraud,https://www.sciencedirect.com/science/article/pii/S0165188921000920,23 May 2021,2021,Research Article,124.0
Williamson Stephen D.,"Department of Economics, Social Science Centre, University of Western Ontario, London, ON N6A 5C2, Canada","Available online 20 May 2021, Version of Record 20 August 2022.",https://doi.org/10.1016/j.jedc.2021.104146,Cited by (28),", is considered. Central bank digital currency tends to encourage banking panics, in part because panics are less disruptive with central bank digital currency than with physical currency. It may be optimal to live with banking panics, as eliminating them may be too costly.","Continuing advances in payments technologies have created a pressing need for central bankers to understand the implications for monetary policy, payments efficiency, and financial stability of the introduction of central bank digital currency (CBDC). Should CBDC work as widely-accessible central bank reserve accounts, be traded using blockchain technologies, as with cryptocurrencies, or should CBDC function in some other fashion? Should CBDC replace old-fashioned physical currency, or should it coexist with physical currency, perhaps alongside it in a modified form? Should CBDC be designed to avoid competition with private means of payment, or are there potential efficiency gains from moving electronic payments from private banks to central banks? These are only some of the interesting questions that need answers in this area of research.====This paper will be focused specifically on the implications of CBDC issue for financial stability. To understand what is at stake, a model of banking and banking panics is developed here, which has some new features. These new features are designed to highlight how central bank action to replace physical currency with CBDC would affect the incidence of banking panics and their effects on economic welfare. We also want to show how monetary policy, in the form of conventional interest rate policy and central bank crisis intervention, matters in the context of physical currency and CBDC regimes.====What has evolved as the typical – though not universal – structure for central banks includes three key features: (i) a monopoly on the issue of physical currency; (ii) a constraint that the central bank’s activities are restricted to swaps of its liabilities for other assets – principally the debt of the central government; (iii) powers that permit intervention in financial crises. Thus, a typical central bank is in the retail payments business, transforming assets not used as means of payment into assets that are, and it may intervene in ways intended to mitigate dysfunction in the payments system, or in the financial system as a whole. Many central banks, since at least 2008, have been expanding the scope of their mandates, but we will not deal directly with issues related to unconventional monetary policy here.====Secular changes in the relative usage of means of payment issued by the central bank – physical currency – and privately supplied electronic means of payment have been well documented. For example, in Canada and the United States, surveys indicate that the use of currency has been declining relative to alternative means of payment, at least in legal, reported transactions (see Henry et al., 2018 and Foster et al., 2020). However, in most countries of the world, the ratio of currency to GDP has been increasing, as has been the case in Canada and the United States. This increase in currency holdings may have been driven by an increase in the use of currency in the underground economy, by an increase in the size of the underground economy, or both.====The long-run changes in the demand for physical currency relative to other means of payment are important with regard to the potential introduction of CBDC. However, of greater importance for this paper is the short-run substitution from private means of payment to central bank means of payment that results from a change in perceptions about the stability of private financial institutions. For example, banking panics were an important feature of banking in the U.K. and in the United States in the late 19th century and the Great Depression. An idea enshrined in the Federal Reserve Act of 1913 in the United States is that the stock of physical currency supplied by the central bank should respond elastically to short run changes in demand for means of payment, which could be driven at times by banking panics. The authors of the Federal Reserve Act envisioned that an elastic currency could be supported by central bank lending during financial crises, in a manner invented by the Bank of England in the 19th century (see Bagehot, 1873). Alternatively, currency elasticity could be achieved through open market purchases in response to financial crises. But, whatever the mechanism, the idea is that a key role for a central bank in a financial crisis is to accommodate the flight from private bank liabilities to central bank liabilities by issuing more central bank liabilities.====So, in terms of the traditional functions of central banking, the fact that physical currency supplied by the central bank could be a safe harbor in a crisis need not be viewed as a bad thing. In line with this view, the supply of central bank currency should expand during a panic, mitigating disruption in retail payments. But, concern has been raised that central bank liabilities that are too attractive could engender financial instability. That is, in providing a safe harbor, the central bank could encourage financial instability by making it easy for the holders of private financial intermediary liabilities to flee at the first sign of trouble.====An example of concern over the flight to attractive central bank liabilities relates to the Fed’s reverse repurchase agreement (ON-RRP) facility, introduced in conjunction with the Fed’s “normalization” policy beginning in 2015. The intention of the ON-RRP facility was to expand the reach of Fed liabilities, permitting better control of overnight interest rates. But, concern was raised about flight-to-safety, for example in Frost et al. (2015). That is, an ON-RRP facility, paying a high interest rate on overnight balances at the Fed, could potentially provide a too-attractive safe harbor for financial market participants, relative to private sector alternatives, or so it was argued. A different view is in Greenwood et al. (2016), where it is argued that a large central bank balance sheet, coupled with an active ON-RRP facility, could promote efficiency.====So, views about flight to safety apparently depend on the nature of the central bank liabilities that market participants might flee to. Currency might be seen as a limited vehicle for flight, as it is not useful in large transactions and in wholesale transactions. Currency can be withdrawn and used in small transactions, but otherwise can only be hoarded as a store of value. Further, currency is costly to store and move around, which limits its usefulness as a safe asset in a financial crisis. However, overnight reverse repos held with the Fed are different. Such assets are a safe and liquid alternative to private assets such as money market mutual funds, and are one step away from reserves, which are used in interbank transactions.====Given this logic, it is not surprising, with rising interest among central bankers in CBDC, that there would be concern about how CBDC could lead to flight-to-safety and financial instability. At the extreme, Kumhof and Noone (2019) have argued that CBDC, if issued by central banks, should have features that make it inconvenient for its users. Private bank deposits and central bank reserves, as they argue, should not be freely convertible into CBDC. In this view, a concern about states of the world when CBDC could exacerbate banking panics leads to policies that throw sand in the gears of the financial system in all states of the world.====To explore these issues, we construct a model of payments supported by central bank liabilities and private bank deposits, in which banking panics can occur. Banks in the model perform two roles. First, banks issue liabilities that are acceptable as means of payment because banks have the incentive, at least in some states of the world, to ultimately redeem those liabilities, in spite of limited commitment. If a bank does not default on its liabilities this is because the bank has too much to lose from default. That is, the acceptability of its means of payment is supported by the bank’s franchise value, similar to ideas in Cavalcanti and Wallace (1999). Second, banks provide insurance against the event that a bank depositor needs central bank liabilities to execute a transaction, in a manner related to Diamond and Dybvig (1983), as in Williamson, 2012, Williamson, 2016, Williamson, 2018, Williamson, 2019, Williamson, 2020.====Banking panics, if they occur in the model, are driven in part by underlying bank insolvency. In particular, given how the acceptability of a bank’s means-of-payment liabilities is supported, if all agents anticipate that a bank will not redeem liabilities issued in the future, then this implies the bank will not redeem liabilities issued today, as its future franchise value is zero. We suppose there is an aggregate sunspot variable that is observed prior to when default occurs, but after the bank has accepted deposit inflows during the current period. On observing a bad sunspot outcome, depositors know that a fraction of banks will fail – essentially, those banks are insolvent. But individual depositors do not know the fate of their own bank. Each depositor then makes a calculation as to whether to withdraw central bank liabilities, given the probability that the depositor’s bank is insolvent, and the potential payoff from withdrawal. Depositors who need central bank liabilities to execute transactions will always withdraw, but those who can use private bank deposits in transactions may or may not withdraw. If all depositors request withdrawal, that is defined to be a banking panic, and there may be partial panics in which some depositors panic while others do not. A panic can disrupt transactions, depending on the nature of central bank liabilities. For example, physical currency may not be accepted in the transactions that a depositor wants to make. So, a banking panic can disrupt retail transactions, because central bank liabilities can only be held temporarily as a store of value, for some depositors, and potentially because central bank liabilities are spread across a larger group of depositors making withdrawals. A feature this banking panic model has – which we view to be desirable – is that panics do not depend on sequential service, as in the Diamond and Dybvig (1983) model. In this view, the role of sequential service in limiting trade and communication among withdrawing depositors is oversold.====The approach taken here is to consider two alternative regimes, one in which the central bank issues physical currency, and another in which the central bank substitutes CBDC for physical currency. In the model, the differences between CBDC and physical currency are that the central bank can pay interest on CBDC, and not on physical currency, and CBDC is designed to be accepted in a wider array of transactions than is physical currency. Paying interest on CBDC potentially matters for the effects of conventional monetary policy on the incidence of banking panics and on economic welfare. The fact that CBDC is potentially more widely-used in transactions cuts two ways, if we take seriously the concerns voiced about flight to safety. That is, we might view CBDC as contributing to the efficiency of the payments system, but perhaps not if this leads to greater financial instability.====In a regime with physical currency, banking panic equilibria can occur in which panics are triggered by aggregate bank insolvency events. These banking panic equilibria tend to exist when the nominal interest rate on government debt, supported by open market operations, is low. But conventional monetary policy can then eliminate banking panics, by maintaining a sufficiently high nominal interest rate. This need not be welfare maximizing, however. Banking panics are disruptive, in that retail transactions are foregone, and the full debt capacity of solvent banks is not used in support of transactions. But a higher nominal interest rate causes inefficiencies, reflected in a lower volume of transactions in all states of the world. It is shown that there are regions of the parameter space in which: (i) banking panic equilibria do not exist; (ii) banking panic equilibria exist at low nominal interest rates, but welfare is higher if monetary policy eliminates panics; (iii) banking panic equilibria exist at low nominal interest rates, and welfare would be lower if panics were eliminated.====We then examine what happens when there is deposit insurance. The deposit insurance system is set up to be actuarially fair, with viable banks paying deposit insurance premia that fund insurance payments to insolvent banks. The friction that mitigates the beneficial effects of deposit insurance in the model is not moral hazard, which is frequently cited as an issue with deposit insurance. Here, limited commitment associated with deposit insurance premia matters, in that, in an aggregate bank insolvency state, a solvent bank must pay off on its deposit liabilities and pay a deposit insurance premium. So, deposit insurance potentially tightens incentive constraints, given limited commitment. In a physical currency regime, deposit insurance tends to discourage banking panics, as it reduces the net payoff from panicking. But it is still possible for banking panics to occur, and even for optimal policy to support an equilibrium with partial banking panics. In general, deposit insurance tends to increase welfare.====A regime with deposit insurance and emergency open market operations is also considered. In this regime, banks acquire government debt as a precaution, as government debt can be sold to finance withdrawals during a panic. Emergency open market operations, while they mitigate the effects of panics, act to encourage banking panics, because they increase the net payoff for a panicking depositor.====In this model, it is assumed that CBDC can be used in transactions in which physical currency would otherwise be used, and in transactions in which bank deposits are used. If CBDC is too attractive, that is if the interest rate on CBDC is sufficiently high, then it will drive deposit-taking banks out of the means-of-payment market altogether. Here we assume that, perhaps because of unspecified costs of CBDC issue, CBDC is issued under conditions such that, in non-panic states of the world, bank deposits are used in some transactions.====With CBDC, in qualitative terms banking panic equilibria and equilibria with no panics have similar features to what happens in the physical currency regime. The key difference is that existence of these equilibria depends on the difference between the interest rate on government debt and the interest rate on CBDC, rather than on the difference between the interest rate on government debt and zero-nominal-interest currency. Banking panic equilibria exist when the interest rate differential is low, and equilibria without panics exist when the interest rate differential is high. CBDC tends to encourage banking panics, in the sense that, given the nominal interest rate differential, banking panics will in general occur when the probability of an individual bank failure is lower.====But, the reason that CBDC encourages panics is that CBDC is less disruptive of retail payments when a panic occurs – those economic agents typically using bank deposits in exchange will withdraw CBDC and use it in transactions in a banking panic. So there is a greater tendency with CBDC for banking panic equilibria to dominate equilibria without panics, when the central bank is conducting conventional policy optimally.====With deposit insurance in place, CBDC encourages panics relative to a regime with physical currency. As well, deposit insurance is welfare improving, and mitigates panics relative to a CBDC regime without deposit insurance. But, as in a regime with physical currency, deposit insurance does not in general eliminate banking panics. With deposit insurance and emergency open market operations, there are multiple equilibria – conventional monetary policy cannot eliminate banking panics, though such panics are actually irrelevant for economic welfare. That is, CBDC issue may encourage banking panics, but that goes hand-in-hand with panics being less disruptive than in a world with physical currency.====The nature of the banking panics in this model is somewhat related to what exists in models studied by Gertler and Kiyotaki (2015), Gertler et al. (2020), Gu et al. (2019), Andolfatto et al. (2019), and Robatto (2019). In contrast to Diamond and Dybvig (1983), as noted above, banking panics are unrelated to sequential service. Other work that explores the implications of CBDC issue includes Andolfatto (2020), Bech and Garratt (2017), Chiu et al. (2019), Davoodalhosseini (2018), Hendry and Zhu (2019), Keister and Sanches (2018), Brunnermeier and Niepelt (2019), and Williamson (2020). Fernandez-Villaverde et al. (2020) study banking panics in the context of CBDC issue, but their environment is more closely related to baseline Diamond and Dybvig (1983) constructs.====The remainder of the paper is organized as follows. In Section 2, the model is laid out. Then, in Sections 3 and 4, two regimes are considered, one where the central bank issues physical currency and no CBDC, and another where the central bank issues CBDC and no physical currency, respectively. The final section is a conclusion.",Central bank digital currency and flight to safety,https://www.sciencedirect.com/science/article/pii/S0165188921000816,20 May 2021,2021,Research Article,126.0
Davoodalhosseini Seyed Mohammadreza,"Bank of Canada, Canada","Available online 20 May 2021, Version of Record 20 August 2022.",https://doi.org/10.1016/j.jedc.2021.104150,Cited by (41),"Many central banks are contemplating whether to issue a central bank digital currency (CBDC). A CBDC has certain potential benefits, including the possibility that it can bear interest. However, using a CBDC is costly for agents. I study the optimal ==== when only cash, only a CBDC, or both cash and a CBDC are available to agents. If the cost of using a CBDC is not too high, more efficient allocations can be implemented by using a CBDC than using cash, and the first best can be achieved. Having both cash and a CBDC available may result in lower welfare than in the cases where only cash or only a CBDC is available. The welfare gains of introducing a CBDC are estimated under various scenarios for the United States and Canada. For example, if the cost of using a CBDC relative to cash is around 0.25% of the transaction value, introducing a CBDC can lead to an increase of 0.12–0.21% consumption for the United States and 0.04–0.07% for Canada.","There has been a great deal of discussion in recent years about the potential effects of introducing a central bank digital currency (CBDC) into economies and whether cash should be eliminated. Some central banks have started the decision-making process on whether to introduce a CBDC into their respective economies. For example, the Sweden’s Riksbank has already started testing a CBDC (what they call “e-krona”) to show how general public could use it.==== Some officials at the central bank of China have expressed their desire to issue their own digital currency as a way to support their digital economy.==== If central banks issue a CBDC, important questions arise, some of which are as follows: Should central banks eliminate cash from circulation? What would be the optimal (i.e., welfare-maximizing) monetary policy if agents can choose between cash and a CBDC? And quantitatively, what would be the welfare gains of introducing a CBDC into the economy?====To address these and similar questions, I use the framework of Lagos and Wright (2005) to build a model in which two means of payment could be available to agents: cash and a CBDC. What I mean by a CBDC in this paper is the money issued by the central bank in electronic format and universally accessible; i.e., all agents in the economy can use it to purchase goods and services.==== I study the optimal monetary policy when only one or both means of payment are available to agents. Cash and a CBDC are different along two dimensions in this paper. First, the ability of the central bank to implement monetary policy is different across these means of payment. The central bank can allocate transfers to agents based on their CBDC balances but cannot do so based on their cash balances because the central bank cannot see agents’ cash balances. Therefore, the only policy that the central bank can implement with cash is to distribute the newly created cash evenly across all agents or through an Open Market Operation (OMO, exchanging cash by CBDC).==== Second, carrying a CBDC is more costly for agents relative to cash. This cost summarizes in a reduced form the cost of adopting an electronic device, working with the CBDC application or managing the CBDC balances. This cost creates a sensible tradeoff for the central bank regarding the means of payment that the central bank would like agents to use. While a CBDC is a more flexible policy instrument, it is more costly for agents than cash.====There are two main results of the paper. First, given that the cost of carrying a CBDC is not too high, the fact that the CBDC is interest bearing in a non-linear fashion allows the central bank to achieve better allocations than with cash. In particular, it is possible to achieve the first-best level of production by using the CBDC if the agents are patient enough and if the bargaining power of buyers is sufficiently high, while it is never possible to achieve the first best by using cash. Second, when cash and a CBDC are both available to agents and valued in equilibrium, the monetary policy may be more constrained (i.e., welfare may be lower) compared with the case in which only one means of payment is available.====To elaborate on these results, consider three different schemes: only cash is available to the agents (cash-only scheme), only a CBDC is available to the agents (CBDC-only scheme), and both cash and a CBDC are available (co-existence scheme).==== If only cash is available, then the optimal inflation in this economy is zero. A negative inflation rate would be impossible to implement, as the central bank cannot force agents to pay taxes on their cash balances, and a positive inflation rate would lead agents to economize on their real balances relative to the first best, so the production level would be distorted. If only a CBDC is available, then the set of implementable allocations is larger, because the balance-contingent transfers are allowed with the CBDC, not with cash, and even the first-best level of production can be achieved. However, there is welfare loss resulting from the cost of carrying the CBDC. Comparing the cash-only and CBDC-only schemes, I find that the tradeoff for the central bank is simply between distorting the allocation relative to the first best under the cash-only scheme or having the agents incur the cost of carrying the CBDC under the CBDC-only scheme.====Under the co-existence scheme, agents with lower transaction needs endogenously choose to use cash, and agents with higher transaction needs choose to use the CBDC. In this case, the central bank faces a constraint stemming from the endogenous choice of means of payment. Because cash is available, agents whose welfare level is higher under the CBDC-only scheme relative to the cash-only scheme can now use cash as a way to evade the taxation that CBDC users are subjected to. To discourage these agents from using the CBDC, the central bank could set the cash inflation too high, but it would hurt cash users. Therefore, the availability of cash in the presence of a CBDC imposes a constraint for the central bank’s maximization problem. Whether or not the co-existence scheme is optimal (i.e., leading to higher welfare) relative to cash-only or CBDC-only schemes depends on how tight this constraint is. If the constraint is too tight, then the central bank would prefer to have only one means of payment used by agents. In this case, if the cost of carrying the CBDC is not too high, then the central bank eliminates cash, and if the cost is too high, then the central bank eliminates the CBDC. On the other hand, if the constraint is relatively relaxed, then the central bank would have both cash and CBDC circulate in the economy.====For both cash and a CBDC to be used by agents, the cash inflation must be strictly positive. This result is obtained despite the fact that, when both cash and a CBDC are available, it may seem feasible to implement a negative cash inflation rate through an OMO. However, a negative inflation rate on cash would induce users to switch from the CBDC to cash, as the return on holding cash would be higher than holding the CBDC and agents do not need to incur the cost of carrying the CBDC. Since the CBDC would not be used, the central bank could not conduct an OMO under a negative cash inflation rate.====To give a sense of the welfare gains of introducing a CBDC, I calibrate the model to the Canadian and United States data. I show that introducing a CBDC can lead to an increase of up to 0.15% in consumption for Canada and up to 0.34% for the United States, compared with their respective economies if only cash is used. Assuming that there are only two sizes of transactions (large-value and small-value transactions), I calculate the welfare gains of introducing a CBDC for different values of the cost of carrying the CBDC. As an example, if the cost of carrying a CBDC relative to cash is around 0.25% of the transaction value, introducing the CBDC can lead to an increase in consumption of 0.04–0.07% for Canada and 0.12–0.21% for the United States.====In an extension of the model, I assume that the agents’ privacy with respect to the size of their transaction should be protected. That is, the planner’s power toward CBDC is limited: the planner cannot see agents’ types, whether they want to consume a low or high level of consumption good, so the planner can see only the agents’ CBDC balances. Interestingly, welfare under the cash-only or co-existence scheme is identical with or without private information, but welfare under the CBDC-only scheme is lower with private information compared with complete information. Altogether, cash is more likely to be used in the optimal scheme, and the CBDC-only scheme is less likely to be optimal.====We see in practice that the governments raise large amounts of revenues from taxes, but central banks do not. One advantage of this model compared with many models in the literature is that the central bank is not granted an unrealistic power with respect to taxation. In the model, the central bank cannot tax agents, not only in the decentralized (anonymous) market, but in the centralized market as well. In other words, my model studies the optimal monetary policy in isolation from fiscal policy, when a central bank cannot count on support from the fiscal authorities, but is free to choose how to use seigniorage revenues. I believe that this is a reasonable description of the current institutional setup in advanced, low-inflation economies.====One may argue that implementing the type of CBDC addressed in this paper is difficult in practice because the interest payments suggested here are traditionally in the realm of fiscal policy, not monetary policy. This argument ignores two facts: First, the central banks in most advanced economies already make interest payments on reserves, but only to some financial institutions that have exclusive access to the central bank facilities. Second, those interest payments are non-linear in that the interest rate paid on reserves is different from the rate charged to borrowers. Central banks have recognized that the payments on reserves in the current system can serve their policy objectives, so why not extend access to all agents if economic efficiency requires that?====The rest of the paper is organized as follows. After briefly discussing the related literature in Section 2, I lay out the model in Section 3. I assume that cash and a CBDC are perfect substitutes; i.e., both can be used in all transactions. In Section 4 and as a benchmark, I assume cash and a CBDC are both costless to carry. In Section 5, I assume that a CBDC is more costly to carry relative to cash. I show, among other results, that if both cash and the CBDC are used by agents under the optimal policy, cash is used in small-value transactions and the CBDC is used in large-value transactions. In Section 6, I focus on a special case in which there are only two sizes of transactions—large-value and small-value transactions—and characterize conditions under which the co-existence scheme is not optimal. I also explore the case in which agents’ privacy should be protected. In Section 7, I calibrate the model to the Canadian and the United States data to estimate the welfare gains of introducing a CBDC into these economies. Section 8 concludes. I collect proofs, an extension and further discussions in the Appendix (which is online).",Central bank digital currency and monetary policy,https://www.sciencedirect.com/science/article/pii/S0165188921000853,20 May 2021,2021,Research Article,127.0
"Choi Michael,Rocheteau Guillaume","University of California, Irvine, USA","Available online 19 May 2021, Version of Record 20 August 2022.",https://doi.org/10.1016/j.jedc.2021.104152,Cited by (2),"We develop a random-matching model to study the price dynamics of divisible monies produced privately through mining. The equilibrium set is composed of a unique equilibrium where the value of money increases until it reaches a steady state and, if money has no intrinsic value, a continuum of perfect-foresight equilibria where the price of money inflates and bursts over time. Early on private money is held for a speculative motive and it acquires a transactional role when money supply becomes sufficiently abundant. We study different mining and matching technologies, fiat and commodity monies, single and competing currencies.","In Choi and Rocheteau (2020a) — CR thereafter — we developed a search-theoretic model of a pure currency economy where money is produced privately according to a time-consuming mining technology. Our model aimed at capturing the production of commodity monies, e.g., gold, or cryptocurrencies, e.g., Bitcoin, and derived implications for price dynamics. In the case where money has no intrinsic value, we showed the existence of a continuum of equilibria, indexed by the initial value of money, where the price of money and its supply increase early on despite money not being used as a means of payment. While the equilibrium with the highest initial value for money reaches a monetary steady state, all other equilibria feature an initial boom followed by a bursting phase where the value of money declines and vanishes asymptotically.====The environment in CR is based on Shi (1995) and Trejos and Wright (1995) where money is indivisible and individual money holdings are restricted to ====. While these assumptions provided tractability to study dynamic equilibria in a continuous-time setting where all trades take place in pairwise meetings, they prevented us from considering competing currencies. In this paper, we relax these assumptions and study a continuous-time economy with divisible monies that can be traded either in pairwise meetings or in competitive exchanges, as in Choi and Rocheteau (2020b).==== We characterize the set of equilibria under various assumptions for mining and matching technologies and properties of competing monies.====We start by considering an economy with a single money. The mining technology is such that the rate at which new units of money are produced decreases with the existing stock of money. For instance, the more gold has been dug, the harder it is to find new gold. The cost of mining is an opportunity cost associated with an occupation choice: agents have to choose between producing consumption goods and mining monies. If money is fiat (i.e., it has no intrinsic value), then the results in CR hold: there is a continuum of equilibria where the economy features a boom/bust cycle and the value of money vanishes asymptotically. Across equilibria the peak for the price of money increases with its initial value. It is only when the money supply reaches a threshold that money starts being used as a medium of exchange and production takes place.====Some comparative statics differ from the ones in CR. For instance, changes in the mining efficiency affect the endogenous supply of money but not the real allocations in the long run since fiat money is neutral. If money takes the form of a Lucas tree that pays a constant dividend flow, and if the potential money supply is large enough, then there exists a new type of equilibrium where the economy reaches the first-best allocation in the long run and the price of money is constant over time.====We study different matching and mining technologies in order to map characteristics of the environment into various price dynamics. First, we explore alternative matching functions between consumers and producers. Under our benchmark specification, the participation of producers does not generate congestion, i.e., the rate at which each active producer meets a consumer is constant. In contrast, if matching exhibits congestion — as more producers participate, the matching rate of each producer declines — then the opportunity cost of mining decreases with the measure of active producers. If the congestion effect is sufficiently strong, the price of money declines over time or is non-monotone.====We also check the robustness of our results to alternative mining technologies. In our benchmark model, the money supply depends on the endogenous time-path for the measure of miners. In contrast, for several crypto-currencies (e.g., Bitcoin), the money growth rate is determined ex ante, as part of the initial design of the currency, independently of the number of miners. Moreover, the mining of crypto-currencies does not require miners to forego their occupation — the cost of mining is no longer an opportunity cost associated with an occupation choice. We formalize such a case and show that, in accordance with our benchmark model, the market capitalization of the currency increases over time. However, its price decreases over time. This last result can be over-turned if the acceptability of the currency is endogenous, in which case the price of the currency can feature a boom-bust cycle.====The second part of the paper investigates economies with two monies. The first economy has two interest-bearing monies (or two commodity monies) that can be produced according to two distinct mining technologies. The two monies coexist in the long run provided that their potential supplies and the rate at which they are mined are not too far apart. The transition to the long-run equilibrium features an economy with a single money in an initial phase followed by a dual-money regime in a second phase where both monies are produced and exchanged. If the production rate of one money increases, then the value of both monies decreases. If the second money is introduced after the steady state with one money has been reached, then the value of the two monies falls initially and increases afterwards. Hence, the discovery or invention of new monies can generate price fluctuations.====The second economy has two fiat monies, one produced by the government and one produced privately. Each money is only accepted in a fraction of all meetings. We characterize equilibria where the monetary authority targets the aggregate value of its own currency. We show that the two monies can coexist provided that the rate of return of the government money is not too high and the fraction of matches where only the private money can serve as means of payment is not too low. Along the transitional path to a long-run equilibrium where both monies coexist, the value of the private money increases over time.====We extend the money mining model of CR to have divisible assets in a continuous-time version of the Lagos and Wright (2005) model. In an online appendix of CR we give a short preview of our model with divisible money, but we provide here a much more detailed presentation with various extensions and robustness checks, including different matching and mining technologies, time-varying and endogenous acceptability, and multiple currencies. The privately-produced asset is either a fiat money or a Lucas tree, as in Geromichalos et al. (2007) and Lagos (2010). The supply of assets is endogenous as in Lagos and Rocheteau (2008), Rocheteau and Rodriguez-Lopez (2014), and Geromichalos and Herrenbrueck (2018), among others. Relative to these papers, we emphasize the creation of assets through an occupation choice and explicitly formalize a time-consuming mining technology. Branch et al. (2016) also have private provision of liquidity and an occupation choice but the focus is different: the privately-produced asset takes the form of homes produced by Pissarides firms and the occupation choice is made by unemployed workers who can either be in the construction sector or the consumption-good sector. The study of dynamic equilibria in this class of models includes Lagos and Wright (2003) and Rocheteau and Wright (2013). The continuous-time assumption allows us to eliminate some exotic dynamics, such as cycles or chaotic dynamics, as shown by Oberfield and Trachter (2012) and Choi and Rocheteau (2020b). Berentsen (2006) is the first to study the private provision of fiat currency in the context of a search-theoretic model with divisible money.====Fernandez-Villaverde and Sanches (2019) study currency competition in the Lagos-Wright model with a fix measure of entrepreneurs who can issue money at some exogenous cost. They show that the existence of a stationary equilibrium depends on the shape of the cost function of issuing money. In our model mining is a time consuming activity and both the measure of miners and the cost of mining are endogenous. Moreover, they focus on stationary equilibria while we study price dynamics. Our extensions with two competing assets are related to Zhang (2014) and Gomis-Porqueras et al. (2017). Both papers focus on eliminating the indeterminacy of the nominal exchange rate in dual currency economies. Our approach to pin down the exchange rate between privately-produced and government monies is closer to Zhang (2014), which in turn follows Lester et al. (2012). Schilling and Uhlig (2019b) consider the coexistence and exchanges of government money and Bitcoin in a stochastic endowment economy and show the exchange rate between Bitcoin and fiat money is a martingale. Schilling and Uhlig (2019a) study a deterministic version of the same endowment economy and derive sufficient conditions such that currency exchanges arise or do not arise in equilibrium. Lotz and Vasselin (2019) develop a New Monetarist model to study the coexistence of fiat and E-money.",Money mining and price dynamics: The case of divisible currencies,https://www.sciencedirect.com/science/article/pii/S0165188921000877,19 May 2021,2021,Research Article,128.0
"Barrdear John,Kumhof Michael","Bank of England, UK","Available online 15 May 2021, Version of Record 20 August 2022.",https://doi.org/10.1016/j.jedc.2021.104148,Cited by (45),"We study the ==== consequences of issuing central bank digital currency (CBDC) - a universally-accessible and interest-bearing central bank liability that competes with bank deposits as medium of exchange. In a ==== tool, could substantially improve the central bank’s ability to stabilise the business cycle. Risks to banks can be minimized through appropriate issuance arrangements.","This paper studies the macroeconomic consequences of introducing a retail ==== (CBDC), in which the central bank grants universal, electronic, 24x7, national-currency-denominated and interest-bearing access to its balance sheet. CBDC, which could be issued through public spending, public lending or the purchase of eligible assets (excluding bank deposits), would coexist alongside bank deposits as an alternative medium of exchange.====Any study of the consequences of adopting a CBDC faces the problem that there is essentially no historical experience upon which to draw.==== There is therefore also very little empirical material that could help us to understand the costs and benefits of CBDC, or to evaluate the different ways in which monetary policy could be conducted under it. Our approach therefore instead relies on constructing and simulating a New Keynesian DSGE model, calibrated to match the US economy in the pre-2008 period, and extended to add features related to CBDC. The model, which was first presented in Barrdear and Kumhof, (2016), treats CBDC as an imperfect substitute for bank deposits, which are themselves created through loans or asset purchases as in Jakab, Kumhof, Jakab, Kumhof. The model is detailed rather than stylized, both in order to make the exercise credible for policymakers and in order to avoid prejudging what may be the most important economic mechanisms that determine the effectiveness of a CBDC.====As a baseline, we consider a setting in which the central bank maintains a stock of CBDC equal to 30% of GDP in steady state, backed by government debt, and potentially varied over the business cycle. Our choice of 30% is admittedly arbitrary. We have chosen it because this is an amount loosely similar to the magnitudes of QE conducted by various central banks since 2008, but we also comment on how different magnitudes would affect our results.==== We find that this policy may have a number of beneficial effects.====First, it increases steady-state GDP by around 3%, through three channels: (i) a reduction in real interest rates, due to a reduction in the quantity of defaultable debt and its replacement by non-defaultable low-interest CBDC;==== (ii) a reduction in distortionary taxes as a result of a lower cost of government financing; and (iii) a reduction in transaction costs due to increased liquidity throughout the economy. A decomposition of the contributions and interactions of these three channels is a key contribution of this paper.====Second, CBDC improves business cycle stabilization by granting policymakers access to a second policy instrument, the quantity of or the interest rate on CBDC. The efficacy of this instrument is higher when there is lower substitutability between CBDC and bank deposits in facilitating payments, and when the economy is faced by significant shocks to private money demand or money supply. We are also able to show that the choice between a CBDC quantity and interest rate rule is unlikely to have sizeable effects unless the substitutability between CBDC and bank deposits is extremely low. Studying these issues is another important contribution of this paper.====Third, financial stability considerations generally also favour the issuance of CBDC, provided that the issuance arrangements are well designed. Our paper discusses issuance arrangements when presenting the model’s specification of monetary policy. In our view, the only major concern is therefore the proper management of the operational difficulties and risks involved in transitioning to a different monetary and financial regime.====Current electronic payment systems are tiered, with central banks at their centre. Private agents gain access by holding claims on specific banks, with transaction settlement taking place in the balance sheets of banks that are higher up the hierarchy. In order to preserve trust in the system, banks are regulated and subject to capital, leverage and liquidity requirements. Although necessary, these regulations are costly for banks and therefore for their customers. They also grant market power to banks in the pricing of deposits, which serve as the economy’s primary transaction medium. A CBDC would strip banks of their exclusive ‘gate keeper’ role by implementing Tobin (1987)’s proposal for “deposited currency accounts” at the central bank, thereby offering an alternative means of access to the payment system.====The idea is motivated by the emergence of ==== digital currencies that offer both alternative units of account and new payment systems, with claims of superiority over standard banking.==== The first and most famous of these is Bitcoin (Nakamoto (2008)), which operates without a central bank, instead maintaining a ==== of transactions. Trust in the system is maintained by requiring that proposed changes to the ledger be accompanied by a costly, cryptographic proof of work.==== Ali, Barrdear, Clews, Southgate, 2014a, Ali, Barrdear, Clews, Southgate, 2014b) argued that it would be hypothetically possible to make use of distributed ledger technologies to offer a payment system that transfers claims against a national central bank. In such a situation, the overall system would remain centralized, but the operation of the ledger (transferring claims against the central bank) would be outsourced, operating in an analogous fashion to the nodes of a cryptocurrency. Such operators would not have any market power in the deposit market, as occurs at present with commercial banks.====In this paper, we abstract away from the technological particulars of how a CBDC payment system might operate.==== Instead we focus on the macroeconomic consequences of its introduction. We envisage an economy in which CBDC coexists with bank deposits, credit provision remains the purview of existing banks and, crucially, banks remain the creators of the marginal unit of money. We also abstract away from a lower bound on interest rates and from physical cash, the latter due to its small size, its endogenous supply, and its different use case from electronic means of payment. Nevertheless, we emphasize that any issuance of CBDC need not be predicated on the withdrawal of banknotes from circulation. It would be perfectly plausible for the two to operate in tandem alongside commercial bank deposits.====Bank for International Settlements (2020), in a recent joint report with eight of the world’s leading central banks,==== sets out the foundational principles and essential features of any CBDC. They emphasize that a potential CBDC must (i) “do no harm” to existing mandates for monetary and financial stability, (ii) coexist with existing forms of money (cash, reserves and bank deposits), and (iii) promote innovation and efficiency within the payment system. The paper then identifies several features of CBDC that would be necessary to satisfy these criteria. The variant of CBDC that we consider in this paper satisfies all of these criteria.====The rest of the paper is organized as follows. Section 2 discusses the literature on CBDC. Section 3 introduces the theoretical model, and Section 4 discusses its calibration. Section 5 presents simulation results, and discusses policy lessons. Section 6 concludes.",The macroeconomics of central bank digital currencies,https://www.sciencedirect.com/science/article/pii/S016518892100083X,15 May 2021,2021,Research Article,129.0
Mariscal Asier,"U. Rovira i Virgili, Department of Economics, and ECO-SOS. Campus Bellissens, Av. Universitat 1, Reus, 43204, Tarragona, Spain","Available online 31 May 2022, Version of Record 31 May 2022.",https://doi.org/10.1016/j.jedc.2022.104432,Cited by (0),None,None,Corrigendum to ‘Global ownership patterns’ [Journal of Economic Dynamics and Control (2021) 104213],https://www.sciencedirect.com/science/article/pii/S0165188922001373,31 May 2022,2022,Research Article,133.0
"Shin Yongseok,Vandenbroucke Guillaume","Washington University , St. Louis, United States,Federal Reserve Bank of St. Louis, United States","Available online 5 May 2022, Version of Record 21 June 2022.",https://doi.org/10.1016/j.jedc.2022.104429,Cited by (1),None,None,Covid-19 economics: Introduction,https://www.sciencedirect.com/science/article/pii/S016518892200135X,5 May 2022,2022,Research Article,134.0
"He Xue-Zhong (Tony),Wang Pengfei,Yang Liyan","International Business School Suzhou, Xi'an Jiaotong-Liverpool University (Suzhou),Peking University HSBC Business School and PHBS Sargent Institute of Quantitative Economics and Finance (Shenzhen),Rotman School of Management, University of Toronto (Toronto),Guanghua School of Management, Peking University, Peking 100871, China","Available online 29 April 2022, Version of Record 26 July 2022.",https://doi.org/10.1016/j.jedc.2022.104418,Cited by (1),None,None,Markets and Economies with Information Frictions,https://www.sciencedirect.com/science/article/pii/S0165188922001245,29 April 2022,2022,Research Article,135.0
Kahn Charles M.,"Department of Finance, Gies College of Business, University of Illinois, United States","Available online 11 June 2021, Version of Record 20 August 2022.",https://doi.org/10.1016/j.jedc.2021.104174,Cited by (0),None,None,"Discussion of “Payments on digital platforms: Resiliency, interoperability and welfare”",https://www.sciencedirect.com/science/article/pii/S0165188921001093,11 June 2021,2021,Research Article,141.0
Schilling Linda,"École Polytechnique, France","Available online 11 June 2021, Version of Record 20 August 2022.",https://doi.org/10.1016/j.jedc.2021.104172,Cited by (0),None,None,Discussion of “Entrepreneurial incentives and the role of initial coin offerings”,https://www.sciencedirect.com/science/article/pii/S016518892100107X,11 June 2021,2021,Research Article,142.0
Sultanum Bruno,"The Federal Reserve Bank of Richmond, USA","Available online 26 May 2021, Version of Record 20 August 2022.",https://doi.org/10.1016/j.jedc.2021.104156,Cited by (0),"The volatility of crypto currencies hinders their ability to be media of exchange or stores of value, leading to the implementation of exchange-rate pegs in an attempt to stabilize these currencies. This strategy has been used by crypto currencies such as US Dollar Tether, Steem Backed Dollar and TrueUSD; and was previously adopted in countries such as Brazil, Mexico and Argentina. However, an exchange-rate peg is vulnerable to speculative attacks if it is not 100% backed by reserves, as discussed in Obstfeld (1996). Using insights from the bank-run literature, Routledge and Zetlin-Jones (2018) build on Green and Lin (2003) and propose a model of speculative attacks. They show that adjustments to the exchange rate can prevent speculative attacks in equilibrium. They also show how to implement such contracts using ==== technology. In this discussion paper, I provide a cautionary tale. I show also in a version of ==== that the information content in the ==== prevents agents from attaining all the gains from risk sharing—highlighting the downsides of too much public information.","Crypto currencies tend to be extremely volatile, which hinders their ability to be media of exchange or stores of value. A common solution to reduce exchange-rate volatility is to peg the exchange rate. This solution has been previously used in countries such as Brazil, Mexico and Argentina, and is currently being used by managers of crypto currencies such as US Dollar Tether, Steem Backed Dollar and TrueUSD. However, as pointed out in Obstfeld (1996), an exchange-rate peg is vulnerable to speculative attacks if it is not 100% backed by reserves. These attacks are similar in spirit to bank runs caused by a fixed redemption rate, and Routledge and Zetlin-Jones (2018) build on Green and Lin (2003) to show how the same approach used to prevent bank runs can also be used to prevent speculative attacks on exchange-rate pegs.====In the (Green and Lin, 2003) setting, (Routledge and Zetlin-Jones, 2018) propose a sophisticated contract, which arguably is more adequate in the context of crypto currencies. The reason for this is twofold. First, it is a complex contract that would be hard to implement if not by a script. Second, and more importantly, it requires commitment. Ennis and Keister (2009a) have shown that common solutions to bank runs fail in a setting where the planner cannot commit to a payment scheme. This can happen even with a benevolent planner. One could imagine the same issue would arise here, but with crypto currencies, this is not a problem. Once the contract is coded into the blockchain, there is no going back. The decentralized nature of the ledger serves as a commitment device.====Encoding smart contracts in a decentralized ledger creates a new world of possibilities. As mentioned before, commitment is not a problem since individual parties cannot change the code, and the contracts can be made very complex. This allows governments, firms and individuals to issue and invest in extremely contingent liabilities. And the processes of issuing and investing both become simpler since they do not rely on a legal infrastructure. In principle, anyone can write (or buy) a smart contract in a platform such as Ethereum—making smart contracts accessible to the general public.====There are difficulties associated with smart contracts, too. Many of them are technical. For example, how to link the outside world with the blockchain? If you are financing a house or investing in US treasuries, how do you post it as collateral? If you have to add an intermediary, such as a bank or other legal entity, the process could undermine some of the advantages of smart contracts.====In this discussion paper, I put aside the technical difficulties related to smart contracts and focus on a problem with more economic content—the incentives of agents associated with the information structure. In particular, I focus on how the information content in smart contracts can limit agents’ ability to share risk. Smart contracts are implemented in decentralized ledgers and are public information. However, in order to implement constrained efficient allocations, it might be desirable to reduce the information agents have because their actions have to be incentive compatible for each possible information set available to them. As a result, by giving more information, the contract increases the number of incentive constraints that have to be satisfied, which potentially decreases the set of implementable allocations and welfare.====Building also on the bank model of Green and Lin (2003), I illustrate this point using a variation of their model with only a small change to preferences and the distribution of shocks. These changes can be understood as a generalization of Green and Lin (2003) and follow more closely (Andolfatto et al., 2017). I then compare two versions of the model–one in which depositors observe the redemption decisions of other depositors and one in which they do not observe.====The interpretation of these two different information structures is that if the contract is implemented by an intermediary, such as a bank, it has the option to reveal or not what the redemption rates are. However, if the contract is decentralized, using blockchain technology through a smart contract, the redemption rates are always observed since they are written in the decentralized ledger. I show how the constrained efficient allocation in the economy where depositors have information on redemption rates can always be implemented in the economy without information, but not the other way around. As a result, agents have higher expected utility when the contract reveals less information.====It is worth mentioning that (Cong and He, 2019) also point out that more information embedded in smart contracts can lead to lower welfare. However, the mechanism that they highlight is different. In Cong and He (2019), more information allows firms to punish deviations and make it easier for firms to collude, which can harm consumers. So the extra information improves firms’ profits but at the expense of other agents in the economy. Here, as we show, every agent can end up being ==== worse off. That is because the information limits the ability of agents to engage in risk-sharing contracts.====The bank-run literature has studied different information structures. The information structure in Green and Lin (2003) has agents knowing their queue position—but not the previous announcements—while (Andolfatto et al., 2007) study a setting where agents know the history of announcements, in addition to knowing their queue positions, and the information structure in Peck and Shell (2003) is one where agents do not have any information besides their own types. Follow-up papers have mostly considered one of these information structures. Ennis and Keister (2009b) share the same information structure as Green and Lin (2003) when studying the role of independent types, while Andolfatto and Nosal (2008) share the information structure as (Peck and Shell, 2003) when studying the incentives of self-interested banks, and (Cavalcanti et al., 2011) compare the welfare associated with these two information structures in large economies. Finally, Nosal and Wallace (2009) and, more recently, Huang (2021) study optimal information disclosure. While these papers focus on the existence of bank runs, my focus here is simply to point out how the information structure embedded in a blockchain can make the optimal risk-sharing arrangement impossible to implement.====Finally, I emphasize that it is understood in the literature that information disclosure can reduce the set of implementable allocations in general, and that this is true in some versions of the (Diamond and Dybvig, 1983) model—which is implied by some of the numerical examples in Nosal and Wallace (2009). The contribution I make in this discussion paper is not to point this out, but to relate this finding to the information structure embedded in blockchains.====The paper is organized as follows. Section 2 describes the environment; Section 3 discusses the two information structures, defines the incentive compatibility constraints and the optimal allocation problem for each one; Section 4 provides the main results; Section 5 concludes; and Appendix Appendix A provides the proofs.",Discussion of “Currency Stability Using Blockchain Technology”,https://www.sciencedirect.com/science/article/pii/S0165188921000919,26 May 2021,2021,Research Article,143.0
Chapman James,"Bank of Canada, Canada","Available online 25 May 2021, Version of Record 20 August 2022.",https://doi.org/10.1016/j.jedc.2021.104149,Cited by (0),None,"The way consumers have paid for goods and services has been undergoing fundamental changes in recent years; and especially in the last year due to the pandemic. Canadians using cash to pay at retail outlets has gone from 54% of transactions in 2009 to 33% of transactions in 2017 (Henry et al., 2018). This trend of the reduction in cash has appeared to continue during the midst of the COVID pandemic with 28% of Canadians reporting holding no cash, compared to 20% pre-pandemic (Chen et al., 2020).====At the same time, new alternative forms of money such as Bitcoin and Diem have been or are in the process of being created. They could become widely accepted in the the near future. The world of payments is changing and central banks need to keep abreast of this change. This explains the recent work by central banks on designing and understanding the implications of a central bank issued digital currency, or CBDC for short.====As CBDCs have become a subject of intense focus by central banks, central bank researchers have begun to model their implications for the economy. This is a brand new topic with many researchers contributing. A salient point is that-as the authors remark-there is no historical data to understand these implications. Therefore, it falls on economic theorists to build model economies to analyze the effects of a CBDC.====In this respect Barrdear and Kumhof (2020) is a seminal piece of research that has helped guide the thinking on this subject. This paper has been an important and early contributor in the discussion of CBDC as it relates to the macroeconomy; with the first version of this paper appearing in 2016 (Barrdear and Kumhof, 2016).====This paper has made an early and important contribution to the nascent literature on Central Bank Digital Currencies (CBDCs) in monetary economics. The authors conduct a fulsome and detailed analysis of the effects on the macroeconomy using a New Keynesian-type model.====A key question in the paper is how a CBDC would interact with (and be a competitor to) bank deposits. There the model in the paper builds off of Benes and Kumhof (2012) and Jakab and Kumhof (2015); which focus on banking regulation and banks’ role in the economy.====While it may seem natural to use this class of models, it has a clear downside in terms of interpretability of results due to the many moving parts; in the words of new monetarists, money is not “essential” in the model. I’ll clarify what I mean by these downsides below. But the advantage the authors get for moving away from a money where model is essential is that they can model a transition to a CBDC economy as well as conduct quantitative experiments.",Discussion of “The macroeconomics of central bank digital currencies”,https://www.sciencedirect.com/science/article/pii/S0165188921000841,25 May 2021,2021,Research Article,144.0
Koeppl Thorsten V.,"Queen’s University, Canada","Available online 23 May 2021, Version of Record 20 August 2022.",https://doi.org/10.1016/j.jedc.2021.104158,Cited by (0),None,None,Discussion of “A Search-Theoretic Model of Double-Spending Fraud”,https://www.sciencedirect.com/science/article/pii/S0165188921000932,23 May 2021,2021,Research Article,145.0
Carapella Francesca,Board of Governors of the Federal Reserve System,"Available online 21 May 2021, Version of Record 20 August 2022.",https://doi.org/10.1016/j.jedc.2021.104147,Cited by (0),None,None,Discussion of “Central bank digital currency and flight to safety”,https://www.sciencedirect.com/science/article/pii/S0165188921000828,21 May 2021,2021,Research Article,146.0
Rojas-Breu Mariana,"Université Paris-Dauphine, PSL, LEDa, CNRS & IRD, Paris 75016, France","Available online 19 May 2021, Version of Record 20 August 2022.",https://doi.org/10.1016/j.jedc.2021.104151,Cited by (1),None,None,Discussion of “Central bank digital currency and monetary policy”,https://www.sciencedirect.com/science/article/pii/S0165188921000865,19 May 2021,2021,Research Article,147.0
Araujo Luis,"Michigan State University and Sao Paulo School of Economics - FGV, United States","Available online 19 May 2021, Version of Record 20 August 2022.",https://doi.org/10.1016/j.jedc.2021.104153,Cited by (0),None,"Choi and Rocheteau (2020) use a random matching model to study price dynamics of monies privately produced with a time-consuming mining technology. The current paper extends their work in many dimensions, allowing for various matching and mining technologies, time-varying endogenous acceptability of money, and competing monies. The analysis is rendered quite tractable with the use of a continuous time model, as in Choi and Rocheteau (2021).====An important message of the paper is that the main results of Choi and Rocheteau (2020) are robust to various specifications. In particular, if money delivers a positive flow of dividend, there exists a unique equilibrium. In this equilibrium, if liquidity is relatively scarce, the price and the quantity of money strictly increase over time, and the economy converges to a steady-state in which the quantity of goods is below its efficient level. If, instead, liquidity is abundant, the price of money remains constant over time, while the quantity of money increases, and the economy converges to a steady state in which the quantity of goods is equal to its efficient level. Finally, if money does not deliver any dividend flow, in addition to the equilibrium leading to the steady-state, there exists a continuum of perfect-foresight equilibria exhibiting a boom and burst trajectory, in which the value of money increases and then decreases, converging asymptotically to zero.",Discussion of “Money mining and price dynamics: The case of divisible currencies”,https://www.sciencedirect.com/science/article/pii/S0165188921000889,19 May 2021,2021,Research Article,148.0
"Guo Bin,Huang Fuzhe,Li Kai","School of Finance, Nankai University, Tianjin 300350, China,Macquarie Business School, Macquarie University, NSW 2109, Australia","Available online 13 February 2021, Version of Record 1 March 2022.",https://doi.org/10.1016/j.jedc.2021.104080,Cited by (0),"This paper studies the impact of time to build on the term structure of interest rates in an otherwise standard (====; ====, CIR) production economy. Due to time to build, production depends not only on the current business condition as in the original CIR, but also on past conditions over the production period. This causes equilibrium quantities, including the short rate, forward rates, and bond returns, to depend on the historical path of the production opportunities. Production delay that accumulates uncertainty over the time to build generates significant time variations in bond risk premia. Bond returns can be predicted by current forward rates, as well as their lagged values, since current market states not only affect the current short rate but also the short rate in a distant future. Due to the path dependence, risk premia cannot be fully spanned by current yields. We show that time to build improves the ability of the CIR in generating empirical facts.","Built on a general production-based equilibrium framework of ====, the classical (====, CIR) model of the term structure of interest rates has become one of the foundational tools for studies and practices in the fixed-income area. The CIR and its extensions are found to successfully generate many important empirical facts observed in bond markets.====However, the CIR struggles with matching the recent empirical evidence that the cross-section of yields does not span all information relevant to the forecasting of bond risk premia (e.g, ====, ====, ====).==== Indeed, one assumption made by ====, is that the economy’s technology can transform new investment into the final good ==== (i.e., without time to build). This assumption, although it significantly simplifies the analysis, rules out the unspanned risk premia. In this case, all the variation in yield is due to variation in the future short rates. On the other hand, time to build has been extensively studied in macroeconomics and is found to significantly affect business conditions (e.g., ====); however, very few theories study its implications on asset pricing, especially on the term structure. In this paper, we fill this gap and examine to what extent time to build can help explain the return predictability in an otherwise standard CIR model.====Due to time to build, the output of an investment is not realized until several periods later. This implies that production depends not only on the current states as in the original CIR, but also on past business conditions over the period of production, making it more difficult for the agent in the economy to use investment to smooth consumption. When making investment and consumption decisions, the agent needs to account for all historical market states over the time to build; thus, her optimal consumption depends on the entire historical paths of state variables. This directly leads to a path-dependent stochastic discount factor (SDF).====Our model augmented with time to build helps explain a number of empirical regularities observed in bond markets. Ample evidence challenges the expectations hypothesis that postulates a constant risk premium and implies that bond risk premia are unpredictable (e.g., ====, ====, ====, ====). Our model helps resolve the expectations puzzle that cannot be explained by the original CIR model. Indeed, time to build causes the short rate to mean-revert to a weighted average of both current and past production growth rates. As a result, production delay increases risks in the economy by accumulating uncertainty over the time to build. This leads to significant time variations in risk premia.====The path-dependent SDF causes equilibrium quantities, including the short rate, forward rates, and risk premia, to depend on historical paths of the state variables characterizing production. In this case, both current forward rates (or yields) and their historical paths contain information of future bond returns since current market states not only affect current short rate but also the short rates in a distant future. As a result, bond risk premia can be predicted by both current forward rates (or yields) and their lagged values. This is consistent with the evidence documented by ==== and ==== that lagged forward rates help predict bond excess returns, even in the presence of current forward rates. More importantly, different from the conventional wisdom, rational expectations do not eliminate return predictability by past information in our model. This is ==== due to the non-Markovian feature. Our paper in turn provides insight into this fundamental notion and highlights the role of non-Markovian asset pricing.====Due to the path dependence, to define bond prices in a continuous-time framework, we need to specify a continuously infinite number of historical values of the state variables (over the time to build). Thus, bond prices are infinitely dimensional. This directly implies that excess bond returns cannot be fully captured by linear combinations of yields. This is consistent with the considerable evidence that a part of excess bond returns cannot be explained by the principal components of yields (e.g., ====, ====, ====, ====, ====).====Our model also shows that macro factors help predict future bond returns. Due to the path dependence, more state variables, in addition to the short rate, are needed to capture the dynamics of equilibrium. As a result, bond prices depend not only on the short rate but also on the state variables reflecting current and historical business conditions. This is consistent with the findings of ==== that macro factors have important forecasting power for the risk premia of U.S. government bonds, above and beyond the predictive power of forward rates and yield spreads. In contrast, in the original CIR model, the short rate captures all information of production (macro factors) and hence in itself constitutes a sufficient statistic of the production equilibrium. In this case, future bond risk premia are completely determined by the current short rate, implying that macro factors do not help predict bond returns in the presence of the short rate. More broadly, the evident predictability by macro factors also challenges standard affine models in which the predictability of bond returns is completely summarized by the cross-section of yields or forward rates. Our model that incorporates time to build into the single-factor affine model of the CIR allows predictability by macro factors.====Time to build causes production to depend on historical business conditions and leads to intractability in general. Indeed, time to build is studied by typically using discrete-time models (e.g., ====). In this setting, different lengths of delay lead to distinctly different systems with different dimensions: A larger delay involves more state variables and hence increases the dimensionality of the systems. It is tremendously difficult to numerically solve a high dimensional system. Further, because these systems are mathematically characterized by different sets of state variables, they have to be analyzed separately even though their economic difference lies only in the lengths of time to build. In contrast, we study a continuous-time model in which the length of time delay is simply measured by a parameter and because of this, we provide a unified study of time to build.==== Our setup allows tractability.====Most equilibrium pricing models are Markovian, in which equilibrium dynamics are completely characterized by the state variables underlying the original setting. This paper adds to the asset pricing literature by studying ====. In our model, time to build causes the dynamics of equilibrium to depend on its entire path over production delay, which has infinite dimensions. Therefore, equilibrium cannot be simply spanned by the original state variables as in a Markovian setting. To the best of our knowledge, there is no known examples of exact characterizations of non-Markovian equilibrium. In this paper, we explicitly solve for the equilibrium with production delay by applying the piecewise dynamic programming approach developed recently by ====.==== In the spirit of ====, we further devise a novel method to handle the problem that there is no Itô’s lemma for stochastic delay differential equations that characterize our economy. Our method allows for closed-form solutions that provide clear characterizations of the non-Markovian dynamics and effectively isolate time to build.====Our paper contributes to the large literature of term structure models of interest rates. Statistical models of the SDF are found to successfully match many empirical facts, e.g., ==== and ====. However, these reduced-form models do not speak to the underlying economic mechanisms. The aim of this paper is not to develop a model to better fit the data (as those statistical models) but to provide explanations for the economic mechanism in generating the stylized facts in the bond market. We show that time to build helps explain simultaneously a number of empirical facts in a parsimonious production economy.====Some stylized facts in the bond market, e.g., the expectations puzzle, have long posed a challenge for general equilibrium models of the term structure. ==== show that a model with a representative agent and time-varying expected consumption growth cannot account for the expectations puzzle in a pure exchange economy. Indeed, only a few papers provide economic mechanisms in explaining the puzzle. For example, ==== finds that habit formation helps explain the expectations puzzle. In her model, during recessions, the agent is more risk averse, leading to a high intertemporal substitution (high short rate). As a result, the short rate relates negatively to the surplus consumption ratio, generating positive bond risk premia. Different from ====, the agent in our model has a constant relative risk aversion. Production delay increases risks in the economy by accumulating uncertainty over the time to build, generating significant time variations in bond risk premia.====The literature also shows that learning (either Bayesian or non-Bayesian) can generate time-varying risk premia and return predictability, e.g., ====, ====, ====, ====, and ====, amongst many others. There are two important differences between the dynamic equilibrium models with learning and our model. First, different from the learning models that focus on unobservable economic forces, in this paper, we study time to build that has been widely documented to radically affect business conditions but of which the effect on the term structure has not been studied. The agent in our economy observes the production process and is informed about the time to build; thus, there is no learning in our model. Second, and more importantly, time to build naturally leads to non-Markovian dynamics and we show that the non-Markovian feature plays the key role in generating the stylized facts in our model. In contrast, most learning papers study Markovian models, in which past information plays no role in price dynamics given all state variables.====Our paper also adds to the literature of production-based asset pricing models. Subsequent to ====, production economy has been used to study both the aggregate market and the cross section of asset returns, e.g., ====, ====, ====, and ====.==== These models study the effects of investment frictions (e.g., capital adjustment costs, investment irreversibility, and capital immobility) and stochastic productivity shocks on equity returns. Only a few papers study asset pricing implications of time to build, e.g., ==== and ====. ==== is closely related to this paper. ==== considers both delays in transforming new investment into productive capital and making final outputs from productive capital. He shows that time to build helps generate a sizable equity premium, fit investment regressions, and explain the lead-lag patterns between asset prices and macroeconomic quantities. But ==== doesn’t explore the bond yields. This paper fills this gap in the literature.====The remainder of the paper is organized as follows. ==== presents some motivating regression evidence that historical information helps predict bond risk premia. ==== develops a CIR model with time to build and ==== solves for equilibrium. ==== examines model implications. ==== provides further empirical evidence of the failure of Markovian models and ==== concludes. Appendix provides the proofs.",Time to build and bond risk premia,https://www.sciencedirect.com/science/article/pii/S0165188921000154,13 February 2021,2021,Research Article,149.0
"Qi Shi,Schlagenhauf Don","Economics Department, William and Mary, P.O. Box 8795, Williamsburg, VA 23187, USA,Emeritus of Florida State University, Tallahassee, FL 32306, USA","Received 31 March 2021, Revised 21 October 2021, Accepted 22 October 2021, Available online 28 October 2021, Version of Record 10 November 2021.",https://doi.org/10.1016/j.jedc.2021.104271,Cited by (1),Using a dynamic stochastic ,"Conservative politicians have advocated business tax cuts as a way to foster economic growth. Using this rationale, the State of Kansas passed a sweeping business tax-cut program in 2012. With the intent of increasing investment and raising employment, the tax-cut eliminated the state-level tax on pass-through business income. However, the Kansas economy did not respond as expected as the State of Kansas had experienced slower growth compared to its neighboring states. In addition, during the years of the tax cut, the state government faced a severe fiscal crisis resulting from a significant tax revenue shortfall. As a result, the Kansas tax cut program is widely reported as a dismal failure and is a cautionary tale for all future tax cuts. This paper investigates “The Kansas Tax Experiment” by focusing on a firm’s choice of legal forms of organization (LFO). We argue that the Kansas tax cut program’s failure is mainly due to its design, which targeted a particular legal form of organization.====To evaluate the impacts of business income tax policies, this paper develops a dynamic stochastic occupational choice model with heterogeneous agents, similar to that of Chen et al. (2018). Model dynamics are similar to that of Hopenhayn and Rogerson (1993). Occupational choices are modeled akin to Lucas (1978), and agents in the economy can choose between being non-employed, a worker, or an entrepreneur. Most importantly, the model incorporates the entrepreneurial choice of LFOs.====In terms of LFOs, a firm in the U.S. must choose to be either a pass-through business or a C corporation. Pass-through legal forms include sole proprietorship, partnership, limited liability firms, and S corporations. In 2011, for example, 75 percent of firms were of the pass-through legal form, and they hired nearly half of all workers. Although many legal differences exist between the pass-through and the C corporation legal forms, our model focuses on the essential trade-off a firm must face when choosing an LFO. A major advantage of filing as a pass-through business is that a firm avoids double taxation. A C corporation must pay corporate income tax on its corporate profits. If any remaining profits are distributed to shareholders, and are subject to personal income tax. On the other hand, if a firm files as a pass-through entity, all profits are passed through to the business owners, subject only to personal income tax. The LFO choice also has implications for access to capital. Pass-through entities are subject to legal restrictions on access to capital. Legally, a pass-through entity can have no more than 100 shareholders and no foreign, institutional, or corporate shareholders. This type of firm cannot issue preferred stock, limiting its ability to attract “deep-pocket” investors such as venture capitalists. Given these restrictions, pass-through businesses are more likely to be capital constrained compared with C corporations.====Because firms of different legal forms face different tax obligations, a change in the relative business income tax rates can affect business LFO choices. A decline in the pass-through business income tax may prompt business owners to switch from the C corporate to pass-through legal form. The Kansas tax cut, targeting only the pass-through businesses, distorts a firm’s LFO choices and affects capital allocative efficiency among firms in the economy.====Ideally, marginal products of capital should be equalized across all firms in the economy. In our model, entrepreneurs finance their operations by using personal wealth or accessing the external capital market. More-productive entrepreneurs with small individual personal wealth are more likely to be capital constrained without external financing. If these firms were to choose the pass-through legal form to avoid double taxation, their access to the external capital market broadly-defined would be restricted. Therefore, inefficiency can result from the distortion in LFO choices, which leaves some high marginal-return-to-capital firms without sufficient access to capital. The resulting capital misallocation would consequently impact overall employment and output in the economy.====The model is calibrated to match key statistics from the data. Our data was obtained directly from the Kansas Department of Revenue, which started tracking firms’ legal form of organization in 2012. The data are organized by LFOs, industry sectors, and levels of business income earned in Kansas. The data reports the total number of firms, the number of entry firms, and the number of firms that switch their LFOs. The empirical analysis shows that exempting personal income tax on business income encourages both existing businesses to change their LFOs and new firms to adopt the pass-through legal form.====Using the calibrated model, this paper quantitatively evaluates the impacts of two contrasting tax-cut policies for firm LFO choices, as well as the aggregate implications for output, consumption, capital utilization, and employment. When exempting the state-level personal income tax on pass-through business income, our model can largely replicate the significant tax revenue shortfall, as well as the sluggish economic growth in output and employment following the Kansas tax experiment. The key to understanding this outcome is that the capital constraint on businesses becomes more severe on average, resulting in a decline in capital demand. In comparison, if the corporate income tax rate is lowered, the C corporation sector would expand significantly, leading to an increase in output, consumption, and capital formation, similar to the findings in Fehr et al. (2013). A reduction in the corporate income tax rate can also lead to moderate job growth.====This paper follows the literature that studies tax policy changes impact on the business organizational decision. This literature has largely focused on the distortions caused by the double taxation of corporate income. Gravelle and Kotlikoff (1989) posited that the efficiency loss due to the corporation income tax depends on the responsiveness of firms’ incorporation decisions to tax changes. Following Gravelle and Kotlikoff (1989), papers such as Gordon and MacKie-Mason (1994), Mackie-Mason and Gordon (1997), Gordon and Slemrod (2000), and Kotlikoff and Miao (2010) explicitly modeled firm incorporation decisions to measure the size of this tax distortion. Empirically, Gordon and MacKie-Mason (1994), Goolsbee (1998), and Mackie-Mason and Gordon (1997) have generally found no major shifts between organizational forms in response to tax changes at the federal level. One reason is that historical variation in federal corporate tax rates had been almost negligible. However, using cross-sectional variation in state corporate tax rates, Goolsbee (2004) found a significant organizational response to tax policy changes. Similar organizational responses can also be found outside the United States. Examining business income declarations in the United Kingdom, Devereux et al. (2014) found that the elasticity of the share of income recorded as corporate profits is small and positive with respect to the difference between personal and corporate income tax rates. Contributing to the literature, this paper focuses on the taxation of pass-through business income in addition to the corporate income tax. In doing so, this paper considers an additional dimension of how tax policy can affect economic efficiency. Importantly, encouraging firms to switch out of the C corporate sector can be detrimental, as they are likely to face more stringent capital constraints.====This paper also contributes to the productivity misallocation literature. The pioneering works of Restuccia and Rogerson (2008) and Hsieh and Klenow (2009) are among the first to measure aggregate productivity losses due to resource misallocation (i.e., the dispersion in marginal products). Complementary to measuring misallocation, this article directly identifies a cause of misallocation – tax distortions on business LFO choices. Productivity loss occurs when a tax policy incentivizes firms to adopt a business legal form with significant constraints on access-to-capital. In this respect, our paper shares similarities to many other papers documenting the implications of credit or capital market imperfection on productivity, such as Amaral and Quintin (2010), Caselli and Gennaioli (2013), and Midrigan and Xu (2014).==== In addition, this paper follows previous papers and uses the Lucas (1978) style occupational choice model to study productivity misallocation. For example, Buera et al. (2011) adopt an occupational choice model with financial constraints to evaluate the distortion of financial frictions on the allocation of capital and entrepreneurial talent across production units. Erosa (2001) shows that the financial costs of raising capital could affect the entrepreneurial occupational choices and cause misallocation of financial assets. Guner et al. (2008) use a version of the Lucas (1978) model to study size-dependent policies and misallocation. This paper builds upon the previous literature by focusing on a firm’s choice of legal forms of organization.====The remainder of the paper is organized as follows. Section 2 presents the model in detail. Section 3 provides background information for “The Kansas Tax Experiment” and lends empirical support to the modeling of capital constraints. Section 4 details the calibration procedure and presents the calibration results. Section 5 conducts counterfactual policy experiments. Section 6 concludes.",The Kansas tax experiment: The matter of legal form of organization,https://www.sciencedirect.com/science/article/pii/S0165188921002062,28 October 2021,2021,Research Article,150.0
Sohail Faisal,"Department of Economics, University of Melbourne, Australia","Received 8 October 2021, Accepted 20 October 2021, Available online 28 October 2021, Version of Record 14 November 2021.",https://doi.org/10.1016/j.jedc.2021.104270,Cited by (3),"Most new firms are founded by former employees of existing firms – ==== firms and for workers in response to policies that target ==== firms. Taken together, this paper establishes a connection between incumbent and entrant firms and shows that it is important for aggregate outcomes.","New firms contribute significantly to the economy. For instance, recent studies emphasize their impact on employment dynamics and productivity growth in both developed and developing economies.==== However, not all new firms contribute equally, with only a few entrants growing quickly while most others remain small or exit (see, for example, Sterk et al. 2021). Hence, understanding what determines the quantity and quality of new firms is important for understanding macroeconomic outcomes. In this paper, I explore how existing firms shape the entry and post-entry dynamics of new firms and study the aggregate implications of this relationship. To do this, I focus on an important subset of new firms – ====.====Spinouts are business ventures founded by former employees of existing firms and they account for the majority, around 60 to 80%, of all new entrepreneurs in both Mexico and the United States (see Fig. 1). Interestingly, the entry and post-entry dynamics of spinouts are systematically related to the characteristics of a spinout founder’s previous employer. In particular, while employees from small firms are more likely to form new firms, the spinouts from larger firms are initially larger and also grow faster. In other words, employer size – a proxy for productivity – is negatively correlated with the probability of spinout entry and positively correlated with spinout performance.====Although these empirical patterns are well-explored, the theoretical literature on firm dynamics and occupational choice largely abstracts from the relationship between existing firms and spinouts. Indeed, workhorse models of firm dynamics assume that characteristics of entrants are unrelated to characteristics of incumbent firms (see, for example, Jovanovic 1982, and Hopenhayn 1992). As such, the aggregate implications arising from the relationship between employer size and spinout dynamics are not well-studied. A key contribution of this paper is to explore these implications in a model that reconciles the empirical evidence.====I begin by presenting new evidence from Mexico on the relationship between employer size and spinout dynamics. Consistent with existing evidence from other economies, I use individual- and firm-level data to show that spinout entry (performance) is negatively (positively) associated with employer size. I argue that this relationship can arise naturally from the occupation choices of employees when they can learn from their employers.====I formalize this argument by developing a general equilibrium model of occupational choice and firm dynamics that features an endogenous relationship between existing firms, employees, and new firms. In the model, agents are heterogeneous in entrepreneurial productivity and choose between employment and entrepreneurship. Entrepreneurs hire workers and make risky investments to improve their productivity. Workers are employed by entrepreneurs and can learn from their employers to improve their entrepreneurial productivity and potentially form spinouts. A novel aspect of the model is that both the benefit and the cost of learning are increasing in the gap between the know-how of the entrepreneur (the teacher) and the worker (the learner). This form of learning captures the intuitive notion that more productive workers find it easier to learn from their employers, and that there is more to learn from more productive employers.==== Importantly, when learning depends on the productivity of both employees and employers, the characteristics of existing employers determine the characteristics of new entrants – a channel novel to models of occupational choice and firm dynamics.====When calibrated to match worker and firm-level outcomes in Mexico, the model replicates un-targeted features of the data. The stationary equilibrium features a negative relationship between employer size and spinout entry since the calibrated cost of learning is increasing in employer productivity. So, employees of larger, more productive firms are less likely to learn from their employers and hence less likely to form spinouts. However, conditional on learning, employees of larger firms learn more compared to employees of smaller firms. As a result, spinouts from larger firms tend to be larger, at entry, than spinouts from smaller firms – as observed in the data.====Spinout growth and employer size are also positively associated in the model. This relationship is driven by the option value of learning in employment. In particular, the ability of agents to learn from their employers as ==== lowers their incentives to invest as ====. The option to learn as a worker is especially salient for low productivity entrepreneurs close to the margin between employment and entrepreneurship. This means that more productive firms invest relatively more in their growth and, as a result, spinouts from larger firms not only start large but also grow faster than spinouts from smaller firms.====I use the calibrated model to study the quantitative implications of learning and the relationship of employer size and spinout dynamics on aggregate outcomes, including the response to policies targeting existing firms.====First, I explore the impact of changes in the efficiency with which workers learn from employers on aggregate outcomes in the model. I interpret learning efficiency as representing managerial practices, with better practices increasing the amount that employees can learn from their employers. In the model, higher learning efficiency leads to higher output per worker and average firm size while lowering the share of entrepreneurs. Interestingly, the model predicts that improving learning efficiency decreases the share of workers transitioning into entrepreneurship – the aggregate spinout rate. This is due to a general equilibrium effect: Improving employee learning increases the return to employment as workers can learn more from their employers. This changes the equilibrium occupational choices of agents and results in an endogenous shift of the firm size distribution to the right. The resulting firm size distribution features higher productivity employers from whom it is costlier to learn, lowering the aggregate spinout rate. While fewer spinouts are formed, those employees that do ‘spinout’ operate higher quality firms raising entrant quality. Studying the impact of learning efficiency is salient for understanding cross-country differences as relatively developed economies feature better management practices, larger firms, fewer entrepreneurs, and a lower aggregate spinout rate.==== These results highlight the importance of considering factors that affect employee learning and spinout formation in order to fully understand differences in aggregate outcomes across economies.====Next, I study the response of the model economy to two common types of policies targeting existing firms. By endogenizing the link between existing firms, employees and new spinouts, the model delivers new insights and implications that are useful to policymakers but absent in frameworks that abstract from this link.====The first policy is a temporary subsidy to small firms. I compare the response to this policy in the benchmark model to that in a version of the model without learning where employer size and spinout dynamics are independent. I find stark differences in the transitional dynamics across these two model economies. For instance, the benchmark economy predicts an increase in the new firm entrants and lower worker welfare following the policy. In contrast, the same policy in the economy without learning features a decrease in entrants and greater worker welfare. In both economies, a subsidy to small firms results in a temporary reallocation of workers towards less productivity firms. However, this reallocation has long-lasting implications in the benchmark model where the characteristics of entrants depend on characteristics of their former employers. This analysis has implications for policy design since it shows that policies targeting existing firms will have a meaningful and long-term impact on welfare of workers and the creation of new firms.====The second type of policy I explore is the provision of management training programs. In particular, I evaluate the macroeconomic impact of such training programs by simulating a training intervention in the benchmark model. To do this, I introduce changes in key model parameters so that the re-calibrated model matches (partial equilibrium) results from randomized controlled trials of training programs. Then, I use the model to evaluate the impact from an economy-wide (general equilibrium) roll-out of management training programs. I find significant welfare gains for workers resulting from training programs and an amplification of overall welfare gains in general equilibrium compared to partial equilibrium. Further, I show that improving employee-management practices (that raise learning efficiency) ==== production-related operational practices (that raise firm productivity) result in significantly better outcomes compared to improving only operational practices. These results are a consequence of workers simultaneously benefiting from management training programs – an insight that is currently under-emphasized in the existing literature and may be overlooked by policymakers.====Taken together, this paper highlights the endogenous relationship between existing firms, employees and new firms and shows that this relationship is important for aggregate outcomes both within and across economies.==== This paper contributes to several strands of literature. First, I extend the existing empirical literature on spinout dynamics and its relationship with employer characteristics. Country-level studies include, Sørensen and Phillips (2011) (Denmark), Elfenbein et al. (2010) (US), Muendler et al. (2012) (Brazil), Berglann et al. (2011) (Norway), and Andersson and Klepper (2013) (Sweden). These studies are based primarily on matched employer-employee data and also document a negative relationship between employer size and spinout entry. Existing research on the relationship between employer size and spinout performance is more limited and with mixed results. This paper is the first to study spinouts in Mexico. An important advantage of using Mexican data is that it includes detailed information on individual demographics and work-related variables. This allows me to uncover novel relationships between spinout formation and characteristics such as job tenure, age, and income, among others. Furthermore, these data include information on both formal and informal activity – a crucial distinction in developing economies. This more comprehensive coverage allows for cross-country comparisons of spinout dynamics and their implications for aggregate outcomes.====This paper also relates to recent literature on knowledge diffusion within the workplace and particularly between employers and employees. The empirical evidence and theoretical implications from the model developed here are consistent with those of Jarosch et al., 2021 and Herkenhoff et al., 2018 who use administrative-level data and find strong evidence in support of employees learning from their co-workers, particularly those with higher ability and in managerial positions. Brooks et al. (2018) quantifies the (positive) impact of entrepreneurs learning from other entrepreneurs through a field experiment.====The model in this paper introduces employee learning in an occupational choice framework that features firm dynamics. Learning here is distinct from existing works such as Roys and Seshadri, 2014 and Lucas and Moll, 2014 since I assume that it is not only a function of an agent’s own productivity but also that of their employer. Theoretical work from Chari and Hopenhayn, 1991, Dasgupta, 2012 and Jovanovic, 2014 introduce learning where both worker and employer productivity is important but abstract from new firm dynamics and focus on technology adoption, gains from trade and economic growth, respectively. The findings in this paper also provide a potential source for the ex-ante heterogeneity among new firms recently highlighted by Sterk et al. (2021).====Spinout formation and employees learning from employers is also studied in Baslandze (2017) which extends the theoretical framework of Franco and Filson, 2006 and studies the consequences of non-compete laws for spinout dynamics and aggregate growth. However, Baslandze (2017) focuses on highly innovative, patent issuing firms as sources of spinouts and documents a ==== relationship between spinout entry and employer size which differs from the aggregate relationship observed in this and other country-level studies.==== Also related is Engbom, 2020 who develops a model of firm and worker dynamics in which employee learning can depend on employer productivity. Engbom (2020) also highlights the importance of the link between the workers’ experience in the labor market and firm dynamics and studies the implications of an aging population on new firm entry and growth.====Finally, this paper has implications for the implementation and design of policies targeting existing firms. Of particular relevance is the literature that studies the impact of management practices on firm-level outcomes. Much of this literature investigates the role of management through randomized controlled trials. Examples include Bloom et al., 2013, Abebe et al., 2019, Brooks et al., 2018, and Bruhn et al., 2018, among others. The findings in this paper show that the impact of improving management practices within existing firms is relevant not only for the creation, size, and growth of new firms but also the welfare of workers.====The study of the aggregate impact of business training programs conducted here joins a growing literature that studies the general equilibrium, macroeconomic impact of policies disciplined by results from micro-evaluations (see, for example, Buera et al. 2021 and Brooks et al. 2021).====An outline of the paper is as follows. Section 2 details the empirical evidence from Mexico. Section 3 presents the model, and Section 4 includes a quantitative analysis of the mechanisms of the model as well as the importance of learning efficiency. Section 5 discusses the implications for policy and Section 6 concludes. Additional empirical and computational details can be found in the appendix.","From employee to entrepreneur: Learning, employer size, and spinout dynamics",https://www.sciencedirect.com/science/article/pii/S0165188921002050,28 October 2021,2021,Research Article,151.0
Sorge Marco M.,"Department of Economics, University of Bologna, Italy,University of Salerno, University of Göttingen and CSEF - DISES, Via Giovanni Paolo II 132 Fisciano 84084 SA, Italy","Received 1 April 2021, Revised 7 October 2021, Accepted 11 October 2021, Available online 20 October 2021, Version of Record 9 November 2021.",https://doi.org/10.1016/j.jedc.2021.104265,Cited by (1),Recent structural VAR studies of the ==== under empirically tenable parameterizations. This alleviates concerns about identification and lag truncation bias: low-order Cholesky-VARs do well at retrieving the true aggregate effects of ==== shocks in a Cholesky world.,"Recursive identification schemes based on short-run exclusion restrictions have traditionally been used to identify the macroeconomic effects of monetary policy shocks (e.g. Christiano, Eichenbaum, Evans, 1999, Sims, 1980). Making the recursive scheme operative in the context of vector autoregressive (VAR) models is accomplished by generating a Cholesky decomposition of the variance-covariance matrix of the reduced-form residuals, with the policy rate placed between slow and fast moving variables, e.g. Kilian (2013). Albeit empirically appealing, the Cholesky approach calls for a conceivable structural interpretation of the enforced recursive ordering.====The lack of conformity between the conventional timing in structural macroeconomic frameworks and the identifying assumptions in Cholesky-VARs has in fact spurred interest in the development of dynamic stochastic general equilibrium (DSGE) models exhibiting some degree of recursiveness, e.g. Rotemberg and Woodford (1997), Christiano et al. (2005), Boivin and Giannoni (2006), Altig et al. (2011). While resorting to peculiar (and often overly restrictive) assumptions on the timing of decisions, these studies do not engage in a thorough investigation of the DSGE-VAR mapping in the presence of Cholesky-type restrictions.====The present paper fully characterizes conditions for existence of causal VAR representations of general, multivariate DSGE models featuring informational constraints that embody classical Cholesky-timing restrictions; and explores the relevance of these conditions for empirical VAR-based exercises aimed at identifying the monetary transmission mechanism or rather at validating/estimating DSGE models via the use of impulse-response analysis. Formally, we show that information-based timing restrictions enlarge a model’s equilibrium state space and modify rational expectations cross-equation restrictions (CERs), opening room to the emergence of (i) nonfundamental VARMA representations for any set of observables, or (ii) invertible non-trivial MA equilibrium components vis-à-vis their counterparts free of timing restrictions. As a result, even when impulse responses are rightly constrained by the Cholesky scheme to be an exact match to the theoretical ones on impact, a finite order Cholesy-VAR may still prove an inaccurate approximation of the true VARMA structure that shapes the endogenous adjustment paths to monetary policy shocks.====It is well-known that identifying a given DSGE model’s structural shocks via VAR analysis require that the observed variables contain sufficient information to recover the unobserved state variables which are assumed to produce the observables. In particular, if the model’s solution does not admit an invertible (or fundamental) representation for the observables, then there exists no (linear) invertible mapping from the VAR innovations to the structural shocks, see e.g. Alessi et al. (2011).====Our first contribution is to uncover a novel invertibility issue in theoretical macroeconomic models that conform with a Cholesky-type structure. Differently from dynamic structures with persistently dispersed information (e.g. Kasa, 2000, Kasa, Walker, Whiteman, 2014), the specification of information-based timing restrictions does not involve an infinite regress of expectations, and the underlying model’s representation will generically be finite dimensional. However, while being fundamental in terms of the innovations to agents’ information sets, this representation can prove nonfundamental, with respect to ==== set of observables, in terms of the structural shocks. We isolate conditions for existence of fundamental equilibrium representations of restricted DSGE models, that solely rely on the imposed information structure and the reduced-form coefficients of the model’s solution under conventional (unrestricted) timing. In the same spirit of Fernández-Villaverde et al. (2007), the procedures to check for fundamentalness and to characterize the finite-order VAR equilibrium representation of restricted DSGE models are cast in a straightforward algorithmic form, that can be used very broadly in structural macroeconomic modeling, well beyond the applications developed next.====Even when warranting invertibility, timing restrictions can enforce VARMA equilibrium representations that exhibit slowly converging VAR polynomial matrices at fairly long horizons. In principle, this might prevent low-order VARs, of the type required by actual data availability, from being informative about the dynamic effects of monetary shocks, even though the identification scheme correctly reflects the structure of the model that generates the observables, e.g. Ravenna (2007) and Poskitt and Yao (2017). To investigate this issue, we set up a series of controlled Monte Carlo experiments on the assumption that the true data generating process (DGP) belongs to the class of restricted DSGE economies. Specifically, we rely on several specifications of two fully-fledged models of monetary policy transmission in the New Keynesian tradition. The first one is the sticky-price framework with consumption habits and inflation inertia advanced by Boivin and Giannoni (2006) for their quantitative exploration of the effectiveness of monetary policy in the U.S. post-WWII macroeconomic history. The second is the three-equation monetary model with lagged expectations employed by Guerron-Quintana et al. (2017) to exemplify the properties of impulse-response matching estimators for DSGE models. We leverage the flexibility of Kormilitsina (2013)’s perturbation approach to solving restricted DSGE models in order to bring the above mentioned frameworks in closer conformity with the Cholesky-VAR recursive structure. We then submit the properly restricted models to our formal test for VAR representability, and explore what the practical consequences of failing this test are for the impulse response functions of interest.====We show that both dynamic New Keynesian (DNK) models admit a causal (possibly infinite order) VAR representation under empirically tenable parameterizations, thus validating the use of VAR-based approaches to evaluation of monetary impulse responses. Inspection of the evolution of the coefficients of the theoretical VAR(====) further reveals that they rapidly decay towards zero, mitigating concerns about lag truncation bias arising from the adoption of low-order VAR systems. We then scrutinize the ability of Cholesky-VARs to uncover the true aggregate effects of monetary shocks through the lens of our model laboratories; and also assess the relative performance of the Cholesky scheme vis-à-vis the agnostic sign restrictions procedure, popularized by e.g. Canova and De Nicolo (2002) and Uhlig (2005). Our simulation results convincingly suggest that Cholesky-VARs, ==== and as opposed to the competing identification scheme, are likely to provide tight and reliable inference about the monetary transmission mechanism in restricted DNK structures, no matter how long-lasting the dynamic effects of timing restrictions are. Sign restrictions, by contrast, may fail to correctly unveil the true size of the monetary impulse responses for they prove sensitive to the relative volatility scale of structural disturbances; a finding in line with e.g. Paustian (2007) and Castelnuovo (2016). Finally, an application to the estimation of restricted DSGE models via impulse response function (IRF) matching techniques is proposed.====A last remark about the scope of our analysis is in order. While we focus on the DSGE-VAR mapping in the presence of structural Cholesky-type restrictions that arise from ever-binding informational constraints, a large strand of literature has rather explored alternative approaches to shock identification that abstract from the ordering of the variables in the VAR and thereby comply with the standard (non-recursive) timing protocol of DSGE models, such as the use of external instruments (e.g. Angelini, Fanelli, 2019, Gertler, Karadi, 2015, Stock, Watson, 2018), the heteroskedasticity in the data (e.g. Bacchiocchi, Fanelli, 2015, Kilian, Lütkepohl, 2017), the independence and non-Gaussianity of the shocks (e.g. Lanne et al., 2017), or direct coefficient restrictions on monetary policy rules (e.g. Arias et al., 2019). We of course acknowledge the relevance of these research avenues, whether or not associated with the empirical validation of shocks’ transmission mechanisms of DSGE frameworks. The specific questions we address and the answers we provide are not to be read as an endorsement of classic exclusion restrictions against competing alternatives for identification purposes; rather, they are meant to warn applied researchers against the potential emergence of econometric issues inherent in the reduction of restricted DSGE models into VAR form.====The paper proceeds as follows: Section 2 presents a simple example that illustrates the potentially harmful consequences of information-based timing restrictions on the VAR-based analysis of DSGE models. Section 3 formally discusses how to construct first-order approximate solutions to general DSGE economies featuring timing restrictions. Section 4 provides easy-to-check conditions under which restricted DSGE models admit a fundamental (of possibly finite-order) equilibrium representation for the observables. Section 5 lays out the DNK model laboratories, that are next used (Section 6) to assess the ability of classical short-run exclusion restrictions to unveil the model-implied monetary transmission mechanism, and to document the relative performance of Cholesky-VARs versus a sign restrictions-type of approach. Section 7 offers concluding remarks.","Under the same (Chole)sky: DNK models, timing restrictions and recursive identification of monetary policy shocks",https://www.sciencedirect.com/science/article/pii/S0165188921002001,20 October 2021,2021,Research Article,152.0
Cherubini Umberto,"University of Bologna, Department of Economics, Piazza Scaravilli, 2 Bologna, 40126, Italy,New York University, Tandon School of Engineering, Finance and Risk Engineering Department (Research Collaborator), 1 Metrotech Center, Brooklyn, NY, 11201, USA","Received 5 May 2021, Revised 8 October 2021, Accepted 12 October 2021, Available online 20 October 2021, Version of Record 9 November 2021.",https://doi.org/10.1016/j.jedc.2021.104268,Cited by (0),"We study the dependence between redenomination and default risk for Italy, France, Germany and the Netherlands exploiting the 2014 revision of ISDA CDS contract standard. For these countries redenomination has become a credit event under the new standard, while it was not so under the old standard. Both contracts are currently traded. Using ==== on transformed data we simultaneously estimate the dependence of the two risks and an unbiased measure of redenomination risk. The unbiased estimate unveils a feature common to all four countries. Redenomination risk has risen and remained above default risk since February 2017, when the U.K. Parliament officially approved the formal request to start the Brexit procedure. We find that after February 2017 the CDS market has mostly priced redenomination as the most likely outcome of the next sovereign crisis in the Euro area. Finally, we use a Marshall–Olkin approach to model the dependence of redenomation risk of the countries and to estimate a measure of the end of the Euro. The measure obtained is very close to the value of the German CDS, in support of the widespread market practice of using that contract as tail-hedge against end of the Euro.","A sovereign debt crisis in Europe could materialize in terms of outright default on public debt obligations and/or redenomination of the currency. It is natural to expect dependence between these two events since they typically depend on some common frailty factors in the sustainability of public debt. The purpose of this paper is to estimate this dependence, and to show that ignoring it induces a bias in the estimated measure of redenomination risk.====On technical grounds, if default and redenomination risk are dependent, the instantaneous conditional probability that either of the two could happen is not equal to the sum of the marginal intensities (that is the marginal instantaneous conditional probabilities). Instead, it could be broken in the sum of the conditional probabilities of either of the two events occcuring before the other. These intensities (called ====-intensities in Bielecki et al., 2007) collapse to the marginal intensities only if default and redenomination risk are independent.====Since 2014, an institutional innovation in the CDS market has made possible the estimation of dependence between default and redenominaton risk and the unbiased estimation of the latter. In 2014 ISDA introduced a vast revision of the CDS standards. Among other changes, the ISDA 2014 standard stated that for all the European countries, sovereign CDS can be triggered the first time that either default or redenomination occurs.==== This marked a change with respect to the previous standard, dated 2003, for those European countries that are part to the G7 group (Italy, France and Germany) and for OECD countries with AAA-rated debt (the Netherlands).==== For these 4 European countries, under the 2003 standard redenomination was not a credit event and has become a credit event under the 2014 one.====Both the contracts under the 2003 and 2014 are currently traded in the market, with the 2003 standard trading at a lower premium than the 2014 one. Unfortunately, to the best of our knowledge a formal analysis of the relative level of trading activity is not available. However, a casual observation of the number of contracts that we performed for July 20, 2021 and July 21, 2021 showed that the trading activity on contracts underwritten under the 2003 contract remains significant, even though lower than that on the 2014 standard contract. In the two days we reckoned 142 new contracts under the 2014 standard and 96 under the 2003 standard. The 2003 contract is still traded for two reasons: first, the 2003 contract is cheaper than the other, even though it provides less protection; second, when redenomination risk is material, it can be used to build a synthetic hedge. The evidence for Italy seems to be consistent with the latter argument. In fact, for Italy, where one may expect high redenomination risk (mostly because the Italian debt/GDP ratio is the second highest in Europe, after Greece), we counted 52 contracts under the 2014 standard and 47 under the old one. So, the trading activity in the two contracts is comparable. Moreover, the trading volume on Republic of Italy represented more than 40% of the trading activity of the four countries, confirming a robust demand for hedging associated to higher risk.====Following this innovation, a measure of redenomination risk called ISDA basis was proposed in blog posts by Minenna (2017) and GrosItalian Risk Spreads, 2018. The ISDA basis is the difference between CDS premia under the 2014 ISDA standard and the corresponding ones in the 2003 definition. The robustness of this measure with respect to liquidity considerations was proved by Kremens (2019). Bonaccolto et al. (2020) also exploit the ISDA basis, exploring the information content of CDS denoted in different currencies. In this paper we build on this idea to show that using the CDS quotes under the two definitions makes it possible to estimate the dependence between default and redenomination risk and to compute an unbiased measure of redenomination risk. In other words, while in general the ISDA basis considered as the outright difference between the CDS spreads in the alternative ISDA standards would be biased, we illustrate here that it is possible, and relatively easy, to correct this bias, by Maximum Likelihood Estimation (MLE) on transformed data (Duan, 1994; Duan, 2000).====Technically, the cases of Germany, France, Italy and the Netherlands provide an interesting inverse derivative estimation problem: we observe the price of ==== (FTD) derivative contract on two events, default or redenomination, and the price of just one of the two, namely default. The former is the CDS quote under the 2014 ISDA standard, the latter it the CDS quote under the 2003 one. The estimation procedure allows to recover the premium of a CDS contract representing protection against redenomination only.====Of course, the bias generated by dependence between default and redenomination could also affect other measures of redenomination. In particular, De Santis (2019) proposes a Quanto-CDS measure, defined as the difference of Quanto-CDS (the spread between CDS quotes in dollars and euros) for several countries and those of Germany. Also in this case the purpose is to clean the CDS quotes from credit risk. This may be subject to debate since Quanto CDS is not a direct measure of redenomination, and actually measures the conditional depreciation of the currency given a credit event, including those occurring to corporate borrowers (see Brigo, Pede, Petrelli, Ehlers, Schoenbucher, 2004, El-Mohammadi). However, for what matters here, even if one accepted that a Quanto-CDS measure could clear credit risk, the outright subtraction of CDS in euros from CDS in dollars would produce a biased measure of redenomination risk if default and redenomination risk were dependent. The same applies to similar measures based on prices of other products denominated in different currencies (Eichler and Rövekamp, 2017).====Of course, the measure extracted from the ISDA standard innovation is available only for four countries (even though a core part of the Euro area market), while the other measures proposed in the literature are available for all the countries of the Euro, at the cost of being less direct and more noisy. We will show that the measure obtained for the four countries affected by the ISDA change can play a role as a reference model to check the robustness of the other measures. Finally, recognizing dependence among risks may also be relevant for the related literature that considers redenomination among other risk factors explaining the dynamics of sovereign bond spreads (Di Cesare et al. 2012; Afonso, Arghirou, Gadea, Kontonikas, 2018, Bayer, Kim, Kriwolutzky, Dewachter, Iania, De Sola Perea, 2015, Krishnamurthy, Nagel, Vissing-Jorgensen, 2017).====Having addressed the question of the unbiased estimation of redenomination risk for every country, the next obvious goal would be to study the cross-border link of redenomination events and the relevance of a catastrophic event of break-up of the Euro. Among practitioners it is well known that tail-hedgers use the CDS on Republic of Germany as a proxy for this extreme risk. Here we propose to model the break-up of the Euro as a common shock Marshall–Olkin model. Redenomination of each country is decomposed in a break-up of the Euro component and a idiosyncratic component. We show that this model produces an estimate of the risk of Euro break-up that is quite close to the German CDS, confirming the market practice.====The structure of the paper is as follows. In Section 2 we model dependence between default and redenomination risk with constant intensities. In Section 3 we show how to estimate the redenomination measure allowing for dependence with the risk of default. In Section 4 we report empirical evidence on redenomination risk for Italy, France, Germany and the Netherlands, both at the country level and in a cross-country analysis including a scenario of simultaneous redenomination events leading to the end of the Euro. Section 5 concludes.",Estimating redenomination risk under Gumbel–Hougaard survival copulas,https://www.sciencedirect.com/science/article/pii/S0165188921002037,20 October 2021,2021,Research Article,153.0
Dissanayake Ruchith,"Queensland University of Technology, QUT Business School, Brisbane, Australia","Received 17 June 2021, Revised 8 October 2021, Accepted 13 October 2021, Available online 20 October 2021, Version of Record 8 November 2021.",https://doi.org/10.1016/j.jedc.2021.104267,Cited by (0),I examine the effects of geographic distribution of firms on the expected ,"Firm production is quite concentrated in space. The high concentration of advertising industry in Manhattan and auto industry in Detroit are well-known examples of geographic agglomeration of firms. Firms geographically concentrate near one another for many reasons. The primary advantage of agglomeration is to pool the demand for specialized labor, first emphasized by Marshall (1890). Other positive externalities of agglomeration include increasing returns to scale, information sharing, and intellectual spillovers.==== Although knowledge spillovers and input sharing are important benefits of agglomeration, the evidence is strongest for labor market pooling (Dumais, Ellison, Glaeser, 2002, Rosenthal, Strange, 2001). The literature documents benefits of agglomeration on aggregate consumption growth (Davis et al., 2014), firm productivity (Henderson, 2003), and wages (Amiti, Cameron, 2007, Glaeser, Maré, 2001, Rosenthal, Strange, 2008). However, to the best of my knowledge, none of the prior studies have examined the time-varying risks associated with geographic distribution of firms and the implications on the cross-section of stock returns. The goal of this paper is to fill this gap in the literature.====The geographic agglomeration of firms allows managers the opportunity to establish informal local labor market networks to hire workers with better abilities and negotiate better wage contracts. Montgomery (1991) theorizes that firms learn about a potential worker’s ability if the firm employs individuals from the potential worker’s network. Furthermore, in equilibrium, individuals are more likely to receive and accept wage offers from businesses that employ others in their network. Underlying most network models is some form of information imperfection in which networks serve, at least partially, to mitigate these imperfections. There is a large body of work that use employer-employee micro data at the establishment level to show the presence and importance of labor market networks based on proximity (e.g., Bayer, Ross, Topa, 2008, Hellerstein, Kutzbach, Neumark, 2014, Hellerstein, McInerney, Neumark, 2011, Hensvik, Skans, 2016, Rees, 1966, Saygin, Weber, Weynandt, 2021). Managers use these social networks to attract workers with better qualities in hard-to-observe dimensions (Hensvik and Skans, 2016). Also, there is evidence that displaced workers are more likely to become re-employed at a firm in their geographical network that employs former co-workers of the displaced worker (Saygin et al., 2021).====These local labor market networks provide firms in geographically agglomerated industries the opportunity to better negotiate wage contracts during market downturns. During low growth periods, workers are reluctant to move outside their labor market networks and have lower expectations about outside opportunities. This allows managers in agglomerated industries to better negotiate wage contracts with workers during recessions. Geographic proximity of firms increases the correlated actions among interacting managers and induces amplified responses to aggregate shocks (Guiso and Schivardi, 2007).==== This amplification effect is supported by information spillover models; agents face a common problem in an uncertain environment and each agent holds private information, which can be inferred from other agents’ actions (Banerjee, 1992, Bikhchandani, Hirshleifer, Welch, 1992). When one firm negotiates lower wage contracts, the information revealed triggers further actions, and start a self-reinforcing process that prompts other managers in the local labor market networks to adjust wages within a short time span. This wage adjustment provides firms in agglomerated industries a natural hedge (procyclicality of wages) against aggregate shocks. Since fewer people are making long-distance moves in the U.S. Molloy et al. (2011), the effects of geographical networks are likely to become even stronger in the future.====The procyclicality of wages in agglomerated industries, relative to dispersed industries, have direct consequences on firm valuations. For firms in agglomerated industries, the lower average wage costs during market downturns reduces the ==== covariance between firm’s cash flow and business conditions, which translates to lower cost of equity. Firms in dispersed industries, on the other hand, have relatively stickier wages. Consequently, firms in dispersed industries face a greater drop in cash flow following an adverse systematic shock, which generally leads to a recession. The lower valuations increase the expected returns for firms in dispersed industries relative to firms in agglomerated industries.====Sophisticated investors can obtain information about firm geography through costly sources such as analyst reports. I use an indirect approach to identify industry agglomeration that is both unbiased and systematic; I use the Ellison and Glaeser (1997) (EG) index of geographic agglomeration in manufacturing industries. EG index is independent of the number of plants and of their distribution and controls for the industrial concentration. As shown by Dumais et al. (2002), geographic concentrations for industries are strikingly stable over time. I map the EG index with the universe of stocks from the CRSP dataset. An appealing aspect of this approach is that the classification of firms is based on a highly researched and economically meaningful characteristic.====An important caveat is that the entire system of informal labor market networks is not observable to the econometrician. However, I can infer the consequences of the networks, without observing the actual network connections, by examining changes in the aggregate variables of interest (i.e., wages, cash flow, and expected returns). It is important to emphasize that the geographically local informal network structure need not be interpreted literally. Instead, it can also describe connectedness among firms in terms of social attributes, research interests, compatibility of R&D programs, etc. I emphasize the labor market network since it is well established in the literature. Any other attributes related to economic geography that both co-vary over the business cycle and provide a natural hedge against aggregate shocks strengthen the arguments presented in this paper.====This paper makes three contributions. ====, I show that cash flows are more procyclical in geographically dispersed firms, and hence more exposed to systematic risk, than that in agglomerated firms. Specifically, using manufacturing industry data, I test whether wages are more procyclical in agglomerated industries than that in dispersed industries. I control for unobserved heterogeneity by including industry fixed effects estimates, which remove any industry fixed characteristics. The regressions also include year fixed effects, which control for any changes in the aggregate investment opportunities over time. I find that the average wages are more procyclical (countercyclical) in geographically agglomerated (geographically dispersed) industries. This implies that firms in agglomerated industries with formal and informal labor market networks can better adjust wages during market downturns than firms in geographically dispersed industries.====The cyclicality of wages leads to a reduction in cash flow risk for firms in geographically agglomerated industries. For example, a negative demand shock, which generally leads to an economic downturn, can reduce a firm’s future cash flow. If a firm can reduce labor costs following the negative shock, then some of the adverse effects on cash flow are abated. Following a negative shock, firms in geographically agglomerated industries can better adjust wages and reduce the impact of the adverse effects on cash flow. This wage adjustment process in geographically agglomerated firms acts as a “natural hedge” against adverse systematic shocks.====This natural hedge in geographically agglomerated firms should be reflected in a firm’s cash flow over the business cycle. To test this conjecture, I construct a measure of firm-level geographic risk. I construct a “hedge factor,” which is a mimicking portfolio that goes long on geographically dispersed industries minus agglomerated industries based on the EG index classification (==== portfolio).==== The ==== portfolio captures aggregate shocks that are naturally hedged in geographically agglomerated firms but not in geographically dispersed firms. To measure the geographic risk at the firm level, at each point in time, I compute a stock’s sensitivity to the ==== portfolio returns (====) using a 60-month window. Firms with low ==== have low exposure to geographic risk since geographic agglomeration provides a natural hedge against aggregate shocks. In contrast, firms with high ==== have high exposure to geographic risk.====Using panel regressions on a sample of publicly listed firms in Compustat, I show that earnings - measured using ROA and cash flow to assets - are more cyclical in firms with high ==== controlling for both firm and year fixed effects. I also estimate the parameters for (1) the sample of manufacturing firms for which the geographic risk can be computed at the industry level ==== EG index and (2) the sample of firms for which the geographic risk can be computed using only the ====. The results are similar for both sub-samples; firms with high ==== have more cyclical earnings. Hence, during recessions when cash flow is needed the most, cash flows are significantly lower for firms with high geographic risk (high ====) than for firms with low geographic risk (low ====).==== I show that the expected stock returns are higher in geographically dispersed firms than in agglomerated firms. Since firms in geographically agglomerated industries are better naturally hedged against aggregate shocks, the expected returns are lower for agglomerated firms than for geographically dispersed firms; the investor is willing to pay a higher price, hence lower expected return, to hold stocks in geographically agglomerated industries than for stocks in geographically dispersed industries.====The benefit of the natural hedge in agglomerated firms is heterogenous across-time and across-firms. In the time-series, the marginal benefit of hedging is greater during recessions - periods of low demand and productivity - than during economic expansions. The natural hedge against aggregate shocks in agglomerated firms helps curb the cash flow risk during times of low growth. Firms in geographically dispersed industries, in contrast, are fully exposed to aggregate shocks, and hence more vulnerable to cash flow risk. These effects are significantly exacerbated during recessions - time periods of heightened economic uncertainty when investors avoid risk. During low growth periods, the exposure to adverse shocks plays even a greater role. Hence, the premium an investor demands to hold stocks with high geographic risk should be higher during recessions.====Following Fama and French (1993), I use value-weighted portfolio sorts to examine the geographic risk premium. I use the classification by NBER as the primary indicator of recessions.==== I show that stocks that co-vary closely with the ==== portfolio returns (high ==== stocks) earn higher expected returns. Using univariate sorts on all stocks in CRSP, I show that stocks with high ==== have significantly higher expected returns than firms with low ====, especially during recession months. During recessions, the annualized long-short portfolio spread over the CAPM and Carhart (1997) 4-factor model ==== is approximately 15.2 percent and 12.2 percent, respectively. The annualized long-short portfolio spread is statistically insignificant during times of economic growth. The findings are consistent with a time-varying geographic risk premium.====I also perform several tests to examine the robustness of the results. Hou et al. (2020) show that majority of the anomalies reported in the asset pricing literature becomes statistically insignificant once microcap stocks are removed from the sample. To mitigate this concern, I follow Hou et al. (2020) and examine the equal-weighted returns on ==== sorted portfolios excluding all microcap stocks. I continue to find a significant geographic risk premium over the CAPM. The risk premium is significantly larger during recessions. I also conduct a sub-sample test using non-manufacturing stocks for which geographic risk can only be computed using ====. Again, I continue to find qualitatively similar results.====, I show that the geographic risk premium is more pronounced among firms that are more vulnerable to adverse systematic shocks. If the time-varying premium is driven by changes in the exposure to systematic risk, then the effects should be more pronounced among firms that are more vulnerable to aggregate shocks. Firms that have higher production costs or lower revenues than competitors should have higher probability of default during market downturns. For such firms, the exposure to systematic risk plays a more consequential role.====To test the cross-sectional heterogeneity in the geographic risk premium, I use two-way portfolio sorts. I employ 5 ==== 3 double sorts on pre-ranked ==== and previous year’s profitability (ROA). I then examine the long-short portfolio spread on the ==== sorted quintiles across ROA terciles. The ==== sorted long-short spread is statistically significant for all profitability terciles during recessions when investors avoid risk. However, during economic expansions, the geographic risk premium is significantly larger for firms in the lowest ROA tercile. Firms with low earnings are more vulnerable to negative aggregate shocks even during high growth states. During high growth states, investors remain concerned about geographic risk for firms with low earnings and command a significant risk premium. I repeat the tests using double sorts on pre-ranked ==== and previous year’s cash flow-to-assets ratio and find consistent results.====The NBER definition of recessions is commonly used in the literature to proxy low growth states. However, this measure is unavailable in real time and are subject to subsequent revision as more macroeconomic information becomes available. To reduce the look-ahead bias, I compute an alternative measure of market downturns based on financial data. I estimate a Markov regime-switching dynamic model using only the excess market returns and its lags. I then predict low growth state probabilities based on the parameters obtained from the regime-switching model. A Kalman filter is used to predict the low growth states using only the past and contemporaneous information.==== The results are consistent using the regime switching model generated economic growth states. In the time-series, the geographic risk premium is larger during recessions. In the cross-section, the geographic risk premium is larger firms with low earnings, especially during recessions.====The literature reporting the relationship between a firm’s location and stock returns is limited. Pirinsky and Wang (2006) show a positive co-movement in stock returns among firms headquartered in the same geographical area. Garcia and Norli (2012) find that investors display a preference for investing in local firms. Korniotis and Kumar (2013) find that local economic conditions predict the stock returns of firms in the local geographical area. Tuzel and Zhang (2017) find that firm location affects firm risk through local factor prices. Smajlbegovic (2019) examines the diffusion of regional macroeconomic information into stock prices. I show that geographic agglomeration provides a natural hedge against aggregate shocks and the effects vary over the business cycle.====This paper also contributes to the labor-based asset pricing literature. Chen et al. (2011) show that the cost of capital is higher for industries with high unionization levels. Merz and Yashiv (2007) and Belo et al. (2014) introduce labor frictions and Donangelo et al. (2019) introduce labor leverage in asset pricing models. Donangelo (2014) shows that differences in labor mobility leads to differences in risk premiums in the cross section. Zhang (2019) show asset pricing implications when firms replace routine task labor with technology. I emphasize the link between wage costs, geographic risk, and stock returns.====The paper is organized as follows. Section 2 introduces the testable hypotheses. Section 3 introduces data and the measures. Section 4 explores the geographic risk on industries and firms. Section 5 shows the asset pricing results. In Section 6, I conduct tests based on a regime switching model and Section 7 concludes.",Geographic distribution of firms and expected stock returns,https://www.sciencedirect.com/science/article/pii/S0165188921002025,20 October 2021,2021,Research Article,154.0
"Roncoroni Alan,Battiston Stefano,D’Errico Marco,Hałaj Grzegorz,Kok Christoffer","Dept. of Banking and Finance, University of Zurich, Switzerland,Ca’ Foscari, University of Venice, Italy,European Systemic Risk Board Secretariat, European Central Bank, Frankfurt Germany,Bank of Canada, Ottawa Canada,European Central Bank, Frankfurt Germany","Received 6 August 2020, Revised 22 June 2021, Accepted 10 October 2021, Available online 19 October 2021, Version of Record 4 November 2021.",https://doi.org/10.1016/j.jedc.2021.104266,Cited by (7)," interconnectedness, via a network of interbank loans, banks’ loans to other corporate and retail clients, and securities holdings. The second channel concerns ","The financial system is characterized by a wide range of interconnections that can, in various ways, give rise to contagion effects with potentially pernicious implications for financial stability. While direct contagion through, for instance, bilateral links between banks and other financial institutions have long been recognized as an obvious transmission channel, more indirect contagion effects through, for instance, banks’ common exposures to similar economic sectors are gaining increasing attention as a potentially more potent channel of contagion (Clerc et al., 2016). In this paper, we are interested in exploring how the structure of financial exposures interacts with market conditions to influence systemic risk in the European banking system. The notion of systemic risk applied here refers to direct interbank contagion effects and indirect contagion related to portfolio overlaps across economic sectors. To measure contagion effects, we apply a comprehensive analytical framework brought to a unique and detailed data set about the European banking system.====Risks related to asset portfolio overlaps, or, in other words, asset commonality or similarity in business models, have been studied from a theoretical perspective and were assessed as a relevant source of potential contagion losses in the financial system. Risks related to portfolio overlaps are part of the so-called indirect channel of contagion.==== So far, the empirical analysis has been conducted either for some isolated sectors of the economy, e.g., Duarte and Eisenbach, 2021, Hałaj et al., 2015, Cont and Schaanning (2019), from a perspective of financial system in one country, e.g., Bardoscia et al. (2019), or using broad aggregates of exposures, e.g., Hałaj (2018). However, a methodical analysis of the importance of economic sectors, domestically and across borders, for the systemic risk that captures risk characteristics of interbank and broader asset markets, is missing.====To fill this gap, we borrow a recently developed methodology to measure the systemic importance of financial linkages and adapt it to study systemic risk of country sectors across the banking systems. In this way, we account for both the first-round losses, i.e., due to direct exposures, and second-round losses, i.e., due to indirect exposures via banks’ limited liabilities and other loss amplification effects. To this end, we apply the Network Valuation (NEVA) model of Barucca et al. (2020), which is a generalized model of contagion, to estimate losses caused by various distress scenarios directly affecting banks’ exposures and propagated in the interbank system. Moreover, we explain the mechanics of different contagion patterns based on the underlying matrix of bank exposures across countries and sectors. Within this framework, different patterns of contagion and different patterns of overlaps can be discerned. We apply our methodology to a unique supervisory data set covering 26 of the largest banks in the euro area along with their exposures to detailed sectors of the real economy (broken down at the level of one-digit NACE==== code) and to other financial institutions.====Our study is motivated by the fact that we observe a high degree of overlap in bank exposures. Measuring it by leverage overlap, a specific metric that in a straightforward way combines overlapping exposures among banks with the banks' capital buffers, we are able to perform a litmus test on the importance of the indirect channel of contagion transmission. Specifically, we rely on the concept of sectoral leverage overlap ratios that combine three elements: (i) they gauge the shock amplification potential of highly leveraged exposures, (ii) they account for direct contagion channels related to defaulting exposures, and (iii) they include indirect channels capturing asset revaluation and potentially fire sales. Leverage overlap represents a simplified and quick test for contagion risk. Other studies have proposed alternative composite systemic risk measures based on network data (see e.g., the Systemic Risk Index by Cont et al. (2013), the Systemic Probability Index by Hałaj and Kok (2013) or the Indirect Contagion Index by Cont and Schaanning (2019)). These other measures try to capture banks’ risk exposure stemming from interconnectedness in relation to the banks’ buffers against this risk, measured by their capital or counter-balancing capacity of liquid assets. However, the static nature of the measures limits their ability to explain the emergence of systemic risk, as captured instead by dynamic processes of contagion.====To empirically study the role of the structures of the financial linkages in the transmission of contagion, we introduce a range of fictitious allocations of exposures across countries and sectors, capturing a varying degree of diversification of exposures. In particular, we consider a quasi-domestic allocation in which banks tend to lend to firms and households in their own country and a diversified allocation in which banks diversify their exposures as much as they can. These two allocations satisfy constraints given by observed sizes of interbank lending and borrowing portfolios and the observed, country-specific sizes of sectors. In addition to these two fictitious allocations of exposures, we observe an empirical one derived from the supervisory data. In the introduced ensemble of market structures, we analyze the contagion based on some stylized shock scenarios across different dimensions: shock location, shock size, interbank recovery rate, asset prices volatility, and time, see Section 3.4 for more details. The importance of the comparison between these allocations is to provide policy insights into the potential impact of different market structures on the contagion risk. The relevance of market structures is, for instance, highlighted by Wagner (2011) and Acemoglu et al. (2015), which we are able to replicate in our detailed empirical study: a more modular financial system is more fragile than a more diversified financial system to small shocks, while it is more robust to big shocks. At the same time, the relevance of market conditions has been highlighted by Battiston et al. (2012); Roukny et al. (2013): even for intermediate shocks, the relation between diversification and risk is non-monotonic in the presence of loss amplification mechanism due to reactions of market participants. Similar results on the impact of diversification on financial stability are discussed in Bardoscia et al. (2017), Silva et al. (2018) and Stiglitz (2018).====Additionally, after performing analysis based on the stylized shock scenarios, which sheds light on the interplay between structures of exposures, shocks characteristics and contagion risk, we conduct a contagion analysis with a consistent macro-financial scenario designed for the EU-wide 2016 European Banking Authority (EBA) stress-test exercise.==== The results of the analysis are complementary to the first-round accounting losses calculated in the official EU-wide stress test. Notably, it shows the important role of the second-round effects in stress tests to adequately assess financial vulnerabilities. From a methodological perspective, our approach can be used to enhance the macroprudential assessment of bank stress tests conducted by the European Central Bank (see ECB (2016)).====Our main findings are the following:====The rest of the paper is structured as follows: in Section 2 we discuss the data set, in Section 3 we explain the methodology, in Section 4 we discuss the results, and in Section 5 we conclude the paper.",Interconnected banks and systemically important exposures,https://www.sciencedirect.com/science/article/pii/S0165188921002013,19 October 2021,2021,Research Article,155.0
"Lee Sangjun,Zhao Jinhua","Climate Change Research Team, Korea Energy Economics Institute, 405-11 Jongga-ro, Jung-gu, Ulsan 44543, Republic of Korea,Dyson School of Applied Economics and Management, Cornell University, 114 Warren Hall, Ithaca, NY 14853, United States","Received 7 January 2019, Revised 3 October 2021, Accepted 10 October 2021, Available online 17 October 2021, Version of Record 31 October 2021.",https://doi.org/10.1016/j.jedc.2021.104262,Cited by (4),"Global climate change will lead to increased frequency and severity of extreme weather events, on top of the gradual changes in temperature and precipitation. We develop a ","One of the major consequences of global climate change is the increased frequency and severity of extreme weather events, on top of gradual changes in temperature and precipitation (Zilberman et al., 2012; Hsiang and Jina, 2014; Weitzman, 2014; Kelly and Tan, 2015). As evidence is growing linking extreme weather events to climate change (Stott et al., 2004; Rahmstorf and Coumou, 2011; Trenberth, 2011, 2012; Field et al., 2012), there is increasing importance in studying adaptation strategies to extreme events. However, there are few structural economic models of climate change that investigates decision responses to extreme events vis-à-vis gradual changes (Mechler et al., 2010; Zilberman et al., 2012; Hsiang and Narita, 2012; Zhao, 2013). Traditional benefit-cost analysis based on expected net present values (ENPV) is often employed (e.g., Boyd and Ibarrarán 2008), but this approach fails to capture some important characteristics of extreme events (Zilberman et al., 2012). Specifically, the ENPV framework treats extreme events and gradual changes equally: both are expressed in terms of the expected net present values of their damages. Hence, incentives to adapt to extreme events and gradual changes are the same under this framework if both cause the same expected damages. This treatment goes against conventional wisdoms about adaptation processes, which suggest that adaptation may be stimulated by extreme events rather than changes in average weather conditions (Berrang-Ford et al., 2011; Füssel, 2007). Extreme events often act as a catalyst for adaptive actions, for example, new irrigation technologies are adopted only after severe droughts, dikes are constructed only after major flooding, etc.====Adaptation to climate change has several important and unique features. First, such decisions face high degrees of uncertainty and significant sunk costs. These factors call for dynamic decision frameworks such as the real options approach (Dixit and Pindyck, 1994). The importance of uncertainties is even more apparent when the effects of extreme events are considered as we tend to know less about extreme events than gradual changes. Second, the probability distribution of climate change's damages may have “fat tails,” in which case the traditional ENPV of the damages may “explode” (Weitzman, 2014). Extreme events can be main contributors to the fat tail distribution (Weitzman, 2009), and adaptation models should be able to accommodate differing degrees of tail thickness caused by extreme events relative to gradual changes. Third, extreme events such as major natural disasters have long-lasting negative impacts on economic growth (Li et al., 2019; Hsiang and Jina, 2014; Hornbeck, 2012). As such, they should be modeled as sticky changes in underlying economic processes rather than “one-shot” events with temporary damages.====In this paper, we develop and analyze a conceptual model of adaptation that incorporates the distinctive processes of both gradual changes and extreme events, identifies the effects of thick tails on adaptation, and captures the long-lasting damages of extreme events. We take the real options approach but model the uncertainties by a jump diffusion process to include both gradual changes and extreme events and to incorporate a wide range of tail behaviors. Specifically, we represent gradual climate impacts by a continuous Brownian motion process, and extreme events by a discontinuous Poisson jump process whose magnitude follows a hyper-exponential distribution. By varying key parameters of the hyper-exponential distribution, we can generate different “thickness” of the tail distribution of jump sizes and study the optimal adaptation decisions as the tail becomes thicker. We study “complete adaptation,” which takes the form of exiting from the current activity to an alternative with a return stream unaffected by climate change at all. We distinguish between the ==== and the ====: the former is represented by an adaptation threshold where adaptation occurs if and only if the return from the current enterprise falls below this threshold, while the latter equals the probability that the threshold is crossed and thus adaptation occurs. The distinction is important because the ranking of the effects of gradual changes versus extreme events based on the adaptation incentive tends to be opposite to that based on the probability of adaptation.====We study how the incentives and probabilities of adaptation are affected by four different types of climate change consequences: (i) gradual unfavorable changes in weather patterns, (ii) increased frequency of (negative) extreme events, (iii) increased magnitude of extreme events, and (iv) thicker-tailed size distribution of extreme events. The changes are calibrated so that they lead to the same expected losses to the current enterprise, so that they are equivalent in the traditional ENPV framework. We show that when decision makers are allowed to optimize dynamically and to learn, gradual changes and extreme events can lead to different adaptation incentives as well as different probabilities of adaptation even though they are ENPV-equivalent. Specifically, although gradual unfavorable changes tend to raise the adaptation incentive more than extreme events, it is the extreme events, especially increased magnitudes and tail thickness of the extreme events, that lead to higher probabilities of adaptation. Further, we show that as the tail of the return uncertainties becomes thicker, the distinction between the incentive and probability of adaptation becomes more pronounced, and thick tails of climate change's impacts will lead to higher probabilities of adaptation.====Our findings highlight the relative importance of extreme events versus gradual changes in driving adaptation decisions in the context of global climate change. Economics models on climate change, such as major integrated assessment models including DICE, have often failed to explicitly account for extreme events (Ackerman et al., 2010; Cai et al., 2016; Cai and Lontzek, 2019). In recent years, researchers and practitioners have increasingly realized the importance of extreme events in assessing the damages of climate change and in evaluating the adaptation strategies (Cai et al., 2016; Cai and Lontzek, 2019). For example, Weitzman (2009, 2014) posited in a series of papers that climate change uncertainties might follow fat-tailed distributions, where the probabilities of extreme events are higher than those in thin-tailed distributions (Conte and Kelly, 2021). Further, extreme events might be the major drivers of behavior such as the demand for disaster insurance (Conte and Kelly, 2021).====Nevertheless, the literature is lacking in explicitly and structurally comparing the effects of extreme versus gradual changes, so that there is insufficient knowledge about how important extreme events are relative to gradual changes. Our paper makes the case that extreme events are more important than gradual changes in driving adaptation decisions, that magnitudes of extreme events might be more important than their frequencies in driving adaptation, and that extreme events leading to heavy tails as in Weitzman (2014) are the most likely to bring about adaptation actions.====Our paper also contributes to the growing literature on adaptation that uses real options models to capture uncertainties (Zhao, 2013; Zilberman et al., 2012). The previous literature has studied a variety of practical adaptation problems, ranging from flood risk management (Woodward et al., 2011; Gersonius et al., 2013) to investments in costal defense (Linquiti and Vonortas, 2012), water resource management (Jeuland and Whittington, 2014; Sturm et al., 2017), land use (Regan et al., 2017) and agriculture (Hertzler, 2007; Sanderson et al., 2015). These studies usually take the dynamic programming approach (Dixit and Pindyck, 1994; Copeland and Antikarov, 2003), but it is difficult to use this approach to handle situations when both gradual changes and extreme events are present. The difficulty arises because extreme events in the real world often lead to non-Gaussian processes of the state variable. This makes the traditional dynamic programming approach, which relies on Ito's lemma and the smooth pasting principle, analytically intractable (Boyarchenko, 2004; Boyarchenko and Levendorskiĭ, 2007; Chen and Insley, 2012; Niu and Insley, 2016; Insley, 2017). We take advantage of recent advances in the finance and actuarial science literature that rely on the Wiener-Hopf factorization to solve real options models with Lévy processes (Boyarchenko, 2004; Boyarchenko and Levendorskiĭ, 2002a; Asmussen and Albrecher, 2010; Cai and Kou, 2011; Kou, 2002). This approach leads to intuitive analytical solutions and enables relatively straightforward numerical solutions. Despite its convenience, this approach has not been widely used to study environmental problems involving real options.====The paper is organized as follows. We set up the model in Section 2 and provide the analytical solutions in Section 3. We conduct a series of comparative dynamic analyses in Section 4 to compare the effects of gradual changes and extreme events, highlighting the distinction between the incentive for adaptation and probability of adaptation. We study the effects of heavy tails on adaptation in Section 5, and conclude in Section 6. The proofs of the propositions are given in Appendix 1.",Adaptation to climate change: Extreme events versus gradual changes,https://www.sciencedirect.com/science/article/pii/S0165188921001974,17 October 2021,2021,Research Article,156.0
"Garcia-Lazaro Aida,Mistak Jakub,Gulcin Ozkan F.","University of Bath, England, United Kingdom,King’s College London, King’s Business School, Bush House, 30 Aldwych, London WC2B 4BG, United Kingdom,University of Edinburgh, Scotland, United Kingdom","Received 6 March 2021, Revised 5 October 2021, Accepted 7 October 2021, Available online 12 October 2021, Version of Record 14 November 2021.",https://doi.org/10.1016/j.jedc.2021.104254,Cited by (0),We develop a multi-country ,"Disintegration of economic ties in the form of withdrawal from a major economic block has no historical precedence. The UK’s exit from the EU, therefore, constitutes a rare economic experiment. By ending the UK’s membership of the EU single market - with free movement of goods, services, people, and capital across borders - Brexit has reintroduced frictions to trade and migration flows between the UK and the EU from the 1st of January, 2021, following the completion of the long-awaited Brexit-deal - formally known as the Trade and Cooperation Agreement (TCA) - between the two sides.====It is commonly acknowledged that the appeal of the UK’s ability, as a non-EU country, to control the EU-UK immigration flows played a major role in the outcome of the Brexit referendum (see, for example, Portes, 2016). It was therefore widely expected that any post-Brexit arrangement would feature at least some restrictions on the free movement of workers across the UK-EU border. Indeed, the new UK immigration system that also came into effect on the 1st of January 2021, entails such restrictions in the form of a points-based structure and a salary threshold. As such, the new system is widely expected to bring about a fall in net immigration of low-skilled labour from the EU countries (see, for example, Rolfe et al., 2019).====Another key aspect of the post-Brexit arrangements is undoubtedly the nature of trade between the UK and the EU with the former now outside the EU’s single market. Although the risk of tariffs is averted by the provisions of the TCA, the UK-EU trade is now subject to non-tariff barriers (NTBs). NTBs derive from border controls; rules-of-origin checks; divergence in regulations about product standards and safety, workers’ rights and environmental protection and are seen as an important source of losses for the UK economy beyond Brexit (see, for example, Ottaviano et al., 2014, Sampson, 2017, Van Reenen, 2016). Conversely, as is consistently argued by the proponents of the UK’s withdrawal from the EU, Brexit allows the UK to forge its own trade agreements with the third countries, which she was unable to do while operating in a customs union with the EU.====While the resumption of tariff and quota-free trade in goods across the UK-EU border, as secured by the TCA, has been hailed as a major achievement for both sides, the omission of the services sector from the agreement is noted as a significant risk factor. This is particularly the case for the UK economy 80 percent of which is made up of the services sector, a vast proportion of which are financial services (see, for example, Hall, 2021). As the UK has left the single market with no provision for the services, the British financial sector runs the risk of losing the so-called ‘passporting rights’ that has allowed British financial entities to provide financial services in the European Economic Area (EEA) (see, for example, Armour, 2017, Erken et al., 2018, Ramiah et al., 2017). The resulting frictions to bilateral financial services trade is likely to increase the cost of providing financial services across the UK-EU border. TCA also changes the structure of the UK-EU trade in non-financial services due to the absence of mutual recognition of qualifications, for example, in engineering, architecture and legal services, with important implications for the cost of their cross-border provision.====Naturally, Brexit has led to an explosion of interest in its potential consequences, generating a burgeoning new literature on how withdrawal from the EU would impact the UK economy.==== There is now a substantial body of work on the economic consequences of alternative forms of Brexit. Evaluation of potential post-Brexit scenarios has been carried out by using dynamic trade models (Dhingra et al., 2016, Dhingra et al., 2017, Sampson, 2017, Van Reenen, 2016); macroeconometric models (Berthou et al., 2020, Erken et al., 2018, Hantzsche et al., 2019, Kierzenkowski et al., 2016); and dynamic general equilibrium models (Broadbent et al., 2019, McGrattan and Waddle, 2020, Pisani and Vergara-Caffarelli, 2018, Steinberg, 2019). A separate and large body of work has explored the effects of Brexit utilising empirical methods including expectations augmented vector-autoregressions (Born et al., 2019); structural VAR (Driffield and Karoglou, 2019); gravity models (Campos and Timini, 2019); and event studies (Oehler et al., 2017). Findings from both the simulation exercises based on general equilibrium models and empirical studies suggest that Brexit is likely to exert a significant impact, particularly on the UK economy, and the size of that impact varies with the scale of frictions in the UK-EU economic arrangements.====The completion of the Brexit-deal, TCA, on the 24th of December, 2020 allows us to move beyond alternative scenarios and carry out a comprehensive analysis of the agreed form of the future UK-EU economic relationship. We argue that, in addition to the need to fully address the effects of the new arrangements for the services sector and of NTBs in trading of goods, as set out earlier, any proper analysis of the Brexit-deal will need to explicitly consider substantial supply chain networks between the UK and the EU. Indeed, global value chains (GVC) - production being broken into stages that are completed in different countries - now make up a significant share of international trade (70% in 2020) (see, for example, Cattaneo et al., 2010, Miroudot, Rouzet, Spinelli, 2013). When countries are interconnected in production to such an extent, any frictions to cross-border trade has the potential to disrupt supply chains, and hence production processes in multiple countries. Therefore, even in the absence of tariffs and quotas, new frictions underlying the NTBs on the UK-EU trade are likely to have significant consequences for both the UK and the EU as well as their major trading partners, working through the substantial supply chains in place.====Following from the arguments presented above, this paper develops a multi-country dynamic general equilibrium model incorporating (i) migration flows across borders; (ii) explicit supply chains across sectors and across countries; (iii) explicit consideration of services sector; and (iv) a separate banking sector enabling exploration of financial transmission channel through which the new frictions impact the economy.====In doing so, we build upon Gali and Monacelli (2005) by incorporating supply chains along the lines of Altinoglu (2021), Imura (2019) and Steinberg (2020); by including labour flows similar to Canova and Ravn (2000); and by integrating a banking sector along the lines of Gertler and Kiyotaki (2010) and Gertler and Karadi (2011) to carry out a comprehensive analysis of the economic disintegration between the UK and the EU. To the best of our knowledge, our paper is the first to utilise a dynamic multi-country framework combining network effects in trade in both goods and services; migration between countries; and a separate banking sector in studying Brexit. As such, our model also provides a suitable setting for analysis of other recent events, such as the rewriting of NAFTA; tariffs on the US-China trade flows; and the slowdown in trade integration since the global financial crisis in 2009.====We derive our quantitative results by calibrating our multi-country model economy to the UK, the EU, and the third countries - with explicit configuration of the UK’s trade links with advanced and emerging economies, separately. We consider permanent NTBs in cross-border trade in both goods and services between the UK and the EU; potential permanent reduction in tariff barriers in trade between UK and third countries that move the model economy to a new steady-state; and stochastic migration shocks.====We have a set of interesting findings that are of significant policy relevance. First, we find that the existence of supply chain networks plays a significant role in determining the size of fluctuations even in the absence of tariffs, raising the cost of Brexit significantly. For example, the contraction in the UK’s output in the face of NTBs to trade in goods and services is nearly three times as large when supply chains are present, irrespective of the size of the shock. More specifically, we find that the contraction in the UK’s output varies between 0.75% and 0.24% with and without supply chains, respectively.====Second, incorporating the significant share of services in the UK’s trade, using input-output data, we find that frictions to trade in services reduce the UK’s output by 0.23%, smaller than in the goods sector due to the latter’s greater reliance on inputs from other countries.====Third, the reduction in immigration from the EU is a source of further fluctuations for the UK, although the size of this effect is considerably smaller than that from trade frictions. Moreover, we find that while it is possible to replace reduced inflow of workers from the EU with those from non-EU countries, quantitatively the benefit from the latter falls short of the loss from the former.====Our multi-country setting also enables us to quantify the implications of potential trade liberalisation between the UK and the non-EU countries, pointing to favourable effects. It is clear, however, that the size of this favourable effect is significantly smaller than the losses from frictions to the UK-EU trade, even in the limiting case of lifting all trade barriers against the third countries. We also show that the scope of gains from trade liberalisation with emerging economies is greater given the already low trade barriers against the advanced economies.====Our characterisation of the labour market featuring heterogenous households enables us to also provide a welfare analysis, with important insights into the distributional implications of Brexit. Importantly, we find that losses from Brexit are not shared equally and fall disproportionately on low-skilled households. This is due to the greater drop in the output of the goods sector which employs a greater share of low-skilled labour.====The remainder of this paper is organised as follows. Section 2 presents a detailed description of a benchmark multi-country model featuring homogeneous households and its calibration. Section 3 introduces three extensions to the baseline model: heterogeneous households, supply chain networks, and banks; and presents the calibration of the extended model. Our policy experiments are presented in Section 4, covering frictions in migration and trade in goods and services between the UK and the EU; and trade agreements between the UK and its non-EU trading partners, with a comprehensive examination of impacts for all four sets of countries. In Section 5 we discuss the features of our model and provide a welfare analysis. We also present a rich set of robustness and sensitivity checks in Section 6 and show that our main findings prevail across a wide range of features and parameter values. Finally, Section 7 concludes.","Supply chain networks, trade and the Brexit deal: a general equilibrium analysis",https://www.sciencedirect.com/science/article/pii/S0165188921001895,12 October 2021,2021,Research Article,157.0
Li Kai,"Macquarie Business School Macquarie University, NSW 2109, Australia,Institute of Financial Studies Southwestern University of Finance and Economics, Chengdu, China","Received 15 March 2021, Revised 23 September 2021, Accepted 28 September 2021, Available online 12 October 2021, Version of Record 19 October 2021.",https://doi.org/10.1016/j.jedc.2021.104253,Cited by (3),"I study a Lucas ====. However, from the point of view of an outside econometrician, the market price of risk relates negatively to sentiment. This, together with the subjective momentum, causes returns on momentum strategies to be a ==== of sentiment, leading to a downside risk of momentum. I find empirical evidence consistent with model predictions.","Momentum is one of the most prominent empirical regularities in financial markets and is amongst the few anomalies that are still persistent after their publication (e.g., Hou, Xue, Zhang, 2020, McLean, Pontiff, 2016, Schwert, 2003).==== The pervasive explanations of momentum are behavioural, and these theories predict that momentum tends to be stronger during high investor sentiment periods (e.g., Antoniou et al., 2013). However, distinct from other strategies, momentum strategies are found to generate negatively-skewed returns (Harvey and Siddique, 2000) and tend to suffer infrequent crashes (Daniel and Moskowitz, 2016). This feature makes momentum even more puzzling. Can investor sentiment explain this downside risk of momentum?====In this paper, I develop a Lucas exchange economy with many trees by following (Menzly et al., 2004) and a representative agent who forms extrapolative beliefs on market returns (referred to as market-wide sentiment). First, I show that the investor sentiment can generate cross-sectional momentum. Further, I show that the expected profits produced from momentum strategies can be a concave function of sentiment, negatively skewing the returns to momentum strategies and leading to a downside risk. Momentum returns do not line up with the exposure to the downside risk. I also find evidence consistent with my predictions: market-wide sentiment can be a common source of variations in expected returns, and the concave pattern exists in various momentum strategies and is also reflected in both autocovariance and lead-lag relations of assets.====The agent in my economy has a CRRA utility. As documented in recent survey evidence of return extrapolation, the agent does not set prices based on fundamentals but forms her beliefs based on past market returns.==== She becomes overoptimistic (overpessimistic) after experiencing good (bad) recent market performance. Put differently, the agent believes that there is time series momentum in market returns. When the agent forms beliefs about the market returns, she also simultaneously forms beliefs about individual asset returns by extrapolating from past asset returns, leading to spillovers of sentiment from the market to individual assets.==== As a result, she believes that assets with good (bad) recent performance would continue overperforming (underperforming) in the near future. That is, she also perceives momentum in the cross section of asset returns. Furthermore, high sentiment causes the agent to expect high returns of assets and hence to expect high cross-sectional momentum profits.====Due to sentiment, the subjective probability measure of the agent differs from the objective measure of an outside econometrician. An asset’s objective risk premium (the risk premium under the objective measure) equals the product of its subjective risk premium (the risk premium under the subjective measure) and the ratio of the objective market price of risk to the subjective market price of risk. It further means that objective expected momentum profits equal the subjective expected momentum profits multiplied by the ratio of risk prices. The objective risk price is, on average, positive, since it converges to the subjective risk price in the long run as sentiment converges to its long-run mean.==== This means that cross-sectional momentum strategies are also profitable under the objective measure.====Furthermore, with high sentiment, the agent expects a high consumption growth rate since beliefs about consumption growth and market returns determine each other. An econometrician knows that the consumption growth rate is, however, IID and that positive future returns should be less probable than the agent thinks. As a result, although the subjective risk price is a constant, the objective risk price actually depends negatively on sentiment. This negative relation, together with the positive relation between subjective momentum profits and sentiment, leads objective momentum profits to be a concave function of sentiment, which negatively skews momentum returns.====The downside risk of momentum in my model is not in line with the positive average return of momentum strategies. Indeed, the agent who believes that momentum returns are compensation for risk exposure does not realize the downside risk of momentum. When sentiment is high, she overestimates the risk price (relative to an econometrician). In this case, she expects high momentum returns, which, however, are actually less profitable. Her incorrect estimation of risk causes momentum returns to not line up with the exposure to the objective downside risk.====The closed-form solutions I derive in this paper effectively isolate time series return predictability from cross-sectional return predictability. The former is generated solely by market sentiment, while the latter is due to cross-sectional dispersion of assets’ exposure to sentiment, which results from the interaction of sentiment and asset-level cash-flow news. I show that sentiment alone cannot generate cross-sectional return predictability, highlighting the role of asset-level news (in driving cross-sectional variations). Due to the different sources of predictability, my model can simultaneously produce momentum in the cross section of asset returns and reversal in aggregate market returns. These results are consistent with Li and Liu (2021) who demonstrate that an agent’s biased return expectations directly specify the discount rates and almost uniquely imply the pricing kernel, but do not put any restriction on her dividend expectations. Our setup effectively disentangles the effects of the beliefs about discount rates and cash flows.====I find evidence consistent with model predictions. First, I show that market-wide sentiment is priced in the cross section. A long-short zero investment strategy that sorts stocks based on their exposures to a market-wide sentiment index, which measures investors’ biased beliefs of market return extrapolation as documented in Greenwood and Shleifer (2014), generates a monthly return of 0.30%, and the result is robust to controlling for the momentum factor and the Fama and French (2015) five factors. I also find that the concave patterns exist in many momentum strategies, including both price momentum and earnings momentum, as well as in the cross-serial correlations of asset returns.====Cochrane et al. (2008) and Martin (2013) study asset pricing with multiple Lucas trees, where dividend growth of each asset is independent and identically distributed over time. This setup does not possess stationary distributions of dividend shares. Menzly et al. (2004) and Santos, Veronesi, 2006, Santos, Veronesi, 2010 model dividend shares as stationary processes, avoiding one tree dominating the economy in the long run. The latter approach is used by Ehling and Heyerdahl-Larsen (2017) to study the dynamics of return correlations. In this paper, I also adopt this approach that allows me to isolate the effect of market-wide beliefs on the cross section of asset returns and to disentangle the effects of market-wide sentiment and asset-level sentiment.====Many studies find that market-wide sentiment has significant effects on the cross section of stock returns and that cross-sectional anomalies tend to be stronger during high sentiment periods due to cross-sectional variation in sentimental demand shocks or arbitrage constraints, e.g., Baker and Wurgler (2006) and Stambaugh, Yu, Yuan, 2012, Stambaugh, Yu, Yuan, 2015, among others. My results confirm the generally positive relation between momentum profits and market-wide sentiment. Further, they show a more significant pattern of a concave relation between momentum and sentiment, which is due to the interaction of subjective momentum and the difference in objective and subjective risk prices. I verify the concave pattern for different momentum strategies and return decompositions.====In existing behavioural theories, e.g., Barberis et al. (1998), Daniel et al. (1998), and Hong and Stein (1999), momentum is due to investors’ underreaction or delayed overreaction to asset-specific news. Momentum arises in my model because the agent’s (biased) belief formation rule prescribes that she believes there is momentum. My explanation is consistent with the widespread survey evidence on investors’ expectations about asset returns. In my model, the agent sets prices by discounting future cash flows, and the resultant equilibrium accounts for all future effects of a current change in consumption or dividend. In this sense, there is no under/overreaction (to news) in such an economy and because of this, momentum does not reverse in the long run. This is consistent with Conrad and Yavuz (2017) who show that the stocks that contribute to momentum profits do not experience significant long-run reversals.==== In addition, momentum in many models is measured by positive return autocorrelations of a single asset, a time series property. In this paper, I explicitly examine cross-sectional momentum profitability, and my model isolates time series and cross-sectional return predictability.====My paper is broadly related to the literature on nonlinear relations between historical prices and future stock returns, e.g, Lo et al. (2000) and Murray et al. (2020). These studies use advanced numerical algorithms to extract historical information and document a significantly nonlinear role of past returns in predicting future returns. The nonlinear feedback between past prices and current price through population evolution is also widely exploited in the evolutionary asset pricing literature, e.g., He and Li (2012) and Chiarella et al. (2013). My general equilibrium model shows that expectation bias naturally has a twofold effect on return dynamics, which generates nonlinearity.====Belief formation plays a key role in asset pricing. Rational expectations models assume that the joint distribution of all the variables of interest is the same as what agents believe (along with Bayesian updating), e.g., Lucas (1972). This assumption, which implies that agents are correct on average, effectively pins down beliefs. Recently, a growing body of literature has begun to examine alternative belief distortions to address inconsistencies between implications of rational expectations and the data, as well as some survey or experimental evidence of expectation bias, e.g., Sargent (1999), Chakraborty and Evans (2008), Fuster et al. (2010), Hirshleifer et al. (2015), Li and Liu (2021), and Bordalo et al. (2019). When agents’ beliefs deviate from rational expectations, there will be a “sentiment premium” that accounts for the discrepancy between the subjective and objective premia. The sentiment premium is an important departure from the rational expectations models where the subjective and objective premia are the same; however, its asset pricing implications have not been well explored. My parsimonious model shows that the sentiment premium can lead to a concave transformation of momentum profits, generating negatively-skewed momentum.====The remainder of the paper is organized as follows. Section 2 presents the economic setup and characterizes equilibrium. Section 3 provides some basic analysis of equilibrium, and Section 4 studies momentum. Section 5 conducts empirical analysis, and Section 6 concludes. The Appendix provides the proofs.",Nonlinear effect of sentiment on momentum,https://www.sciencedirect.com/science/article/pii/S0165188921001883,12 October 2021,2021,Research Article,158.0
"Fanelli Sebastián,Gonzalez-Eiras Martín","CEMFI, Casado del Alisal 5, 28014 Madrid, Spain,University of Copenhagen, Oster Farimagsgade 5, 1353 Copenhagen K, Denmark","Received 27 July 2021, Accepted 28 September 2021, Available online 11 October 2021, Version of Record 6 November 2021.",https://doi.org/10.1016/j.jedc.2021.104252,Cited by (0),A financial crisis creates substantial ==== in the U.S. during the ,"Financial crises often are preceded by asset price booms and increased borrowing, typically against appreciating assets. Once the boom reverses and asset prices collapse, large financial losses are realized. If these losses are concentrated in the productive sector of the economy, a deep and protracted decline in economic activity follows - a balance-sheet recession (Kiyotaki, Moore, 1997, Koo, 2003).====Such a balance-sheet recession would not occur if the productive sector could write state-contigent contracts that insulates it from volatility in asset prices (Di Tella, 2017, Krishnamurthy, 2003), i.e. if it has insurance against aggregate shocks. While in reality these contracts may be unavailable, agents can still obtain partial insurance by defaulting and renegotiating debts ex post. Indeed, recent empirical studies have shown that bankrupcy law and the ease of renegotiating outstanding debt have been key determinants of the recovery in the United States during the Great Recession (Agarwal, Amromin, Ben-David, Chomsisengphet, Piskorski, Seru, 2017, Mian, Sufi, Trebbi, 2015).====In this paper, we study the resolution of financial crises in an environment where agents write non-contingent contracts that are subject to default and renegotiation ex post. Our analysis emphasizes how institutions and technological factors determine the magnitude of default costs and the distribution of financial losses among agents. Among others, these factors include bargaining power, the cost of default, and the informational frictions between creditors and debtors.====To illustrate the new implications of these partial insurance mechanisms, we depart as little as possible from what is arguably a canonical model of balance sheet recessions: the Kiyotaki and Moore (1997) model - henceforth KM. The model features two kinds of agents: entrepreneurs, who are the most productive, and financiers, who are the least productive.==== Entrepreneurs may borrow from financiers, but face a collateral constraint: They can only borrow up to the value of capital next period. In the steady-state entrepreneurs are fully levered (i.e. the collateral constraint is binding).====We study the response of the economy against an unforeseen shock, which could be either a technology shock to entrepreneurs (as in KM) or a preference shock to financiers, which can be interpreted as a financial shock. As in KM, these shocks depress asset prices, leaving entrepreneurs underwater. Unlike KM, who assume that entrepreneurs honor their debts ex post, we allow for default and bargaining between creditors and debtors. More precisely, we assume that entrepreneurs can default, keeping their output but paying a cost to do so. Financiers can repossess the collateralized asset at a cost. To avoid these losses, entrepreneurs and financiers may bargain, with financiers offering a haircut on the entrepreneurs’ outstanding debt.====We show four main results. First, the threat of default and the possibility of renegotiation only matter if shocks are large enough. When shocks are small, the threat of default is not credible. Entrepreneurs honor existing debts, which depresses the demand for capital and leads to a collapse in asset prices. The response of the economy is, thus, the same as in the original KM model. By contrast, when shocks are large, the threat of default is credible, which triggers a renegotiation. In this case, entrepreneurs manage to extract haircuts from financiers, cutting their financial losses. This cushions the reduction in capital demand and dampens the decrease in asset prices.====Second, institutional and technological factors that shape the debt-restructuring process determine the extent of amplification of macroeconomic shocks. More precisely, we show that there is more amplification when financiers’ repossession costs are low, default costs are high, or entrepreneurs have little bargaining power. Furthermore, larger shocks are required to trigger renegotiation. In addition, when agents renegotiate, bargained haircuts are smaller, leading to a larger drop in entrepreneurs’ capital holdings and asset prices, slowing down the recovery.====Third, renegotiation is socially desirable. Not only does it avoid potentially deadweight losses of default, but it also accelerates the recovery from the financial crisis by increasing entrepreneurs’ net worth. This suggests that policies that make renegotiation easier, such as the Home Affordable Modification Program (Agarwal et al., 2017), are particularly valuable in this environment.====Fourth, asymmetric information may exacerbate the crisis by inducing equilibrium default, but it also accelerates the recovery by shielding entrepreneurs’ net worth. We make this point in an extension of our baseline model where entrepreneurs’ default costs is private information, i.e. financiers only know the distribution of default costs in the population. When shocks are large they propose a haircut that is accepted only by entrepreneurs with relatively high default costs. The remaining entrepreneurs default. To the extent that default is a deadweight loss, this exacerbates the crisis. However, since financiers must offer every entrepreneur the same haircut, entrepreneurs with high default costs get a better deal than they would in an economy with perfect information. As a result, entrepreneurs’ capital holdings fall less under asymmetric information, accelerating the recovery after the shock.====We also characterize how the distribution of unobservable default costs affect macroeconomic outcomes. If the set of defaulting agents is fixed, and default costs go up, then entrepreneurs extract a smaller haircut and the recovery slows down. On the other hand, financiers may also find it optimal to affect the extensive margin, i.e. who defaults. Perhaps surprisingly, we construct examples where higher ==== default costs may translate into lower ==== default costs and more sizeable haircuts.====An important assumption of the model is that the ex-post resolution of debt crises does not affect the ex-ante behavior of agents. We believe this is a reasonable approximation of behavior in credit markets for rare events such as financial crises. For example, in the credit boom before the Great Recession, lenders paid little attention to borrowers’ repayment capacity. Mian et al. (2015) show that in the late 1990s and early 2000s lenders did not differentiate lending based on states’ foreclosure requirements. In commercial real estate markets debt was often issued with minimum covenants, and commercial real estate had low risk premia relative to other assets. These facts point to lenders assigning a very low probability to states of the world in which foreclosure requirements and covenants would be important. Furthermore, the paucity of renegotiations suggests the presence of widespread information asymmetries between borrowers and lenders (Adelino et al., 2013).====Our work contributes to the theoretical literature on the balance sheet channel, going back to the seminal work of Bernanke et al. (1999), Carlstrom and Fuerst (1997), and Kiyotaki and Moore (1997) and, more recently, the work of Brunnermeier and Sannikov (2014) and He and Krishnamurthy (2013), among others. This strand of work stresses how the concentration of aggregate risk in one sector of the economy leads to significant amplification of shocks via their effect on balance sheets. A critique of this channel is that it would disappear if agents were allowed to write contracts contingent on the aggregate state of the economy (Krishnamurthy, 2003 and Di Tella, 2017). This motivated papers to explain why insurance contracts may not be available (e.g. Cooley, Marimon, Quadrini, 2004, Krishnamurthy, 2003) or why agents may optimally decide to become exposed to aggregate risk (e.g. Asriyan, 2020, Di Tella, 2017). By contrast, our paper does not seek to explain balance sheets from an ex-ante perspective. Rather, we ask how the possibility of default and bargaining ex post affect the depth and posterior recovery of a financial crisis. We derive new results characterizing the evolution of the macroeconomy ex post as a function of the size of the shock, the institutional and technological background, and the observability of default costs.====Our model is also related to the literature on the limited enforceability of debt contracts, allowing for strategic default. Cooley et al. (2004) assume lending can take the form of long term state-contingent debt contracts, borrowers can divert capital, and default is costly. They solve for the optimal dynamic contract that is self-enforceable and find that the equilibrium features amplification. Jermann and Quadrini (2012) also allow borrowers to default and derive borrowing constraints by assuming that lenders can recover the collateral with an exogenous probability (otherwise, recovery is zero). They interpret this time-varying probability as “financial shocks” and find that they can explain a large share of observed dynamics of real and financial variables. These two papers abstract from the effect of (endogenous) asset prices on borrowing constraints, while in our model, as in KM, it is precisely this variable that drives results. Furthermore, we also allow for financial shocks as we consider a temporary increase in the discount factor of lenders (and thus in the equilibrium interest rate).====The paper is organized as follows. Section 2 presents the basic framework, which introduces default costs and renegotiation into KM’s model. Section 3 develops an extension with asymmetric information about default costs. Section 4 discusses how our model can be used to interpret existing empirical findings in the context of real estate markets in the United States during the Great Recession, as well as to rationalize recent changes introduced into the U.S. Bankruptcy Code as a response to the COVID-19 crisis. Section 5 concludes. Appendix A contains all proofs and detailed derivations. Appendix B describes the parametrization and calibration used to create the figures.",Resolution of financial crises,https://www.sciencedirect.com/science/article/pii/S0165188921001871,11 October 2021,2021,Research Article,159.0
"Parra-Alvarez Juan Carlos,Polattimur Hamza,Posch Olaf","Department of Economics and Business Economics, Aarhus University; CREATES; Danish Finance Institute, Fuglesangs Allé 4, Aarhus V 8210, Denmark,Faculty of Business, Economics and Social Sciences, Universität Hamburg, Von-Melle-Park 5, Hamburg 20146, Germany,Faculty of Business, Economics and Social Sciences, Universität Hamburg; CREATES, Von-Melle-Park 5, Hamburg 20146, Germany","Received 3 September 2020, Revised 5 June 2021, Accepted 26 September 2021, Available online 5 October 2021, Version of Record 9 November 2021.",https://doi.org/10.1016/j.jedc.2021.104248,Cited by (1),.,"There is a consensus among economists that uncertainty affects the consumption-saving decision of individuals. Neglecting the effects of risk in macroeconomics and finance often generates substantial pricing errors. Hence, recent research is concerned with the ability of local approximations of nonlinear stochastic macroeconomic models to account for risk, with a particular focus on perturbation methods originally introduced in economics by Judd and Guu (1993). Although perturbation-based methods only provide local precision around a particular point, usually the model’s deterministic steady state, many authors suggest that they can generate high levels of accuracy, comparable to that delivered by global approximation techniques, as the order of the approximation is increased (see Aruoba, Fernández-Villaverde, Rubio-Ramírez, 2006, Caldara, Fernández-Villaverde, Rubio-Ramírez, Yao, 2012, Judd, 1998, Parra-Alvarez, 2018). In many applications, however, we are interested in the first-order perturbation and the resulting linear approximation of the equilibrium conditions. A linear structure not only provides analytical insights and helps to understand key features of the model, but also facilitates its estimation, e.g., by means of the Kalman filter (see Fernández-Villaverde, Rubio-Ramírez, 2007, Harvey, 2006, Harvey, Stock, 1985, Singer, 1998).====A known limitation of the first-order perturbation around the deterministic steady state is that the approximate solution of discrete-time models typically exhibits certainty equivalence (see Simon, 1956, Theil, 1957). In other words, the first-order approximation to the solution of stochastic economic models with forward-looking agents is identical to the solution of the same model under perfect foresight. The direct implication is that the solution becomes invariant to higher-order moments of the underlying shocks. Therefore, this paper addresses the following questions. What are the costs of neglecting the effects of risk in linear approximations? Put differently, what would be the benefits of using a linear approximation that is not certainty equivalent? In particular, by how much could such an approximation reduce the errors that one makes when not accounting for risk? How can these errors be interpreted in an economically meaningful way?====Certainty equivalence prevails in the classical linear-quadratic optimal control problem, popularized in economics by Kydland and Prescott (1982) and Anderson et al. (1996). In the early 1950s the introduction of certainty equivalent stochastic control problems with quadratic utility and linear constraints aimed at providing a practical solution for decision problems under uncertainty. Even today, if risk is negligible for the research question at hand, certainty equivalent solutions are still useful. In this case, one may conclude that “certainty equivalence is a virtue” (see Kimball, 1990a). Conversely, when there is a reason to believe that the effects of risk are important, one notices that “certainty equivalence is a vice”. Thus, if risk matters, breaking certainty equivalence is desired in order to account for the effects of risk. As discussed in Fernández-Villaverde et al. (2016), the approximated solution of the model under certainty equivalence (i) makes it difficult to talk about the welfare effects of uncertainty; (ii) cannot generate any risk premia for assets; and (iii) prevents analyzing the consequences of changes in volatility.====To break the property of certainty equivalence in the class of perturbation methods, economists have restored to the computation of higher-order Taylor expansions, the underlying apparatus behind any perturbation method, which translate into nonlinear approximations of the model’s solution. Originally proposed in Judd and Guu (1993), higher-order approximations became popular with the work of Schmitt-Grohe and Uribe (2004) for second-order approximations, and that of Andreasen (2012) and Ruge-Murcia (2012) for third-order approximations. More recently, Levintal (2017) extended the perturbation package to include fifth-order approximations. However, the use of high-order approximations for medium-scale macroeconomic models (i) could be computationally expensive, (ii) often results in explosive solutions, and (iii) requires computationally demanding nonlinear estimation methods, such as the particle filter, for the estimation of the model’s structural parameters.====In contrast to stochastic discrete-time models, certainty equivalence breaks in the first-order approximation when time is assumed to be continuous (see Gaspar, Judd, 1997, Judd, 1996). This property, which allows us to account for risk in a linear world, is the product of two complementary results. First, while in discrete time the approximation is built inside the system of expectational equations that collects the equilibrium conditions of the economy, in continuous time we may use Itô’s lemma to eliminate the expectation operator prior to the construction of the approximation. The resulting ==== system of equations, although deterministic, will capture the effects of uncertainty by including information on the sensitivity to risk of the yet unknown solution (see Chang, 2009). Second, as shown in Fleming (1971), who provides the mathematical foundations of perturbation methods for continuous-time stochastic optimal control problems, regular perturbation theory produces asymptotically valid approximations of the unknown solution when the variance of the shocks is used as perturbation parameter. As discussed in Jin and Judd (2002), this choice of the perturbation parameter follows basic economic intuition, whereby optimal behavior of agents is affected by the variance in the economy. This is in contrast to discrete-time models where the appropriate perturbation parameter is shown to be the standard deviation (cf. Judd, 1998, Jin, Judd, 2002, Fernández-Villaverde, Rubio-Ramírez, Schorfheide, 2016). Combining these two results, the linear approximation to the model’s solution, which results from a first-order perturbation around the deterministic steady state, will exhibit a constant correction term that depends on the variance of the shocks that drive the dynamics of the economy.====In this paper, we revisit the ability of a first-order perturbation to capture the effects of risk. First, we derive a first-order perturbation solution for a general class of dynamic, continuous-time, rational expectations models, thereby formalizing the framework in Gaspar and Judd (1997) and Parra-Alvarez (2018). We show analytically that, as opposed to discrete-time models, the first derivative of the policy function with respect to the perturbation parameter is different from zero at the deterministic steady state implying that the resulting linear approximation is risk-sensitive, i.e., it breaks the certainty-equivalence property. More specifically, the first-order perturbation corrects for risk through an additional constant term that incorporates information on the slope and curvature of the optimal policy functions at the deterministic steady state. Second, we explore how the effects of uncertainty are internalized by this perturbation approach using as a benchmark an RBC model with habit formation and capital adjustment costs ==== Jermann (1998). By calibrating the parameters of the model to values that are standard in the literature, we compare, along different dimensions, the first-order certainty equivalent (CE) and the risk-sensitive first-order approximations obtained from perturbation to a second-order nonlinear approximation and a global solution obtained by collocation methods. We show that each of the approximations converges to different long-run equilibria in the absence of shocks. While the first-order CE converges to the deterministic steady state, the risk-adjusted solutions converge to their respective approximated risky steady states. This property is reflected in the policy and impulse response functions.====We find that the risk effects captured by the first-order approximation in continuous time are economically significant. We assess the asset pricing implications of the approximations using a partial differential equation approach rather than the standard simulation framework used in discrete time. When relying on the linear CE solution, the pricing errors made on a three month zero-coupon bond are about 1 dollar for each 100 dollar spent at the deterministic steady state. The risk-adjustment of the first-order perturbation approximation leads to errors of about 10 cents for each 100 dollar spent, reducing pricing errors by about 90%. Hence, we conclude that the risk-sensitive first-order perturbation provides a sensible approximation to the effects of risk in continuous-time models.====Our work relates to that of Collard, Juillard, 2001, Coeurdacier et al., 2011, de Groot (2013), Meyer-Gohde (2015) and Lopez et al. (2018), who compute first-order approximations around the model’s risky steady state instead of the deterministic steady state in order to break certainty equivalence in discrete-time models. Collard, Juillard, 2001 consider a bias reduction procedure to compute the approximation around the risky steady state; Coeurdacier et al., 2011, whose approach is generalized by de Groot (2013), approximate the risky steady state based on the second-order solution. Meyer-Gohde (2015) constructs a risk-sensitive linear approximation using policy functions resulting from higher-order perturbations. Lopez et al. (2018) differ from the previous studies as they consider lognormal affine approximations, often used in macro-finance, which are shown to be a special case of a first-order perturbation around the risky steady state. We argue that it is possible to account for risk in an economically meaningful way using ==== first-order (====) perturbations around the ==== steady state when time evolves continuously.====The rest of the paper is organized as follows. In Section 2, we introduce our model and define the equilibrium conditions used in the perturbation method to approximate the solution. Section 3 summarizes the perturbation approach, presents the main theoretical contribution of the paper, and revisits the certainty equivalence property of linear models. Section 4 derives the pricing implications of the approximated solution and introduces a pricing error measure that can be used to evaluate the accuracy of the approximation. Section 5 discusses the main results by comparing policy functions, impulse-response functions, and pricing errors for the different approximations. Finally, Section 6 concludes.",Risk matters: Breaking certainty equivalence in linear approximations,https://www.sciencedirect.com/science/article/pii/S0165188921001834,5 October 2021,2021,Research Article,160.0
Kanazawa Nobuyuki,"Lecturer, Faculty of Economics, Soka University, 1-236, Tangi-machi, Hachioji-shi, Tokyo, 192-8577, Japan","Received 13 April 2020, Revised 31 January 2021, Accepted 23 September 2021, Available online 2 October 2021, Version of Record 19 October 2021.",https://doi.org/10.1016/j.jedc.2021.104247,Cited by (4),"This study contributes empirical evidence of the macroeconomic impacts of public investment. I extract public investment news shocks from the excess returns of narrowly defined road pavement firms and use them as an instrument for future public investment spending. Using Japanese data for the period between 1980 and 2014, I find that when the news shock is followed by a persistent increase in public investment and a weak real interest rate response, the public investment spending has a significant stimulative effect over the medium term. The estimated cumulative multiplier is as large as 6.10, four years after the shock. However, the cumulative multiplier eventually falls below 1 after 10 years. I also report a substantial temporary improvement in aggregate labor productivity associated with a rise in public investment spending.","What is the public investment multiplier, defined as the percentage increase in GDP caused by an increase in public investment spending by one percent of GDP? Understanding the macroeconomic impact of public investment is critical, given that public investment typically constitutes a large part of countercyclical fiscal packages. For example, approximately ==== of non-transfer spending in the American Recovery and Reinvestment Act (ARRA) of 2009 was allocated to public infrastructure investment. Yet, it remains an open question whether the public investment has a larger or smaller multiplier. Leduc and Wilson (2013) and Ilzetzki et al. (2013) have provided some evidence that public ==== has larger multipliers, while Boehm (2019) has suggested that public ==== has larger multipliers. This study contributes empirical evidence of the dynamic macroeconomic effect of public investment by exploiting the news shocks extracted from the excess returns of road pavement firms in Japan.====Estimating public investment multipliers is challenging because of the long implementation lag associated with public investment projects. As Leeper et al. (2013) emphasize, the failure to control for public expectations about future government spending can lead to incorrect inferences. To address this concern, I use the excess returns of Japan’s road pavement firms to identify surprise components of the changes in public expectations about future public investment, following Fisher and Peters (2010). If the profitability of the selected road pavement firms depends heavily on public road investments, the shocks to their excess returns can be interpreted as surprise news about future public road investment spending. I regress the excess return on a number of contemporaneous and lagged economic and financial variables. The residuals from the regression are my measure of extracted news shocks.==== To verify that the extracted news measure is orthogonal to the current state of the economy, I conduct a series of robustness checks.====I employ the local projection-IV method using the extracted news shocks as an instrumental variable to estimate the public investment multiplier. My identification strategy relies on two crucial assumptions: the instrumental variable 1) captures news about future public investment (relevance condition) and 2) affects output only through public investment spending (exclusion restriction). The relevance condition is directly testable. I show that the extracted news shocks predict future public investment at longer horizons in a statistically significant manner. I deal with the exclusion restriction by regressing the excess return on current and lagged macroeconomic/financial variables, as described above.====I find that the cumulative public investment multiplier is approximately 1.98 a year after the shock and as large as 6.10 after four years. The cumulative multipliers then slowly decline and reach 0.60, 10 years after the shock. The estimated multipliers in the middle horizons between the second and eighth years after the shock exceed 2 and are considerably larger than conventional estimates of fiscal multipliers. However, these estimates are in line with Leduc and Wilson (2013), who estimated cumulative public investment multipliers of 1.4 on impact and 6.6 at peak using state-level US data.==== The cumulative multipliers fall below 1, which is consistent with the conventional estimates of the fiscal multipliers, 10 years after the shock. The estimated results are robust to different specifications. In addition, I find that public investment crowds in consumption and investment and that both multipliers in the middle horizons are larger than the multipliers in previous studies, which used different government spending shocks.====Finally, I empirically explore factors that could explain the large public investment multipliers. As suggested by Ramey (2011), the size of the fiscal multiplier depends on 1) the tax rate response, 2) real interest rate response, 3) persistence of government spending, and 4) the type of government spending. Investigating whether any of these factors contribute to the large public investment multiplier, I conclude that the combination of a weak real interest rate response and high persistence of public investment spending play key roles. Put differently, when these conditions are met, public investment can be an effective tool for stimulating the aggregate economy over the medium term. Additionally, I find a substantial temporary improvement in aggregate labor productivity over the medium term associated with a rise in public investment.",Public investment multipliers: Evidence from stock returns of the road pavement industry in Japan,https://www.sciencedirect.com/science/article/pii/S0165188921001822,2 October 2021,2021,Research Article,161.0
Gu Jiajia,"Institute for Economic and Social Research Jinan University, 113A Zhonghui Building, 601 Huangpu West Avenue, Guangzhou 510630, China","Received 17 February 2021, Revised 14 September 2021, Accepted 16 September 2021, Available online 27 September 2021, Version of Record 7 October 2021.",https://doi.org/10.1016/j.jedc.2021.104238,Cited by (0),This paper explores how financial intermediation costs affect ,"Own-account workers represent a large fraction of the labour force in low-income countries.==== These workers are self-employed and have no paid employees. They differ from wage workers in that they work for themselves and invest at least some of their wealth in their own business. They differ from employers in that they have no paid employees and typically work with small amounts of capital.====Cross-country evidence reveals a negative relationship between the share of own-account workers and per capita income but a positive relationship between the share of employers and per capita income. This implies that, as countries develop, individuals tend to shift away from own-account work toward wage work; few become employers (Gindling and Newhouse, 2014). Empirical evidence also suggests that own-account workers have a lower income than both wage workers and entrepreneurs (Gindling, Mossaad, Newhouse, 2016, Gindling, Newhouse, 2014), and are more likely to share the characteristics of wage workers than those of employers (De Mel et al., 2010). From the labour market perspective, own-account workers withdraw from this market, whereas employers actively participate in it by creating jobs, representing genuine entrepreneurship (Earle and Sakova, 2000). These facts suggest that own-account workers and employers should be treated separately.====This paper examines how the cost of financial intermediation affects occupational decisions, emphasising a novel channel that operates through the return on saving. In my model, the cost of intermediation drives a gap between the cost of borrowing (lending interest rate) and the return on saving (deposit interest rate).====Using net interest margin as a proxy for this gap, cross-country evidence shows that a larger net interest margin is associated with a higher share of own-account workers. Micro-evidence from China also points to the relevance of the interest rate gap with respect to occupational choices.====To interpret these facts, I build a three-occupation model with costly financial intermediation. In this model, agents differ in their worker ability, managerial ability, and wealth. They choose to become wage workers, own-account workers or employers. Wage workers earn wage income and entrust their wealth to financial intermediaries. Both own-account workers and employers invest capital in their businesses, but only employers hire paid employees.====The cost of financial intermediation is modelled as a fraction of resources lost during the intermediation process. In the case of perfect intermediation, this cost is zero. In this scenario, the return on savings equals the cost of borrowing, wealth is irrelevant, and agents sort into occupations based on ability only. However, as the cost of intermediation increases, the equilibrium wage rate and return on saving decrease, and the cost of borrowing rises. The lower wage rate does not distort occupational choices because two agents with the same abilities would still choose the same occupations. The interest rate gap, however, leads agents with the same abilities to choose different occupations depending on their level of wealth.====A higher cost of borrowing resembles a borrowing constraint in that both decrease borrowing. Agents who borrow to become own-account workers or employers are more likely to scale down their business size or even switch to an occupation that requires less or no capital.====A lower return on saving, which is the novel channel explored in this paper, encourages agents to manage their own wealth, thereby avoiding the low returns offered by financial intermediaries. As the return on saving declines, wage workers who hold some wealth are more likely to become own-account workers or employers. In addition, existing own-account workers and employers increase the amount of wealth they invest in their own businesses, and some wealthy own-account workers may even become employers.====Qualitatively, the changes in occupational shares are not clear when the cost of intermediation increases, since the high borrowing cost prevents agents with low wealth from becoming self-employed, and the lower return on saving encourages agents with high wealth to switch to self-employment. To determine whether and to what extent variation in costs can explain observed cross-country patterns, I build and calibrate a quantitative model and find the intermediation cost for each country. Comparing the model predictions with the data, I conclude that the model explains approximately 26% of the decline observed in the data. This decline in the model is driven by wage workers switching to become own-account workers.====To assess the quantitative relevance of the return on saving, I set the return on saving under costly intermediation as equal to its value under perfect intermediation, and calculate the share of agents willing to becoming own-account workers. This counterfactual exercise suggests that approximately 40% of the change in the share of own-account workers is due to the decrease in the return on saving.====The majority of papers on occupational choice allow a binary choice between being a wage worker and becoming an employer, as in Lucas (1978); however, studies as early as Banerjee and Newman (1993) have considered a three-occupation framework, with one option being self-employment without employees.====Gollin (2008), which focuses on TFP, represents the earliest attempt to understand cross-country differences in occupational shares. In his paper, although TFP does not directly affect occupational shares, a higher TFP leads to an increase in steady-state capital stock. This increase in capital stock, together with an elasticity of substitution between labour and capital that is below one, increases the wage rate and reduces the share of self-employment. This paper suggests that capital accumulation is key to understanding occupation shares. It is natural, then, to think that ownership of capital matters in a world with imperfect financial markets.====In this vein, a large body of literature examines how financial development affects occupational choices (see Buera et al., 2015 for a survey). Here, financial development usually takes the form of a credit constraint, and one interest rate is applied to both borrowers and savers. Allub and Erosa (2019) and Feng and Ren (2021) consider models of this type and incorporate own-account workers. In both papers, the authors use quantitative exercises to predict that tighter constraints will increase the share of own-account workers.====Although a higher intermediation cost and tighter credit constraints both increase self-employment, the mechanisms underlying these effects are somewhat different. The effects mediated by the interest rate gap and return on savings are specific to models that include an intermediation cost.==== Whereas I model a simple proportional cost, papers such as Khan (2001), Greenwood et al. (2010) and Greenwood et al. (2013) show that costly state verification can generate a gap between the the cost of borrowing and return on savings.====A recent paper by Cavalcanti et al. (2021) incorporates both credit constraints and interest rate spreads in an otherwise standard framework. These interest rate spreads differ across firms, and arise from intermediation costs and financial intermediaries’ market power. The authors calibrate their model to match firm-level data in Brazil and conclude that credit spreads have larger effects on development aggregates than do typical collateral constraints.====Other papers also find evidence consistent with the presence of mechanisms other than credit constraints. Banerjee (2013) and Buera et al. (2016) find that the take-up rate for micro-credit is not always high. Blattman et al. (2019) suggest the possibility that, in developing countries, owning a business is simply a way to avoid a negative real return on cash. Of course, this does not exclude the potential effects of forces other than financial intermediation costs. The calibration exercise in this paper finds that intermediation costs explain some, but not all, of the cross country differences in occupation shares and the credit-to-GDP ratio. The intermediation cost channel proposed in this paper can be seen as a complement to the financial development literature.====Intermediation costs and credit constraints have differing policy implications. Credit constraints emphasise the need to identify and alleviate the barriers that prevent high-ability entrepreneurs from borrowing. In the model presented here, the appropriate policy intervention would improve the efficiency of financial intermediation and thus reduce the gap between the cost of borrowing and return on saving. In addition to making it easier for firms to borrow, policy should also seek to give households better opportunities to save.====Literature beyond the field of financial development also addresses own-account workers. Cuberes and Teignier (2016) studies the gender gap in a three-occupation framework. Poschke (2019) examine the relationship between frictions in the labour market and the share of own-account workers. Feng et al. (2018) study occupational choices in a search and matching framework, with an emphasis on unemployment.====The remainder of this paper is organised as follows: Evidence from cross-country and Chinese data is presented in Section 2. Section 3 presents a static model and illustrates its mechanisms. Section 4 extends the static model to a dynamic setting, calibrates it, and compares its predictions with the observed cross-country data. Section 5 concludes.",Financial intermediation and occupational choice,https://www.sciencedirect.com/science/article/pii/S0165188921001731,27 September 2021,2021,Research Article,162.0
Pavlov Oscar,"Tasmanian School of Business and Economics, University of Tasmania, Australia,Centre for Applied Macroeconomic Analysis, Australian National University, Australia","Received 9 March 2021, Revised 23 July 2021, Accepted 17 September 2021, Available online 25 September 2021, Version of Record 10 October 2021.",https://doi.org/10.1016/j.jedc.2021.104239,Cited by (0),"Recent literature has addressed how product creation amplifies economic fluctuations via the love of variety. Yet, the empirical evidence on variety effects is sparse. The current paper demonstrates that decreasing returns in the variety-level production technology, which leads to increasing marginal costs, similarly amplify business cycles. Product scope expansions reduce marginal costs and firms have an incentive to produce multiple products even if the variety effects are entirely absent. The efficiency gains from adjusting product scopes makes the economy more susceptible to sunspot equilibria. The indeterminate model is estimated via ==== methods and data favors the multi-product structure with animal spirits explaining a significant fraction of U.S. business cycles.","An important line of research has demonstrated how product creation via the entry of firms can amplify shocks and be a source of sunspot equilibria that leads to fluctuations driven by self-fulfilling beliefs. The two central mechanisms that produce these results are countercyclical markups and the love of variety.==== More recently, Minniti and Turino (2013) and Pavlov and Weder (2017) extend these entry models by utilizing variety effects as an incentive for firms to produce multiple products. Yet, the empirical evidence on the size of these effects is sparse - casting doubt on the ability of this mechanism to explain the contribution of product creation on the business cycle.==== The current paper addresses this issue by laying out a model where intra-firm product creation amplifies business cycles and makes the economy more susceptible to sunspot equilibria even in the complete absence of the love of variety.====Specifically, it investigates the role of increasing marginal costs in a general equilibrium model with endogenous entry and oligopolistic multi-product firms. When the production technology of intermediate good firms has decreasing returns, marginal costs increase with output per variety. That is, the short-run marginal cost schedule (a.k.a. supply curve) is upwardly sloping. This gives firms an incentive to produce multiple products even in the absence of variety effects. The efficiency gains of adjusting product scopes amplifies economic fluctuations and creates sunspot equilibria at more realistic situations, which are not attainable with mono-product firms. Hence, technological decreasing returns and increasing marginal costs provide a novel mechanism for product creation within firms and for generating indeterminacy.==== This is in stark contrast to Benhabib and Farmer (1994) and Farmer and Guo (1994), where indeterminacy is a result of technological increasing returns for mono-product firms in the absence of entry. Finally, the model is estimated by Bayesian methods and the results support recent findings that belief shocks (i.e. animal spirits, sunspots) explain a significant fraction of U.S. business cycles.====The way indeterminacy arises is most easily understood in terms of the equilibrium wage-hours locus. Product creation and countercyclical markups generate an endogenous efficiency wedge that makes this locus upwardly sloping. If the locus is steeper than the labor supply curve, then sunspots can act as self-fulfilling expectation shocks. For example, if people become optimistic about the future path of income, then the wealth effect shifts the labor supply curve upwards, raising employment and output - thereby confirming the initial belief. More precisely, when the labor supply curve shifts up due to optimistic expectations, the higher demand for output and profit opportunities induce firm entry. Greater competition pushes markups downwards and causes firms to expand output. Since marginal costs would increase with production, firms choose to expand their product scopes rather than ramp up the production of existing varieties. The fall in output per variety due to the cannibalization effect (new varieties reducing the demand for existing varieties) then leads to falling marginal costs.==== Together, the efficiency gains of product creation and falling markups shift out the labor demand curve far enough to allow the initial belief about higher income to become self-fulfilling.====On the theoretical side, the paper is most closely related to Minniti and Turino (2013) and Pavlov and Weder (2017). The former investigates the role of the product scope in magnifying fundamental shocks, while the latter shows how the multi-product structure makes the economy more susceptible to sunspot equilibria. Feenstra and Ma (2009) use a similar model in the context of international trade. The variety effect in their papers implies that consumers place a higher value on a bundle of goods that each firm produces when it consists of more differentiated varieties. As a result, final goods are produced more efficiently when there are more intermediate good varieties and product creation generates an efficiency wedge similar to the increasing marginal costs mechanism described earlier. Without the love of variety, the amplification mechanism is eliminated, consumers do not value product differentiation, and introducing a new product only leads to additional costs: firms then become single-good producers. In contrast, the current paper does not utilize variety effects. Instead, increasing marginal costs provide an incentive for firms to expand their product scopes that similarly amplify business cycles.====The empirical approach and results are closely connected to Pavlov and Weder (2017) and Dai et al. (2020), with the latter employing financial frictions to generate indeterminacy. Both use Bayesian methods to estimate indeterminate models and their results on the importance of self-fulfilling beliefs parallel the findings of the current paper. In a related work, Lubik (2016) estimates a real business cycle model with productive externalities and finds evidence for close to constant returns to scale in aggregate U.S. data.====Several recent works have highlighted the importance of multi-product firms. Bernard et al. (2010) find that about 90 percent of total sales in the manufacturing sector are made by multi-product firms. Broda and Weinstein (2010) report that over 90 percent of product creation and destruction occurs within firms and that the contribution of product scope adjustments within firms is at least as important to the evolution of aggregate output as firm entry and exit. Hottman et al. (2016) reach similar conclusions and also find that the largest firms generate variable markups and significant cannibalization effects. Similarly, Guo’s (2020) empirical and theoretical findings show that product scopes are procyclical and that product adjustments within firms dominate the firm entry margin.====With regard to evidence on increasing marginal costs, Hottman et al. (2016) estimate a model of heterogenous multi-product firms and their elasticity of marginal cost with respect to output is consistent with upwardly sloping marginal cost curves. Earlier empirical works by Bils (1987) and Shea (1993) also provide supporting evidence. The importance of increasing marginal costs is often emphasized in international trade literature. For example, Ahn and McQuoid (2017) provide a good overview and find that they are predominant for exporting firms. Findings on U.S. returns to scale by Burnside (1996), Basu and Fernald (1997), Ahmad et al. (2019), and Gao and Kehrig (2020) indicate that while they vary across industries, the average is close to constant or slightly decreasing returns to scale.====The paper proceeds as follows. Section 2 outlines the model and Section 3 analyzes the local dynamics. In Section 4, the model is estimated and its ability to fit the data is compared to a mono-product model with increasing returns to scale and to a multi-product economy with the love of variety. Section 5 concludes.",Multi-product firms and increasing marginal costs,https://www.sciencedirect.com/science/article/pii/S0165188921001743,25 September 2021,2021,Research Article,163.0
"Chang Yoosoon,Maih Junior,Tan Fei","Department of Economics, Indiana University, USA,Norges Bank and BI Norwegian Business School, Norway,Department of Economics, Chaifetz School of Business, Saint Louis University, USA; Center for Economic Behavior and Decision-Making, Zhejiang University of Finance and Economics, China","Received 19 March 2021, Revised 27 July 2021, Accepted 30 August 2021, Available online 10 September 2021, Version of Record 27 September 2021.",https://doi.org/10.1016/j.jedc.2021.104235,Cited by (3),"We examine ==== is allowed to switch endogenously between two regimes, hawkish and dovish, depending on whether a latent regime factor crosses a threshold level. Endogeneity stems from the historical impacts of structural shocks driving the economy on the regime factor. By estimating our ==== model using the U.S. data, we quantify the endogenous feedback from each structural shock to the regime factor to understand the sources of the observed policy shifts. This new channel sheds new light on the interaction between policy changes and measured economic behavior.","In time series analysis, there is a long tradition in modeling structural change as the outcome of a regime switching process [Hamilton, 1988, Hamilton, 1989]. By introducing an unobserved discrete-state Markov chain governing the regime in place, this class of models affords a tractable framework for the empirical analysis of time-varying dynamics that is endemic to many economic and financial phenomena.====Despite the popularity of the Markov switching approach, its dynamics are ultimately governed by a regime switching process that is exogenous. This is especially unsatisfactory if we seek to truly understand the nature of policymaking and its impact on economic phenomena. As argued in Chang et al. (2017), the presence of endogeneity in regime switching is indeed ubiquitous==== and, if ignored, may yield substantial biases and significantly deteriorate the precision in model parameter estimates. It follows that a more desirable approach to modeling occasional but recurrent regime shifts would admit some form of endogenous feedback from the behavior of underlying economic fundamentals to the regime generating process [Diebold et al. (1994), Chib and Dueker (2004), Kim, 2004, Kim, 2009, Kim et al. (2008), Bazzi et al. (2014), Kang (2014), Kalliovirta et al. (2015), Kim and Kim (2018), among others].====The purpose of this paper is to introduce a threshold-type endogenous regime switching framework into dynamic stochastic general equilibrium (DSGE) models. Over the past 20 years, DSGE models have become a useful tool for quantitative macroeconomic analysis in both academia and policymaking institutions. One particularly important development is the effort to incorporate the possibility of exogenous regime shifts (e.g., changes in monetary policy) into the model specification. By making policy regime shifts endogenous, our framework allows for a greater scope for understanding the complex interaction between policy changes and measured economic behavior.====Following Chang et al. (2017), an essential feature of our model is that the monetary policy regime alternates between two regimes, hawkish and dovish, depending on whether an autoregressive latent factor crosses some threshold level. In our approach, two sources of random innovations jointly drive the latent factor and hence the policy regime change: (i) the internal innovations that represent the fundamental shocks inside the model; (ii) an external innovation that captures all other shocks left outside the model. The relative importance of the former source determines the degree of endogeneity in regime changes. The autoregressive nature of the latent factor, on the other hand, makes such endogenous effects long-lasting. Most importantly, regime switching of this type renders the transition probabilities time-varying in that they are all functions of the model’s fundamentals. In the special case where regime shifts are purely driven by the external innovation, our model becomes observationally equivalent to one with conventional Markov switching.====The key contribution of this paper is to provide a framework within which we study the origins of monetary policy shifts. Ever since the seminal work of Clarida et al. (2000), modeling the time-varying behavior of monetary policy has remained an active research agenda for macroeconomists. While regime switching has emerged as a promising approach to modeling the time variation in monetary policy, scant attention in the literature has been paid to the macroeconomic origins that give rise to monetary policy shifts over time. Our paper takes a first step toward filling in this important gap—the aim here is not to identify policy switches that standard approaches would miss but rather to shed light into the reasons why the switching occurs.====Due to the substantial improvement in model fit, a multitude of empirical studies have proposed to estimate regime-switching DSGE models [Schorfheide (2005), Liu et al. (2011), Bi, Traum, 2012, Bi, Traum, 2014, Bianchi (2013), Davig and Doh (2014), Bianchi and Ilut (2017), Bianchi and Melosi (2017), Best and Hur (2019), among others]. We complement the recent literature on likelihood-based estimation of DSGE models with exogenous Markov switching by making regime changes endogenous. At the core of our analysis is the endogenous feedback effect of underlying structural shocks on the regime generating process. As a result, economic agents update their beliefs each period about future regimes conditional on the realizations of shocks disturbing the economy. For pedagogical purposes, Section 2 employs a simple model adopted from Chang et al. (2018b) to endogenize regime switching in monetary policy, which admits analytical characterizations of the mechanism at work. Section 3 extends the simple model to a prototypical new Keynesian DSGE model, and derives its state space form that can be analyzed with our endogenous-switching Kalman filter. We find that price markup shocks play an important role in triggering the historical regime changes in the postwar U.S. monetary policy. To the best of our knowledge, modeling and quantifying such endogenous feedback channel are novel in the literature.====An important precursor to our study is Davig and Leeper (2006a), who applied a projection method to solve and calibrate a new Keynesian model where monetary policy rule changes whenever its target variables (e.g., inflation and output gap) cross some thresholds.==== More recently, Guerrieri, Iacoviello, 2015, Guerrieri, Iacoviello, 2017 developed piece-wise linear solution toolkit and likelihood-based estimation method for DSGE models subject to an occasionally binding constraint (e.g., the zero lower bound on nominal interest rates). In their setup, each state of the constraint—slack or binding—is handled as one of two distinct regimes under the same model. Like these studies, one may argue that it is more natural to assign the immediate triggers of regime switch to the state variables rather than the structural shocks. In this regard, our regime switching is indeed endogenous with respect to shocks. However, linking the regime factor directly to shocks is technically appealing because the dynamics of all state variables are ultimately driven by a small number of structural shocks. Moreover, the ultimate goal of macroeconomic policy is to stabilize the economy by mitigating the impacts of various shocks. A prominent example is the 1970s oil crisis when the U.S. experienced severe petroleum shortages with elevated prices, and this oil price shock serves the dual role of adverse supply disturbance and potential trigger of monetary policy to a more aggressive regime in the 1980s. We therefore view the structural shocks that generate aggregate fluctuations as the macroeconomic origins of regime shifts, and establish a novel feedback channel by which they contribute to regime switching. Our analytical and empirical examples illustrate how the underlying structural shocks impact agents’ expectations formation and monetary policy regimes through this endogenous feedback channel.",Origins of monetary policy shifts: A New approach to regime switching in DSGE models,https://www.sciencedirect.com/science/article/pii/S0165188921001706,10 September 2021,2021,Research Article,164.0
"Andrade Philippe,Galí Jordi,Le Bihan Hervé,Matheron Julien","Federal Reserve Bank of Boston,CREI, UPF and BSE,Banque de France","Available online 10 September 2021, Version of Record 27 October 2021.",https://doi.org/10.1016/j.jedc.2021.104207,Cited by (5),"We address this question using an estimated New Keynesian ====, adjusting the monetary strategy requires considering alternative instruments or policy rules, such as committing to make-up for recent, below-target inflation realizations.","The ECB launched a review of its monetary policy strategy in January 2020. As a motivation for the strategy review, the ECB stated that “declining trend growth, on the back of slowing productivity and an aging population, as well as the legacy of the financial crisis, have driven interest rates down, reducing the scope for the ECB and other central banks to ease monetary policy by conventional instruments in the face of adverse cyclical developments” (ECB, 2020).====The view that interest rates will remain structurally lower than what they used to be is consistent with a recent but sizable literature that has documented a permanent—or, at least very persistent—decline in the “natural” rate of interest in advanced economies, including the euro area (Brand, Mazelis, 2019, Del Negro, Giannone, Giannoni, Tambalotti, 2018, Holston, Laubach, Williams, 2017). The on-going COVID-19 crisis could reinforce these downward pressures on the natural interest rate as agents revise upward their views on the fundamental economic risks they face, inducing larger precautionary savings (Kozlowski et al., 2020). A structurally lower real interest rate matters for monetary policy as, everything else being constant, it will cause the nominal interest rate to hit its effective lower bound (ELB) more frequently, hampering the ability of monetary policy to stabilize the economy, and bringing about more frequent (and potentially protracted) episodes of recessions and below-target inflation.====Central banks can contemplate three main reactions to this new environment. The first option is to keep the same strategy, while at the same time including permanently in their toolbox the unconventional policies adopted since the Great Recession, on the grounds that they can alleviate the ELB constraint and succeed in stabilizing the economy (Bernanke, 2020, Coenen, Montes-Galdón, Smets, 2020). The second option is to raise the inflation rate that the central bank deems to be consistent with the objective of price stability. A higher inflation target would compensate for the decline in the steady-state nominal interest rate induced by the lower real natural rate of interest, and hence contain the increase in the probability of hitting the ELB and maintain the “policy space” (Ball, 2014, Blanchard, Dell’Ariccia, Mauro, 2010). The third option is to change their reaction function. In particular the monetary authority can attempt to go (further) into negative interest rate policies and to lower its effective lower bound (Rogoff, 2017). Another modification of the reaction function is to adopt policies that promise to make up for the inflation lost during ELB episodes so that, thanks to the induced higher inflation expectations, the real interest rate declines despite the constraint on the nominal rate (Bernanke et al., 2019). A particular form of such make-up strategy is the Average Inflation Targeting (AIT) strategy that the Federal Reserve announced in late August 2020 (Mertens and Williams, 2019a).====The present paper contributes to this debate by asking two questions. First, to what extent does a lower steady-state real interest rate (====) call for a change in the optimal inflation target (====) if the central bank keeps its policy rule unchanged? Second, to what extent can a change in the policy rule be an alternative to increasing the inflation target? To address these questions, we conduct a quantitative welfare-based analysis which relies on an estimated structural macroeconomic model of the euro area. Our main findings can be summarized as follows: (i) Not changing the monetary policy strategy is suboptimal; (ii) a 1 percentage point decrease in ==== from its pre-2008 level calls for an increase in the the inflation target of roughly a 0.8 percentage point when the policy rule is unchanged; and (iii) a change in the policy rule can be an alternative to increasing the target if the commitment to making up for inflation lost during ELB episodes is strong and credible enough.====Our results are obtained from extensive simulations of a New Keynesian DSGE model. The model is estimated for the euro area over the 1985Q1-2008Q3 sample, a period preceding the Great Recession, the euro area sovereign crisis, and the Covid-19 crisis that triggered a protracted period of zero and negative interest rates in the euro area. This is intended to capture the “Great Moderation” period which we use as a benchmark for comparison with the “new normal” characterized by, inter alia, a lower natural rate of interest. The framework is similar to that in Andrade et al. (2019b), which contains a similar analysis based on U.S. data. Among other features, it assumes: (i) price stickiness and imperfect indexation of prices to non-zero trend inflation, (ii) wage stickiness and imperfect indexation of wages to both inflation and technical progress, and (iii) a lower bound constraint on the nominal interest rate. The first two features imply the presence of potentially substantial costs associated with non-zero wage or price inflation. The third feature warrants a strictly positive average inflation rate, in order to mitigate the incidence and adverse effects of the ELB.====According to our baseline simulations, the optimal inflation target, conditional on the “pre-crisis” estimate of ==== percent, was around 1.4 percent for the euro area (in annual terms). An important feature of this analysis is to allow for an ELB equal to ==== percent, which reflects the recent euro-area experience. Alternatively, if one postulates a strict zero lower bound on the nominal interest rate, as was arguably internalized by the ECB when it conducted its strategy review in 2003, the optimal inflation target is at ====, a value very much consistent with the “below but close to 2 percent” inflation aim adopted by the ECB at that time. Targeting an optimal inflation level helps keeping the frequency of hitting the ELB at levels lower than 5% in the pre-crisis regime. This relatively low frequency of ELB episodes also results from the relatively small volatility of the shocks in the pre-crisis sample, as it does not include the large shocks associated with the Great Recession and the euro-area sovereign crisis episodes.====Our simulations show that, starting from levels around this baseline, a one percentage point drop of ==== dramatically increases the probability of hitting the ELB, if the monetary authority keeps its inflation target unchanged.==== This makes an unchanged strategy suboptimal, even if our set-up already takes on board some degree of “low for long” monetary policy reaction when the ELB is binding.====When considering the policy option of increasing the inflation objective while keeping the reaction function unchanged, we obtain that, in response to a 1 percentage point drop in ====, it is optimal for the central bank to increase the inflation target by about 0.8 percentage point, that is to roughly 2.2 percent. This optimal reaction does not fully offset the increase in the probability to be at the ELB that, ceteris paribus, the drop in ==== implies. Indeed, the benefit of providing a better hedge against hitting the ELB, comes at the cost of a higher steady-state inflation which, because of imperfect indexation, entails permanent price distorsions and output loss. The above assessment of the “post-crisis” optimal inflation rate is arguably conservative as (i) we consider a 1 percentage point decrease in ==== for illustrative purposes, which stands on the conservative side in view of the existing empirical estimates of the decline in ==== and (ii) it does not take into account the possibility of a structurally larger volatility of shocks. When considering a 2 percentage point decline in ====, an amount well within the range of estimates in the literature (see for instance Constâncio, 2016), the optimal inflation rate then becomes close to 3.2 percent.====We also compute changes in the optimal inflation target under alternative scenarios. First, we assume that fiscal authorities in the euro-area can automatically launch a coordinated and aggressive fiscal emergency package when the economy is hit by a sequence of shocks leading to substantially negative cumulated output gaps. Everything else being constant, under that scenario, the baseline optimal inflation target drops to as low as.4 percent. As emphasized in, among others, Christiano et al. (2011), fiscal policy is a very effective way to escape a trap. As a result, the possibility of launching such an emergency fiscal package mitigates the frequency with which the ELB binds. This reduces the benefits of having a positive inflation target for mildly positive values of ====. However, under that set-up, it is still the case that a 1 percentage point drop in ==== from its pre-crisis value calls for a 0.8 percentage point increase in the optimal inflation target.====Second, we consider allowing for the possibility for policy rates to go even further in negative territory and set the effective lower bound to ==== percent instead of ==== percent. Under this scenario, the optimal inflation target stands at 1.1 percent for the baseline ====, and should be increased to about 2 percent in reaction to a 1 percentage point drop in ====.====Third, we consider changes in the monetary policy rule. We illustrate that an alternative to increasing the inflation target is to adopt a commitment to keep interest rate “lower for longer” at the end of the liquidity trap so as to make-up for past inflation lost at the ELB. This can be sufficient to maintain the optimal inflation target unchanged in reaction to a 1 percentage point decrease in ====. Switching from an inflation targeting strategy to an average inflation targeting (AIT) strategy would also allow to maintain the optimal inflation target unchanged in reaction to a 1 percentage point drop in ==== if the window considered is as long as 8 years. Finally, a limit case is that of Price Level Targeting (PLT), for which the optimal inflation target turns out to be lower than in the baseline, even after allowing for a lower ====.====Our paper is related to the contributions quantifying the optimal inflation rate in New Keynesian (NK) models with an ELB constraint (Billi, 2011, Coibion, Gorodnichenko, Wieland, 2012, Schmitt-Grohé, Uribe, 2010).==== In contrast to these works, and as in Andrade et al. (2019b), our focus is on the impact of changes in ==== on the optimal inflation target. In addition, while these papers study the US, we focus on the euro area, and in particular, we allow for specific features such as the availability of negative interest rate policies in the ECB monetary policy toolbox.====Our work also contributes to the literature assessing how monetary policy should adjust to a lower ==== environment. Eggertsson and Woodford (2003) and Adam and Billi (2006) showed that monetary authorities should adopt rules by which they commit to temporarily exceed their inflation target when the economy reaches the ELB. Reifschneider (2016), Kiley (2019), and Chung et al. (2019) for the US economy, and Coenen et al. (2020) for the euro area, advocate more frequent recourse to the non-conventional monetary policies that were implemented by the Fed and the ECB. Reifschneider and Williams (2000), Bernanke (2017), Kiley and Roberts (2017) and Bernanke et al. (2019) examine the benefits of switching to a price-level targeting framework. Mertens, Williams, 2019, Mertens, Williams, 2019, Arias et al. (2020); Budianto et al. (2020), and Hebden et al. (2020) study average inflation targeting.====Our baseline set-up does not include explicitly forward guidance or quantitative easing policies that can be used at the ELB. However, we consider that the central bank is committed to a simple interest rate rule which features some history dependence, implying that interest rates are being kept lower for longer after ELB episodes. As we document, this rule delivers outcomes that are comparable to what Coenen et al. (2020) obtain for the euro area under the assumption that the ECB can conduct various form of forward guidance possibly combined with asset purchase programmes. We also study how the (====) relation is modified when the central bank commits more strongly to a strategy that will “make-up” for the inflation shortfall at the ELB, by either increasing the inertia of the baseline interest rule or by opting for either an AIT or a PLT strategy. In addition, we consider the possibilities of either lowering the policy rate even further below zero or launching a coordinated emergency fiscal package in case of large recessions, while maintaining the policy rule unchanged.====Overall, with respect to our earlier contribution Andrade et al. (2019b) this work differs not only by considering the euro area and some of its specificities, but also by exploring some additional policy options (AIT, negative interest ELB) that the central bank can contemplate to address the low ==== environment, as well as the possibility of an aggressive countercyclical fiscal policy in response to persistent recessions.====The remaining of the paper is organized as follows. Section 2 presents the New Keynesian model we use. Section 3 describes how the model is estimated and simulated, as well as how welfare is approximated, and presents estimation results. Section 4 presents some properties of the model, based on the baseline estimates and some counterfactuals. Section 5 is devoted to analyzing by how much ==== should change in reaction to a decline in ====. Section 6 analyses changes in strategy that could be substitutes to increasing the inflation target. Section 7 provides some concluding remarks.",Should the ECB adjust its strategy in the face of a lower ,https://www.sciencedirect.com/science/article/pii/S0165188921001421,10 September 2021,2021,Research Article,165.0
"Proaño Christian R.,Lojak Benjamin","Otto-Friedrich-Universität Bamberg, Germany,Centre for Applied Macroeconomic Analysis, Australian National University, Australia","Received 14 September 2020, Revised 17 August 2021, Accepted 5 September 2021, Available online 10 September 2021, Version of Record 29 September 2021.",https://doi.org/10.1016/j.jedc.2021.104236,Cited by (1),In this paper we study the implementation of a state-dependent ,"The definition of the inflation target is a key element of the monetary policy strategy of most central banks. While this is a truism in normal times, it is decisively more relevant in periods where the zero lower bound (ZLB) on nominal interest rates is binding and the expectations channel of monetary policy plays a more important role, and the change of the target itself may be a tool used by central banks.====In his seminal contribution Krugman (1998) already raised the question (in the context of Japan’s long-lasting economic slump) of whether a temporary increase of the central bank’s inflation target would be advisable in situations where nominal interest rates are constrained by the ZLB. The ultra-low inflation and nominal interest rates period following the Global Financial Crisis has also led researchers such as Blanchard et al. (2010), Ball (2013), and De Grauwe and Ji (2019) also to call for a higher inflation target than the 2% pursued by most central banks. There are many theoretical reasons why this could be advantageous: On the one hand, a higher inflation target may increase the public’s inflation expectations and thus, by extension, stimulate aggregate demand particularly at or near the ZLB, see also Summers (1991). On the other hand, as a higher inflation target increases the equilibrium nominal interest rate, an increase of this target will give central banks more space to lower interest rates if needed. The advantages in terms of macroeconomic flexibility related with higher inflation targets of say 4% will clearly outweigh the possible costs of a higher long-run inflation rate, according to the aforementioned researchers.====This views are, however, not uncontested. For instance, Schmitt-Grohe and Uribe (2010) and Coibion et al. (2012) argue that a change in the inflation target is not a convenient stabilization tool since it does not necessarily reduce the costs arising from the ZLB. They further claim that low inflation targets, close to zero, reduce the probability of reaching the zero bound on interest rates. Additionally, Eggertsson and Woodford (2003) show that raising the inflation target may increase the number of periods when the policy rate hits the ZLB, due to a reduction of the natural rate of interest which is conditional for the ZLB to become a binding constraint. Further, Coibion et al. (2012) argue that the optimal rate of inflation is unlikely to be significantly higher than 2% even if possible ZLB episodes, which are assumed to happen in 1% of the time, are taken into account. Using a New Keynesian model with rational expectations, Ascari and Sbordone (2014) find that an increase in trend inflation through a higher inflation target tends to reduce the determinacy region in the interest policy parameters ==== and ==== and to destabilize inflation expectations. Branch and Evans (2017) show in a learning environment how higher inflation targets may lead to large and persistent deviations from the rational expectations equilibrium when agents use constant gain learning. In such an environment, unstable dynamics may be avoided through a transparent communication about the long-run inflation target and the timing of its implementation, or by increasing the inflation coefficient in the Taylor rule significantly. Further, Chung et al. (2012) argue that there are various arguments for much more often ZLB periods than what was assumed in previous studies. In this light, Dordal i Carreras et al. (2016) also find that if the distribution of ZLB durations is modelled in a more realistic manner, the optimal inflation ranges between 1.5% and 4%.====Between these two “extreme” views, various researchers have already investigated or at least argued for a state-dependent definition of the monetary policy target. Evans (2010), while stressing the advantages of price-level targeting relative to the more standard inflation targeting, suggests the implementation of a state-contingent price-level target which would be higher as long as the economy is in a liquidity trap. Once the higher price-level path is achieved with confidence, he argues, the monetary policy strategy would return a targeting of 2% inflation over the medium run. Similarly, Bernanke (2017) argues in favor of a “temporary price-level target” that would kick in only when rates are constrained by the ZLB. Alternatively, Kiley and Roberts (2017) propose an inflation targeting interest rate which takes into account the foregone accommodation – measured by a “shadow rate rule” – because of the ZLB. This “shadow rate rule” would reflect the cumulative deviations of inflation and output from their respective targets during the ZLB episode, being thus history-dependent in a similar way as a price-level targeting rule would be at least during the ZLB period. In a similar approach, Proaño and Lojak (2020) use the ZLB policy rate gap – defined as the spread between the policy rate and the hypothetical Taylor rule rate which would prevail if the ZLB was not binding – as a measure for lack of conventional monetary policy accommodation which affects the risk perception of the market participants, leading to higher risk premia and thus to a deterioration of the financing conditions of households and the government which in turn prolongs the ZLB episode.====To the best of our knowledge, these and the other studies in this strand of the literature have focused on a single closed economy only, abstracting from the international dimension. Further, they have not analyzed the case of a monetary union where more than one country is subject to the same monetary policy and thus to the same inflation target. As the 2% inflation target by the European Central Bank (ECB) may still represent a significant constraint for the macroeconomic flexibility of at least some of the economies within the euro area, it is worthwhile to investigate the potential benefits and drawbacks of a possible varying inflation target in a monetary union context.==== In particular, we focus on the ==== aspect of monetary unions such as the euro area (Jaumotte, Sodsriwiboon, 2010, De Grauwe, Ji, 2013, Proaño, Schoder, Semmler, 2014, Proaño, Lojak, 2017), and how the risk premium charged on government bonds can be affected by the ZLB.====The remainder of the paper is organized as follows. In section 2 we describe the behavioral two-country monetary union model based on Proaño and Lojak (2020) and similar in spirit to Bertasuite et al. (2020). In section 3 we discuss the results of our numerical analysis both concerning the short-run dynamics of the model and its medium-run properties under a constant inflation target as well as under a state-dependent inflation target. We undertake a robustness analysis of our findings in section 4, also delivering additional insights. Finally, we draw some conclusions from this study in section 5.",Monetary Policy with a State-Dependent Inflation Target in a Behavioral Two-Country Monetary Union Model,https://www.sciencedirect.com/science/article/pii/S0165188921001718,10 September 2021,2021,Research Article,166.0
Savagar Anthony,"University of Kent, Canterbury, United Kingdom","Received 29 March 2021, Revised 11 August 2021, Accepted 30 August 2021, Available online 8 September 2021, Version of Record 20 September 2021.",https://doi.org/10.1016/j.jedc.2021.104232,Cited by (0),"I study the effect of dynamic firm entry, ",None,Measured productivity with endogenous markups and economic profits,https://www.sciencedirect.com/science/article/pii/S0165188921001676,8 September 2021,2021,Research Article,167.0
"Graham James,Ozbilgin Murat","University of Sydney, Australia,New Zealand Treasury, New Zealand","Received 25 May 2021, Revised 30 August 2021, Accepted 30 August 2021, Available online 8 September 2021, Version of Record 20 September 2021.",https://doi.org/10.1016/j.jedc.2021.104233,Cited by (9)," of employment. We calibrate the model to data from New Zealand, where the health effects of the pandemic were especially mild. In this context, we model lockdowns as supply shocks, ignoring the demand shocks associated with health concerns about the virus. We then study the impact of a large-scale wage subsidy scheme implemented during the lockdown. The policy prevents job losses equivalent to 6.5% of steady state employment. Moreover, we find significant heterogeneity in its impact. The subsidy saves 17.2% of jobs for workers under the age of 30, but just 2.6% of jobs for those over 50. Nevertheless, our welfare analysis of fiscal alternatives shows that the young prefer increases in unemployment transfers as this enables greater consumption smoothing across employment states.","The COVID-19 pandemic of 2020 was the cause of enormous macroeconomic disruption around the world. The primary economic effects of the pandemic were the result of restrictions on producer and consumer activity (i.e. lockdowns), as well as decreased consumer demand due to health concerns associated with the virus. As stimulus measures, governments quickly implemented large fiscal interventions in the form of employment subsidies, loans to firms, and direct transfers to households. While the fight against the virus continues, many important economic questions remain: what are the macroeconomic costs of the lockdowns used to contain COVID? To what extent did fiscal interventions help to offset these costs? And how important is household heterogeneity in helping to understand the effects of these lockdown and fiscal policies?====This paper studies the macroeconomic and distributional consequences of lockdowns imposed during the pandemic. In general, pandemic recessions are the result of both supply and demand shocks, which complicates the study of the economic impact of lockdowns on their own. For this reason, we build a model of a lockdown calibrated to data from New Zealand, whose experience with the pandemic differed markedly from many other countries. Because New Zealand is a remote island nation, and because its government quickly closed international borders and imposed a strict national lockdown, the virus was effectively eliminated in the community by June 2020.==== As a result, the health effects of the pandemic and their consequences for consumer demand have been limited.==== This is in stark contrast to the effects of the pandemic in countries like the USA.==== Thus, the experience of New Zealand presents a useful case study for investigating the macroeconomic effects of lockdowns in isolation.====We study lockdowns in a heterogeneous agent overlapping generations model with multiple industries and labor market search and matching frictions. Households in the model differ by age, wealth, employment status, and industry of employment. They choose how much to consume and save over their life-cycle, subject to fluctuations in employment determined by equilibrium in the labor market. A fiscal authority raises taxes, issues debt, funds transfers to unemployed workers, and provides employment subsidies during the lockdown. We solve for general equilibrium of the model and calibrate its steady state to match the distributions of wages, employment, and employment risk across age and industry using data on the New Zealand labor market prior to the pandemic.====We solve for equilibrium dynamics of the model in response to a pandemic-induced lockdown shock and the associated fiscal policy responses. We characterize a lockdown as a sequence of negative shocks to industry-level productivity (see also Bayer et al., 2020, Bilbiie and Melitz, 2020, Guerrieri et al., 2020).==== In contrast to many other papers in the literature, but consistent with the limited spread of COVID-19 in New Zealand, we do not model the epidemiological aspects of the virus and we ignore the effects of demand shocks due to health concerns (see Auerbach, Gorodnichenko, Murphy, 2021, Baqaee, Farhi, 2020, Eichenbaum, Rebelo, Trabandt, 2020, Farhi and Baqaee, 2020, Faria-e Castro, 2021, Kaplan et al., 2020, Krueger et al., 2020). Instead, we assume that productivity shocks capture the entirety of the impact of a lockdown since firms cannot utilize labor resources at their previous rates. We further assume that firms in the services sector are disproportionately affected by a lockdown. This is because service workers typically need to interact with customers to carry out their jobs and as a result are much less able to work from home.==== In addition, service sector firms such as those in the tourism, accommodation, and travel industries have been especially affected by ongoing restrictions on international travel.====One of our main contributions to the literature is to emphasize the importance of household age in assessing the distributional effects of lockdowns and their associated fiscal policies.==== While the health effects of COVID-19 have been disproportionately felt by the old, young households are more likely to be affected by fluctuations in the labor market. One reason for this is that the young are much more likely to work in the service sector, which in turn is more exposed to the effects of a lockdown. In our model, we capture the effects of these age-dependent exposures through two novel features of our labor market structure. First, we incorporate life-cycle labor market search dynamics, following Chéron et al. (2013); De la Croix et al. (2013); Lugauer (2012). Since households must first match with firms before starting work, the young take time to settle into employment and are therefore more likely to be unemployed than the old. Second, we incorporate endogenous job separations, following Den Haan et al. (2000); Fujita and Ramey (2012). In combination these model features allow us to match the life-cycle profiles of wages and job separations observed in the data. During a lockdown, larger shocks to the service sector result in young workers being laid off in much larger numbers, which disproportionately raises their unemployment risk.====Another contribution of the paper is our study of the macroeconomic and distributional effects of large-scale wage subsidy policies enacted during the pandemic. We consider the wage subsidy scheme introduced by the New Zealand government. This policy represented an exceptionally large fiscal intervention: between March and June 2020 the wage subsidy scheme paid firms approximately 50% of the median wage for each worker employed and the scheme supported approximately 75% of the New Zealand labor force. In order to be eligible for the subsidy, firms needed to observe a 30% decline in revenues over the previous month. To capture this feature of the policy, we introduce a revenue-dependent subsidy policy in the model and calibrate it to match the 75% of firm-worker matches that received the subsidy during the first quarter of the lockdown. Because revenues fall by more for firms in the service sector and with younger workers, these firms are significantly more likely to receive the wage subsidy. This suggests that the wage subsidy scheme was reasonably well-targeted, in that it most benefited those workers disproportionately affected by the lockdown.====In order to discipline our lockdown exercise in the model, we calibrate the sequence of lockdown shocks to match the declines in employment across the services and non-services sectors in the first two quarters of the pandemic. We then calibrate the wage subsidy policy parameters to match the size of the wage subsidy received by firms and the fraction of employees supported by the subsidy. As in the data, a lockdown in the model generates a 5% decline in service sector employment and a 1% decline in non-services employment in the first quarter of the pandemic. Comparing the baseline economy to a counterfactual economy absent the wage subsidy scheme, we find that the subsidy saves a large number of jobs. In aggregate, the policy preserves 6.5% of steady state employment relationships, which is equivalent to 175,000 jobs in the New Zealand labor market. In the cross-section, the subsidy saves 8.4% of service sector jobs and 5.2% of non-service sector jobs. But the largest differences are by age. The wage subsidy saves 17.2% of jobs for workers under the age of 30, but just 2.6% of jobs for workers over the age of 50.====We also use the calibrated model to study the effects of two alternative fiscal policies. We first consider a policy that raises unemployment benefits, and second we consider a policy that pays a lump-sum transfers to all households. These alternatives mimic policies that were adopted in the United States and other parts of the world during the pandemic.==== In order to compare policies on a dollar-for-dollar basis, we assume that each policy implies the same total fiscal transfer expenditures (i.e. the sum of unemployment benefits, wage subsidies, and lump-sum transfers). We find that although the alternative policies do not prevent unemployment during the lockdown, raising unemployment benefits enables more consumption smoothing among young households than does the wage subsidy policy. Since the young earn low wages in normal times, a large increase in unemployment benefits raises average youth income despite a large increase in the unemployment rate. This represents a large increase in unemployment insurance for those most likely to use it.====Finally, we conduct a welfare analysis to study the relative merits of each fiscal policy response to a lockdown. Average welfare gains are higher for the wage subsidy policy than either of the alternatives. However, this masks significant heterogeneity in the welfare benefits of these policies. We find that young households are much more likely to favor the policy that raises unemployment benefits, and the welfare gains for these households are relatively large. Because young workers are much more likely to become unemployed than other workers during the pandemic, higher unemployment benefit payments help young households smooth consumption better than does a policy that simply preserves employment for many, but not all, young workers. In contrast, older households prefer the wage subsidy policy to other fiscal policies, although the welfare gains are relatively small. Although unemployment risk for older workers remains relatively low during the lockdown, the higher wages that are earned later in life imply larger costs of job loss even if unemployment benefits are raised.","Age, industry, and unemployment risk during a pandemic lockdown",https://www.sciencedirect.com/science/article/pii/S0165188921001688,8 September 2021,2021,Research Article,168.0
"Tubbenhauer Tobias,Fieberg Christian,Poddig Thorsten","University of Bremen, Faculty of Business Studies and Economics, Max-von-Laue-Strasse 1, 28359 Bremen, Germany","Received 23 January 2021, Revised 10 August 2021, Accepted 26 August 2021, Available online 3 September 2021, Version of Record 15 September 2021.",https://doi.org/10.1016/j.jedc.2021.104231,Cited by (1),We analyze the ,"Since the introduction of the Santa Fe artificial stock market by Arthur et al. (1997) and the simple and analytically traceable model of Brock and Hommes (1998), agent-based modeling has become an increasingly popular concept in finance. The success these two models achieved in explaining some stylized facts of financial markets led to the development of several more models (see for example Alfarano, Lux, Wagner, 2008, Chiarella, Iori, Perell, 2009, Cont, Bouchaud, 2000, Diks, van der Weide, 2005, Franke, Westerhoff, 2012, Franke, Westerhoff, 2016, Gaunersdorfer, Hommes, Wagener, 2008, Iori, 2002, Ji, Wang, Xu, 2019, Kirman, Gilles, 2002, Lux, Alfarano, Wagner, 2005, Lux, Marchesi, 1999, Lux, Marchesi, 2000, Raberto, Cincotti, Focardi, Marchesi, 2001, Raberto, Cincotti, Focardi, Marchesi, 2003, Schmitt, Westerhoff, 2017, Tedeschi, Iori, Gallegati, 2012).====Despite this success, there are few empirical applications of agent-based models (ABMs) in finance. Furthermore, most early utilizations of ABMs are designed to explain general observations and specific events in financial time series. For example, the objective of the ABM application in Boswijk et al. (2007) is to determine which of the agent strategies dominate specific time periods of the S&P 500. Applications in the field of predictive output validation are conducted by Recchioni et al. (2015), Ghonghadze and Lux (2016) and Ji et al. (2019). Recchioni et al. (2015) use the model of Brock and Hommes (1998) to make price and directional forecasts. Ghonghadze and Lux (2016) and Ji et al. (2019) utilize ABMs to make risk forecasts. There are few empirical applications because of the lack of a rigorous estimation procedure to validate ABMs (see Recchioni et al., 2015). The estimation approach pursued in this paper can easily be adapted to other multi-agent models or other applications and is demonstrated based on the well-known ABMs of Brock and Hommes (1998) andFranke and Westerhoff (2012) and the value-at-risk (VaR) forecasting application. In our view, ABMs are inherently well suited for risk forecasting due to their ability to explain specific stylized facts of financial markets, such as fat tails and volatility clustering. We are particularly interested in VaR forecasting, as it is an important indicator for measuring the risk of loss in financial risk management since the Basel Committee on Banking Supervision (BCBS) 1996 established that financial institutions have to scope market risk by covering them with equity. Another reason we believe ABMs are generally well suited for empirical applications such as VaR forecasting is their ability to explain macrostructural observations such as prices or returns through the microstructural behavior of interacting agents. By using ABMs, we might therefore gain unique insights regarding behavioral explanations behind volatility patterns that are not obtainable with standard econometric techniques. This creates the possibility to use additional behavioral information to further improve risk forecasting by, for example, creating an early warning system for sudden changes in market volatility.====As indicated above, due to a lack of suitable techniques, ABMs were not calibrated to empirical data for a long time. Instead, the system parameters were chosen manually through trial and error to create the desired stylized facts (this is true for most of the models cited earlier; see for example Arthur, Holland, LeBaron, Palmer, Tayler, 1997, Brock, Hommes, 1998, Chiarella, Iori, Perell, 2009, Cont, Bouchaud, 2000, Diks, van der Weide, 2005, Gaunersdorfer, Hommes, Wagener, 2008, Iori, 2002, Kirman, Gilles, 2002, Lux, Marchesi, 1999, Lux, Marchesi, 2000, Raberto, Cincotti, Focardi, Marchesi, 2001, Raberto, Cincotti, Focardi, Marchesi, 2003, Tedeschi, Iori, Gallegati, 2012). The lack of estimation techniques is because ABMs usually have many parameters, and no closed-form solution can be derived by using standard econometric techniques. For the so-called simple models, the estimation problem could be somewhat solved in the past two decades, but to date, there still exists no method to reliably calibrate the more complex models.==== The method of simulated moments (MSM) was introduced by Gilli and Winker (2003) and later refined by Winker et al. (2007), Franke (2009) and Grazzini and Richiardi (2015). It is a modification of the method of moments, but instead of deriving analytical expressions, the moments are calculated from the simulated time series of the ABMs. There is some subjectivity involved in the selection of the moments, as it is up to the researcher to choose which moments should be included in the estimation process. From our perspective, this subjectivity represents a considerable advantage because it allows the ABMs to be calibrated on the examined application, which is VaR forecasting in our case. While several alternative calibration methods have been developed recently, with the most notable being the information criterion-based methods by Barde, 2016, Barde, 2017 and Lamperti (2017) and the likelihood-based methods by Kukacka and Barunik (2017), Grazzini et al. (2017) and Lux (2018), the MSM is still the standard technique for calibrating ABMs (for more recent applications of the MSM, see for example Chen, Lux, 2018, Franke, Westerhoff, 2012, Franke, Westerhoff, 2016, Jang, 2015, Jang, Sacht, 2016, Ji, Wang, Xu, 2019, Schmitt, Westerhoff, 2017), as it is easy to implement and has proven to reliably produce accurate estimation results. The MSM provides a general estimation of the ABM parameters, but to conduct time series forecasts, it is crucial to determine the state of the ABMs at the current time step. To achieve this, we propose a maximum likelihood (ML) approach as a second estimation step. There are two advantages of this approach. First, the ML estimation allows us to estimate parameters that depend on current empirical observations, which considerably extends the applicability of the method. Second, no large data set is needed to conduct ML estimation, which allows us to re-estimate the parameters with much more recent data points. Altogether, this two-step estimation approach is quite flexible and therefore also usable for ABM applications beyond the VaR forecasting demonstrated in this paper.====In the empirical analysis, we use S&P 500 prices and returns from 01.01.1950 to 30.06.2020 and divide the estimation into two parts. In the first part, which includes prices and returns from 01.01.1950 to 31.12.2006, we estimate the general system parameters of the ABMs using the MSM. Next, we use a rolling window analysis on the remaining (out-of-sample) time series. Here, we re-estimate the parameters with the ML method to find the state of the ABMs at the current time step. Additionally, we divide the out-of-sample price and return series into expansion and recession periods to provide a more in-depth analysis of the VaR forecasts. To quantify the performance of the VaR forecasts, we use the unconditional, independence and joint tests by Christoffersen (1998). Finally, we analyze the correlation between the parameter values and the VaR forecasts to examine the behavioral patterns of the agents.====We find that variants of both the Franke and Westerhoff (2012) and Brock and Hommes (1998) models are able to generate VaR forecasts that outperform the (GARCH) benchmark models. Specifically, for the Franke and Westerhoff (2012) model, the profit-oriented variants with bias or herding perform best. The Brock and Hommes (1998) model generates even better results and outperforms all other ABMs and GARCH benchmark models. Furthermore, it is especially notable that the ABMs perform much better during recession periods. This is because the ABMs react quicker and longer to strong volatility spikes in recession periods than GARCH models. In essence, ABMs are the superior VaR forecasting models in periods characterized by high loss risks. From our analysis of the micro behavior, we find that the main factors leading to higher volatility are a loss of homogeneity among the agents and decreasing rationality in the decision process. Most important, we find that our proposed two-step estimation approach enables ABMs to describe time series dynamics. It allows us to calibrate ABMs to observed data, to use them in various empirical applications and to compare the performance of competing ABMs.====The remainder of the paper is organized as follows. Section 2 introduces and describes the ABMs. Section 3 discusses the MSM and ML applied estimation techniques used to fit the models to empirical data. Section 4 describes the S&P 500 data and reports the results of the estimations. Section 5 provides the results of the VaR forecasts and examines agent behavior. Section 6 concludes the paper.",Multi-agent-based VaR forecasting,https://www.sciencedirect.com/science/article/pii/S0165188921001664,3 September 2021,2021,Research Article,169.0
"Amir Rabah,Machowska Dominika,Troege Michael","Department of Economics, University of Iowa, Iowa City, IA 52242, Iowa,Department of Econometrics, Faculty of Economics and Sociology, University of Łódź, Poland,ESCP Europe, 79. av. de la République, 75543 Paris Cedex 11, France","Received 3 November 2020, Revised 3 August 2021, Accepted 22 August 2021, Available online 28 August 2021, Version of Record 11 September 2021.",https://doi.org/10.1016/j.jedc.2021.104229,Cited by (0),"A finite-horizon Lanchester model of a (continuous-time) differential game of oligopolistic advertising is considered, and the analytical form of the unique closed-loop ==== derived and analyzed. In contrast to previous research, the finite-horizon Lanchester model is modified to include two novel factors. First, a growing market allows us to analyze the competition for a potential market via generic advertising, with the latter giving the otherwise zero-sum-like game a public good dimension. Second, it is assumed that each firm’s market share declines in the absence of advertising efforts. The analysis investigates the Markovian (closed-loop) equilibrium calling for firms operating in a competitive growing market to invest in offensive, defensive or generic advertising in all or part of the decision horizon. In the most novel part of the paper, the non-cooperative outcome is compared to the Pareto-optimal or cooperative solution. The conclusions derived reveal significant differences in the resulting patterns of co-existence of the different dimensions of advertising over time, relative to the existing literature.","Advertising is a critical competitive tool in many markets. Ads attract consumers through their informativeness, building product awareness or persuasive features, creating product loyalty, differentiating from other competing products, or altering consumers’ taste. Advertising budgets for the largest global brands keep growing (Lu and Navas (2020)). A key issue that garners sustained scholarly attention is the nature of advertising strategies at various stages of the product life cycle in competitive markets (Stark (2016)). Buzzell (1999) points out that managers should look not only at product development but also at the evolution of the entire market when they make strategic decisions for the development of the firm. The notion of market life cycle was introduced (Rogers (2010); (Kotler and Keller, 2012, p.331)) to distinguish between four major phases of market evolution: emergence, growth, maturity and decline (see also Shahmarichatghieh et al. (2015)). Dekimpe and Hanssens (1995) make strong empirical arguments about the influence of market evolution on marketing effectiveness. Using econometric models of advertising response, Chandy et al. (2001) show that there are significant differences between consumer reactions to advertising in different phases of market evolution. In the literature, reviewed in Huang et al. (2012), much attention has been paid to the analysis of the first phase, inter alia, the optimal time to enter a market (e.g. Savin and Terwiesch (2005)) or the diffusion of a new product (e.g. the seminal papers of Bass (1969); Fourt and Woodlock (1960); Sale et al. (2017)).====However, advertising in growing markets has received less attention, despite its rising importance. The structure of many product and service markets is rapidly changing, due in large part to technological innovations that enable faster introduction of new products and improvement of existing ones (Soberman and Gatignon (2005)), or to unpredictable major changes, such as COVID19, that impact consumer behavior. Agarwal and Barry (2002) highlight that decisions made at the growing market phase have a significant long-term impact on market position, overall profitability and competitive advantage. It is empirically documented that advertising in a growing market is more intensive than in a mature market (Sethuraman et al. (2011)). Strategic marketing decisions about the allocation of resources between acquisition and retention in a growing market form a significant challenge (Min et al. (2016)). This calls for further research to improve our understanding of the interaction between oligopolistic dynamics and the growth phase of market evolution.====The basis for this paper is the Lanchester model, originally introduced to describe an attrition conflict between two armed forces.==== Kimball (1957) introduced the Lanchester model to analyze long run oligopolistic competition using advertising only, with prices being fixed due e.g., to perfect competition on the product market (e.g., Roberts and Samuelson (1988)). Along with its dynamic nature, the main idea of this model is that offensive advertising increases the market share of each firm in proportion to the market share of its competitor. Empirical studies support the fact that the Lanchester model provides a good reflection of real-world actions and reactions of firms (see e.g., Nguyen and Shi (2006)). This model has been used to capture many aspect of advertising competition (Huang et al. (2012); Jørgensen and Sigué (2020)). An extensive literature has developed using the open-loop Nash equilibrium concept to characterize firms’ advertising strategies (Erickson (1985); Jørgensen and Zaccour (2012)) over a finite or an infinite planning horizon. In the latter case, the game has been extended in several ways. The first group of models includes different types of advertising strategies. In addition to offensive strategies, the authors include defensive and, most recently, generic advertising (among others: Bass et al. (2005b); Erickson (1993); Martín-Herrán et al. (2012)). Recently Jørgensen and Sigué (2016) incorporated the three types of advertising strategies into the model.====Another factor that differentiates the extant Lanchester models with infinite horizon is the stage on the product life-cycle. In a mature market, the sum of market shares is equal to 1 while in a growing market, this sum is less than 1. The latter property reflects the fact that part of the market consists of new or hitherto unreached consumers, to be referred to as a potential market. The case of a mature market was investigated by Bass et al. (2005b); Erickson, 1992, Erickson, 1993; Jørgensen and Sigué (2016). Bass et al. (2005b) pointed out that advertising may increase penetration of a mature market (e.g. by promoting new uses). Therefore, there are models that describe competition between firms operating in a mature market (Bass et al. (2005b); Espinosa and Mariel (2001); Jørgensen and Sigué (2016)). However, growing markets are rarely considered (see Erickson (1995); Fruchter (1999)). Accordingly, the current study aims to fill this gap.====There are studies that consider the Lanchester model under a finite horizon, as summarized in a classification in Table 1. Open-loop Nash equilibrium is considered by Erickson (1985); Wang and Wu (2001); Zhou et al. (2018). There are significant differences between Nash equilibria under open-loop and closed-loop information structures in general dynamic games (e.g., Amir (2003); Dockner et al. (2000)). While tractability favors open-loop strategies, closed-loop strategies are seen as more appropriate in reflecting dynamic information and not requiring unrealistic commitment levels by players.==== Importantly, empirical work on advertising has confirmed that closed-loop strategies provide a better fit for the data (see e.g., Erickson (1992)).====Due to the finite horizon, the game is time varying, and thus the resulting value functions and strategies depend explicitly on time, which makes the HJB equation more complex. In most studies, the authors use numerical methods to determine equilibrium advertising. However, Bass et al. (2005a) and Dockner and Jørgensen (2018); Jørgensen, Sigué, 2020, Jørgensen, Sigué, 2015 derive an explicit form of the equilibrium strategies, as does the present paper. Table 1 shows that potential markets have not been considered in the finite horizon case. A decay rate was considered by Wang and Wu (2001), but only numerically.====This paper is based on the finite-horizon Lanchester model of Jørgensen and Sigué (2015), with three types of advertising: offensive, defensive and generic. We extend this model in two novel ways. First, most existing models assume that, without advertising, market shares remain unchanged.==== Yet Nagy et al. (2017) suggest that even the best advertising strategies do not protect against natural attrition or dropping out by some consumers. This may be due to mismatch between the attributes of a product and consumers’ evolving preferences and lower cumulative satisfaction (Bolton (1998)), demographic changes, changes in job status, or in overall economic situation (see Gustafsson et al. (2005)). Such attrition is especially observed in growing markets, as confirmed by empirical evidence in Libai et al. (2009). Therefore, we also incorporate a market share decay rate to capture this important feature. Second, we assume that the two firms operate in a growing market (for our definition, see (Kotler and Keller, 2012, p.126)). Thus, the two firms need to compete for new consumers in addition to their existing consumers.====These two modifications of the Lanchester model are complementary from a modeling standpoint as the presence of a growing market is necessary for generic advertising. The effect of the latter is modeled as depending only on the firms’ total effort, thereby adding a cooperative or public good dimension to the advertising game. To assess the overall nature of the game, it is useful to first make an observation about the earlier version of the Lanchester model without the two new features that have been hitherto overlooked in the literature. The latter model forms a game akin to a zero-sum game, though not formally one (e.g., Sorger (1989)). In contrast, the present version of the model combines these antagonistic features with the public good component. These changes give rise to a new version of the Lanchester model as a differential game with a finite horizon, a potential market and a natural market share decay rate. Our more complex and realistic formulation of the model still allows for a closed-form solution of the time-varying closed-loop Nash equilibrium strategies, which better describe the patterns of strategic advertising over time (see Nguyen and Shi (2006)).====A key part of the paper considers the novel issue of cooperation between the two firms in their advertising strategies, specifically a Pareto-optimal solution. Various forms of cooperative strategies in advertising are actually common, e.g., there are many shared brands for which the marketing is jointly organized by otherwise competing firms (Tregear and Gorton (2009)). Another frequent channel of marketing cooperation is carried out by trade associations, which may directly take charge of industry-level advertising (Messer et al. (2005)), or impose across-industry self regulation on advertising strategies (Boddewyn (1989)). Not surprisingly, cooperation leads to a cessation of offensive and defensive advertising, thus underscoring their zero-sum nature in the game. On the other hand, cooperation results in a doubling of generic advertising, relative to the Nash equilibrium, in line with its public good nature and the concomitant free-riding behavior. In this sense, beyond its common use as a useful benchmark, the comparison of the two solutions sheds light on the nature of the game itself by highlighting the game-theoretic nature of the separate components of firms’ advertising strategies.====In a nutshell, the main aim of this paper is the characterization of closed-loop equilibrium advertising strategies for firms operating in a duopolistic growing market over a finite horizon. This approach purports to answer these specific research questions: (i) Under which market conditions does Nash equilibrium preclude investing in advertising? (ii) How does the effectiveness of particular types of advertising influence the closed-loop Nash equilibrium? (iii) What are the resulting patterns of the different components of advertising?, (iv) How does the decay rate impact the overall advertising strategies, and (v) How would cooperation impact the outcome?====The rest of the paper is as follows. Section 2 lays out the modified Lanchester model and the closed-loop equilibrium. Section 3 has the economic analysis of the equilibrium. The last section concludes.",Advertising patterns in a dynamic oligopolistic growing market with decay,https://www.sciencedirect.com/science/article/pii/S0165188921001640,28 August 2021,2021,Research Article,170.0
Yin Penghui,"School of Economics, Shandong University","Received 20 October 2020, Revised 18 May 2021, Accepted 22 August 2021, Available online 28 August 2021, Version of Record 7 September 2021.",https://doi.org/10.1016/j.jedc.2021.104230,Cited by (0)," pay less attention to the capital income risk. As a result, wealthier consumers have higher perceived uncertainty about their future capital income and display a stronger precautionary motive by saving at higher rates.","Understanding precautionary saving behavior is of vital importance in answering why the rich save so much (see, e.g., Dynan et al. (2004)). Standard economic models with decreasing absolute risk aversion preferences show that wealthier individuals tend to be more risk-tolerant and save less due to the precautionary motive (see, e.g., Kimball (1990)). However, this is at odds with the empirical fact from the Survey of Consumer Finances (SCF) as shown in Fig. 1, which shows that individuals with more wealth, on average, have a stronger precautionary saving motive.==== One potential explanation for this heterogeneous precautionary saving behavior is that individuals with different wealth levels face different levels of uncertainty in their future income. For example, according to rational inattention theory in Sims (2003), devoting more attention to income shocks can reduce an agent’s perceived uncertainty in future income and affect their choices of precautionary savings.====This paper investigates how wealth, attention, and precautionary saving motive are correlated, both empirically and theoretically. In particular, the paper makes three contributions. First, it uses individual-level SCF data about information choice, precautionary saving, and total wealth (net worth and current income) and investigates the impact of wealth on attention choice and further on precautionary saving decisions. I measure attention using the number of information sources an individual used when making investing decisions.==== I measure precautionary saving motive by the desired amount of savings individuals would like to have against unexpected emergencies. Usually, it is not easy to disentangle precautionary savings from other types of savings. However, a survey question in the SCF can directly indicate the precautionary saving motive. Using cash-on-hand as a measure of total wealth, I find: (1) wealthier individuals pay less attention to capital income risk, and (2) wealthier individuals have stronger precautionary saving motives partly because they pay less attention.====Second, I study these empirical facts in a theoretical model by introducing rational inattention into a standard consumption-saving problem. In the model, agents choose the amount of attention to pay to future capital return when making consumption-saving decisions subject to a cost of attention. More importantly, in such a two-period framework with stochastic capital return and a Gaussian signal, it is possible to analytically solve both consumption and attention problems. Analytical results show a clear mechanism for wealth inequality to affect precautionary savings through attention: individuals with more initial wealth are less risk averse in absolute term and would pay less attention to capital income risk, and as a result, wealthier individuals have higher perceived uncertainty in their future capital income and display a precautionary motive by saving at higher rates.====Third, the paper also confirms several common themes in the rational inattention literature. For example, the optimal amount of attention to capital income risk is negatively correlated with information cost but positively correlated with prior variance and the discount factor.====To check the robustness of the main results, I also study two generalized models: I first relax the assumption of Gaussian signal and second extend the baseline model into an infinite-horizon model with recursive utility. The numerical results in these extensions also show a negative relationship between wealth and attention choice and a positive relationship between precautionary savings and initial wealth. In addition, I find discrete consumption-saving choices in the first extension. More precisely, when acquiring and processing information is costly, agents rationally ignore some information regarding the capital return, and as a result, they do not adjust their consumption-saving decisions as frequently as in the case without information cost.====It is worth detailing why focusing on the role of capital income risk in precautionary saving decisions matters. First, Kennickell and Lusardi (2005) use SCF data and show that precautionary saving motives affect two groups more strongly, namely, retired people and entrepreneurs, who mainly face capital income risk. Second, in the U.S., 49% of individuals invest directly or indirectly in stocks and investment funds, which exhibit highly non-diversified risks (see Bertaut and Starr-McCluer (2002)). Third, Luo and Yin (2020) study a consumption-saving model with both labor and capital income risks under rational inattention and find that the capital income risk is more critical in determining consumption-attention allocations.====This paper builds on Sims, 2003, Sims, 2006. Sims argues that consumers cannot attend perfectly to all freely available information when deciding their lifetime consumption plans. He proposes modeling consumers’ limited attention as a constraint on information flow, which is measured by entropy as in the information theory by Shannon (1948).==== Following Sims, rational inattention theory has been popularly applied to consumption-saving problems. For example, Luo (2008) solves the excess smoothness puzzle and excess sensitivity puzzle. However, a two-period model with capital income risk allows me to solve both consumption-choice and portfolio choice problems analytically within a Gaussian framework. Moreover, such a model also enables me to study the role of wealth inequality in attention choice.==== This paper also builds on Van Nieuwerburgh and Veldkamp (2010). The authors solve static portfolio choice and information choice problems with CARA and CRRA preferences, respectively, in a Gaussian framework. Similar to their work, agents in the present paper also choose the optimal amount of attention to process information regarding stochastic capital returns. Different from Van Nieuwerburgh and Veldkamp (2010), in which utility arises from terminal wealth, in the present paper, agents make optimal intertemporal consumption allocations under limited attention.====The paper contributes to the rational inattention consumption literature by studying attention choice behavior analytically in a non-linear setup. For reasons of tractability, linear-quadratic utility or log utility are popularly used in rational inattention studies.==== However, when investigating saving behavior with capital income risk, neither linear-quadratic nor log-utility can be employed. The former utility specification implies certainty equivalence, and the latter implies that income and substitution effects cancel out.==== In this paper, I can obtain analytical solutions to consumption-saving choice and attention choice problems with stochastic interest rate and CRRA preferences in a Gaussian framework. sThis paper also relates to the literature studying consumption choices with the stochastic capital return.==== A common assumption in these studies is that future capital return is not observable, and agents are endowed with a prior belief about the distribution of capital return. Epstein (1980) extends these models above by allowing agents to learn about future capital return. He shows the optimal savings decrease with the amount of information about future capital return, but the amount of information is still exogenous to agents. In this paper, I go one step further and allow agents to choose how much to consume and how much information to acquire regarding the capital return. Furthermore, Fagereng et al. (2020) empirically show that wealthier individuals face persistently higher returns to wealth, and their findings provide one possible reason why wealthier individuals save more (in risky assets).==== However, in the present paper, I focus on precautionary saving motives across wealth groups through optimal information choice.====The remainder of this paper is organized as follows. Section 2 provides empirical facts on the relationship between wealth and precautionary saving motives. Section 3 presents a two-period rational inattention consumption model with a Gaussian signal. Section 4 solves consumption and attention allocation problems analytically. Section 5 extends the benchmark model into a non-Gaussian framework and an infinite-horizon model with recursive utility to check the robustness of the main findings. Section 6 concludes.",Optimal attention and heterogeneous precautionary saving behavior,https://www.sciencedirect.com/science/article/pii/S0165188921001652,28 August 2021,2021,Research Article,171.0
"Ellison Martin,Macaulay Alistair","Department of Economics & Nuffield College, University of Oxford and CEPR, UK,Department of Economics & St. Anne’s College, University of Oxford, UK","Received 1 June 2020, Revised 11 August 2021, Accepted 17 August 2021, Available online 26 August 2021, Version of Record 9 September 2021.",https://doi.org/10.1016/j.jedc.2021.104226,Cited by (0),"We show that introducing rational inattention into a model with uninsurable unemployment risk can generate multiple steady states, when the model with full information has a unique steady state. The model features persistent, heterogeneous labour market expectations, consistent with survey evidence. In a heterogeneous agent ====, rational inattention to the future hiring rate generates three steady states: an unemployment trap with mild deflation and a low (but positive) job hiring rate, a middle steady state with moderate employment and ====, and an ‘employment trap’ with high employment and ====. Large mutations in the distribution of household beliefs can shift the economy between steady states.","There is a long history of papers suggesting that self-fulfilling expectations might allow an economy to become stuck in a bad steady state (see Diamond (1982) for an early example). An important example concerns the interaction of labour market expectations and precautionary saving. If households believe that their future employment prospects are bleak, they will increase precautionary savings today (Carroll and Dunn, 1997). The resulting fall in aggregate demand causes employment to fall, confirming the pessimistic beliefs. The sharp rise in the savings of wealth-poor households, who are least able to self-insure against unemployment risk, suggests that this was an important factor in the fall in consumption during the Great Recession (Heathcote and Perri, 2018). In existing models of this mechanism households precisely co-ordinate their labour market expectations on a particular equilibrium, as is common in models with multiple equilibria (Morris and Shin, 2000).====In this paper, we show that if it is costly for households to process information about future labour market conditions, their optimal information choices can generate multiple self-fulfilling steady states in a model which would have a unique steady state if households were fully informed. The unemployment trap generated in this way relies on the interaction of labour market expectations and precautionary saving, but it does not feature the strong co-ordination of household beliefs present in existing models of self-fulfilling labour market expectations.====There are two important assumptions that drive this result. First, we assume that the hiring rate out of unemployment, which is crucial for precautionary saving in models with frictional labour markets (e.g. Ravn and Sterk, 2020), is not observed directly by households. Instead, we assume that households can obtain signals about the hiring rate, but that this information processing is costly. Households therefore choose to process somewhat noisy signals before deciding on their consumption, following the literature on rational inattention (Sims, 2003). Importantly, we allow households to choose the structure of their signals, not just how much information those signals contain. The hiring rate is naturally bounded between 0 and 1, and in rational inattention problems with a bounded unknown variable the optimal signal has a discrete number of possible realisations even though the underlying variable is continuous (Matějka, 2016). This nonlinear signal structure is what drives the possibility of multiple steady states.====The information processing cost implies that households have limited information about ==== of the hiring rate. Our second key assumption is that households also have limited information about the ==== of their environment: they do not precisely know the true equilibrium marginal distribution of the hiring rate. This is related to the ‘internal rationality’ studied by Adam and Marcet (2011). To our knowledge, we are the first paper to examine this combination of rational inattention and imprecise prior beliefs. This second assumption is consistent with hiring rate expectations reported in the Survey of Consumer Expectations: expectations have a much greater variance than the underlying hiring rate, which suggests that households do not have a good understanding of the range of values usually observed for the hiring rate.====The information choices made by households in this environment imply labour market expectations which are also consistent with survey data along other key dimensions. Expectations in the model are heterogeneous and persistent. They are only weakly co-ordinated across households: when the hiring rate falls, households on average receive signals that indicate it has fallen, and so average hiring rate expectations fall. These signals, however, are very noisy, so at the same time as the average expectation is falling a little, some households are shifting their expectations up, and others are expecting a large collapse in the hiring rate.====We find support for all of these features of expectations in the Survey of Consumer Expectations. Even after controlling for a range of personal characteristics, there is a great deal of disagreement about the expected hiring rate each month, and expectations are highly persistent at the household level. We also find evidence that co-ordination of beliefs is weak: changes in average expectations account for just 0.3% of the variation in household belief changes, suggesting that households do not simultaneously agree on shifts in the hiring rate. That is, most changes in household beliefs are idiosyncratic, as in the model.====The combination of information processing costs and imprecise prior beliefs is central to our results. To see why, consider an economy at a non-stochastic steady state. The true distribution of the hiring rate contains a single point. If households have precise knowledge of this distribution, then they know the hiring rate with certainty. The model with rational inattention is trivially identical to the model with full information, and the only possible steady state is the unique steady state from the full information model.====More generally, if small shocks imply that the hiring rate has a non-degenerate distribution that remains tightly concentrated around some steady-state level, the actions of a fully-informed household will also be concentrated in a tight range. If an inattentive household knows this distribution there is no need for them to consider actions far away from that full-information range. The actions of inattentive households therefore remain very close to the actions that would be taken under full information, so there is little scope for multiplicity. When households do not know the true distribution of the hiring rate, however, their actions may deviate substantially from those that would be made under full information, and in that case it is possible for there to be multiple steady states, even when the model with full information has a unique steady state.====Given this logic, it is important for our results that beliefs about the hiring rate distribution remain imprecise over time. In the model we find that this is indeed the case, even when households can use the information they processed in previous periods to update their prior beliefs over time, because each period households process information until its marginal benefit equals its marginal cost. As long as that occurs before prior beliefs converge to the true distribution of the hiring rate, households will never choose to process the information required to learn that true distribution. This means that multiple steady states can survive even when beliefs are updated over time. When the information cost is such that there are three steady states, we find that the outer steady states (the unemployment and employment traps) are locally stable, while the middle steady state is unstable, with respect to ‘mutation’ shocks which alter the distribution of prior beliefs in the population. Large mutations can cause the economy to transition between steady states.====Section 2 places this work in the context of the literature. In Section 3 we develop our results within a simple model. In Section 4 we introduce our mechanism into a version of the HANK model from Ravn and Sterk (2020), in which the future hiring rate is very important in household decisions. We show that the combination of rational inattention and imprecise prior beliefs about the hiring rate generates three possible steady states: an unemployment trap with a low hiring rate and mild deflation, a middle steady state with higher employment and moderate inflation, and an ‘employment trap’ with very high employment and high inflation. In Section 5 we show that several key features of our model are found in survey data on hiring rate expectations. Section 6 concludes.",A rational inattention unemployment trap,https://www.sciencedirect.com/science/article/pii/S0165188921001615,26 August 2021,2021,Research Article,172.0
"Mohrschladt Hannes,Schneider Judith C.","Finance Center Münster, University of Münster, Universitätsstr. 14–16, 48143 Münster, Germany,School of Economics and Management, Leibniz University Hannover, Koenigsworther Platz 1, 30167 Hannover, Germany","Received 12 November 2020, Revised 1 July 2021, Accepted 17 August 2021, Available online 26 August 2021, Version of Record 6 September 2021.",https://doi.org/10.1016/j.jedc.2021.104227,Cited by (0),"While the standard to calculate model-free option-implied skewness (MFIS) relies on out-of-the-money (OTM) options, we examine the empirical and economic implications of using in-the-money (ITM) options. We find that the positive short-term return predictability of OTM-based MFIS significantly reverses if ITM-options are used instead. While this reversal is inconsistent with an explanation based on skewness preferences, MFIS apparently reflects information that is not timely incorporated in stock prices due to market frictions. Based on these insights, we introduce ====MFIS as a new measure of additional option-embedded information that significantly predicts subsequent returns beyond a large range of other option-based return predictors.","Measures extracted from option prices such as risk-neutral skewness (Bakshi et al., 2003) provide forward-looking information on the probability distribution of subsequent returns, i.e., perceptions about the future stock development. This information proves useful for investment firms to manage portfolio risk (DeMiguel et al., 2013) and for firm managers to retrieve investor beliefs (Billings and Jennings, 2011). Recent literature on individual stock options suggests that both OTM- and ITM-options are used by investors to express their opinion (Ge et al., 2016 and Augustin et al., 2019). Nonetheless, it has become research and industry standard to estimate risk-neutral moments by constructing smooth implied volatility curves using quoted OTM-options only while ignoring quoted ITM-options.==== We show that ITM-options are traded with sufficient liquidity to also provide relevant forward-looking information. More importantly, this information allows to disentangle competing economic concepts with respect to model-free option-implied skewness (MFIS).====We use either OTM- or ITM-options to construct the option-implied volatility curves which determine the risk-neutral return density and thus MFIS. As OTM- and ITM-option-implied volatility curves should coincide perfectly in frictionless markets, MFIS==== and MFIS==== should represent the skewness of the risk-neutral return density identically. However, we show that the correlation between these two OTM- and ITM-based MFIS-estimates is only 24%. Hence, the information content between MFIS==== and MFIS==== seems to differ substantially. This observation becomes even more striking when examining the ability of the measures to predict subsequent returns: While MFIS==== is a significantly positive predictor of subsequent returns, the relationship between MFIS==== and subsequent returns is significantly negative. The former finding is in line with Rehman and Vilkov (2012), Stilger et al. (2017), and Chordia et al. (2020), but specific to MFIS====.====The reversal for MFIS==== is puzzling at first glance, but allows to disentangle two different economic concepts frequently related to skewness-based return predictability. Skewness preferences would suggest no difference between the findings for MFIS==== and MFIS==== since both specifications are valid proxies for forward-looking skewness. In particular, we empirically confirm that both MFIS==== and MFIS==== significantly predict physical return skewness (also see corresponding findings on MFIS==== in Conrad et al., 2013 and Borochin and Zhao, 2020). Hence, if investors’ return expectations depend on skewness, this effect should be captured by MFIS==== and MFIS==== in the same direction. Since this is not the case, we show that both the positive return predictability of MFIS==== and the negative predictability of MFIS==== can be linked to informed option trading: As pointed out by Stilger et al. (2017), due to market frictions, option prices can contain relevant information that is not immediately reflected in stock prices such that cross-market return predictability can arise. For example, high demand for put options reflects negative information, increases put prices, and should predict low subsequent returns. Since one tail of the risk-neutral distribution is determined by call and the other by put prices, MFIS systematically reflects this asymmetry. In particular, this argument can explain the different return implications of MFIS==== and MFIS==== since they measure demand pressure in put versus call options in an opposed way by construction if market frictions exist.====The information content of MFIS==== and MFIS==== is related to both skewness and information-based demand effects. To isolate the latter from potentially confounding skewness effects, we introduce ====MFIS as the difference between MFIS==== and MFIS====. Market frictions imply that this difference does not always equal zero such that option-embedded information can result in the return predictability of ====MFIS. We show that both MFIS==== and MFIS==== are valid skewness proxies and predict subsequent physical skewness in the cross-section, while ====MFIS does not share this skewness property. This underpins that the return predictability of ====MFIS does not stem from its relation to subsequent moments but from additional option-embedded information. Following this theoretical argument, we show that ====MFIS positively predicts subsequent stock returns and also subsumes the standalone predictive power of MFIS==== and MFIS====. Further, ====MFIS is not subsumed by other measures of informed option trading. These encompass the volatility spreads following Cremers and Weinbaum (2010) and Bali and Hovakimian (2009) and the SMIRK-measure of Xing et al. (2010). Including all measures in Fama-MacBeth-regressions demonstrates that ====MFIS carries unique information content. In particular, as MFIS strongly depends on the tails of the risk-neutral distribution, the superior return predictability of ====MFIS stems from its ability to better capture such tail information compared to the other measures, i.e., ====MFIS becomes a substantially weaker return predictor after excluding extreme tail information from its estimation.====Our interpretation of ====MFIS as a proxy for additional option-embedded information implies that the empirical findings should be related to stock-level news, mispricing, and market frictions such that the option-embedded information is incorporated in stock prices with a time lag. Our further analyses support the interpretation of ====MFIS as a proxy for informed option trading. First, the return predictability of ====MFIS is more pronounced around earnings announcement dates. In addition, earnings surprises are significantly worse for firms that show low ====MFIS-estimates before. These observations support the hypothesis that ====MFIS carries relevant forward-looking information content. Second, this option-embedded information should have stronger forecasting power among speculative, attention grabbing, and relatively overvalued stocks. We support this relation empirically by applying portfolio double sorts on ====MFIS, maximum daily returns, and the mispricing measure of Stambaugh et al. (2012). Third, illiquid stocks are most likely to be mispriced since limits to arbitrage are comparably high (Shleifer and Vishny, 1997). Indeed, the return spreads associated with ====MFIS are largest among illiquid stocks. Finally, while market frictions are a prerequisite for non-zero ====MFIS-estimates, the observed return premium is not merely a compensation for holding stocks that exhibit comparably high frictions. ====MFIS remains significant in Fama-MacBeth-regressions after controlling for a number of illiquidity and friction proxies such as Amihud (2002) illiquidity, bid-ask spreads (Goyenko et al., 2009), idiosyncratic volatility (Ang et al., 2006), and the contribution of market frictions to expected returns (CFER) as recently proposed by Hiraki and Skiadopoulos (2019). To sum up, the broad set of further analyses supports that the return predictability of ====MFIS is driven by market frictions and mispricing that can be detected via informed trading in the option market. In a nutshell, the underlying economic mechanism is as follows: informed investors identify mispricing and trade on it in the option market. As market frictions can prevent this information from being immediately reflected in stock prices, ====MFIS as a proxy for this option-embedded information can predict subsequent stock returns.",Option-implied skewness: Insights from ITM-options,https://www.sciencedirect.com/science/article/pii/S0165188921001627,26 August 2021,2021,Research Article,173.0
"Boungou Whelsy,Hubert Paul","PSB Paris School of Business, 59 rue Nationale, Paris 75013, France,Banque de France, 31 rue Croix des Petits Champs, Paris 75001, France,Sciences Po - OFCE, 10 place de Catalogne, Paris 75014, France","Received 28 July 2020, Revised 6 August 2021, Accepted 17 August 2021, Available online 25 August 2021, Version of Record 6 September 2021.",https://doi.org/10.1016/j.jedc.2021.104228,Cited by (3),Faced with a potential ==== on deposit ==== are the most affected by the implementation of negative interest rates.,"Since 2012, several central banks in Europe and the Bank of Japan have introduced negative policy interest rates. This policy aims to encourage commercial banks to increase their credit supply in order to support economic activity. The effectiveness of such policy depends crucially on the pass-through to the lending and deposit rates (Brunnermeier and Koby, 2018). However, it seems that the pass-through of negative interest rates to deposit rates is not perfect because of their downward rigidity (Hannoun, 2015; Jobst and Lin, 2016). At least, two reasons can explain the reluctance of banks to introduce a negative deposit rate: ==== some legal constraints and ==== the fear of a ""===="" (Scheiber et al., 2016). Therefore, negative interest rate policies (NIRP) could negatively affect banks’ profitability by compressing net interest margins due to the so-far observed zero lower bound for deposit rates. Some recent studies show that negative interest rates lead to a compression of net interest margins, although they have little or no effect on banks’ overall profitability (Lopez et al., 2020; Molyneux et al., 2019).====The question of how commercial banks cushion and offset the fall in net interest margins due to negative interest rates then arises. In order to preserve their profitability, commercial banks face different options: ==== foster credit supply to mitigate the reduction in margins with an increase in volumes, ==== redirect their income from interest products towards non-interest sources (such as fees and commissions), ==== reduce their operating costs (such as salaries and employee benefits), by focusing more on digital banking for instance, or ==== reduce the interest rate paid on non-customer deposit liabilities. Against this backdrop, the main objectives of this paper are to: (==== investigate empirically the channels through which banks respond to negative interest rates; (==== assess how banks adjust their responses as negative rates persist over time; (==== explore which bank characteristics influence banks’ responses to negative rates. The analysis draws on yearly bank-level data for 3637 banks in 59 countries between 2011 and 2018. Our identification relies on the comparison between banks in the 25 countries that have adopted negative interest rates and those in the remaining countries that have not.====We complement the analysis of how banks respond to NIRP (from the perspective of their profit and loss accounts) with an investigation of which transmission channels from the central bank to commercial banks matters. The literature suggests that the effects of NIRP on banks would arise through three broad channels: (i) the improving economic outlook reducing loan defaults; (ii) a term structure effect implying securities valuation changes and additional margin compression; and (iii) the negative remuneration of central bank reserves. We investigate these three channels using our framework.====To explore these questions, we use a difference-in-differences (DiD) approach based on bank-level unconsolidated data (i.e. at the entity level) to compare the behavior of banks at a disaggregated level of their profit and loss account, before and after the introduction of NIRP. This methodology also enables us to examine whether banks’ responses to NIRP depend on their characteristics. The key assumption of the DiD approach is that the control and treatment groups are comparable prior to the treatment. We therefore use the characteristics of the treated group to define the control group. As complementary analyses, we use the cross-sectional variation in measures of bank exposure to NIRP to identify the effects of this policy and the propensity-score matching approach to build a control group in order to cross-validate our results.====This paper differs from the existing literature in two respects. First, the granularity of our dataset enables us to precisely identify the channels of banks’ response to the implementation of NIRP at a fine disaggregated level. We thus explore the dynamic of banks’ response and assess whether keeping this policy for a prolonged period of time might have negative consequences for banks. It also assesses whether the channels of NIRP transmission to banks’ balance sheet differ according to banks’ characteristics (such as deposit reliance, size and leverage). Second, we use a large sample of banks located in several countries and covering a longer period of time. The inference in the empirical analysis relies on the time and cross-sectional variation of detailed balance sheet and profit and loss accounts of 3637 banks operating in 59 low-, middle-, and high-income countries during the period 2011–2018, consisting of around 20.000 observations. Our data cover the implementation of NIRP by all central banks in Europe (within and outside the euro area) and by the Bank of Japan.====The main empirical challenge of this paper is to identify a control group (banks unaffected by NIRP) that satisfies the common trend assumption with the treatment group (banks affected by NIRP) before the introduction of negative interest rates. To do so, we construct a control group by restricting the pool of banks unaffected by NIRP that match those treated using bank-specific characteristics (such as size and holdings of liquid assets) and the market concentration on which they operate (measured with the Herfindahl-Hirschman index, thereafter HHI). The control group closer to the treatment group is composed of banks with a size above the 80th percentile, with a share of liquid assets above the 10th percentile, and whose HHI is above the 40th percentile. For robustness, we construct alternative control groups based on various combinations of these thresholds allowing for different coverages of banks and countries. In addition, tests using the cross-sectional variation on bank exposure to NIRP are performed on the treated group only as a way to provide results independent of the construction of a control group.====The results of our analysis indicate that negative interest rates reduce banks’ net interest income by around 9%. Our results confirm evidence of the existence of a zero-lower bound on deposit interest rates: banks located in countries that have adopted NIRP are reluctant to charge a negative interest rate on customer deposits. We find that the reduction in gross interest expenses does not come from the interest rate on customer deposits (possibly because of its downward rigidity, see Levieuge and Sahuc, 2021) but rather from the reduction in other interest expenses (the interest paid on non-customer deposit liabilities). We also find that banks increase their net non-interest income to offset the effects of negative interest rates on their net interest income. Our results indicate that the increase in banks’ net non-interest income, in response to NIRP, is related to a reduction in non-interest expenses rather than to an increase in fees and commissions. We find a 6% reduction in personnel expenses following the NIRP implementation. Thus, NIRP could be seen as a trigger for accelerating a restructuring process with a view of lowering banks’ costs. Overall, our results suggest that negative interest rates reduce banks’ net interest income, but that banks’ responses (through a reduction in interest paid on non-customer deposits and in their personnel expenses), over our sample period, are not sufficient to fully offset the reduction in net interest income.====While we show that the increase in net non-interest income mitigates in part the decrease in net interest income due to the NIRP implementation, an important question relates to the dynamic of these response and more precisely whether keeping this policy for a prolonged period of time might have negative consequences for banks. We aim to capture empirically the effects of the so-called “negative-for-long” and how banks respond across time to NIRP. We find that how long NIRP is implemented matters. Indeed, we find that the magnitude of banks' responses increases as negative interest rates persist. We also highlight that at the end of our sample, banks start reducing customer deposit expenses and increasing net non-interest income to preserve their profits.====In addition, the investigation of the three broad transmission channels suggests that general equilibrium effects – through lower probability of loan defaults and less non-performing loans – and banks’ exposure to negative remuneration of central bank reserves might be the more prominent one in affecting banks. This analysis thus suggests that the effects of NIRP on banks could be heterogenous and would depend on the bank business model, its structure and characteristics.====Finally, we therefore analyze whether these responses differ according to the characteristics of banks.==== We find that banks’ responses to negative interest rates depend on their size, share of deposits and leverage. Banks most affected by the introduction of negative interest rates are large banks with higher deposits and higher leverage ratios. We also find that these large banks are associated with larger reductions in personnel expenses after the implementation of NIRP. However, we find that smaller banks with lower deposit and leverage ratios appear to have more room to contain the effects of negative interest rates.====Overall, we do not find evidence that banks, whatever their characteristics, use fees to cushion the compression of their net interest margins and limit the increase in customer deposit expenses. But large banks, with higher deposits and higher leverage ratios, are more constrained because they are more dependent on deposits and therefore potentially more reluctant to charge negative interest rates on deposits. At the opposite, smaller banks that are less dependent on deposits are less constrained by the implementation of NIRP. The main message of this paper is that banks are able to adapt to NIRP by shifting revenue sources and compressing costs, and that large banks with higher deposits and higher leverage ratios are more constrained, especially in the early stages of negative interest rate implementation.====Our findings are robust in various respects. First, we build alternative control groups including one using propensity score matching techniques. Second, we use the nominal short-term policy interest rate as our variable of interest replacing the NIRP dummy. Using a continuous variable enables to circumvent the issue that the identification stems from a ==== dummy which could be correlated to other events happening at the same date. This also enables us to use the variability of the policy rate below zero (the deposit facility rate in the euro area, for instance) to obtain a more precise inference of the effect of negative interest rates. Third, we include a proxy for central bank balance sheet policies in our specification to control for the implementation of other unconventional monetary policies that happen at around the same time. Fourth, our dataset has an annual frequency but several central banks introduced the negative interest rate policy in the middle or end of a year - the European Central Bank (ECB), for instance, reduced its deposit facility rate from 0 to -0.10% in June 2014. We therefore redefine the NIRP dummy such that it takes the value of one the year after the NIRP implementation – 2015 in the case of the ECB. These sensitivity analyses all confirm the baseline results.====This work relates to different contributions in the literature. First, our study extends the results of previous studies on the impact of negative interest rates on bank performance.==== Lopez et al. (2020), using data from 5273 banks located in the European Union and Japan, show that negative interest rates have no effect on net income: they argue that banks compensate for the contraction in net interest income by increasing their net non-interest income. Molyneux et al. (2019) also document, using data on 7359 banks from 33 OECD countries, that banks respond by increasing their non-interest income (such as fees and commissions). In this paper, we explore the channels of NIRP transmission to banks and how they respond to this policy at a disaggregated level of their profit and loss account. We also introduce an identification approach using bank exposure to NIRP to cross-validate our results. Our analysis complements these papers in at least three respects: ==== we confirm evidence of banks’ reluctance to charge negative interest rates on customer deposits; ==== we do not find evidence that banks increase their fees in response to NIRP but instead find that banks’ responses to negative interest rates came through a reduction in interest paid on non-customer deposits and non-interest expenses (such as personnel expenses); ==== we show that bank responses to negative interest rates are not immediate and that banks adjust their responses as negative interest rates persist over time.====This paper also relates to the literature that examines the lending channel of monetary policy under negative interest rates. Arce et al. (2018), Heider et al. (2019) and Molyneux et al. (2020) provide evidence that banks located in countries that have introduced negative interest rates have no incentive to increase the supply of credit. However, Boungou (2021), Demiralp et al. (2021) and Grandi and Guille (2020) find that banks highly dependent on deposits increase their lending activities under NIRP.",The channels of banks’ response to negative interest rates,https://www.sciencedirect.com/science/article/pii/S0165188921001639,25 August 2021,2021,Research Article,174.0
"Erceg Christopher J.,Jakab Zoltan,Lindé Jesper","International Monetary Fund, Washington D.C,International Monetary Fund and CEPR, Washington D.C","Available online 14 August 2021, Version of Record 27 October 2021.",https://doi.org/10.1016/j.jedc.2021.104211,Cited by (16),We develop a behavioral ,"The COVID crisis has induced many central banks to cut interest rates to historical lows as well as deploy a host of unconventional policy tools, including negative interest rates, forward guidance and large-scale asset purchases. Moreover, given that secular factors were viewed as depressing equilibrium real interest rates significantly even prior to COVID, central banks are giving prominent attention to how they might modify their strategies and tools to confront the challenges of a prolonged low interest rate environment.====While some options remain squarely within the dominant paradigm of flexible inflation targeting (see Svensson, 2011 for a comprehensive review) – such as raising the inflation target – other approaches involve more fundamental departures, including a shift to some form of price level targeting. But there are important questions about the conditions under which these alternative frameworks would yield beneficial effects, or if they would work at all. One crucial question is whether expectations would adjust to the new framework(s) in the manner implied by standard rational expectations models. Another key issue is whether a framework change should be adopted on a permanent basis, or only activated if the central bank is facing a liquidity trap (as in Bernanke, 2019 proposal of temporary price-level targeting).====In this paper, we analyze alternative policy strategies that may be appropriate in an environment with very low equilibrium real interest rates. Specifically, we estimate a DSGE model of the euro area that builds heavily on the workhorse model of Smets and Wouters (2007) (SW07 henceforth). By working with an estimated DSGE model, we strive to make the analysis less susceptible to the Lucas (1976) critique of policy analysis. Our baseline model allows for deviations from rational expectations (RE) by drawing on the behavioral approach in Gabaix (2020) to address the forward guidance (FG henceforth) puzzle, though we also compare results to an estimated version with fully rational expectations. Intuitively, the large effects of changing policy rules often apparent under rational expectations should be damped if agents are myopic and focus on near-term developments, with our empirical estimates determining the quantitative departure from rational expectations. We use stochastic simulations to study both the conditional and unconditional probability distributions of key model variables, allowing for an assessment of which policies work well both in a liquidity trap and in normal times.====Our estimated model captures several key features of the euro area economy which have important bearing both for the effectiveness of monetary policy tools as well as policy design. First, and most notably, we find support for the behavioral model relative to the rational expectations variant, with the posterior odds favoring the former. Because households and firms substantially discount forward real interest rates at more distant horizons, forward guidance has far less traction to provide economic stimulus than under rational expectations. Second, in line with recent studies using US data and the ECBs NAWN II model by Coenen et al. (2018), the sensitivity of inflation to marginal cost is estimated to be very low, i.e., the Phillips Curve is flat. As a consequence, inflation fluctuations are largely driven by price markup shocks. Third, consistent with the findings in Christiano, Motto, Rostagno, 2003, Christiano, Motto, Rostagno, 2008, Christiano, Motto, Rostagno, 2014, our estimated model implies that shocks originating in the financial sector are a key driver of business cycles in the Euro area.====Turning to policy implications, our model simulations highlight the benefits of a symmetric inflation target under both behavioral and rational expectations. For much of its early history, the ECB focused on keeping inflation pressures contained, and arguably followed an asymmetric reaction function that implied a stronger response to high inflation than to low inflation (Rostagno et al., 2020). Insofar as allowing for such asymmetry can substantially reduce the likelihood of high inflation outcomes – as we show in our simulations – it likely played a constructive role in anchoring inflation expectations and in buttressing the ECB’s credibility. However, the ECB in recent years has been more concerned with low neutral interest rates and low inflation (Lagarde, 2020, Lengwiler, Orphanides, 2020), and have emphasized the benefits of a symmetric target.==== Our simulations show how this symmetric approach can boost both the mean level of inflation and output relative to an asymmetric approach, as well as reduce output volatility.====We then compare the effectiveness of alternative policies in supporting recovery from a deep recession in the behavioral and rational expectations versions of the model. This analysis considers both the efficacy of unconventional monetary policy tools – including forward guidance, negative interest rates, and asset purchases – as well as the implications of a shift in the policy framework toward makeup strategies that include price-level and average inflation targeting. It is important to underscore that our analysis does not attempt to provide a full characterization of policy options open to either the ECB or to other central banks. Rather, we aim to assess whether strategies that work well under rational expectations remain reasonably effective when expectations are considerably less forward-looking.====We find that using a concert of UMP tools, including negative interest rates and asset purchases, is effective in spurring recovery in a deep recession. Even so, these policies are noticeably less potent in boosting the mean level of output and inflation in the behavioral model than under rational expectations, as well as in mitigating downside risks. Moreover, the rebound in inflation can be painfully slow. In addition to using UMP, we show that it is desirable for the policy reaction function to focus on the output gap rather than output growth, even while recognizing that the latter approach may have significant benefits in normal times (e.g., if policymakers are uncertain about potential output as emphasized by Orphanides and Williams (2005).====Make-up strategies including average inflation targeting (AIT) (see Nessén, Vestin, 2005, Svensson, 2020, Vestin, 2006) and price level targeting (PLT) (see e.g., Hunt and Laxton, 2004) can spur somewhat faster recovery relative to the benchmark Taylor-style rule, and limit downside risks. Even so, the benefits of a shift in strategy appear fairly modest under the estimated behavioral model, and much smaller than under rational expectations. One feature evident from our simulations is that these strategies tend to perform less well unconditionally, generating considerably more output volatility than under strategies reacting to current inflation. Accordingly, we find benefits of temporarily deploying these strategies when policy rates hit the effective lower bound, as in Bernanke et al. (2019), rather than adhering to them permanently.====We also undertake robustness analysis aimed at addressing the sensitivity of the findings to some key properties in the model. This includes allowing for a state-dependent slope of the Phillips curve in which the slope is substantially higher in booms than in recessions.====From a methodological perspective, our paper adds value to the existing literature, including the recent prominent papers by Bernanke et al. (2019), and Coenen et al., 2020, by using an estimated dynamic stochastic general equilibrium (DSGE henceforth) that can help quantify departures from rational expectations in an environment with realistic frictions. Our behavioral approach based on Gabaix is appealing insofar as it can easily be incorporated into a medium-sized DSGE model, and be flexibly parameterized to allow for different discounting in the price and aggregate demand equations. Even so, it is likely that a number of factors can help account for why expectations appear to adjust much less in practice than implied by models with fully rational expectations, including liquidity constraints as emphasized in McKay et al. (2016), and information frictions in Angeletos and Lian (2018).====Our paper is structured as follows. Section 2 presents the prototype model – the estimated model of SW07 amended to allow for behavioral expectations to mitigate the forward guidance puzzle (see Del Negro et al., 2012, for further discussion). Section 3 discusses data and estimation, while Section 4 uses the estimated model to perform a posterior predictive analysis comparing the impulse responses to key shocks and to forward guidance. Section 5 considers unconditional and conditional simulations under variants of flexible inflation targeting, while Section 6 focuses on alternative policy strategies such as price level targeting. Following a discussion of robustness in Sections 7 and 8 summarizes our key findings and discusses some challenges for monetary policy and structural economic models in light of the pandemic. The appendices contain technical details on the model.",Monetary policy strategies for the European Central Bank,https://www.sciencedirect.com/science/article/pii/S0165188921001469,14 August 2021,2021,Research Article,175.0
"Dieppe Alistair,Francis Neville,Kindberg-Hanlon Gene","ECB,University of North Carolina and World Bank,World Bank","Received 2 February 2021, Revised 9 June 2021, Accepted 6 August 2021, Available online 13 August 2021, Version of Record 9 September 2021.",https://doi.org/10.1016/j.jedc.2021.104216,Cited by (7),"Frequently, factors other than structural developments in technology and production efficiency drive changes in labor productivity in advanced economies (AEs) and emerging market and developing economies (EMDEs). In this paper, we contrast the responses of AEs and EMDEs to innovations in technology and investigate whether the cross-country co-movement in productivity is due to technological or non-technological factors. We find that technological innovations are associated with higher and more rapidly increasing rates of investment in EMDEs relative to AEs, suggesting that positive technological developments are often capital-embodied in the former economies. Employment falls in both AEs and EMDEs following positive technology developments, with the effect smaller but more persistent in EMDEs. Low cross-country correlations of technological developments suggest that global synchronization of labor productivity growth is primarily due to non-technological influences. Overall, non-technological factors accounted for most of the fall in labor productivity growth during 2007-09 but less than one-half of the longer-term productivity decline after the global financial crisis in the median AE and EMDE.","Labor productivity and TFP growth in advanced economies (AEs) and emerging and developing economies (EMDEs) have undergone many surges and declines historically, usually coinciding with global recessions and slowdown events (Dieppe, 2020). Growth has been less volatile in AEs than in EMDEs, but it has followed a similar series of rapid gains and slowdowns.====A wide range of factors can affect productivity growth. This paper will disentangle the influences of lasting changes to productivity, “technology” - a catchall phrase for the ==== persistent driver of productivity - from temporary or non-technological influences for a large number of economies. Better technologies or organizational and institutional changes are important drivers of sustained productivity improvements. Non-technology factors can also affect productivity growth. For example, demand-led changes in productivity are likely to have played a role in the pre-2008 surge and the subsequent decline in global productivity growth during the Global Financial Crisis (GFC), and they are once again expected to have an impact during the recent COVID-19 pandemic.====A standard growth accounting analysis of productivity developments only provides a partial insight into the drivers of large swings in productivity growth or slower-moving trend changes. One component of labor productivity, TFP, will reflect demand-driven cyclical influences such as changing labor and capital utilization as well as technological and organizational changes (Basu, Fernald, Kimball, 2006, Fernald, Wang, 2016).==== To account for the intensive margin of labor inputs, researchers typically scale the factor inputs using observable proxies for factor utilization. For example, average hours per worker, electricity usage, and surveys of capacity utilization can provide insights, individually and collectively, into how labor effort and capital utilization vary (Burnside, Eichenbaum, Rebelo, 1996, Imbs, 1999, Shapiro, 1993). This approach has also been extended using industry-level data to account for industry heterogeneity. However, data requirements for this approach—in particular, data on the sectoral distribution of hours-worked, employment, and capital—are prohibitive for many AEs and most EMDEs, which makes it impossible to conduct a broad-based analysis across many economies.====SVARs take an alternative approach to removing cyclical components of productivity growth. They identify the dominant persistent and permanent variations in labor productivity. These are assumed to reflect lasting structural influences, such as technological innovation or organizational changes. This paper uses a spectral SVAR identification methodology, developed in Dieppe et al. (2021), that identifies technology shocks as those that drive the largest proportion of low (long-run) frequency labor productivity variation. The SVAR approach effectively filters through other, less persistent changes in labor productivity or changes that drive only a small proportion of long-run labor productivity variation.==== The resulting SVAR allows an exploration of how key macroeconomic variables, including employment, consumption, investment, interest rates, and prices, adjust to technology developments. Secondly, the SVAR identification allows a historical decomposition of labor productivity growth developments into technology and non-technology drivers, including during and following the Global Financial Crisis.====In addition, the wide country coverage of this paper allows an exploration of the drivers of labor productivity synchronization. The literature has pointed towards evidence that a large proportion of advanced economy cross-country synchronization of productivity growth is driven by business-cycle factors as opposed to technology spillovers (Huo, Levchenko, Pandalai-Nayar, 2020, Imbs, 1999). Accordingly, using the identified technology measure, we assess whether these findings also apply to our expanded dataset covering EMDEs.====We make multiple contributions to the literature, which has until now focused on advanced economies. This paper is the first study to separate technology and non-technology drivers of labor productivity across a broad range of countries using the spectral SVAR approach (see Dieppe, Francis, Kindberg-Hanlon, 2019, Dieppe, Francis, Kindberg-Hanlon, 2021). Panel and individual VARs are estimated and identified for over 100 advanced and emerging economies, an unprecedented country sample relative to the existing literature, which has generally focused on the G-7 advanced economies.==== We are also the first to assess the synchronization of labor productivity growth across a broad range of countries for measures that remove non-technology drivers of labor productivity fluctuations. The existing literature focuses on advanced economy synchronization, whereas this study also considers a wide range of EMDEs (Huo, Levchenko, Pandalai-Nayar, 2020, Imbs, 1999).====The following findings emerge:====Our exploration starts with details of our methodology (and its panel extension) used to identify technological innovations. After providing data sources, we then proceed to explore the implications of the identified technological driver of labor productivity, along with their cross-country synchronization.====For the rest of the paper, unless explicitly stated otherwise, when we use the term “productivity” we are referring to “labor productivity” and not TFP. That is, we use the two interchangeably and will alert the reader when this is not the case.",Technological and non-technological drivers of productivity dynamics in developed and emerging market economies,https://www.sciencedirect.com/science/article/pii/S0165188921001512,13 August 2021,2021,Research Article,176.0
"Chen Zhengyang,Valcarcel Victor J.","School of Public Affairs, St. Cloud State University, USA,School of Economic, Political and Policy Sciences (EPPS), The University of Texas at Dallas, USA","Received 9 August 2020, Revised 13 June 2021, Accepted 5 August 2021, Available online 12 August 2021, Version of Record 25 August 2021.",https://doi.org/10.1016/j.jedc.2021.104214,Cited by (3)," fixes such as commodity prices, federal funds futures and forward rate data. We find they occur at monthly and quarterly frequencies. We consider alternative indicators with the same broad monetary aggregates Keating et al. (2019) employed in their investigation of a historical sample. They provide a consistent resolution of the price puzzle and they do not require the ","Important macroeconomic work in the 1960s by Brunner (1961), Brunner and Meltzer (1963), Friedman (1961), Friedman and Schwartz (1963), among others, changed economists’ views regarding the efficacy of monetary policy and the importance of monetary aggregates (see Nelson (2003)).====In the years that followed, however, a New Keynesian “consensus” emerged that centered on de-emphasizing money in favor of a single nominal interest rate in order to link monetary policy and aggregate demand. While various candidate explanations date back to the Keynesian-Monetarist debates of the 1960s and early 1970s, two are particularly salient. First, standard textbook ==== equations do not include a monetary aggregate but a single representative short-term real rate. Second, a once strong empirical relationship between the monetary aggregates, which the Federal Reserve produces, and economic activity began to break down in the 1980s. This erosion in the predictability of these monetary aggregates can be attributed primarily to an explosion of financial innovations and the mass adoption of new money markets, mutual funds, and other assets. Furthermore, changes in banking rules during the 1980s allowed banks to begin offering interest-earning demand deposits. Thus, in a data-rich monetary environment replete with a multitude of monetary instruments, a single relatively narrow measure of money balances, such as M2, loses its appeal.====Subsequently, identification of monetary policy shocks shifted attention to short-term interest rates. Even prior to the Taylor (1993) landmark paper, monetary economists had long recognized that central banks in practice treated the nominal interest rate—rather than the monetary aggregates—as their instrument of choice for the conduct of monetary policy. Interest-rate rules that responded to nominal variables in an appropriate manner could deliver low and stable inflation, even if these rules did not respond directly to movements in the stock of money.====There is extensive literature, some of which is outlined below, on how monetary policy shocks identified from innovations in the federal funds rate affect economic activity—with a number of candidate transmission mechanisms to explain the suggested effects. There has been comparatively less attention devoted to the effects on bank deposits and money markets from these shocks.====The 2007 Financial Crisis and the following protracted effective-lower-bound (ELB) period highlighted some shortcomings of the information content that the federal funds rate alone provides about monetary transmission. This opened the door to revisit the use of information from monetary aggregates in monetary models within the New Keynesian framework (see examples in Belongia and Ireland (2015), Belongia and Ireland (2018), Keating et al. (2019), among others).====We estimate time-varying responses of economic activity, bank deposits, and various money markets to monetary policy shocks. We compare responses to policy shocks identified from innovations on a short-term rate to those identified from innovations in a monetary aggregate. As a preview of results, we outline three main findings. First, multiple VAR specifications with the (shadow) federal funds rate as an indicator of monetary policy yield unremitting price puzzles in a modern sample that begins in the late 1980s. These price puzzles persist even when including commodity prices, federal funds futures, or forward rate data. Second, every puzzle occurrence seems resolved when replacing the shadow rate with the broadest measure of a Divisia monetary aggregate (DM4) or the narrower Divisia measure (DM2). These Divisia specifications do not require the inclusion of commodity prices or futures/forward rate data for price puzzle resolution. Third, the transmission of these shocks onto money markets exhibits considerable time variation. Results show the transmission is magnified following the 2007 Financial Crisis with a larger magnitude response in money markets with lower liquidity properties such as time deposits, commercial paper, and treasuries.====The rest of the paper is organized as follows. Section 2 reviews the literature on policy identification from federal funds rate models. Section 3 provides background on monetary aggregates and Divisia money. Section 4 discusses our choice of data and our reasoning for considering a modern sample that begins in the late 1980s. Section 5 describes the econometric approach. Section 6 elaborates on the effects of expansionary policy shocks on the aggregate economy. Section 7 discusses the transmission into money markets from our identified shocks, and Section 8 concludes.",Monetary transmission in money markets: The not-so-elusive missing piece of the puzzle,https://www.sciencedirect.com/science/article/pii/S0165188921001494,12 August 2021,2021,Research Article,177.0
"Bardoscia Marco,Ferrara Gerardo,Vause Nicholas,Yoganayagam Michael","Bank of England Threadneedle St, London EC2R 8AH, United Kingdom","Received 23 November 2020, Revised 7 June 2021, Accepted 4 August 2021, Available online 11 August 2021, Version of Record 29 September 2021.",https://doi.org/10.1016/j.jedc.2021.104215,Cited by (7),"We investigate whether margin calls on derivative counterparties could exceed their available liquid assets and, by preventing immediate payment of those calls, spread such liquidity shortfalls through the market. Using trade repository data on derivative portfolios, we simulate variation margin calls in a stress scenario and compare them with the cash buffers of the institutions facing the calls. Where buffers are insufficient we assume institutions take remedial action such as borrowing to cover the shortfalls, but only after waiting as long as possible to receive payments before making their own. Such delays can force recipients into more extensive remedial action than otherwise. Hence, liquidity shortfalls can grow in aggregate as they spread through the network. However, we find an aggregate liquidity shortfall equivalent to only a modest fraction of average daily cash borrowing in international repo markets. Moreover, we find that only a small part of this aggregate shortfall could be avoided if payments within the counterparty network were coordinated by an external authority.","Regulatory reforms following the 2007-08 global financial crisis [49] have resulted in the vast majority of derivative exposures in the core of the financial system being backed by collateral. Firstly, mandates have been introduced in major jurisdictions requiring financial institutions to clear new trades in many of the most popular over-the-counter (OTC) derivatives with central counterparties (CCPs). CCPs then collect collateral to cover both the current and potential future value of derivative exposures with their counterparties, the former being known as variation margin (VM) and the latter as initial margin (IM). As a result, the state of OTC derivatives clearing has been shifting towards that of exchange-traded derivatives, which are cleared only with CCPs. Secondly, major jurisdictions have also introduced requirements for financial counterparties to exchange both VM and IM on any new OTC derivative trades that are not centrally cleared.====However, greater collateralisation of derivative exposures means that financial institutions can expect to face larger margin calls when prices change or become more volatile. With few exceptions, these institutions already have to post VM to their financial counterparties whenever the value of their derivatives moves against them, reducing their unencumbered cash buffers.==== If margin calls exceeded an institution’s cash buffers, it would have to take some form of remedial action to bolster it. For instance, it might borrow on a secured basis in the repo market, borrow on an unsecured basis (perhaps drawing on a bank credit line) or sell some less liquid assets. Such remedial action could impose costs not only on that institution but also other market participants. For example, borrowing cash in the repo market could push up the repo rate faced by all market participants. Similarly, selling less liquid assets could push down their prices, and all market participants using mark-to-market accounting would then have to revalue their asset holdings at these new price levels.====In this paper we simulate VM calls on most OTC interest rate and foreign exchange (FX) derivative positions between a set of institutions comprising London Clearing House Limited (LCH.Ltd) and members of its SwapClear and ForexClear clearing services (103 institutions in total, plus the CCP). VM calls originate from changes in risk factors implied by the Severely Adverse scenario used in the Federal Reserve’s 2018 Comprehensive Capital Analysis and Review (CCAR) stress test of US banks.==== In this scenario, some of the trading book shocks move asset prices more sharply than ever before, including during the 2007-08 global financial crisis and the outbreak of the Covid-19 global pandemic in 2020. Where these margin calls exceed an institution’s cash buffers, we record a liquidity shortfall. While this would lead to some kind of remedial action, we do not explicitly model such actions and the consequences they might have. However, to put our simulated shortfalls into context, and because borrowing in the repo market is a common remedial action,==== we compare them to daily volumes of cash borrowing in international repo markets. In our worst-case analysis, liquidity shortfalls amount to a little over ==== of daily global repo volumes, despite us making intentionally conservative assumptions about the cash available to meet margin calls (e.g. we assume that part of cash buffers is depleted by non-derivative business activities, and that even in a stress, institutions nonetheless seek not to allow their liquid asset buffers to fall below regulatory thresholds).====To avoid miscalculating shortfalls, care must be taken to avoid unrealistic assumptions that can affect VM payments. Our methodological contribution addresses two potential sources of such miscalculation, which have not previously been accounted for in the literature.====First, to the best of our knowledge, this is the first paper to model the realistic sequencing of VM payments within the day, which can affect not only the distribution of liquidity shortfalls in the financial system but also their aggregate value. CCPs typically require clearing members facing VM calls to make payment early the next day. Shortly after receiving all of these incoming payments, CCPs then make outgoing payments to clearing members to whom they have a VM obligation. Finally, VM payments between different clearing members on their non-centrally cleared derivatives are normally made later in the day. Neglecting this sequencing would allow clearing members to use incoming payments from other clearing members to pay CCPs, whereas in reality they would not yet have received those funds. Using this realistic sequencing increases the aggregate liquidity shortfall in our stress simulation by $62.8 billion (under our most conservative cash buffer definition).====Second, we adopt a novel approach for dealing with institutions with insufficient cash buffers to pay their non-centrally cleared obligations. Most of the literature on payment networks is based on Eisenberg and (NOE) [27], which assumes that institutions with insufficient resources to meet their obligations use all of those resources to proportionally pay their counterparties as high a fraction of their obligations as possible. This assumption works well in bankruptcy cases, where any institution with insufficient assets to meet its liabilities must use those assets to meet its liabilities to the best of its ability. In contrast, an institution facing a liquidity shortfall would be unlikely to make a partial payment against an obligation because failure to pay in full constitutes an event of default, so such behaviour could trigger bankruptcy proceedings. Hence, we assume that institutions with insufficient cash buffers to pay their VM obligations in full initially wait for any incoming payments from institutions with sufficient cash buffers. This includes payments from institutions that only had sufficient cash buffers because they were paid by others. This corresponds to computing payments and shortfalls using the full-payment algorithm introduced in Bardoscia et al. [7]. If institutions still have a liquidity shortfall after receiving any such payments, they must take remedial action such as borrowing in the repo market in order to pay in full. In our scenario (under our most conservative cash buffer definition), the aggregate shortfall is $42.6 billion larger when institutions do not make partial payments.====This approach to settling payment obligations fits well with the description of the 1987 stock market crash by the current Federal Reserve Chair [47]. He noted that VM payments did not appear sequenced on ‘normal’ days, but during the crash market participants clearly waited to receive incoming payments before making their own outgoing payments.====We also decompose liquidity shortfalls into different components, one of which could be mitigated by policymakers. First, we split contributions into ‘fundamental’ and ‘domino’ components. The fundamental component of an institution’s shortfall is the amount it would have to finance even if all incoming payments were received before it had to take remedial action, such as borrowing in the repo market. In contrast, the domino component is the amount it has to finance because its incoming payments were delayed by counterparties in an effort to avoid remedial action themselves. Subsequently, we split the domino component into ‘avoidable’ and ‘unavoidable’ portions. The avoidable portion is that which an external authority could eliminate by identifying and directing self-fulfilling payments, e.g. if A paid B, B could pay C and C could pay A, which would give A the liquidity it needed to make the initial payment to B.====Finally, we compute the contributions of individual institutions to the aggregate liquidity shortfall. Institutions contribute more than their own shortfalls to the aggregate when their delayed payments to counterparties cause those institutions to also delay payments. Authorities considering emergency liquidity assistance in crises would be most effective if they focused on reaching institutions with the largest aggregate shortfall contributions.====The remainder of the paper is organised as follows: in Section 2 we discuss the literature on empirical studies and the models that are used in this strand of the research, in Section 3 we describe our simulation framework, including the data it employs; in Section 4 we discuss our results regarding liquidity shortfalls and contributions to them; and we conclude in Section 5.",Simulating liquidity stress in the derivatives market,https://www.sciencedirect.com/science/article/pii/S0165188921001500,11 August 2021,2021,Research Article,178.0
Papetti Andrea,"Bank of Italy – Directorate General for Economics, Statistics and Research","Available online 10 August 2021, Version of Record 27 October 2021.",https://doi.org/10.1016/j.jedc.2021.104209,Cited by (7), estimates.,"Advanced economies are undergoing a demographic transition, the ageing process by which “populations move from initially high fertility and mortality with young age distribitions to low fertilty and mortality with old age distribitions” (Lee, 2016). As the number of people entering the world is shrinking and mortality rates are decreasing (i.e. the survival probabilities are increasing), the relative number of the elderly is dramatically increasing. While before the 1980s the ratio of the elderly (aged 65 and over) to working age (aged 15-64) has been less than 2 to 10, the United Nations (UN, 2017) project this proportion to rise above 5 to 10 by year 2050 in Europe.==== Based on definite time-lags, demographic projections offer a relatively reliable knowledge of the future. It is therefore appealing to use demographic data as exogenous variation to explain macroeconomic dynamics.====Questioning the influence of demographic change on the real interest rate is certainly not new in economic research. What is new in recent years is that the topic is on the agenda of central bankers. The fact that real interest rates have been on a downward trend since the late 1980s across many countries leads to ask whether the natural interest rate has decreased as well and whether it will remain low in the years ahead, potentially hampering the effectiveness of monetary policy.====The definition of natural or neutral interest rate dates back to Wicksell (1898)==== and in modern macroeconomics can be thought as the rate of interest consistent with output at its potential or natural level in the absence of transitory shocks or nominal adjustment frictions (cf. Woodford, 2003; Brand et al., 2018).==== It will be identified as the real rate of return on capital (net of depreciation) that allows the saving supply (by households) to meet the capital demand (by firms) in the absence of any allocational friction or arbitrage. The ==== macro model employed in the paper will abstract from frictions and shocks that could capture business cycle variations. The natural rate of return on capital will be shortly called real interest rate.====Researchers have been recently looking for potential “slow-moving secular forces” as explanatory factors behind the downward trend in real interest rates (cf. Eggertsson et al. (2019)). Demographic change is one of such forces and how it can affect the natural real interest rate, an ==== variable, is the exclusive focus of this paper.====In a standard Solow (1956)’s model with homogeneous population and a constant saving rate, as population growth decreases capital per worker rises dampening the marginal product of capital, so that in equilibrium the more abundant factor, capital, receives a lower remuneration than the other factor, labor: the real interest rate falls while the real wage rises. Models embedded in the neoclassical framework, like general equilibrium overlapping generations (OLG) models, can rarely escape the prediction that ageing leads to a lower real interest rate.====Since the seminal contribution by Auerbach and Kotlikoff (1987), OLG models are considered the most reliable tool to evaluate the macroeconomic effect of demographic change as they allow to use the full empirical age distribution, in a context of a flexible life-cycle behavior. Recent contributions employing fully-fledged OLG models all predict a downward trend of the real interest rate due to population aging, no matter if the model encompasses the whole world economy with different countries/areas (cf. e.g. Domeij and Flodén (2006), Krueger and Ludwig (2007), Attanasio et al. (2007)) or a single country/area modeled as closed-economy (cf. e.g. Gagnon et al. (2021) for US, Bielecki, Brzoza-Brzezina, Kolasa, 2018, Bielecki, Brzoza-Brzezina, Kolasa, 2020 for Europe, Sudo and Takizuka (2020) for Japan).====This paper employs a large-scale OLG model, similar to those existing in the literature, to provide a quantification of the impact of the demographic transition on the aggregates for the euro area, with particular focus on the channels through which the real interest rate is affected. There are four channels (cf. Krueger and Ludwig (2007), and Carvalho et al. (2016)).====(1) ====. A decrease of the growth rate of the effective labor supply relative to the total population in the economy is akin to a slowdown in total factor productivity for output per capita growth, which leads firms to demand less capital, reducing the marginal product of capital and so the real interest rate, everything else being equal. This effect is stronger the more the age-distribution shifts towards older cohorts that are parametrized to be less productive and participative in the labor market, and the lower the degree of substitutability in production between capital and labor.====(2) ====. With the goal of smoothing consumption into the future, households anticipate their higher survival probabilities with a willingness of consuming less and saving more, i.e. becoming more patient, thus decreasing the real interest rate, all else being equal. This effect depends on the way households are insured against mortality risk as well as on the pension scheme in place, and is stronger the lower the intertemporal elasticity of substitution in consumption.====(3) ====. According to the life-cycle theory embedded in the model, households build up their wealth during the most productive working ages and dissave in later ages. The shift of the population distribution towards relatively more dissavers along the transition dynamics can decrease the aggregate saving rate thus increasing the real interest rate by making capital scarcer, all else being equal.====(4) ====. A pay-as-you-go (PAYGO) pension system with defined-benefit, as often run in European countries in spite of its different hybrid reformulations, requires an increase of labor taxes for the government budget to be balanced as the relative pool of retirees increases. This tends to crowd out productive capital in the economy and, as capital becomes less abundant, to increase the real interest rate, all else being equal.====The model is calibrated at the annual frequency for the euro area using demographic data and projections by UN (2017) as exogenous variations to study a perfect-foresight transition where the demographic change is perfectly anticipated by the agents in the economy. The quantitative exercise shows that the real interest rate path guided by demographic change exhibits a slight rise throughout the 1970s and 1980s, and then a prolonged and marked fall at least until 2030 (when it roughly reaches its trough according to the demographic projections). The shape is consistent with low-frequency econometric estimates (see Holston et al., 2017; Fiorentini et al., 2018; Brand et al., 2018 and Del Negro et al., 2019).====In the baseline analysis the annual real interest rate declines about 1.4 percentage points going to 2030 compared to the average in the 1980s. Under the various sensitivity exercises the real interest rate is never found to reach a higher level compared to its initial steady state value in 1950. These exercises show that the dampening effect of aging could be mitigated not only by higher substitutability between labor and capital and higher elasticity of intertemporal substitution in consumption, but also by reforms aiming particularly at increasing the relative productivity of older cohorts and the participation rate. An increase of the retirement age with no other supporting reform leads to a limited increase in output per capita growth and to essentially no change in the real interest rate.====A decomposition of the main drivers in the model shows that the presence of the PAYGO pension system with defined-benefits is associated with an upward deviation of the real interest rate from the initial steady state of about 0.5 percentage point by 2030. By the same year the increase of the survival probabilities is associated with a negative deviation of more than 0.9 percentage points. Variations in the number of newborns in the model would account for the residual deviation of the real interest rate, about 0.5 percentage point, going to 2030 compared to the average in the 1980s.====Through the lens of this model, it does not seem to be really the case that demographics can meaningfully “reverse three multi-decade global trends” on real interest rates via the pension system channel as argued by Goodhart and Pradhan (2017). At least not until 2030. But then the system is predicted to stay at a permanent lower level of about 1 percentage point compared to the one prevailed in the 1980s. Coherently with what recently found by Rachel, Summers, 2019, Rachel and Summers, 2019, this paper supports the view that in the absence of offsetting policies such as PAYGO pension benefits the natural real interest rate would have been at a lower level and that, going forward, bigger policy shifts are needed if one wants to overturn the downward impact of aging.====Interestingly, the model predicts that before 2007 one could easily interpret the impact of demographic change on output growth as a type of scaling factor, producing if anything mild variations around a constant, close to zero for the case of output per capita. However, after 2007 – which, coincidentally, is when the “Global Financial Crisis” started – the model predicts a dramatic decrease of output growth due to demographic change.==== For the real interest rate too, the most dramatic phase is after 2007 with a variation of almost -1 percentage point between 2007 and 2030. Hence, what noticed by Gagnon et al. (2018) for the United States, employing a similar OLG model, seems also true for Europe: “[...] the largest effects of demographics on interest rates and GDP growth coincide to some degree with the Global Financial Crisis”. So that “the permanent effects of demographic factors were, for a time, easy to misinterpret as persistent downward pressure on the natural rate of interest stemming from the global financial crisis” (Gagnon et al., 2021). Instead, what OLG models seem to predict is a “new normal”.====The quantitative results are not significantly affected when the model is augmented with exogenous time-varying total factor productivity (TFP) growth (inferred from a measure of the Solow residual). The associated re-calibration allows to be more consistent with the ==== of the econometric estimates of natural outcomes (real interest rate and output) as provided e.g. by Holston et al. (2017) while the ==== that can be attributed to aging remains of the same order of magnitude. Specifically, in the baseline with constant TFP growth in each period the real interest rate decreases by about 1.1 percentage points between 1990 and 2030, going from a value of about 2.24% in 1990 to 1.13% in 2030. Over the same period, the decrease is about 1.65 percentage points (2.22% to 0.57%) with the Hodrick-Prescott (HP) filtered TFP growth rate. Thus, the results from the OLG model allow to qualify the determinants of agnostic econometric estimates and provide a forecast relying on demographic projections. Compared to the average in the 1980s, the real interest rate decreases about 1.8 percentage points going to 2030. A decomposition shows that the progressive decline of TFP growth rates accounts for about 0.55 of those percentage points so that demographics account about two-thirds of the model’s generated decline of the real interest rate.====This paper builds on a vast literature on large-scale OLG models, rapidly growing since the onset of the debate on “secular stagnation” (see Eggertsson et al. (2019)) where the closest match with the framework employed here can be probably found in Domeij and Flodén (2006), and Krueger and Ludwig (2007). The former focuses on capital flows in a multi-country setting, the latter is a seminal paper on the impact of aging on rates of return to capital providing quantitative estimates only from 2000. The purpose here is not to further refine their theoretical structure. Rather, to offer a quantification (for the euro area) of the total effect of aging as well as of the different channels in isolation in a model where the full age-structure of the population is allowed. Carvalho et al. (2016) study the channels above using the analytically-tractable set-up ==== Gertler (1999) (applied to a representative OECD economy). They recognize that “tractability comes at the cost of not endowing the model with any flexibility to match the empirical age distribution”, and “leave for future research an extensive comparison of the results obtained in this framework with those from a large-scale OLG model, disciplined by a richer set of moments from demographics data” (Carvalho et al. (2016), p. 212). Indeed, this paper tires to accomplish that need of future research.====Quantitative estimates for the euro area are provided by Kara and von Thadden (2016) using a Gertler (1999)’s type of model and by Bielecki, Brzoza-Brzezina, Kolasa, 2018, Bielecki, Brzoza-Brzezina, Kolasa, 2020 using a richer OLG model. Their results on the impact of demographic change on the real interest rate are of the same order of magnitude of those provided in this paper, even though there the focus is more on the interaction of demographic change with monetary policy in a New-Keynesian framework and with global factors.====The rest of the paper is organized as follows. Section 2 describes the OLG model environment. Section 3 studies the simplified two-period version of the model to understand and isolate analytically the channels. Section 4 provides the full set of quantitative analyses including calibration, description of the experiment to study the transition dynamics, and a sensitivity analysis of the results to different parameter values. Section 5 introduces non-zero total-factor-productivity (TFP) growth in the model and makes a comparison with econometric estimates. Section 6 concludes.",Demographics and the natural real interest Rate: historical and projected paths for the euro area,https://www.sciencedirect.com/science/article/pii/S0165188921001445,10 August 2021,2021,Research Article,179.0
"Albonico Alice,Ascari Guido,Gobbi Alessandro","Department of Economics, Management and Statistics, University of Milano- Bicocca, Piazza dell’ Ateneo Nuovo 1, Milan 20126, Italy,Department of Economics, University of Oxford, Manor Road, Oxford OX1 3UQ, United Kingdom,Department of Economics and Management, University of Pavia, Via San Felice 5, 27100 Pavia, Italy,Department of Environmental Science and Policy, University of Milan, Via Celoria 2, Milan 20133, Italy,CefES,RCEA","Available online 8 August 2021, Version of Record 27 October 2021.",https://doi.org/10.1016/j.jedc.2021.104204,Cited by (6), to fight deflationary shocks.,"In face of the Great Recession and the unprecedented fall in output, private consumption and investment spending, all the advanced economies responded with a range of fiscal and monetary policy measures: increases in government spending, tax cuts, and various types of “unconventional” monetary policy measures, given that monetary policy was unable to lower further the nominal interest rate already close or at the zero lower bound.====As a result, the Great Recession deteriorated the fiscal positions of most advanced economies. Figs. 1a and 1b show the expansion in both the fiscal deficit and the debt-to-GDP ratio for the United States starting in 2008. Monetary policy also reacted promptly decreasing sharply the nominal interest rate to a level very close to zero (see Fig. 1c). The level of inflation in the US rebounded after the initial drop and then started drifting down, remaining below target. Thus, the US almost doubled the level of public debt from the pre-crisis level of about 60% in 2007 to a level above 110% in 2016. Other countries exhibit very similar dynamics, and the response to the crisis was clearly marked by an increase in public debt in most developed economies. In the euro area the increase in public debt triggered the sovereign debt crisis and a second recession. Evidently, the governments’ aim was to support the collapsing level of aggregate demand.====The increase in debt was brought about both by an increase in government spending and by a decrease in taxation. This boosted a large literature on fiscal multipliers, both spending and tax multipliers. Some works aimed at understanding the analytical mechanism of the fiscal multipliers, and its interaction with the zero lower bound (see Christiano, Eichenbaum, Rebelo, 2011, Eggertsson, 2011, Woodford, 2011). These works showed that multipliers for public spending are larger when the ZLB avoids the crowding out effect in private spending, thus amplifying the effect on output. When nominal interest rates are positive and adjust according to a Taylor rule, the rise in inflation that follows an expansion in government spending causes the central bank to raise the nominal interest rate to counteract the initial increase in demand. Conversely, when the ZLB constraint is binding the nominal interest rate does not rise to curb inflationary pressures, thus not dampening demand. Another strand of the literature focused on quantifying these multipliers empirically, using VARs, and theoretically, using DSGE models (e.g., Leeper, Plante, Traum, 2010, Leeper, Traum, Walker, 2017, Ramey, 2011, Ramey, 2019, Zubairy, 2014). Given the attention drawn by the Great Recession, many works investigated the possibility that fiscal multipliers vary depending on the state of the economy, and empirically investigates whether fiscal multipliers are higher during recessions than in normal times. The rationale behind this assumption is that in a Keynesian world the economy will not always be characterized by full employment of resources, suggesting a higher multiplier during economic slack. The seminal paper by Auerbach and Gorodnichenko (2012) showed that fiscal multipliers are state dependent. The papers by Blanchard, Leigh, 2013, Blanchard, Leigh, 2014 and its preceding Box in the IMF World Economic Outlook 2012 have been widely discussed, and have clearly influenced the policy debate and possibly the policy stance in some countries. These results are corroborated by many other works (Auerbach, Gorodnichenko, 2013, Caggiano, Castelnuovo, Colombo, Nodari, 2015, Fazzari, Morley, Panovska, 2015). More recently, Ramey and Zubairy (2018) cast doubt on these results, because they estimate multipliers which are below one also during recession periods.====This very large literature investigates fiscal multipliers of both spending and different kinds of taxes. In this paper, we aim to fill a gap in this literature by instead analyzing the debt multiplier - that is, the output effect of a pure change in government debt - caused by the mere deferral of future taxes. This topic has acquired even greater importance in light of the COVID-19 pandemic. In fact, while the Fed and other central banks reacted to the adverse shock by bringing interest rates to zero once again, the governments of many affected countries implemented a number of tax relief and tax deferral measures. Analysing the effects of tax cuts and the interplay between the level of debt and the stance of monetary policy is a key issue in the current economic debate.====What are in general the effects of a temporary and pure change in government debt on economic activity? It is well known in the literature that, assuming lump-sum taxes, a standard infinitely-lived representative agent (ILRA) model provides a very simple and disarming answer to this question: none. Ricardian equivalence would hold in such a model, so deferring lump-sum taxes would affect the economy only if (and to the extent that) taxes are distortionary. It is also well-known that in an overlapping generations (OLG) framework, the Ricardian equivalence does not hold because taxes levied at different times affect different cohorts, thus a pure deferral of taxes has a positive impact on private aggregate demand. As such, an increase in the level of public debt, caused by a cut in lump-sum taxes, would have a positive effect on output. This is the effect we want to study. One could consider the debt multiplier as a complement to the fiscal multipliers that have so far been discussed in the literature, mainly derived in an ILRA framework. The debt multiplier would be the additional multiplier that one can get if any temporary fiscal policy measure would be financed through a pure temporary increase in public debt (i.e., a postponement of lump-sum taxes). The aim of this paper is to analyze this debt multiplier.====Hence, we move away from the ILRA assumption and adopt an OLG setting. In particular, we investigate two questions: (i) How does the debt multiplier depend on the ==== of public debt? (ii) Is the debt multiplier higher or lower in crisis time, that is, in a situation where there is a big negative demand shock so that the nominal interest rate hits the zero lower bound (ZLB)?====A clarification at this point is needed. What we call “debt multiplier” is similar to what is often referred to in the literature as “transfer multiplier”. Giambattista and Pennings (2017) and Mehrotra (2018) focus on the analysis of the transfer multiplier in two-agents models (respectively with a hand-to-mouth/Ricardian agents economy and with saver/borrower consumers====). Notably, these are alternative ways of breaking the Ricardian equivalence. By using a different denomination, we would like to stress the role of debt as a fiscal instrument, and the idea that government debt is households’ financial wealth, thus it is part of the supply of assets affecting the equilibrium real interest rate, as in Ascari and Rankin (2013).====Regarding question (i), in an ILRA framework the steady state real interest rate is pinned down by the subjective discount rate of the representative agent’s utility function. Instead, in an OLG framework the real interest rate also depends on the amount of assets in the economy. The larger the amount of assets, the higher will be the steady state real interest rate needed for the OLG agents to be willing to hold those assets. Since government bonds represent net wealth in an OLG framework, an increase in the level of debt means an increase in the level of the real interest rate at steady state.==== Question (ii) arises naturally from the literature on fiscal multipliers, which shows that fiscal multipliers are larger when the nominal interest rate is stuck at the ZLB.====This paper presents two lines of analysis. First, following Eggertsson (2011), we provide analytical results about the sign of the debt multiplier by considering a linearized version of a simple OLG model.==== Our findings indicate that the debt multiplier is positive even if the tax reduction is totally reversed in the future. We also find that debt multipliers increase with the level of debt, because of the positive relationship between fiscal multipliers and steady state real interest rates. Hence, the starting level of public debt affects the debt multiplier, especially during a ZLB episode. In this respect, the calls for fiscal consolidation in recession times seem ill-advised. Second, to account for the presence of non-linear dynamics (see, Lindé and Trabandt, 2018), we take the model in its original non-linear form and run a series of simulation exercises to quantify the debt multiplier. In normal times, or in a period of mild recession in which nominal rates remain positive, the multiplier is generally quite small, unless we shut off the wealth effect on labor supply. The debt multiplier is larger when the recession is severe and interest rates drop to zero. This reinstates the importance of fiscal stimulus when conventional monetary policy is impotent. Quantitatively, three elements are important for the size of the debt multiplier: the average life length (i.e., the survival probability), the income effect on labor supply and the persistence of the debt expansion. If we set the survival probability to 0.95, use Greenwood et al. (1988) (henceforth, GHH) preferences, and consider a permanent expansion in debt, the debt multipliers in our model is equal to 0.77 on impact, not too distant from the estimates of the government spending multipliers reported by the recent literature. Finally, we also show that a high steady state level of debt could provide monetary policy with more room for manoeuvre in case of a deflationary shock, because it increases the steady state nominal rate, through a rise in the steady state real rate, for any given steady state inflation target.====The paper is organized as follows. Section 2 presents some empirical evidence relating long-term real interest rates to the level of debt-to-GDP. Section 3 describes the OLG model using two different specifications for households' preferences. Section 4 contains the analytical derivation of debt multipliers. Section 5 investigates numerically the size of multipliers and the relation with the level of debt. Section 6 concludes.",The public debt multiplier,https://www.sciencedirect.com/science/article/pii/S0165188921001391,8 August 2021,2021,Research Article,180.0
"Coenen Günter,Montes-Galdón Carlos,Schmidt Sebastian","European Central Bank, Frankfurt 60640, Germany","Available online 8 August 2021, Version of Record 27 October 2021.",https://doi.org/10.1016/j.jedc.2021.104205,Cited by (13),"The secular decline in the equilibrium real ==== and economic activity, as well as heightened ","Over the past decade, monetary policy in advanced economies has operated in an environment characterised by record-low nominal interest rates and disinflationary pressures that have, in most cases, kept inflation rates firmly below central banks’ targets. This configuration has unfolded against the background of a secular decline in the global equilibrium real interest rate which has severely reduced the room for monetary policy to lower policy rates in recessions without hitting the effective lower bound (ELB) on nominal interest rates.==== The persistent and global nature of these developments has lead to a broad-based re-assessment of, on the one hand, the incidence and severity of ELB episodes, and, on the other hand, the effectiveness of available monetary policy instruments and alternative monetary policy frameworks in achieving satisfying macroeconomic stabilisation outcomes in the presence of the ELB.====Our paper provides a model-based analysis of these considerations with a focus on the euro area economy. First, we quantify the impairment in macroeconomic stabilisation induced by the ELB. In so doing, we consider a set of widely-used benchmark interest-rate rules and document how the ELB gives rise to downward biases in and heightened volatility of inflation and economic activity. Second, we explore to which extent interest-rate forward guidance and large-scale asset purchases—two nonstandard monetary policy instruments that have been employed by an increasing number of central banks—can curb the distortionary effects due to the ELB. Third, and finally, we assess the capacity of make-up strategies under which the central bank promises to make up for past inflation shortfalls by generating higher inflation in the future to improve macroeconomic stabilisation in a low-interest-rate environment with an occasionally binding ELB.====The analysis is based on the recent extension of the ECB’s New Area-Wide Model II (cf. Coenen et al., 2018), henceforth referred to as NAWM II. The NAWM II is an estimated dynamic, stochastic, general equilibrium (DSGE) model of the euro area as a whole. The model incorporates a rich financial sector that allows for (i) accounting for a genuine role of financial frictions in the propagation of economic shocks as well as macroeconomic policies and for the presence of shocks originating in the financial sector itself, (ii) capturing the prominent role of bank lending rates and the gradual interest-rate pass-through in the transmission of monetary policy in the euro area, and (iii) providing a structural framework useable for assessing the macroeconomic impact of the ECB’s large-scale asset purchases conducted in recent years.====To assess the ramifications of an occasionally binding ELB constraint for macroeconomic stabilisation in a low-interest-rate environment, we conduct stochastic simulations with the NAWM II and present summary statistics of the steady-state distributions for inflation, the output gap and the short-term nominal interest rate. When the short-term nominal interest rate is the only effective monetary policy instrument and the monetary policy strategy is represented by a simple interest-rate feedback rule commensurate with conventional inflation targeting, the ELB can cause substantial macroeconomic costs as reflected in negative biases in inflation and the output gap, a positive bias in the interest rate, and heightened volatility in inflation and the output gap. This result is robust across a range of widely-used benchmark interest-rate rules. Under our preferred benchmark rule, the ELB is binding one-fourth of the time with an average duration of about 12 quarters, leading to a 0.5 percentage-point shortfall of the inflation mean below the central bank’s target, and a doubling in the standard deviation of inflation (relative to the counterfactual case without the ELB). Likewise, the output gap has a negative mean, and is more volatile than without the ELB. When the long-run equilibrium real interest rate is lower than in our benchmark setup, the incidence of ELB episodes increases markedly, and the negative stabilisation biases and heightened economic volatility get amplified.==== ====Expanding the analysis to allow for the use of nonstandard monetary policy instruments, notably interest-rate forward guidance and large-scale asset purchases, we find that they can curb, albeit not fully eliminate, the stabilisation costs associated with the ELB, in particular, if they are used jointly. In our preferred specification, interest-rate forward guidance is only imperfectly-credible with the private sector. In this case, asset purchases can help to increase the credibility of forward guidance via a signalling channel. Under such enhanced interest-rate forward guidance with asset purchases, we find that the negative inflation bias is 0.2 percentage point smaller, in absolute value, and the volatility of inflation is 0.4 percentage point lower, than in the case where the central bank does not use the two nonstandard instruments. Similar improvements are achieved in terms of the output gap bias, which is 1.4 percentage points smaller, in absolute value, than in the case without nonstandard instruments, and in terms of the volatility of the output gap.====Finally, we find that make-up strategies, notably price-level targeting and average-inflation targeting with a sufficiently long averaging window can largely undo the negative biases and heightened volatility induced by the ELB. For instance, under a permanent and symmetric average-inflation targeting rule with an 8-year averaging window, the negative inflation bias shrinks to 0.1 percentage point—0.4 percentage point smaller, in absolute value, than under the benchmark interest-rate rule without a make-up element—and the standard deviation of inflation is even lower than in counterfactual simulations without the ELB where interest-rate policy is governed by the benchmark rule. Likewise, the output gap bias is relatively small, 3.0 percentage points smaller than under the benchmark rule. We also find that temporary price-level targeting and asymmetric average-inflation targeting can be about as effective as strategies with a permanent and symmetric element. The noticeable improvement in stabilisation outcomes associated with the considered make-up strategies is due to their reliance on two key expectation channels. One is that by committing to an inflation overshoot, the central bank is, in effect, committing to a lower-for-longer interest-rate policy. Expectations of future short-term interest rates should get incorporated into longer-term rates and thereby provide additional accommodation. The other channel is that by committing to overshooting, the make-up strategies should boost inflation expectations. Those higher inflation expectations reduce ex-ante real rates, again providing additional accommodation.====Our paper is related to several (interrelated) strands of the literature on monetary policy and the ELB. In light of the volume by which this literature has grown in recent years, our discussion has to be selective. First, focusing on the U.S. economy, Chung et al. (2012), and Kiley and Roberts (2017), among others, find that both the incidence and the severity of episodes with a binding ELB is higher today than prior to the Global Financial Crisis.====Second, a large and growing strand of the literature assesses the capacity of alternative nonstandard monetary policy instruments to stabilise the economy when faced with the ELB. Eggertsson and Woodford (2003) show that optimal monetary policy under commitment uses state-contingent interest-rate forward guidance to steer the economy when the ELB is binding. Cúrdia and Woodford (2011), Gerali et al. (2010) and Gertler and Karadi (2011), among others, show that in the presence of financial frictions, large-scale asset purchases can mitigate economic downturns when short-term nominal interest rates are at or close to the ELB. Reifschneider (2016), Kiley (2018), Mouabbi and Sahuc (2019) and Chung et al. (2019), among others, provide quantitative, model-based assessments of the effectiveness of these nonstandard instruments. Coenen et al. (2020) is most closely related to this part of our paper. Like us, they use the NAWM II to analyse interest-rate forward guidance and large-scale asset purchases, though under a slightly different parameterisation of these policies. In contrast to our paper, they study the interactions of these nonstandard monetary policy instruments with state-contingent fiscal policy interventions.====Finally, a third strand of the literature aims to explore whether in a low-interest-rate environment alternative monetary policy frameworks could help achieve better stabilisation outcomes than the currently predominant inflation-targeting paradigm. Make-up strategies, in particular, are often considered as a possible means to improve upon the stabilisation properties of conventional inflation targeting in the presence of the ELB.==== Reifschneider and Williams (2000) proposed an otherwise standard interest-rate feedback rule that “memorises” the sum of past shortfalls in interest-rate cuts due to a binding ELB, and aims to make up for the accumulated interest-rate shortfalls by keeping interest rates low for longer. More recently, Nakov (2008), Mertens and Williams (2019), Bernanke et al. (2019), Amano et al. (2020) and Arias et al. (2020) have studied average-inflation-targeting and price-level-targeting rules in the context of the ELB, whereas Budianto et al. (2020) analyse average-inflation targeting for the case where the monetary policy strategy is formalised as an objective function.==== These studies employ either relatively stylised macroeconomic models, or models of the U.S. economy. In contrast, we assess make-up strategies in an estimated model of the euro area. Furthermore, while most studies consider only make-up rules with a permanent and symmetric make-up element, we, like Arias et al., 2020, also consider make-up rules with either a temporary or an asymmetric make-up element.====The remainder of the paper is organised as follows. Section 2 documents, for a set of commonly used benchmark interest-rate rules, that the ELB constraint makes it more difficult for a central bank to achieve satisfactory macroeconomic stabilisation. Section 3 analyses the effectiveness of interest-rate forward guidance and large-scale asset purchases to mitigate the distortionary effects due to the ELB. Section 4 extends the analysis to make-up strategies, and Section 5 concludes. The Online Appendix provides a more detailed description of NAWM II and the methods used for solving and simulating the model, as well as detailed simulation results.",Macroeconomic stabilisation and monetary policy effectiveness in a low-interest-rate environment,https://www.sciencedirect.com/science/article/pii/S0165188921001408,8 August 2021,2021,Research Article,181.0
"Pfeiffer Philipp,Roeger Werner,Vogel Lukas","DG ECFIN, European Commission, Belgium,DIW, EIIW, Germany, and VIVES, KU Leuven, Belgium,IRES, UC Louvain, Belgium","Available online 8 August 2021, Version of Record 27 October 2021.",https://doi.org/10.1016/j.jedc.2021.104210,Cited by (0),"This paper analyzes optimal fiscal policy when the rate at which governments can borrow changes persistently. To analyze trade-offs, we allow for fiscal distortions and productive government spending and characterize the optimal mix between spending and revenue measures in a low rate environment. We find that low ==== for as long as the shock persists. The associated financing needs should be met by a small increase in government debt and a temporary capital ====.","Interest rates on private and public debt in industrialized countries have been low in recent years. Macroeconomists and policymakers have discussed the implications from two different angles. The first line of research has highlighted implications for short-term macroeconomic stabilization policy, notably the constraints on monetary policy at the effective lower bound (ELB) on nominal interest rates, and the rather favorable conditions for fiscal policy (lower borrowing costs, higher multipliers) under the same circumstances (e.g., Blanchard and Summers 2017, Eggertsson 2010). The second debate takes a long-term perspective and argues that lower borrowing costs provide a good opportunity for increasing public investment in order to promote productivity growth and the green and digital transformation (e.g., IMF, 2020). This perspective is linked to the ==== of public finance (associated with Musgrave 1939, (Musgrave 1959), which stipulates that the government may borrow only to invest in projects that will pay off in the future.====Our paper belongs to the second line of analysis. We investigate how low rates affect intertemporal budget balances and the paths of taxation and government spending, and we explore implications for optimal policy with commitment. The analysis shows how low interest rates change the optimal policy mix between spending and revenue measures. Low interest rates provide an opportunity to the Ramsey planner to reduce distortions in the economy, which, in our case, is distortionary labor taxes. In addition, there may be a shift towards public investment, which is productivity-enhancing in our model.====Contrary to the policy debate's emphasis on public investment, the theory discussion, notably Blanchard (2019), has focused on government transfers and their welfare implications in a low-rate environment. There is no paper, to our knowledge, that investigates the consequences of low interest rates for the optimal mix of public investment and tax distortions in a structural macroeconomic model.====Analyzing the implications of low interest rate for the optimal level of public investment in the absence of lump-sum taxes is the contribution of the following pages. In other words, we discuss the government's optimal use of the ‘windfall’ gain from (temporarily) lower financing costs. Should the government use lower financing costs to increase (productive) spending or, instead, lower distortionary taxes? The level of public debt would tend to increase in both cases. Or should it use the fiscal space associated with a lower interest burden to reduce the outstanding amount of debt?====Caballero et al. (2017a), OECD (2019) and Summers (2018) have advocated increases in spending on public investment to stimulate demand, increase the supply of safe assets, and improve the economy's productive potential in the longer term. The alternative policy option, which we consider in this paper, is to lower distortive taxes. The seminal contributions by Chamley (1986) and Judd (1985) show that capital taxes should be zero in the neoclassical growth model and labor should be taxed, so that using the fiscal windfall for reducing capital taxes would be a likely outcome of optimal policy, although Straub and Werning (2020) demonstrate that the zero-tax result does not hold with generality. We exclude capital tax reduction and consider a baseline economy in which the only distortive tax is a tax on labor. We concentrate on labor taxation, given its link to labor market reforms and employment support, as surveyed in Meyermans et al. (2020). The option of reducing the labor tax fits, therefore, well into the discussion on how to use fiscal policy to support economic reforms.====We analyze the optimal fiscal policy response under commitment to a temporary, but persistent decline in the government bond rate in a standard neoclassical growth model. The model defines the (long-run) core of DSGE models, stripped off nominal and real rigidities. In particular, we use the model to characterize the optimal combination of higher productive public investment, lower distortionary labor taxes, and, by implication, the level of government debt in response to a reduction in financing costs in an economy in which lump-sum transfers or taxes are unavailable. The exclusion of lump-sum transfers or taxes provides a realistic policy setting, in our view. Non-productive government consumption is kept constant.====Our model is similar to Baxter and King (1993) and Leeper et al. (2010), who introduce productive government spending and distortionary taxes in a RBC framework. These models capture interactions between public and private capital and labor. Our focus differs from these two papers, however. We explore optimal Ramsey policy rather than the economy's response to exogenous fiscal shocks. Ramey (2020) derives the optimal level of public capital in a neoclassical growth model, but in a setting with lump-sum taxes and without allowing for different interest rates across different assets. Our model distinguishes three (four) types of assets with respective returns, i.e. government bonds, private bonds that are traded among households, and private physical capital (as well as public capital). We show that the optimal fiscal policy prescriptions can be very different, depending on which regime (declining government bond rates alone, or decline in interest rates in a wider sense) applies.====We find that the desirability of higher public investment in our simulations depends on the nature of the low-rate environment. Low interest rates for government bonds alone are no sufficient argument for higher public investment, since they do not affect the rate of return to public investment, which remains determined by the discount factor of private households. The optimal policy, in this case, rather uses the windfall gain from low government financing costs to permanently lower the level of distortions in the economy, which in our model means lowering the labor tax. In a generalized flight-to-safety environment, to the contrary, where households ask for a premium for holding physical (private) capital, it becomes optimal to increase public investment. A wedge appears between the rate of return on public capital and private capital in this scenario, which makes it optimal to increase the public-to-private capital ratio as long as the flight-to-safety shock persists. The financing needs that are associated with a temporary increase in public investment should be met, at least partly, by an increase in government debt and a temporary tax on capital.====The paper proceeds in five additional sections. Section 2 recalls stylized facts to motivate the scenarios in the paper. Section 3 develops our RBC-type model with productive government spending and distortionary taxes. Section 4 describes the optimal policy problem. In Section 5, we present simulation results for the optimal fiscal response to persistently declining bond rates. Section 6 summarizes the findings and concludes.",Optimal fiscal policy with low interest rates for government debt,https://www.sciencedirect.com/science/article/pii/S0165188921001457,8 August 2021,2021,Research Article,182.0
"Kumhof Michael,Wang Xuan","Bank of England, Room TS-03-414, Threadneedle Street, London EC2R8AH, UK,Vrije Universiteit Amsterdam and Tinbergen Institute, De Boelelaan 1105, Amsterdam 1081 HV, The Netherlands","Available online 6 August 2021, Version of Record 27 October 2021.",https://doi.org/10.1016/j.jedc.2021.104208,Cited by (3),", the Phillips curve is very flat at the ZLBD. This is because ==== has far larger effects on output relative to inflation, and inflation feedback rules stabilize output less effectively than rules that also respond to credit. For post-COVID-19 policy, this suggests urgency in returning inflation to targets, caution with negative policy rates, and a strong influence of credit conditions on rate setting.","Since the Global Financial Crisis, there has been renewed interest in understanding the role of bank financing and deposit creation in the transmission of monetary policy. In the policy arena, an appreciation of the importance of bank financing has helped with the design of new tools for crisis management. For example, in combating the economic effects of COVID-19 on struggling businesses, the UK government unveiled a package of £330bn in loans, totaling around 15% of UK GDP, to be made available through the banking system with the help of the Bank of England. The motivation was in large part to help overcome the reluctance of banks, under such severe economic conditions, to lend and create the purchasing power that is necessary for businesses to invest and grow. Meanwhile, in academia a growing theoretical literature is reintroducing the notion of bank financing or inside money creation, specifically the creation of deposits through the disbursement of bank loans, into macroeconomic banking models.==== Recent contributions include Jakab, Kumhof, 2015, Jakab, Kumhof, 2020, Faure and Gersbach (2017) and Donaldson et al. (2018). Jakab and Kumhof (2020) show that aggregate banking sector balance sheets in key economies exhibit very large and rapid quarter-on-quarter changes, and that these are driven almost exclusively by changes in bank lending rather than by changes in bank securities holdings or in aggregate non-bank saving. They show that the bank financing channel is consistent with these phenomena====This new work is well suited to complement existing New Keynesian models, by providing a more complete framework where both the financing channel and the traditional real interest rate channel of monetary policy transmission can be studied simultaneously. Our paper therefore incorporates deposit use, via deposits-in-advance constraints that apply to all economic transactions, and deposit creation, via bank credit whose magnitude depends on banks’ net interest margin (financing channel), into an otherwise canonical infinite horizon New Keynesian DSGE model with sticky nominal goods prices, and with a role for monetary policy responses to inflation that affect the real interest rate (real interest rate channel).====While our theoretical focus is the incorporation of the financing channel into New Keynesian DSGE models, our policy focus is the post-crisis low interest rate environment. As suggested empirically by Bech and Malkhozov (2016), Lucas et al. (2019), Goodhart and Kabiri (2019), and a number of other references cited in Section 2.4, lowering policy rates further when they are already very low can have detrimental effects. The novel contribution of our model is its emphasis on the contractionary effects of insufficient creation of purchasing power by the banking sector. In this model, both individual and aggregate purchasing power can be increased beyond prior income through deposit creation, and as a result bank financing is a far more critical determinant of macroeconomic outcomes than in standard banking models. This carries very important and timely messages for policy design, including for the specification of monetary policy rules.====While the literature has mostly emphasized the role of the zero lower bound on policy rates (ZLB), the key role in our paper is played by the zero lower bound on deposit rates (ZLBD). The relevance of the ZLBD friction is emphasized by the empirical work of Heider et al. (2019), who show that commercial banks generally do not push deposit rates into negative territory====, and that while central banks can drive the policy rate to very low levels and even below zero, they are reluctant to do so because of the effects of the ZLBD on bank profitability. This is because the policy rate is generally the reference rate for lending rates, so that at the ZLBD it directly affects the net interest margin (NIM), the difference between lending rates and deposit rates. In the data the NIM is generally positive even when the policy rate has reached the ZLB, because of lending margins whose size could for example depend on the market power of banks in the loans market as in Gerali et al. (2010). But the NIM may nevertheless be compressed when deposit rates have reached the ZLBD and the policy rate is sufficiently low.==== In that case, when the policy rate is lowered further, the NIM and thus banks’ incentive to lend are also reduced further. This exact concern has played a critical role in many recent policy decisions by central banks. For example, in 2016 the Bank of England voiced concerns that absent additional policy measures further reductions in policy rates might not be fully transmitted to the real economy, due to perverse effects on lending caused by an erosion in banks’ NIMs near the ZLBD (Bank of England, Bank of England). The rate cut decision at that time was therefore complemented by the introduction of the Term Funding Scheme (TFS), which provided low-interest funding and additional lending incentives to banks. More recently, Bank of England (2020) and Ramsden (2020) have reemphasized that very low rates have knock-on effects on banking system profitability that need to be taken into account when deciding on future policies.====Our model introduces a bank-based monetary payment system into an otherwise standard environment with representative firms and households and a government. Households consume, supply labor and accumulate capital, firms own a technology to produce output using capital and labor, and government spends, taxes labor and issues bonds. Banks provide the economy’s payment technology whereby all agents interact with each other through a sequence of deposits-in-advance constraints that cover each payment. Simplifying for the purpose of exposition, the intra-period sequence of events is as follows: Firms obtain bank loans that are disbursed to them in the form of new deposits. Firms then use their new deposits to purchase capital and labor from households - inside money enters the economy. In the next step output is produced. Households then use the deposits that they acquired in exchange for labor and capital in order to purchase this output for consumption. Firms therefore receive their sales revenue in the form of deposits. Finally, firms use these deposits to repay their loans - inside money exits the economy. In the model this happens for an instant at the end of each period. In the real world many such payment cycles overlap, so that bank balance sheets never vanish.====Banks are unique in their ability to offer a payment system, because only banks are able to credibly commit to honoring their IOUs vis-à-vis any subsequent holder, thereby making these IOUs acceptable as a universal medium of exchange. The main reason why banks have this unique status is their support by the system of central banks and regulatory agencies (Goodhart, 1988). In our model bank deposits are therefore risk-free, and are the only circulating medium of exchange.====We model deposits-in-advance constraints in a similar manner to Shapley and Shubik (1977) and Lucas and Stokey (1987). Bank deposits are sometimes referred to as inside money, which together with outside money and interest rate rules help to establish equilibrium existence and nominal determinacy in a general equilibrium with incomplete markets (Dubey, Geanakoplos, 2003, Dubey, Geanakoplos, 2006). In practice, and in our model, when central banks target nominal interest rates, all central bank money is inside money. Outside money is not required for determinacy, which is instead established as in standard New Keynesian models, at the cashless limit and through a policy interest rate rule in the presence of sticky goods prices. However, the presence of commercial bank money affects determinacy regions, especially at the ZLBD.====In this model, two features are important for the determination of interest rates. First, banks can sell their loans to the central bank, to obtain reserves that pay the policy rate. By arbitrage, the interest rate on loans must therefore equal the policy rate. This is in line with the empirical evidence in Ippolito et al. (2018), who show that the floating rates of bank loans are tied to monetary policy rates. Second, banks incur a convex cost of making loans, and at the margin the NIM, the difference between loan and deposit rates, has to be sufficient to cover this cost. By arbitrage, the deposit rate in our model must therefore be lower than the policy rate. This is in line with the US empirical evidence in Drechsler et al. (2017), who document that there is a wide spread between the Fed funds rate and the household deposit rate.====We compare the behavior of the ZLBD-constrained economy to that of an otherwise identical unconstrained economy. The key difference is in the role played by banks’ loan supply curve, which relates the amount of credit banks are willing to extend to the NIM they are able to earn. In the unconstrained economy, deposit interest rates adjust to achieve the spread that banks require in order to cover the cost of making the loans. As a result, the NIM is very stable, and the creation of loans and deposits responds highly elastically to changes in credit demand. Solving for the equilibrium of this economy is straightforward, as the deposit rate clears the credit market without credit rationing. In this unconstrained economy, consistent with the empirical findings of Drechsler et al. (2017) for the (pre-ZLBD) sample period 1994–2014, a fall in the policy rate increases deposits and loans, albeit for different reasons.==== At the ZLBD, deposit interest rates cannot adjust further, and any change in the policy rate leads to a one-for-one change in the NIM. This change is exogenous for banks, and it exogenously changes their ability to cover the cost of making loans.==== As a result, the creation of loans and deposits responds highly inelastically to changes in credit demand. Solving for the equilibrium of the ZLBD-constrained economy applies the concepts of equilibria with quantity rationing of Bénassy, 1990, Bénassy, 1993 and Dréze (1975). Specifically, the loan supply curve becomes a credit rationing constraint, because the NIM cannot increase to allow banks to satisfy borrowing firms’ additional demand for credit. Borrower optimization problems that are subject to constraints imposed by lender behavior are very common in the literature, with Carlstrom and Fuerst (1997) and Bernanke et al. (1999) perhaps the most well-known examples. In this ZLBD-constrained economy, consistent with the empirical findings of Drechsler et al. (2018)==== for the US, and of Bittner et al. (2020) for Germany, a fall in the policy rate tends to reduce the NIM, and thereby deposits and loans.====However, some caveats apply when comparing our theoretical model results to results in the empirical literature. First, we focus on the ZLBD-constrained case, which in US data would only be relevant from around 2014. A large part of the empirical literature studies the pre-ZLBD period. Second, the deposits of our model are exclusively created through working capital loans, which are very important for many firms, but which are only a subset of aggregate deposits. The empirical literature typically does not study this subset. Third, once created, deposits can be used for multiple transaction types, which would make it very hard to identify working capital deposits.====We use our model to show that at the ZLBD the output-inflation trade-off is dominated by a very flat steady state Phillips curve. The reason is the strength, relative to the real interest rate channel, of the financing channel, whereby reductions in inflation that lead to reductions in nominal interest rates must also reduce NIMs. We find that lower NIMs have large negative effects on credit and output, while at the same time acting as cost-push shocks that limit the overall drop in inflation.====Two model ingredients are responsible for the strong output response to lower NIMs. The first ingredient is the modelling of money demand through deposits-in-advance constraints, which implies a tight connection between corporate credit and real activity. This modelling choice is strongly supported by the data. Fig. 1 shows quarterly data for the US, Japan and Spain, for the period from 1990 to 2019. It shows that periods of reductions (increases) in the growth rate of corporate credit, and especially of absolute reductions (increases) in corporate credit (values below zero on the left-hand axis), coincide with periods of strong increases (decreases) in the unemployment rate.==== The correlations between the two series are 0.52 for Japan, 0.63 for the US, and 0.74 for Spain. In this context an important question is whether equity share issuance could substitute for bank financing to obtain the initial liquidity needed for input purchases. The key observation is that equity is purchased using existing money, not physical resources. In the model all money that is created at the beginning of the period is created for firms, who are therefore the only possible hypothetical buyers of equity shares, but this easily generalizes to the case where households can invest in equity shares using deposits of their own. Therefore, if a firm A obtained its initial money stock by way of equity issuance to a firm B rather than debt issuance to banks, then firm B would have to replace the missing money by borrowing more, because it cannot pay for its own input purchases using equity shares. The aggregate need for bank credit would therefore be undiminished, unless we make an additional and ad hoc model change whereby equity issuance for some reason increases the velocity of circulation of money. Very similar comments apply to retained earnings invested in liquid assets, which for a given amount of bank credit can only increase one firm’s liquid assets at the expense of other firms. The key is that, at the macro level, the only significant way to increase liquidity is additional bank lending.====The second ingredient that is responsible for the strong output response to lower NIMs is the semi-elasticity of credit supply with respect to the NIM. Our calibration is based on a combination of values used in the existing literature and our own estimation based on US data. In the literature, Cúrdia and Woodford (2010) use a semi-elasticity of 10, in other words bank credit contracts by 10% when NIMs decline by 1 percentage point, while Drechsler et al. (2017) estimate that same semi-elasticity at 5.3. Our own IV estimate of 10, reported in the appendix, is identical to Cúrdia and Woodford (2010). We will use 10 as our calibration for the short-run elasticity, and 5 as our calibration for the long-run elasticity. The reason for the lower long-run value is that banks can partially adjust to lower NIMs over time, for example through a greater share of fee-earning activities. These values imply that, at the ZLBD, the financing channel of monetary policy is very strong relative to the real interest rate channel. Output responses to shocks near the ZLBD are therefore significantly amplified relative to the unconstrained economy.====The determinants of inflation in the ZLBD-constrained economy include not only user costs and wages but also the multiplier of the credit rationing constraint. Disinflationary shocks and the accompanying drops in the policy rate are therefore characterized not only by a drop in user costs and wages but also by much tighter bank financing conditions, or external cash flow. This is inflationary rather than deflationary, by giving producers an incentive to generate sufficient internal cash flow through higher prices. This implies that the inflation response to shocks at the ZLBD is much weaker than in the unconstrained economy.====We use this framework to establish the following results for ZLBD-constrained economies. First, monetary policy has far larger output effects and far smaller inflation effects than in unconstrained economies. Over the medium term, even a small permanent increase in the nominal policy rate due to higher steady state inflation facilitates a large permanent expansion in credit and output. The current efforts of central banks to return inflation rates to their medium term targets are therefore extremely important. In the meantime, even a modest temporary monetary easing facilitates a sizeable temporary expansion in credit and output at a modest cost in terms of inflation. While the argument that higher inflation can help to get an economy out of a deep recession is not new, our transmission mechanism, from inflation to nominal interest rates to NIMs to deposit creation to economic exchange to real activity, is new. Second, the ==== output effects of Taylor-type changes in monetary policy rates in response to changes in inflation are the opposite of unconstrained economies. For example, a reduction in policy rates in response to lower inflation following a contractionary demand shock makes the shock more rather than less contractionary. This suggests that central banks should exercise great caution in pushing policy rates towards negative territory. Third, modifications of monetary policy rules that emphasize responses to reductions in credit through monetary easing, while maintaining Taylor-type responses to inflation, make monetary policy far more effective at stabilizing output, consumption and hours worked. This suggests that central banks should not only pay attention to the interactions of their rate setting decisions with inflation, but also with credit conditions.====Policy at the ZLBD therefore has new and significantly stronger levers. The reason is that, with the financing channel, the level of aggregate expenditure is determined directly by the quantity of bank-created purchasing power, so that, consistently with Fig. 1, bank financing becomes a far more critical determinant of macroeconomic outcomes than in other macroeconomic models. Without the financing channel, an individual agent’s purchasing power is constrained by a traditional budget constraint, which says that an agent’s purchasing power must equal that agent’s prior income plus income transferred from other agents through non-bank borrowing. However, because any debt-financed increase in the purchasing power of the borrower is offset by the diminished purchasing power of the lender, at the aggregate level purchasing power is necessarily constrained by prior income. The key insight is that in modern financial economies, with widespread access to bank credit, non-banks are not constrained in this way. Instead, their purchasing power is constrained by their access to deposits. Deposits in turn equal the sum of prior income received in the form of deposits, deposits transferred from other agents through non-bank borrowing, and crucially, net new deposit creation through new bank borrowing. New deposits are created in ledgers, through an equal increase in the assets and liabilities of the banking sector, and are therefore not dependent on prior income.==== Therefore, any agent that has access to credit faces a deposits-in-advance constraint, which allows for net new deposit creation. This does not imply that bank credit allows agents to violate the economy’s overall resource constraint, which is a key equation of the model. Rather, the creation of additional deposits permits a mobilization of additional resources that would otherwise have remained idle. This increases real incomes, especially when the economy is financially constrained. And to the extent that it does not increase real incomes, it increases inflation.====We also emphasize another feature of our model, the endogenous strong comovement between consumption and investment at the ZLBD. The reason is that the ZLBD constrains the overall quantity of deposits, with its allocation between consumption and investment purchases left to the market. An overall shortage of deposits will therefore tend to affect both sectors in the same way.====Discussions of banking still frequently appeal to the deposit multiplier model, which argues that the size of bank balance sheets is a multiple of the policy-determined quantity of central bank money. However, modern central banks invariably target interest rates. During normal times they are therefore committed to supplying as much cash and reserves as households and banks demand at that rate, while during a QE period the quantity of reserves exceeds banks’ demand, and is therefore also not a direct determinant of the quantity of private money creation in the sense of the deposit multiplier model, while cash remains demand determined. Therefore, in our model cash is not present at all, while the quantities of both deposit money and central bank reserves are determined by the interaction of the profit-maximizing decisions of banks and their customers.====The rest of the paper is organized as follows. Section 2 reviews the related theoretical and empirical literatures. Section 3 develops our theoretical model. Section 4 studies illustrative simulations based on this model. Section 5 concludes. The appendix presents empirical evidence on the semi-elasticity of credit supply with respect to the NIM.","Banks, money, and the zero lower bound on deposit rates",https://www.sciencedirect.com/science/article/pii/S0165188921001433,6 August 2021,2021,Research Article,183.0
Kollmann Robert,"Solvay Brussels School of Economics & Management, Université Libre de Bruxelles, CP114, 50 ,. Franklin Roosevelt, Brussels 1050, Belgium,Centre for Economic Policy Research (CEPR), London, United Kingdom,ERUDITE, Université Paris-Est Créteil (UPEC), Créteil, France","Available online 6 August 2021, Version of Record 27 October 2021.",https://doi.org/10.1016/j.jedc.2021.104206,Cited by (2), is assumed. The presence of the ZLB generates multiple equilibria. The two countries can experience recurrent ==== induced by the self-fulfilling expectation that future ,"This paper studies fluctuations of inflation, real activity and the exchange rate in a two-country New Keynesian sticky-prices model. A zero lower bound (ZLB) constraint for nominal interest rates is imposed. When the ZLB binds, i.e. in a “liquidity trap”, the central bank cannot stimulate real activity by lowering the policy interest rate (Hicks, 1937; Keynes, 1936). The recent experience of persistent low interest rates and low inflation in many advanced economies has led to a resurgence of theoretical research on liquidity traps. Two types of liquidity traps have been discussed in the literature: Firstly, an extensive modeling strand building on Krugman (1998) and Eggertsson and Woodford (2003) considers “fundamentals-driven” liquidity traps that are induced by large shocks to household preferences, or to other fundamentals, which sharply reduce aggregate demand and push the nominal interest rate to the ZLB.==== Secondly, Benhabib et al. (2001a,b; 2002a,b) have studied “expectations-driven” liquidity traps, namely liquidity traps that are induced by the self-fulfilling ==== that future inflation will be low; Benhabib et al. show that the combination of the ZLB constraint and an “active” Taylor monetary policy interest rate rule gives rise to multiple equilibria, and that expectations-driven liquidity traps can arise ==== when there are no shocks to fundamentals. Fundamentals-driven liquidity traps have been analyzed in both open- and closed economies;==== by contrast, the literature on expectations-driven liquidity traps has concentrated on closed economies.====The contribution of the present paper is to study expectations-driven sunspot equilibria with occasionally binding ZLB constraints, in ====; a floating exchange rate regime is assumed.==== The paper shows that the cause of liquidity traps matters for the dynamics of the world economy. A model with expectations-driven ZLB regimes is better suited for generating persistent liquidity traps than a theory of fundamentals-driven ZLB regimes. A key finding is that expectations-driven ZLB regimes can either be synchronized or unsynchronized across countries: the cross-country correlation of expectations-driven liquidity traps is indeterminate, and unrelated to the correlation of fundamental business cycle shocks. By contrast, the cross-country correlation of fundamentals-driven liquidity traps equals the international correlation of the shocks triggering those traps. I show that the domestic and international transmission of fundamental business cycle shocks (disturbances to productivity, government purchases and household preferences) in an expectations-driven liquidity trap can differ markedly from shock transmission in a fundamentals-driven liquidity trap.====The model variants with expectations-driven liquidity traps studied here postulate that a country's ZLB regime is ==== driven by agents’ self-fulfilling inflation expectations; in those model variants, fundamental shocks are assumed to be sufficiently small, so that fundamental shocks cannot trigger a change in the ZLB regime. This allows a sharp distinction between expectations-driven liquidity traps and fundamentals-driven liquidity traps (that are induced by large fundamental shocks).====Building on Arifovic et al. (2018) and Aruoba et al. (2018), I consider equilibria with expectations-driven liquidity traps in which the decision rule for inflation depends on the ZLB regime and on the natural real interest rate (i.e. the expected real interest rate that would obtain under flexible prices). The natural real interest rate is stationary. Thus, the inflation rate, in an expectations-driven liquidity trap, is likewise stationary.==== Away from the ZLB, a policy of inflation targeting (implemented via an “active” Taylor rule) ensures that the actual real interest rate tracks the naturel real interest rate. Persistent fundamental shocks only have a muted effect on the natural real interest rate, as the latter is a function of expected growth rates of the fundamental drivers. Away from the ZLB, persistent shocks thus trigger muted responses of inflation. In an expectations-driven liquidity trap, the inflation response to persistent shocks too is muted.====This explains why the transmission of ==== fundamental shocks to domestic and foreign real activity and to the real exchange rate, in an expectations-driven liquidity trap, is similar to transmission when the economy is away from the ZLB, and to transmission in a flex-prices world. In particular, a persistent positive shock to Home country productivity raises Home output, and it depreciates the Home terms of trade and the Home real exchange rate; a persistent positive shock to Home government purchases raises Home output and appreciates the Home terms of trade. For a trade elasticity greater than unity, as assumed in many macro models, the present model with expectations-driven liquidity traps predicts that a persistent rise in Home productivity raises Home net exports and lowers Foreign output, while a persistent rise in Home government purchases lowers Home net exports and raises Foreign output. Domestic and foreign output multipliers of persistent fiscal spending shocks are smaller than unity, in expectations-driven liquidity traps.====A ==== liquidity trap generates very different responses to persistent shocks. Analyzes of fundamentals-driven liquidity traps presented in the literature postulate a baseline liquidity trap scenario in which a large shock to preferences (or other fundamentals) moves the ==== nominal interest rate into negative territory; the liquidity trap ends when the (mean-reverting) unconstrained nominal rate becomes non-negative again (e.g., Cochrane 2017, Erceg and Lindé 2010). Inflation during the fundamentals-driven liquidity trap is determined by iterating the Euler and Phillips equations backward, from the trap exit date. The “backward” dynamics of inflation (during the liquidity trap) is explosive. Therefore, small exogenous shocks that are added to the baseline fundamentals-driven liquidity trap scenario can have big effects on inflation, during the liquidity trap. In the model here, a positive Home productivity shock, occurring during a fundamentals-driven liquidity trap, triggers a sizable fall in Home inflation on impact; this sizable drop in inflation ==== Home output and consumption and ==== the Home terms of trade and the Home real exchange rate. By contrast, a positive shock to Home government purchases induces a sharp rise in Home inflation, which strongly boosts Home output and ==== the Home real exchange rate. The previous literature on fundamentals-driven liquidity traps has highlighted non-standard (topsy-turvy) output responses to productivity shocks, as well as the large fiscal multipliers in fundamentals-driven liquidity traps (e.g., Eggertsson 2010, Eggertsson and Krugman 2012). However, the “unorthodox” response of the real exchange rate to productivity and fiscal shocks has apparently not previously been noticed.====I find that ==== spillovers of fundamental business cycle shocks can be much larger and qualitatively different in fundamentals-driven liquidity traps than in expectations-driven liquidity traps. For a trade elasticity greater than unity, model variants with a ==== liquidity trap predict that a rise in Home productivity lowers Home net exports and raises Foreign output, while a rise in Home government purchases raises Home net exports and lowers Foreign output. These international spillover effects are ==== of those predicted in an expectations-driven liquidity trap, with persistent shocks (see above).====Shock transmission in an expectations-driven liquidity trap is more similar (at least qualitatively) to transmission under a fundamentals-driven liquidity trap, when fundamental shocks are ====. Intuitively, transitory fundamental shocks have a stronger effect on the natural real interest rate than persistent shocks. In a liquidity trap, a transitory shock drives a larger wedge between the actual real interest rate and the natural real interest rate. An “active” Taylor rule implies that, away from the ZLB, the nominal interest rate is cut aggressively in response to a short-lived positive productivity shock, which stabilizes inflation, boosts output and triggers a depreciation in the (nominal and real) exchange rate. In an expectations-driven liquidity trap, the nominal interest rate cannot adjust, which triggers a transitory drop in inflation, a ==== in domestic consumption and output and an exchange rate appreciation. These responses are qualitatively similar to the responses predicted under a fundamentals-driven liquidity trap.====This paper contributes to a burgeoning literature on business cycle models with expectations-driven liquidity traps, but that literature has assumed closed economies (as mentioned earlier). The paper is related to Mertens and Ravn (2014) who showed, in a closed economy model, that the effect of fiscal shocks differs across expectations-driven and fundamentals-driven liquidity traps (fiscal spending multipliers are smaller in an expectations-driven liquidity trap). Given the recent experience of persistent liquidity traps in several major economies (Euro Area, US, Japan), it is important to study the effect of expectations-driven liquidity traps in models of the ==== economy, for a range of domestic and external shocks. This seems especially relevant as models of fundamentals-driven liquidity traps are assumed in influential policy studies that contribute to the ongoing monetary strategy debates in the US and the Euro Area; see, e.g., Andrade et al. (2019, 2021), Coenen et al. (2021) and Erceg et al. (2021). Other recent studies on expectations-driven liquidity traps include Aruoba et al. (2018), Benigno and Fornaro (2018) and Nakata and Schmidt (2020), who also provide detailed references to the literature. By contrast to the paper here, that literature has not identified the key role of shock ==== for the transmission of business cycle shocks, in an expectations-driven liquidity trap.",Liquidity traps in a world economy,https://www.sciencedirect.com/science/article/pii/S016518892100141X,6 August 2021,2021,Research Article,184.0
MARISCAL ASIER,"U. Rovira i Virgili, Department of Economics, and ECO-SOS. Campus Bellissens, Av. Universitat 1, Reus, 43204, Tarragona, Spain","Received 17 April 2020, Revised 26 July 2021, Accepted 30 July 2021, Available online 6 August 2021, Version of Record 23 September 2021.",https://doi.org/10.1016/j.jedc.2021.104213,Cited by (0),"To analyze multinationals’ market entry, I assemble a new dataset of cross-border mergers and acquisitions (M&A) for 1985–2019, with sales for the parent and target firms. Seven main facts emerge: (1) the number of acquisitions per parent has a Pareto distribution; (2) the number of ====) quantitatively matches many of the facts due to the sparsity of M&A data. Three unmatched facts provide information for future theories: parent entry by market popularity, single-market multinational parents’ prevalence and sales share, and assortative matching between parent and target sales.","The activities of multinational firms are a prominent feature of globalization. Table 1 summarizes data on aggregate exports and sales by affiliates of multinational firms. Sales of foreign affiliates more than double the value of world exports. The table suggests that affiliates generate a large percentage of value-added in the host economies—it is 62% for United States manufacturing (Barefoot and Mataloni, 2011). Interestingly, affiliates concentrate on selling to local markets rather than on exporting since only 14% of affiliate sales are exports—reassuringly, in the United States’ Census of Manufacturers, the average across multinational establishments is 18% (see Flaaen, 2014). Together, these facts point to affiliates’ sales being quantitatively more relevant than exports as a means to reach foreign markets. Moreover, Ramondo et al. (2016) show that most foreign affiliates do not trade within the boundaries of the firm, which suggests that market-seeking motives are most fundamental in determining multinationals’ plant location choices.====This paper uses a new dataset on cross-border M&As to provide novel facts on multinational firms, and a stylized probabilistic model to understand their relevance. The dataset contains multinational parents’ cross-border M&As, their global sales, and those of their targets, which I use to analyze their entry patterns into and from multiple industry-country pairs.==== I ask: How do multinational companies enter markets? How are entry patterns influenced by parent, industry, and country characteristics? Is there sorting between the acquiring parent and target sizes? Seven new facts emerge.====First, few parents engage in many M&A deals and most in just a few—more precisely, in log scale, the number of parents with exactly ==== deals is proportional to ====, or the distribution of deals per parent is Pareto distributed. To my knowledge, it is the first time that such a relation has been documented in the context of M&As originating from multiple industries and countries. This is an interesting fact because it is well known that firm size follows a Pareto density, (see, e.g., Axtel, 2001, Gabaix, 2009, Sutton, 1997), and so the data suggests a connection between the two.====The second fact looks at the number of deals in two independent dimensions: industry and country. Interestingly, a Pareto distribution with a similar shape parameter emerges when one looks at either the number of ==== industries or the number of different countries a parent enters. This is surprising because the microeconomic determinants of entry into an industry (e.g., firms’ “core competence,” product innovation, or competition) are different from those of entry into countries (e.g., market potential, institutions, or geographic distance). Nonetheless, parents acquire affiliates in both dimensions with a similar intensity. Because the shape is similar to the number of deals distribution, the result is consistent with a common driving force (firm “ability”) pushing for parents’ expansion in both the country and industry dimensions.====Third, to analyze the determinants of entry into industries and countries, I again open up both dimensions independently. For each dimension, ==== on the parents’ home industry or country, I calculate a destination’s ==== as the number of parents entering a given industry or country, respectively. I find that multinationals that enter less popular destinations are characterized by higher average global sales, a result reminiscent of the French exporters in Eaton et al. (2011), hereafter EKK. To fix ideas, take the industry dimension: conditional on the main business activity of a parent, large parents enter industries where few other parents from their home industry do. The same basic pattern holds for both entry dimensions, industries and countries.====Fourth, across parents, I find a strong positive relation between the number of countries and industries entered. Moreover, parents do ==== replicate their industries in each destination country.====Fifth, I show that entry into new industries and countries is a rare event—the percentage of zero parent-level M&A flows across destination industry-country is 99%. Interestingly, they follow a well-defined spatial pattern: more parents enter countries that have a higher GDP, are geographically closer, or are closer in terms of industry proximity.==== Additionally, more deals occur between industry-countries with more dissimilar value-added.====Sixth, single-industry, single-country, and single industry-country parents are very prevalent, at 56%, 52%, and 46%, respectively, but have a low share of sales among multinationals, at 11%, 11% and 1%, respectively.====Seventh, pooling domestic and international deals, I find positive assortative matching (PAM) between parent and target sales deals. There is a weaker PAM for both international M&As (relative to domestic ones) and interindustry M&As (relative to intraindustry deals).====Using a balls-and-bins model à la Armenter and Koren (2014), hereafter AK, 2014, the paper also contributes by discriminating which of the above facts are informative for structural models. Whenever the model matches a fact, a large class of structural models will be consistent with it due to the sparsity of the data, that is, the fact that the number of observations is low relative to the number of categories (industries/countries) in the analysis. Conversely, whenever the model misses a fact, it informs us of the economic structure that theories need. In fact, AK, 2014 show that the balls-and-bins model provides a lower bound for how well structural models can quantitatively match a fact.====The physical environment is as follows. Take a given number of balls (deals) and randomly assign them to bins (destination industries and countries). Each ball independently lands in a bin, according to some probability. The baseline calibration assumes no systematic relation between parents, industries, and countries: (1) the probability of entering an industry is assumed to be independent of the destination country—each country is expected to receive the same distribution of industries; (2) larger parents are represented by more balls, but each ball has the same monetary value for all parents—there is no intensive margin (or sales per deal is constant); (3) the bin structure is constant regardless of the parent’s number of balls. Hence, taking the distribution of the number of deals and bin sizes as given by the data, I randomly and independently allocate parents’ deals across industries and countries, and construct the model counterparts to the aforementioned data moments.====On the one hand, the model does very well in matching many of the facts due to the sparsity of multinationals’ market entry data. On the other hand, the facts that are informative to discriminate among theories are: (1) parent entry by market popularity, where the model overpredicts the average sales of parents in the least popular markets (industries/countries); (2) the prevalence and sales share of single-industry, single-country, and single-industry-country multinationals, which the model overpredicts; and (3) assortativity between parent and target sales, which is positive in the data but negative in the model.====The literature on multinationals is vast and I can only provide a brief overview. Helpman et al. (2008), Kortum et al. (2011), Chaney (2014), and Ramondo (2014) emphasize the relevance of the extensive margin to understand firms’ (and aggregate) trade or foreign direct investment (FDI). Unlike them, with the model I discriminate which facts on multinationals’ entry into markets are informative. This is important because two workhorse models fail in important dimensions. First, in models à la Eaton and Kortum (2002), without fixed costs, the number of entrants is unrelated to the size of the destination market, a counterfactual result (see Eaton, Kortum, Kramarz, 2011, Ramondo, 2014). Second, in theories à la Melitz (2003)-Chaney (2008), with fixed market entry costs, there is a stark separation between domestic and international firms—however, in the data, multinationals and exporters come from all over the firm size distribution (Bernard, Eaton, Jensen, Kortum, 2003, Flaaen, 2014, Mayer, Ottaviano, 2008) and Armenter and Koren (2015) show that an extremely large amount of randomness is required for models in this class to quantitatively match the size premium of exporters in the data.====Focusing on cross-border M&As, Head and Ries (2008) propose a proximity-ability trade-off to explain multinational entry and use aggregate bilateral M&A flows to estimate a structural gravity relation. Contrary to their approach, I uncover new micro facts about multinational parents and their affiliates. Helpman et al. (2004) propose a model of heterogeneous producers that choose to access a market through exports or FDI when faced with a “proximity-concentration” trade-off. Nocke and Yeaple (2007) analyze the same margin, formalizing mergers as a means to combine both firms’ capabilities. While I provide new evidence on market entry through M&As, I do not study the proximity-concentration trade-off for two reasons. First, I do not observe exports. Second, roughly half this M&A sample is done by parents in the service sector for which Oldenski (2012) shows that, in US data, exports to FDI sales are much less relevant than for manufacturing firms, for which, as aforementioned, the ratio is already low. Nocke and Yeaple (2008) model mergers as a means to assign production divisions to headquarters. An untested prediction of their theory is that parents and targets feature PAM in sales, for which I provide new evidence, and the balls-and-bins model shows it is an informative fact for structural models.====Regarding the empirical evidence, Alfaro and Charlton (2009) is the only reference that has comparable data to this paper, but it uses it in a very different way. They use a large cross section of companies’ ownership to regress measures of multinational activity on bilateral industry-country factor intensity variables in a comparative advantage regression. They conclude that industries’ “horizontal proximity” matters for ownership.==== In terms of the evidence, Eaton et al., 2011 is relevant to this paper. They use export shipment data and sales for French firms to estimate an extended Chaney (2008)-Melitz (2003) model and perform counterfactual analysis. Irarrazabal et al. (2013), henceforth IMO, provide facts à la EKK on Norwegian multinationals, and Chen and Moore (2010) show that more productive French multinationals enter less popular countries. I advance the literature by showing these facts hold with multiple source countries, documenting equivalent industry popularity evidence, and, with the model, establishing that firm entry by market popularity is informative for structural models.====This paper speaks to the literature on the determinants of Pareto distributions. I show that the figures are well approximated by a Pareto distribution, giving, as Gabaix (2009) states, “the hope of robust, detail-independent economic laws.” Also, I empirically show that, starting from a Pareto distribution for parents’ number of deals, and allocating deals into industries and countries according to a multinomial distribution, a similar Pareto shape persists in the number of ==== industries/countries entered—a new result to my knowledge. Geerolf (2017) provides a theoretical explanation for the emergence of Pareto distributions on firm size that is based on deep complementarities in production, rather than ex-ante heterogeneity. Interestingly, the uncovered evidence on PAM between parent and target sales is consistent with his theory.====A recent set of papers deals with the complexity of multinationals’ choices to serve a market. Ramondo and Rodríguez-Clare (2013), Tintelnot (2017), and Arkolakis et al. (2018) allow three market-entry modes: FDI, export, and export-platform FDI.==== While I uncover new market-entry facts at the parent level that are consistent with these models, I cannot speak directly to them because I do not observe exports or imports. Hence, I also cannot speak to the complex sourcing strategies of multinationals (see, e.g., Antràs, 2003, Antràs, Helpman, 2004, Antràs, de Gortari, 2020; Antràs and de Gortari, 2020).====The remainder of the paper is organized as follows. Section 2 introduces the dataset. Section 3 describes the model, and Section 3.1 its calibration. Sections 4 through 6 present the data regularities interspersed with the model’s predictions, with the latter section specifically dealing with data moments that are unmatched by the model, and Section 7 concludes.",Global ownership patterns,https://www.sciencedirect.com/science/article/pii/S0165188921001482,6 August 2021,2021,Research Article,185.0
"Bensoussan Alain,Chevalier-Roignant Benoît,Rivera Alejandro","University of Texas at Dallas City University of Hong Kong, Hong Kong,Emlyon Business School, France","Received 15 June 2020, Revised 20 July 2021, Accepted 25 July 2021, Available online 4 August 2021, Version of Record 6 September 2021.",https://doi.org/10.1016/j.jedc.2021.104203,Cited by (5),"We model the expansion decision of a levered firm. Straight debt distorts both timing and scaling: the firm invests less and later than its all-equity financed counterpart. The inclusion of performance sensitivity in the debt contract mitigates such distortions. Moreover, performance sensitivity is consistent with firm value maximization within a standard trade-off theory of capital structure. As a result, our model rationalizes the widespread use of performance sensitive debt (PSD), especially amongst fast growth firms.","This article studies the optimal exercise of real options with time and scale flexibility when the firm is financed with performance-sensitive debt (PSD). The issuance of hybrid debt instruments, such as PSD, has become popular as an alternative to straight debt, especially for corporate bank loans (see, e.g., Asquith, Beatty, Weber, 2005, Manso, Strulovici, Tchistyi, 2010). A PSD contract stipulates interest payments that are not constant, but depend on a measure of the borrower’s performance. The focus in the literature has been so far on (i) pricing such PSD instruments and (ii) developing rationales for their use. A case in point where our setting would apply would be a pharmaceutical company that decides on when to open another laboratory and how large to build it, aware that the interest on the outstanding debt depends on the firm’s Debt/EBITDA ratio. Asquith et al. (2005) detail Core Laboratories syndicated revolving loan performance-pricing grid. Moreover, it is known since Myers’s (1977) paper that a firm financed with straight debt underinvests compared to its unlevered counterpart; this is because shareholders pay for the upfront cost of investment, but do not fully internalize its benefits (because of a debt overhang). The purpose of this article is to address the following set of interrelated questions: (a) What are the optimal default and capacity expansion policies for shareholders when the firm is financed with PSD? (b) To which extent can PSD mitigate the debt-overhang problem in the timing ==== scale of investment? (c) Can the widespread use of performance-pricing clauses in bank loans (documented by Asquith et al., 2005) be rationalized within a standard tradeoff theory of capital structure (see Kraus and Litzenberger, 1973)?====This paper answers these questions by modeling a firm partially financed with ‘risk-compensating’ PSD (Manso et al., 2010). Shareholders choose an optimal default policy (Leland, 1994) and decide on the exercise of a real option (Dixit, Pindyck, 1994, Trigeorgis, 1996). Optimal leverage and sensitivity of debt are jointly optimized in our setting. That is, we model a setting where the initial shareholders (e.g., entrepreneurs) decide on the best debt contract among a menu offered by their bank. Banks can offer straight debt, but they can also offer debt whose coupon payments depends on the firm’s performance (PSD). We characterize how this financing choice depends on the available investment opportunities the firm has, which we model as expansion real options. We are particularly interested in the role played by PSD in mitigating debt overhang, and how such mitigating effect can rationalize the use of PSD by certain type of firms. To that end, we compute the equilibrium capital structure and investment decision that maximizes shareholder returns subject to the participation constraint of the banking sector.====The firm’s ==== and ==== determine its free cashflows to equity (FCFEs). The management can increase the firm’s scale at a time of its choosing. As in Myers (1977), shareholders finance this expansion project. If the firm is financed with PSD, the management factors in the reduction in debt payments resulting from greater scale when deciding about an expansion.====The timeline is as follows. Ex ante, the initial owners decide on the firm’s capital structure by specifying a debt amount and the performance sensitivity of its debt given a preexisting scale resulting from past investment decisions. That is, the initial shareholders decide which is the optimal debt contract that can be offered to them by the bank. Banks can offer straight debt, but they can also offer debt whose coupon payments depends on the firm’s performance.====The optimal capital structure trades off (a) bankruptcy costs, (b) tax advantage of debt and (c) underinvestment induced by debt. Ex post, given outstanding debt, the management chooses the expansion & default policy that maximizes shareholder value.====Our model helps us derive three novel insights. First, the expansion decision of a firm financed with straight debt is distorted (compared to the decision of its unlevered counterpart) in ==== timing and scaling: the former invests less and later. Our paper is the first to highlight such a distortionary effect in a unifying framework showcasing optimal timing and scaling of investment.==== Intuitively, shareholders finance the new project, yet do not fully internalize the benefits as debtholders capture some benefits in the event of bankruptcy. Consequently, the management will defer the option exercise until the benefits to shareholders are sufficiently large (in a sense that we shall specify explicitly). Moreover, because of debt overhang, the marginal benefit of investment to shareholders will always be lower than that of an unlevered firm, leading to a lower scale of investment.====Second, we show that PSD can mitigate the debt-induced distortions: the performance-sensitivity criterion can induce a firm to invest sooner and in a larger amount than a similar firm financed with straight debt. Because PSD rewards a firm with a larger scale with lower debt payments, the management will approve a growth plan financed by shareholders. Under straight debt, investment increases the size of the pie, but also increases the size of the slice accruing to debtholders, thereby constituting a transfer of wealth from shareholders to debtholders. By contrast, under PSD, debtholders receive lower debt payments after investment, while shareholders internalize a larger slice of the pie. It is therefore optimal for shareholders to invest earlier and more.====Third, the initial shareholders may maximize total firm value by issuing PSD. Because PSD brings the firm’s investment policy closer to the investment policy followed by an unlevered firm, total firm value is enhanced by introducing performance sensitivity. Our model provides a theory for optimal leverage ==== optimal sensitivity of debt, and rationalizes the extensive use of performance sensitive clauses in private debt contracts documented by Asquith et al. (2005).====The paper is organized as follows. Section 2 provides a review of the literature. Section 3 introduces the model and motivates the assumptions. Section 4.1 presents the benchmark model where the firm decides uniquely on the time of default. section 4.2 elaborates on the decision on the expansion lump, while Section 4.3 discusses the timing decision. Section 5 justifies the use of PSD as a devise to mitigate the debt overhang.",Does performance-sensitive debt mitigate debt overhang?,https://www.sciencedirect.com/science/article/pii/S016518892100138X,4 August 2021,2021,Research Article,186.0
"Braga Joao Paulo,Semmler Willi,Grass Dieter","The New School for Social Research, United States,The New School for Social Research, United States; University of Bielefeld, Germany; and IIASA, Austria,Vienna University of Technology and IIASA, Austria","Received 28 September 2019, Revised 13 June 2021, Accepted 21 July 2021, Available online 3 August 2021, Version of Record 8 September 2021.",https://doi.org/10.1016/j.jedc.2021.104201,Cited by (16),"A substantial increase of green investments is still required to reach the Paris Agreement’s emission targets. Yet, capital markets to expedite innovative green investments are generically constrained. Literature has shown that governments could de-risk such investments. Empirical beta pricing and yield estimates reveal some public involvement in the green bond market, especially for long maturity bonds. We provide empirical evidence that Governments and Multilateral organizations can de-risk green investments by supporting the issuance of green bonds in contrast to private green bonds - that show higher yields, volatility and beta prices - and conventional energy bonds, that are more volatile due to oil price variations. Since lower betas also mean lower capital costs, we use these empirical results and run a dynamic model with two types of firms, modeling the economic behavior of innovators (renewable energy firms) and incumbents (fossil fuel firms). Our model reveals that de-risked ","Since 2009, when the 15th Conference of the Parties (COP 15) to the United Nations Framework Convention on Climate Change (UNFCCC) took place in Copenhagen, climate finance has come to the forefront.==== Governments have a role in financing and in risk-bearing investments that exhibit higher externalities and uncertainties by reducing risk premia for such projects (Arrow, Joseph, Lind, 1970, Arrow, Joseph, Fisher, 1974, Stiglitz, Joseph, 1993). This holds in particular for the case of innovative renewable energy sources, especially in developing countries: the higher fixed and upfront relative costs demand a de-risking effort for green investments (Ondraczek, Janosch, Komendantova, Patt, 2015, Sweerts, Bart, Longa, van der Zwaan, 2019, Waissbein, Oliver, Yannick, Glemarec, Bayraktar, Schmidt, 2013).====Green bonds can provide an adequate funding for these investments by sharing its cost between current and future generations (Flaherty, Michael, 2017, Orlov, Sergey, Rovenskaya, Puaschunder, Semmler, Sachs, Jeffrey, Bernard, Lucas, Jeffrey, Semmler, 2014). Green debt has recently risen as an innovative instrument to fund sustainable projects==== but Governments and Multilateral organizations can help expediting this trend. Asset holders need to be induced to hold green bonds into their portfolio which in turn depends on the performance of these bonds in financial markets.==== Green bonds seem to have a higher Sharpe ratio, due to lower volatility (Semmler et al., 2021b) If issued by reliable agents, such as the public sector, these bonds tend to have lower yields and higher liquidity as it attracts investors concerned with better ESG practices and with the financial risks of holding carbon-intensive assets (Bachelet, Jua, Becchetti, Manfredonia, 2019, Fatica, 2019, Kapraun, Julia, Scheins, Carney, 2015). This is likely to reduce the capital costs of green investments and aid the energy sector structural change.====This paper discusses the role of green bonds and explores if Governments and Multilateral organizations can de-risk these bonds as a strategy to increase innovative green investments. We calculate bond yields and beta prices and find that public agents can actually de-risk green projects by acting as an issuer or guarantor of green bonds.====The paper also studies the impact of this strategy on the implementation of renewable energy by entrant firms in the context of a dynamic model. Our model is particular influenced by the evolutionary approach in economics (Arthur, 1989), following a Schumpeterian innovation dynamics. We allow interaction effects between firms, as in existent technology diffusion models (Pistorius & Utterback, 1996) and as in advertising diffusion models (Feichtinger, 1992). Our model also builds upon a Lotka-Volterra system as those applied in the bioeconomic literature for the use of renewable resources, such as the fishery model in Clark (1976) and Semmler & Sieveking (1994).====Our approach is also related to the work of dynamic limit pricing - as in Judd & Petersen (1985), Gaskins (1971), Kato & Semmler (2011), and Semmler et al. (2021a) - in which there is no heterogeneity on the entrants’ or incumbents’ side and each group take collective actions. However, in our model, only the entrants are dominantly pursuing an intertemporal strategy of joint pay-off maximization. We run a small-scale model of two types of firms studying the performance of the innovators (renewable energy firms) and the incumbents (fossil fuel firms). We assume that the market entrants (innovators), implementing a new renewable energy technology, exhibit an intertemporal pay-off function, cooperate to innovate, and are attracted to the market by excess profits (high mark-ups). We also introduce a debt dynamics for the innovators and explore analytically and numerically the debt sustainability. The model is designed to explore the impact of de-risking bonds on the new technology market viability, especially because innovators normally face higher competitive pressures and low mark-ups. Whereas de-risked finance can play a role in the take-off and debt control of new entrants from the cost side, mark-ups and profit flows can also do so from the market side. We also evaluate a model variant with taxes on the carbon-intensive sector and subsidies for green activities.====The paper is organized as follows. In Section 2, we present the theoretical background that justifies the role of the public sector in de-risking green bonds to expedite green investment. Section 3 gives a brief overview of the Bloomberg database of corporate green bonds and studies the beta prices and returns of de-risked bonds. Based on these results, Section 4 introduces the dynamic model of the two types of firms and proves under what conditions debt sustainability can be achieved. Section 5 presents the results from the model’s numerical simulations. Section 6 concludes the paper. The Appendix presents the solution procedure of the model, the data background and an evaluation of the volatility of the returns of green and fossil fuel bonds.",De-risking of green investments through a green bond market – Empirics and a dynamic model,https://www.sciencedirect.com/science/article/pii/S0165188921001366,3 August 2021,2021,Research Article,187.0
"Dizioli Allan,Pinheiro Roberto","International Monetary Fund, 700 19th St., N.W. Washington D.C., 20431, USA,Federal Reserve Bank of Cleveland, 1455 E 6th St., Cleveland, OH, 44114, USA","Received 22 March 2021, Revised 20 July 2021, Accepted 23 July 2021, Available online 29 July 2021, Version of Record 9 August 2021.",https://doi.org/10.1016/j.jedc.2021.104202,Cited by (2),"We introduce two types of agent heterogeneity in a calibrated epidemiological search model. First, some agents cannot afford to stay home to minimize virus exposure. Our results show that poor agents bear most of the epidemic’s health costs. Furthermore, we show that recessions are usually deeper and recoveries are faster when a larger share of agents fail to optimally adjust their behavior during the epidemic. Second, agents develop symptoms heterogeneously. We show that for diseases with a higher share of asymptomatic cases, even when less lethal, health and economic outcomes are worse. For both types of heterogeneity, economic effects are driven by a large share of the agents taking voluntary precautions to minimize virus exposure. Due to this mechanism of voluntary precautions, testing and subsequent quarantining are particularly beneficial in economies with larger shares of poor agents. In contrast, unless a health system collapse is large enough, lockdowns are quite costly for both developing and developed economies.","COVID-19 infections present a key feature that makes them challenging to control. Many patients take longer to develop symptoms than with other infectious respiratory diseases, and some are completely asymptomatic. Consequently, it is much harder to isolate infected individuals before they transmit the virus to others. Hence, outbreaks can happen rather quickly.====Moreover, the impact of the pandemic has varied across income levels. Poor neighborhoods have seen a significantly larger number of cases and deaths (see Table 1). There are a few reasons for this pattern. First, many of the services deemed essential that have continued to be supplied during the pandemic are provided by low wage workers.==== Second, individuals in these poorer communities are usually in occupations that do not allow for remote working (see Dingel and Neiman (2020) and Mongey et al. (2020)). Third, these communities are likely to have denser populations. And fourth, residents of poorer neighborhoods tend to have little in fallback savings and wealth and can therefore not afford to miss work to shelter from the virus (see Wright et al. (2020)).====Meanwhile, higher-income individuals are better able to minimize their exposure to the virus. Even without stay-at-home orders, unconstrained individuals who have the means to adjust their behavior may optimally reduce their exposure to infection. In fact, Wright et al. (2020), using movement derived from cellphone location data, find that much of the nationwide reduction in population movement was driven by counties above the median level of income.====In this paper, we propose and calibrate a search model that allows us to introduce some of these stylized facts in an epidemiological setting. In particular, we consider how the presence of asymptomatic and pre-symptomatic infected agents, as well as the presence of agents who can and cannot optimally adjust their time at work allocation (henceforth time-constrained agents), impacts the epidemic. Moreover, we consider how these agents’ heterogeneity interacts with the effectiveness of tests (both viral and antibody) and lockdowns in reining in the spread of the disease and ameliorating economic outcomes.====Our results show that introducing asymptomatic and pre-symptomatic agents as well as time-constrained agents is vital to evaluating these policies. If the incidence of asymptomatic agents is larger, then the numbers of expected infected agents and expected deaths are larger as well. The reason for this result is that asymptomatic and pre-symptomatic agents continue with their daily activities, infecting healthy but susceptible agents. In turn, the economy will suffer as unconstrained agents are compelled to even more drastically reduce their time spent outside the home as it is riskier to engage in outside activities. The impact of testing is significantly larger for a virus with more asymptomatic cases. Viral tests allow us to quarantine infected asymptomatic and pre-symptomatic agents, reducing the infection rate. This not only has the benefit of lowering the expected number of casualties caused by the disease in the first two years, but also allows unconstrained agents to optimally increase their labor supply, improving economic activity. Meanwhile, antibody tests reveal to unconstrained agents who had an asymptomatic infection that they might have acquired immunity, allowing them to optimally adjust their labor supply. As a result, the economic impact of testing is substantial when the incidence of asymptomatic infections is high. If we consider the case in which 80 percent of the infections are asymptomatic, increasing testing from 0 to 20 percent represents a cumulative gain of around 2.2 percent of pre-virus GDP after two years. We see two years as the relevant horizon to study public policy interventions. After this period, the level of infections is already small and we assume that mass vaccinations can take place.====Similar patterns are observed when the incidence of time-constrained agents is larger. The numbers of infected and dead are significantly higher, and time-constrained agents are also particularly affected. Following a calibration that represents an emerging market economy, our results show that 76 percent of time-constrained agents would be infected after two years and 0.4 percent would die. In contrast, only 23 percent of unconstrained agents would get infected and less than 0.12 percent would die after two years. Since the likelihood of infection is significantly higher in an economy with more time-constrained agents, unconstrained agents dramatically reduce their labor supply, with significantly negative impacts on GDP, offsetting the higher labor supply of time-constrained agents. So, even though more agents do not change their behavior, the economic effects are larger in the economy with more time-constrained workers at the peak of the pandemic. However, the presence of a larger share of time-constrained workers speeds up the infection process. As a result, a significant share of time-constrained workers either recovers or dies in a short period of time. Consequently, infection rates drop more quickly in the case with a larger share of time-constrained agents. Hence, the economic activity bounces back quicker in this case.====The effect of testing is particularly pronounced in an economy with more time-constrained agents, because these agents do not adjust their time allocation as the infection rate goes up. Thus, testing lowers the risk of infection by more in this environment. In quantitative terms, in an economy with a 50 percent share of time-constrained agents, moving from a 0 percent to a 20 percent incidence of tests predicts a reduction of 110,000 and 80,000 in the number of deaths from the disease in the first and second year, respectively, in a stylized economy reflecting the size of the US. Similarly, compared to an economy with no testing, a 20 percent testing rate increases GDP by 3.3 percent in the first year and an accumulated 1.7 percent of pre-COVID GDP after two years.====In terms of the effect of lockdowns, we show that while they are able to slow down the spread of the virus, they have low effectiveness in reducing the economic and human costs of the epidemic if they are implemented without further measures. In our benchmark calibration, the introduction of lockdowns reduces the number of deaths due to COVID-19 by less than 33,000 in the first year and less than 10,000 in the second year. Given the sharp recession in the first year, our quantitative results imply a high cost per life saved. Depending on the type of lockdown – comprehensive and short or mild and long – the cost per life is around $20 million and $5 million, respectively. Lockdown costs in our model are significantly higher than the ones featured in other papers in the literature, such as Greenstone and Nigam (2020). The main reason is that in our benchmark model, unconstrained agents already optimally adjust their time allocation in response to the possibility of infection. Consequently, the benchmark case already factors in most of the potential benefits of the lockdown at least in a developed country with few time-constrained agents.====The analysis of lockdowns in the benchmark case highlights that lockdowns may be more useful in stopping the spread of the disease in developing economies, with high shares of time-constrained agents. In fact, considering the extreme case in which all agents are time-constrained, a mild and long lockdown would be able to save 325,000 lives at a cost of $2.5 million per life, well below our benchmark. However, our calibration, which is closer to an average emerging economy, with around 50 percent time-constrained agents, still shows a cost per life that could be of around $20 million. Moreover, this figure does not factor in the significant costs of preventing time-constrained agents from going to work.====Clearly, developed and developing countries are different in many respects. Developing economies have not only a larger share of time-constrained agents, but also much younger populations, which face a lower probability of death due to COVID-19 complications. Moreover, developing countries’ health systems have significantly fewer resources, so the likelihood of collapse due to an explosion in the number of cases is significantly larger. When we incorporate several of these features into the model, we find that only an ICU collapse could reduce the lockdown’s cost per life significantly.====Our results are relevant from a policy perspective. They highlight the importance of identifying how asymptomatic and pre-symptomatic infected agents spread the virus. While contagion by pre-symptomatic agents has been well-established (He et al. (2020)), there are still some questions about the likelihood of asymptomatic agents transmitting the disease (Bai et al. (2020)). Second, they show that viral testing, a vital tool for controlling the spread of the disease in all countries, is particularly important for developing economies and poor neighborhoods. Unfortunately, as we can see in Fig. 1,==== poor countries have lagged significantly behind rich countries in testing their populations.====The paper is divided into 6 sections. Section 2 discusses the literature on COVID-19. Section 3 presents our theoretical model. Section 4 describes the functional forms used in our quantitative results and the calibration of the parameters. Section 5 shows our main quantitative results, while Section 6 concludes the paper.",Information and inequality in the time of a pandemic,https://www.sciencedirect.com/science/article/pii/S0165188921001378,29 July 2021,2021,Research Article,188.0
"Duernecker Georg,Herrendorf Berthold,Valentinyi Ákos","University of Frankfurt, CEPR, and IZA, Germany,Arizona State University, CEPR, and CESifo, USA,University of Manchester, CEPR, and KTI KRTK, United Kingdom","Received 29 April 2021, Accepted 18 July 2021, Available online 27 July 2021, Version of Record 8 August 2021.",https://doi.org/10.1016/j.jedc.2021.104200,Cited by (4),The productivity growth slowdown that started in the 1970s presents a challenge to Kaldor’s growth facts and the one-sector growth model. We ask: What natural modification of the growth model can generate a prolonged productivity growth slowdown? We show that the two-sector version with separate consumption and investment sectors has a balanced growth path equilibrium along which productivity growth slows down if two conditions hold: real GDP is measured with the Fisher index; productivity growth in the consumption sector slows down. We also show that real GDP measured with the Fisher index is a welfare measure in the two-sector version.,"The neoclassical growth model is a workhorse that has successfully been applied to many important questions in macroeconomics. The popularity of the growth model is due to its analytical tractability and its ability to reproduce the salient features of long-run growth. Kaldor (1961) summarized them in five stylized facts: the growth rates of real GDP per worker and real capital per worker exhibit no trend; the gross return on capital, the capital-to-output ratio, and the GDP share of the payments to capital exhibit no trend growth. More than half a century later, being consistent with the Kaldor facts is still viewed as a minimum requirement for a credible model of economic growth. If a model economy is consistent with the Kaldor growth facts, then it is commonly referred to as exhibiting balanced growth or being on a balanced growth path (“BGP” henceforth).====After the publication of Kaldor’s article, a new feature of long-run growth surfaced: starting in the 1970s, the growth rate of real GDP per worker slowed down considerably in the U.S. and many other developed countries. The resulting productivity growth slowdown lasted for several decades and productivity growth rates have never fully recovered.==== A prolonged productivity growth slowdown presents a challenge for the first Kaldor fact that the growth rate of real GDP per worker does not exhibit a trend. It also presents a challenge for the one-sector growth model, which was constructed to imply a constant growth rate of real GDP per worker along the BGP. These observations raise the question: Is there a natural modification of the growth model that is consistent with the productivity growth slowdown? The goal of our paper is to answer this question.====It is instructive to start thinking about the tension between the productivity growth slowdown and the first Kaldor fact in the context of the one-sector growth model. Although productivity growth cannot slow down along the BGP of the one-sector growth model, a productivity-growth slowdown may occur during a transition to a new BGP with a lower level of real GDP per worker. For several reasons, the transition interpretation is not ideal though. To begin with, it is simpler to analytically characterize a BGP than a transition path. Moreover, if the growth model is used for the analysis of business cycle fluctuations, detrending growing variables is easiest when there is only one BGP. Lastly, we do not have many observations from the possible new BGP – in particular, given the major disruption during the Great Recession – and so it is unclear where exactly the transition is supposed to end up. We therefore take a different route and insist on some form of balanced growth while looking for a critical feature of reality that implies a productivity growth slowdown but is not captured by the one-sector growth model. We argue that a natural one is the considerable increase in the price of consumption relative to investment, which started in the late 1970s.====Recognizing that the price of consumption relative to investment has been changing considerably introduces the new issue of how to measure real GDP. In the one-sector model, the issue does not arise because the relative price of consumption to investment is assumed to be constant and equal to one, implying that real GDP per worker is the same in units of consumption as in units of investment. In fact, in the one-sector model, real GDP per worker is also the same when measured with the Fisher index, which is what the Bureau of Economic Analysis does in the National Income and Product Accounts (“BEA” and “NIPA”, henceforth). In contrast to the one-sector growth model, the three measures of GDP per worker behave completely differently in the NIPA data. In particular, we document that in units of investment the growth rate of GDP per worker has no trend and, somewhat unexpectedly, the first Kaldor fact holds. But in units of consumption or measured with the Fisher index, the growth rates of GDP per worker slow down and the first Kaldor fact does not hold.====Our main contribution is to develop a version of the two-sector growth model that is consistent with these observations. In particular, our model version has an equilibrium path along which the Kaldor facts hold when real GDP is measured in units of investment. Perhaps surprisingly, a productivity slowdown arises nonetheless along the same BGP when instead real GDP is measured in units of consumption or with the Fisher index. We also show that, in context of our model version, real GDP measured with the Fisher index is the only of the three measures that has a welfare interpretation. This suggests that while GDP measured in investment may be useful for modeling purposes when one is after constructing a BGP, GDP measured with the chained Fisher index is the relevant statistic when one is interested in welfare changes. This also suggests that “rescuing the first Kaldor” by focusing on real GDP in units of investment assumes away the detrimental welfare implications of the productivity growth slowdown.====Before we describe our analysis in some detail, it is important to establish that the differences among the three measures of GDP per worker are economically significant. To this effect, Table 1 shows for the postwar U.S. the average annual growth rates of real GDP per worker and the implied accumulated levels after 71 years. Compared to the numeraire consumption, the average annual growth rate is about twice as large with the numeraire investment and a quarter larger with the Fisher index; the level of GDP per worker after 71 years is two and a half times larger with the numeraire investment and almost a third larger with the Fisher index. Such stark differences among the growth rates of GDP per worker imply that the question of how to measure real GDP is of more than just academic interest. The table also shows that all three measures of GDP per worker experienced a prolonged slowdown starting in the late 1970s, and only GDP per worker measured in units of investment has fully recovered.====We now turn to the details of establishing that the two-sector version of the growth model with separate consumption and investment-producing sectors can generate the productivity growth slowdown while being consistent with the remaining Kaldor facts. Key to this result is that in the 1970s the growth of the relative price of consumption started to increase at an ====. While it is widely recognized that the relative price of consumption increased, it is typically overlooked that its growth rate accelerated. We capture the acceleration by relaxing the usual restriction that the growth rate of total factor productivity (“TFP” henceforth) in the consumption sector is constant and instead allow it to slow down over time. Since changing growth rates violate strict balanced growth, we follow Kongsamut et al. (2001) and impose the weaker equilibrium concept of generalized balanced growth path (“GBGP” henceforth). GBGP requires a constant real interest rate while leaving the other variables free to grow at non-constant rates. Our key result is that with these modifications, the two-sector model has a GBGP with two properties: if real GDP is measured in units of investment, then real GDP per worker grows at a constant rate and the Kaldor facts including the first one hold; if real GDP is measured in units of consumption or with the Fisher index, then the growth rate of real GDP per worker slows down and all but the first Kaldor fact hold. We also show that, if we choose the model parameters to match the observed changes in the price of consumption relative to investment and the observed GDP shares of consumption and investment, then the model-implied productivity growth slowdown broadly resembles the one in the U.S. NIPA data.====Given how different the three measures of real GDP behave in the data and in the model, we are left with the natural question as to whether one is preferable to use. There is a notion in macroeconomics that real GDP in units of consumption is preferable over real GDP in units of investment because the households care about consumption and not investment; see for example Greenwood et al. (1997). The intuitive reasoning behind using real GDP in units of consumption was first formalized by Weitzman (1976), who showed that Net Domestic Product (or NDP that is, GDP minus depreciation) has a welfare interpretation if both components are measured in units of consumption.==== We briefly replicate Weitzman’s core argument in the context of the two-sector growth model in discrete time. This serves to highlight a critical condition underlying his result: the TFP growth rates in both sectors must be constant. Unfortunately, as we have pointed out above, the assumption of constant TFP growth in ==== sectors rules out a productivity growth slowdown in the two-sector model. Thus, NDP is not a useful welfare measure in our context.====Turning now to the alternative of measuring real GDP with the Fisher index, we show that it has a welfare interpretation as long as the two-sector growth model is in equilibrium. A similar welfare result was first shown by Durán and Licandro (2017) for the continuous-time version of the two-sector growth model. We provide a new proof for discrete time, which is more direct than their proof and which provides separate first-order approximations for the Laspeyres and the Paasche indexes.==== We note that the welfare result challenges a common view in the literature that GDP constructed with the Fisher index is appropriate for productivity analysis whereas NDP in units of consumption is appropriate for welfare analysis; see for example Hulten (1992).====To sum up, we establish three things. First, in the U.S. NIPA data, the first Kaldor fact holds only if real GDP is measured in units of investment. In contrast, if real GDP is measured in units of consumption or with the Fisher index, a prolonged productivity growth slowdown starts in the 1970s and the first Kaldor fact does not hold. Second, the two-sector growth model with a consumption and an investment sector can be made consistent with these facts, both qualitatively and quantitatively, if the TFP growth rate in the consumption sector is allowed to slow down. Third, real GDP measured with the Fisher index is a welfare measure in the context of the two-sector version of the growth model.====Our results suggest to bring the growth model to the data by following three steps: construct a GBGP in the two-sector version by matching the constant trend growth rate of GDP per worker in units of investment, the changing growth rate of the relative price of consumption, and the nominal shares of consumption and investment in GDP; construct real GDP from the model by applying the chained Fisher index to the quantities and prices generated by the GBGP; compare the resulting measure of real GDP per worker with real GDP per worker in the NIPA data. This procedure has the advantages that the model generates the observed productivity growth slowdown, real GDP has a welfare interpretation, and real GDP is constructed in the same way in the model and the data.====The rest of the paper is organized as follows. Section 2 lays out the two-sector growth model. Section 3 presents key facts about the productivity growth slowdown. Section 4 constructs a GBGP of the two-sector model that is consistent with the facts and that generates a productivity growth slowdown. Sections 5 and 6 present the welfare results. Section 7 discusses the related literature and Section 8 concludes. An Appendix contains the proofs and some background material.",The productivity growth slowdown and Kaldor’s growth facts,https://www.sciencedirect.com/science/article/pii/S0165188921001354,27 July 2021,2021,Research Article,189.0
"Diegel Max,Nautz Dieter","Department of Economics, Freie Universität Berlin, Boltzmannstr.20, Berlin D-14195, Germany","Received 26 January 2021, Revised 12 July 2021, Accepted 14 July 2021, Available online 17 July 2021, Version of Record 27 July 2021.",https://doi.org/10.1016/j.jedc.2021.104192,Cited by (11),This paper investigates the role of long-term ,"There is a general consensus among both, academics and central bankers that the anchoring of long-term inflation expectations is of crucial importance for monetary policy. As a result, a growing literature investigates the anchoring of inflation expectations assuming that well-anchored inflation expectations do not respond to macroeconomic news. Accordingly, any response of inflation expectations to a monetary policy shock is considered a sign of de-anchoring, and thus must be undesirable.====However, the role of inflation expectations for the monetary transmission mechanism might be more complex than the early anchoring literature suggests. During the past decade, long-term inflation expectations have been persistently below official inflation targets. Consequently, central banks have discussed how monetary policy can contribute to ==== inflation expectations. In this case, the response of long-term inflation expectations to monetary policy might not be undesirable but is the intended policy outcome. However, the empirical relevance of the re-anchoring channel of monetary policy is still under debate, see e.g. Andrade et al. (2016), Ciccarelli et al. (2017) and Gambetti and Musso (2020).====The aim of our paper is twofold. First, we use structural vector autoregressive (VAR) analysis to investigate if U.S. long-term inflation expectations respond to monetary policy shocks in line with the re-anchoring channel. Second, we propose counterfactual analysis to assess the quantitative importance of long-term inflation expectations for the conduct and transmission of monetary policy.====The focus of earlier contributions that include measures of inflation expectations into empirical macro models is typically not on the role of long-term inflation expectations for the monetary transmission mechanism and their response to monetary policy shocks. By contrast, the contemporaneous response of inflation expectations to macroeconomic shocks, including monetary policy shocks, is often restricted to zero, see e.g. Leduc et al. (2007) and Geiger and Scharler (2020). To illustrate, Clark and Davig (2011) assume that neither shocks to short-term inflation expectations nor other macroeconomic shocks have a contemporaneous impact on long-term inflation expectations. Such a restriction of the reaction of long-term inflation expectations can be plausible if inflation expectations are perfectly anchored, but cannot account for the re-anchoring channel of monetary policy. Nautz et al. (2019) document an immediate response of long-term inflation expectations to macro news shocks in a bi-variate structural VAR. Yet, abstracting from economic key variables, bi-variate models of short- and long-term inflation expectations are not informative about the interaction of inflation expectations and monetary policy. To overcome these issues, the current paper identifies monetary policy shocks in a structural VAR that (i) includes inflation expectations, inflation, the monetary policy rate, and unemployment and (ii) exploits a minimal set of sign and zero restrictions. To identify the monetary policy shock we use uncontroversial sign restrictions for inflation and unemployment. Moreover, in the spirit of Arias et al. (2019) we rule out that monetary policy reacts to inflation expectations systematically in an implausible way.====The counterfactual analysis of the role of inflation expectations for the transmission of monetary policy requires identification of an expectations shock that can be used to isolate the re-anchoring channel. In line with the literature and the timing of the expectations data, this shock is identified assuming that inflation expectations are predetermined with respect to current inflation and unemployment. However, and most importantly for our purpose, the identification scheme leaves both, the response of inflation expectations to a monetary policy shock and the monetary policy response to expectations shocks unrestricted.====Our paper relates to studies that avoid zero restrictions on the response of inflation expectations by using Proxy SVARs. Interestingly, the available evidence is mixed. While Gertler and Karadi (2015) and Gambetti and Musso (2020) find no response of longer-term inflation expectations to monetary policy shocks, Jarociński and Karadi (2020) confirm that inflation expectations decrease in response to a monetary policy tightening. However, the studies do not consider the monetary policy response to expectations shocks. Confirming Jarociński and Karadi (2020), our empirical results show that U.S. long-term inflation expectations respond significantly and plausibly signed to monetary policy shocks. In our benchmark model monetary policy shocks account for 16% of the variation of long-term inflation expectations on impact. In sharp contrast to the assumptions made by the earlier literature, this indicates that monetary policy has a sizable impact on the dynamics of long-term inflation expectations.====However, a plausibly signed and significant response of long-term inflation expectations to monetary policy shocks is only a necessary condition for the working of the re-anchoring channel of monetary policy. The question remains how important the estimated response of inflation expectations is for the transmission of monetary policy shocks to the ultimate target variables of monetary policy, i.e. to inflation and unemployment. Therefore, we perform a counterfactual analysis where the transmission of monetary policy shocks via inflation expectations is shut down, see Wong (2015). We find that the response of unemployment to a monetary policy shock is only weakly affected by the assumption of constant long-term inflation expectations. By contrast, the estimated effects are much more pronounced for inflation. Confirming the re-anchoring channel of monetary policy, this suggests that monetary policy shocks are also transmitted to inflation via long-term inflation expectations.====The estimated SVAR implies that expectations shocks are an important driver of the policy rate. Following Antolín-Díaz et al. (2021), we employ structural scenario analysis to demonstrate that the response of U.S. monetary policy to expectations shocks has contributed to the stabilization of the economy and the anchoring of inflation expectations. In particular, inflation would have been 57 basis points lower and unemployment 99 basis points higher on average if monetary policy had not responded to expectations shocks since 2012.====The rest of the paper is structured as follows. Section 2 presents the data and reduced-form evidence on the importance of long-term inflation expectations for monetary policy. Sections 3 presents our empirical model. We briefly review the identification problem of structural VARs, explain the identification strategy and introduce the underlying identifying assumptions. Section 4 presents the empirical results on (i) the effect of monetary policy on long-term inflation expectations and (ii) the role of inflation expectations for the transmission of monetary policy shocks to inflation and unemployment. This section also includes the counterfactual analysis as well as the robustness analysis related to the shadow rate and the identifying assumptions. Section 5 provides some concluding remarks.",Long-term inflation expectations and the transmission of monetary policy shocks: Evidence from a SVAR analysis,https://www.sciencedirect.com/science/article/pii/S0165188921001275,17 July 2021,2021,Research Article,190.0
"Chen Zilin,Guo Li,Tu Jun","School of Finance, Southwestern University of Finance and Economics, 555 Liutai Avenue, Chengdu, 611130, China,School of Economics, Fudan University, 600 Guoquan Road, Shanghai, 200433, China,Shanghai Institute of International Finance and Economics, China,Lee Kong Chian School of Business, Singapore Management University, 50 Stamford Road, Singapore, 178899","Received 4 January 2021, Revised 3 July 2021, Accepted 8 July 2021, Available online 13 July 2021, Version of Record 3 August 2021.",https://doi.org/10.1016/j.jedc.2021.104191,Cited by (0),"Media news may cover multiple firms in one article, which establishes a media connection across firms. We propose a media connection strength (MCS) measure between two given firms, which is defined as the number of news articles co-mentioning these two firms. We show that the MCS measure can significantly explain and forecast return comovement of media-connected firm-pairs. Further analyses show that our results are robust to various alternative explanations. We argue that the MCS measure can capture comprehensive and complex correlated fundamental information among media-connected firms and hence may provide a new mechanism for return comovement beyond the existing rational- and behavioral-based explanations.","Understanding the driving forces of stock return comovement has important implications for many applications, such as risk management (Philippe (2001)), portfolio allocation (Qian et al. (2007)), asset price dynamics (Rosenberg and Schuermann (2006) and Brooks et al. (2002)), and trading strategies (Gatev et al. (2006); Papadakis and Wysocki (2007); Chen et al. (2019)). Theoretically, the stock return comovement of two firms can be driven by common shocks to their fundamentals, such as future cash flows or discount rates. Therefore, empirically explanatory variables based on the fundamental information of a firm, usually quantitative information, have been proposed to explain comovement (e.g., Shiller (1989); Fama and French (1993); Chen et al. (2019); Green and Hwang (2009)). In addition, recent studies document ǣexcess comovementǥ that cannot be explained by fundamental information and propose alternative behavioral bias-based explanations for return comovement (e.g., Karolyi and Stulz (1996); Barberis and Shleifer (2003); Barberis et al. (2005); Bekaert et al. (2005); Kumar and Lee (2006); Kallberg and Pasquariello (2008); Anton and Polk (2014); Kumar et al. (2016)).====In this study, we investigate a new potential channel for return comovement; namely, media connection, which is beyond the existing fundamental information channels and is not based on behavioral bias. In particular, we examine whether correlated fundamental information contained in multi-firm news articles could be an important driver for return comovement. Multi-firm news articles covering two or more firms may reflect journalists collective opinions on a certain complicated fundamental connectivity between the firms that are cited. In addition to well-documented fundamental links including customer-supplier links (Cohen and Frazzini (2008)), text-based links (Hoberg and Phillips (2016)), and EDGAR co-search links (Lee et al. (2015)), multi-firm news articles may be a proxy of other complex economic linkages, such as strategic partnerships, intra- and inter-sectoral competitive links, and credit, financing, banking, and subsidiary relations (Schwenkler and Zheng (2019)). Therefore, we propose a media connection strength (MCS) measure based on the news data collected by Thomson Reuters News Analytics (TRNA), which is defined as the number of news articles covering two firms.==== We expect that the MCS measure can offer incrementally better explanatory power regarding the comovement of fundamental performance and return comovement for media-connected firm pairs that goes beyond those existing variables measuring correlated fundamentals across firms.====We conduct empirical tests to verify our hypothesis. First, we examine the determinants of the MCS. We run cross-sectional regressions of our MCS measure on the firm-pair characteristics that are documented in the literature as associated with return comovement. We find that variables such as the mutual fund common ownership and common analyst coverage are significantly and positively associated with MCS. Further tests show that many fundamental similarities between co-mentioned stocks can significantly explain MCS, suggesting that fundamental similarities of stock pairs are indeed captured by our MCS.====Next, we examine whether the MCS measure can predict fundamental similarities of media-connected firm pairs. Following Livnat and Mendenhall (2006), we define the standardized unexpected earnings (SUE) as a proxy for the fundamental information of a firm and find that the correlation of SUE, when controlling for other well-known return comovement sources and other specific fundamental similarities, can be positively and significantly predicted by the MCS among the co-mentioned stock pairs. Specifically, one standard deviation of the increase in the MCS is associated with an increase of 0.6% in the correlation of SUE in quarter t and an increase of 1.0% in quarter t+1. Meanwhile, to further validate our fundamental argument for a media connection, we employ the correlation of sales growth (SALEG) as an alternative measure of fundamental comovement, and the results are also consistent with our expectation; that is, a one standard deviation increase in the MCS of quarter t is associated with an increase of 1.8% in the correlation of SALEG among firm pairs in quarter t and an increase of 1.9% in quarter t+1, when controlling for other well-known return comovement sources and other fundamental similarities. Finally, we also measure the correlated fundamentals of stock pairs as the negative value of the absolute difference in percentile ranking of SUE or SALEG for the media-connected stock pairs, and the results remain similar. This suggests that media connection contributes to an increasing fundamental news comovement for media-connected firm pairs beyond what can be explained by other stock-pair similarities documented in the literature.====Subsequently, we conduct a simple univariate analysis to examine if the predictive power of our MCS on fundamental similarities can be extended to an association between MCS and excessive return comovement. We first split all firm pairs into three groups based on the MCS and calculate the average return correlation of these three groups; we then form a high-minus-low group that captures the return correlation difference between the high-MCS group and the low-MCS group. We find that the return correlation of the high-MCS group is significantly higher than that of the low-MCS group. In addition, the average return correlation of the stock pairs with no-zero MCS is significantly higher than that of those with zero MCS, indicating that our MCS is indeed positively associated with a stronger return comovement. We then perform cross-sectional regressions to formally investigate if the contemporaneous and future excess return comovement of media-connected firm pairs can be significantly explained through the MCS measure. Consistent with our hypothesis, the MCS is positively and significantly associated with not only cross-sectional variation in the strength of fundamental similarities but also a stronger return comovement among firm pairs in the current month and next month, when controlling for other well-known return comovement sources and a list of firm-pair fundamental similarities. A one standard deviation increase in MCS is associated with an increase of 0.9% in return comovement and an increase of 1.0% in excess return comovement (under the Fama and French (2015) five-factor model (FF5)) in the current month, and the results are similar to that of the next month. Moreover, the predicted variation in FF5 residual correlation is in the average range of 5.33% to 16.75%, with a mean abnormal correlation of 7%, suggesting that the effect of MCS is also economically significant.====Kumar and Lee (2006), Kumar et al. (2013), and Kumar et al. (2016) find that stock prices tend to co-move if they share the same sentiment. Given that stocks mentioned in the same news article may share a similar sentiment, we examine whether the return comovement predictability of our MCS is independent of investor sentiment. Through defining high- and low-sentiment periods based on the sentiment index of Baker and Wurgler (2006), we find that the predictability of the MCS is robust and similar in magnitude under different sentiment regimes, suggesting that investor sentiment is unlikely to explain the results. In addition, we find that the MCS measure is positively associated with trading activity comovement, measured as the correlation of order flow or turnover ratio between paired firms. This is consistent with the hypothesis that the correlated fundamentals are captured through the MCS measure, which may induce correlated trading activities and the corresponding return comovement.====Nevertheless, investors may simply trade on media-connected stock pairs due to category trading habitat instead of the fundamental similarities between those stock pairs. Barberis and Shleifer (2003) argue that investors allocate capital at the level of asset categories rather than individual stocks. Firms are mentioned frequently in the same news article because they are viewed as being in the same asset category by market participants, including news media and investors. Alternatively, investors view of asset categories could be influenced by the frequency of stocks being mentioned together in news articles. In either case, investors trading in and out of media-connected stocks that are viewed as being in the same category could induce an excessive comovement in their returns.====However, this is less likely to explain our results, given that the coefficients of MCS on return comovement remain similar during both low- and high-sentiment periods. We further rule out this alternative hypothesis in two ways. Firstly, we replace the correlation of order flow (OFCOR) or the correlation of turnover (TOCOR) in the current month with the similarity in turnover (TO) and the correlation of TO in the past 60 months (VOLCOR) in the cross-sectional regressions of the return comovement so as to control for the correlated flow-induced trading induced by category trading habitat. The results show that the effect of our MCS is still there after the inclusion of these control variables. Secondly, as is detailed in the latter part of the paper, we conduct portfolio analysis to further show that category trading habitat is unlikely an explanation of our results.====Finally, we conduct robust checks to examine whether other fundamental similarities documented in the literature can be used to explain our results. Pirinsky and Wang (2006) find that stock prices tend to co-move if their headquarters are located in the same place. Thus, we add a dummy variable to our regression indicating whether two stocks are headquartered in the same U.S. county or not. Consistent with Pirinsky and Wang (2006), firm pairs located in the same county exhibit an excess return comovement in the next month. Nevertheless, the addition of the same-county dummy variable does not affect the return correlation predictability of our MCS measure.====Furthermore, Hoberg, Phillips, 2010, Hoberg, Phillips, 2016 construct a text-based industry classification based on the production description of 10,000 annual reports and find that this classification is more informative than traditional ones such as SIC or NAICS. We control this possible channel of similarity in the production of firm pairs for return comovement by including the text-based industry similarity score in our regression. The economic magnitude of the coefficient of our MCS measure is slightly weaker but remains statistically significant at the 1% level. Finally, Lee et al. (2015) find that stocks that investors co-search in the EDGAR system exhibit higher fundamental similarities than traditional industry classifications. As a measure of the attention paid by investors to EDGAR searches, the co-search measure could subsume our MCS in predicting the excess return comovement, so we include it in our regression model. Again, the return correlation predictability of our media connection measure remains significant. Overall, our results are robust to these alternative sources of stock return comovement.====Our paper contributes to several strands in the literature. First, we contribute to the debate on whether stock comovement is caused by information or noise. On the one hand, stock return comovement is driven by similarities in fundamentals such as size and book-to-market (Fama and French (1993), cash flows (Chen et al. (2019), price level (Green and Hwang (2009)), and analyst coverage (Chan and Hameed (2006); Muslu et al. (2014); Hameed et al. (2015)). On the other hand, some studies attribute the excess return comovement that cannot be explained by fundamental commonalities to behavioral biases such as categorical trading (Barberis and Shleifer (2003); Barberis et al. (2005)), sentiment (Kumar and Lee (2006); Kumar et al. (2013); Kumar et al. (2016)), investor attention (Peng and Xiong (2006); Huang et al. (2019)), forecast errors (Israelsen (2016)), and trading pressure from institutional investors (Anton and Polk (2014)). Our paper shows that stock return comovement for media-connected stock pairs is driven by the common fundamental information conveyed by media reports. Moreover, our results are consistent with the predictions based on the models of profit-maximizing information producers (Veldkamp (2006)). In particular, information production is non-rival with a high fixed cost of discovery and a low marginal cost of replication. Therefore, competitive producers such as news media tend to provide news that can maximize their profits from investors. Moreover, the information useful for predicting a subset of stocks could attract more investors than information that is useful for predicting only a single stock. As a consequence, given the high per-unit cost of information production, information useful for more stocks would induce news media to extend their coverage to these stocks, especially those whose fundamentals correlate more with each other.====Second, this study contributes to the literature on lead-lag effects in the returns of economically related stocks. Early studies tend to group firms according to their fundamental characteristics so as to find economically related firms, such as Lo and MacKinlay (1990), Brennan et al. (1993), Badrinath et al. (1995) and Chordia and Swaminathan (2000), while recent studies focus on specific economic links to identity firm connections, such as Cohen and Frazzini (2008), Lee et al. (2019), and Ali and Hirshleifer (2019). In this paper, we propose another way of identifying firm peers based on media connection, which can drive return lead-lag effects among firms via information spillover across media connection.====Third, we contribute to the literature that investigates the role of news media in financial markets ((Huang et al., 2021). Existing studies largely focus on two perspectives, one of which is to show that news tone or sentiment can be used to predict a firms future performance. Tetlock (2007) argues that language, especially negative language, could be used to predict excess market returns. Tetlock et al. (2008) analyze firm-specific news to explore the predictability of cross-sectional return. Another perspective examines whether media coverage can strongly affect future stock returns. For example, Fang and Peress (2009) find that stocks with no media coverage can earn higher returns than stocks with high media coverage, even after controlling for well-known risk factors. Engelberg and Parsons (2011) find that local media coverage can strongly predict local trading. However, there is limited evidence for the interactive effect induced by common media coverage on return comovement. In this paper, we construct a proxy for fundamental similarities between two media-connected firms through media connection, and our empirical results suggest that our MCS measure is significantly associated with fundamental similarities and return comovement.====The rest of the paper is organized as follows. Section 2 describes the data used in this paper and explains how the main variables are constructed. In Section 3, we present the main empirical results. Alternative explanations are examined and robustness tests are conducted in Section 4 to rule out alternative hypotheses. Section 5 concludes.",Media connection and return comovement,https://www.sciencedirect.com/science/article/pii/S0165188921001263,13 July 2021,2021,Research Article,191.0
"Lopez-Martin Bernabe,Perez-Reyna David","Central Bank of Chile, Chile,Universidad de los Andes, Colombia","Received 9 December 2020, Revised 30 June 2021, Accepted 1 July 2021, Available online 7 July 2021, Version of Record 18 July 2021.",https://doi.org/10.1016/j.jedc.2021.104190,Cited by (1)," on production decisions. In our model we find that contract enforcement accounts for differences in output per worker of 22 percent when we compare India to the U.S. The impact on firm life-cycle growth, the age and size distribution of firms is quantitatively significant.","A central area of research in macroeconomics and development aims at identifying sources of distortions that account for significant differences in total factor productivity and output per capita across countries. A recent literature has focused on the analysis of these distortions at the firm level and their aggregate consequences.==== It is understood that idiosyncratic distortions not only affect the allocation of inputs of production across firms, but also the incentives to invest in technology and productivity within the firm. Both channels have, at least in theory, a significant impact on aggregate productivity. Identifying the sources of these distortions is of paramount importance to assist the design of economic policies aiming at promoting economic development. In turn, the development of quantitative frameworks provides an understanding of the mechanisms and the potential impact of different distortions faced by firms on aggregate outcomes.====In line with this literature, we construct a dynamic framework of heterogeneous firms to evaluate the impact of contract enforcement on firm life-cycle growth and aggregate productivity. We build upon the model of Acemoglu et al. (2007), who provide a tractable structure where firms that produce final goods (henceforth, firms) need to procure intermediate goods from suppliers. The first building block of this model is the representation of technology as the range of intermediate inputs used by firms. The second building block is the well established approach to incomplete-contracting models of the firm originated by Grossman and Hart (1986) and Hart and Moore (1990). The producer of final goods decides the range of intermediate goods that it will employ for production. This range represents the technology of the firm: a more advanced technology is more productive, but entails higher costs in terms of direct pecuniary costs as well as those that emerge from contracting with more suppliers. Suppliers undertake relationship-specific activities, some of which are contractible while the rest are nonverifiable and noncontractible.====In our model the quality of the contracting institutions in an economy is represented by the range of contractible activities. Producers of final goods decide the investment levels in contractible activities undertaken by the supplier of each intermediate good. However, suppliers choose investment in noncontractible activities, a decision that anticipates the results of a bargaining game. This results in an allocation of resources that is not efficient: suppliers tend to underinvest in noncontractible activities given that they are not the full residual claimants of the output gains obtained from their investments. Thus, contractual incompleteness has a negative impact on technology adoption and can potentially generate sizable productivity differences across countries.====We expand the analysis of this friction by analyzing its impact in a framework of firm dynamics (Hopenhayn, Rogerson, 1993, Hopenhayn, 1992). This approach allows us to make a contribution that we outline in the following manner. First, we show that the friction under study generates different ==== (or distortions equivalent to taxes) on production and profits. We show that the wedge on profits is dependent on the technology of the firm. Second, we show how these wedges affect not only the size of the firm but also the dynamic incentives to invest in technology and productivity within the firm, which will determine the life-cycle growth profile of firms and aggregate productivity. Additionally, we establish its impact on the age and size distribution of firms, which is consistent with a series of studies that have documented the smaller size of firms in developing economies (e.g., Garcia-Santana, Ramos, 2015, Poschke, 2018, Tybout, 2000). Third, our analysis allows us to connect our quantitative results with the literature that studies alternative frictions in similar theoretical frameworks. For example, an extensive literature has studied the role of financial frictions, by examining alternative specifications (incomplete enforcement, collateral constraints, etc.), parameterizations, and margins through which they affect aggregate productivity.==== A similar comparison can be made with the literature that studies firm registration costs or labor market regulation. To the best of our knowledge, we are the first to explore the role of firm-supplier contract enforcement in a quantitative framework of firm dynamics.====We provide empirical support for our theoretical analysis. At the country level we estimate a robust and economically significant relationship between judicial quality and output per capita and TFP. The econometric results are strengthened with the use of well established instrumental variables. Then, we exploit cross-country firm-level data and document strong correlations of the quality of our institutions of interest with firm size, and revenue per worker. Furthermore, we find support for the dynamic mechanism of our model by documenting the effect of judicial quality on revenue growth, investment in research and development, and growth of revenue per worker.====We use our theoretical model to highlight the mechanisms that make contract enforcement relevant to account for differences in output per worker. For our quantitative analysis we consider the U.S. economy as a benchmark and calibrate our model under the assumption of contract completeness. Some of the parameters are standard and obtained from the literature of firm dynamics, while others are calibrated to replicate key statistics of the U.S. economy, such as firm exit rates, firm life-cycle growth, and the distribution of employment by age of the firm. We then document how the economy performs, in general equilibrium, as the range of contractible activities is reduced. This affects the investment in technology at the firm level, the age and size distribution of firms, and aggregate productivity. The model is able to account for differences in output per worker of up to 33 percent, which is comparable to losses generated by financial frictions in similar quantitative frameworks. Furthermore, we observe considerable differences in firm growth when comparing economies with and without contract incompleteness: average firm size for 26 to 30 year old firms is 2.6 times that of young firms when contracts are complete (replicated by calibration in the baseline reference), while firm growth is negligible when contracts are incomplete. Finally, we assess the role of key parameters of the model. Based on our results, we stress the importance of frictions that distort the ability of firms to enforce contracts with suppliers.====Our work is related to different strands of the literature on firm dynamics, misallocation, and aggregate productivity. It is connected to the literature that evaluates the effects of idiosyncratic distortions, in models where productivity is endogenous. The analysis of these models has shown that assuming an exogenous distribution of firm productivity can lead to the underestimation of the consequences of distortions that affect the allocation of resources across production units. Distortions can affect incentives to improve productivity, which adds to the effect on the allocation of resources across firms, thus generating an amplification mechanism. This effect can be particularly detrimental when distortions are more severe for the most productive firms, often termed ====, as in Bento and Restuccia (2017), Hsieh and Klenow (2014), and Buera and Fattal-Jaef (2016).==== In line with this general area of research we analyze a particular source of distortions, potentially correlated with firm productivity or technology, which reduces incentives for investment in innovation and firm growth. In our model firms invest each period to increase the stock of technology, and this decision is affected by contract incompleteness. The returns to this investment are decreasing in the size of firms.====Our article is related to several pieces of research in the literature of contractual frictions. Mukoyama and Popov (2020) embed the contract incompleteness structure of Acemoglu et al. (2007) in a dynamic general equilibrium growth model with evolving institutions during the process of industrialization. They show that incompleteness of contracts leads to two types of misallocation that generate production inefficiency: unbalanced use of inputs and unbalanced production of different goods. Additionally, Antràs and Helpman (2006) and Schwarz and Suedekum (2014) introduce the model of Acemoglu et al. (2007) in a context of international trade. Boehm and Oberfield (2020) use microdata on Indian manufacturing firms to show that production and sourcing decisions appear systematically distorted in states with weaker enforcement. Boehm (2020) conducts an analysis at the cross country level and computes the losses that result from contracting frictions employing a static multi-industry model, with a focus on input-output relationships across industries. For the reasons described above, our work is complementary to this literature, as well as to their forceful motivation of the study of contract enforcement.====Finally, the empirical results presented in the next section are related to a set of studies that document the impact of the quality and efficiency of judicial institutions on different dimensions of firm performance, such as size and growth, across regions within particular countries, and across different economies. The evidence we present is primarily related to Beck et al. (2005) and Beck et al. (2006), who use cross-country firm-level data. We add to their results in several aspects. First, by considering more recent firm-level data (2006–2017), as their databases span the periods 1996–1999 and 1988–2002, respectively. Second, their measures of judicial development are based on survey responses reflecting the perception of managers with respect to legal obstacles, which may be subject to different drawbacks that they discuss, while we rely on the measure of contract enforcement provided by the World Bank in its Doing Business Report. Third, Beck et al. (2006) study the largest publicly traded firms in each country which, as they caution, are relatively more likely to have access to legal resources potentially allowing them to circumvent legal obstacles.==== We use data from the World Bank Enterprise Surveys, which compile a representative sample of private formal companies in manufacturing and services sectors. Additionally, they centre on firm size as measured by sales and assets. Finally, in addition to size variables and growth in revenues and the number of workers, we also consider revenues per worker and R&D.====This article is organized as follows. In Section 2 we present our own empirical motivation and results. In Section 3, the quantitative framework is described and the characterization of the equilibrium is provided. The parameterization of the model is outlined and discussed in Section 4. In Section 5, we examine in more detail how contract incompleteness with suppliers implies wedges for the firms, how these affect incentives, and their potential size-dependence. Section 6 presents the quantitative analysis of the model. Lastly, Section 7 concludes with final comments.","Contracts, firm dynamics, and aggregate productivity",https://www.sciencedirect.com/science/article/pii/S0165188921001251,7 July 2021,2021,Research Article,192.0
"Dong Feng,Liu Jianfeng,Xu Zhiwei,Zhao Bo","School of Economics and Management, Tsinghua University, China,School of Finance, Shanghai University of International Business and Economics, China,Peking University HSBC Business School and Shanghai Jiao Tong University, China,China Center for Economic Research, National School of Development, Peking University, China","Received 30 October 2020, Revised 26 June 2021, Accepted 28 June 2021, Available online 7 July 2021, Version of Record 18 July 2021.",https://doi.org/10.1016/j.jedc.2021.104189,Cited by (8),"We empirically detect the flight to safety vis-a-vis housing in China: Great economic uncertainty causes the prices of housing assets to soar. To stabilize housing prices, China has imposed purchase restrictions on the housing market. We study the aggregate and distributional effects of this housing policy by developing a two-sector model with heterogeneous households. An uncertainty shock generates a countercyclical housing boom by shifting outward households’ demand for housing as a store of value. A vibrant housing sector then leads to an economic recession by crowding out resources that could have been allocated to the real sector. Our quantitative analysis suggests that the policy limiting housing purchases effectively curb surging housing prices. However, the policy restricts households’ access to housing that can be used to buffer idiosyncratic uncertainties, creating a larger consumption dispersion. Consequently, the housing policy creates a trade-off between macro-level stability and micro-level consumption risk sharing.","As the second-largest economy and the largest developing economy in the world, China has been the major engine of global economic growth in the past decade. Meanwhile, as in other developing economies, China generally suffers from a safe-asset shortage. On the one hand, China has considerable demand for safe assets as stores of value and contributes to the largest global saving glut. On the other hand, the underdeveloped financial system constrains the capacity of the country to produce safe assets. Furthermore, tight regulation on financial accounts intensifies the scarcity of safe assets in the domestic market as it is costly for Chinese households to hold prime assets issued by advanced economies like the US. In this paper, we provide some evidence that the shortage of safe assets makes Chinese households choose real estate assets as the store of value. The recent Chinese real estate boom along with the economic slowdown provides us with an ideal opportunity to evaluate housing assets as a store of value, or the flight to safety vis-à-vis housing.====Real estate constitutes the largest part of Chinese households’ net worth. Between 2004 and 2019, the housing wealth accounted for approximately 50% of Chinese household’s net worth, while housing loans accounted for only 10% of the housing wealth.==== Panel (a) in Fig. 1 plots the logarithmic national average real housing price in China. From 1999 to 2019, the average real housing price in China increases by approximately 1.2 logarithmic points, with an average annual growth rate of 6.2%.==== In contrast to the steady growth of housing prices, China’s economic growth has entered a stage of continuous decline since 2010. Panel (b) in Fig. 1 plots the year-on-year GDP growth rate and the Economic Policy Uncertainty (EPU).==== We observed that the EPU series rises as the economy slows down. However, we have not noticeably observed a slowdown in the growth rate of housing prices.====We argue that the continued rapid increase in housing prices under adverse economic conditions is consistent with the role of housing assets as a store of value (or safe assets, which are expected to retain their value in adverse systemic events). Uncertainty shocks increase the demand for housing as a store of value, leading to soaring housing prices. To understand why real estate is a safe asset in China, we follow Fang et al. (2016) to compare the annual growth rate of stock prices and housing prices over a longer period (1999–2019). The two price growth rates we compared are precisely what investors (who only pay attention to capital gains and ignores dividends) would value.==== The results are summarized in Table 1. We have similar findings to Fang et al. (2016): the housing price growth rate is not as volatile as the stock price growth rate. At the same time, the Sharpe ratio (defined as the mean divided by the standard deviation) of the housing price growth rate is much higher than Sharpe ratio of stock price growth rate.==== Given that the stock return is more volatile than the real return on capital (Gomme et al., 2011), we further compare the real return of non-residential capital and residential capital in China. We find that for both China and the United States, the volatility of the return on real estate capital is less than that of non-real estate capital.====In the empirical analysis, we show that great economic uncertainty leads to high housing prices. Using the households’ data from China Family Panel Studies (CFPS) 2010–2018, we find that the county-level uncertainty (measured by the standard deviation of households’ permanent labor income risks) increases homeowners’ average housing price and housing-wealth-to-income ratio. When labor income uncertainty increases by 25%, real housing prices will rise by 22% to 26%.====Motivated by the empirical findings, we construct a two-sector dynamic general equilibrium heterogeneous agent model in which housing assets emerge as stores of value. Liquidity constraints confine the households’ capacity to insure idiosyncratic uncertainties. As a result, housing assets endogenously serve as stores of value. The Euler equation for asset pricing implies that housing prices in the current period are determined by the expected price and a premium term. As the economy becomes more uncertain, the demand for housing as a store of value increases. Then, an uplifted premium for holding housing leads to a boost in housing prices. The expansion in the housing sector, in turn, diverts resources allocated to the real estate sector, resulting in an aggregate recession.====After calibrating the model to the Chinese economy, we find that a 25% increase in uncertainty (standard deviation of idiosyncratic shocks) raises the equilibrium housing price by 20%, which is consistent with the relatively large impact of income uncertainty on housing prices found in the data. In an extended model where multiple liquid assets (e.g., government bonds) are introduced, we find that the transmission mechanism in our baseline model remains important as long as the supply of other liquid assets is limited.====To stabilize housing prices, the Chinese government implemented a policy that limits the number of homes households can purchase. Correspondingly, we quantitatively evaluate this kind of policy intervention. The tractability of our model allows us to derive, in a transparent way, the process of housing prices as well as the individual optimal decisions following the government’s intervention. We show that the policy limiting home purchases can effectively impede the demand for housing and thus the housing boom. The dampened crowding-out effect from the housing sector mitigates the adverse consequences of economic uncertainty on the real sector. However, we also find that the policy intervention prevents households from investing in housing assets as a buffer for consumption risks and reduces the degree of consumption insurance. As a result, social welfare is reduced due to the rising consumption growth dispersion. We conclude that there exists a trade-off between aggregate housing price stability and consumption risk sharing using housing as a safe asset.====The current paper is generally related to a large body of the literature, which we do not attempt to go through here. Instead, we highlight only the papers that are most closely related to this study.====First, the flight to quality (safety) and liquidity during the last financial crisis has created considerable demand for the analysis of the shortage of safe assets. Our paper contributes to this strand of the literature. Caballero et al. (2016) and Caballero and Farhi (2017) explore the macroeconomic implications of safe asset shortages. Another relevant paper, Quadrini (2017), shows that, in addition to the standard lending channel, financial intermediation affects the real economy through a novel banking liability channel by issuing liabilities, which are recognized as safe assets by agents facing uninsurable idiosyncratic risk. The underlying mechanism of the global scarcity of safe assets and its aggregate consequences has been well documented in the recent literature (e.g., Gorton, Ordonez, 2013, He, Krishnamurthy, Milbradt, 2016; ====). While theoretical works mainly focus on safe assets in the form of debt instruments and their impacts on advanced economies, real assets (especially housing) that are stores of value and their resulting consequences on developing economies are rarely explored. To this end, we fill the gap in the literature by using the Chinese economy as a laboratory to study the conundrum of safe-asset shortage.====Second, our paper contributes to the housing literature through the anatomy of the aggregate and distributional effects of housing policies. Iacoviello (2005) and Liu et al. (2013) show that the collateral channel induced by housing can stimulate private investment. Chen et al. (2016) find that China’s housing boom crowds out real investment. Fang et al. (2016) empirically find that housing prices have experienced enormous appreciation from 2000 to 2012, which was accompanied by equally impressive growth in household income, except in a few first-tier cities. Zhao (2015) shows in an OLG model that the housing bubble may emerge as a store of value due to the tight financial friction. Chen and Wen (2017) also argue that China’s housing boom is a rational bubble emerging naturally from its economic transition. In contrast, Han et al. (2018) link housing values to fundamental economic variables such as income growth, demographics, migration, and land supply. Dong et al. (2019) integrate housing investment into a New Keynesian DSGE model for the Chinese economy and quantitatively evaluate the implications of stabilization policies.",Flight to housing in China,https://www.sciencedirect.com/science/article/pii/S016518892100124X,7 July 2021,2021,Research Article,193.0
"Galí Jordi,Giusti Giovanni,Noussair Charles N.","Centre de Recerca en Economia Internacional (CREI), Universitat Pompeu Fabra and Barcelona GSE, Spain,Escola Superior de Ciències Socials i de l’Empresa de Tecnocampus-Universitat Pompeu Fabra and Economic Science Laboratory at the University of Arizona, USA,Department of Economics, University of Arizona, USA","Received 1 December 2020, Revised 14 May 2021, Accepted 18 June 2021, Available online 6 July 2021, Version of Record 19 July 2021.",https://doi.org/10.1016/j.jedc.2021.104184,Cited by (2),"Leaning-against the-wind (LAW) policies, whereby ==== in place. We observe that the bubble increases (decreases) when ==== are lower (higher) in the period of a policy change. However, the opposite effect is observed in the following period, when higher (lower) interest rates are associated with greater (smaller) bubble growth. Direct measurement of expectations reveals that traders expect prices to follow previous trends and tend to correct for prior errors in their predictions.","The Global Financial Crisis of 2007-2008 and the subsequent Great Recession, commonly attributed to the bursting of housing bubbles in a number of countries, has shown how damaging the effects of a collapse in asset prices can be to the real economy. This episode has renewed the debate regarding the stance that central banks should take in response to a growing asset bubble. The common view among policy makers before the crisis was that central banks should restrict their mandate to the stabilization of inflation and the output gap. This view, however, has not gone unchallenged in the aftermath of the crisis, with many authors and policymakers arguing for a more active role for central banks in preventing overinflated asset prices by means of ”Leaning Against the Wind” (henceforth, LAW) monetary policies. A policy of this type specifies that the interest rate be raised in response to asset price increases that are viewed as purely speculative, i.e. not justified by fundamentals, in order to attenuate the growth of a bubble. The rationale for LAW is that higher interest rates increase the opportunity cost of holding a a bubble asset, reduce its demand, and lower the size of the bubble.====The use of LAW policies in response to asset price bubbles has been criticized on several grounds. Firstly, it is argued that in practice it is difficult, if not impossible, to establish whether or not asset prices reflect their fundamental values, given the difficulty in measuring the latter. Secondly, such a policy is viewed as potentially having undesired consequences on sectors of the economy not affected by the bubble. Thirdly, Galí (2014) calls into question, on theoretical grounds, the notion that a higher interest rate would reduce the size of the bubble. He argues that, if agents are rational, the bubble component must grow in expectation at the rate of interest. As a result, a higher interest rate could end up increasing, rather than decreasing, the size of the bubble.==== On the other hand, Miao et al. (2019) point to the existence, under certain conditions, of equilibria in which a bubble decreases in size in response to an increase in the interest rate.====In this paper, we study the relationship between interest rate policy and bubble dynamics in the laboratory. The use of laboratory experimental methods provides a controlled environment, in which we can isolate the impact of different interest rate policies on the size and evolution of an asset price bubble. In the experiment, the bubble component of the asset price can be observed with precision: it corresponds to the price itself, since the fundamental component is zero. Different interest rate policies can be studied and their effects compared, while keeping all other aspects of the environment constant.====Our experimental environment has an overlapping generations structure. Each participant plays the role of ==== and ==== consecutively over two periods. When young, participants decide how to allocate their cash endowment between a single-period riskless bond, yielding a known interest rate, and a long-lived asset that pays no dividends. The only incentive to purchase the long-lived asset comes from the possibility of reselling it to a ”young” participant in the following period, hopefully with a capital gain. A positive price for that asset indicates a pure bubble. When they are old, participants collect the principal plus interest from their bonds, as well as the proceeds from the sale of the asset to the current young generation.====Within this environment, we implement three different monetary policies, each specifying a particular rule determining the interest rate on the riskless bond. Each of these policies represents a treatment in our experiment. In the first two treatments, the interest rate remains constant at either a ”low” (3%) or a ”high” (15%) level, over the entire experimental session. In the third treatment, the interest rate varies as a function of the change in the price of the long-lived asset in the previous period. The economy begins with a ==== interest rate, which we raise (lower) by 3% each time that the asset price increases (declines) by more than 10% from one period to the next. We interpret this third treatment as a LAW policy.====For an asset that pays a dividend and is priced at its fundamental value, a higher interest rate lowers the asset’s present discounted dividend stream and thus its price. A LAW policy thus has the effect of lowering asset prices when interest rates are raised and increasing prices when rates are lowered. Higher rates increase the opportunity cost of committing capital to an asset. The attraction of using LAW policies to combat asset prices that seem too high appears to be based on this logic. However, as argued by Galí (2014), if agents have rational expectations, an asset whose price has a bubble component may behave in the opposite manner. While the initial effect of a change of interest rate on price is indeterminate, the bubble component would subsequently grow at the rate of interest (and at a greater rate if traders are risk averse and the asset carries some risk), meaning that higher interest rates may lead to larger bubbles, all else equal. In our experiment, we study the behavior of an asset that has ==== a bubble component, a favorable scenario for a LAW policy to be counterproductive. If we observe that the policy is effective in stabilizing asset prices in our study, it would suggest that the theoretical arguments against the use of a leaning against the wind policy rely on assumptions that are not satified in our environment, and that may not be satisfied in the outside world.====Our experiment is designed to address the following questions. (1) Do bubbles grow faster under high or low interest rates? (2) What is the initial effect of an interest rate change on the size of a bubble? (3) What is the effect of an interest rate change on the size of the bubble in the period subsequent to the change?====Our main findings can be summarized as follows. First, the average price increase for the risky asset is close to the riskless rate when the latter is low and constant, but significantly smaller when the riskless rate is high. Second, a LAW policy of increasing (decreasing) interest rates in response to asset price increases (decreases) has two effects. There is an immediate effect in the current period of decreasing (increasing) the size of the asset price bubble. However, in the subsequent period, the result is reversed and the higher (lower) interest rates tend to exacerbate (mitigate) asset price bubbles. Our results thus suggest a means of reconciling the apparent success of LAW policies in guiding asset price bubbles in the short run with the theoretical claims arguing that they would be counterproductive afterward. Increasing interest rates reduces a bubble in the short-run, but increases it thereafter. The latter pattern is consistent with the model of Galí (2014).====Some of the asset pricing patterns suggest that the assumption of rational expectations may not be appropriate. To study this possibility, we conduct a follow-up experiment. This experiment assesses how participants form their expectations about future asset prices. We find that expectations are backward-looking, with adaptive and trend extrapolating elements, rather than rational, suggesting that the source of the departures from the theoretical framework lies in the manner that agents form expectations. We then argue that assuming ==== or ==== expectations can account for a number of patterns in our data.====The paper is structured as follows. In Section 2, we review the related literature and in Section 3 we propose a benchmark theoretical model. In Section 4, we describe the experimental design and state our hypotheses. The results are reported in Section 5. We discuss the role of expectations in Section 6 and we conclude with a discussion in Section 7.",Monetary Policy and Asset Price Bubbles: A Laboratory Experiment,https://www.sciencedirect.com/science/article/pii/S0165188921001196,6 July 2021,2021,Research Article,194.0
"Luo Shaowen,Villar Daniel","Department of Economics, Virginia Tech, 3016 Pamplin Hall, 880 West Campus Drive, Blacksburg, VA 24061, United States,Federal Reserve Board of Governors, 20th & Constitution Ave. NW, Washington, DC 20551, United States","Received 13 October 2020, Revised 9 April 2021, Accepted 20 April 2021, Available online 6 July 2021, Version of Record 28 July 2021.",https://doi.org/10.1016/j.jedc.2021.104135,Cited by (2)," periods. We find that the relation between ==== and higher moments of the price change distribution is particularly informative for the shape of the hazard function. Our estimated hazard function is relatively flat with positive values at zero. It implies weak price selection and a high degree of monetary non-neutrality: about 60% of the degree implied by the Calvo model, and much higher than what menu cost models imply. In addition, our estimated function is asymmetric: price increases are considerably more likely to occur than price decreases of the same magnitude.","It has long been established that monetary policy has real effects (or is non-neutral) if firms face constraints in changing the prices of their goods and services (i.e. if prices are “sticky”). This has led to a proliferation of micro-founded models of price-setting that feature various nominal rigidities (Caplin and Spulber, 1987, Golosov and Lucas, 2007, Midrigan, 2011). These studies have notably shown that a key determinant of monetary non-neutrality is the extent to which the prices that change are selected based on their misalignment, the selection effect. While most work in this field has involved micro-founded models, Caballero and Engel (1993) proposed an alternative, model-free approach to analyze the relation between sticky prices and non-neutrality. They introduced the state-dependent price adjustment hazard function: the probability of an individual price changing, as a function of the gap between a firm’s current and desired price. This model-free approach also provides a way to determine monetary non-neutrality through the mapping between the hazard function and the flexibility of the price level (as derived in Caballero and Engel, 2007). In particular, every price setting model implies a certain hazard function, thus this general approach does not require formulating and solving a specific model.====In this paper, we estimate the state-dependent price adjustment hazard function using the micro data that underlies the U.S. Consumer Price Index (CPI) from 1977 to 2014.==== While other studies have offered estimates of the hazard function (such as Berger and Vavra, 2018, Caballero, Engel, 1993, Gagnon, Lopez-Salido, Vincent, 2012, Caballero and Engel, 2006), our estimation is based on a richer data set covering high inflation periods and novel empirical moments, notably the correlation between inflation and the skewness of the price change distribution. Our estimates uncover two features of the hazad function that have important implications for monetary policy and inflation dynamics. First, we find that the slope of the hazard function is small. As we will show, using the correlation between inflation and price change skewness is what enables us to determine the slope of the hazard function. This property also implies a high degree of monetary non-neutrality: about 60% of the degree implied by the Calvo model. Second, the hazard function is asymmetric: price increases are considerably more likely to occur than price decreases of the same magnitude.====The slope of the hazard function is particularly important because of its close relation to the selection effect in price setting. Caballero and Engel (2007), in deriving the mapping between the hazard function and aggregate flexibility, made explicit how the selection effect depends critically on the slope of the hazard function. A large slope means that more misaligned prices have a much higher probability of changing. Consider the standard menu cost model as an extreme state-dependent example: firms decide to change their price if and only if the price misalignment is large enough to justify the fixed adjustment cost. The hazard is zero within the inaction region, and one elsewhere, which implies an infinite slope at the inaction thresholds. Consequently, the model implies a large aggregate price response and small real effects of monetary shocks, given that adjusting firms are the ones that react strongly to nominal shocks. Conversely, a model (such as a Calvo model) with a small hazard function slope generates weak aggregate price responses to nominal shocks, thus high monetary non-neutrality.====In order to infer the slope of the hazard function, we use patterns related to the skewness of price change that are not considered by previous attempts to estimate the hazard function. If the hazard function slope is large, the resulting price dynamics will imply that the skewness of price changes falls with inflation. Price change skewness does not fall with inflation in our data, which leads us to conclude that the hazard function must have a small slope (and that non-neutrality is high). In order to determine the empirical relation between inflation and skewness, it is necessary to observe the skewness in periods of intermediate or high inflation. Most price data sets used in the literature only cover recent periods in which inflation has been low and stable (such as the U.S. CPI data from 1988 onwards, Dominicks and the Nielsen Homescan Dataset), making them unsuitable for our approach. Instead we use the U.S. CPI micro data extended back to 1977, which covers periods of high and intermediate inflation.====The skewness patterns that help identify the hazard function slope has been discussed throughly in Luo and Villar (2020), where we showed that a broad class of menu cost models predict that skewness falls with inflation. Only models that are weakly state-dependent could match the non-negative inflation-skewness correlation, and such models predict a high degree of non-neutrality. While we conclude that non-neutrality is high in both papers, the results in this paper are, in a sense more general. In Luo and Villar (2020), we present a random menu cost model characterized by firm-level random menu cost draws from a calibrated distribution in every period (as in Dotsey et al., 1999). Instead, this paper uses a hazard function approach, which is model-free. Our results here reject hazard functions with a large slope, and thus all models that would imply such a hazard function.====We want to highlight that our results also offer insight into the price setting process beyond the magnitude of the hazard function slope. For instance, our estimated hazard function takes a significantly positive value at zero. Thus, even extremely small price imbalances have a positive probability of occurring (so there are no inaction regions). In addition, the hazard function makes it clear that the probability of price adjustment stays far below one even for large price misalignments. This is related to the small slope feature of the hazard function and is contrary to what would be obtained from menu cost, or hybrid menu cost models. These two features together mean that the hazard function implies a high degree of monetary non-neutrality. As we will show, this is because our estimated hazard function features a small amount of price selection compared to menu cost models. This result is consistent with other studies that find that non-neutrality is high, such as Costain and Nakov (2011), Luo and Villar (2020) or Karadi et al. (2020).====Another interesting feature of our estimated hazard function that is that the hazard function is asymmetric around zero. Specifically, for an equivalent magnitude, price increases are more likely to occur than price decreases. We show that the asymmetry of the hazard function implies that inflation responds more quickly to expansionary aggregate shocks than to contractionary shocks, which could have important implications for the effectiveness of monetary policy and for inflation dynamics. These findings are relevant for attempts to model the constraints faced by firms in setting and changing their prices.====The fact that the hazard function involves an unobserved variable (the desired price gap) makes it particularly difficult to estimate. An estimation strategy must make assumptions about the desired price gap to relate different hazard functions to observable facts about prices and price changes. Two recent papers (Berger and Vavra, 2018, Petrella et al., 2018) also attempt to estimate the price adjustment hazard function with assumptions that differ from those in our paper. One difference is that both of those papers estimate a flexible functional form for the distribution of desired price gaps and a quadratic hazard function. In contrast, we estimate a more restricted form for the distribution of desired price gaps, but a more flexible form for the hazard function. Perhaps more importantly, we use a broader set of empirical moments in our estimation. Our approach has the advantage of using information from the variation of price change moments over time (mainly in the form of the skewness correlation previously discussed) to identify the key parameters of the hazard function. As we will show, these moments provide a considerable amount of information that identifies the slope of the hazard function.==== In addition, it is also the moment correlations that allow us to determine the asymmetry of the hazard function. Finally, our data sample includes the high inflation periods of the U.S. going back to 1977, which gives us a more reliable estimate of the moment correlations with inflation.====Another recent paper, Alvarez et al. (2020), presents a theoretical framework that provides a mapping between the hazard function and the observable distribution of realized price changes, which is closely related to this paper. An important difference of that framework from ours is that it is based on a hazard function that is symmetric around zero. In contrast, our approach allows for a more flexible hazard function. Furthermore, we find evidence suggesting that the hazard function is considerably asymmetric, which appears to be robust under various specifications. Finally, while Alvarez et al. (2020) and Alvarez et al. (2016) emphasize the kurtosis of price change distribution being one key component in the sufficient statistics to assess monetary non-neutrality, in this paper and Luo and Villar (2020), we show that the correlation between skewness and inflation plays an important role at identifying the properties of the random menu cost distribution, the hazard function and monetary non-neutrality. It is also interesting to notice that our finding is related to the discussion by Baley and Blanco (2021). They present a theoretical framework that can generalize the Alvarez et al. (2016) result to lumpy price adjustment environment with non-zero drifts. They find that the sufficient statistics assessing non-neutrality switch from kurtosis to skewness of the price change distribution,which relates to our discussion here.====This paper (as well as Berger and Vavra, 2018, Petrella et al., 2018) contribute the sticky price literature with non-structual estimates of price changes following Caballero and Engel, 1993, Caballero and Engel, 2007 using high order moments of price changes. An important finding in our work is that an asymmetric hazard function fits the data best, which implies a stronger monetary non-neutrality during deflationary episodes. Berger and Vavra (2018), in contrast, estimate a symmetric quadratic hazard function that can not generate this result. Petrella et al. (2018) estimate a hazard function that is potentially asymmetric, but they do not seem to find a major role for asymmetry.====The rest of this paper is organized as follows. Section 2 formalizes the hazard function approach, illustrates it with various examples, and describes our estimating method. Section 3 presents our main results: the general hazard function estimates (parametric and non-parametric), and an illustration of how different key features of the hazard function are identified by our set of moments. In Section 4, we derive results on monetary non-neutrality from the estimated hazard function. Finally, we provide some concluding remarks in Section 5.",The price adjustment hazard function: Evidence from high inflation periods,https://www.sciencedirect.com/science/article/pii/S0165188921000701,6 July 2021,2021,Research Article,195.0
Prüser Jan,"TU Dortmund Department of Statistics, Dortmund 44221, Germany","Received 12 February 2021, Revised 23 June 2021, Accepted 24 June 2021, Available online 30 June 2021, Version of Record 12 July 2021.",https://doi.org/10.1016/j.jedc.2021.104188,Cited by (7),"Time-varying parameter VARs have become the workhorse models in empirical ====. These models are usually equipped with tightly parametrized prior distributions which favor a small and gradual change in parameters. Do such prior distributions suppress some degree of time variation in the VAR coefficients? We address this question by proposing a flexible global-local prior, namely the horseshoe prior. It turns out that conventional priors may suppress economically relevant patterns of time variation. Using the global-local prior, we observe that parameter change and changes in systematic ==== can be abrupt rather than smooth. Furthermore, we provide a comparison of the horseshoe prior with a range of plausible alternatives. Finally, we find that a VAR with a ==== specification using the horseshoe prior is well suited to modelling the extreme observations due to Covid-19. In contrast, the conventional prior (spuriously) picks up an increase of volatilities even before the Covid crisis.","There is an ongoing debate about the causes of the Great Moderation. Some authors (e.g., Primiceri (2005), Sims and Zha (2006)) emphasize that the variance of the exogenous shocks was higher in the 70s and early 80s than in the rest of the sample. Other authors (e.g., Boivin and Giannoni (2006), Cogley and Sargent (2001) and Lubik and Schorfheide (2004)) emphasize the changes in the transmission mechanism, i.e., the way macroeconomic variables respond to shocks. Particular attention has been given to monetary policy. It is often argued that the reaction of the Fed to inflation has changed over time (e.g., under the Volcker chairmanship, the Fed was more aggressive in fighting inflation pressures). The time-varying parameter VAR (TVP-VAR) of Cogley and Sargent (2005) and Primiceri (2005) allows both the VAR coefficients and the shock size to change over time. This provides a flexible framework for the estimation and interpretation of time variation in the systematic and non-systematic part of monetary policy and their effect on the rest of the economy. Primiceri (2005), Canova and Gambetti (2009), Benati and Mumtaz (2007), Gambetti et al. (2008), Koop et al. (2009) and Chan and Eisenstat (2018) find evidence that points towards a decisive role played by a time-varying shock size.====The TVP-VAR is flexible and can capture many different forms of structural instabilities and the evolving nonlinear relationships between the variables. In order to regularize the degree of time variation of the parameters, TVP-VARs are typically equipped with tightly parametrized prior distributions, favoring a small and constant gradual change in parameters. Is it possible that such prior distributions suppress some degree of time variation in the VAR coefficients? We address this question by proposing a flexible global-local prior, namely the horseshoe prior, to regularize the degree of time variation in the parameters. In addition to a gradual change in parameters, it can also favor abrupt changes of different size if empirically warranted. This flexibility turns out to be important for uncovering economically relevant patterns of time variation. We contribute to the literature by studying the influence of the different prior distributions on the empirical findings and provide a simulation study which compares different prior distributions.====We find that the conventional (inverse gamma) prior may suppress some degree of time variation in the coefficients. Using the more flexible horseshoe prior, we observe that parameter changes can be more abrupt rather than smooth.==== In particular, we provide empirical evidence that the systematic long-run response of the Fed to a permanent change in inflation frequently changes over time. Results obtained by using the inverse gamma prior show that the long-run response changes rather smoothly over time. In contrast, the horseshoe prior reveals that the long-run response shows both smooth changes as well as abrupt changes. Furthermore, the response of monetary policy to supply shocks has changed over time. It turns out that during the chairmanship of Paul Volcker (1979–1987) the Fed has been fighting inflation pressures by raising the interest rate in response to a negative supply shock. During the chairmanship of Alan Greenspan (1987–2006), this policy came to an end. These changes are not observed with the conventional prior. However, both prior distributions confirm the view that the variance of the exogenous shocks was higher in the 70s and early 80s than in the rest of the sample. Overall, we find that both unsystematic and systematic monetary policy have changed over time.====In a simulation study we first compare the horseshoe prior with the inverse gamma prior to shed some light on the implied dynamics by the different priors. The simulation study reveals that the estimates under the inverse gamma prior are overly smooth. In contrast, the horseshoe prior is much better suited to capture abrupt breaks. In addition, the estimates under the inverse gamma prior may indicate that there is no time variation even though there is time variation and the horseshoe prior is able to detect it. Furthermore, we compare the estimation accuracy of the horseshoe and inverse gamma prior with the inverse Wishart prior, the horseshoe plus prior and the normal gamma prior. Moreover, we consider to estimate the hyperparameters of the inverse gamma and inverse Wishart prior using the approach of Amir-Ahmadi et al. (2020). We find that the horseshoe prior performs particularly well in comparison with the other priors if there is a large abrupt structural break in the coefficients. However, the horseshoe prior performs also reasonably well in case of many small breaks in coefficients.====Furthermore, we compare the impact on the empirical results using the different priors. While we find that the horseshoe plus prior provides results similar to the horseshoe prior, the inverse Wishart prior and the inverse gamma prior with estimated hyperparameters provide results similar to the inverse gamma prior with fixed hyperparameters. In contrast, the normal gamma prior allows for an unreasonable degree of time variation and thereby overfits the data. Hence, the horseshoe prior seems to provide a good balance between overfitting that data (normal gamma prior) and suppressing interesting patterns of time variation (inverse gamma prior). In this sense we provide evidence that the horseshoe prior may be a good default choice or at least can be used as a sensitivity check.====Finally, we study how the TVP-VAR with the inverse gamma and horseshoe prior can handle the sequence of extreme observations due to Covid-19. These extreme observations can have quite a big influence on the parameter estimates. To address this Lenza and Primiceri (2020) propose to explicitly modeling the change in the shock volatility to account for the exceptionally large macroeconomic innovations during the period of the pandemic and Carriero et al. (2021) adopt an outlier-adjusted stochastic volatility model for VAR residuals that combines transitory and persistent changes in volatility. We find that a VAR with a stochastic volatility specification using the horseshoe prior turns out to be well suited to modelling the extreme observations due to Covid-19. In contrast, in turns out that the inverse gamma prior (spuriously) picks up an increase of volatilities even before the Covid-19 crisis.====On a broader level, by proposing a global-local prior for TVP-VARs, we contribute to the literature of modeling TVP-VARs. The prior specification of Koop et al. (2009) can discriminate between very few (but usually large) breaks or many (usually small) breaks in the parameters. They find evidence for gradual change in all of their parameters and reinforce the findings of Primiceri (2005). Koop et al. (2009) use the algorithm of Gerlach et al. (2000) to estimate a single latent indicator for each period to discriminate between time constancy and parameter variation in the autoregressive coefficients, the covariances, and the log-variances, respectively. This assumption, however, implies that either all autoregressive parameters change over a given time frame or none of them do. Huber et al. (2019) circumvent this issue by avoiding the computationally intensive simulation of the latent indicators by proposing a straightforward approximation to these indicators. A potential shortcoming of both prior specifications is that they favor either no parameter change or parameter change of a constant size. Thus, both prior specifications assume two extreme cases: either there are few larger changes or many small changes. As is often the case, we argue that the truth lies in between. We find evidence for both: many small breaks and a few large breaks of different sizes. Another related approach is proposed by Cogley et al. (2010). They use a prior specification that allows the amount of time variation to change gradually over time. Thus, it might not be ideally suited to model cases of abrupt parameter changes. Moreover, it requires a careful parametrization of the prior distributions, see Cogley et al. (2010) for a discussion. In contrast, to other approaches, our prior specification does not require any tuning of prior hyperparameters.====The remainder of this paper is organized as follows. Section 2 lays out and discusses the econometric framework. Section 3 contains a simulation study. Section 4 provides an overview of the data and presents the empirical findings. Section 5 concludes.",The horseshoe prior for time-varying parameter VARs and Monetary Policy,https://www.sciencedirect.com/science/article/pii/S0165188921001238,30 June 2021,2021,Research Article,196.0
"Jeong Hanbat,Lee Lung-fei","Department of Economics, The Ohio State University, 1945 N. High Street,Columbus, Ohio","Received 13 October 2020, Revised 25 April 2021, Accepted 18 June 2021, Available online 24 June 2021, Version of Record 6 July 2021.",https://doi.org/10.1016/j.jedc.2021.104186,Cited by (0),"This paper introduces a spatial ==== describing local agents’ intertemporal decision-making with their network interactions and network evolution. Our model’s purpose is to give a tool to analyze local governments’ behaviors when there exist network interactions among them with endogenously changing networks. To provide a theoretical foundation of our model, we establish a network interaction model for forward-looking agents. An agent’s current action can affect his future network links via time-varying economic indicators. To estimate the model’s parameters, we consider a GMM estimation method based on first-order conditions of agents’ approximated lifetime problems. ","This paper establishes a new spatial dynamic panel data (SDPD) model by considering endogenously changing networks based on an economically founded model. In our hypothetical economy, there are ==== forward-looking local agents (e.g., local government) and their socio-economic relations are represented by network links. Each local agent sequentially chooses a local expenditure on public goods to support its residents. Since a local agent's decision on public services can improve its future economic consequences and lead to welfare-motivated moves of its residents, strategic interactions among local agents can arise and change over time. Hence, our research question is how a local agent chooses its public expenditure when his decision changes his relationship with other local agents.====We formulate local agents’ intertemporal decisions by a network game with evolving networks. After that, we build estimation methods for recovering parameters of agents’ payoff functions when their decisions are recorded in a panel data set. Our model has an explainable channel of endogenously evolving networks. Jeong and Lee (2020) specify the forward-looking agents’ decisions with a fixed network constructed by geographic distances. As contrary to their work, we assume that each network link can also be affected by time-varying economic consequences (e.g., average personal income of residents). An agent’s current public expenditure can improve his future economic indicator. It leads to changing his socio-economic relations with other agents. In consequence, each forward-looking agent decides on its public expenditure by considering that his decision can change his future socio-economic relations.====Also, our model provides a tool to study local government’s decision-making under the intertemporal decision-making framework. Studying a central government policy mostly relies on a dynamic optimization model since its policy is sequentially determined. That framework is justified by the maximization of a representative infinite-lived agent’s lifetime utility (Barro (1990)). Traditional SDPD models (e.g., Lee and Yu (2010)) have provided a tool to analyze local governments’ policy interactions. However, the existing SDPD models and the treatments of time-varying and/or endogenous networks are based on myopic behavior, which does not concern inter-temporal optimization in decision-making. Since decisions of local agents also arise for each period, analyzing a local government’s policy should be considered by the forward-looking agent framework. By Brueckner (2003) and Revelli (2005), a local government’s payoff function can also be interpreted as a utility function of a local representative resident. Our study’s major distinction for local governments’ behaviors is to consider their intertemporal strategic interactions with endogenously evolving networks.====Due to endogenously evolving networks, the resulting economic structure is more complex than that of Jeong and Lee (2020). We establish linear moment orthogonality conditions by Euler equations (Hansen, Singleton, 1982). This approach has an advantage in implementation since we can identify the parameters relying on linear moment orthogonality conditions rather than fully revealing the complex model’s structure. The Euler equation provides the structure of local agent’s decision, which consists of the first-order derivative of the agent’s current payoff and the expected discounted value function. Using the Euler equation system, we can study the marginal effect of changing a current action on future spatial network links. By estimating some parameters in Euler equations, we can detect the existence of co-dynamics between agents’ current actions and their future network links. However, a closed-form value function does not exist since a future network link is a highly nonlinear function of agents’ current actions.==== Since a large number of local agents ==== implies a large number of state variables, numerically evaluating value functions is computationally infeasible when ==== is large. Hence, we approximate each agent’s value function as a linear-quadratic (LQ) function of state variables, and replace the true value function components in the Euler equation with the approximated ones.====With the linear moment orthogonality conditions, the nonlinear two-stage least squares estimation (NL2S) method can be considered. To improve estimation efficiency, in addition to instrumental variables (IV) moment conditions, we also consider quadratic moment conditions capturing spatial correlation. By those moment conditions, we study the GMM estimation for our model. With both large numbers of spatial units (====) and time-series observations (====), consistency and the asymptotic distribution of the GMM estimator (GMME) are studied. As a traditional panel data model, our model accomodates individual and time fixed effects including agents’ time-invariant characteristics and common shocks. From the derived asymptotic distribution, we observe the existence of asymptotic biases due to incidental parameters of individual and time effects. Hence, we suggest a bias correction method. From Monte Carlo simulations, we observe reasonable performance of NL2SE and GMME. A large efficiency gain of using a quadratic moment is detected in estimating the network interaction parameter. A bias correction for GMME performs well.====Lastly, we apply our model to study policy interdependence of U.S. states’ public welfare expenditures. A spatial-economic network at each period is constructed by geographic and economic distances. An economic distance between two states is captured based on their personal income levels. We capture that a state’s public welfare expenditure is reinforced by neighboring states’ decisions. This positive spillover effect provides evidence of welfare competition among the U.S. states. We find the existence of coevolution of states’ public welfare expenditures and their spatial-economic networks. A state government has an incentive to consider its future relations when it decides its public welfare expenditure.",Spatial dynamic game models for coevolution of intertemporal economic decision-making and spatial networks,https://www.sciencedirect.com/science/article/pii/S0165188921001214,24 June 2021,2021,Research Article,197.0
Gelfer Sacha,"Bentley University, Waltham, MA, USA","Received 29 May 2020, Revised 13 April 2021, Accepted 15 June 2021, Available online 18 June 2021, Version of Record 2 July 2021.",https://doi.org/10.1016/j.jedc.2021.104177,Cited by (3),This paper examines the inferences and forecasting benefits that can be made when one incorporates a large quantity of economic time series into international structural ,"New open-economy macroeconomics (NOEM) have generated significant empirical and theoretical innovations in international macroeconomic modeling. Estimated dynamic stochastic general equilibrium (DSGE) models became the marquee policy analysis tools for most central banks around the world because of their structural foundations and their competitive forecasting performance against other reduced-form forecasting models such as vector autoregressions (Gurkaynak et al., 2013) and judgement based forecasts by experts (Kolasa et al., 2012). However, many of the DSGE models’ forecasts that have been evaluated empirically are centered around a closed-economy and have had significant declines in their predictive power following the Great Recession. (Del Negro and Schorfheide, 2013).====Computational gains have led to additional estimation techniques to become possible for medium scale models that were previously thought to be computationally burdensome. One example includes the estimation of DSGE models using a high dimensional data vector, first introduced by Boivin and Giannoni (2006), and often referred to as DSGE dynamic factor model estimation or DSGE-DFM estimation for short. Gelfer (2019) found that a closed-economy DSGE model with financial frictions for the United States estimated in the DSGE-DFM fashion was able to significantly out-forecast modern DSGE models not estimated in a data-rich environment and the Survey of Professional Forecasters (SPF), in regard to core macroeconomic growth variables and many labor and financial metrics.====In this paper, I examine the inferences and forecasting benefits that can be made when one incorporates the DSGE-DFM estimation technique of Boivin and Giannoni (2006) and Gelfer (2019) into a small open-economy DSGE model with capital, analogous to the model used in Adolfson, Laséen, Lindé, Villani, 2007, Adolfson, Laséen, Lindé, Villani, 2008. The data set I use in the paper to estimate the DSGE-DFM model includes 93 series of Canadian, American, Asian and European macro-financial data. To my knowledge, this is the first time an open-economy DSGE model has been estimated in this fashion. Given the construction of traditional open-economy DSGE model estimation (henceforth, DSGE-Reg) researchers were only able to evaluate the forecasts and dynamics of a few macroeconomic series, while the DSGE-DFM model of this paper will be able to look at the dynamics and forecasts of such series as domestic employment by sector, domestic credit measures and international financial series.====After estimating the open-economy DSGE-DFM model for Canada, I find that many of the structural parameter estimates are more in line with firm level survey evidence and that many of the exogenous shocks are estimated to be far less volatile and less persistent than the open-economy DSGE-Reg model of this paper. As a result, many of the theoretical foundations and co-movements the structural model is built upon still hold when I compare the impulse response functions (IRF’s) of the DSGE-Reg model to the IRF’s of the DSGE-DFM model, but the magnitudes and inertia of the model’s variables are very different.====For example, the DSGE-DFM model’s variance decomposition attributed to international shocks for domestic GDP growth and its components is much larger when compared to the DSGE-Reg model of this paper and other open-economy DSGE models in general. Further, the risk premium shock (FX shock) of the DSGE-DFM model plays a significant role in the variance of domestic inflation and the policy rate in the long-run. This result is what will drive the more accurate forecasts and international co-movements that I see in the DSGE-DFM model of this paper. The large amount of real-time global macro-financial data that is incorporated in the open-economy DSGE-DFM model can help identify the global and FX shocks, which in return help identify the dynamics of the domestic economic variables.====Open-economy DSGE models have been known to have three drawbacks connected to them. First, the strong theoretical structure entrenched in them has not resulted in improvement in the forecast accuracy for domestic macroeconomic variables. In some cases open-economy DSGE model forecasts of the domestic variables can be less accurate than their closed-economy DSGE model counterpart (Kolasa and Rubaszek, 2018). Second, open-economy DSGE models shed little light on the short and medium term dynamics of real and nominal exchange rates. They often result in the inability of open-economy DSGE models to out-perform a naive random walk forecast for the real and nominal exchange rate in the short to medium term for a variety of countries (Ca’Zorzi et al., 2017). Third, small open-economy models fail to generate the cross-correlation that empirically exists between the domestic economy and its international trading partners when it comes to the business cycle and price co-movements (Corbo, Strid, et al., De Walque, Jeanfils, Lejeune, Rychalovska, Wouters, Justiniano, Preston, 2010).====The contribution of this paper is to provide an evaluation of whether open-economy DSGE models estimated in a data-rich environment can be successful in mitigating any of these three key issues of open-economy DSGE models that are estimated in the traditional fashion. DSGE-DFM estimation has the potential to address these three as some of the additional data used in the estimation may be informative about the exogenous shocks or other state variables. For example, some exogenous shocks or other state variables, that are assumed to be unobserved, may be partially observed by a combination of other domestic and international data sets. Utilizing the information from additional series could result in more accurate estimations of the model’s parameters and states and thus creating more accurate conclusions about the model’s dynamics. The key insight of this paper is that in fact, an open-economy DSGE-DFM model can mitigate all three of these issues.====First, like Kolasa and Rubaszek (2018), I find that the open-economy DSGE-Reg model does not produce better forecasts for GDP growth, consumption growth, investment growth, interest rates and inflation when compared to the Bank of Canada’s Staff Economic Projections (SEP) or other reduced from forecasting models. However, the open-economy DSGE-DFM model that incorporates real-time domestic and international macro-financial data was able to significantly out-forecast both the SEP and the open-economy DSGE-Reg model for domestic growth variables, export growth and import growth in regard to both the short and medium term horizon.====Second, in accordance with Ca’Zorzi et al. (2017), I find that the open-economy DSGE-Reg model of this paper is able to slightly out-forecast a naive random walk for the real exchange rate, but not the nominal exchange rate. However, the open-economy DSGE-DFM model of this paper is able to out-forecast a naive random walk in regard to both the real exchange rate and the nominal exchange rate when it incorporates real-time international macro-financial data that would have been currently available to the econometrician.====Third, in contrast to Justiniano and Preston (2010), I find that there are significant and empirically validated co-movements in the DSGE-DFM model of this paper between the United States and Canada. When I evaluate the model implied correlation between the Canadian economy and the U.S. economy, I find that GDP growth and its components resemble empirical correlations seen in the data. The DSGE-DFM model is still not able to produce any policy rate correlation between the United States and Canada and a smaller than empirically-observed correlation for inflation between the two countries. However, when I input values for United States macro-financial data into the model that correspond to an empirical generic U.S. recession of the last 70 years, I am able to generate the probabilistic co-movements and correlations between Canada’s and the United States levels of inflation, policy rates and expenditure growths.====I choose to use Canada as the domestic country in the open-economy model for various reasons. First, Canada is included in the forecasting applications of both Kolasa and Rubaszek (2018) and Ca’Zorzi et al. (2017). Second, I will be able to directly address the findings of Justiniano and Preston (2010) in this paper. Finally, the extensiveness of the Bank of Canada’s SEP collected and analyzed by Champagne and Sekkel (2018) allows me to compare the model’s forecasts to real-time expert-based forecasts for a number of macroeconomic variables.====The remainder of this paper is structured as follows. Section 2 briefly explains the features of the open-economy DSGE model and presents its linearized equations. Section 3 outlines the estimation technique used to incorporate the large set of international economic and financial series. Section 4 discusses the priors for the state-space and structural parameters as well as an overview of the data series used in the estimation. Section 5 presents the estimation results and the resulting dynamics of the DSGE-DFM model. Section 6 examines the forecasting performance of the DSGE-DFM model against its DSGE-Reg counterpart, other reduced form models, and the Bank of Canada’s Staff Economic Projections for various macroeconomic variables. In addition, the forecasts of the real and nominal exchange rates in the DSGE-DFM model are compared against random walk forecasts and other small and large data forecasting models. Section 7 examines the model implied correlations between Canada and the United States. It also considers the impact a U.S. recession that is calibrated using historical averages would have on Canada’s macroeconomy. Finally, section 8 concludes and discusses future extensions.",Evaluating the forecasting power of an open-economy DSGE model when estimated in a data-Rich environment,https://www.sciencedirect.com/science/article/pii/S0165188921001123,18 June 2021,2021,Research Article,198.0
"Albertini Julien,Fairise Xavier,Terriau Anthony","GATE, UNIVERSITY OF LYON 2., France,GAINS, UNIVERSITY OF LE MANS, France","Received 12 November 2020, Revised 8 June 2021, Accepted 9 June 2021, Available online 18 June 2021, Version of Record 27 June 2021.",https://doi.org/10.1016/j.jedc.2021.104170,Cited by (5),"What is the impact of bad health on labor market trajectories, consumption, and ","What is the impact of bad health on labor market trajectories, consumption, and wealth accumulation in a developing country characterized by a large informal sector and strong inequalities? Unemployment is high in several developing countries, and the shadow economy encompasses a large share of employment (Schneider and Enste, 2000). Informal jobs, which are not under the purview of regulation, often have poor working conditions, pay lower wages, and involve an important labor market turnover. Several studies suggest that informal work may be detrimental to health (See Ludermir and Lewis (2003), Giatti et al. (2008), and López-Ruiz et al. (2015) for South and Central America; Alfers and Rogan (2015) for South Africa). Informal workers are exposed to airborne particles and handle hazardous products or machinery without proper protection, which leads to a higher prevalence of respiratory diseases and work injuries (Dike, Loewenson, 1998, Romero, Oudin, Strömberg, Karlsson, Welinder, Sequeira, Blanco, Jiménez, Sánchez, Albin, 2010). Moreover, informal work may have damaging effects on mental health (Ludermir, Lewis, 2003, Ruiz, Vives, Martínez-Solanas, Julià, Benach, 2017).====In this study, we first show that in South Africa, informal workers have a higher probability of being unhealthy and of moving to non-employment than formal workers. In addition, non-employed individuals in bad health are less likely to find a job than their healthy counterparts. As bad health spells persist more for non-employed than for employed individuals, the interaction between labor market risks and health risks generates a ====. Individuals facing recurrent spells of bad health are more likely to be excluded from the labor market and become trapped in poverty. If individuals are not perfectly insured against these risks, what is the cost of bad health? Our study aims at quantifying the impact of health shocks and disentangling the channels through which they affect wealth and consumption over the life cycle using a heterogeneous agents model.====We focus on South Africa because it is one of the most unequal countries in the world. The poorest 50% of households account for only 8% of incomes and 4% of the net wealth, whereas the richest 10% of households own more than 55% of incomes and 71% of the net wealth (Hurlbut, 2018). In addition, almost half of the jobs are informal and approximately 60% of the working-age population is non-employed. As in most developing countries, the unemployment benefits system is almost nonexistent, thereby exacerbating the impact of income risks. The strong economic inequalities and the poor performance of the labor market provide an interesting environment to evaluate the link between health and labor market risks.====To quantify the impact of health risks, we first present some empirical facts concerning the labor market transitions, the survival probabilities, and the income and consumption profiles by health status using data from the National Income Dynamics Study (NIDS)==== We find that health status affects the transitions between employment and non-employment over the life cycle. Compared to formal workers, informal workers are more likely to experience bad health spells and to move to non-employment when unhealthy. The probability of finding a job declines sharply when an individual becomes unhealthy and state dependence is particularly important for informal workers and the non-employed. In addition, individuals in poor health have a lower labor efficiency, face a shorter life expectancy, and have a higher (smaller) propensity to consume (save). All these channels affect the lifetime wealth gap between healthy and unhealthy individuals.====To quantify the respective contributions of the different channels, we develop a life-cycle heterogeneous agent model in line with Huggett (1996), where individuals face uninsured income and health risks (De Nardi et al. (2017); French (2005)). The model takes advantage of continuous-time techniques developed by Achdou et al. (2017) so that it reduces to a system of partial differential equations that can be solved efficiently and quickly. Economic agents can be employed (either in the formal sector or the informal sector) or non-employed. As in Aiyagari (1994), we assume that wealth takes the form of productive capital. Consistent with empirical evidence provided by La Porta, Shleifer, 2008, La Porta, Shleifer, 2014, we assume that firms operating in the formal sector use labor and capital as inputs in the production process whereas firms in the informal sector are less productive and use only labor. Formal workers receive higher wages but pay taxes. Changes in labor market and health statuses as well as in the survival probability are represented by exogenous stochastic processes estimated using the NIDS data. We solve and estimate the structural parameters of the model using a method of simulated moments. The model reproduces remarkably well several moments of the life-cycle profile of wealth in South Africa. The model generates wealth inequalities well within the range of those found in the data. Although it slightly underestimates the health-wealth gradient, it succeeds in accounting for the observed heterogeneities in the wealth evolution across job statuses.====Our next goal is to quantify the cost of bad health. To do so, we run counterfactual experiments to measure the lifetime costs of bad health in South Africa. We simulate individual life cycle trajectories and perform counterfactual experiments to quantify the cost of health shocks. We show that the lifetime cost of health risks may represent up to 50% of the average annual net formal wage at age 60. The decrease in consumption evaluated in terms of the average annual net formal wage might reach 5% at age 60. Importantly, the channel through which health risk propagates more strongly depends on the initial job status. Using an impulse response function analysis, we show that for formal workers, the bulk of variations in wealth and consumption caused by a one-period-long health shock is equally explained by the decline in labor efficiency and the increase in consumption. For informal workers and the non-employed, the shock mainly translates into labor market probabilities. In the informal sector, job separation into non-employment rises while the probability of finding a job declines substantially for the non-employed. As a consequence, the surge in non-employment duration causes the shock to be highly persistent. For instance, for formal workers, it takes 12 years to compensate for half of the impact of the shock. This duration is approximately 15 years for informal workers and more than 30 years for the non-employed. Our results illustrate the poverty trap in which informal workers and non-employed fall when affected by health shocks.====The remainder of this paper is organized as follows: Section 2 presents our data, details the empirical strategy used to estimate the transition matrices and life-cycle profiles, and provides some empirical facts on South Africa. Section 3 develops the life-cycle heterogeneous agent model. Section 4 presents the estimation of the model and the numerical simulations. Section 5 summarizes our findings and concludes.","Health, wealth, and informality over the life cycle",https://www.sciencedirect.com/science/article/pii/S0165188921001056,18 June 2021,2021,Research Article,199.0
"Attaoui Sami,Cao Wenbin,Duan Xiaoman,Liu Hening","Finance Department, NEOMA Business School, France,Department of General Business and Finance, Sam Houston State University, USA,Alliance Manchester Business School, UK","Received 15 April 2021, Revised 6 June 2021, Accepted 16 June 2021, Available online 18 June 2021, Version of Record 5 July 2021.",https://doi.org/10.1016/j.jedc.2021.104176,Cited by (7),"We introduce ambiguity about the asset value dynamics of a firm into a trade-off framework of capital structure. We characterize investors’ preferences by recursive multiple priors utility. We show that equity holders’ ambiguity aversion significantly reduces the optimal leverage and debt capacity. In particular, equity holders with sufficiently strong ambiguity aversion perceive their asset value dynamics to be “too valuable to lose” upon ","The zero-leverage (ZL) phenomenon, which has been observed widely in the U.S. (Strebulaev and Yang, 2013) and internationally (El Ghoul et al., 2018), suggests that up to 20% of public firms completely forgo the net tax benefits of debt by choosing zero leverage. While various models have been developed to explain the low-leverage puzzle on the grounds of dynamic trade-off (e.g., Fischer, Heinkel, Zechner, 1989, Goldstein, Ju, Leland, 2001, Strebulaev, 2007) or agency frictions (DeAngelo, DeAngelo, Whited, 2011, Leland, 1998, Titman, Tsyplakov, 2007), our understanding of ZL as an equilibrium outcome of the above-mentioned trade-off models is still limited. For instance, Strebulaev and Yang (2013) highlight that “What we show is that to explain the low-leverage puzzle one needs to explain why some firms tend not to have debt at all instead of why firms on average have lower outstanding debt than expected, and most of extant models fail on this dimension.” The primary objective of this paper is to demonstrate that ZL can be an optimal policy in a static trade-off framework with ambiguity-averse investors.====In our model, equity and debt investors price contingent claims on a firm’s asset value dynamics that is perceived to be ambiguous. We characterize their preferences with the recursive multiple priors utility (RMPU) function proposed by Chen and Epstein (2002).==== In the multiple priors framework, investors’ beliefs are represented by a set of priors over the stochastic process of the asset value. In line with the existing studies on applications of multiple priors utility in economics and finance, the adoption of RMPU implies that investors value their holdings under their respective worst-case bankruptcy scenarios.====We maintain the fundamental assumptions of the trade-off framework of Leland (1994) in that the default boundary is endogenous and that immediate costly liquidation occurs upon default. We further assume that investors face ambiguity about the “true” dynamic process of the asset value and consider a set of candidate models with different mean growth rates, in light of ample empirical evidence that it is often challenging to estimate mean returns of financial assets (Merton, 1980). Consequently, ambiguity aversion, characterized by RMPU, dictates that equity holders make optimal capital structure choice based on the worst-case probability measure. Throughout the paper, we refer to “default ambiguity” as the scenario in which equity holders of the firm are uncertain about the asset value dynamics that they will lose in bankruptcy.====We first highlight that default risk increases equity holders’ default option value while default ambiguity does the opposite. Under limited liability, equity investors of a levered firm hold a perpetual American put on the assets with a strike equals to the after-tax coupon stream’s value. In the absence of ambiguity, uncertainty only originates from the Brownian component and is characterized by asset growth volatility. The value of the default option increases in default risk, following standard arguments that volatility increases option values. In the presence of ambiguity, uncertainty also stems from the lack of knowledge about the true dynamics of the asset value. Importantly, we show that the worst-case scenario within the set of priors arises when the default option attains the lowest value under the highest possible growth of the asset value dynamics. Intuitively, this worst case implies that equity holders worry about losing the most valuable asset to debt holders upon default. Thus, the default option value decreases in default ambiguity.====We then show that equity holders with sufficiently high ambiguity aversion will optimally choose not to borrow because they view their assets as “too valuable to lose.” The economic intuition is as follows. If the firm were to be levered, the worst-case scenario for equity holders in default would be to lose the asset value dynamics with the highest growth constrained by the set of priors. If equity holders perceive the asset growth to be high, they have the incentive to keep the default option alive by postponing default indefinitely. However, to postpone default to any horizon in this case, equity holders must continue to inject sufficiently high capital into the firm, which would then result in negative payout at all times hence negative equity value as perceived by equity holders. This unrealistic scenario contradicts limited liability, and as such, equity holders are unable to postpone default in any case when they hold a default option with strictly negative values. Consequently, equity holders fear that they would lose their “valuable” assets if the firm were to be levered and thus optimally choose not to borrow.====Next, we show that the optimal debt policy is discontinuous in ambiguity aversion. When equity holders are marginally less ambiguity averse, the firm issues debt and leverage increases sharply from zero. Moreover, debt investors have a distinct worst-case measure than the equity holders, even when both equity holders and debt holders have the same belief on the set of priors over the asset value dynamics. For debt holders, their worst case arises when the asset value dynamics has the lowest possible growth constrained by the set of priors, leading to the correspondingly high likelihood of default. This finding indicates that both default risk and default ambiguity command positive premia to debt investors. Our analysis suggests that ambiguity aversion can make equity investors and debt investors inherently heterogeneous in terms of pricing ambiguity in the asset value dynamics and therefore precludes the assumption of a representative investor in our model.====We further extend our analysis by examining the effect of ambiguity on optimal leverage conditional on whether or not a firm’s equity and debt markets are partially segmented in the spirit of Rubinstein (1973). When the markets are partially segmented, the set of equity investors and that of debt investors are disjoint. Hence, equity (debt) is priced under the worst-case measure for equity (debt) investors.====The opposite case is when a firm’s equity and debt markets are perfectly integrated. We find that all securities are priced with debt holders’ measure. To investigate this case, we adopt the general framework of Rubinstein (1974) and Jouini and Napp (2006), among others, on aggregating heterogeneous pricing measures in competitive markets. Since perfectly integrated equity and debt markets are competitive, the First Welfare Theorem holds. We recover the aggregated measure from the first-best outcome. In our setting, we follow Broadie et al. (2007) to define the social planner’s objective as the total firm value instead of equity value. In bankruptcy, the total firm value is penalized with the deadweight bankruptcy cost. Hence, the worst-case scenario occurs when default happens with the highest probability implied by the constrained set of priors, under debt holders’ pricing measure.====Under these two market settings, we find that ambiguity aversion lowers optimal leverage and debt capacity. The impact of ambiguity aversion is more pronounced when a firm’s equity and debt markets are partially segmented. Specifically, under market segmentation and given that equity and debt investors have the same belief on the set of priors over the asset value dynamics, equity investors’ worst-case scenario has the highest possible growth, and thus the optimal default is associated with a relatively high default boundary. In contrast, under market integration, the integrated worst-case scenario has the lowest possible growth constrained by the set of priors. In this case, equity investors view default more likely, and thus the optimal default is associated with a relatively low default boundary. Although debt investors’ worst-case scenario coincide under these two market settings, they view the default probability lower under market integration. This lowered probability follows from taking into account equity holders’ optimal default boundary under market integration. Consequently, market integration increases equity holders’ willingness to borrow and debt holders’ willingness to lend, resulting in higher optimal leverage and debt capacity relative to the case of market segmentation.====Our theoretical results are connected to several studies on capital structure. The result that ambiguity lowers debt capacity is consistent with the findings of Devos et al. (2012), Bessler et al. (2013) and Dang (2013) that debt capacity constraint, instead of corporate governance, is the main determinant of ZL. Additionally, we show that when the equity and debt markets are integrated, the distortion effects of ambiguity on leverage and debt capacity weaken. This result echoes the empirical studies on dual holders who invest in the same firm’s equity and debt. Jiang et al. (2010) find that syndicated loans with dual holder participation have loan yield spreads that are 18–32 bps lower than those in the case without dual holder participation. If dual ownership better align the interests of equity holders to those of debt holders, our theoretical analysis suggests that the enhanced uncertainty sharing through market integration fulfills the goal. That is, the enhanced uncertainty sharing can induce equity holders to deviate from operating under their worst-case scenario and thereby improve social welfare.====Our paper is closely related to several papers on optimal capital structure. Lee, 2016, Lee and Izhakian et al. (2021) also study optimal capital structure under ambiguity. However, none of them has ZL as an equilibrium outcome. Both Lee (2017) and Izhakian et al. (2021) utilize a one-period trade-off framework with alternative frameworks for modelling ambiguity. Because the utility preferences that they adopt are inherently smooth, the optimal leverage in their models is always positive.==== Like our model, Lee (2016) integrates RMPU into a static trade-off model of capital structure. The model of Lee (2016) can be viewed equivalently as modeling earnings before interest and taxes (EBIT) with a geometric Brownian motion. However, the model still cannot obtain ZL as an optimal outcome because the total payout to debt and equity holders is implicitly restricted to be positive in the model. In our model, the total payout rate is the difference between the risk-free rate and the asset growth under equity holders’ worst-case measure, which can be negative when the worst-case asset growth exceeds the risk-free rate. We show that ZL will arise as an equilibrium outcome in such a case. Moreover, Lee (2016) does not investigate the role of the market setting (market segmentation versus integration) as we do in this paper.====Yang (2013) considers a model in which insiders and potential outside investors (in both equity and debt) have heterogeneous beliefs and the firm will refinance. Yang (2013) shows that the model can generate ZL. Instead, heterogeneous beliefs of equity holders and debt holders emerge endogenously in our model from the assumption that both equity holders and debt holders have the same set of priors. Additionally, in contrast to Yang (2013), our model does not require the firm to refinance in order to generate low and realistic optimal leverage ratios. Lotfaliei (2018) explains the ZL puzzle based on the value in waiting to have debt and shows that ZL is transient, which is, however, inconsistent with the evidence of Strebulaev and Yang (2013) and Graham (2000) that both ZL and debt conservatism are persistent phenomena.====The rest of our paper is organized as follows. Section 2 presents the model of capital structure incorporating ambiguity. Section 3 shows that ZL can be an optimal outcome in equilibrium under certain conditions. Section 4 provides theoretical analysis on the impacts of ambiguity on optimal leverage and debt capacity in different market settings. Section 5 discusses numerical results. Section 6 concludes. Additional discussion and proofs of main results are included in the Appendix.","Optimal capital structure, ambiguity aversion, and leverage puzzles",https://www.sciencedirect.com/science/article/pii/S0165188921001111,18 June 2021,2021,Research Article,200.0
"Zormpas Dimitrios,Ruble Richard","CY Cergy Paris Université, CY Advanced Studies and ESSEC Business School, F-95000 Cergy-Pontoise, France,Emlyon business school, GATE UMR 5824, Ecully F-69130, France","Received 21 May 2020, Revised 18 May 2021, Accepted 12 June 2021, Available online 17 June 2021, Version of Record 8 July 2021.",https://doi.org/10.1016/j.jedc.2021.104175,Cited by (1),"We study how overlapping ownership affects investments in a preemption race with market uncertainty. Internalization of rival payoffs delays follower entry if product market effects are moderate, implying longer incumbency which intensifies the race to lead. Preemptive and follower investment thresholds increase with volatility as in standard ==== models but firm value can decrease, and greater volatility makes internalization more profitable. From a welfare perspective there is a tradeoff between a dynamic benefit and a static cost of overlapping ownership. Whereas it is socially optimal not to have any overlapping ownership in some markets, at low volatility levels we find firms have an insufficient incentive to internalize.","Ownership structures in many industries overlap significantly nowadays, leading researchers to question whether firms maximize only their own value or also internalize rival values (Backus et al., 2019), and prompting concern among regulators and academics about anticompetitive effects (Posner et al., 2017; Frazzani et al., 2020).==== There is a clear theoretical link between overlapping ownership and weakened product market competition (Reynolds and Snapp, 1986), which has been the object of significant empirical study (Azar et al., 2018). The consequences of overlapping ownership for non-price competition are more involved, and empirical studies find the effect of overlapping ownership on investment measures to be either positive (He and Huang, 2017), negative (Gutiérrez and Philippon, 2016; Newham et al., 2019) or insignificant (Koch et al., 2020). R&D investment has been shown to increase with overlapping ownership if spillovers are large, providing a counterweight to anticompetitive product market effects (López and Vives, 2019). But factors other than R&D spillovers are likely to be germane to investments in many industries, such as consumer goods where the timing of product introductions accelerates with overlapping ownership (Aslan, 2019).====Among the decisions which overlapping ownership is likely to weigh upon is the exercise of a firm’s real options in the face of competition and uncertainty, which represents a key strategic choice for top management (Smit and Trigeorgis, 2017). In this article we propose therefore to account for the contrasting patterns of over- and under-investment in the literature by studying a preemption race between firms with overlapping ownership. Irreversibility and uncertainty play a key role in this framework (Dixit and Pindyck, 1994), which results in heterogeneous investment outcomes. As our model is prompted by common ownership (where third parties own shares in several firms in an industry) but applies equally if firms have symmetric cross holdings, we follow other authors by using the umbrella term overlapping ownership to refer to ownership structure.====We model two symmetric firms holding competing projects that await opportune market conditions. Investment is discrete and allows firms to access a profit flow, either as leader by investing first or as follower by investing second. Overlapping ownership drives firms to factor rival value into the timing of their investments. We also allow the reach of overlapping ownership to extend to output or pricing decisions, in which case overlapping ownership is said to have product market effects.====Once the leader is operating as an incumbent, the follower determines when to enter based on its perceived profit flow, which consists of expected duopoly profit net of the incumbent rent which the follower internalizes. High internalization levels can result in a negative perceived profit flow which deters entry (Proposition 1). We find increasing internalization has an ambiguous effect on the follower’s investment incentive. If the product market effects of overlapping ownership are not too strong, greater internalization lowers the follower’s perceived profit flow and delays its entry, in line with empirical evidence of generic entry in the pharmaceutical industry for example Newham et al. (2019). We show product market effects are not too strong if firms compete in quantities (Proposition 2), and provide an example involving price competition to illustrate the theoretical possibility that strong product market effects lead to earlier follower investment. Greater volatility unequivocally delays the follower’s investment (Proposition 3), but has an ambiguous effect on firm values in contrast with standard real option models because the follower’s investment threshold is inefficient with respect to firm value maximization (Proposition 4).====Firm roles are endogenously determined by preemption in our model. The preemptive threshold of the leader and the follower threshold are inversely related. If product market effects are not too strong therefore, the delayed follower entry induced by overlapping ownership also lengthens the monopoly phase and raises the incentive for each firm to enter ahead of its rival (Proposition 5). Relaxing ==== static competition thus intensifies dynamic competition ====, implying overinvestment by the industry leader. As dynamic competition equalizes rents, internalization paradoxically reduces firm values in the absence of product market effects. Turning to the effect of market uncertainty we find that greater volatility causes firms to delay preemptive investment (Proposition 6) and attribute more weight to product market effects, which leads them to prefer more internalization (Proposition 7).====Finally, we study welfare assuming that regulators can target a level of internalization by restricting overlapping ownership while firms are free to time their investments. If product market effects are not too strong, there is a welfare tradeoff between the dynamic benefit of overlapping ownership due to increased preemption and the static cost associated with delayed follower entry and relaxed product market competition. We show total welfare is quasiconcave with quantity competition if there are no product market effects (Proposition 8) and study its behavior with product market effects numerically. We find that positive product market effects can increase welfare by accelerating follower investment. Turning to the effect of uncertainty, at lower volatility levels the dynamic benefit of increasing overlapping ownership outweighs static cost leading regulators to prefer more internalization than firms (Proposition 9), though in markets with moderate uncertainty it is socially optimal not to have any overlapping ownership at all.====Our model relates to a theoretical literature dating back to Reynolds and Snapp (1986) which studies how overlapping ownership affects product market competition and to recent articles which have incorporated different forms of non-price competition. López and Vives (2019) find that overlapping ownership increases R&D and welfare when firms compete in both innovation and product markets if knowledge spillovers are large. Brito et al. (2020) find increasing quality investment in a vertical product differentiation model with overlapping ownership. Li et al. (2015) study entry and find that cross-shareholdings can induce deterrence in a sequential move game, whereas Sato and Matsumura (2020) show that overlapping ownership mitigates excess entry in a circular market model. Both of these findings are reminiscent of the follower underinvestment arising in our model if product market effects are weak. In an overlapping generations model, Shy and Stenbacka (2019) show that common ownership lowers real investment, distorting intertemporal consumption choices. These studies all assume as we do that firms compete, and an alternative stream exemplified by Gilo et al. (2006) studies coordinated effects of overlapping ownership. Finally, by incorporating internalization of rival value into a preemption race our work contributes to the literature on preemption under uncertainty (see Azevedo and Paxson, 2014 for a survey).====The rest of the paper is organized as follows. Section 2 presents our model. Section 3 describes how overlapping ownership affects leader and follower investment. Section 4 relates welfare to overlapping ownership and uncertainty. Section 5 concludes.",The dynamics of preemptive and follower investments with overlapping ownership,https://www.sciencedirect.com/science/article/pii/S016518892100110X,17 June 2021,2021,Research Article,201.0
"Moran José,Fosset Antoine,Kirman Alan,Benzaquen Michael","Chair of Econophysics and Complex Systems, Ecole polytechnique, 91128 Palaiseau Cedex, France,Centre d’Analyse et de Mathématique Sociales, EHESS, 54 Boulevard Raspail, 75006 Paris, France,LadHyX UMR CNRS 7646, Ecole polytechnique, 91128 Palaiseau Cedex, France,Mathematical Institute and Institute for New Economic Thinking at the Oxford Martin School, University of Oxford, Oxford, United Kingdom,Complexity Science Hub Vienna, Josefstädter Straße 39, A-1080, Austria,Capital Fund Management, 23 Rue de l’Université, 75007 Paris, France","Received 30 September 2020, Revised 25 May 2021, Accepted 6 June 2021, Available online 12 June 2021, Version of Record 30 June 2021.",https://doi.org/10.1016/j.jedc.2021.104169,Cited by (0),"We analyse the dynamics of fishing vessels with different home ports in an area where these vessels, in choosing where to fish, are influenced by their own experience in the past and by their current observation of the locations of other vessels in the fleet. Empirical data from the boats near Ancona and Pescara shows stylized statistical properties that are reminiscent of Kirman and Föllmer’s ant recruitment model, although with two ant colonies represented by the two ports. From the point of view of a fisherman, the two fishing areas are not equally attractive, and he tends to prefer the one closer to where he is based. This piece of evidence led us to extend the original ants model to a situation with two asymmetric zones and finite resources. We show that, in the mean-field regime, our model exhibits the same properties as the empirical data. We obtain a phase diagram that separates high and low herding regimes, but also fish population extinction. Our analysis has interesting policy implications for the ecology of fishing areas. It also suggests that herding behaviour here, just as in financial markets, will lead to significant fluctuations in the amount of fish landed, as the boat concentration on one area at a given point in time will diminish the overall catch, such loss not being compensated by the reproduction of fish in the other area. In other terms, individually rational behaviour will not lead to collectively optimal results.","A problem of general interest is that of the individual and collective exploitation of a resource. Depending on the particular context, the dynamics can be very different. A crucial factor is the effect of the behaviour of individuals on the collective outcome. In financial markets for example, the decision to buy may enhance the value of the resource for others as the price of an asset may increase as the demand for it grows. This positive feedback can lead to “herd behaviour” and to the emergence of “bubbles”. If, on the other hand, the resource is in fixed supply or can only generate a limited flow, as in the case of agricultural production, overexploitation can lead to its exhaustion when individuals do not take account of the overall consequences of their actions. This leads to the famous “Tragedy of the Commons”, see Hardin (1968).====In this paper we use a version of a model which was developed in the context of financial markets but we modify it to look at a problem of exhaustible resources, in particular that of fisheries. There is a substantial literature on fishing management which analyses the causes of overexploitation and the behaviour that leads to this. Much of that literature was based on understanding the strategies that individual boats use to decide when and where to fish. The simplest idea is that the individuals base their decisions on Catch per Unit Effort (CPUE), see (Gavaris, 1980). This suggests that boats fish until their catch falls below a certain threshold and then move on. This is a purely individualistic model and argues that past individual experience is an adequate basis for decision making. Two questions arise here. Firstly, can one deduce the aggregate behaviour from the observed behaviour of individual vessels? And secondly, does the behaviour of other vessels influence the choices of a particular boat? The answer to the first question lies in the development of satellite technology which allows individual vessels to be identified and followed; this information provides a basis for analysing the individual and collective behaviour of fishing fleets. It is, of course, known that vessels do not act in total isolation and a model using tracking data for New Zealand fisheries was, for example, studied in Vignaux (1996). This came to the conclusion that “there is evidence that vessels make decisions about where to fish based on both their own recent catch history and on observation about the location and aggregation of other vessels. There is no evidence that there is enough information transfer for vessels to make decisions on the basis of catch rates of the other vessels in the fleet.” What was suggested was that while the influence of other players is taken into account, because of the limited information about the performance of other vessels it may not be the major driving influence for collective behaviour.====However, a more radical approach, abandoning simple optimization, had been developed earlier by Allen and McGlade (1986). They developed models in part based on the Lotka-Volterra equations which already incorporated recent advances in the understanding of the evolution of complex systems. They studied herd behaviour and simulations of a dynamic model of a Nova Scotia fishery. Their analysis revealed that human responses amplify rapid random fluctuations in recruitment and excite strong Lotka-Volterra type oscillations in a system that would normally settle to a stable stationary state. Their dynamic, multi-species, multi-fleet spatial model was calibrated to the Nova-Scotian groundfish fisheries. They examined the role of “exploration” and “exploitation”. They identified two types of hunters, “stochasts” or high-risk takers, and “Cartesian” followers, or low risk takers. The result of the interaction between the two reveals, as they say, “the ‘out of phase’ relationship between abundance and the ease with which fishermen locate a highly sought species and its converse”. They emphasize, contrary to more conventional analysis, “the importance of information exchange in defining the attractivity of a particular fishing zone to different fleets and the ability of the model to take into account coded information, misinformation, spying and lying; and the fact that models based on global principles, such as ‘optimal efficiency’ or ‘maximum profit’, are clearly of dubious relevance to the real world.” The crucial difference between this and the work previously cited is that much more weight is given to information about the activity of others and the content of the messages about that activity is assumed to be much richer.====Our approach is in this spirit and is based on a model in which agents are “recruited” to a source of profit by those already benefiting from that source. The actors follow simple rules but their interaction can produce interesting dynamics. A related approach by computer scientists in Dascalu et al. (2013) suggested that the result might be that of a uniform distribution across the space in which the resource is found. We show that, depending on the weight given to the behaviour of others, vessels can typically operate near to their home port with occasional excursions to another area, but that changing the parameters of the model can lead to a persistent mixing of the two fleets with some boats from each area fishing in the other area. Since what is important is the probability that a boat follows others, the distribution of the boats over the two areas is determined by a stochastic process. This recalls a result of Allen and McGlade in which the survival of the fishery was dependent on the existence of some vessels which chose the place to fish at random and where it was shown, as in many models of interaction, that a degree of randomness may make an important contribution to the overall dynamics of the system.====Our model is mathematically equivalent to many models appearing in different fields. Our original inspiration is Kirman and Föllmer’s ant model, described in detail in  Kirman (1993), where ants in a colony have to decide between two identical food sources. They may decide to switch from one to the other randomly, or do so by imitating their peers. Depending on the parameters, the model can exhibit two distinct types of behaviour: either the ants are always evenly split between the two sources, or they all concentrate on one before randomly switching to the other. The first instance of this model was originally proposed in Moran (1958) as a way to model the population dynamics between two competing, but similar, alleles of the same gene, A and B; the equivalent of the imitation between agents is in this case the self-reinforcing reproduction of each allele, while the spontaneous switching corresponds to a random mutation from A to B and vice-versa. Again, one can obtain a situation where neither of the alleles is dominant and both coexist, or another situation where one of the alleles is dominant.====The extension of the ant model that is most similar to ours is the one described in Alfarano et al. (2005), where the authors study a model that is equivalent to having two ==== food sources, meaning that one is more attractive than the other, as an analogy to model herding and volatility bursts in financial markets. The same idea of using herding to explain fat tails in the distribution of financial returns was also exploited by Sano (2015), although without the introduction of asymmetry. As we shall see, similar behaviour in our context can lead to increased volatility in both the fish population and the amount of fish that is caught.====This type of modelling has also been used in the social sciences within the so-called noisy voter model studied in Carro et al. (2016) and is closely linked to other voter models (see e.g.  Redner (2019) for a recent review). In this setting the ants become voters that have to choose between two competing political choices, and make their pick because of some idiosyncratic bias (akin to the spontaneous switching) or because they are influenced by their peers.====There is a key difference, however, between the ant model and some instances of the voter model. At each iteration, an agent must make one of the two available choices; it may be that the mapping between the two models is exact, and that a voter can pick one of their neighbours and imitate their choice, leading to exactly the same dynamics as for the ant model. Another version of the model, however, can instead imagine that the voter has again two different choices, but polls his peers and decides to adopt the choice of the majority. This second version leads to different dynamics, where multiple equilibria appear and correspond to states where the agents self-reinforce their choice, and may collectively change their mind after an exogenous shock, however small, as it happens with the random field Ising or random utilities model described in Galam et al. (1982); Gordon et al. (2005); Michard and Bouchaud (2005); Nadal et al. (2003) and Bouchaud (2013). These models often lead to a very spectacular phenomenology of “avalanches” and exhibit true phase transitions, along with hysteresis, see e.g. Kuntz et al. (1999).====In the finance and economics community, these models have been used as a way to understand the origins of large endogenous fluctuations in financial markets(see  Lux (1995)), whose origin may be found in the word-of-mouth way that information diffuses among market participants, as described by Shiller and Pound (1989). In fact, the model can be embedded into an equilibrium asset pricing model, and the dynamics one obtains are strikingly similar to models of financial markets with two types of agents – fundamentalists who attempt to find the true, “fundamental” price of an asset and followers who imitate them. Good examples of these models can be found in e.g. Chiarella (1992) and Alfarano et al. (2008), and recent empirical confirmation of the main qualitative findings of these models is given in Majewski et al. (2020) and Bouchaud et al. (2017).====This interplay between imitators and fundamentalists in financial markets is strongly reminiscent of the “Cartesian” and “stochast” type of hunters described by Allen and McGlade (1986). An interesting feature of the case that we examine is that we can actually observe the activity of the individual boats and to what extent they follow each other. Our model provides an explanation for this by observing the individual activity of all boats, while in financial markets it is not, in general, possible to monitor the activity of individual investors and one can only infer the motivation of individuals from the aggregate data.====The paper is organised as follows. In Section 2, we present an overview of the data used in this article and introduce all relevant definitions. Section 3 introduces a model intended to reproduce the main stylized facts present in the data. We show the links between our model and existing literature in choice theory and more broad mathematical modelling, such as models related to genetics and population dynamics, thereby highlighting the generic character of our work. While similar models are mostly intended as qualitative or metaphorical models, we go further by relating our model to empirical data and accurately describing the statistical properties of the dynamics we observe. We also show explicitly how to write the corresponding Fokker-Planck or Kolmogorov forward equation for this model, and later use stochastic calculus techniques to predict features of the model in the form of higher-order correlators that are later validated by the data. The model is developed in Section 4 using simplifying assumptions that are justified by numerical simulations, notably a mean-field approximation. These allow us to solve for the stationary distribution of the Fokker-Planck equation. These developments are the main methodological contributions of this paper, as they introduce various mathematical techniques in an explicit and pedagogical fashion. Finally, in Sections 5 and 6, we discuss further consequences of the model and in particular the different collective “phases” that describe the aggregate behaviour of fishing vessels. We find that herding, which may be rational from the individual point of view, has a detrimental effect on the overall yield of the areas recalling the “Tragedy of the Commons”.",From ants to fishing vessels: a simple model for herding and exploitation of finite resources,https://www.sciencedirect.com/science/article/pii/S0165188921001044,12 June 2021,2021,Research Article,202.0
Coulombe Raphaelle G.,"Department of Economics, Middlebury College, United States","Received 12 November 2020, Revised 24 May 2021, Accepted 25 May 2021, Available online 28 May 2021, Version of Record 30 June 2021.",https://doi.org/10.1016/j.jedc.2021.104167,Cited by (0),) more political polarization and (====) a high persistence in government ideology.,"The literature on fiscal policy has been rapidly expanding in the last decade. The years following the financial crisis were particularly relevant for this research since the short-term nominal interest rate in many countries has reached the zero lower bound, making fiscal policy the macroeconomic stabilizer of last resort. While many countries initially adopted stimulus packages in response to the financial crisis, many European countries later turned to austerity measures in the face of rising budget deficits and debt levels.====While an important part of the flourishing literature on fiscal policy has focused on the effect of such measures on output as summarized in Ramey (2019) and Ramey (2011), the focus of this paper is to provide a deeper understanding of government decisions towards spending, which amounted to 38% of GDP on average in advanced economies in 2018 (IMF estimates, 2019) by modeling and documenting the origins of the variations in fiscal policy stance both across countries and time. To that end, I relax a common assumption in traditional Keynesian and neoclassical macroeconomic models that government spending is an exogenous process.====I first develop a theoretical model of the joint determination of economic and electoral outcomes in which government decisions towards spending and taxation vary according to the ideology of the ruling party and the state of the economy. In the model, government ideology is summarized along a left-right spectrum, which is meant to be country-specific so that a left-wing party in one country, for example in the United States, is not necessarily viewed as a left-wing party in another country. The theoretical model introduces heterogenous parties and voters into a one-sector neoclassical model extended with repeated elections at fixed intervals. The key assumption of the model, which is motivated by the empirical evidence, is that the ruling party follows a fiscal rule for expenditure that operates like a ratchet mechanism by gradually turning the government spending-to-output ratio in its preferred direction. The government’s choice over how much public goods to provide in turn affects output via its effect on hours worked. Voters have stochastic, heterogenous preferences over private and public consumption and decide how much to consume and work. They vote according to their own ideological preferences and the perceived competence of the incumbent party as in Persson and Tabellini (1990).====In the second part of the paper, I exploit the large variations in the domestic political environment across countries and time as well as variations in the business cycle and in the stance of fiscal policy since 1945. I show how to identify the country-specific economic and political parameters of the model for twelve OECD countries in North America and western Europe.==== To capture key differences in ideology and voting behavior across countries, I use a rich dataset from the Manifesto Project database that documents electoral outcomes and time-varying ideological party positions estimated from electoral platforms since 1945.====The model is able to make accurate average predictions regarding the cyclical and time series properties of government spending. The predicted volatility of government spending is larger than that of output and is of similar magnitude to what is found in the data. As well, the predicted persistence in government spending corresponds closely with that found in the data and is within the 95% confidence interval of the estimated persistence for all countries. In addition, I find that an important part of the variance in government spending (59% on average) can be explained by the political sector for most countries studied. Even in the absence of the total factor productivity shock, the model produces larges deviations of government spending from its long-run trend originating from the political sector. Overall, the political sector appears to be a key driver of the fluctuations in government spending in countries like the United States and the United Kingdom with (====) more political polarization and (====) a high persistence in government ideology.====   Political economists have long recognized that the ideology of governments is an important determinant of economic policies. The argument in this literature, initially pioneered by Hibbs (1977) and later extended by Alesina (1987) and Alesina (1988), is that partisan-politicians pursue different policy outcomes according to their ideological preferences. Empirically, there is a voluminous body of literature showing evidence in favor of partisan models.==== One contribution of this paper is to incorporate partisan differences into a neoclassical macroeconomic environment where political and economic outcomes are jointly determined. A key assumption of my framework, which is motivated by the empirical evidence found in this paper and differs from standard partisan models, is that governments follow a fiscal rule for expenditure that operates like a ratchet mechanism by gradually turning the government spending-to-output ratio in their preferred direction. This assumption is consistent with the findings of Brender and Drazen (2013) who show that it takes several years for leaders in advanced economies to change the composition of government expenditure programs.==== The ratchet is identified from the data and varies by country. To classify whether governments are right-wing or left-wing, I construct a measure of government ideology that accounts for the different types of political regimes. For countries with a parliamentary regime, I consider the ideology of all the parties in the ruling coalition when there is no majority party. For a country with a presidential regime like the United States, I consider the ideology of the president as well as the ideology of the majority party in both houses.====The paper also contributes to the literature on endogenous fiscal policy. For example, Ambler and Paquet (1996), Klein et al. (2008), and Bachmann and Bai (2013b) augment a standard neoclassical model and endogenize government purchase by assuming that the government maximizes the utility of a representative agent. In Bachmann and Bai (2013a), the government chooses government expenditure to maximize a weighted social welfare where the weights are function of households’ wealth. The politico-economic inequality in their model is driven by a political wealth bias and is able to generate comovement of government spending and output that is consistent with the data for the United States. A number of studies such as Bachmann and Bai (2013b) and Battaglini and Coate (2008) have analyzed the effect of a shock to the preference for the public good. Bachmann and Bai (2013b) find, however, that the preference shock alone does not lead to realistic moments of government expenditure. Interestingly, Bachmann and Bai (2013b) find that 41% of the variance in government spending in the U.S. is linked to the preference shock, which is smaller to what I find in this paper for the U.S. (76%). A key difference is that in my framework, election results are endogenous and are determined by the choice of the median voter in the spirit of Krusell and Ríos-Rull (1999). This implies that a shock to the preference of the public good leads to a change in the ideology of the government, which in turn affects government spending and taxation. A robustness exercise in Section 9 illustrates that voting appears to be key for the political mechanism, and in particular, for the importance of the preference shock in the model. When I shut down the voting mechanism and assume instead that there is exogenous power fluctuations as in Azzimonti (2011) and Azzimonti and Talbert (2014), I find that the political environment explains very little variations in government spending. The polarized business cycle model of Azzimonti and Talbert (2014), which incorporates heterogenous parties and voters as well as political polarization into a neoclassical framework is closely related to the one used in this paper. While their focus is on the effect of greater polarization on the fluctuations in output, investment, and private consumption, I focus on analyzing the importance of the political environment for the variations in government spending across countries.====I contribute to the literature on endogenous fiscal policy by studying variations of government spending for twelve OECD countries in a neoclassical model that is consistent with two key results of the political economy literature: (====) the ideology of governments affects policy outcomes, and (====) economic conditions matter for election results. In a literature review on the economic determinants of voting decisions, Lewis-Beck and Stegmaier (2000) show that the performance of the economy in election years is often the most important factor for election results in many advanced economies. This realistic feature of the model allows for credible feedback from the economy to policy via elections and voting decisions. The identification of economic and political parameters for a wide range of advanced economies allows for country-specific predictions. I find that elections are more likely to be a precursor to government spending shocks in countries that have more polarization and higher persistence in government ideology such as the United States and the United Kingdom.",The electoral origin of government spending shocks,https://www.sciencedirect.com/science/article/pii/S0165188921001020,28 May 2021,2021,Research Article,203.0
"Serletis Apostolos,Xu Libo","Department of Economics, University of Calgary, Calgary, Alberta T2N 1N4, Canada,Department of Economics, University of San Francisco, San Francisco, California, CA 94117, U.S.A.","Received 29 October 2020, Revised 17 January 2021, Accepted 1 May 2021, Available online 21 May 2021, Version of Record 5 June 2021.",https://doi.org/10.1016/j.jedc.2021.104144,Cited by (4),This paper uses neoclassical demand theory and applied consumption analysis to calculate the welfare cost of ,"This paper provides a major advance to the literature on measuring the welfare cost of inflation that results from the use of more sophisticated demand for money function specifications and broad Divisia monetary aggregates. We calculate the welfare cost of inflation, in the context of the Bailey (1956) approach, using the flexible Normalized Quadratic expenditure function, paying explicit attention to the demand interactions among consumption goods, leisure, and money. We show that raising the inflation rate from 2% to 4% in the United States, as it has been suggested in the aftermath of the global financial crisis, would impose a welfare cost equivalent to a loss of 0.30 percent of output when money is measured by the Center for Financial Stability (CFS) Divisia M4 monetary aggregate. This is a sizable cost, which ought to be taken seriously by those who argue that raising the inflation target is a good way of avoiding the stabilization challenges posed by the zero-lower-bound constraint on the Fed’s policy rate.====. One method for calculating the welfare cost of inflation is the Bailey (1956) approach, associated with consumer surplus analysis in the literature of public finance and applied microeconomics. It has been pursued by Cysne (2003); Ireland (2009); Lucas et al. (2000); Mogliani and Urga (2018), and Dai and Serletis (2019). In this approach, the welfare cost of inflation is defined as the change in the area under the inverse money demand curve corresponding to the change in the holdings of real money balances — the consumer surplus that can be gained by reducing the nominal interest rate, ====, from a positive level to zero. In this approach, the first step in the calculation of the welfare cost of inflation is the estimation of a money demand function, ====, with ==== being the ratio of nominal money balances to nominal income. In general, two money demand specifications have dominated the literature since the 1960s — the semi-log, adapted from Cagan (1956), and the log-log, inspired by Meltzer (1963). In particular, Lucas et al. (2000), using annual U.S. data, over the period from 1900 to 1994, uses the log-log specification, defines the money supply as simple-sum M1, assumes an interest elasticity of ==== (as in the Baumol-Tobin model), and reports a welfare cost of inflation of about ==== of real income per year if the annual inflation rate is ====. However, Ireland (2009) argues that the semi-log specification provides a better fit to quarterly U.S. data over the post-1980 period. Using the simple-sum M1 monetary aggregate and quarterly data from 1980:q1 to 2006:q4, he estimates the welfare cost of inflation to be around ==== of real income per year (if the annual inflation rate is ====), which is significantly lower than Lucas et al. (2000) estimate.====More recently, motivated by a vast literature devoted to money demand instability issues — see, for example, Goldfeld (1976); Judd and Scadding (1992), and more recently Lucas and Nicolini (2015) and Benati et al. (2020) — Mogliani and Urga (2018), using annual data, over the period from 1900 to 2013, find structural breaks and estimate a substantially lower welfare cost of inflation after 1976. Also, Dai and Serletis (2019) use the Markov switching approach, associated with Hamilton (1989), quarterly data from 1967:q1 to 2013:q4, make a distinction between the simple-sum M1 monetary aggregate and the Divisia M1 monetary aggregate, following Lucas et al. (2000) suggestion “to replace M1 with an aggregate in which different monetary assets are given different weights,” and find that the welfare cost of inflation declined significantly (by close to 50%) after the 1980s. However, the issue regarding the welfare cost of inflation is not closed, as there are studies reporting welfare cost estimates that are much larger than those just mentioned. For example, Bullard and Russell (2004), in the context of a quantitative-theoretic general equilibrium model of the U.S. economy, report that “a permanent, 10-percentage-point increase in the inflation rate — a standard experiment in this literature — imposes an annual welfare loss equivalent to 11.2 percent of output.” More recently, Ascari et al. (2018) use an augmented dynamic stochastic general equilibrium model and report that increasing trend inflation from 2% to 4% generates a consumption-equivalent welfare loss of about 4%. Also, Kurlat (2019), in the context of a model in which bank deposits pay interest, reports that a one percentage point increase in inflation has a welfare cost of 0.086% of GDP, which is 6.9 times higher than traditional estimates.====. We develop a new approach to measuring the welfare cost of inflation, within the Bailey (1956) consumer surplus framework. Instead of assuming money demand specifications such as the log-log and semi-log forms, we take a microeconomic- and aggregation-theoretic approach to the demand for money and monetary assets. Our approach allows estimation in a systems context, assuming a flexible functional form for the representative agent’s utility function, based on the dual approach to demand system generation developed by Diewert (1974) — see, for example, Barnett (1997); Barnett et al. (1992); Barnett and Serletis (2008); Jadidzadeh and Serletis (2019), and Serletis and Xu (2020), among others. In doing so, we also consider the demand interactions between goods, leisure, and money, consistent with earlier work by Abbott and Ashenfelter (1976); Barnett (1979), and Serletis and Xu (2021), suggesting that such interactions are likely to be substantial and of significant quantitative importance. We investigate the demand for consumption goods, leisure, and money in the United States in the context of the locally flexible Normalized Quadratic (NQ) expenditure function, developed by Diewert and Wales (1988), paying explicit attention to theoretical regularity, as suggested by Barnett (2002).====Most of the papers on the welfare cost of inflation have used the standard (simple sum) M1 monetary aggregate, as Lucas et al. (2000) did. The exception is Dai and Serletis (2019) who also use the Divisia M1 aggregate, following Lucas et al. (2000) suggestion to replace simple sum M1 with an aggregate in which different assets are given different weights, and the ‘NewM1’ aggregate of Lucas and Nicolini (2015), which augments the standard M1 aggregate with money market deposits accounts (MMDAs). But Lucas et al. (2000, p. 270–271), also said the following: “I share the widely held opinion that M1 is too narrow an aggregate for this period [the 1990s], and I think that the Divisia approach offers much the best prospects for resolving the difficulty.” In other words, what Lucas et al. (2000) was suggesting was not just to switch from simple sum M1 to Divisia M1, but to switch to broader Divisia monetary aggregates, for which the change from simple sum to Divisia is very important. The need for the use of properly weighted broader aggregates in measuring the welfare cost of inflation has also been recognized by Kurlat (2019) and Cysne (2003); the latter computed the welfare cost of inflation with a broad Divisia monetary aggregate.====In this paper we use the broader Divisia monetary aggregates, as suggested by Lucas et al. (2000), to provide better measures of the welfare cost of inflation. As is well known in the literature on modeling demand for consumer goods, aggregation internalizes substitution effects, thereby decreasing the number of explanatory variables in the model, needed to account for substitutes and complements. Modeling demand for any goods at the lowest levels of aggregation is always more difficult than at higher levels of aggregation. As a result, demand models are likely to be most unstable at the M1 level of aggregation and our conclusions regarding the welfare cost of inflation will undoubtedly change with broader Divisia monetary aggregates. We use quarterly data for the United States, over the period from 1967:q1 to 2019:q4; this sample period is dictated by the availability of the Divisia monetary aggregates. In particular, we use the new Divisia monetary aggregates (and their corresponding user costs), maintained within the CFS program Advances in Monetary and Financial Measurement (AMFM), called CFS Divisia aggregates and documented in detail in Barnett et al. (2013). The broad CFS Divisia monetary aggregates have also been favored in the empirical analyses by Jadidzadeh and Serletis (2019); Serletis and Xu (2020), and Dery and Serletis (2021).====. The rest of the paper is organized as follows. Section 2 sketches related neoclassical demand theory and applied consumption analysis. Section 3 presents the NQ expenditure function, derives the associated system of budget share equations, and discusses the Diewert and Wales (1988) method of imposing global concavity with the objective of achieving theoretical regularity. Section 4 discusses the data and related econometric issues. Section 5 discusses the method for calculating the welfare cost of inflation in the context of the Bailey (1956) approach, presents the empirical results, and investigates the robustness of the results to the use of broader Divisia monetary aggregates. The final section contains concluding remarks.",The welfare cost of inflation,https://www.sciencedirect.com/science/article/pii/S0165188921000798,21 May 2021,2021,Research Article,204.0
"Nunes Cláudia,Oliveira Carlos,Pimentel Rita","CEMAT and IST- Instituto Superior Técnico, Universidade de Lisboa, Portugal,ISEG-School of Economics and Management, Universidade de Lisboa and REM - Research in Economics and Mathematics, CEMAPRE, Portugal,RISE, Research Institutes of Sweden, Sweden,Department of Industrial Economics and Technology Management, NTNU, Norway","Received 15 July 2019, Revised 15 January 2021, Accepted 11 May 2021, Available online 20 May 2021, Version of Record 22 July 2021.",https://doi.org/10.1016/j.jedc.2021.104154,Cited by (3),"In this paper we address, in the context of ","The optimal time to undertake an investment opportunity has been an important research question for both economists and mathematicians, mainly since the pioneering works of Dixit and Pindyck (1994) and Trigeorgis (1996). Over time, the models to solve these problems have become more complex, since researchers and practitioners intend to represent the economic reality in a more realistic way. As a consequence, both the number of sequential decisions and the number of sources of uncertainty in these models have increased.====One particular aspect that has been under the spots of real options literature is the impact of technology innovations, that may lead to significant changes in the revenue and costs of the companies. In the past few years, given a large number of breakthroughs innovations, some industries have seen their investment costs decrease over time. Therefore, nowadays companies understand better the value of technology innovations (Guney et al., 2017), as they have been realizing that such innovations may create incentives for an early investment due to the lower costs. Due to the importance of these innovations to companies and governments, this has been broadly reported in the media (some examples are presented in the next paragraphs) and discussed in the economic literature (see for instance Flor and Hansen (2013) and Murto (2007)).====For example, in the renewable energy sector we see a large impact of falling costs. In December 2018, the International Renewable Energy Agency (IRENA) mentioned that the solar photovoltaic module prices have fallen by around 80% since 2009, and the wind turbine prices have fallen around 30–40%==== In the same line, Hanno Schoktlish, CEO of Kaiserwetter argues that “the decreases in the cost of renewable energy... has occurred for several reasons. These include technological improvements... ”====. Also Hunt and Shuttleworth (1996) report about decreasing investment cost in energy investment. According to these authors, as a result of studies sponsored by space programs, it was possible to build turbines much more efficient and smaller than before, reducing in a drastic way the optimal power plant size, with enormous cost reduction. Aside from technological innovations, other factors may lead to a sudden decrease in the investment cost. For instance, as a result of government interventions in key areas of the economy (as the renewable energies), as Lin and Wong (2013) analyze.====A recent global survey of IT and finance leaders by independent research firm Vanson Bourne showed that in manufacturing, 42% of the CEO’s of 900 companies said that they have already reduced operational costs through innovation====. This impact is particularly important in high-tech companies, where the progress in technology takes advantage of other industries, as it is the case of pharmaceutical companies. For example, advances in technology related to biomarkers, as well as developments in the understanding of the human genome, have changed the cost structure for firms developing products targeting small patient populations====. Still in the health sector, research studies (see for instance Lee and Choi (2019)) have demonstrated that investing in health IT in a hospital setting has potential benefits, that impact in the reduction of the cost, by increasing efficiency and productivity metrics.====We consider an investment model with two sources of uncertainty: the price (reflected in the revenue of the firm) and the level of technology, which impacts in the investment cost. The firm needs to optimize its investment decision by taking into account the random fluctuations of the revenue and the changing investment cost. Problems with two sources of uncertainty have been studied in the real options literature, as one can see, for example, in Alghalith (2016); Dixit and Pindyck (1994); McDonald and Siegel (1986); Murto (2007); Pennings and Sereno (2011), and Zambujal-Oliveira and Duque (2011), among others.====According to Bethuyne (2002), ====. In this paper, we assume that technology innovations impact the investment cost and not the price. Additionally, the innovation process (assumed exogenous to the company), is driven by a Poisson process. Then, the investment cost decreases with technology innovation by means of downward jumps. Furthermore, the size of the downward jump in the investment cost is known beforehand. These assumptions are in common with most literature about real options models for technology innovations (see for instance, Doraszelski (2001); Farzin et al. (1998) and Hagspiel et al. (2015)).====The discontinuity of the cost process can be observed for instance in the following example. A firm that develops apps or games for mobile devices often needs to buy a large amount of smartphones. The purchase of smartphones is an investment cost for the firm. For example, in the Apple’s case, when a new generation of the iPhone is launched, the price of the current generation jumps down==== This means that the technology innovation has an impact in the price, and consequently in the firm’s investment cost. Moreover, between consecutive launches of new versions, the price usually stays constant.====This type of assumption in the dynamics of the investment cost is also considered in other papers. For instance, Mauer and Ott (1995) make similar assumption regarding the impact of technology progress in the optimal replacement policy, when a technological breakthrough lowers the initial maintenance and operation cost. The authors also assume that such breakthroughs follow a Poisson process with constant intensity. Cheevaprawatdomrong and Smith (2003), also in the context of replacement problems, assume a simpler model, as they consider that ====.====Our framework is very close to the one assumed by Murto (2007), since we both study the timing of investment under effects of technological and revenue uncertainty. Additionally, we both assume that the revenue stream generated by the investment follows a geometric Brownian motion (GBM), and that the technology progress follows a Poisson process.====In his paper, Murto (2007) states that, although the problem is well-posed, there is no analytical solution to the Hamilton-Jacobi-Bellman (HJB) equation that characterizes the value function. The reason of this difficulty lays in the fact that the stopping region may be attained continuously (due to an increase in the price) or discontinuously (as consequence of a downward jump in the investment cost). Then Murto (2007) proposes an analytical solution only in the following particular cases: either the price process is deterministic (meaning that he assumes that there is no volatility in the GBM); or the technological progress is deterministic (leading to an exponential decline in the investment cost). Therefore, in the cases he studied, instead of having a problem with two sources of uncertainty (that would lead to an exercise boundary and not to a point), he transforms it in a problem with just one source of uncertainty, where the classic tools (including verification theorems) may be used.====A similar model, where an analytical solution is obtained when there are two sources of uncertainty, can be found in Nunes and Pimentel (2017). In this paper, the authors consider that both the revenue and the costs are jump-diffusion processes, where the jumps in the revenue are downward jumps and the jumps in the investment cost are upwards. The direction of these jumps is such that, contrary to the case that we analyze in the current paper, the stopping region is always attained through a continuous movement. This, combined with the fact that the value function is homogeneous of degree one (and therefore one may consider a change of variable, as proposed in Dixit and Pindyck (1994)), leads to an optimal stopping time problem where an analytical solution can be found. There are other cases, outside of the context of real options, where we can reduce a two dimensional optimal stopping problem to one dimensional one. For instance, the optimal stopping of Bessel processes with integer index (Dubins et al., 1994) and the one-asset-for-other problem of the right to exchange one asset by another, used mostly in the context of American options (Gerber and Shiu, 1996). We refer to Christensen et al. (2019) for an updated review on optimal stopping of multidimensional diffusions.====Our contribution to the state of the art is to propose an approximation for the value function, and for the prices and levels of technology that trigger the investment decision. The approximation proposed in this paper is based on a truncation approach, and, for that reason, we call the approximated model by truncated problem. The truncated method was first addressed, in the field of real options, by Guerra et al. (2017). Using the results found for this approximation, we are able to provide insights about the original investment problem. Furthermore, we prove that (i) the thresholds of the truncated problem converge to the thresholds of the original model, and (ii) the solution of this problem converges to the solution of the original problem. The approach considered in our paper differs from other papers that also deal with the numerical analysis of an investment problem with two sources of uncertainty. We cite, for instance, Lange et al. (2020), and references therein. Our approach relies on a probabilistic framework rather than a numerical scheme to solve differential equations, which allows us to gain an important economic intuition about the behavior of the model’s solution. Besides the advantage of the interpretation of the results for the truncated problem, we are able to provide the solution and to prove analytically its optimality, using a suitable verification theorem, for a specific case. We also present a numerical illustration that highlights the behavior of the truncated value function.====Furthermore, we are also able to provide formal proofs regarding the behavior of the investment thresholds with some parameters. Indeed, we show that the classical results of real options regarding the impact of the drift and volatility of the price hold in our case (namely, that the investment threshold increases with the volatility but decreases with the drift). But when one considers other parameters, in particular the ones related with the technology innovation process, the results are not standard, since they show non-monotonicity.====The method proposed in this paper can be useful for other problems with the same features. For instance, it may be used to analyze the problem addressed by Nunes and Pimentel (2017), but assuming now that jumps may also lead directly to an investment decision. In the same line, it can be used in the context of the problem addressed by Couto et al. (2012), where it is assumed that the processes that model the uncertainty is a jump-diffusion process. This method can also be applied to the problem presented in Hagspiel et al. (2018), where, similar to our case, the stopping region may be attained by a jump, in a discontinuous way.====The paper is organized as follows: in Section 2 we define the investment model, in Section 3 we present a truncated problem and we prove the convergence of its solution to the solution of the original one. In Section 4 we derive the solution to the truncated problem for a specific case, and we show a numerical illustration of the results. In Section 5 we present the comparative statics for the investment threshold with respect to the relevant parameters and in Section 6 the main conclusions of the paper are presented. Finally, there are three appendices: in the first one we provide the general expressions of the truncated problem, the second is where we present all the proofs and in third we provide some numerical results.",Quasi-analytical solution of an investment problem with decreasing investment cost due to technological innovations,https://www.sciencedirect.com/science/article/pii/S0165188921000890,20 May 2021,2021,Research Article,205.0
"Ma Jingtang,Yang Wensheng,Cui Zhenyu","School of Economic Mathematics, Southwestern University of Finance and Economics, Chengdu 611130, PR China,School of Business, Stevens Institute of Technology, Hoboken, NJ 07030, United States","Received 19 January 2021, Revised 9 April 2021, Accepted 5 May 2021, Available online 13 May 2021, Version of Record 1 June 2021.",https://doi.org/10.1016/j.jedc.2021.104145,Cited by (3), model and the ,"Almost all single-name individual options traded in the US options market are of the American type, i.e., it allows early exercise before the maturity, while only index options such as the S&P500 index options (with ticker SPX) are usually of the European style. In contrast to European options, the valuation of finite-maturity American options generally does not permit closed-form formulae, except for the case of an American call when there is no dividend. The main challenge for the valuation of American options lies in that both the option price and the early exercise boundary (or surface) have to be simultaneously determined, and the resulting pricing equation is a free boundary partial differential equation (PDE), which is very difficult to solve either theoretically or numerically.====Due to the lack of closed-form solutions, there has been a vast amount of literature proposing different valuation methods for American options when the underlying assets follow different stochastic processes. The integral equation method is widely used to solve the early exercise boundary. Representative classical literatures include (Carr, Jarrow, Myneni, 1992, Detemple, Feng, Tian, 2003, Detemple, Tian, 2002, Huang, Subrahmanyam, Yu, 1996, Kim, 1990); recent literatures include: (Kim, Lim, 2021, Lin, He, 2021, Lu, Putri, 2020). Other methods can be classified into several categories: tree or lattice-based methods (Broadie, Detemple, 1996, Broadie, Glasserman, 1997); Monte Carlo simulation method (Longstaff and Schwartz, 2001; Chen, Liu, 2014); analytical approximation method based on approximating the early exercise boundary (Ju, 1998); method of lines or maturity randomization method (Carr, 1998); finite difference method for the free boundary partial differential equation (Ikonen and Toivanen, 2004); static hedging approach (Chung and Shih, 2009); Laplace transform approach (Ma, Cui, Li, 2020, Wong, Zhao, 2010), etc. For a comprehensive review of different methods in the literature, please refer to (Detemple, 2005) and the references therein.====Note that most of the afore-mentioned literature has focused on the Black-Scholes model, in which the underlying asset price follows a geometric Brownian motion. It is well-known that the actual financial market exhibits features that are not compatible with the Black-Scholes model, including the phenomenon of volatility smile, sudden asset jumps (e.g. flash crash), and random fluctuation of interest rates. There are also a strand of literature that are devoted to the valuation of American options for alternative stochastic processes, such as general diffusion processes (Detemple et al., 2003), stochastic interest rates and/or stochastic volatility models (Chockalingam, Muthuraman, 2011, Medvedev, Scaillet, 2010), hyper-exponential jump diffusion models (Cai and Sun, 2014), etc. However, these methods proposed in the literature are usually specific to the stochastic process studied, and there is no universal single methodology that applies to most of the stochastic processes encountered in financial modeling. The motivation of this paper is precisely to address this. More specifically, we aim to propose a general valuation framework for (finite maturity) American options when the underlying assets follow stochastic local volatility (SLV) models. This class of models are general enough to incorporate many well-known stochastic processes as special cases, including the Black-Scholes model, the Heston model, and also the stochastic alpha beta rho (SABR) model, etc. Please refer to Table 1 for a list of popular SLV models and corresponding references.====This paper proposes a novel continuous-time Markov chain (CTMC) approximation method for the American options when the underlying asset follows one dimensional time-homogeneous Markov processes (e.g. diffusions) or two-dimensional stochastic local volatility models. It is important to distinguish our work from two recent papers that also utilize the CTMC method in the valuation of American options.====It is also important to distinguish this paper from the existing literature on integral equation approach to finite-maturity American options. There are two important components in the integral equation, with one being the corresponding European option price, and the other term the early exercise premium (EEP). For general stochastic models beyond the Black-Scholes model, even the European option price is not available in closed-form, hence the integral equation is at most ==== defined, and it is challenging to solve it. This is the reason why the integral equation method for American options is only applied in a handful of stochastic models, such as the Black-Scholes model, the constant elasticity of variance (CEV) model, and the Heston model, etc, and see (Cheang, Chiarella, Ziogas, 2013, Detemple, Tian, 2002). Our CTMC approximation approach precisely addresses this limitation, and allows us to extend the power of the integral equation method to a general class of stochastic processes, including in particular the class of stochastic local volatility models. The CTMC method has the advantage that it allows us to explicitly express both the price of European option and the EEP term, hence the corresponding approximate integral equation governing the early exercise boundary is given explicitly.====The contributions of the paper are three-fold: a novel algorithm design, the proof of theoretical convergence order and the numerical comparison. First, we propose a novel method, the CTMC integral equation (CTMCIE) method, for the valuation of finite-maturity American options in the class of general stochastic local volatility models. This presents a unified theoretical and computational framework. Note that, to the best of our knowledge, this paper is the first to establish semi-explicit formula for the Greeks of American options in general SLV models. Second, we establish the theoretical convergence of our algorithm and determine the convergence order, for both the prices and the corresponding Greeks. Last but not least, numerical examples demonstrate the high accuracy and efficiency of the method, and it compares favorably with alternative methods in the literature.====The remainder of the paper is organized as follows. Section 2 presents the early exercise premium (EEP) representation for American option prices under the stochastic local volatility (SLV) models. Section 3 presents the main results, namely, the CTMC integral equation method for the early exercise surface, values and Greeks of American options. Section 4 gives the proof of the convergence order of the CTMC integral equation method and the results of the numerical experiment are shown in Section 5. Finally, Section 6 concludes.",CTMC integral equation method for American options under stochastic local volatility models,https://www.sciencedirect.com/science/article/pii/S0165188921000804,13 May 2021,2021,Research Article,206.0
"Magnus Jan R.,Pijls Henk G.J.,Sentana Enrique","Department of Econometrics and Data Science, Vrije Universiteit Amsterdam, and Tinbergen Institute, Amsterdam, The Netherlands,Korteweg-de Vries Institute for Mathematics, University of Amsterdam, Amsterdam, The Netherlands,CEMFI, Madrid, Spain","Received 17 June 2020, Revised 4 March 2021, Accepted 28 March 2021, Available online 8 May 2021, Version of Record 8 May 2021.",https://doi.org/10.1016/j.jedc.2021.104122,Cited by (2),: a continuous-time macro model and the parameterization of ====.,"The exponential function ==== is one of the most important functions in mathematics and its history goes back to the brothers Jacob and Johann Bernoulli in the late 17th century. The matrix exponential ==== is more complicated and it was not introduced until the late 19th century by Sylvester, Laguerre, and Peano.====The matrix exponential plays an important role in the solution of systems of ordinary differential equations (Bellman, 1970), multivariate Ornstein–Uhlenbeck processes (Bergstrom, 1984 and Section 9 below), and continuous-time Markov chains defined over a discrete state space (Cerdà-Alabern, 2013). The matrix exponential is also used in modelling positive definiteness (Linton, 1993, Kawakatsu, 2006) and orthogonality (Section 10 below), as ==== is positive definite when ==== is symmetric and orthogonal when ==== is skew-symmetric.====The derivative of ==== is the function itself, but this is no longer true for the matrix exponential (unless the matrix is diagonal). We can obtain the derivative (Jacobian) directly from the power series, or as a block of the exponential in an augmented matrix, or as an integral. We shall review these three approaches, but they all involve either infinite sums or integrals, and the numerical methods required for computing the Jacobian are not trivial (Chen, Zadrozny, 2001, Tsai, Chan, 2003, Fung, 2004).====The purpose of this paper is to provide a closed-form expression which is easy to compute, is applicable to both defective and nondefective real matrices, and has no restrictions on the number of parameters that characterize ====.====We have organized the paper as follows. In Section 2 we discuss and review the matrix exponential function. Three expressions for its Jacobian (Propositions 1–3) are presented in Section 3 together with some background and history. These results are not new. Our main result is Theorem 1 which is new and is presented in Section 4 and discussed in Section 5. In Sections 6 and 7 we apply Theorem 1 to defective and nondefective matrices (Theorem 2) and discuss structural restrictions such as symmetry and skew-symmetry. In Section 8 we derive the Hessian matrix (Theorem 3). Two applications in macroeconometrics demonstrate the usefulness of our results: a continuous-time multivariate Ornstein–Uhlenbeck process for stock variables observed at equidistant points in time (Section 9) and a structural vector autoregression with non-Gaussian shocks (Section 10). In both cases, we explain how to use our main result to obtain the loglikelihood scores and information matrix in closed form. In Section 11 we further illustrate the usefulness of our analytical expressions in an empirical application which analyzes the economic impact of macro and financial uncertainty by means of a trivariate structural ==== model for monthly observations from August 1960 to April 2015 on a macro uncertainty index, a financial uncertainty index, and the rate of growth of the industrial production index. Section 12 concludes. The appendix contains all proofs. As a byproduct, Lemma 2 in the Appendix presents an alternative expression for the characteristic (and moment-generating) function of the beta distribution, which is valid for integer values of its two shape parameters.",The Jacobian of the exponential function,https://www.sciencedirect.com/science/article/pii/S0165188921000579,8 May 2021,2021,Research Article,207.0
"Choi Jaehyuk,Wu Lixin","Peking University HSBC Business School, University Town, Nanshan, Shenzhen 518055, China,Department of Mathematics, The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong, China","Received 12 December 2019, Revised 28 March 2021, Accepted 28 April 2021, Available online 6 May 2021, Version of Record 27 May 2021.",https://doi.org/10.1016/j.jedc.2021.104143,Cited by (4),"This study presents new analytic approximations of the stochastic-alpha-beta-rho (SABR) model. Unlike existing studies that focus on the equivalent Black–Scholes (BS) volatility, we instead derive the equivalent constant-elasticity-of-variance (CEV) volatility. Our approach effectively reduces the approximation error in a way similar to the control variate method because the CEV model is the zero vol-of-vol limit of the SABR model. Moreover, the CEV volatility approximation yields a finite value at a zero strike and thus conveniently leads to a small-time asymptotics for the mass at zero. The numerical results compare favorably with the BS volatility approximations in terms of the approximation accuracy, small-strike volatility asymptotics, and no-arbitrage region.","The stochastic-alpha-beta-rho (SABR) model proposed by Hagan et al. (2002) is one of the most popular stochastic volatility models adopted in the financial industry. Its commercial success is owing to a few factors. The model is intuitive and parsimonious. It provides a flexible choice of ====, the trace of the at-the-money volatility against the spot price. Most importantly, Hagan et al. (2002) provide an analytic approximation of implied Black–Scholes (BS) volatility in closed form (hereafter, the HKLW formula), from which traders can readily convert to the option price using the BS formula.====The HKLW formula is an asymptotic expansion valid for small time to maturities (up to the first order in time) and near-the-money strike prices. Several authors have attempted to improve the HKLW formula. Based on the results of Berestycki et al. (2004), Obłój (2007) corrects the leading order term of the HKLW formula. Henry-Labordère (2005) derives the same leading order term from the heat kernel under hyperbolic geometry. Paulot (2015) further provides a second-order approximation which is accurate in a wider region of strike prices at the cost of numerical integrations for the second-order term. Further, Lorig et al. (2017) obtain implied BS volatility up to the third order in time, which unfortunately is valid only near the money. A more accurate solution of the SABR model, however, requires large-scale numerical methods such as the finite difference method (Hagan, Kumar, Lesniewski, Woodward, 2014, Park, 2014, von Sydow, Milovanović, Larsson, In’t Hout, Wiktorsson, Oosterlee, Shcherbakov, Wyns, Leitao, Jain, Haentjens, Waldén, 2019), continuous time Markov chain (Cui et al., 2018), multidimensional numerical integration (Antonov et al., 2013, Islah, 2009, Korn, Tang, 2013, Henry-Labordère), or Monte-Carlo simulation (Cai, Song, Chen, 2017, Chen, Oosterlee, Van Der Weide, 2012, Choi, Liu, Seo, 2019).====By nature, analytic approximation methods suffer two important drawbacks when some parameters or strike price go beyond the ==== of the concerned methods: non-negligible price error from the true value and the occurrence of arbitrage opportunity. Nevertheless, these methods are still attractive to practitioners because they are fast and robust. Note that practitioners need to compute the prices and Greeks of thousands of European options (or swaptions) frequently during trading hours. The calibration of the model parameters to the observed volatility smile also requires fast option evaluation because the parameters must be found using iterative methods. The numerical methods mentioned above are computationally intensive and not fast enough to use for those purposes.====Fortunately, the errors in analytic approximations are not a significant issue for those who use the SABR model primarily to price and manage the risk of European options. Specifically, the model parameters should first be calibrated to the market prices of the options at several liquid strike prices near the money. Then, the calibrated model is used to price the options at other strike prices. In this sense, the SABR model serves as a tool to ==== (as well as ====) the volatility smile, meaning that the accuracy of the price formula is less a concern.====Arbitrage under the analytic approximations occurs because an absorbing boundary condition is actually not imposed at the origin, as the small-time asymptotics of the transition density does not ==== the boundary. The SABR process has a non-zero probability of hitting zero for ====, and an absorbing boundary condition should be explicitly imposed at the origin for ==== for the price process to be a martingale and arbitrage-free. For this reason, the analytic approximations exhibit arbitrage opportunity in the low-strike region. The arbitrage outbreak is still an important concern to options market makers; savvy hedge funds can exploit them by purchasing a butterfly of options with a negative premium. To avoid such trades, market makers carefully keep track of the lower bound of the arbitrage-free region (and often patch a different arbitrage-free model below the bound). Therefore, the degree of arbitrage should be yet another performance measure for testing newly proposed analytic approximations, as in Obłój (2007), as much as the approximation error.====We contribute to the SABR model literature by proposing new analytic approximations that are more accurate and have a wider arbitrage-free strike region than existing studies. We derive the equivalent volatility under the constant-elasticity-of-variance (CEV) model, from which the option price is computed with the analytic CEV option price formula (Schroder, 1989). We provide two formulas for the equivalent CEV volatility as spin-offs from existing studies. The first one (Theorem 1) is obtained by following the approximation method of Hagan et al. (2002). The second one (Theorem 2) is simplified from Paulot (2015)’s original CEV volatility formula.====Our CEV-based approach is motivated by the simple intuition that the SABR model should converge to the CEV model when the volatility of volatility (vol-of-vol) approaches zero. Such motivation for using the CEV model is not novel in the SABR model literature. Yang et al. (2017) show that the CEV option price (with the CEV volatility being the initial SABR volatility) is a good approximation in certain parameter ranges and is, naturally, arbitrage-free. The practical use of the result, however, is limited because the parameters related to the volatility process (i.e., vol-of-vol and correlation) are ignored in the approximation and only one degree of freedom is left to fit the volatility smile. Our work extends Yang et al. (2017) as our CEV approximations have full dependency on the SABR parameters. Paulot (2015, §3.6, 4.5) also discusses the equivalent CEV volatility as an alternative to BS volatility and outlines the derivation. His emphasis, however, is placed on BS volatility and the CEV volatility approximation is not tested numerically. Further, there is no discussion about the implications such as the mass at zero. In short, the advantage of the CEV approach has not thus far been explored. We fill this research gap by advocating the use of CEV volatility for the SABR model.====The numerical results show that our CEV-based approximations are more accurate than the corresponding BS-based methods from which they stem. In particular, the presented CEV approaches are more accurate when the initial volatility is large. This finding complements Paulot (2015)’s refinement, which makes the approximation more accurate for large vol-of-vol. Having both advantages, the second CEV approximation based on Paulot (2015) performs the best among all the methods over wide parameter ranges. In the numerical test for comparing the degree of arbitrage, the second CEV approximation also performs the best among all the methods in that negative implied probability density starts to appear at the lowest strike price (see Section 4.3).====Surprisingly, the projection of the SABR model to the CEV model has the effect of imposing an absorbing boundary condition at the origin because the CEV price formula assumes the same boundary condition. Our CEV approximations offer finite CEV volatility at a zero strike, making them capable of implying the probability of hitting the origin. The mass-at-zero approximation in a closed-form formula (Theorem 3) shows excellent agreement with the numerical results in small time and is consistent with the exponentially vanishing asymptotics in the limit (Chen and Yang, 2019). To the best of our knowledge, our approximation is the first closed-form approximation that works for all ====, as the existing estimation methods either work on the uncorrelated case only (Gulisashvili et al., 2018) or require numerical integration for the correlated case (Yang and Wan, 2018). Even if the mass at zero from our CEV approximations becomes less accurate in large time or vov-of-vol, our CEV approximations remain internally consistent with the model-free smile shape determined by the (possibly incorrect) mass at zero (De Marco et al., 2017). This explains why our CEV approach has a wider arbitrage-free region.====The remainder of this paper is organized as follows. Section 2 reviews the SABR model and existing BS volatility approximations. Section 3 derives the equivalent CEV volatility and mass-at-zero approximation. Section 4 presents the numerical results. Finally, Section 5 concludes.",The equivalent constant-elasticity-of-variance (CEV) volatility of the stochastic-alpha-beta-rho (SABR) model,https://www.sciencedirect.com/science/article/pii/S0165188921000786,6 May 2021,2021,Research Article,208.0
"Da Fonseca José,Malevergne Yannick","Auckland University of Technology, Business School, Department of Finance, Private Bag 92006, 1142 Auckland, New Zealand,PRISM Sorbonne EA 4101, Université Paris 1 Panthéon-Sorbonne, 17 rue de la Sorbonne, 75005 Paris, France,PRISM Sorbonne EA 4101 & Labex ReFi, Université Paris 1 Panthéon-Sorbonne, 17 rue de la Sorbonne, 75005 Paris, France","Received 21 July 2020, Revised 19 February 2021, Accepted 14 April 2021, Available online 1 May 2021, Version of Record 1 June 2021.",https://doi.org/10.1016/j.jedc.2021.104137,Cited by (3),"We develop a microstructure model whose order flow is driven by a Cox-BESQ process. We derive important analytical properties of the Cox-BESQ process in order to explicit the stock price dynamics at different time scales, provide different parameter estimators and solve the optimal execution problem. We implement the model using a large data set of stock index and bond futures. Our results show that the Cox-BESQ process provides an alternative framework to the Hawkes process to build a microstructure model that is very flexible and has an explicit solution.","Building a microstructure model for high frequency data is a challenging task as it must be able (ı) to handle trades and quotes of different types; (ıı) to explicitly relate the asset’s dynamics at high frequency with its dynamics at low frequency that typically involves a continuous time diffusion process, this feature is often called the time scale consistency;==== and (ııı) to solve, possibly explicitly, the optimal execution problem whereby a certain number of stock shares can be purchased/sold over a given time interval. The Hawkes process, introduced by Hawkes (1971), enables such an achievement. Indeed, Large (2007) and Bowsher (2007) show how it provides a natural framework to study the statistical properties of the limit order book. Bacry, Delattre, Hoffmann, Muzy, 2013, Bacry, Delattre, Hoffmann, Muzy, 2013 develop a model that allows explicit computations of the asset’s statistical properties such as the first two moments as well as the autocorrelation function of price increments. What is more, they prove a remarkable result, that is they solve analytically the time scale property of the model thereby relating the high frequency asset dynamics to its low frequency dynamics (typically daily). Regarding the problem of optimal execution, it is solved explicitly in a Hawkes microstructure model by Alfonsi and Blanc (2016). Their lengthy and elegant proof heavily relies on the affine property of the Hawkes process and the quadratic nature of the objective function. It is rather uncommon that a stochastic process enables explicit solutions to so diverse problems.====Quite naturally these works were extended in several directions. In the technical works of Jaisson, Rosenbaum, 2015, Jaisson, Rosenbaum, 2016, the authors obtain under different parameter assumptions a low frequency model for the stock given by a diffusion process driven by a Brownian motion whose volatility is either an integrated square root process driven by a Brownian motion (Jaisson and Rosenbaum, 2015) or an integrated fractional square root process (Jaisson and Rosenbaum, 2016). This latter work is of particular interest as it relates to the growing literature on rough diffusion processes applied to finance. Lee and Seo (2017) propose a Hawkes based microstructure model that leads to a low frequency dynamic for the stock given by a diffusion process driven by a Brownian motion with stochastic volatility following a square root process (also driven by a Brownian motion). Regarding microstructure models that lead to an explicit optimal execution strategy, several extensions were recently proposed. Fruth et al. (2014) consider a deterministic time-varying resilience and depth order book model, Siu et al. (2019) develop a regime-switching model to take into account the stochastic nature of the market resilience while Chen et al. (2018) propose a stochastic liquidity model based on a finite Markov chain.====We contribute to the literature by developing a microstructure model based on the Cox-BESQ process which is a Cox process whose intensity is given by a scalar square root process (BESQ process or Cox-Ingersoll-Ross process). This process belongs to the standard affine class of stochastic processes as defined in Duffie et al. (2000) and has been extensively used in the credit risk literature (Duffie and Singleton, 2003). Quite surprisingly it has never been used in the microstructure literature.==== A simple remark shows how the Cox-BESQ process is related to the Hawkes process, which has far reaching consequences that are illustrated throughout the paper.====Within this framework, our contribution is twofold. First, we develop a microstructure model and build a robust and fast estimation strategy that is designed to handle large data sets; we compute the basic statistical properties of the asset as well as its diffusive limit, thus specifying the asset dynamics at different time scales. Second, we develop a microstructure model with mixed-market-impact and explicitly solve the optimal execution strategy thanks to the affine nature of the process and show that it evolves stochastically around Obizhaeva and Wang (2013)’s strategy. Lastly, using a large data set, we show how both applications can be implemented and how well the model performs.====The structure of the paper is as follows. Section 2 contains the analytical results related to the Cox-BESQ process and the estimation methodology. Section 3 presents the microstructure model based on the Cox-BESQ process, derives the statistical properties of the asset as well as solves the optimal execution problem. Section 4 concludes the paper while the proofs are gathered in the appendix.",A simple microstructure model based on the Cox-BESQ process with application to optimal execution policy,https://www.sciencedirect.com/science/article/pii/S0165188921000725,1 May 2021,2021,Research Article,209.0
"Juselius Mikael,Takáts Előd","Bank of Finland; Bank of Finland P.O. Box 160 FI-00101 Helsinki, Finland,Bank for International Settlements and London School of Economics","Received 7 September 2020, Revised 5 March 2021, Accepted 20 April 2021, Available online 30 April 2021, Version of Record 26 May 2021.",https://doi.org/10.1016/j.jedc.2021.104136,Cited by (3),Demography accounts for a large share of low frequency ,"The rise of inflation in the 1960s and 70s and its subsequent reduction in the 1980s has been widely discussed in the literature (see e.g. Goodfriend and King (2005)). The consensus today is that central banks initially lost control over inflation expectations, only to regain it later when they started to forcefully tighten monetary policy. This narrative has been influential in the development of forward looking macro models, and its policy lessons remain relevant today. In this paper we document an empirical regularity that has the potential of broadening this narrative: there is a robust relationship between inflation and demographics through history that largely accounts for inflation trends in the data.====Possible links between demographics and inflation have surfaced recently. For instance, Goodhart et al. (2015) observe that the current stubbornly low inflation rates in advanced economies coincide with a demographic shift: the imminent retirement of baby-boomers. Increased savings associated with this shift could have driven down the natural interest rate to the extent that nominal rates have become constrained by the effective lower bound (see e.g. Summers (2014)).==== In such a liquidity trap, central banks cannot cut rates further and might therefore not be able to counter deflationary pressures. However, this would at most generate a temporary relationship between demographics and inflation when the effective lower bound binds in contrast to the long-lasting relationship that we report in this paper.====There are at least two ways in which a more lasting relationship between demography and inflation could emerge. One possibility is the interest rate misalignment explanation. Interest rates might be misaligned to stabilize inflation (i) if central banks target other goals (such as stable exchange rates) or (ii) if inflation targeting central banks fail to internalize slow-moving changes in the natural rate of interest. Indeed, measuring the natural rate in real-time is difficult (see Beyer and Wieland (2019)).==== The other possibility is the political economy hypothesis. Central bank inflation targets may be influenced by voter preferences, which, in part, depend on the composition of the age structure (Bullard (2012)). In both explanations, central banks set interest rates too low or too high compared to the inflation stabilizing rate, allowing demography to drive inflation.====A nascent empirical literature has found links between demographics and inflation in post-World War II data, but struggled with a number of issues.==== One key issue is that demographic cycles are very long and may resemble trends in shorter samples. This can produce artificially strong correlation with inflation in those samples where inflation also trends, even if this correlation is not meaningful. A second issue is that some common drivers of inflation, such as inflation expectations and output gaps, are not perfectly observed, making it harder to exclude omitted variables as a likely cause of any detected demographic effect.====We clarify the extent to which there is a pervasive link between demographics and inflation in the data, as well as its characteristics, by addressing the two key empirical challenges head-on:====First, we use a long panel of 22 countries from 1870 to 2016. In contrast to existing studies that only focus on post-war samples, this allows us to study the robustness and stability of a possible link between demographics and inflation in several sub-samples. This includes a pre Second World War sample that is not affected by the baby-boomer generations and the high-inflation episode of the 1960s and 70s. We also try dynamic specifications and include time fixed effects throughout to remove the possibility of spurious results due to coinciding trends in specific sub-samples.====Second, we nest our demographic specification within both backward and forward looking Phillips curve specifications to ensure that more conventional business cycle drivers of inflation are not mistaken for a demographic effect. We also use non-overlapping averages of the data to reduce the influence of business cycle forces on our estimates, as well as, control for several other slow-moving factors, such as the fiscal balance and non-demographic drivers of the natural rate.====In all of these approaches, we find a robust and stable relationship between demography and inflation. In terms of age structure, the dependents (the young and the old) are generally associated with higher, and the working age cohorts with lower inflation. In other words, a higher dependency ratio is associated with higher inflation. The only substantial exception to this pattern is the very old cohort (80+ year olds) that has a negative effect on inflation. Yet, this cohort is more heavily affected by longevity than the other cohorts. We also find significant positive association between population growth and inflation, and negative association between increasing longevity and inflation.====The uncovered relationship is broadly consistent with the interest rate misalignment explanation, i.e. that monetary policy incorporates slow movements in the natural rate only with a delay. The interest rate misalignment explanation would predict higher inflation when the natural rate is higher, i.e. when the share of dependents (young and old) is high relative to the share of working age populations. This is what our empirical estimates suggest. In contrast, the empirical results do not fully support the political economy explanation - according to which older population with high nominal savings prefer lower inflation while working age cohorts with labor income higher inflation.====The demographic effects are economically significant for trend inflation, both in individual countries and at the global level. For example, demography accounts for around a seven percentage point increase in inflation from the 1950s to the mid-1970s in the United States, and a similar decline thereafter. In line with this finding, demography also matters for inflation persistence. Adding the demographic variables to dynamic specifications of the inflation process reduces estimated persistence from near unit-root to stationary levels. Of course, demography does not explain higher frequency variation in inflation well.====We also attempt to address potential reverse causality by exploiting “natural experiments” associated with the two world wars. We examine whether exogenous wartime casualties affect inflation going forward through their long-lasting material impact on population age structure. In particular, we collect data on military casualties (dead and wounded) and the decline in birth rates during the wars. We observe associations that are in line with our main findings between these demographic “shocks” and inflation up to 20 years after each war. These results should be viewed with some caution, however, as the available sample is small and it is difficult to control for all relevant confounding factors associated with the wars.====Our findings have implications for both macroeconomic theory and monetary policy. For theory, our results highlight the possibility that informational frictions together with demographics might lead to unexpected and persistent dynamics in inflation. Gaining deeper understanding of the mechanism that generates this outcome seems critical and may, for instance, help shed light on the current conundrum with low and stable inflation despite substantial monetary easing. Going forward, our results suggest that current demographics headwinds with respect to inflation will turn into tailwinds over the coming decade as the baby boomer generations retire. Unless central banks are able to adapt policy frameworks to counter this force, inflation could rise by up to three to four percentage points at a global level by the end of 2030.====The rest of the paper is organized as follows. The next section presents the data and the full sample empirical results. The third section discusses the economic impact and its implications. The final section concludes.",Inflation and demography through time,https://www.sciencedirect.com/science/article/pii/S0165188921000713,30 April 2021,2021,Research Article,210.0
"Khalaf Lynda,Lin Zhenjiang","Department of Economics, Carleton University, Canada,School of Economics, University of Nottingham Ningbo China, China","Received 6 April 2020, Revised 13 April 2021, Accepted 15 April 2021, Available online 30 April 2021, Version of Record 21 May 2021.",https://doi.org/10.1016/j.jedc.2021.104138,Cited by (0),This paper introduces ==== [====] to ,"Important developments in economics and econometrics have often been driven by improvements and increased use of efficient numerical and computational tools. In econometrics, one important and recent research area relates to non-standard test inversion and projection-based inference methods, most of which can require technically complex computing. Projection-based inference methods have gained popularity following the introduction of non-standard tests in Instrumental Variable [====] regression and with the Generalized Method of Moments [====], to account for weak-identification and/or weak IVs.==== From the applied econometrics perspective and despite the breadth of supportive theoretical work, very little guidance is available regarding implementing projections.====While non-linear global optimization methods such as the Genetic Algorithm [====] and Simulated Annealing [====] are well known in econometrics, their properties have not been analyzed as yet for projection-based test inversion purposes.==== Furthermore, the Particle Swarm Optimization [====] method, that may be traced back to Kennedy and Eberhart (1995), seems to have escaped notice in this field despite its popularity in ==== engineering and computer science. This paper introduces PSO to statistical functions with focus on numerical projection-based test inversion.====Inverting a test, which produces a joint Confidence Set [====] for intervening parameters, means assembling the parameter values that are not rejected by this test at a given level. The least rejected parameter value can be treated as a point estimate. Projecting a CS, which produces confidence intervals for individual parameters, entails finding the smallest and largest values of each parameter component within this region. The resulting sets can be unbounded or empty, which would suggest identification or specification problems, respectively.====Methodologically, projection-based test inversion amounts to - typically numerical - complicated and often high-dimensional constrained optimization problems (which we formally describe in Section 3 of this paper). Due to the non-convexity of the underlying objective functions, traditional ==== gradient-based techniques are not appropriate for this purpose. Stochastic search methods including SA, GA and PSO will work, yet their efficiency in this case remains to be documented.====PSO is an unconstrained global optimization method inspired by bird flocking. In contrast to SA which relies on a single candidate solution at every iteration, PSO - similarly to GA - works with a group (the so-called ====) of candidate solutions (the so-called ====). The key difference with GA is that PSO evolves towards the optimum through a mixture of: (i) adaptive learning which improves efficiency, and (ii) random information exchanges that break the search away from local optima.==== In this context, this paper has two contributions.====First, we document the performance of PSO relative to SA and GA on projection-based test inversion, using a familiar macroeconometric model and commonly used data. We consider four tests to be inverted, three of which follow the (Anderson and Rubin, 1949) principle====: (i) the linear Anderson–Rubin test for univariate i.i.d. IV regressions, which we denote the ==== test and its generalized Limited-Information [====] multivariate counterpart which we denote the ==== test, and (ii) the Full-Information [====] VAR-based statistic introduced by Dufour et al. (2013) which focuses on Dynamic Stochastic General Equilibrium [====] systems; we denote the latter as the ==== test. These tests embed a specification check of the underlying structure and can thus yield empty sets. This may lead to statistical interpretation hurdles, as was raised by Davidson and MacKinnon (2014) and Müller and Norets (2016). We thus complement the analysis with a single equation Limited-Information Maximum Likelihood [====] test, denoted the ==== test, that imposes the structure under the null and alternative hypotheses.====Projections based on the ==== test admit an analytical solution which provides a well-known benchmark. In this case, results reveal that the numerical PSO solution is reached with remarkable speed and perfectly matches its analytical counterpart. Projections based on the remaining statistics are compared against a grid search method, as well as SA and GA. Overall, we find that PSO is at least as accurate as existing tools and generally converges faster.====Second, the paper contributes to the literature on the New Keynesian model. We analyze a three-equation New Keynesian model with nine parameters using U.S. data from Dufour et al. (2013). Results can be summarized as follows. The model treated as DSGE is rejected using the ==== test with both pre and post-Volcker data and all optimizers. What differs is convergence time: PSO and GA converge markedly faster that SA, which we also observe with the ==== statistic. Using the latter test, the three solvers yield differing solutions. In this case, and on comparing the optima we reach via PSO to their counterparts, we find that GA and SA as well as the considered grid may fail to escape local optima. Furthermore, the confidence limits to which PSO converges suggest severe under-identification, at least in the post-Volcker subsample. In sharp contrast to the bounded ==== based intervals, the ==== intervals corroborate identification failure. This provides a useful special case to illustrate the differences between a confidence interval that embeds a misspecification check and one that imposes the structure.====Although conditional on the considered statistics as well as IVs and structures, our findings with PSO are thus not unexpected. Several fundamental factors that may compromise identification in this context are well documented by now.==== Furthermore, structural stability issues have long been reported; see e.g. Benati (2008), on the New Keynesian Phillips Curve [====, and (Mavroeidis, 2010), on the Taylor rule. These issues combined complicate the projection exercises and justify our focus in this exercise, which is mainly numerical.====Indeed, more important than our specific under-identification result is the consequence of failing to detect its extent via popular tool kits such as GA and SA. With the ==== statistic, GA and SA converge to local optima that suggest misleading economic decisions on the nature of the NKPC, determinacy of monetary policies and the persistence of the Taylor rule. The grid search may also yield equally disappointing optima. This exercise is repeated with a simulated data set; we find qualitatively similar results knowing the data generating process. These findings suggest that far more attention needs to be paid to numerical precision as test inversion gains popularity in applied econometrics.====The paper is organized as follows. In Section 2, we introduce PSO algorithm. In Section 3, we describe the test inversion and projection method. In Section 4, we report our numerical analysis and conclude in Section 5.",Projection-based inference with particle swarm optimization,https://www.sciencedirect.com/science/article/pii/S0165188921000737,30 April 2021,2021,Research Article,211.0
"Castle Jennifer L.,Kurita Takamitsu","Magdalen College and Climate Econometrics, University of Oxford, UK,Faculty of Economics, Fukuoka University, Japan","Received 3 August 2020, Revised 13 April 2021, Accepted 15 April 2021, Available online 29 April 2021, Version of Record 22 May 2021.",https://doi.org/10.1016/j.jedc.2021.104139,Cited by (3),We employ a newly-developed partial cointegration system allowing for level shifts to examine whether economic fundamentals form the long-run determinants of the dollar-pound exchange rate over a recent period characterised by structural breaks and policy regime shifts. The paper models both long-run and short-run dynamic features of the exchange rate using a set of economic variables that explicitly reflect quantitative ==== and the influence of a forward exchange market. Out-of-sample forecasts comparing the model with economic fundamentals to benchmarks including the random walk indicate that fundamentals can help at short horizons but less so at longer horizons.,"Relationships between foreign exchange rates and economic fundamentals have been thoroughly investigated in the international economics and finance literature since Meese, Rogoff, 1983b, Meese, Rogoff, 1988 but there is still no consensus with regard to deviations of observed exchange rates from their fundamental-based values, see e.g., Engel and West (2005), Sarno and Sojli (2009), Bacchetta and Wincoop (2013) and Balke et al. (2013). Applied research has found weak support for the view that macroeconomic variables are critical factors in exchange rate fluctuations; see Baxter (1994), Isaac and De Mel (2001), Cheung et al. (2005), and Sarno (2005), inter alia.====This paper addresses the role of economic fundamentals in exchange rate determination by adopting a class of novel econometric techniques that can explicitly model structural breaks and by selecting those variables that explicitly reflect unconventional monetary policy and influences of a forward market. We focus on modeling the dollar-pound exchange rate but the methodology is general. The paper develops a cointegrated vector autoregressive (CVAR) system for processes integrated of order 1 (denoted ====), pioneered by Johansen, 1988, Johansen, 1996, which provides a basis for a feasible general-to-specific modeling scheme applicable to multivariate non-stationary time series data. See Hunter et al. (2017) for various recent developments in CVAR-based methods.====There have been various CVAR analyses conducted in the exchange rate literature, such as Johansen and Juselius (1992), Juselius and Donald (2004), Kurita (2007), Juselius and Assenmacher (2017) and Juselius and Stillwagon (2018). In this paper we make four novel contributions. First, a classical theory in international economics is adapted in such a manner that it directly reflects quantitative monetary policy (see Joyce et al. (2012)) adopted by the central banks in two countries (UK and US) and some other aspects of international financial markets. More specifically, both countries’ monetary bases are incorporated into a theoretical model proposed by Frankel (1979), along with an expectation formation based on a foreign-exchange forward premium. Second, the empirical investigation commences with a partial CVAR (PCVAR) system with level shifts, a member of a new class of econometric models introduced by Kurita and Nielsen (2019). This avoids an intractable large-scale econometric system subject to a number of problems such as vanishing degrees of freedom, and includes a structural break caused by the financial crisis and subsequent global recession starting in September 2008. The PCVAR system reveals the underlying long-run relationships between the two countries. It is further reduced to a trivariate vector equilibrium correction model (VECM) by exploiting revealed evidence for the existence of additional weakly exogenous variables.====Having developed the system for the dollar-pound exchange rate, we then focus on the exchange rate equation in the system and apply impulse-indicator saturation (IIS: Hendry et al. (2008) and Johansen and Nielsen (2009)) to test for breaks and location shifts. This is particularly beneficial towards the end of sample when the Brexit referendum led to two large falls in the spot exchange rate in close succession. Finally, the data sample is extended and we evaluate the model based on forecast performance for different horizons over 2018–2020. The out-of-sample forecasting exercise shows that economic fundamentals do help to forecast at short horizons but the random walk is hard to beat at longer horizons, in line with Meese and Rogoff (1983a). Our results also align with Engel and West (2005) who use an asset pricing model to argue that the exchange rate can exhibit close to random walk behaviour if a fundamental variable has a unit root and the discount factor is near unity.====The modeling approach we use embeds testable theory relationships within an empirical model. If the model satisfies all assumptions needed for valid inference (i.e. is congruent) then the tests of theory relationships are valid. Therefore, we start by outlining the underlying theory relationships in the empirical model. Section 2 re-visits a classical exchange-rate theory to obtain a set of candidate variables for long-run economic relationships underlying the UK and US, with Section 3 outlining the econometric methods used in the empirical study. Section 4 provides an overview of the data. The empirical analysis is given in Section 5, where a level-shift PCVAR system is employed to reveal the long-run economic relationships, and the system is reduced using general-to-specific principles. The system-based analysis paves the way for a single-equation analysis in Section 6, where we focus on achieving a congruent model for the dollar-pound rate using saturation methods. Section 7 conducts the forecasting exercise for the spot exchange rate, and finally Section 8 concludes.",A dynamic econometric analysis of the dollar-pound exchange rate in an era of structural breaks and policy regime shifts,https://www.sciencedirect.com/science/article/pii/S0165188921000749,29 April 2021,2021,Research Article,212.0
"Torri Gabriele,Giacometti Rosella,Tichý Tomáš","Department of Economics, University of Bergamo. Via dei Caniana, 2, 24127 Bergamo, Italy,Department of Finance, VŠB-TU Ostrava. Sokolská třída 33, Ostrava, 701 21, Czech Republic","Received 20 July 2020, Revised 2 April 2021, Accepted 7 April 2021, Available online 27 April 2021, Version of Record 13 May 2021.",https://doi.org/10.1016/j.jedc.2021.104125,Cited by (11)," and the proposed framework can be considered as a network extension of the ==== approach by Adrian and Brunnermeier (2016). From the conditional tail risk networks we can then compute synthetic indices of ==== for each bank. An additional set of systemic risk indicators is computed by considering together the network of conditional tail risk and bank-specific indicators of credit risk (as an example we use the ratio of non-performing loans, NPL). The empirical analysis focuses on the European banking system and considers a panel of 36 representative banks. Among the main findings, we found evidence of regional clusters of interconnected banks, especially in crisis period. Moreover, in terms of interconnectedness alone, systemic risk is diffused relatively evenly across European banks, while the set of systemic indicators built using also NPL highlighted a concentration of risk in southern European countries.","Since the global financial crisis in 2008, systemic risk is a main concern for regulators and academics. Although a common definition is still missing (see e.g. Montagna, Torri, Covi, 2020, Schwarcz, 2008 for an extended discussion and recent developments), most of authors agree that interconnection is a major determinant of systemic risk.====Indeed, a relevant strand of literature studies systemic risk using a network approach, modeling institutions as nodes and their relations as edges. Understanding the interconnectivity structure of a financial system is of paramount importance for regulators and analysts interested in understanding the determinants of risk in the system, or monitoring how crises can unfold. Network models can shed light on such mechanisms and in recent years several contributions in the literature proposed to use a network perspective, extending the toolbox available to regulators to measure and prevent systemic risk. Within this literature, a first group of works develop micro-structural contagion models that allow us to study directly the complex dynamics of a banking crisis considering the balance sheet composition of the institutions and the actual credit exposures among them. Such approaches rely on the accurate modelization of banks’ actions and reactions to financial shocks. They are very powerful and flexible, allowing to estimate the systemic risk stemming from solvency contagion Eisenberg and NOE (2001); Elsinger et al. (2006), liquidity crises (Gai et al., 2011), fire sales dynamics Greenwood et al. (2015), or a combination of multiple determinants (Montagna et al., 2020). However, the main drawback of these models is that they require granular datasets that are difficult to obtain, even for regulators.==== It is therefore relevant to develop models that study systemic risk and interconnectedness relying on publicly available data. A relevant class of models focuses on the estimation of network structure using the price of listed securities. Such market based models are built on the assumption that market prices carry relevant information on the interconnectivity of the institutions that issue these securities, and use a wide array of statistical techniques. They have an increasing number of applications, also thanks to their relatively easy implementation and minimal data requirements.====In these frameworks banks are modeled as nodes in the network, and edges are constructed considering statistical indices. The technique used to estimate the network depends on the goal of the analysis and the characteristics of the data. Among the most relevant approaches, Billio et al. (2012) rely on the concept of Granger causality and, using equity data, construct networks in which the presence of an edge between node ==== and node ==== denotes that ==== is a useful predictor of the future value of ====. Diebold and Yılmaz (2014) use variance decomposition techniques to assess the interconnectedness of equity volatilies obtaining directed network that reflect the potential channels for the synchronous spillover of risk. By contrast, Puliga et al. (2014) use Pearson correlation to estimate the network structure of the US and European financial sector using credit default swap data, obtaining undirected networks that describe the comovement of spreads. Anufriev and Panchenko (2015) and Torri et al. (2018) also focus on correlations, but instead of considering direct correlations, they use partial correlations, which allow to estimate the comovement of the equity returns of two institutions, conditional to the effect of all the other institutions in the system. Constantin et al. (2018) consider the tail dependence of the comovement of banks’ equity prices, constructing networks using multivariate extreme-value theory, in which each edge is computed based on the extremal dependence of the two connected nodes.====The models focus on different properties of the time series and are often used together to characterize more comprehensively a financial system. Among the main applications of market-based network models we can cite the identification of systemically relevant or systemically fragile institutions in a system using network centrality measures (Brownlees, Nualart, Sun, 2018, Diebold, Yılmaz, 2014), the characterization of crisis periods (Billio, Getmansky, Lo, Pelizzon, 2012, Deev, Lyocsa, 2020), the set up of early warning systems for financial crises (Constantin et al., 2018), and the implementation of stress test procedures to estimate the resilience of the system to shocks (Puliga et al., 2014). finally, market-based networks may provide information on the bilateral credit exposures among institutions in the absence of granular data, as shown by Abbassi et al. (2017), who establish that partial correlation networks strongly reflects the exposures of banks towards the market.====In addition to interconnectedness, several authors relate systemic risk to tail events and, more specifically, on the co-occurrance of tail events. Joint tails of equity returns’ distribution are the focus of a family of systemic risk measures constructed on conditional tail risk: among the most relevant, ==== measures the Value at Risk (====) of the system conditional on a particular bank or financial institution being distressed and ==== compares the ==== to the ==== in a non distressed situation (see Adrian and Brunnermeier, 2016). Related to these measures, Marginal Expected Shortfall (MES) measures the marginal constribution of each institution to the expected shortfall of the system (Acharya et al., 2012), and SRISK provides a prediction of the level of capital shortfall of an institution based on their MES, leverage and regulatory capital requirements (Brownlees and Engle, 2016).====In this work we propose a new methodology to assess tail risk interconnectedness using quantile graphical models (see Ali, Kolter, Tibshirani, 2016, Belloni, Chen, Chernozhukov, 2016), constructing networks based on conditional tail risk interdependence. We then use such networks to build indicators aimed at assessing systemic relevance and systemic fragility of individual banks in the European banking system. The proposed methodology exploits information available from market data, requiring minimal distributional assumptions, and the derived tail risk indicators can be integrated with other bank-specific risk ones. We also develop a novel estimation procedure based on SCAD-penalized quantile regression (Fan et al., 2014).====From a methodological standpoint, our work contributes to the literature by extending ==== and ==== into a network dimension, discussing their properties under parametric and non-parametric settings, highlighting the relation with partial correlation networks and addressing their estimation using penalized quantile regression with SCAD penalty.====Our methodology aims at providing regulators a more complete view on systemic risk in order to implement more effective policies, complementary to other approaches. Overall, the analysis highlights some structural features of the European banking system, especially the role of conditional tail risk as a channel of transmission of financial distress.====The paper is structured as follows: Section 2 introduces quantile graphical models, Section 3 discusses the applications to systemic risk analysis, introducing network ==== and systemic relevance/fragility indicators, Section 4 describes the estimation procedure of ==== Section 5 presents the empirical application to the European banking system and Section 6 concludes.",Network tail risk estimation in the European banking system,https://www.sciencedirect.com/science/article/pii/S0165188921000609,27 April 2021,2021,Research Article,213.0
"Hochmuth Brigitte,Kohlbrecher Britta,Merkl Christian,Gartner Hermann","Friedrich-Alexander Universität Erlangen-Nürnberg (FAU), Germany,Institute for Advances Studies Vienna (IHS), Austria,IZA, Germany,Institute for Employment Research (IAB), Germany","Received 24 November 2020, Revised 2 March 2021, Accepted 18 March 2021, Available online 27 April 2021, Version of Record 27 April 2021.",https://doi.org/10.1016/j.jedc.2021.104114,Cited by (17),"This paper proposes a new approach to evaluate the macroeconomic effects of the “Hartz IV” reform, which reduced the generosity of long-term unemployment benefits. We propose a model with different unemployment durations, where the reform initiates both a partial effect and an equilibrium effect. We estimate the relative importance of these two effects and the size of the partial effect based on the IAB Job Vacancy Survey. Our approach does not hinge on an external source for the decline in the replacement rate for long-term unemployed. We find that Hartz IV was a major driver for the decline of Germany’s steady state unemployment and that partial and equilibrium effect were of equal importance. In addition, we provide direct empirical evidence on labor selection, one potential dimension of recruiting intensity.","Since the implementation of an unemployment benefits reform in the year 2005 (dubbed as “Hartz IV”), which reduced the average benefit replacement rate for long-term unemployed, the German unemployment rate has declined from 12% in 2005 to 5% in 2019 (see Fig. 1). The existing literature has not yet reached a consensus on how much the Hartz IV reform has contributed to this decline. On the one hand, there are microeconometric studies (in particular, Price (2019) that show a significant increase of the unemployment-to-employment transition rate for those close to the expiration of short-term unemployment benefits. However, these studies remain silent about the potential equilibrium effects of the reform. On the other hand, quantitative results in macroeconomic models of the labor market depend strongly on the assumed reduction of the replacement rate for long-term unemployed. However, because of the complex structure of the unemployment benefit system, there is no consensus on the overall reduction of the replacement rate (see Section 2). Due to different assigned values for the reduction of the replacement rate, the quantitative unemployment effects in existing macroeconomic studies (Krause and Uhlig, 2012, Krebs, Scheffel, 2013, Launov, Wälde, 2013) range from close to zero to a 2.8 percentage point reduction of steady state unemployment.====Our paper offers a solution to this disagreement. We propose a labor market model with a rich unemployment duration structure. In our model, hiring is a two-stage process where firms have to establish a contact and select a certain fraction of workers. A benefit reform triggers two effects. First, it stimulates vacancy posting, thereby increases market tightness and aggregate contacts between workers and firms. We call this the equilibrium effect. Second, firms select a larger fraction of workers (i.e. they are less selective), which we call the partial effect. We can measure this worker-firm-level effect directly in the data. For this purpose, we propose a new proxy for the selectivity of firms based on the IAB Job Vacancy Survey and estimate the increase of the selection rate (share of selected suitable applicants) due to Hartz IV.==== In addition, we use the IAB Job Vacancy Survey to estimate the relative importance of partial and equilibrium effects over the business cycle, which we find to be equally important. The estimation results discipline the partial and the equilibrium effect in our macroeconomic model of the labor market. Our quantitative exercise yields a 2.1 percentage point decline of unemployment. This strategy replaces the conventional approach to assume a certain decline of the replacement rate, which is based on an exogenous source.====We validate our model along three dimensions (see Appendix C for details). First, the time series properties of our model are in line with the data in various dimensions. Second, our partial effects are similar to the causal microeconometric results from Price (2019). Third, there is supportive evidence for our simulated aggregate outcomes. The aggregate unemployment effects are in line with time-series evidence by Klein and Schiman (2021). In addition, consonant with descriptive evidence, the job-finding rate for short-term unemployed reacts more strongly than for long-term unemployed. Furthermore, the model-based Beveridge Curve movement in response to the Hartz IV reform is similar to the actual movement of the Beveridge Curve in the years after the reform.====Our model combines three ingredients: First, workers and firms meet randomly. Meetings are driven by a Cobb-Douglas constant returns contact function (Mortensen and Pissarides, 1994).==== Second, upon meeting, only a certain fraction of contacts is hired. All meetings draw an idiosyncratic training cost shock from a stable density function (see Chugh and Merkl, 2016 or Sedláček, 2014). Firms only select workers with an expected positive present value. Third, as Hartz IV was a reform of long-term unemployment benefits, we model a detailed deterministic unemployment structure. In our model, less generous long-term unemployment benefits trigger two effects. First, vacancy posting is stimulated (due to lower negotiated wages) and thereby the probability of making a contact with a firm increases for all workers. This equilibrium effect would not be measured in microeconometric studies. Second, less generous long-term unemployment benefits will affect the surplus of employment relative to unemployment. Due to this higher joint surplus, a larger fraction of applicants will be selected by firms upon meeting (partial effect).====Our paper contributes to the existing literature in several dimensions. First, we complement the microeconomic literature on unemployment benefits reform and estimate the partial effect at the firm level. For our empirical exercise, we use the IAB Job Vacancy Survey, which is a representative survey among up to 14,000 establishments. We find that the average share of applicants that firms selected after the reform increased by 11% when controlling for business cycle effects. The estimated increase of the selection rate is similar at different aggregation levels and when controlling for composition of the unemployment pool. This establishes the partial effect of an unemployment benefit reform at the firm level.==== Reassuringly, our quantitative results for the partial effect are in a similar order of magnitude as the ones by Price (2019), who causally estimates the partial effect of the reform at the worker level based on individual administrative data (see Appendix C.2 for details).====Second, we contribute to the literature on the relative importance of individual (partial) and equilibrium effects of unemployment benefit reforms. Similar to Hagedorn et al. (2019) and Karahan et al. (2019), our framework allows us to decompose the job-finding rate into a market-level (equilibrium) and an individual level (partial) effect:====The probability of finding a job is the product of the contact rate, ====, which depends on the aggregate labor market tightness, ====, and the selection rate, ====, which is determined at the worker-firm level. The contact rate represents the equilibrium effect, as it varies with aggregate market tightness. Upon contact, the selectivity (share of selected workers) matters. As this is a decision at the worker-firm level, which happens independently of aggregate market tightness movements,==== we refer to the latter as partial effect. We estimate the relative size of partial and equilibrium effects based on business cycle fluctuations. We find that these two effects are equally important.====Our methodology to determine the relative size of the partial and the equilibrium effect is completely different and thereby complementary to Chodorow-Reich et al. (2019), Hagedorn et al. (2019), and Karahan et al. (2019). While these authors identify the macro effects based on data revisions, a county-border discontinuity, and an unexpected cut in the duration of unemployment insurance, we use the firm-level variation of the selection rate over the business cycle to identify the relative importance of the equilibrium effect. One additional key difference is that we evaluate the macroeconomic implications of a permanent reform of long-term benefits, while they evaluate temporary changes of benefits that were triggered during the Great Recession.====We further contribute to the literature on the effects of the German Hartz IV reform. In contrast to the existing literature on the macroeconomic labor market effects of the Hartz IV reform (Krause and Uhlig, 2012, Krebs, Scheffel, 2013, Launov, Wälde, 2013), we do not use an exogenous source to determine the decline of the replacement rate on the job-finding rate. By contrast, we choose the decline of the replacement rate endogenously to target the partial effects from our estimation. One strength of this approach is that it determines the estimated partial effect independently of the chosen bargaining regime (individual bargaining vs. collective bargaining).====Our approach is highly complementary to a recent paper by Hartung et al. (2020). While these authors put particular emphasis on the role of the separation rate, our paper proposes a new approach how to pin down the increase of the job-finding rate due to Hartz IV. Their paper and our paper both find that the Hartz IV reform played an important role for the decline of German unemployment.====Finally, our paper looks into the black box of recruiting intensity for Germany. Davis et al. (2013) and Gavazza et al. (2018) argue (in the context of the Great Recession for the United States) that firms use other channels beyond vacancy posting. However, these channels are difficult to pin down for the United States due to a lack of suitable micro datasets. Our empirical exercise shows that the selection rate in Germany is strongly procyclical over the business cycle and drives one half of the fluctuations of the job-finding rate over the business cycle. Thus, our paper identifies one important dimension of recruiting intensity: firms use time-varying hiring standards. Our time series evidence complements existing cross-sectional evidence on firms’ hiring standards and, thus, closes an important research gap. Based on the Employment Opportunity Pilot Project (EOPP), Barron et al. (1985, p.50) document for the United States that “(...) most employment is the outcome of an employer selecting from a pool of job applicants (...).” More recently, Faberman et al. (2017) show – based on a supplement to the Survey of Consumer Expectations – that only a fraction of worker-firm contacts translate to job offers.====The rest of the paper proceeds as follows. Section 2 briefly outlines the institutional background on Hartz IV and the consequences for the replacement rate of different population groups. Section 3 derives a suitable search and matching model with labor selection, which allows us to look at the data in a structural way. Section 4 explains our identification strategy for the partial and equilibrium effects and provides empirical results. Section 5 explains the calibration strategy. Section 6 shows the aggregate partial and equilibrium effects of Hartz IV, performs several numerical exercises and puts the results in perspective to the existing literature. Section 7 concludes.",Hartz IV and the decline of German unemployment: A macroeconomic evaluation,https://www.sciencedirect.com/science/article/pii/S016518892100049X,27 April 2021,2021,Research Article,214.0
Liu Xiaochun,"Department of Economics, Finance and Legal Studies, Culverhouse College of Business, University of Alabama Tuscaloosa, AL, United States","Received 8 December 2020, Revised 9 March 2021, Accepted 28 March 2021, Available online 26 April 2021, Version of Record 26 April 2021.",https://doi.org/10.1016/j.jedc.2021.104123,Cited by (0)," historical volatility dynamics, respectively, more than other policy shocks since the mid-1980s. The impact of a one-unit government spending level shock on output and ==== uncertainties is equivalent to the impact of about a half unit of a monetary policy volatility shock in the long run, or of about a quarter unit of a monetary policy level shock in the short run.","The recent literature shows that policy changes can lead to changes in macroeconomic uncertainty. Mumtaz, Theodoridis, 2020 find that monetary policy shocks increase the volatility of unemployment rate and inflation. Rother (2004) estimates a panel sample of the OECD countries and finds a destabilizing impact of activist fiscal policies on inflation volatility. In addition, the literature has documented that uncertainty about policy can cause business cycle fluctuations. Fernández-Villaverde et al. (2011) and Mumtaz and Zanetti (2013) find that an increase in monetary policy volatility triggers a fall in the real economy. Fernández-Villaverde et al. (2015) report that unexpected changes in fiscal volatility can have a sizable adverse effect on economic activity. The findings are important in that macroeconomic volatility is associated with risk premia and welfare cost of the economy.====This paper investigates to what extent policy shocks affect volatility dynamics of output and inflation, and the substitution effects of the shocks.==== In order to analyze policy-induced macroeconomic volatility dynamics, I derive explicit functions of macroeconomic volatility impulse responses to policy level and volatility shocks. Specifically, I identify policy level and volatility shocks through sign restrictions for government spending and monetary policy, from a quarterly four-variable SVAR model with time-varying coefficients and stochastic volatility that is included in mean equation. Time-varying coefficients capture changes in the structure of the economy, including changes in systematic monetary policy which may induce changes in macroeconomic volatility (Baele et al., 2015). The volatility-in-mean specification closely resembles the reduced-form of a DSGE model approximated to the third order around the steady state with stochastic volatility.====Essentially, stochastic volatility, as the volatility of level shocks in the SVAR model, allows for separating volatility shocks from level shocks (Fernández-Villaverde et al., 2011). A number of recent studies==== interpret the unexpected changes in the volatility of fiscal and monetary policy level shocks as a representation of unexpected variations in uncertainty about fiscal and monetary policies. Following those studies, this paper uses policy volatility shocks as shocks to uncertainty about government spending and monetary policy, i.e., the current or future policy stances or the effectiveness of short-run policies. Mumtaz and Zanetti (2013) find from a DSGE model that an increase in monetary policy uncertainty causes the nominal interest rate, inflation and output growth to fall. In addition, Mumtaz and Theodoridis (2018) find the detrimental and small effect of government spending uncertainty on output. Hence, I apply their findings as sign restrictions to identify government spending and monetary volatility shocks.====Furthermore, I analytically decompose historical and forecast error volatilities of macroeconomic variables conditional on each policy shock.==== Following Jurado et al. (2015), I use the forecast error volatility of macroeconomic variables as a model-based measure of macroeconomic uncertainty. To this end, policy-induced macroeconomic volatility dynamics can be analyzed over time and across different horizons. To the best of my knowledge, analyzing policy-induced macroeconomic volatility dynamics via volatility impulse responses and decompositions in a time-varying SVAR model with stochastic volatility is new to the macroeconomics literature.====The main findings are highlighted as follows. First, the paper finds an important transmission channel from government spending and monetary policy to output and inflation uncertainties. The results show that policy level and volatility shocks increase output and inflation volatilities in short, medium and long runs. Monetary policy shocks explain about 25–30% of output and inflation volatility dynamics on average, while government spending shocks cause less output and inflation uncertainties, i.e., less than 10% on average.====Second, policy shocks transmissions vary over time. A shock to uncertainty about monetary policy has been increasingly more important since the mid-1980s, explaining output volatility dynamics from about 9% in 1984Q3 to 41% in 2007Q1, and since then it has remained at a high level around 40%. In contrast, the relative importance of a monetary policy level shock in explaining output volatility dynamics is reduced on average from about 10% to about 2% after the mid-1980s. Moreover, the responses of output and inflation uncertainties to monetary policy shocks appear to be persistent and much stronger in the horizons longer than 2 years.====Last but not least, monetary policy shocks are more effective than government spending shocks in affecting output and inflation uncertainties, as their policy marginal substitution rates (PMSRs) are much less than 1. For instance, the impact of a one-unit spending level shock on output and inflation uncertainties can be substituted by the impact of about a quarter unit of a monetary policy level shock in the short run. By contrast, about a half unit of a monetary policy volatility shock is required in longer horizons to produce the same amount of changes in output and inflation uncertainties caused by a one-unit spending level shock.====Compared to the literature on the impact of uncertainty shocks (Fernández-Villaverde, Guerrón-Quintana, Rubio-Ramírez, Uribe, 2011, Fernández-Villaverde, Guerrón-Quintana, Rubio-Ramírez, 2015, Mumtaz, Zanetti, 2013), this paper highlights the importance of both policy level and volatility shocks. In addition, this paper attempts to model the transmission of policy shocks to economic volatility and thus takes a step towards treating economic volatility as endogenous.====The work closely related to this paper is the study of Mumtaz, Theodoridis, 2020. They estimate an SVAR model for the impact of monetary policy shocks on the volatility of unemployment rate and inflation. Nonetheless, this paper differs in several important dimensions. First, the present study estimates the macroeconomic effect of both policy level and uncertainty shocks, while Mumtaz, Theodoridis, 2020 investigate only the impact of the policy level shock. Second, their work considers only monetary policy shocks. Nonetheless, this paper identifies both government spending and monetary policy shocks, in order to take into account the finding of Rossi and Zubairy (2011) that failing to recognize the interaction between monetary and fiscal policies might empirically attribute macroeconomic fluctuations to wrong sources. Third, as discussed earlier, I find a time-varying shock transmission to macroeconomic volatility, such as the increased importance of a shock to uncertainty about monetary policy accompanied with a reduced importance of the monetary policy level shock since the mid-1980s. Such empirical dynamics are not revealed in Mumtaz, Theodoridis, 2020. Lastly, this paper has derived explicit functions of macroeconomic volatility impulse responses and decompositions for each policy shock, novel to the macroeconomics literature.====This paper is thus related to the volatility impulse response literature represented by Gallant et al. (1993), Lin (1997), Hafner and Herwartz (2006) and recently Liu (2018). These studies exclusively rely on GARCH models and statistical shocks. However, this paper has tackled several critical and common issues in the earlier studies. First, they either exclude VAR conditional mean or assume its coefficients time-invariant, missing the effect of changes in the structure of the economy on volatility impulse responses and decompositions. Second, GARCH volatility is not consistent with theoretical models of uncertainty (Alessandri, Mumtaz, 2019, Fernández-Villaverde, Guerrón-Quintana, Rubio-Ramírez, Uribe, 2011, Mumtaz, Zanetti, 2013). Lastly, their statistically identified shocks lack economic interpretations in a theoretically consistent manner.====This paper is also related to the time-varying SVAR literature. Most of studies in the literature, following Cogley and Sargent (2005) and Primiceri (2005), restrict contemporaneous effects to be recursive with a block diagonal covariance assumption. However, I relax those restrictions by reparameterizing the SVAR model as in Canova and Pérez Forero (2015), such that the structural framework is more flexible in that zero and sign restrictions can be handled in a theoretically consistent manner for recursive and nonrecursive structural models.====The rest of this paper is organized as follows. Section 2 specifies a time-varying SVAR model for four quarterly macroeconomic variables. Section 3 identifies the model. Section 4 derives explicit functions of volatility impulse responses to level and volatility shocks, and further decomposes historical and forecast error volatilities into components associated with each shock. Section 5 briefly describes Bayesian inference. Section 6 reports empirical results. Section 7 concludes the paper.",On fiscal and monetary policy-induced macroeconomic volatility dynamics,https://www.sciencedirect.com/science/article/pii/S0165188921000580,26 April 2021,2021,Research Article,215.0
"Drautzburg Thorsten,Fernández-Villaverde Jesús,Guerrón-Quintana Pablo","Federal Reserve Bank of Philadelphia, USA,University of Pennsylvania, USA,Boston College, USA","Received 11 November 2020, Revised 30 March 2021, Accepted 30 March 2021, Available online 26 April 2021, Version of Record 26 April 2021.",https://doi.org/10.1016/j.jedc.2021.104121,Cited by (0),"We argue that social and political risk causes significant aggregate fluctuations by changing workers’ bargaining power. Using a ==== proxy-VAR estimated with U.S. data, we show how distribution shocks trigger output and unemployment movements. To quantify the aggregate importance of these distribution shocks, we extend an otherwise standard neoclassical growth economy. We model distribution shocks as exogenous changes in workers’ bargaining power in a labor market with search and matching. We calibrate our economy to the U.S. corporate non-financial business sector, and we back out the evolution of workers’ bargaining power. We show how the estimated shocks agree with the historical narrative evidence. We document that bargaining shocks account for 28% of aggregate fluctuations and have a welfare cost of 2.4% in consumption units.","In this paper, we argue that the social and political distribution risk between labor and capital is a quantitatively relevant source of aggregate fluctuations. To do so, we begin by documenting political distribution risk in the data, as one specific form of distribution risk. We use three empirical exercises. First, using the volatility of capital shares, we document considerable changes in how income was divided between capital and labor in France, the U.K., and the U.S. over the last century and a half. Second, we analyze the consequences for distribution of the adoption of right-to-work legislation by several U.S. states. We estimate that the introduction of right-to-work legislation in a state is followed, on average, by increases in the state capital share of 1.5–1.6 percentage points (pp.) relative to the U.S. five years after adoption. Third, we focus on the U.S. by estimating a Bayesian proxy-vector autoregression (VAR). We proxy the redistributive shock by legislated changes in the federal and state-level minimum wages. In different specifications, we document the significant effects of these distribution shocks on output, factor shares, and labor markets.====This evidence motivates us to augment a standard stochastic neoclassical growth model with labor search and matching à la Shimer (2010) with shocks to the bargaining power of workers. These shocks are a simple way to capture a central mechanism through which distribution risk operates. Formally, bargaining shocks can be interpreted as arising from social or political influences on the protocol of an underlying dynamic bargaining game (Binmore et al., 1986) through mechanisms such as collective bargaining rules, minimum wage regulations, etc. In our model, this risk is separate from changes due to endogenous movements in the bargaining position of workers and firms, since those movements are reflected in the outside values of the agents.====We identify our model by exploiting the differences in the responses of output and wages to shocks to productivity and the bargaining power of workers. After both a positive shock to productivity and a shock that lowers the bargaining power of workers, output grows. However, wages rise if the former shock occurs, but fall if the latter shock hits the economy. Hence, looking at the comovements of output and wages disentangles one shock from another.====With this identification, we calibrate our model to the U.S. non-financial corporate business sector and the labor market. As a baseline, we look at long-lived bargaining shocks with a half-life of 34 quarters. Thus, our approach deals not only with standard business cycles, but also with medium-term aggregate fluctuations. The half-life of 34 quarters is based on our reading of the changes in the social and political climate regarding the bargaining power of workers in the postwar U.S. and matches the average duration of control of the different branches of the federal government by each party after World War II.====We use U.S. data to back out the bargaining shocks implied by our quantitative model. We do so by applying the ==== recently introduced by Drautzburg et al. (2021). The central idea of the partial filter is to take one of the key optimality conditions of the model (in our case, the wage-setting equation), substitute the different conditional expectations of a product of variables by their conditional covariance plus the product of conditional expectations of single variables, and approximate those conditional covariances and expectations of single variables from a Bayesian VAR (BVAR). Crucially, the backed-out shocks agree with our narrative evidence for the U.S. since WWII, with peaks at moments of significant labor union victories (e.g., the 1970 GM strike) and troughs at moments of weakness of unions (e.g., the early 2000s).====As a robustness check, we explore the behavior of the model with even more persistent bargaining shocks (half-life of 80 quarters) and with less persistent ones (half-life of 14 quarters, a standard business cycle persistence). The qualitative dynamic properties of the model are not significantly affected by this persistence. However, we find that the higher the persistence of bargaining power shocks, the more significant their impact on real fluctuations and the smaller their impact on the income distribution.====We solve our model using a third-order perturbation since we document how the non-linear features of the solution can be significant. These non-linearities also mean that we must calibrate the model to match the moments of its ergodic distribution and not its steady-state properties. To assess the properties of our economy, we compare it with a version of the model without bargaining shocks, with a benchmark real business cycle (RBC) model with productivity shocks, and with an RBC model augmented with factor share shocks in the production function (as in Danthine, Donalson, Siconolfi, 2006, Ríos-Rull, Santaeulàlia-Llopis, 2010, and Lansing, 2015).====Other significant findings of the paper are as follows. First, our model replicates the near acyclicality of wages highlighted by Lucas (1977) as an obstacle for equilibrium business cycle models that want to rely on movements in real wages as a source of fluctuations. In our economy, output can increase either because productivity grows, which raises wages, or because bargaining power shifts toward capital, which lowers wages. This finding allows us to discriminate between models. An RBC model with factor share shocks yields wages that are too pro-cyclical: a shock toward labor makes it more productive and, thus, raises wages.====Second, our model accounts reasonably well for the pro-cyclicality of the capital share and the net capital share (i.e., after depreciation). This stands in stark contrast to the version of the model without bargaining shocks and the RBC models (with and without factor share shocks). This result is robust to reasonable values of the elasticity of substitution between capital and labor in the production function.====Third, the bargaining shocks account for around 28% of the volatility of output, nearly all of the model-generated volatility of the gross capital share, and around 45% of the net capital share. When the model is calibrated to match observations from the U.S. labor market, the surplus of the labor relation is small. Minor variations in how this surplus is allocated induce substantial changes in the number of recruiters firms employ to hire new workers. This leads to lower output and employment but also higher wages. We illustrate this point as follows. The U.S. has benefited from a more stable capital share than other industrialized countries. For example, the overall volatility of the capital share is about 40% lower than in the U.K. If increased distribution risk would cause the capital share to become 40% more volatile, our model predicts that output and consumption volatility would be 15% higher. The welfare cost of bargaining shocks is a sizable 2.4% of consumption, much larger than in Lucas (1991).====Finally, we look at the dynamic effects of a bargaining power shock, and, in an Appendix, we perform an extensive battery of robustness exercises. We document, for example, how our main results are not affected when we partly endogenize the bargaining power process by having bargaining power or unemployment benefits depend directly on the business cycle. Nonetheless, the endogeneity of the bargaining shock (and how it reacts to issues such as technological change, globalization, inequality, and others) is a topic that deserves much further exploration than we can cover in this paper.====Our paper builds on a large literature. The recent evolution of the capital share has commanded much attention (e.g., among many others, Autor, Dorn, Katz, Patterson, Reenen, 2017, Barkai, 2017, Berger, Herkenhoff, Mongey, 2019, De Loecker, Eeckhout, 2017, Karabarbounis, Neiman, 2014, Koh, Santaeulàlia-Llopis, Zheng, 2015). Some of the proposed explanations highlight technological change, the fall in the relative price of capital, increases in firm concentration, globalization, or the role of intellectual property products. For our investigation, we can remain agnostic about these mechanisms. Our point is not that all fluctuations in the capital share have a social or political origin: we only claim that part of them do. Also, we focus on fluctuations around a trend rather than on the trend (although we perform some high-persistence exercises). One should expect that the effects of technological change, increased market power, or structural transformation on factor shares would manifest themselves more clearly in the trend than in middle- and high-frequency movements.====Previous work has also focused on how changes in bargaining power affect factor shares. Examples include Blanchard (1997), Caballero and Hammour (1998), and Blanchard and Giavazzi (2003). The papers cited above are concerned with the trend decline in the labor share in Europe, whereas we focus on aggregate fluctuations. Gertler et al. (2008) and Liu et al. (2013) also allow, in passing, for time-varying bargaining power, but they do not study its implications in full. Foroni et al. (2017) have presented VAR evidence of the importance of bargaining supply shocks in employment fluctuations. Ríos-Rull and Santaeulàlia-Llopis (2010) and Danthine et al. (2006) interpret distribution shocks as technological shocks to the production function. As we will see later, our model outperforms an RBC model with factor share shocks in terms of matching important aggregate fluctuation statistics.====Our bargaining shocks resemble wage markup shocks such as those in Galí et al. (2012). There are some important differences, though. First, our model endogenously generates the equivalent to state-dependent markups over disutility from labor because the surplus of the match is time-varying. Second, we document the importance of higher-order effects related to labor market tightness. These effects are absent in models with markup shocks. Third, we link our shocks to a precise historical narrative. For instance, our partial information filter points out that, from 1950 to the late 1970s, the evolution of bargaining power mostly followed the fate of unions (except for the Kennedy-Johnson wage-posting guidelines). After 1980, changes such as Reagan’s regulation, Clinton’s welfare reform, immigration, and labor market policies such as minimum wages and unemployment extension played a bigger role.====We also link with papers dealing with wage bargaining and aggregate fluctuations. This literature is too large to do justice to it in a few lines, but we can highlight the textbook treatment in Shimer (2010) and the references there. Interestingly, Shimer (2005) pointed out the potential of bargaining power shocks for resolving the unemployment volatility puzzle, but emphasized the need for clear identification. Our paper focuses much attention on identification and the link between historical evidence and bargaining power shocks.====Finally, our work is closely related to Danthine and Donalson (2002), who study the link between financial and labor markets. Their focus is to understand how time-varying labor shares affect the equity premium. In particular, they show that exogenous and stochastic variation in labor shares faced by workers and firms is essential to account for asset prices. The high welfare cost of the business cycle we find is the dual of the high equity premium documented by these authors. Relative to Danthine and Donalson (2002), first, we provide empirical evidence of the impact of changing bargaining power on the macroeconomy; second, we characterize the bargaining protocol between workers and firms; and third, we analyze the economy’s dynamics, including unemployment following a shock to bargaining power. Also, we show the welfare implications of this bargaining process.====The rest of the paper is organized as follows. Section 2 reviews the historical evidence on the variation in factor shares and political interventions in the U.S. Section 3 presents and computes our model. We take the model to the data in Section 4. Section 5 explains our partial information filter. Section 6 presents the quantitative results and Section 7 reports the dynamic effects of bargaining power shocks. Section 8 concludes. An extensive online appendix discusses further details.",Bargaining shocks and aggregate fluctuations,https://www.sciencedirect.com/science/article/pii/S0165188921000567,26 April 2021,2021,Research Article,216.0
"Ellington Michael,Martin Chris,Wang Bingsong","Management School, University of Liverpool, Liverpool L69 7ZH UK,Department of Economics, University of Bath, Bath BA2 7AY UK,Department of Economics, University of Warwick, Coventry CV4 7AL, UK","Received 14 August 2020, Revised 4 December 2020, Accepted 16 March 2021, Available online 21 April 2021, Version of Record 6 May 2021.",https://doi.org/10.1016/j.jedc.2021.104104,Cited by (1),"This paper puts Search Frictions models under novel empirical scrutiny. To capture changing dynamics, we fit a ==== time-varying parameter VAR to US labour market data from 1965–2016. Using a ==== with Search Frictions, we identify several structural shocks, including a shock to worker bargaining power that we name a wage shock. We argue that the wage shock is a key driver of cyclical variation, explaining a higher proportion of the variation of these variables than productivity, demand or job separation shocks. We also document stark differences between empirical and theoretical ==== that cast doubt on the core transmission mechanism of search and matching models.","Macroeconomic models of the labour market look to explain cyclical, long-run, and secular relationships among key variables; namely unemployment, job vacancies and wages. Search Frictions models are the workhorse of modern labour economics (e.g. Diamond, 1982, Mortensen, Pissarides, 1994, Pissarides, 2000). This framework examines the incentives of firms to post vacancies, how unemployed workers find a job match, and the resulting wage of a successful job match. They provide an explanation for the underlying structural dynamics of the labour market, and historically, are successful in assessing the welfare implications of labour market policies. Their success stems from their ability to match key empirical regularities in the data, such as the negative link between unemployment and vacancies.====In this paper, we subject the Search Frictions framework of labour markets to novel empirical scrutiny. We have three main findings. First, we find evidence of important parameter change; this results in changes in impulse responses for key variables that are difficult to explain using standard theoretical models. Second, we find that a shock to the bargaining power of workers, which to date the literature overlooks, is an important driving force for the business cycle. Third, we argue that the key transmission mechanism implicit in the Search Frictions framework of the labour market, i.e. a strong response of vacancies to shocks, leading to volatile movements in unemployment across the business cycle, does not receive empirical support.====One of the main drawbacks of the Search Frictions framework is the presumption that structural relationships are constant over time. A growing empirical literature using models accounting for parameter and volatility variation within labour markets casts doubt on this (see e.g. Benati and Lubik (2014); Guglielminetti and Pouraghdam (2017); Mumtaz and Zanetti (2015))====. We extend on this literature by fitting a time-varying parameter VAR with stochastic volatility (TVP VAR) comprising of US data on productivity, real wages, vacancies, unemployment and inflation from 1965Q1–2016Q4====Following Benati (2014), we identify a non-stationary structural productivity shock that maximises the long-horizon covariance between productivity and real wages. We identify other strcutural shocks through robust sign restrictions, by applying the procedure of Canova and Paustian (2011) to a Dynamic Stochastic General Equilibrium model with Search Frictions (DSGE-SF) similar to Mumtaz and Zanetti (2012). Using this approach, we identify structural job destruction, demand and wage bargaining shocks. We use estimates from our structural TVP VAR to evaluate central features of Search Frictions models of the labour market. We question the focus in the literature on productivity shocks as the primary source of cyclical variation in vacancies and unemployment. We present forecast error variance decompositions from our TVP VAR which show that wage shocks explain a higher proportion of the variation in the vacancy rate and the unemployment rate than productivity shocks, demand shocks, and job separation shocks, throughout our estimation sample. In particular, we observe peaks in the percent share of forecast error variance attributable to wage shocks consistent with business cycle troughs while the proportion of variance associated with productivity shocks remains relatively stable throughout the sample.====In our next exercise, we examine the core transmission mechanisms of the Search Frictions approach by calibrating a DSGE-SF model to match, on average over the impulse horizon, estimates of the impulse response functions for unemployment with respect to job separations shocks, wage shocks, and demand shocks. If the model is correct, impulse response functions for vacancies and wages from our model calibrations should also match the corresponding empirical impulse response functions. We find that they do not. Specifically, in order to match the empirical response of the unemployment rate to shocks, the DSGE-SF model requires a large response in vacancies to generate a response of unemployment consistent with the data. In the case of demand and wage shocks, we do not observe this vacancy surge in the data. In addition, the DSGE-SF model suppresses the response of the wage to shocks in order to stimulate vacancy creation. However, the empirical response of wages to separations shocks is always much larger than the simulated response. Meanwhile the empirical response of wages to demand and wage shocks are much larger than the simulated response in the latter part of our sample. Taken together, these results cast further doubt on the empirical validity of Search Frictions models of the labour market====.====The structure of the remainder of this paper is as follows. Section 2 describes data and outlines the econometric model; Section 3 presents reduced form results. In Section 4, we outline our identification strategy, describe the DSGE-SF model we use to derive sign restrictions and present our structural estimates. Section 5 presents evidence on FEVD decompositions and on the fit between empirical and simulated impulse responses and draws conclusions from these. Section 6 concludes and outlines areas for future research.",Search Frictions and Evolving Labour Market Dynamics,https://www.sciencedirect.com/science/article/pii/S0165188921000397,21 April 2021,2021,Research Article,217.0
"Lou Youcheng,Strub Moris S.,Li Duan,Wang Shouyang","MDIS, Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Beijing 100190, China,Department of Information Systems and Management Engineering, Southern University of Science and Technology, Shenzhen 518055, China,School of Data Science, City University of Hong Kong, Hong Kong,Center for Forecasting Science, Chinese Academy of Sciences, Beijing 100190, China,School of Economics and Management, University of Chinese Academy of Sciences, Beijing 100190, China","Received 1 March 2020, Revised 15 January 2021, Accepted 25 March 2021, Available online 15 April 2021, Version of Record 10 May 2021.",https://doi.org/10.1016/j.jedc.2021.104120,Cited by (0),"We study the impact of a reference point determined by social comparison on ==== remains constant. On the other hand, if the reference point is determined solely by social interactions, then it is possible that the network simultaneously experiences high wealth growth and a reduction in ====. Finally, for the general case where the reference point incorporates both personal and social components and some extensions of the model, we numerically show that increasing the degree of social interactions is beneficial for both increasing wealth growth and reducing inequality.","In financial markets, investors’ decisions are often influenced not only by their individual experiences in the market and perceptions of the market opportunity, but also by other investors’ behavior and performance. This motivates us to formulate a behavioral portfolio choice model where the reference point contains two components: a personal component depending on the investor’s own wealth history and a social component depending on the wealth levels of other investors in her network. Our main objective is to study the impact of social interactions on wealth growth and inequality in such a network model.====Our model builds on key ingredients of the (cumulative) prospect theory of Kahneman and Tversky (1979) and Tversky and Kahneman (1992). Their pioneering work aims to capture investors’ psychology in decision-making, which is ignored by the classical expected utility theory. One of the main elements of prospect theory is that people evaluate their assets on gains and losses with respect to a reference point.==== We incorporate a reference-dependent and loss averse value function into a multi-investor, infinitely repeated portfolio choice model in an environment of social networks. All investors are assumed to be myopic==== in the sense that, at the beginning of each period, every investor maximizes an ====-shaped, reference dependent value function for the current time period in a pure-investment framework with exogenous asset prices based on her wealth level and her current reference point.==== At the end of each time period, investors adjust their reference points based on their own wealth levels as well as the wealth levels of the neighbors in their social network and proceed forward to make the optimal investment decision for the next period. This process repeats infinitely many times.====The most prominent feature of our model is that investors’ reference points contain a social component. The reference points impact each investor’s investment behavior and the investment outcome under this behavior in turn impacts investors’ reference point formation. The wealth dynamics and reference points hence evolve jointly and mutually depend on one another. We are interested in the impact of the reference point formation on the long run evolution of wealth growth and inequality as measured by the wealth gap and the Gini coefficient of the network.====Since our investors are myopic, we start with a single-period behavioral portfolio choice model for a single investor with an arbitrary reference point. Such a problem was investigated in greater generality for example in He and Zhou (2011a). We here do not take probability weighting into account in order to get more explicit results for a general position of the reference point. We find that the optimal investment strategy (OIS) is piecewise linear and increasing in the distance between current wealth and reference point. Besides obtaining the OIS, we are also able to determine conditions under which a loss averse investor with an ====-shaped utility function would long, respectively short a risky asset. Whereas a classical expected utility maximizer would always take a long position in a stock with a positive expected excess return, this is not necessarily the case if the utility function is not concave.====We then extend the single-period setting for a single-investor with an arbitrary reference point to a multi-period model for a social network of multiple investors who form their respective reference points as a convex combination of both personal and social components. We consider two possibilities for the personal component: Either the reference point is updated recursively as a function of the investor’s previous reference point and current wealth as was suggested in Arkes, Hirshleifer, Jiang, Lim, 2008, Arkes, Hirshleifer, Jiang, Lim, 2010, or the reference point is updated non-recursively taking the whole history of wealth levels into account as was proposed by Baucells et al. (2011). These models have already been incorporated into dynamic behavioral portfolio selection models, for example, the recursive updating rule was employed in Shi et al. (2015b) and He and Yang (2019), and the non-recursive updating rule in Strub and Li (2020). A reference point containing a social component on the other hand has not yet been studied in the existing literature on portfolio selection to the best of our knowledge, although there have been some empirical work in other fields, for example, Viglia and Abrate (2014) study the effect of social comparison on reference point formation in a service context. We propose that such a reference point satisfies the two properties of “Upward-Looking” - each investor’s reference point is never strictly lower than her own wealth - and “Moderation” - the difference between each investor’s wealth and her reference point, is in some sense proportional to the wealth gap to the richest person within her network. These properties are motivated by empirical evidence, see, e.g., Collins (1996), Duesenberry (1949), or Ferrer-i-Carbonell (2005).====We first consider the case where the reference point is solely determined by the personal component and there are no social interactions. This case serves as a benchmark for our later analysis. We find that, in this case, each investor’s wealth grows at a common rate and inequality as measured by the generalized Gini coefficient remains constant over time.====We then investigate in detail the case where the reference point is determined by social comparison alone, independent of each investor’s history of wealth. We first show that a positive wealth gap is necessary and sufficient for a sustained wealth growth in this case. We then consider a specific reference point formation rule, where the reference point is a convex combination of the individual investor’s wealth level and the maximum wealth level among all the members in her network. We call the weight given to the maximum wealth level in her network the coefficient of aspiration as it represents how strong the investors aspire to achieve high social status. We establish a threshold for the coefficient of aspiration which determines the long run behavior of the network and which can be computed in semi-closed form. If the coefficient of aspiration is above the threshold, each investor’s expected wealth grows indefinitely with a common growth rate and the expected wealth gap diverges. We find that it is possible to have both wealth growth and a reduction of inequality in the presence of social interactions. On the other hand, if the coefficient of aspiration is below the threshold, each investor’s expected wealth converges to a common finite level and the expected wealth gap disappears.====Finally, we consider a general reference point containing both personal and social components. We term the weight given to the social component the degree of social interactions and investigate numerically how it influences the long term dynamics of wealth growth and inequality. Our main finding is that, within our model, increasing the degree of social interactions typically is beneficial for achieving higher wealth growth and lower inequality simultaneously. In addition, we also consider several extensions of the model such as a no-bankruptcy constraint, heterogeneous risk attitude, and heterogeneous loss aversion, and find that the main finding is robust with regards to these extensions. The intuition behind this finding is as follows. Social interactions move the reference point upward from the benchmark case where the reference point does not include a social component. For an investor whose current wealth lies above the personal component of the reference point, taking social interactions into account thus typically leads to a reduction in the distance between wealth and reference point and, therefore, to smaller investments in the risky asset. These tend to be the richer investors of the network. Vice versa, for those investors whose wealth lies below their personal component of the reference point, taking the social component into account increases the distance between their wealth and the reference point and thus leads to higher investments in the risky asset. The social component thus tends to cause richer investors to reduce their appetite for risk because of a fear of loosing their position in the social network and falling below the reference point, while it simultaneously increases the risky investments of the poorer investors because they want to catch up with the richer ones. Because excess returns of the risky asset are positive on average, this leads to a reduction in inequality.====We remark that empirical evidence shows that richer investors typically invest more in stocks than poorer ones, and that this is a possible factor behind rising inequality, see, e.g., Fagereng et al. (2020). The reasons allowing richer investors to hold more stocks than poorer investors are manifold and integrating them into our model goes well beyond the scope of this paper. We stress that it is not our objective to build a model which replicates empirical wealth dynamics, but, rather, to study the effect of social interaction in isolation of other possible constraints on investors strategies.",The impact of a reference point determined by social comparison on wealth growth and inequality,https://www.sciencedirect.com/science/article/pii/S0165188921000555,15 April 2021,2021,Research Article,218.0
Cho Seonghoon,"School of Economics, Yonsei University, 50 Yonsei-ro, Seodaemun-gu, Seoul 03722, Republic of Korea","Received 14 September 2020, Revised 20 March 2021, Accepted 22 March 2021, Available online 29 March 2021, Version of Record 17 April 2021.",https://doi.org/10.1016/j.jedc.2021.104115,Cited by (3),"In a general class of Markov-switching rational expectations models, this study derives ==== for determinacy, indeterminacy and the case of no stable solution. Classification of the models into these three mutually disjoint and exhaustive subsets is completely characterized by only one particular solution known as the minimum of modulus solution in the mean-square stability sense. The rationale behind this result comes from the novel finding that the solution plays the same role as what the generalized eigenvalues do for linear rational expectations models. Moreover, the solution has its own identification condition that does not require examining the entire solution space. The accompanying solution procedure is therefore computationally efficient, and as tractable as standard solution methodologies for linear rational expectations models. The proposed methodology unveils several important implications for determinacy in the regime-switching framework that differ from the linear model counterpart.","The Great Moderation, a period of lower macroeconomic volatilities, and the Zero Lower Bound era are symbolic examples of some of the well-known economic regimes observed in the modern economy. A large number of papers have studied the underlying forces of those regime shifts and the transitional dynamics using the standard time series models shaped by the seminal work of Hamilton (1989). This approach has also been complemented by Markov-switching rational expectations (MSRE) models, which provide deeper economic accounts of recurrent regime shifts in terms of changes in the behavior of private agents and policymakers, as well as volatilities of structural shocks (See Bianchi, 2013, Davig, Leeper, 2007, Sims, Zha, 2006, Svensson, Williams, et al., 2008, and Baele et al., 2015 for instance). For the analysis of these economic models to be relevant, however, the existence and uniqueness of a stable rational expectations equilibrium – determinacy – must be clearly understood.====While there has been significant progress in the literature, necessary and sufficient conditions for determinacy have not been clearly identified for general regime-switching models. Moreover, different stability concepts lead to different determinacy results. Farmer et al. (2009) adopts mean-square stability and derives those conditions for purely forward-looking models only. Using the same concept of stability, Cho (2016) derives the conditions for general models including lagged endogenous variables, but they are only sufficient. Barthélemy and Marx (2019) also derive a necessary and sufficient condition for a unique bounded solution when models are purely forward-looking, and a sufficient condition when predetermined variables are present in the model. Using an alternative stability concept based on the Lyapunov spectrum, Neusser (2019) proposes a condition for determinacy, but it is only necessary.====The present work adopts mean-square stability as a stability concept among alternatives following Farmer et al. (2009), on the grounds of tractability of the methodology and feasibility of econometric inference. Then we provide a complete classification result for a general class of MSRE models, encompassing the linearized system of dynamic stochastic general equilibrium (DSGE) models with regime-switching structural parameters. Specifically, we derive both necessary and sufficient conditions for a unique stable solution (determinacy), multiple stable solutions (indeterminacy) and no stable solution. The classification of MSRE models – partitioning into those three mutually disjoint and exhaustive subsets – is fully characterized by only one particular rational expectations solution, which is one of the minimum state variable (MSV) solutions. This is a very powerful result given that these regime-switching models are inherently non-linear and the full set of solutions is too large to identify.====The novelty of our approach lies in the finding that the solution – referred to as the minimum of modulus (====) solution in the spirit of McCallum (2007) – to MSRE models plays an equivalent role as what generalized eigenvalues do for linear rational expectations (LRE) models. Specifically, it provides the conditions for the non-existence of stable sunspots and the uniqueness of a stable MSV (or fundamental) solution. These conditions are shown to completely classify all MSRE models. Another important feature is that there is a simple identification condition for the ==== solution, hence, the full set of solutions need not be computed for examining their stability. This is the key factor that makes our methodology including the solution technique as tractable as the standard methods for LRE models such as those of Blanchard and Kahn (1980), Uhlig (1997), Klein (2000) or Sims (2002). There exists, however, a subset of MSRE models for which the identification condition fails and in such cases, our solution procedure may not be implementable. Fortunately, it will be shown that the failure of the condition is equivalent to the statement that determinacy can never arise: a model is either indeterminate or it has no stable solution. Except for such cases, our proposed methodology is shown to be computationally very efficient.====Our methodology uncovers several new equilibrium properties of MSRE models. First, the parameter space implied by determinacy, indeterminacy and no stable solution is neither necessary nor sufficient for each of the LRE model counterparts. Second, a unique stable MSV solution does not imply determinacy in a regime-switching model with lagged endogenous variables. This is a general phenomenon arising from non-linearity of the model, highlighting that the non-existence of stable sunspots must also be examined to correctly identify determinacy. Third, the long-run Taylor principle of Davig and Leeper (2007), an important equilibrium characteristic in regime-switching models, is only necessary for determinacy in the mean-square stability sense. These findings are illustrated through a simple model allowing shifts in monetary and fiscal policy stances.====This study builds upon and complements previous research. Farmer et al. (2009) show that a purely forward-looking Markov-switching model is determinate if and only if there is no stable sunspot. Their approach to find all sunspots and examine their stability is, however, difficult to implement because the sunspot space is extremely large to identify. The uniqueness of a stable MSV solution must also be verified for models with lagged endogenous variables for determinacy. Farmer et al. (2011) and Maih (2015) propose efficient numerical solution techniques for computing some, but not necessarily all of MSV solutions. The Gröbner basis technique of Foerster et al. (2016) can identify the uniqueness of a stable MSV solution by computing all of the MSV solutions, but it is computationally very demanding even for modest dimensional models because the number of MSV solutions is surprisingly large. More importantly, the unique stable MSV solution alone does not imply determinacy as mentioned above. The determinacy and indeterminacy conditions proposed by Cho (2016) are very similar to ours, but they are only sufficient because the conditions are derived by using the forward solution, which does not always coincide with the ==== solution. Therefore, the method is silent about the model classification when these conditions are not met. Our approach – referred to as the ==== method in what follows – resolves all of these problems by using just two function values of the ==== solution, which is always well-defined.====This paper is organized as follows. Section 2 introduces a simple regime-switching model to provide an intuition behind our methodology. Section 3 presents the class of MSRE models, solutions and examines their mean-square stability. Section 4 formally develops our methodology and presents the main results for MSRE models, including an efficient solution procedure. In Section 5, we apply our methodology to analyze the economic example introduced in Section 2.==== Section 6 concludes.",Determinacy and classification of Markov-switching rational expectations models,https://www.sciencedirect.com/science/article/pii/S0165188921000506,29 March 2021,2021,Research Article,219.0
"Bairoliya Neha,Miller Ray","Marshall School of Business, University of Southern California, 701 Exposition Blvd, Ste 231 Los Angeles, CA 90089, United States,Colorado State University, Clark C320, Fort Collins, CO 80523, United States","Received 16 November 2020, Revised 22 March 2021, Accepted 23 March 2021, Available online 29 March 2021, Version of Record 24 April 2021.",https://doi.org/10.1016/j.jedc.2021.104117,Cited by (11),"We assess the impact of demographic changes on human capital accumulation and aggregate output using an overlapping generations model with endogenous savings and ==== decisions. We focus on China as it has experienced rapid changes in demographics as well as human capital levels between 1970 and 2010. Additionally, further variations in demographics are expected due to the recently introduced two-child policy. Model simulations indicate that education shares and income per capita will be lower with a fertility rebound as compared to ==== fertility. We find ==== to be effective in mitigating these adverse outcomes associated with higher fertility. While long-run declines in output per capita can be offset by a 4.7% increase in the government education budget, it requires a 28% increase to achieve the same outcome in the short run.","Population aging and an associated slowdown in economic growth is a major concern in many countries. Rising old age dependency ratios may increase the private burden of caring for elderly parents and threaten the fiscal sustainability of pay-as-you-go pension and public healthcare systems. This is particularly true in China, which has recently expanded its partially funded pension and health insurance systems into rural areas and to migrants (Bairoliya and Miller, 2020). While such social insurance programs may overcome market failures and improve welfare (Bairoliya et al., 2017), they may not be as sustainable as a fully funded personal account system (Feldstein and Liebman, 2008). As population aging is driven more by below replacement fertility than longer life spans (Bloom et al., 2010), it seems natural to propose higher fertility rates as one of the potential remedies (Banister, Bloom, Rosenberg, 2010, Turner, 2009).====China’s fertility decline has been hastened by its one child policy and fertility is now well below replacement at a fairly low level of income, raising the prospect that China will get old, slowing economic growth, before it gets rich. It may be, therefore, that relaxing fertility restrictions in China improves individual welfare, by allowing families to have the number of children they want, while also improving macroeconomic performance. In 2015, China moved to a universal two child policy which has been forecast to raise the total fertility rate (TFR) from a level near 1.5 children per women to 1.8 by 2030 (Zeng and Hesketh, 2016). This is bound to cause interesting variations in demographics in the near future. We analyze the effects of these expected changes in demographics on human capital levels and macroeconomic outcomes relative to a counterfactual of a continuation of fertility at the level of 1.5 children per woman.====Bloom et al. (2010) show in a theoretical framework that a reduction in fertility below replacement levels can result in a sharp decline in the working-age share of the population and potential slow down of economic growth. Aging could also substantially increase the tax burden of health care and pension programs due to declining support ratios and increased health expenditures per capita (Bloom, Boersch-Supan, McGee, Seike, et al., 2011, Christiansen, Bech, Lauridsen, Nielsen, Seshamani, Gray, 2004). However, declining fertility can induce higher investments in health and human capital which can offset some of the negative effects of aging by raising average effective labor supply (Fougère, Mérette, 1999, Lee, Mason, 2010a, Lee, Mason, 2010b, Prettner, Bloom, Strulik, 2013). It can also induce higher physical capital accumulation by encouraging workers to save for retirement rather than rely on their children for old-age support (İmrohoroğlu and Zhao, 2018). In the light of these potential countervailing mechanisms, the macroeconomic effects of the recent relaxation of fertility controls by the Chinese government are unclear. Moreover, the macroeconomic outcomes may differ in the short run versus the long run.====In order to quantitatively assess the impact of these demographic variations in China in a general equilibrium framework, we use an overlapping generations (OLG) model featuring inter-generational altruism to mimic the important role of family in China in providing social insurance. The unit of analysis in the model is a household composed of several generations living together and engaging in various economic activities. While we treat fertility as exogenous, we allow for endogenous human capital accumulation to capture the quality-quantity trade-off as it is an important mechanism to determine the effect of fertility changes on macroeconomic outcomes. We allow for public subsidies on education, health insurance, social security and private savings and model uncertainty in survival, labor productivity and medical expenditures. The government operates public pension and health insurance programs and subsidizes primary, secondary and college education in the model. While pension payments are financed through labor income taxes, public spending on health insurance and education is jointly financed through consumption taxes.====Our quantitative exercise yields four main insights. First, we find that the impact of fertility on output in the long run crucially depends on how fertility affects saving, education, and female labor supply. If savings and physical capital accumulation is the only behavioral mechanism (i.e. education and female labor supply are held constant), an increase in fertility can increase income per capita in the long run by reducing the old age dependency rate and the taxes needed to finance public pensions and health care. However, if we allow for even modest effects on female labor supply and investment in children’s education, income per capita is lower in the long run with higher fertility. An important point is that increasing the working-age share of the population is not the same as maximizing income per capita. Income is affected not just by labor supply but by human and physical capital investments that may move in opposite directions to labor supply.====Second, the short-run effects of higher fertility along the transition path to the long run involve a lower level of income per capita than with no fertility increase. In the short run, the higher fertility rate increases the youth dependency ratio and these children require consumption, child care, and education, while not producing any output until they reach working age. The surprising point here however is how long the short run lasts. In our benchmark experiment, it takes approximately sixty years for the working-age share to increase with higher fertility. While the first cohort of children from the fertility shock enters the workforce at around age 15, higher fertility means more children coming after them and it takes a considerable period of time for the age structure to reach a new steady state.====Third, through alternate fertility experiments, we find that the effect of fertility changes on long-run income per capita is not monotonic. While moving from a total fertility rate of 1.5 to 1.8 lowers income per capita in the long run, reducing fertility to 1.2 also lowers income per capita. At this very low fertility level the increased female labor supply and education effects from lower fertility are insufficient to counterbalance the negative effect of population aging. This is consistent with Lee et al. (2014) who find that the total fertility rate that maximizes consumption for China is slightly lower than the replacement fertility. Whereas both very low or very high fertility rates have adverse economic effects.====Finally, we find that there are significant externalities (both positive and negative) associated with higher fertility through taxes and subsidies. Higher fertility on one hand reduces the fiscal burden of financing old-age pensions and medical expenditures by increasing the fiscal support ratio. On the other hand, it also lowers the education subsidies per child under the assumption of a fixed government budget (as a share of output) for education. We conduct two policy experiments to further elucidate the importance of these general equilibrium effects. Specifically, we search over the space of public education subsidies to find the required permanent increase in the education budget with a TFR of 1.8 that results in the same per capita output as the 1.5 TFR benchmark. We find that while a modest 4.7% increase in the government education budget can offset the negative long-run effects of higher fertility on output, it requires an almost 28% increase to achieve this in the short run (within the next forty years).====Our quantitative analysis is not without limitations, but one that warrants mention at the outset is the assumption of exogenous fertility. One way to address the issue studied in this paper would be to allow households in the model to endogenously make decisions on both their fertility and human capital investments once the policy constraints are lifted. However, we abstract away from this in favor of a richer exogenous demographic structure for two reasons. First, it not clear what the long-run total fertility rate will be in China due to the recent policy changes. Hence, it would be difficult to interpret the model predictions for both TFR and economic outcomes. Moreover, the focus of the current analysis is on the macroeconomic changes that might occur due to the potential variations in future demographics. Second, endogenizing fertility decisions is computationally feasible only in a model with a much simpler demographic structure. However, we find that realistically introducing multiple generations (both child and elderly dependents) generates important transition dynamics in the model.====Our paper contributes to a growing body of related literature on demography and economic growth in China. First, demographics have been shown to have important implications for savings in China. There is empirical evidence that fertility has a negative effect on savings at the household level (e.g. Banerjee, Meng, Porzio, Qian, 2014, Banerjee, Meng, Qian, 2010, Choukhmane, Coeurdacier, Jin, Ge, Yang, Zhang). At the aggregate level, Modigliani and Cao (2004) use time series data from China to argue that fertility influenced savings over the past several decades through changes in demographic structure. Structural OLG models have since been used to analyze and quantify the link between demographics and the observed increases in aggregate savings in China (e.g. Banerjee, Meng, Porzio, Qian, 2014, Choukhmane, Coeurdacier, Jin, Curtis, Lugauer, Mark, 2015, He, Lei, Zhu, 2015). With a two-way altruism model most closely related to ours, İmrohoroğlu and Zhao (2018) find the interaction of demographics, productivity growth, and uncertain long-term care of elderly parents to be an important driver of Chinese savings rates. This is consistent with Chamon and Prasad (2010) who find evidence in support of rising average savings rates due to rising private burden of both health care spending and education. However, İmrohoroğlu and Zhao (2018) abstract from human capital considerations and the role of children more broadly. Using a general equilibrium model of endogenous fertility decisions, Liao (2013) looks at the welfare effects of relaxing fertility constraints in China but abstracts away from some key modeling details. For instance, this paper is not able to match the evolution of age-structure over time due to a simple demographic structure. Matching the precise evolution of age-distribution is crucial in pinning down the demographic dividend, hence the short-run and the long-run effects of fertility changes. It also abstracts away from the government programs on education, pensions and health care which have assumed a significant role in China in the recent times. Our general equilibrium effects indicate that the tax externality associated with these public transfer programs is significant.====We include an endogenous schooling decision in our model as fertility has been theoretically and empirically linked to human capital investments in China. Using Chinese twin births for identification, Li et al. (2008) find that higher fertility significantly reduces educational attainment and enrollment while Rosenzweig and Zhang (2009) also find reductions in schooling progress, expected college enrollment, and school grades. Compared to savings, the impact of demographics on human capital accumulation in China has received far less attention in the structural macro literature. An exception is Choukhmane et al. (2013) whose partial equilibrium model predicts that changing demographics lead children of the one-child policy generation to have at least 20% higher human capital compared to their parents.==== Meng (2003) and Chamon and Prasad (2010) also highlight the potential role of underdeveloped financial markets in amplifying savings motives under demographic change, particularly in terms of education spending. Importantly, we restrict the borrowing capacity of families and allow for an interaction between demographics and public spending on education through a government budget constraint. Finally, as previous studies have established important connections between demographics and the macroeconomic fluctuations since the end of China’s centralized economy, we turn our eye to the future. In the wake of renewed interest in relaxing the restrictive fertility policies in China, we examine the implications of changes in demographic structure moving forward under alternate fertility paths. An important point in our paper is that we assess the economic effects of these upcoming demographic variations, not the welfare effects. Families may enjoy having additional children, and these children may improve their parents’ utility level, even if it lowers income per capita and their economic circumstances. In addition, a welfare analysis would have to take into account the utility of the children born due to the policy change, which raises difficult ethical questions of measuring welfare with different population sizes (Blackorby et al., 2005).","Demographic transition, human capital and economic growth in China",https://www.sciencedirect.com/science/article/pii/S016518892100052X,29 March 2021,2021,Research Article,220.0
"Cai Ning,Li Chenxu,Shi Chao","Department of Industrial Engineering and Decision Analytics, The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong SAR, PR China,Guanghua School of Management, Peking University, Beijing 100871, PR China,School of Information Management and Engineering, Shanghai University of Finance and Economics, Shanghai 200433, PR China","Received 27 September 2020, Revised 23 March 2021, Accepted 25 March 2021, Available online 29 March 2021, Version of Record 15 April 2021.",https://doi.org/10.1016/j.jedc.2021.104113,Cited by (1),", 18(3), 337–384) have proved to be particularly useful for pricing discretely monitored barrier options but are difficult to apply directly to multidimensional models. By innovatively making these two methods complement each other, our approach takes advantage of both of them and overcomes their respective limitations. Numerical results suggest that the resulting recursive expansion pricing method is accurate and efficient under a broad range of prevalent option pricing models, including not only affine models such as the Heston-SV model and the Bates-SVJ model but also non-affine models such as the CEV model and the GARCH-SV model with/without jumps.","Barrier options are among the most popular path-dependent options traded in various financial markets and their payoffs are activated or extinguished whenever the underlying asset price process crosses certain barrier(s) over a finite time period. There exist both single- and double-barrier options and the former consist of eight types – up (down)-and-in (out) call (put) options. Moreover, there are both continuously and discretely monitored barrier options; the former’s payoffs rely on the underlying asset prices at all time points by maturity, whereas the latter’s payoffs are determined only by the underlying asset prices at a finite number of discrete monitoring dates.",Pricing discretely monitored barrier options: When Malliavin calculus expansions meet Hilbert transforms,https://www.sciencedirect.com/science/article/pii/S0165188921000488,29 March 2021,2021,Research Article,221.0
"Boer Lukas,Lütkepohl Helmut","German Institute for Economic Research (DIW Berlin), Mohrenstr. 58, Berlin 10117, Germany,Humboldt-Universität Berlin, Spandauer Str. 1, 10178 Berlin, Germany,Freie Universität Berlin, Boltzmannstr. 20, 14195 Berlin, Germany","Received 19 November 2020, Revised 15 February 2021, Accepted 18 March 2021, Available online 29 March 2021, Version of Record 17 April 2021.",https://doi.org/10.1016/j.jedc.2021.104118,Cited by (5),"A major challenge for proxy vector autoregressive analysis is the construction of a suitable external instrument variable or proxy for identifying a shock of interest. Some authors construct sophisticated proxies that account for the dating and size of the shock while other authors consider simpler versions that use only the dating and signs of particular shocks. It is shown that such qualitative (sign-)proxies can lead to impulse response estimates of the impact effects of the shock of interest that are nearly as efficient as or even more efficient than estimators based on more sophisticated quantitative proxies that also reflect the size of the shock. Moreover, the sign-proxies tend to provide more precise impulse response estimates than an approach based merely on the higher volatility of the shocks of interest on event dates.","In structural vector autoregressive (VAR) analysis, external information in the form of instrumental variables is often used to identify the shocks of interest. The external information is sometimes only available in qualitative form. The goal of this study is to investigate the possible loss in estimation efficiency for the structural parameters due to having only qualitative rather than quantitative information.====Proxy VAR analysis has become quite popular lately, mainly as it does not require economically problematic timing or exclusion restrictions on the behavior of variables and the resulting impulse responses are often more in line with economic theory than responses stemming from timing restrictions. In proxy VAR models, all variables are free to respond simultaneously to all structural shocks (e.g., Mertens, Ravn, 2013, Stock, Watson, 2012, Gertler, Karadi, 2015, Kilian, Lütkepohl, 2017, Chapter 15). However, the construction of a suitable proxy may be a main challenge because it requires additional information from sources external to the model.====Given the difficulties in constructing suitable quantitative instruments, proxies considered in this context sometimes use only qualitative information on the shock of interest. For example, Budnik and Rünstler (2020) construct a qualitative proxy for macroprudential policy shocks that takes on positive or negative values in periods where a change in capital requirements has occurred, depending on the sign of the corresponding shock, and it is zero in periods with no known policy shocks. Likewise, the Romer and Romer (1989) dummy is an indicator for monetary policy shocks that has been used as an instrument variable in an early proxy VAR study by Beaudry and Saito (1998). More generally, Plagborg-Møller and Wolf (2020, Appendix B.3) point out that narrative sign restrictions of the type considered in the literature can be used to construct instrument variables for proxy VARs that assume values ==== if a positive or negative shock occurs at the dates of special events, respectively, and that are zero otherwise. All that is needed to construct this type of sign-proxy is knowledge of the dates of the special events and the signs of the possible shocks that may have occurred at the event dates. Such knowledge is available for a number of shocks that have been used in structural VAR analysis. For example, some crises in the Middle East are known to have caused disruptions in oil supply. Such information can be employed to construct a sign-proxy for identifying oil supply shocks. Likewise, there are a number of events such as the 9/11 attacks on the US that have caused increases in economic uncertainty and could be used for constructing a sign-proxy to identify uncertainty shocks (see also Carriero et al., 2015 for a related proposal).====In this study, we compare the estimation precision of impulse responses based on quantitative versus sign-proxies in a frequentist setting.==== Through simulation, we show that, in terms of root mean squared error (RMSE), sign-proxies may yield more precise estimates of the impact effects of the structural shocks of interest than conventional, more sophisticated quantitative proxies that are not strongly correlated with the shock of interest. Moreover, sign-proxies may yield confidence intervals for impulse responses which have a coverage and width of similar size as quantitative proxies.====If the dates of specific events for the emission of shocks are known but the size and the sign of the shocks is unknown, Wright (2012) proposes to utilize potential changes of the volatility of the shocks on event dates for identifying and estimating the impact effects of the shock. For example, a monetary policy shock may be more volatile at dates of central bank council meetings. Using the additional moment conditions obtained from the heteroskedasticity for estimation, only the dates of the specific events must be known and there is no need to construct a proper instrument variable associated with the specific events. As a drawback, the approach requires more restrictive assumptions for the variances of the structural shocks than the proxy VAR approach, which we discuss in Section 2. We also compare the estimates of the Wright approach to the proxy VAR and sign-proxy estimates for the impact effects of the shock of interest. The Wright estimates turn out to be less efficient in terms of RMSE than estimators based on quantitative proxies or sign-proxies. Moreover, the approach tends to yield wider confidence intervals for impulse responses than its competitors.====In Wright’s approach and in the standard proxy VAR approach, the impact effects of the shock of interest and, hence, its impulse responses are typically estimated by the generalized method of moments (GMM). As using more moment conditions may lead to more efficient GMM estimators and since the proxy VAR approach and the heteroskedasticity approach for estimating the impact effects of the shock of interest use different sets of moment conditions, one may conjecture that combining the moment conditions may lead to efficiency gains in estimating the impact effects of the shock. Therefore we also consider this combination approach. It turns out, however, that in the present situation, the combination approach does not lead to uniform improvements of estimation efficiency for the impulse responses. We use a model from Wright (2012) to illustrate the alternative approaches in the context of an empirical example.====The remainder of the paper is structured as follows. In the next section, the general model setup is presented and the different estimators of the impulse responses are discussed. In Section 3, a Monte Carlo experiment is conducted to compare the small sample performance of the estimators. In Section 4, an illustrative example based on a model due to Wright (2012) is presented. Conclusions and extensions are discussed in the final section. Additional simulation results as well as more details on the computational methods used are available in an Online Appendix which accompanies this article.",Qualitative versus quantitative external information for proxy vector autoregressive analysis,https://www.sciencedirect.com/science/article/pii/S0165188921000531,29 March 2021,2021,Research Article,222.0
Zhang Tongbin,"Institute for Advanced Research, Shanghai University of Finance and Economics (SUFE), Guoding Road 777, Shanghai, China,Key Laboratory of Mathematical Economics (SUFE), Ministry of Education, China","Received 25 April 2020, Revised 13 March 2021, Accepted 16 March 2021, Available online 21 March 2021, Version of Record 6 April 2021.",https://doi.org/10.1016/j.jedc.2021.104103,Cited by (2),"The co-movement of stock prices and the risk-free rate in the United States is weak in terms of the correlation and ====. It is essential for investors and policymakers to understand such co-movement, especially when several well-known asset pricing models imply a much stronger relationship than the one empirically observed. To explain this inconsistency, this paper presents a model with “internally rational” agents who optimally update their subjective beliefs about stock prices. Compared with the risk-free rate, agents’ subjective beliefs are essential for generating stock market volatility. Quantitatively, our model can jointly produce basic asset market facts and the weak co-movement.","This paper examines the co-movement in prices between the stock and short-term bond markets. We use two sets of statistics to characterize the co-movement and show that it is weak in the U.S. data. First, the contemporaneous correlation between the price-dividend ratio and risk-free rate is not statistically significant from zero.==== Second, using the variance decomposition approach introduced by Campbell (1991) and Campbell and Ammer (1993), we show that the variance of news about future risk-free rate contributes little to the variance of the unexpected excess stock returns. In fact, unsurprisingly, the two top components are news about future excess returns and news about future dividend growth.====Several stock market facts have been studied extensively over the past 30 years, such as the equity premium, volatility of stock prices, and predictability of long-term excess stock returns. Hence, we investigate whether two asset pricing models with rational expectations (RE)—the external habit model (Wachter, 2006) and the long-run risk model (Bansal et al., 2012), which can explain these long-standing puzzles—are consistent with the weak co-movement. However, we demonstrate that the model-implied correlations between the price-dividend ratio and risk-free rate are much stronger than those observed empirically. Furthermore, neither model’s variance decomposition results match the data. Although news about future risk-free rate contributes little to the variance of the unexpected excess stock returns in both models, the small contribution comes from disproportionally larger contributions of other components, which are inconsistent with the data.====Although these well-known RE models miss the co-movement, co-movement still has received insufficient attention in the literature. Understanding such co-movement is important for both institutional and individual investors’ asset allocation decisions when they add stocks and short-term bonds into their portfolios. Additionally, since the 2007–2009 global financial crisis caused by the collapse of asset markets, policymakers’ increasing concern about the role of policy tools such as monetary policy and macro-prudential policy in governing stock market fluctuations drives them to see the full picture of this co-movement. From the aspect of monetary policy, having a model that can capture the co-movement should be the stepping tone before answering other questions, such as how monetary policy affects asset prices. From the aspect of macro-prudential policy, regulatory stress testing for financial stability also requires a framework for modeling the co-movement well-established.====The failure of these RE models and importance of understanding the co-movement motivate us to study whether some deviation from RE can help. Thus, we extend the work by Adam et al. (2016) into a small open economy model (exogenous risk-free rate process), which introduces agents with “internal rationality” (IR) who do not know the mapping from fundamentals to stock prices and optimize their behaviors based on their subjective beliefs about all variables beyond their control. In such circumstances, stock prices are no longer the discounted sum of the future dividend stream. Given the subjective beliefs we specify, agents optimally update their expectations about stock price behaviors using the Kalman filter. Agents’ subjective expectations in turn influence the equilibrium stock prices, and the realized stock prices feed back into agents’ beliefs. This self-referential aspect of the model implies that agents’ endogenous expectations are dominant in driving stock price fluctuations. Meanwhile, although the risk-free rate can affect the stochastic discount factor (SDF), the SDF is not important for driving stock market volatility since the present value formula no longer holds. Our learning model therefore provides a natural resolution to match the weak co-movement of stock prices and the risk-free rate.====The quantitative evaluation used in this paper relies on the simulated method of moments (SMM) to estimate and test all models. The quantitative results confirm that our learning model outperforms the above-mentioned RE models in jointly matching basic stock market moments, the correlation, and the variance decomposition. Using the ====-statistics derived from asymptotic theory, we cannot reject the null hypothesis that any of the individual data moments is the same as those in the estimated learning model. However, the large ====-statistics of the correlation and variance decomposition moments in the two RE models imply that they are inconsistent with the empirical observations.====The remainder of this paper is organized in the following manner. Section 2 discusses the related literature. Section 3 presents our empirical findings on the stock and short-term bond markets. Section 4 tests the implications of the external habit model and long-run risk model. The theoretical IR learning model is outlined in Section 5. The dynamic analysis of the model with IR agents is presented in Section 6. Section 7 shows the quantitative performance of the learning model. Section 8 derives the explicit expression for the RE equilibrium. Finally, Section 9 concludes.",Stock prices and the risk-free rate: An internal rationality approach,https://www.sciencedirect.com/science/article/pii/S0165188921000385,21 March 2021,2021,Research Article,223.0
"Lee Dong Jin,Kim Tae-Hwan,Mizen Paul","Economics and Finance Department, Sangmyung University, Seoul, South Korea,School of Economics, Yonsei University, South Korea,School of Economics, University of Nottingham, Nottingham, UK","Received 25 March 2020, Revised 21 February 2021, Accepted 7 March 2021, Available online 11 March 2021, Version of Record 2 April 2021.",https://doi.org/10.1016/j.jedc.2021.104102,Cited by (3), shock on the US economy.,"The quantile regression method, originally pioneered by Koenker and Bassett (1978), has become a useful part of the modern econometric toolbox because of its flexibility in permitting researchers to investigate the relationship between economic variables over the entire conditional distribution of interest and particularly at the tails. Recent years have witnessed the surge in applications of the method to time-series models, either theoretically or empirically. Some representative papers include (Cho, Kim, Shin, 2015, Galvao, 2009, Galvao, 2009, Greenwood-Nimmo, Shin, Kim, van Treeck, 2013, Koenker, Xiao, 2006, White, Kim, Manganelli, 2015, Xiao, 2009), which provide new insights that conventional mean-centered regression models would not have revealed, such as, for example, a measure of the degree of tail interdependence in terms of value at risk (VaR).====A number of recent papers have explored the use of quantile regressions for counterfactual analysis. For example, White et al. (2015) trace the effects of shocks in impulse response functions in quantile regression models, as opposed to the conventional mean-centered regressions. They derive a pseudo quantile impulse response function tracing the effect of a shock on the conditional quantile function, but in a fairly restrictive setting. The pseudo quantile impulse response function is set up under conditions where (i) White et al. (2015) do not allow any dynamics in the first moment of variables in their quantile models; and (ii) they consider only a special case in which a shock is given to the observable variables rather than to the structural error. This paper presents a new and proper impulse response analysis in quantile models by solving the two problems present in White et al. (2015). We allow dynamics in the first moment of structural variables by employing the structural vector-autoregression (SVAR) model, and introduce a shock to structural errors rather than to the observable structural variables.====Chavleishvili and Manganelli (2016) propose another way of deriving quantile impulse response functions independently. Their setting is different from ours in that they consider only a bivariate system of two variables and one of the two is assumed to evolve exogenously to the system. Such a setting may be suitable for financial markets where the market portfolio can be assumed to be exogenous to individual stock returns. The method used to define a structural shock is also different. They set the structural error for the exogenous variable to zero in such way that the exogenous variable is equal to a specific quantile. Hence, the shock is given ==== to the observable exogenous variable, similarly to White et al. (2015). On the contrary, we will consider a general multivariate system where all the variables are endogenous and a shock is given to the relevant structural error. Recently, other researchers such as (Han, Jung, Lee, 2019, Montes-Rojas, 2019) independently propose different ways of constructing and estimating quantile impulse response functions. Unlike our proposed method, these two recent studies employ the local projection (LP) method by Jorda (2005). These have some advantages due to being more robust to model misspecification than VARs, although LP accuracy has been questioned by Kilian and Kim (2011) on the basis of their effective coverage rate and the average length of the impulse response confidence bands.==== We view our paper as a parallel development with these recent papers with the common objective to explore the distributional implications of shocks.====We permit an intervention in the structural errors to affect the entire conditional distribution, and the effect of an identified structural shock on the conditional quantile function is called the “quantile impulse response function” (QIRF). This offers a method to observe the effect of shocks given to the structural error on the entire conditional distribution of the observable structural variables, and not just the mean. It also has two other advantages over conventional mean-based impulse responses. First, QIRF can measure how the dispersion of a variable of interest is changing after a structural shock to the system; i.e., whether the dispersion of the conditional distribution of the key variable after the shock will be larger or smaller, compared to what could have happened to the counterfactual conditional distribution of the same variable if the shock had not occurred. In our framework, such a change in dispersion can be measured by comparing QIRFs evaluated at different quantile indices. Second, QIRF can capture any changes in the degree of skewness in the response variable under different circumstances, so that behavior of economic agents under high risk does not need to be identical (symmetric) to behavior towards low risk. Whether a distribution is symmetric or not can be measured by the difference between the mean and the median. Similarly, any change in skewness in the conditional distribution in our framework can be measured by the difference between conventional mean-based impulse responses and quantile impulse responses at the median. These two effects in both dispersion and skewness, which will be termed “distributional effects” cannot be measured in the conventional mean-based impulse response analysis, but they are easily identified in our framework. Therefore, our method provides researchers and policy makers with a broader perspective on the dynamics of macroeconomic and finance variables following a shock.==== For example, the new methods are useful to central banks in setting policies under conditions that their key variables are likely to be in the tails of their conditional distributions, rather than at the mean, that is, during deep recessions, ultra low inflation and interest rates. In this sense, the proposed QIRF can allow researchers to investigate the effects of a monetary shock to some key macroeconomic variables at the tails as required without making the assumption (which may not be valid) that the effects are the same as those reported at the conditional mean or symmetric around the mean.====In addition to the main contribution of proposing the QIRF, another contribution of this study to the literature is to present a new way to jointly estimate a system of multiple quantile functions. Jun and Pinkse (2009) developed a system estimation method for multiple conditional quantile functions, but their method is not directly applicable to serially dependent variables such as ours. Hence, we extend the system quantile estimator of Jun and Pinkse (2009) to the time series context. Specifically, we first suggest a set of consistent estimators for all parameters in the system, based on some weighted quantile moment conditions. Then, an efficient GMM type estimator is proposed where the moment weight follows the idea of Jun and Pinkse (2009). The estimator is specialized to Koenker and Vuoung’s (2009) efficient estimator in univariate cases and is equivalent to Jun and Pinkse (2009) if the variables are IID (independent and identically distributed). Considering both the possibility of multiple local optima and the curse of the dimensionality problem, we suggest using the Laplace type quantile estimation (LTE) technique of Chernozhukov and Hong (2003). We provide conditions for the consistency of our estimator, and derive its asymptotic distribution.====We apply the proposed method to assess the impact of monetary policy shocks on the US economy using a standard three variable VAR, in employment growth, inflation, and the (Romer and Romer, 2004) measure of monetary policy shocks. Other authors, such as (Mumtaz and Surico, 2015) have done something similar, taking an instrumental variable quantile regression model to evaluate whether transmission of policy (fiscal or monetary) is asymmetric depending on whether the economy is expanding or contracting. Their estimates allow them to assess the extent of forward- or backward-lookingness and the effectiveness of policy during expansions versus contractions. They conclude that a constant-parameter model such as a standard VAR or DSGE model over (under) estimates the response in a model during low (high) activity. Using our QIRF approach, we support this view. Our paper demonstrates the effects of contractionary and expansionary monetary policy shocks on the whole conditional distributions for employment growth and inflation using the responses of the distribution in each of the tails and measures the change in the dispersion of the distribution after contractionary and expansionary monetary policy shocks. The responses are asymmetric and these additional pieces of information provide the policy maker with a fuller understanding of the effects of policy on the conditional distribution of variables of interest.====Recently Adrian et al. (2019) use multiple quantile models to analyze the asymmetric patterns in the conditional distributions of US growth and inflation rates. They assume an asymmetric t-distribution for the growth and inflation variables, and use the fitted values from multiple quantile regressions to estimate the parameters of the conditional distributions. A similar method can be applied to our quantile impulse response functions in such a way that multiple quantile impulse responses can be used to estimate the changes in the shape of generalized parametric conditional distribution functions such as skewed t-distribution (Fernandez and Steel, 1998), generalized t-distribution (Theodossiou, 1998)), and asymmetric power distribution (Fernandez, Osiewalski, Steel, 1995, Komunjer, 2007) after an economic shock. Our VAR model for QIRF can be also generalized to nonlinear quantile models such as CAViaR of Engle and Manganelli (2004) and MQCAViaR of White, Kim, Manganelli, 2014, White, Kim, Manganelli, 2015 if we add lags of the quantile functions as exogenous variables.====The rest of this paper is organized as follows. Section 2 introduces a linear conditional quantile model in the SVAR framework and Section 3 proposes the quantile impulse response function. Section 4 provides estimation methods. Section 5 shows an application of the quantile impulse response to US monetary policy. Section 6 provides some concluding remarks.",Impulse response analysis in conditional quantile models with an application to monetary policy,https://www.sciencedirect.com/science/article/pii/S0165188921000373,11 March 2021,2021,Research Article,224.0
"Germaschewski Yin,Horvath Jaroslav,Rubini Loris","Department of Economics, Paul College of Economics and Business, University of New Hampshire, Durham, NH 03824, USA","Received 23 September 2020, Revised 22 February 2021, Accepted 22 February 2021, Available online 27 February 2021, Version of Record 9 March 2021.",https://doi.org/10.1016/j.jedc.2021.104100,Cited by (4)," the model-generated expropriation series is positively correlated with a commonly used measure of property rights; ==== a placebo test estimating the model for the U.S. finds expropriations to be about one eighth of those in China, and to account for only a small share of the U.S. aggregate fluctuations.","Real business cycles in China follow different patterns compared to many other countries. Notably, consumption is more volatile than output, and displays a lack of correlation with investment and output. China’s institutional structure, with a large government presence and relatively weak property rights, is potentially an important factor affecting macroeconomic fluctuations. Many Chinese enterprises are state-owned, where the decision making processes is quite different from those in the private sector (Song et al., 2011). Moreover, expropriations tend to be common and take many forms, such as selectively enforcing taxes to surprised business owners (Blustein, 2019), or setting mark-ups for land to be privately leased as a function of expected profits (Ding and Lichtenberg, 2011). In this paper, we introduce these features into a standard real business cycle (RBC) model to understand their role in shaping the business cycle dynamics.====While activities by state-owned enterprises (SOEs) are relatively straightforward to model by focusing on readily available theories and data, expropriations are not. Datasets on expropriations are rare, predominantly based on surveys, and available mostly at annual frequency, which presents problems for quantitative business cycle work. We tackle this by building a model with micro-founded expropriations. We exploit the fact that expropriations affect the dynamics of national account variables, such as output, consumption, and investment, differently than productivity or other shocks. This allows us to estimate the parameters related to expropriations with Bayesian techniques.====To this end, we extend the standard RBC model in four ways. Building on Rubini (2019), we first consider private-owned enterprises (POEs) with decreasing returns to scale technology, and politicians that expropriate firms. Expropriations lower the present value of a firm’s future profit stream, depressing investment and increasing consumption. This lowers the correlation between the two series, and increases the volatility of consumption relative to output.====Expropriations arise endogenously, that is, more valuable firms are more “enticing” to expropriate. We model this by assuming expropriations require the use of productive resources. Empirically, many of these require labor. For example, if a surprise tax is to be collected, the government requires administrative resources.==== In addition, expropriations depend on a stochastic process determining expropriation efficiency, which represents the degree of property rights protection. Shocks to expropriation efficiency add a layer of uncertainty to expropriation, which Campos et al. (1999) find to be a central determinant of investment when property rights protection is weak.====Second, we introduce SOEs that differ from POEs along several dimensions. Following Song et al. (2011), SOEs are more capital intensive. In addition, they are not subject to expropriations since profits already accrue to the government. Goods produced by SOEs are combined with goods produced by POEs via a constant elasticity of substitution (CES) aggregator to form a final good.====Third, both POEs and SOEs face time-to-build technologies as in Kydland and Prescott (1982). This prevents investment from being too volatile, especially in the SOE sector.====Fourth, we consider exogenous shocks to government expenditures. When going to the data, we bundle government expenditures together with net exports, which allows us to account for the large fluctuations in both variables and makes the model more consistent with the data.====Our estimated baseline model accounts reasonably well for several unique moments of the Chinese economy. It generates ==== more volatile consumption than output; ==== a close-to-zero correlation between consumption and private investment; ==== a SOE investment that is more volatile than private investment; ==== a negative correlation between consumption and SOE investment; and ==== a weak correlation between private and SOE investment—features that the standard RBC model cannot replicate.====A key result of this paper is the importance of expropriations. The variance decomposition shows that shocks to expropriation efficiency play the main role in generating macroeconomic volatility in China. These shocks account for over 70% of the volatility in consumption and output, and over 60% of private investment volatility.====To evaluate the empirical validity of our model-estimated expropriation series, we conduct three exercises. First, we use a Kalman filter to extract a smoothed time series of expropriations from our model. We then compare this smoothed series with a relatively standard measure of property rights enforcement, the Political Risk (PR) index of the International Country Risk Guide, used extensively in the literature (Acemoglu et al., 2003, Angelopoulos et al., 2011, Hall, Jones, 1999, Knack and Keefer, 1995, Mauro, 1995). We find that the correlation between the two series is positive, equal to 0.57.====Second, we investigate the role of expropriations after 2012, when President Xi Jinping rose to power with reducing corruption as his priority. To do this, we estimate the model before and after 2012, using the same set of prior distributions. We find that expropriations are still the main driver of macroeconomic volatility in the earlier period, but no longer in the latter one, during which they account for less than 10% of the volatility in consumption and output, and for less than 15% of the volatility in private investment, consistent with the evidence of the anti-corruption campaign’s effectiveness provided by Hao et al. (2020) and Tao (2020). In the post-2012 period, private TFP shocks become the main driver of aggregate volatility.====Lastly, we estimate our model using U.S. data. Given the strong institutional framework in the U.S., this exercise serves as a placebo test. As such, finding a similar role of expropriations to that in China would be discouraging. This is, however, not the case. Expropriations in the U.S. are small, almost one eighth of those in China, and account for a minor fraction of aggregate fluctuations. Moreover, introducing expropriations in the U.S. does not considerably affect the fit of the model compared to the standard RBC model.====This paper is closely related to Angelopoulos et al. (2011), who study the effects of weak property rights protection on business cycle fluctuations in Mexico. The main difference between our setup and theirs is that expropriations are exogenous in their model. We endogenize expropriations for three reasons. First, there is empirical evidence supporting the fact that a firm’s value affects the incentives to expropriate in China. For example, Ding and Lichtenberg (2011) document that the price the government charges for the lease of land to private firms depends greatly on the expected profits of the lessee. Second, the parameters in Angelopoulos et al. (2011) cannot be identified with statistical methods, since TFP and expropriation efficiency shocks are isomorphic: they have qualitatively identical effects on macroeconomic variables. Third, our framework allows us to rebate the proceeds of expropriations back to the households, thereby abstracting from any income effect.====Our paper contributes to a large strand of literature that explores real business cycle properties in developing countries. For example, Neumeyer and Perri (2005) and Uribe and Yue (2006) show that interest rate shocks in developing countries amplify the business cycle; and García-Cicco et al. (2010) show that financial constraints are particularly well-suited to study countries like Mexico and Argentina. Li et al. (2008) and Wang and You (2012) find that while interest rates or financial constraints may be high for some firms, they are not for those with government connections. Song et al. (2011) also argue that the large trade surplus in China is the result of high savings, which makes the case for economy-wide financial constraints hard to defend.====Other studies that examine business cycle properties in developed and developing economies include Aguiar and Gopinath (2007), who introduce productivity trend shocks and find that the trend is much more volatile in developing countries. Restrepo-Echavarria (2014) and Horvath (2018) study the role of the informal economy and find that its size, when mismeasured, amplifies macroeconomic volatility. Chen et al. (2018) find similar effects of home production, and Dogan (2019) of investment specific technologies. Our paper adds to this literature by examining the impact of institutions on aggregate fluctuations in the world’s two largest economies. In particular, we show that introducing expropriations not only allows us to account for excess volatility of consumption, but also for other moments unique to China such as the lack of correlation between consumption and investment, and the low comovement between private and SOE investments.====The rest of the paper is organized as follows. Section 2 describes China’s institutional framework and property rights protection. Section 3 outlines the model. Bayesian prior and posterior analysis of the parameter values and equilibrium solutions are discussed in Section 4. Section 5 presents the main results. Section 6 conducts three external validation exercises of our model. Section 7 assesses the contribution of different model features. Section 8 concludes.","Property rights, expropriations, and business cycles in China",https://www.sciencedirect.com/science/article/pii/S016518892100035X,27 February 2021,2021,Research Article,225.0
"Quemin Simon,Trotignon Raphaël","Climate Economics Chair, Paris-Dauphine University, PSL Research University, France,Potsdam Institute for Climate Impact Research, Member of the Leibniz Association, Germany,Grantham Research Institute, London School of Economics and Political Science, England, United Kingdom","Received 17 October 2020, Revised 13 January 2021, Accepted 17 February 2021, Available online 20 February 2021, Version of Record 1 March 2021.",https://doi.org/10.1016/j.jedc.2021.104099,Cited by (15),We develop an emission permits trading model where covered firms can (1) utilize rolling planning horizons to deal with uncertainty and (2) exhibit bounded responsiveness to supply-side control policies. We calibrate the model to reproduce annual market outcomes in the ,"Emissions trading systems (ETS) have become a widely used climate and energy policy tool. Most allow for some form of flexibility in the covered firms’ emission streams through banking and borrowing of emission permits over time (ICAP, 2020, World Bank, 2020). That is, firms must comply with the cumulative cap on emissions over the length of the program, but how they manage their emissions’ calendars is left to their discretion, at least to a certain extent. In many respects, therefore, determining the optimal timing of emissions and permit usage is isomorphic to Hotelling’s problem of efficiently extracting an exhaustible resource sold in a competitive market under conditions of uncertainty.==== In principle, as firms cost minimize over time, intertemporal trading and absence of arbitrage entail that the permit price reflects the expected discounted long-term scarcity of permits implied by the cap trajectory and that the cumulative cap is attained at least discounted cost in expectation (e.g. Ellerman, Montero, 2007, Kling, Rubin, 1997, Rubin, 1996, Schennach, 2000).====As the opening quote recognizes, however, the market equilibrium outcomes prevailing under this cost-effective approach to timing emissions crucially hinge on the planning horizon firms effectively employ. In this paper, we formalize and answer the three questions it raises with a specific focus on the EU ETS. That is, we first provide ample anecdotal evidence suggesting that firms use ‘truncated horizons that are updated and moved forward as time progresses’ in their decision-making process, what we call ====. Second, we develop a model of competitive emissions trading where firms can utilize rolling horizons, which we calibrate to market developments over 2008–2017 to obtain a measure of the ‘relevant time span’. Third, we analyze how rolling horizons ‘change the equilibrium paths’ relative to infinite horizons. This has important implications for policy design and evaluation, some of which we highlight by assessing the impacts of the 2018 EU ETS reform with our calibrated model.====Specifically, the equilibrium outcomes when firms utilize rolling horizons with a given discount rate may prima facie be seen as qualitatively equivalent to those when firms have an infinite horizon with a larger discount rate.==== Crucially, however, this may only hold assuming market design considerations away while in practice permit markets are often equipped with supply-side controls, typically in the form of a price collar (e.g. Roberts and Spence, 1976) or a banking corridor in the post-reform EU ETS (e.g. Perino and Willner, 2016). In the presence of a supply control, short- and long-term equilibrium outcomes will depend on the interaction between the control’s design features and the firms’ horizon. Additionally, they will also depend on the firms’ sophistication in understanding the implications of the control on their intertemporal decision making, what we call firms’ ====. In this paper, we analyze and quantify the interplay between the post-reform EU ETS design elements and the firms’ horizon and degree of responsiveness. This delineates the three main contributions of this paper, which we present below in further detail in relation with the existing literature.====As a first contribution, we develop a model of competitive emissions trading under uncertainty with supply-side control which departs from the existing literature along two key dimensions. First, firms can use rolling horizons in the face of uncertainty and limited information about future permit supply and demand. As a case in point, future demand may vary substantially depending on the economic activity and the achievements of complementary policies (see e.g. Borenstein et al., 2019). This essentially involves an iterative optimization procedure over a sliding truncated horizon given realistic forecasts of the relevant exogenous factors. Only the first period of the plan is implemented, and then a new plan is formulated over an equally long horizon with updated forecasts. Second, firms can exhibit two polar degrees of responsiveness. With zero (resp. full) responsiveness, firms have no (resp. a perfect) understanding of how their intertemporal equilibrium decisions interact with the supply control impacts over time. The full responsiveness case coincides with the rational expectations equilibrium framework of Muth (1961) and obtains as the fixed point between the firms’ beliefs about the control impacts and its impacts in equilibrium in the spirit of Lucas and Prescott (1971).====By contrast, the archetypal permit trading model considers fully rational firms with an infinite horizon, or at least extending to the end of the program, see Hasegawa and Salant (2015) for a review. The concept of rolling horizons was first introduced by Goldman (1968) and can be seen as a way of addressing increasing uncertainty and costly informational requirements (e.g. Easley, Spulber, 1981, Grüne, Semmler, Stieler, 2015). Additionally, rolling planning is extensively used by firms in their routine production and supply management processes (see Sahin et al., 2013, for a review) and was proposed in the similar context of exhaustible resource extraction to help rationalize the lack of empirical support for the Hotelling rule (Spiro, 2014, van Veldhuizen, Sonnemans, 2018).==== As another source of bounded rationality, we consider that firms may face cognitive or computational limitations in optimally adjusting their decisions to supply control. We believe that introducing limited responsiveness is an alley worth exploring as it fits well into the context of the EU ETS where there is evidence that, even in the absence of supply control, firms do not fully appreciate the trading and profit opportunities created by the market (e.g. Baudry, Faure, Quemin, 2020, Karpf, Mandel, Battiston, 2018, Martin, Muûls, Wagner, 2015, Venmans, 2016). Additionally, the firm-level implications of a banking corridor such as the one introduced by the 2018 reform may arguably not be as transparent and straightforward as those of a price signal conveyed by a price collar (e.g. Perino, 2018, Wettestad, Jevnaker, 2019).====As a second contribution, we tailor the model to the EU ETS design and parametrize permit supply and demand using historical emissions and allocation data in alignment with prevailing regulation (e.g. ETS Directive, EU renewable or energy efficiency targets). We calibrate the market-wide discount rate and planning horizon by fitting simulated outputs to past annual banking and price dynamics and compare the merits of two alternative assumptions – infinite vs. rolling horizon – in how well they can rationalize observations. A rolling horizon of a dozen years can (1) reconcile annual banking levels over 2008–2017 with discount rates inferred from futures’ yield curves (3% on average) where an infinite horizon can only do so with ‘too high’ rates, (2) reproduce average annual price changes over 2008–2017 twice better than an infinite horizon (both in size and sign), and (3) pick up the 2018 price rally and regime shift induced by the market reform where an infinite horizon falls short of it (Fig. 1).====It is important to recognize one caveat that must be applied to interpreting this result based on the resultant of individual firms’ behaviors. Specifically, we offer an interpretation in the spirit of Friedman’s (1953) black box approach. That is not to say that all firms actually use rolling horizons, nor a fortiori the same horizon, but this representation has the comparative advantage of reproducing past market outcomes more satisfactorily than the infinite horizon. Additionally, despite the existing heterogeneity in firms’ risk preferences and strategic abilities (e.g. Hortaçsu, Luco, Puller, Zhu, 2019, Obara, Zincenko, 2017),==== our representative firm approach allows us to develop a parsimonious model within the canonical Rubin–Schennach framework.==== In this context, our calibration estimates are a novel and valuable contribution to the empirical literature on the EU ETS, which identifies firms’ degrees of optimization, foresight levels and horizons as open research questions (e.g. Ellerman, Marcantonini, Zaklan, 2016, Fuss, Flachsland, Koch, Kornek, Knopf, Edenhofer, 2018, Hintermann, Peterson, Rickels, 2016). Our calibration exercise also enriches the methodology for the ex-post analysis of banking behavior proposed by Ellerman and Montero (2007) in the context of the US Acid Rain Program and it exemplifies how rolling horizons can be key in understanding price and banking dynamics as well as evaluating ETS’ performances.====If firms (behave as if they) focus more on the short to mid term than on the long term, this has important ramifications for policy design and outcomes. As a third contribution, we thus use our calibrated model to assess the impacts of the 2018 market reform (European Parliament & Council, 2018) and compare them based on the firms’ horizon and responsiveness degree. We decompose the impacts of reform’s three main elements, viz. (1) an increase in the annual Linear Reduction Factor of the cap from 1.74% to 2.20% from 2021 on, (2) the introduction of the Market Stability Reserve (MSR) in 2019, a banking corridor that adjusts current auctions downwards (resp. upwards) when the bank level in the previous year was above (resp. below) some threshold, and (3) its reinforcement by the cancellation mechanism (CM) in 2023, which cancels permits stored in the MSR in excess of auctioned volumes in the previous year.====We highlight three key results for policy design. First, quantitative MSR impacts can differ considerably depending on the firms’ horizon and responsiveness. For instance, cumulative cancellations vary by a factor of two under an infinite vs. rolling horizon (5 vs. 10 GtCO====).==== Second, under a rolling horizon, the MSR can compensate for the firms’ limited foresight and reduce the costs of meeting a given cumulative emissions target relative to a sole equivalent linear cap. That is, by postponing auctions (i.e. by frontloading abatement efforts), the MSR makes the long-term stringency implied by the linear cap more tangible early on (i.e. it raises the cap’s transitional stringency), steering the abatement path more in line with the long-term target.==== Note that the CM is instrumental in realizing this cost-effectiveness improvement as it prevents fixed, inflexible reinjections of MSR permits into the market in the future. Third, MSR-induced cumulative supply adjustments in response to demand shocks are limited in size and time, which is more marked under a rolling horizon. This suggests a limited stabilizing potential in reflecting the interactions with complementary energy policies and responding to unexpected shocks such as the Covid-19 pandemic or major recessions.====In this respect, our paper relates and adds to the burgeoning literature on the MSR initiated by Richstein et al. (2015), Fell (2016) and Perino and Willner (2016).==== This literature finds that market outcomes and MSR impacts are sensitive to model parametrization and calibration as well as underlying assumptions about complementary abatement policies. Our finer-grained calibration thus confers a comparative advantage to our simulation results.==== Other related papers introduce risk aversion on the part of firms (Kollenberg, Taschini, 2016, Kollenberg, Taschini, 2019, Tietjen, Lessmann, Pahle, 2021) or an output market (Chaton et al., 2018), typically electricity, or they account for firms’ investment decisions (Bruninx, Ovaere, Delarue, 2020, Mauer, Okullo, Pahle, 2019, Tietjen, Lessmann, Pahle, 2021). In these contexts, as with a rolling horizon, the mere postponement of auctions via the MSR (irrespective of the CM) has noticeable impacts on market outcomes, either because it affects the temporal availability of permits and variability thereof, and thus the permit-specific risk premium and equilibrium price path, or because it affects equilibrium investment decisions and thus the speed and path of low-carbon capacity deployment.====The remainder is structured as follows. Section 2 introduces the concept of rolling planning horizons in relation with the literature and provides anecdotal evidence for why it is relevant in the context of the EU ETS. Section 3 embeds rolling horizons and limited responsiveness to supply control into an archetypal permit market modeling framework. Section 4 describes the model parametrization and calibration to the EU ETS. Section 5 analyzes the impacts of the 2018 reform, focusing on the interplay between market design elements and the firms’ horizon and responsiveness degree. Section 6 concludes.",Emissions trading with rolling horizons,https://www.sciencedirect.com/science/article/pii/S0165188921000348,20 February 2021,2021,Research Article,226.0
"Choi So Eun,Jang Hyun Jin,Lee Kyungsub,Zheng Harry","Department of Mathematical Sciences, Korea Advanced Institute of Science and Technology (KAIST), Daejeon 34141, Republic of Korea,School of Business Administration,Ulsan National Institute of Science and Technology (UNIST),Ulsan 44919, Republic of Korea,Department of Statistics, Yeungnam University, Gyeongsan, Republic of Korea,Department of Mathematics, Imperial College, London SW7 2AZ, United Kingdom","Received 19 March 2020, Revised 26 January 2021, Accepted 17 February 2021, Available online 20 February 2021, Version of Record 28 February 2021.",https://doi.org/10.1016/j.jedc.2021.104098,Cited by (6), and compare its performance distribution with that of other feasible strategies. We find that our estimation of the optimal market-making placement allows significantly stable and steady profit accumulation over time through the implementation of strict inventory management.,"The emergence of innovative technologies has accelerated the paradigm shift in trading activities in financial markets. In particular, automated trading based on ultra-low latency electronic systems facilitates order generation, routing, and execution, all within a fraction of a second. It uses computerised algorithmic trading called high-frequency trading (HFT) that has grown dramatically over the past decade and now accounts for 55% and 40% of all trades in the US and European equity markets, respectively; it is also rapidly growing in the Asian market for a variety of asset classes (Miller and Shorter, 2016).====In general, little is publicly known about HFT strategies. According to the concept releases of the US Securities and Exchange Commission (SEC, 2010, SEC, 2014), these strategies can be categorised into two groups by risk appetite: passive and aggressive. Passive strategies include market-making and arbitrage trading, which rarely depend on the direction of price movements. Market-making trades mainly provide liquidity to the marketplace, exploiting both bid and ask orders to generate a profit from the bid/ask spread. Arbitrage trading is generally pursued to generate a profit from the price disparities among related securities such as exchange-traded funds and baskets of underlying stocks. Meanwhile, aggressive strategies involve momentum ignition and order anticipation strategies, which use the price direction along with either a long or a short position. A momentum ignition strategy aims to trigger sharp price movements – either up or down – by initiating a series of orders. An order anticipation strategy seeks to identify large institutional orders and then trade ahead of those orders in anticipation that they will move market prices. Although these strategies are not new, the advanced technology now available may enable traders to better identify profit opportunities and execute their strategies more effectively than in the past.====These HFT strategies can lead financial markets to become more synchronised and substantially increase correlations in the price structure because HFT is more likely to occur by tracking price movement patterns than changes in market fundamentals. In addition, orders tend to be submitted as a pair of long and short sides (i.e. round-trip trades) and they are also executed subsequently and repeatedly on one side – either buy or sell. Gerig (2015) proposes a single-period model of synchrony in financial markets caused by HFT by drawing similarities with the behaviour of animal groups such as schooling fish and herding birds. Given the current market circumstances, it is worthwhile to discuss how a model can capture the highly dependent structure of order flow arrivals attributed to HFT activities. In addition, for market-makers that may be designated by a firm or pursue a market-making HFT strategy, the optimal placement of the bid/ask spread is a crucial issue.====Despite the market structure changing towards a hyper-correlation regime, most studies modelling high-frequency dynamics still employ exciting factor-based Hawkes models. However, synchronisation in order flow arrivals might not be temporary. In other words, it can disappear if no subsequent orders are placed, owing to the continued likelihood of orders being synchronised because of HFT activities. In this context, we propose a model for market order arrivals and limit order book dynamics based on the Hawkes process with the addition of a novel variable, referred to as a ====. This factor makes contemporaneous interactions between buy and sell orders feasible and also enables the interacting effect to remain permanent in the long-term mean of order flows. We then consider a decision-making problem for a market-maker when the synchronised market and limit orders enter a trading book. We finally derive the market-maker’s optimal trading strategy by maximising his or her profit and liquidating the inventory over a finite period.====Our order arrival model is based on Hawkes processes (Hawkes, 1971, Hawkes, Oakes, 1974) that have been employed as a tool for modelling price movements in high-frequency dynamics because of their great flexibility and versatility. As a pioneering work, Bowsher (2007) introduces a bivariate Hawkes process to model the joint dynamics of trades and mid-price changes on New York Stock Exchange (NYSE) stocks. Large (2007) formulises the resiliency of limit order books based on a 10-variate Hawkes process by testing stocks on the London Stock Exchange. In addition, many studies have employed Hawkes processes in high-frequency finance (Aït-Sahalia, Cacho-Diaz, Laeven, 2015, Bacry, Dayri, Muzy, 2012, Da Fonseca, Zaatour, 2014, Lee, Seo, 2017). The key feature of Hawkes-based models is the inclusion of exciting factors in their intensity processes. This means that a counting process is more likely to increase when the counting event arrives because its intensity can instantly jump depending on the movement of the original process. Such a design enables us to capture the clustering phenomenon of arrivals in the counting process by using a feedback kernel of its intensity that communicates with the counting arrivals.====In the proposed Hawkes intensity process, the exciting and synchronising components appear to play similar roles, which leads to a strong correlation over time. Nevertheless, from the perspective of the mechanism causing the abnormal impacts, synchronisation is different from excitation. Exciting events are activated by certain external stimulations such as a status change in the original and other relevant processes, and hence they accelerate the arrivals of subsequent orders instantaneously. By contrast, the synchronising factor enhances the likelihood of integrating two processes irrespective of exogenous events. In other words, for the exciting factor, intensity processes are assumed to be independent unless the underlying process changes because order arrivals lead to a temporary increase in intensity. On the contrary, the synchronising factor forces one process to lead the other, and vice versa, implying that the two processes tend to be associated endogenously in the long-term beyond market fundamentals.====This study makes three primary contributions. First, we examine an evidence of the existence of the synchronising factor using market order data. For the six representative stocks in the pool of US large caps (IBM, Chevron, Apple, Amazon, JP Morgan, and Microsoft), we find a remarkable increase in the synchronising tendency between market buy and sell order arrivals in 2018 compared with in 2008. The increase over the past decade differs across stocks, with the highest change observed for the JP Morgan stock. This finding implies that the frequencies of market buy and sell orders in a unit time tend to interact more dynamically with each other than they used to do in the past.====Second, we adopt the deep neural network (DNN) technique to solve the optimisation problem derived as a high-dimensional partial differential equation (PDE) with the discontinuity terms driven by the exciting factors and discretely varying inventory amount in a market-maker’s trading. This is generally known to be difficult to solve analytically. To solve the PDE numerically, we propose the DNN-based approximation inspired by the deep Galerkin method (Sirignano and Spiliopoulos, 2018), a mesh-free simulation suitable for applying to high-dimensional PDE problems. We verify that the existence of a DNN structure that guarantees that the loss function – defined using the boundary and terminal conditions of a given PDE – is sufficiently small in a compact domain. The DNN algorithm enables us to construct an approximate solution with a low numerical error, which is trained to satisfy the fact that the loss function is minimised for the generated random samples over the PDE domain.====Third, we conduct a variety of computational simulations to derive implications from the perspective of wealth management in high-frequency market-making by assuming models with and without the synchrony effect as well as those with different stability levels. In terms of synchrony, we compare the profit performance of posting optimal strategies with presence of the synchronising factor under scenarios in which the order dynamics are fully synchronised. The results show that the strategy considering the synchronising effect produces more gains and less risks than the one that partly considers or does not. In terms of stability, the more unstable the high-frequency market, the more likely market-makers are to obtain higher expected returns. These findings are meaningful to market practitioners given the increase in instability in the high-frequency market for some particular stocks in the past ten years.====The remainder of this paper is organised as follows. Section 2 reviews the relevant literature. Section 3 develops the model of market and limit order dynamics, including the synchronising factor, and builds an optimal execution problem for market-makers with the derivation of the associated PDE. Section 4 presents empirical evidence of the existence of synchrony in the proposed model. Section 5 discusses the DNN to estimate the PDE and its convergence, and Section 6 presents the simulation results under the DNN estimation to see the difference in a market-maker’s wealth using the optimal strategies when facing various market situations. Finally, Section 7 concludes. The technical proofs are presented in Appendix A. Additional simulations are displayed in Appendix B and Appendix C.",Optimal market-Making strategies under synchronised order arrivals with deep neural networks,https://www.sciencedirect.com/science/article/pii/S0165188921000336,20 February 2021,2021,Research Article,227.0
"Chung Hess,Fuentes-Albero Cristina,Paustian Matthias,Pfajfar Damjan","Federal Reserve Boardx Research and Statistics 20th and Constitution Ave. NW Washington, DC 20551 United States","Received 8 June 2020, Revised 12 February 2021, Accepted 16 February 2021, Available online 19 February 2021, Version of Record 12 March 2021.",https://doi.org/10.1016/j.jedc.2021.104097,Cited by (1),"), or into contributions of the observable variables (====). We propose to link the ==== of the latent variables and the ==== of the structural shocks in what we call the ====. This decomposition allows us to better gauge the influence of data on latent variables by taking into account the transmission mechanism of each type of shock. We show the usefulness of the ==== by analyzing the role of observable variables in estimating the output gap in two models and by studying the role of news in revisions of the output gap.","Kalman filter methods are commonly used for the estimation and analysis of macroeconomic models with a state space representation. Researchers usually decompose the estimated latent vector into contributions of the estimated structural shocks using the ====. [6] and [1] highlight that the connection between incoming data and latent variables can be opaque in complex models and propose to decompose the estimated latent vector in terms of contributions of the observable variables using the so-called ====. However, we argue that a better understanding of the relationship between the observable variables and the latent vector may be achieved by linking the ==== of structural shocks and the ==== of the latent vector in what we label as the ====. Because the ==== traces the influence of the data on latent variables through estimates of the structural shocks and their subsequent propagation, it provides a causal narrative of the linkage between observable and latent variables. This way of analyzing the estimated path of latent variables can be particularly illuminating when the focus is on inference regarding highly theoretical constructs, such as the natural rate of interest or the “flex-price” output gap, where the relation between observable and latent variable is not intuitive and depends heavily on the model’s theoretical structure. Moreover, the double decomposition can reconcile puzzling or unintuitive features of each standard decomposition when taken separately by providing a clear link between the data decomposition and the structural shocks.====We first show how the ==== can be used to study the behavior of a latent variable—the output gap—in a simple model by [5]. In this model, the ==== shows that the estimated path of the output gap is almost entirely explained by news on inflation, with a very small role for news on GDP growth. Working through the logic of the double decomposition traces this outcome to the relatively high sensitivity of inflation to real activity in this model. In particular, news on GDP growth is associated with a configuration of shocks with offsetting implications for the output gap, attenuating the informativeness of GDP growth. Second, we illustrate the value of the ==== for practitioners studying the output gap in a model where inference is more complex—a version of the model presented in [3]. The ==== shows that the estimated path of the output gap in this model is explained by a disparate set of observables, including indicators of real activity, such as consumption growth, and financial indicators, such as the federal funds rate and the corporate bond spread. The ==== explains the role of consumption growth, the funds rate, and the spread by showing that forecast errors in these variables are highly informative about permanent technology shocks and shocks to firm risk (effectively, shocks to the spread between the return on capital and the return on risk-free assets), which are the primary drivers of the output gap according to the ====. Importantly, our discussion of the ==== results shows that the close link between the observables most informative about the output gap and shocks to permanent productivity and firm risk emerges quite transparently from core dynamic features of the model, such as the nature of the financial frictions in the model and consumption smoothing by households.====Finally, we illustrate the value of the ==== for interpreting the [3] model’s reaction to incoming news, examining in detail the effects of news flow at the beginning of 2014. In this case, shock decompositions of the revisions to the forecast and to the estimate of the output gap show large positive contributions of productivity shocks, despite disappointing news on labor productivity. Moreover, the data decomposition of the revisions is dominated by news about consumption growth, despite disappointing investment and GDP news. Both responses stem from the strong association between consumption growth and permanent productivity shocks, a fact made obvious by the double decomposition.====The rest of the paper is organized as follows. Section 2 overviews the analysis of the latent vector in structural models. Section 3 then illustrates how the ==== works in a simple 3-equation New Keynesian model. We then turn to a more complex case, a medium-scale DSGE model, in Section 4. Section 5 concludes.",Latent variables analysis in structural models: A New decomposition of the kalman smoother,https://www.sciencedirect.com/science/article/pii/S0165188921000324,19 February 2021,2021,Research Article,228.0
Faria-e-Castro Miguel,"Federal Reserve Bank of St. Louis, Research, One Federal Reserve Bank Plaza, St. Louis, MO 63102, United States","Received 31 July 2020, Revised 8 February 2021, Accepted 11 February 2021, Available online 16 February 2021, Version of Record 1 March 2021.",https://doi.org/10.1016/j.jedc.2021.104088,Cited by (64),I study the effects of the 2020 coronavirus outbreak in the United States and subsequent fiscal policy response in a nonlinear ,"The ongoing COVID-19 outbreak is causing widespread disruption in the world’s advanced economies. Monetary authorities were quick to react, with the Federal Reserve and other major central banks promptly reactivating their 2008–09 Financial Crisis toolkits. Following these steps, fiscal authorities around the globe proceeded to design and implement stabilization packages to help sustain household and firm balance sheets.====In this paper, I adapt a ==== to simulate the macroeconomic effects of a pandemic and study the effects of different types of fiscal policy instruments. The pandemic is modeled as a sudden stop of a contact-intensive services sector. Through aggregate demand externalities, the shutdown of this sector propagates to the non-services sector. Through balance sheet linkages, it also propagates to the financial sector. The rise in unemployment leads to a wave of defaults, disrupting financial intermediation and amplifying the recession. The pandemic shock results in a large spike in the unemployment rate, as in the data. Borrower households, who derive most of their income from employment and rely on bank credit to fund consumption, are the most affected group. I assume that there is endogenous entry and exit in the affected sector, which means that fluctuations in demand can have persistent effects in this sector’s productive capacity and that the economy does not immediately recover when the pandemic is over.====I calibrate the model to the US on the eve of the pandemic and combine it with data on fiscal outlays to estimate a sequence of “pandemic shocks” that allow the model to match the path of the US unemployment rate in 2020. I then use those estimated shocks to study the effects of fiscal policy and counterfactuals. I study the effects of different types of discretionary fiscal policy: (i) an increase in non-service government purchases, (ii) a decrease in the ====, (iii) an expansion of unemployment insurance (UI), (iv) unconditional transfers, and (v) payment of wages by the government to service firms.====In terms of measuring the effectiveness of different measures, it is not clear that the traditional concept of GDP multiplier is appropriate in this context. The shut down of economic activity is largely intentional and part of pandemic suppression measures, and focus on GDP stabilization could be detrimental to fighting the pandemic. For that reason, I evaluate different policies based on consumption and household income multipliers, which measure the dollar impact of fiscal spending on consumption, and on labor income net of government transfers. I find considerable variation in the distributional effects of different types of policies. Borrowers, who are most affected by the crisis, receive a larger consumption boost from policies that resemble cash transfers, such as an increase in UI benefits. I find that unconditional transfers of the type that are currently being proposed generate similar distributional effects, with the added benefit of potentially less-costly implementation. I find that liquidity assistance to firms has the longest-lasting effects and can be very effective in terms of stabilizing employment in the medium run.====I validate the ==== by computing fiscal multipliers in the absence of the pandemic shock and showing that they are in line with those that have been estimated in the literature. The model is highly nonlinear, and these fiscal multipliers are extremely state dependent: policies with positive multipliers during the pandemic may generate negative ones in normal times. The ranking of policies in terms of multipliers changes during the pandemic: tax cuts, for example, are more effective in “normal times,” while the UI multiplier becomes larger during the pandemic. Finally, I analyze the aggregate and policy-by-policy fiscal multipliers for the Coronavirus Aid, Relief, and Economic Security (CARES) Act of 2020, the $2.2 trillion coronavirus aid package. As in the baseline model, I find that the expansion of UI and liquidity assistance to firms were the most effective components of the package in terms of stabilizing income and employment, respectively.==== The exercise in this paper is very similar to the analysis conducted by ==== and ==== conducts a similar analysis while also taking into account financial sector interventions such as the Troubled Asset Relief Program (TARP), among others. In this paper, I mostly abstract from issues related to financial sector interventions.====This paper also contributes to the modeling of a pandemic in a macroeconomic model. ==== and ====, among many others. Since they endogenize the dynamics of the epidemic, their models allow them to study optimal health policy responses. They find that a severe recession, generated by agents’ optimal decision to cut back on consumption and hours worked, helps reduce the severity of their epidemic. My analysis is complementary to theirs: I take the epidemic as exogenous and given and study how a fiscal authority can help stabilize income and consumption during the epidemic.====More closely related is the work of ====, who show that supply shocks can generate aggregate effects that “look like” aggregate demand shocks in a multiple-sector, incomplete markets economy under certain conditions. While I study the effects of fiscal policy in the context of a pandemic that is modeled as an aggregate demand shock, I show that a supply shock can generate very similar aggregate effects under parameter conditions that are related to theirs.====Other analyses of fiscal policy in response to the COVID-19 crisis include ==== and ==== have a much richer setup in terms of household heterogeneity, which includes idiosyncratic income and unemployment risk. This leads them to find much larger differences in terms of the multipliers of targeted (UI) vs. untargeted (lump-sum) transfers. In this paper, I show that even in the absence of rich household heterogeneity, the multipliers for targeted transfers are larger than those for untargeted ones. Importantly, this result reverts in the absence of the pandemic shock, suggesting that the current model features a significant amount of state dependence. ==== presents the model; ==== describes the calibration, the modeling of the pandemic, and the estimation of the pandemic shocks; ==== discusses the effects of different fiscal policies in the model; ==== estimates multipliers for the different components of the CARES Act of 2020; and ==== concludes.====Borrowers (==== is the Lagrange multiplier on the borrowing constraint):====Banks (==== is the Lagrange multiplier on the leverage constraint):====Savers:====Non-services sector:====Services sector:====Government and central bank:",Fiscal policy during a pandemic,https://www.sciencedirect.com/science/article/pii/S0165188921000233,16 February 2021,2021,Research Article,229.0
Takayama Shino,"University of Queensland Economics, School of Economics University of Queensland, St Lucia, Australia","Received 14 June 2020, Revised 8 February 2021, Accepted 9 February 2021, Available online 13 February 2021, Version of Record 1 March 2021.",https://doi.org/10.1016/j.jedc.2021.104086,Cited by (0),"We study the manipulation of prices in a dynamic version of the Glosten and Milgrom (1985) model with a long-lived informed trader. We clarify the conditions under which a unique equilibrium exists and show that when the equilibrium is unique, bid and ask prices are monotonically increasing functions of the market maker’s belief about the value of the asset. We also characterize the situations in which this equilibrium involves manipulation of prices by the informed trader. Finally, we describe a computational method to find equilibria in the model, and simulation results confirm and extend our theoretical findings.","This paper studies a dynamic version of the Glosten and Milgrom (1985) (hereafter GM) model of asset trading with a long-lived, privately informed trader who can buy and sell (or sell and buy) the same asset in multiple periods. Knowing that the value of the asset is high (low), such an informed trader may find it profitable to sell (buy) with positive probability in some periods. In the literature, such a strategy by the informed trader is called a ==== (see Chakraborty and Yilmaz, 2004). Our focus in this paper is: when the equilibrium strategy is manipulative. We provide theoretical and computational analyses of the dynamic informed trading.====The basic framework of our model is the following. There is a single risky asset that is traded repeatedly. At the beginning of the game, Nature chooses the liquidation value of the risky asset, which is either high or low. This ex post liquidation value stays the same until the end of the trading process. There are liquidity traders, a “market maker,” and a single informed trader. Prior to the beginning of the trading process, the liquidation value is observed by the informed trader. In each period, there is a random determination of whether the market maker faces the informed trader or a liquidity trader. The informed trader follows a strategy that maximizes her dynamic profit. The choice of whether to buy or sell by a liquidity trader is determined by an exogenous distribution. The market maker represents a continuum of competitive dealers, and sets bid and ask prices that satisfy a zero-profit condition.====Theoretically, modeling manipulative strategies involves solving a dynamic problem and can be technically intractable. In this paper, we adopt the notion of Markov equilibrium, so that the informed trader’s strategy depends only on the market maker’s belief about the asset value and the number of periods remaining. This allows the informed trader’s problem to be viewed as the maximization of the sum of the current trading profits and a continuation value, which is a function of belief prior to the next period. In this way, we truncate the serial problem into that of a two-period decision making, and provide conditions under which manipulation arises. Furthermore, our analysis shows that our finding about the possibility of manipulation in the basic setting with a finite horizon case is robust in the stationary setting with an infinite horizon where trade ends with some probability in each period. Finally, we provide a computational method which quantifies the dynamic informed strategy and show how dynamic informed trading develops over time. Thus, this study contributes to understanding the informed trading strategies and the market maker’s learning process, both theoretically and computationally, by showing how manipulation arises and develops over time, and what the effect of manipulation on the market maker’s learning is.====To see how manipulation arises, two factors need to be considered. One is the degree to which manipulation affects the future payoff by shifting the market maker’s belief about the asset value, which depends on the slope of the value function. The slope of the value function represents the benefit of manipulation on the future payoff by shifting the market maker’s belief about the asset value through manipulation. The other factor is the cost of manipulation, which combines the foregone trading profits that would result from a nonmanipulative trade and the trading losses resulting from the manipulation. Manipulation arises only when the benefit exceeds the cost, and thus it arises only when the value function is sufficiently steep.====We find that two cases of manipulation are possible: case (A) manipulation arises when the market maker assigns a very high probability to the true state of the asset value, and case (B) manipulation arises when the market maker assigns a very low probability to the true state.====For simplicity let assume that a buy and sell liquidity order arrives with probability ====. Under this assumption, an equilibrium is symmetric with respect to belief ==== and we can focus on one type.==== Consider the low type. When the informed trading probability (denoted by ====) is high, we observe that both cases of manipulation hold: case (A) holds in a region near belief 0 and case (B) holds in a region of beliefs near belief 1. Meanwhile, because the future payoff for the low type is small at belief near 0, the cost of manipulation is very low compared with the one at belief near 1. Thus, we can say that case (A) is the one with low cost and small benefit and case (B) is the one with high cost and large benefit.====However, when ==== is low, we observe that the value function becomes close to linear and the value function becomes steeper at any belief when the number of remaining trading periods increases. As the number of remaining periods increases, the low type start to manipulates at a belief close to 0 (case (A)). As the number of remaining periods increases further, the region of beliefs at which manipulation arises expands, and the two regions in which each informed trader could manipulate in equilibrium start to overlap at belief ==== where we observe multiple equilibria in each of which only the low type manipulates, only the high type manipulates and both types manipulate co-exist. As the number of remaining periods increases further, the region of beliefs at which multiple equilibria arise expands more.====In our theoretical analysis, we focus on the case where ==== is small and prove some features that the computer simulation demonstrates.==== When the time interval is [0,1] and the number of trading periods is ==== the time interval between two trades is given by ====. We show that ==== is the boundary case and if the number of trading periods is greater than ==== then manipulation arises; otherwise (====), the equilibrium is unique and no manipulation arises. Furthermore, we show that with an intermediate number of periods (====), the equilibrium is unique with the low type manipulating in the region of beliefs smaller than ==== the high type manipulating in the region of beliefs larger than ==== and nobody manipulating at belief ==== while with a large number of periods (====), three equilibria as described above start to arise for a given belief around belief ==== in one period. In attempt of obtain an equilibrium for the period before that period by backward induction, depending on each of the three equilibrium, different multiple equilibria can arise and as the number of periods grows, solving for an equilibrium becomes very complicated.====Extending the basic framework to the stationary setting, we show that a similar result to the first framework holds: when ==== is small compared with the likelihood of trade continuation, an honest strategy is an equilibrium, and when ==== is large, both types manipulate. In the intermediate situation, an equilibrium exists uniquely, and only case (A) manipulation can arise.====We also demonstrate an interesting feature of the model regarding the dynamics of bid-ask spreads. In this model, the bid–ask spread is the difference of posterior beliefs after the two trades. As a result of the smaller adverse-selection risk by manipulation, the amount of information revealed to the market decreases when manipulation arises. Thus when the terminal period gets closer, the bid–ask spread increases for a given belief. Basically it works in the following way. In the model, the market maker sets the spread to hedge the adverse-selection risk of trading with the informed trader. When each informed trader trades honestly with her information, the adverse-selection risk is highest and the bid–ask spread is largest. When manipulation occurs, the adverse-selection risk becomes lower and the spread becomes smaller. When there are more chances to re-trade, the informed trader’s incentive to manipulate increases and the bid–ask spread is smaller. However, as the number of remaining periods decreases, there are less opportunities to recoup the immediate costs of manipulative trading from future re-trade. Hence the informed trader’s incentive to manipulate decreases and the bid-ask spread becomes larger. Computationally we observe that the spread increases gradually as the terminal period approaches, and our theoretical result also indicates that at the last phase of trading game, the spread is largest because there is no manipulation.====Finally, our computer simulation indicates that when the number of trading periods is not so large (==== as our theory predicts), even if manipulation arises, its effect on the market maker’s learning is small. However, when the number of trading periods is large (====), manipulation has a larger impact on the market maker’s learning. Further, our simulation demonstrates that the market maker’s updated belief moves toward the true state over time. The intuition of this result is that when the informed trader makes profits, in equilibrium, trades reveal information when the number of trading periods is large.","Price manipulation, dynamic informed trading, and the uniqueness of equilibrium in sequential trading",https://www.sciencedirect.com/science/article/pii/S016518892100021X,13 February 2021,2021,Research Article,230.0
Matyska Branka,"CERGE-EI (Center for Economic Research and Graduate Education - Economics Institute), Politickych veznu 7, 111 21 Prague, Czech Republic","Received 17 March 2020, Revised 2 November 2020, Accepted 8 February 2021, Available online 12 February 2021, Version of Record 6 March 2021.",https://doi.org/10.1016/j.jedc.2021.104085,Cited by (0),This paper evaluates the effectiveness of macroprudential capital requirements in the form of market risk measures in alleviating ,"The severity and longevity of the 2008 financial crisis has prompted policymakers and economists to search for effective macroprudential regulation. The extensive policy and academic debate highlights the lack of a strong consensus regarding macroprudential tools and financial regulation and supervision objectives. As emphasized by Borio (2003), the primary aim is to limit widespread financial instability, and the ultimate goal is to minimize macroeconomic costs associated with financial instability. To date, macroprudential policy has focused on countercyclical capital requirements, loan to value ratio, and systemic surcharges to ensure financial stability.==== However, the design of market risk measures has been relatively neglected from the new prudential framework, although bank solvency crucially depends on their ability to withstand market losses. In the aftermath of the crisis, the Financial Crisis Inquiry Commission (FCIC) concluded that the failure of policymakers to adequately measure the risks associated with asset-backed securities of the largest financial firms was among the prime causes of the crisis.====This paper fills the important gap in the design of regulatory market risk measure by answering three questions. First, how effective are spectral risk measures in reducing the likelihood of financial crises and improving social welfare? Second, could spectral risk measures prevent fire sales caused by adverse financial shocks? Third, who might benefit or lose from adverse shocks, savers or the financial sector? The paper evaluates the effectiveness of alternative financial regulations in achieving macroprudential stability and efficiency goals.====We start by developing a heterogeneous agent stochastic general equilibrium model with a binding capital requirement constraint and endogenous systemic risk measured by the probability of the financial sector being undercapitalized. We juxtapose Value at Risk from the Basel framework and three spectral risk measures as risk-based capital requirements. The prominent feature of the spectral risk measures of Acerbi (2002) is that they relate the market risk measure to the decision maker’s subjective probabilities. From a regulatory viewpoint, spectral risk measures are a promising generalization of Expected Shortfall as a market risk measure on Banking Supervision (2011). We first analyze the steady-state equilibrium in the presence and absence of macroprudential policy. Then, we investigate the ex-post role of four risk measures in crisis management following a sudden increase in borrowing costs, a decline in bank equity capital, and an uncertainty shock. Three shocks aim to proxy for an exogenous drop in asset prices, comparable to the downfall of the housing market during the 2008 crisis. The critical questions are: what are the implications of macroprudential policy for systemic risk, endogenous risk, fire sales, and welfare?====The most important and novel feature of our model is its formulation of market risk measures consistent with the psychology of attention in Bordalo et al. (2013b) and Tversky and Kahneman (1992). Specifically, decision-makers overweight or underweight outcomes relative to their objective probabilities because their ability to comprehend and evaluate probabilities is limited, and over/underweighting creates probability distortions. We devise macroprudential regulation such that the associated probability weighting function is convex, has an inverse S-shape, or is S-shaped.==== With the convex weighting function, regulators are pessimistic and overweight bank exposure to tail market losses according to Wang (2000). The inverse S-shaped probability weighting function overweights small probabilities and underweights large probabilities. This implies the mixture of regulatory pessimism and optimism as worst and best-case outcomes are overweighted, while intermediate are underweighted. With an S-shaped weighting function, regulators underweight extreme outcomes and overweight intermediate ones. In this framework, regulators focus on neither favorable or unfavorable scenarios, but pay attention to average losses that arise in normal times. We denote three market risk measures Wang, Kahneman-Tversky (KT), and anti-KT.====The results show that financial crises are more likely when banks are unregulated than in the equilibrium attained by VaR capital requirements. Comparing four regulatory regimes, we find that focusing solely on tail market losses can limit the probability of systemic crisis and endogenous risk. Therefore, VaR and Wang regulations fulfill the primary macroprudential objective of mitigating widespread financial instability. However, focusing on upside risks by overweighting intermediate or best-case market scenario achieves higher output per unit of bank equity. Specifically, KT and anti-KT fulfill the ultimate macroprudential goal of minimizing macroeconomic costs related to instability. In this respect, our result contributes to the literature that reports on the systemic risk-return tradeoff of capital requirements, in which lower crisis probability comes at the cost of lower output (e.g., Adrian and Boyarchenko (2018)).====Our results also contribute to evidence on the redistributive effect of financial regulation (e.g., Korinek and Kreamer, 2014). In equilibrium, KT and anti-KT capital requirements redistribute wealth from the financial sector to the rest of the economy. Bank welfare is lower under KT and anti-KT than under VaR and Wang, while household welfare is higher. At the same time,  under KT and anti-KT, additional equity hurts bankers and benefits households. The welfare transfer is possible because capital requirements based on probability weighting play a twofold role: leverage control and altering risk-sharing incentives.====Turning to crisis experiments, if aggregate bank equity declines or borrowing becomes more costly, the main result delivers the volatility paradox of Brunnermeier and Sannikov (2014), in that lower crisis probability is associated with higher price volatility and vice versa. In particular, borrowing frictions destabilize price but lower crisis probability. Conversely, if aggregate bank equity becomes scarce, this leads to a rise in crisis probability and a decline in endogenous risk. All four macroprudential policies can manage either the likelihood of a crisis or financial amplification when banks face adverse funding conditions. Still, systemic-endogenous risk trade-off is reduced if regulators measure market risk using KT and anti-KT. When an economy faces an uncertainty shock, results indicate substantial fire sales made from the banking sector to households under VaR and Wang regulation, and the output and welfare decline of both agents. Meanwhile, anti-KT generates welfare improvements for banks. The advantage of three risk measures is that regulators can mitigate the likelihood of a crisis despite fire sales. The results further show that the KT policy increases the probability of financial crises but successfully prevents fire sales and leads to a rise in output.====Given our findings, regulators could implement two macroprudential policy interventions in practice. The Tinbergen principle highlights the necessity of at least one independent policy instrument for each policy objective. What our results suggest is that VaR and Wang could target lessening crisis probability and endogenous risk, while KT and anti-KT can target preventing negative welfare and output spillovers. In practice, capital buffers can be designed to balance the ex-ante prevention of systemic risk and ex-post crisis management. In our framework, this objective translates into weighting downside and upside market risk measures according to regulators’ preferences for systemic risk reduction or output and welfare loss. Second, regulators may enforce VaR or Wang policies during normal times to reduce the likelihood of a crisis, while adjusting their choice of risk measures when financial markets are disrupted.hisRR","Salience, systemic risk and spectral risk measures as capital requirements",https://www.sciencedirect.com/science/article/pii/S0165188921000208,12 February 2021,2021,Research Article,231.0
"Gersbach Hans,Liu Yulin,Tischhauser Martin","CER-ETH Center of Economic Research at ETH Zurich and CEPR Zürichbergstrasse 18 8092 Zurich, Switzerland,Bochsler Finance & Associés SA, Gotthardstrasse 26, 6300 Zug, Switzerland","Received 1 July 2019, Revised 14 October 2020, Accepted 8 February 2021, Available online 12 February 2021, Version of Record 22 May 2021.",https://doi.org/10.1016/j.jedc.2021.104087,Cited by (2), forward guidance is preferable over ," (Woodford, 2012).",Versatile forward guidance: escaping or switching?,https://www.sciencedirect.com/science/article/pii/S0165188921000221,12 February 2021,2021,Research Article,232.0
"Mandel Antoine,Veetil Vipin P.","Paris School of Economics and Centrre d’Economie de la Sorbonne UMR CNRS 8174, Université Paris 1 Panthéon-Sorbonne. Maison des Sciences Économiques, 106–112 Boulevard de l’hôpital 75647 Paris Cedex 13, France,Department of Humanities and Social Sciences, Indian Institute of Technology Madras, Chennai, India 600036","Received 8 May 2020, Revised 28 January 2021, Accepted 5 February 2021, Available online 11 February 2021, Version of Record 23 February 2021.",https://doi.org/10.1016/j.jedc.2021.104084,Cited by (3),We develop a tractable model of price dynamics in a ,"A basic implication of the quantitative theory of money is that monetary contractions induce a decrease in the price level. However, numerous empirical studies report monetary contractions generate a temporary increase in the price-level. This “price puzzle” is sizeable: a monetary contraction that generates a 0.1% decrease in the price-level in three years is capable of generating a 0.1% increase in three months (Rusnak et al., 2013, Table 4). The sizeable wrong directional movement in the price-level is significant because the price-level mediates the relation between money and output in theoretical models of monetary neutrality (Ball, Mankiw, 1994, Lucas, 1972). Within sticky-price and sticky-information models of monetary neutrality, a wrong directional change in the price-level implies a counterfactual relation between money and output (Mankiw and Reis, 2002). The Price Puzzle is therefore empirically sizeable and theoretically significant.====In this paper, we show that the price puzzle can be explained by the propagation of monetary shocks through the production network of the economy. The propagation of monetary shocks generate differential and time-varying responses in the supply and demand for consumer goods. The price puzzle emerges if monetary shocks have a ==== short-term impact on the supply and a ==== short-term impact on the demand of consumption goods. Two conditions prove sufficient to induce these dynamics: downstream firms must be disproportionally affected by monetary contractions and they must account for a sufficiently small share of the wage bill, or more precisely sufficiently small to ensure that demand for final goods does not decline relative to supply following a monetary shock. Both conditions find empirical support. Also, our model calibrated to the US economy generates the empirically observed magnitude of the wrong directional movement in the price-level following a monetary contraction.====Our approach builds on Acemoglu et al.’s (2012) model of the network origins of aggregate fluctuations. We consider an economy with a representative consumer and a finite number of firms with Cobb-Douglas production functions. The production network of the economy is identified with the weights of these Cobb-Douglas functions. We introduce two novel elements in this framework. First, we consider prices are set locally by firms. Second, we assume firms face cash-in-advance constraints.====These assumptions yield an analytically tractable model of price and monetary dynamics.====Building on the existing literature emphasizing the heterogeneous impact of monetary shocks on firms (Clementi, Hopenhayn, 2006, Greenwald, Stiglitz, 1993, Holmstrom, Tirole, 1997), we model negative monetary shocks as (non-uniform) decrease of the working capital of firms.====A monetary contraction then affects both the demand and the supply of consumer goods but through different mechanisms and with different time lags. On the demand side, firms hurt by the initial impact and subsequent propagation of a monetary contraction decrease wages. The decrease in wages decreases the household’s income and therefore the demand for consumer goods. The time necessary for a monetary contraction to generate the long run decrease in nominal demand for consumer goods depends on the topology of the production network and the distribution of labor among firms. On the supply side, firms hurt by a monetary contraction and its propagation temporarily decrease their output as their inputs are bid away by competing users. This decrease in output propagates downstream through the production network====We then investigate how the propagation of monetary shocks affect price dynamics. In the long run, the model converges to general equilibrium and the quantity theory of money holds. In the short run, monetary disequilibrium propagates upstream via changes in nominal demand and downstream via changes in real supply. In this setting, we analytically characterise two conditions under which the price puzzle emerges. The first condition is firms hurt by monetary contractions are sufficiently downstream. This condition guarantees that monetary contractions generate a decrease in the supply of consumer goods. The second condition is firms hurt by monetary contractions bear a sufficiently small share of the economy’s wage bill. This condition guarantees that monetary contractions take sufficient time to generate the long run decrease in the nominal demand for consumer goods.====Theory and data support the two aforementioned conditions. The first condition follows from the joint observation that downstream firms are disproportionately small ==== small firms are disproportionately hurt by monetary contractions (Carbó-Valverde, Rodríguez-Fernández, Udell, 2016, Gaiotti, Generale, 2002, Gertler, Gilchrist, 1994, Iyer, Peydró, da Rocha-Lopes, Schoar, 2013).====Our second condition emphasizes that nominal demand decreases slowly because small firms which bear the brunt of the initial impact of monetary contractions account for a limited portion of the economy’s wage bill. Firms with annual receipts of less than one million account for less than 8% of the wage bill in the US economy. Therefore the initial impact of a monetary contraction generates a limited decrease in wages.====We further investigate the empirical validity of the model by calibrating it on the US production network using a novel data set with more than 100,000 buyer-seller relations between more than 50,000 US firms including all major publicly listed firms. We use Monte Carlo methods to study the dynamics of the calibrated synthetic economy with computational experiments. In the experiments, the initial impact of monetary shock scales sublinearly with firm size: small firms are disproportionately hurt by contractions. The model calibrated to the US production network robustly reproduces the empirically observed magnitude of the increase in price level after monetary contractions.",Monetary dynamics in a network economy,https://www.sciencedirect.com/science/article/pii/S0165188921000191,11 February 2021,2021,Research Article,233.0
Shiono Takashi,"Economics Research, Credit Suisse Securities (Japan) Limited, Tokyo, Japan,Graduate School of Economics, The University of Tokyo, Japan","Received 1 July 2020, Revised 22 January 2021, Accepted 25 January 2021, Available online 10 February 2021, Version of Record 21 February 2021.",https://doi.org/10.1016/j.jedc.2021.104082,Cited by (4),This study examines the possibility of applying the novel likelihood-free ,"Agent-based models (ABMs) have been widely used in the field of artificial markets and related studies to investigate the microstructure of financial markets. In recent years, they have also been increasingly adopted in macroeconomic analysis as an alternative to DSGE models. According to Grazzini et al. (2017), ABMs are characterised by the following three features: 1) there are a multitude of agents that interact with each other and the environment, (2) these agents are autonomous, in the sense that there is no central coordinator such as a Walrasian auctioneer or the concealed time axis in which the central coordinator works, and (3) aggregation is performed numerically.====Thus, an ABM allows for extreme flexibility in setting the behavioural patterns of each agent in a heterogeneous manner. There is no prerequisite for rational expectation or (ex-ante) market equilibrium, as the agents’ adaptive processes of learning and selection are explicitly modelled. This assumption regarding an agent's behaviour is a fundamental difference between DSGE and ABM. Recent studies of agent-based macroeconomic modelling such as Assenza et al. (2015) and Caiani et al. (2016) have succeeded in reproducing stylised macroeconomic phenomena such as the Phillips curve and the Okun's law from the simulations of ABMs with adaptive agents. In other words, a rational agent was not a necessary condition for these observed phenomena of the macro-economy. Furthermore, their models precisely describe the relationship between the financial sector and the real economy by explicitly modelling the balance sheet for each economic agent, which are, therefore, useful in discussing the impact on the real economy from prudent policies on financial institutions. It is because of their flexibility that ABMs are increasingly being adopted for their usefulness as a complement to DSGE and will continue to be applied to various economic phenomena.====However, it has often been pointed out that an ABM has its weaknesses in the lack of empirical validation (Gallegati and Richiardi, 2009). The parameters in an ABM are usually calibrated manually to make the model's simulation outputs reproduce some particular characteristics of economic observables (e.g. fat-tail distribution or volatility clustering in stock returns). In response to this challenge, studies that statistically estimate the parameters of ABM have been vigorously explored. Lux and Zwinkels (2018) reviewed the development of empirical literature on ABMs over the past decade.====First, the prominent work of Ghonghadze and Lux (2016) estimated parameters using the Generalised Method of Moment (GMM) for a small-scale ABM, where the dynamic evolution of the agent's microstates can be described analytically using the Fokker-Planck equations. Lux (2018) applied the Sequential Monte Carlo (SMC) method to the same class of models where the likelihood is analytically tractable. Although such approaches are straightforward because they directly adopt the established procedure of GMM or SMC, they can only be applied to the ABMs whose dynamic behaviour and likelihood can be described in the closed form. Such models must be small and restrictive. In other words, the advantages of ABM such as flexibility and variety of agents’ behaviour are lost.====Grazzini and Richiardi (2015) surveyed the growing literature on ABM estimation at that time and defined the framework of Simulated Minimum Distance (SMD). This is a method of determining the parameters to minimise the pre-selected distance between the actual data and the aggregate variables generated from the ABM simulation. The distance is typically defined by specific moment values (Method of Simulated Moment: MSM), or the parameter values of a meta-model such as Vector Auto-Regression estimated using the actual data and simulated data (Indirect Inference, II). This method is applicable to ABMs with intractable likelihood as long as the stationarity of simulated data by a model can be ensured.====Regarding the SMD method, Grazzini et al. (2017) pointed out that the estimator's sensitivity to an arbitrary pre-selected design is a critical weakness. When applying the SMD framework, researchers need to pre-select certain moments or meta-models, and the estimation accuracy is critically dependent on the adequacy of such pre-selection or handcrafted design. For example, if the ABM includes a parameter that only affects the mean of the simulated data and another parameter that only affects the variance, the estimation results differ greatly depending on which moments are selected as summary statistics. Moreover, the requirement of stationarity in the simulated data by ABMs limits the scope of the applicable class of models.====Addressing this arbitrariness, Kukacka and Barunik (2017) adopted the non-parametric simulated maximum likelihood (NPSML) estimation originally proposed by Kristensen and Shin (2012). It utilises non-parametric kernel smoothing over multiple i.i.d. draws of simulated time series from ABMs to construct an approximate time-by-time conditional density. This non-parametric approach has virtues==== of avoiding critical dependency on the arbitrary pre-selections (except the lag orders of autocorrelation in conditioning variables) and dispensing with the assumption of ergodicity on ABMs. Meanwhile, it is computationally heavy (compared with the methods assuming ergodicity), because a large number of ABM simulation runs has to be implemented by every single updating step of parameter exploration for maximum likelihood. Further, the choice of optimisation method is another concern given the simulation-based nature of the approximated likelihood; heuristic methods need to be selected since gradient-based optimisations are not applicable.====Another direction of sophistication is to reduce the large computational burden by replacing costly ABM simulations with computationally light outputs from a surrogate model. Lamperti et al. (2018) proposed an approach to create a fast surrogate model using a limited number of ABM simulations and approximated the nonlinear relationship between the parameters and outputs of ABM via machine learning techniques. The actual exploration of the parameters was conducted over the surrogate model. This is a good performing and widely applicable calibration technique, but not based on the formalism of statistical inference, and requires many pre-selections critical to estimation accuracy.====While most of the above approaches are frequentist, ABM estimation via the Bayesian approach has also been explored recently. Grazzini et al. (2017) proposed three types of Bayesian inferences with the approximation of the model likelihood. First, they approximated the likelihood by smoothing the histogram of simulated data generated from an ABM with a set of candidate parameters via Kernel Density Estimation (KDE), and subsequently evaluated the probability of the real observation data (1. Non-parametric Bayesian). This method does not require pre-selection of moments or meta-models. However, as the dimension of observables in the ABM increases, the likelihood calculation with KDE becomes infeasible owing to the curse of dimensionality. The study also proposed two other computationally cheaper methods: one is a method to parameterise the steady-state distribution of the simulation data with a normal distribution (2. Parametric Bayesian), and the other is to approximate the likelihood with the 1–0 indicator function which assesses whether the relation between actual data and simulation data meets certain pre-selected criteria or not (3. Approximate Bayesian Computation). However, both the methods require the same sort of discretionary pre-selections as the SMD method. Moreover, the requirement of ergodicity reduces the flexibility of ABMs applicable to the parameter estimation.====In terms of the advantages of Bayesian approaches with respect to frequentist approaches, Grazzini et al. (2017) insisted that 1) Bayesian methods generally do not require pre-selection of moments or meta-models, and 2) they allow the incorporation of prior information, allowing a proper statistical treatment of the uncertainty of our knowledge, and how it is updated given the available observations.====Furthermore, Platt (2020) has demonstrated that as a result of equal-footing comparisons, the Bayesian method (KDE-MCMC) of Grazzini et al. (2017) outperformed a number of sophisticated frequentist approaches, and concluded that its superiority can be attributed to a combination of the flexible likelihood approximation by KDE and the adoption of a Bayesian paradigm. The study argued that more interest should be placed on the development of Bayesian ABM estimation.====The methodological propriety of KDE-MCMC of Grazzini et al. (2017) still essentially depends on the assumption of ergodicity of ABMs, which most ABMs of practical interest are unlikely to hold. To overcome this limitation of KDE-MCMC by allowing for temporal dependencies (non-ergodicity) to be taken into account, Platt (2019) proposed a Bayesian estimation protocol that uses a neural-network-based approximation of conditional density (Mixture Density Network: MDN). It reported compelling performances in the validation tasks of recovering the ground-truth values of parameters from the simulated datasets against the KDE-MCMC of Grazzini et al. (2017). This confirmed the benefit of incorporating possible temporal dependencies in simulated time series of ABMs. One potential shortcoming of the MDN method is the huge computational burden, because it requires multiple runs of ABM simulations to train deep neural networks by every single step of a likelihood approximation.====In addition to the successes of the Bayesian methods, the discussion of a singular statistical model in the statistical learning theory may support the use of the Bayesian approach in ABM estimation. It is proved that if a stochastic model does not hold a regularity condition (i.e. if the Fisher information matrix is singular), the Cramer-Rao inequality has no meaning, and the maximum likelihood estimator is not the asymptotically best estimator (Watanabe, 2009). In this case, the Bayesian approach has more desirable properties than the maximum likelihood approach in general. It could be more beneficial to explore a Bayesian approach, because there is no guarantee that a general class of ABMs has a log-likelihood function with a positive definite Fisher information matrix. Hence, following Platt (2020), it is argued that more interests are worthwhile to be placed on Bayesian methods.====Against this background, the present study adopts a novel likelihood-free Bayesian inference called ==== proposed by Radev et al. (2020) for the statistical estimation of ABMs. The core motivation to adopt this new method is its scalability, in the sense that the desirable properties shown with a small ABM can seamlessly be reproduced when applied to a larger ABM, with an efficient increase in computational burden.====Obstacles to the scalability of an estimation method typically involve the following points. First, justifying strong assumptions on simulated time series of ABMs (such as stationarity or parametric densities) becomes more difficult, because a larger ABM tends to have more complex features in its output time series. Second, the estimator's high sensitivity to arbitrary pre-selections (i.e. summary statistics, moments, meta-model, etc.) could be another hurdle to scalability, as appropriate choices are thought to become more difficult when dealing with a larger ABM. Finally, an expansion in the computational burden with an increased ABM size should not be practically prohibitive. The so-called curse of dimensionality in the non-parametric density approximation (such as KDE) is a typical example of this obstacle. As KDE requires progressively larger draws of simulational data when dealing with higher-dimensional observation variables, and a larger ABM inevitably requires more time for a single simulation run, an estimation would become practically (quantitatively) infeasible without the use of high-performance computing equipment.====BayesFlow appears better for dealing with these three obstacles to scalability. BayesFlow is a fully likelihood-free approach, which directly approximates a Bayesian posterior rather than a likelihood function. This method has general applicability, requiring only the ability to output simulation data from a mathematical forward model, and has a theoretical guarantee for sampling from the true posterior without any specific assumption on the shape of the target posterior or prior. Hence, it does not need to presume ergodicity or stationarity of simulated time series of a model. Radev et al. (2020) have showed high accuracies of BayesFlow even in the cases of potentially chaotic (the Ricker population model) and non-ergodic (the Levy-Flight model) mathematical models.====Second, in contrast to the typical likelihood-free methods such as approximate Bayesian computation, BayesFlow does not involve discretionary pre-selections or handcrafted design in the critical parts of inference. It accompanies a learnable summary network which can compress variable-length potentially large dimensional observation time series into fixed-length small dimensional summary statistics. In other words, a researcher does not need to pre-select specific moments or meta-models from specific observables as handcrafted summary statistics.====Finally, it is computationally efficient, particularly in the case that repeated inferences with different observation datasets are required. BayesFlow realises ====, where estimation is split into a computationally intensive training phase and a much cheaper inference phase. In the training phase, BayesFlow tries to learn neural networks to output an approximate posterior that works properly for any possible observation sequence. Evaluating the trained model over a specific observation dataset (e.g. real data) is computationally cheap; therefore, the upfront training efforts amortise over multiple inferences. This amortised inference is a clear advantage with respect to the MCMC-based methods, which incur significant computational costs in collecting posterior samples, because some ABM simulations need to be run by every single step of exploring parameters. Dimensionality reduction implemented by the aforementioned summary network also contributes to reducing the computational burden of handling high-dimensional observations, in contrast to the KDE-based methods.====In the next section, I explain the basic structure of BayesFlow compared with other related methodologies. Subsequently, the procedure and results of validation are presented in Section 3, before discussing the advantages and limitations with the conclusion of the study.",Estimation of agent-based models using Bayesian deep learning approach of BayesFlow,https://www.sciencedirect.com/science/article/pii/S0165188921000178,10 February 2021,2021,Research Article,234.0
"Wan Xiangwei,Yang Nian","Antai College of Economics and Management, Shanghai Jiao Tong University, Shanghai, China,Department of Finance and Insurance, School of Business, Nanjing University, Nanjing, Jiangsu, China","Received 27 September 2020, Revised 17 December 2020, Accepted 25 January 2021, Available online 6 February 2021, Version of Record 23 February 2021.",https://doi.org/10.1016/j.jedc.2021.104083,Cited by (2),. Extensive numerical experiments illustrate the accuracy and effectiveness of our approach.,"Continuous-time jump-diffusion processes are widely used in derivatives pricing, among which affine models win popularity because of their analytical tractability. Empirical evidences also document that both nonaffine feature (see, e.g. Aıt-Sahalia, Kimmel, 2007, Andersen, Benzoni, Lund, 2002, Chernov, Gallant, Ghysels, Tauchen, 2003, Christoffersen, Jacobs, Mimouni, 2010, Durham, 2013, Ferriani, Pastorello, 2012, Jones, 2003, Kaeck, Alexander, 2012, Medvedev, Scaillet, 2007, Yang, Kanniainen, 2017) and (double exponential) jumps (see, e.g. Andersen, Fusari, Todorov, 2015a, Andersen, Fusari, Todorov, 2015b, Andersen, Fusari, Todorov, 2017, Kou, Yu, Zhong, 2017, Yang, Kanniainen, 2017) can fit return or/and option data. However, such models, e.g., a nonaffine stochastic volatility model with double exponential jumps, are lack of analytical formulas of their transition densities and option prices, which may limit their applicability when calibrating the model based on discretely observed return and option data. To overcome the difficulty, the seminal paper of Aıt-Sahalia (2002) develops the closed-form Hermite expansion for univariate diffusions.====In this paper we develop the Hermite expansion for multivariate diffusions with jumps, which converges as the time interval shrinks to zero. By introducing a quasi-Lamperti transform unitizing the process’ diffusion matrix at the initial time, we expand the transition density of the transformed process around the standard normal density and compute the coefficients via the Itô-Taylor expansion. Thanks to the structure property of the Hermite expansions of transition densities, we can easily obtain explicit expansions of European option price for multivariate diffusions with and without jumps.====Our contribution is two-fold. First, we show that a small-time Hermite expansion is feasible for multivariate diffusions. The introduction of the quasi-Lamperti transform, which reduces a multidimensional problem into independent one-dimensional problems, allows us to successfully develop the Hermite expansion for multivariate diffusions and establish its equivalence with other expansions. In detail, the Hermite expansion unifies the pathwise expansion of Li (2013) and the delta expansion of Yang et al. (2019) under a special choice of the parameter, see, e.g, Wan and Yang (2020). The equivalence provides new insights and enlarges the applicability for each single method. For example, applications of Li (2013)’s approach could be extended to the time-inhomogeneous case.====Second, we derive explicit recursive formulas for the expansion coefficients of transition densities and European option prices for multivariate diffusions with and without jumps, particularly the time-inhomogeneous non-affine models with double exponential jumps. To the best of our knowledge, the explicit recursive formulas especially for models with jumps are new to the literature. We can use some symbolic computing softwares such as Mathematica to speed up the calculation of the expansion coefficients. The symbolic calculation does not require any preprocessing because the derived recursive formulas are explicit, and readers can compute the formulas for specific models by themselves without taking too much effort.====Several alternative methods have already been proposed to derive closed-form approximations for the transition densities of multivariate diffusions. Aıt-Sahalia (2008) introduces the Kolmogorov method, which postulates an appropriate form for the (log-) transition densities as an expansion of both time and state and then uses the Kolmogorov equations to compute each expansion coefficient. Choi, 2013, Choi, 2015 extends this method to time-inhomogeneous diffusions. For time-homogeneous case, Li (2013) proposes a pathwise expansion of the Dirac delta function on the diffusion to approximate the transition density, where the expansion coefficient is computed by recursively calculating the conditional expectations of iterated stochastic integrals. Yang et al. (2019) approximate the transition densities using the Itô-Taylor expansion of the conditional expectation of the Dirac delta function, which is valid for both time-homogeneous and time-inhomogenous cases. With the help of the Hermite expansion developed in this paper, these methods can be classified into two categories: the Kolmogorov method (Aıt-Sahalia, 2008, Choi, 2013, Choi, 2015) and the Hermite method (Li, 2013, Yang, Chen, Wan, 2019, and this paper).====The Hermite method is shown to be essentially different from the Kolmogorov method of Aït-Sahalia (1999); Aıt-Sahalia (2008) and Choi, 2013, Choi, 2015. For example, for reducible diffusions, Aıt-Sahalia (2002) proves that the Hermite series expansion converges as the order of the Hermite polynomial tends to infinity. The (reducible) Kolmogorov method gathers the infinite terms of the whole Hermite series according to increasing powers of the time interval (i.e., ====) (cf. (4.10) and (4.11) in Aıt-Sahalia, 2002); while the Hermite expansion collects terms of a finite truncated Hermite series according to increasing powers of ==== (see also Theorem 1 in Lee et al., 2014; Proposition 5.1 in Yang et al., 2019). It is worth mentioning that the numerical performance of the Kolmogorov method and the Hermite method are comparable (see Li, 2013, Yang, Chen, Wan, 2019).====As to density expansions for jump diffusions, Yu (2007) extends the Kolmogorov method to cover models with jumps. Filipović et al. (2013) provide orthogonal polynomials based expansion for affine jump-diffusions. Li and Chen (2016) develops pathwise expansion for time-homogeneous jumps-diffusions. As a comparison, we provide the explicit recursive expansion formulas for (possibly nonaffine, time-inhomogeneous) diffusions with jumps.====Our research is also related with literatures of analytic approximations for European option prices. For reducible models, Xiu (2014) use the Hermite expansion method developing closed-form series expansion of European option prices, which converges to the true option prices for a fixed time interval. However, his method may have difficulty in obtaining analytical approximations for multivariate reducible diffusions because the multivariate integral may not be simplified to a one-dimensional integral by changing variables. In contrast, we succeed in obtaining closed-form approximation of European option prices under both reducible and irreducible multivariate models because the quasi-Lamperti transform keeps the orthogonality of the Hermite polynomials. For multivariate (irreducible) diffusions, Kristensen and Mele (2011) approximate the option price by expanding the difference between the true model price and the Black-Scholes price. Li (2014) derives the asymptotic formula for the option price using the pathwise expansion, where the expansion coefficient is computed via the method of Li (2013). None of them are related to Hermite polynomials. In contrast, the expansion of option prices in this paper consists of Hermite polynomials. Moreover, the Hermite expansion admits explicit recursive formulas and can cover the models with jumps. One can also refer to Bompis and Gobet (2013) for an overview of different expansion methods for European option prices such as the singular perturbation method and the small noise expansion method. As noted by Bompis and Gobet (2013), the calculation of the expansion terms are very involved. For example, one needs to solve a hierarchy of partial differential equations (PDEs) to get some high order expansion terms. In contrast, the Hermite expansion derived in this paper provides explicit expansion formulas for high order terms.====Instead of European option prices, some literatures focus on expansions for implied volatilities (Gatheral, 2011). For stochastic volatility models with jumps, Medvedev and Scaillet (2007) provide low-order expansion of the implied volatility with respect to the time-to-maturity and the moneyness degree. By inverting the approximate option price formula, Aıt-Sahalia et al. (2020a) derive a closed-form bivariate expansion of the implied volatility surface in terms of time-to-maturity and log-moneyness generating by a (multifactor) stochastic volatility model with jumps in return, using the expansion methods of Li, 2013, Li, 2014. Aıt-Sahalia et al. (2020b) applies the expansion to advocate an implied stochastic volatility model from market dada. One can refer to Aıt-Sahalia, Li, Li, 2020a, Aıt-Sahalia, Li, Li, 2020b for more literaures about implied volatility expansions. There are also literatures deriving implied volatility based on the PDE approach, see, e.g., Berestycki, Busca, Florent, 2002, Berestycki, Busca, Florent, 2004, Lorig et al. (2017). Comparing with these literatures, we provide explicit recursive formulas for computing high order terms, thus preprocessing for symbolic calculation is not necessary. Based on the equivalence between the expansion of Li (2013) and the Hermite expansion, our recursive formula has the potential to be used to simplify the analysis of Aıt-Sahalia et al. (2020a). Moreover, our method is applicable to time-inhomogeneous models.====The true transition density is also known as the heat kernel on a Riemannian manifold in differential geometry. The heat kernel expansion is an alternative method to approximate the true density (see, e.g., Henry-Labordere (2009) for applications in finance), but the computation of the heat kernel coefficients is quite complicated. Hagan et al. (2015) and Paulot (2015) apply the heat kernel expansion to derive three and two terms asymptotic expansions for transition density, option price, and implied volatility of the SABR model (Hagan, Kumar, Lesniewski, Woodward, 2002, Yang, Chen, Liu, Wan, 2017). However, it is difficult for them to compute high order terms. In comparison, we provide explicit recursive formulas for high order terms in the Hermite expansion. Even though the leading term of the Hermite expansion may not be as close to the true density as that of the heat kernel expansion, the convergence result of the Hermite expansion ensures that we can achieve the required accuracy of approximation by increasing the number of the expansion terms.====The rest of the paper is organized as follows. Section 2 develops the Hermite expansion for transition densities of multivariate diffusions and proves the convergence, whereas the explicit formulas are presented in Section 3. Section 4 derives an explicit asymptotic formula for the European option price under multivariate diffusions. Section 5 adds jumps to the log-return process, and derives explicit asymptotic formulas for the transition density and the European option price under multivariate diffusions with either normal jumps and double exponential jumps. Examples and numerical results are presented in Section 6. Section 7 concludes the paper. Technical lemmas and proofs are collected in the Appendix.====In this paper we will use the following notational conventions. Let ==== be a positive integer representing the dimension of the state variable. Let ==== be the set of ====-dimensional integers, and ==== be the subset of ==== the element of which has nonnegative component. For ==== define ==== and ====. Let ==== be a special index vector, in which the ====-th component is 1, and the others are 0. We write ==== for any ==== where ==== denotes transposition. Let ==== and ==== be a vector and a matrix respectively. We use either ==== or ==== to denote the ====-th element of the vector ==== without confusion. Similarly, denote the ====-element of the matrix ==== to be ==== or ====. For ==== let ==== denote the smallest integer larger than or equal to ==== and ==== denote the largest integer smaller than or equal to ====. Let ==== denote the density of the standard ====-dimensional multivariate normal distribution with mean 0 and identity covariance matrix, and let ==== denote the corresponding multivariate Hermite polynomial; that is, ==== where ====. In particular, ==== where ==== is the ====-th order standard univariate Hermite polynomial.",Hermite expansion of transition densities and European option prices for multivariate diffusions with jumps,https://www.sciencedirect.com/science/article/pii/S016518892100018X,6 February 2021,2021,Research Article,235.0
"Liu Xia,Liu Shanchun,Lu Lei,Shi Yongdong,Xiong Xiong","School of Economics and Management at the University of Science and Technology Beijing, China,School of Economics and Management at Beihang University, China,Asper School of Business at the University of Manitoba, Canada,School of Applied Finance and Behavioral Sciences at Dongbei University of Finance and Economics, China,College of Management and Economics at Tianjin University, China","Received 10 June 2020, Revised 24 January 2021, Accepted 25 January 2021, Available online 3 February 2021, Version of Record 14 February 2021.",https://doi.org/10.1016/j.jedc.2021.104081,Cited by (5),"We present a model in which an insider (i.e., manager or CEO) and an informed outsider (i.e., financial analyst or professional) have heterogeneous beliefs on their shared information about a risky asset and analyze the insider's incentive to voluntarily disclose this information to the public. We find that with heterogeneous beliefs the insider and informed outsider exploit their shared information differently and this might give rise to the insider's voluntary disclosure of this shared information to the public to seek excess profits. Specifically, the insider is more likely to release the information to the public when she has a greater relative information advantage than the informed outsider and that the informed outsider is more optimistic in the shared information. Our findings shed light on why some firm insiders prefer to trade against informed outsiders while others prefer to drive informed outsiders out of trading through voluntary disclosure.","Since the enforcement of the SEC Act of 1934, listed firms have been required to disclose information to the public. While Fishman and Hagerty (1995) argue that firm insiders’ disclosure cannot be voluntary and must be mandatory, other papers demonstrate that firm insiders (i.e., managers or CEOs) may strategically choose voluntary disclosure under certain conditions, for example, when in hope of enhancing the liquidity of listed stocks (Hong and Huang, 2005) or when competing with informed outsiders (i.e., financial analysts or professionals) in the market (Bushman and Indjejikian, 1995; Kumar et al., 2016). As is known, an informed outsider can have access to some private information on a stock's intrinsic value that is also observed by a firm insider. However, the informed outsider might interpret the information differently than the insider due to rational inattention or different prior beliefs, thus causing him to disagree with the insider on the perception of the information. In this scenario, how would market outcomes be affected by the difference in beliefs between the insider and the informed outsider? Does the insider have an incentive to strategically disclose the information shared with the informed outsider to the public?====To address these problems, we develop a typical noncompetitive Kyle-type model (Kyle, 1985) to analyze a firm insider's strategic information disclosure. In this model, an insider and an informed outsider trade with liquidity traders on a risky asset whose liquidation value consists of two independent fundamentals. The insider observes a private signal on the payoff of the first fundamental, and both the insider and informed outsider receive a signal on the payoff of the second fundamental. However, the insider and informed outsider have heterogeneous beliefs in their shared signal on the second fundamental.==== In particular, both of them process the shared information rationally, but explain it differently due to rational inattention (Sims 2003; Nimark and Sundaresan, 2019) or different information processing ability (Kim and Verrecchia, 1994; Crego, 2020; He et al., 2021). Thus, the insider and informed outsider disagree on the relation between the realized value of the shared signal and the payoff of the second fundamental. The insider perceives the shared signal correctly while the informed outsider might overvalue or undervalue the payoff of this fundamental conditional on shared signal.====We find that with heterogeneous beliefs the insider and the informed outsider trade on their shared information differentially in equilibrium. When the informed outsider is more optimistic (pessimistic), he trades more (less) aggressively on the shared information and thus explores more (less) profits from the second fundamental than the insider. In addition, the difference of beliefs affects the market maker's response to the aggregate order flow – the price sensitivity of the market.==== Hence, the insider's expected profits are affected by the informed outsider's beliefs. Considering that the informed outsider competes against and shares profits with the insider in the trading game, the insider might choose to release the shared information to the public before trading to earn more profits (Bushman and Indjejikian, 1995).====The insider's voluntary disclosure is motivated by her desire to improve her expected profits. The reasoning is as follows. Even though the disclosure of shared information dilutes the insider's absolute information advantage, this disclosure reduces the informed outsider's information advantage as well. Therefore, the insider's disclosure decision actually forces the informed outsider to split the cost of information disclosure with her. When information is disclosed, the information asymmetry between the informed traders (including the insider and informed outsider) and the market maker will be reduced. The price sensitivity of the market is thus decreased, resulting in changes in the profits of both the insider and informed outsider. The trade-off between the decrease in the insider's information advantage and the reduction of the market maker's reaction to the order flow determines the insider's information disclosure choice. When the benefit of disclosure outweighs the cost of disclosure for the insider, she will strategically choose to disclose the shared information to the public. Moreover, disclosure of shared information can eliminate the informed outsider's information advantage relative to the market maker, and thus drive the informed outsider out of the trading game, and so help the insider exploit alone from her exclusive private information about the first fundamental, which may also be more profitable than a situation with no information disclosure.====We find that not only the uncertainty of the two fundamentals but also the heterogeneity in beliefs between the insider and the informed outsider determine the insider's choice of voluntary information disclosure. When the informed outsider has the same belief as the insider about the shared information, the insider never chooses to disseminate the shared information to the public, as that would harm her profits. In contrast, when they have different beliefs, the insider has the potential to choose pre-trade voluntary disclosure to earn more profits. More specifically, when and only when the informed outsider is more optimistic in the shared information, and the variance ratio of the first fundamental (for which insider has exclusive private information) to the second one (for which both insider and informed outsider share the same information) is sufficiently large, the insider has an incentive to release the shared information to the public. The informed outsider's optimism in the shared information renders the insider more fierce competition, and then squeezes the insider's profit space. As a consequence, the insider may reveal the shared information, the only private information received by the optimistic informed outsider before trading, to the public to amplify her expected profits if the uncertainty of the first fundamental is large or the uncertainty of the second fundamental is small.====We discuss three extensions of our main model. First, the informed outsider does ==== change his beliefs about shared information even if the insider discloses it to the public. In this case, the informed outsider still participates in the market with his initial beliefs about shared information, but earns zero expected profits. The insider still has an incentive to release the shared information to the public when the informed outsider is optimistic in the shared information and when her relative information advantage is sufficiently large. This has the same results as the main model. Second, the insider ==== discloses the shared information to the public. This alters both the insider's and informed outsider's profiting abilities from the residual information of the shared information, improves market liquidity, and thus affects the insider's expected profits. In this case, the insider still tends to voluntarily disclose share information to the public. Third, we generalize the parameters of the model. In general, the results of these three extensions are qualitatively the same as the findings of the main model.====Our paper relates to three strands of literature. First, some papers show that a well-informed trader chooses to disclose information to the public (Bushman and Indjejikian, 1995; Kumar et al., 2016) or to selective traders (Indjejikian et al., 2014) to extract more profits when facing fierce competition from a sufficiently large number of less-informed outsiders. However, we find that even when trading against only a small number of less-informed outsider, the insider still might choose voluntary information disclosure to maximize her expected profits due to the less-informed trader's different opinions. This finding indicates that the informed outsider's heterogeneous beliefs might cause the insider to give up profiting from the shared information and thereby voluntarily disclose it to the public. The impetus behind voluntary disclosure illustrated in our paper differs from that given in Bushman and Indjejikian (1995) and Hong and Huang (2005). In Bushman and Indjejikian (1995), the fierce competition comes from a large number of rational, less-informed traders. However, the competition stems from the outsider's optimism in our paper. Hong and Huang (2005) document that firm insider adopts voluntary disclosure as an investor relation policy to improve the liquidity of firm shares instead of earning trading profits. In our paper, the insider's voluntary disclosure is driven by her incentive to extract more profits through affecting price sensitivity to trade and thus changing profit allocation between the insider and informed outsider. Xiong and Yang (2020) study information sharing between strategic investors who have private information about an asset's fundamental and find that the coarsely informed investor gains from the information sharing but that the well-informed investor loses. Unlike their paper, we demonstrate that the insider can be better off when she strategically discloses shared information to the public when competing with an informed outsider.====Second, some studies have found that informed traders sell information to others for fees (Cespa, 2008; Garcia and Sangiorgi, 2011; Bimpikis et al., 2019) or spread rumors or incorrect information to manipulate the market (Van Bommel and Rumors, 2003; Cheng and Lo, 2006; Beyer and Guttman, 2012). Unlike these works, we argue that the insider's information disclosure is ==== and ==== as stated in Langberg and Sivaramakrishnan (2010). In our paper, the information is disclosed by insider to all of market participants, rather than to some specified traders (Brunnermeier, 2005; Indjejikian et al., 2014; Bimpikis et al., 2019; Kurov et al., 2019). Moreover, unlike Fishman and Hagerty (1995) and Huddart et al. (2001), a voluntary disclosure decision is made before insider's trading in our paper rather than after the trading.====Finally, our paper is also related to studies on heterogeneous beliefs or differences in opinion.==== These works mainly investigate the implications of heterogeneous beliefs for traders’ speculation behaviors and asset pricing rather than voluntary information disclosure. To our knowledge, Thakor (2015) is the most related work to our paper. He investigates a firm manager's decision on whether to disclose information on investment strategy to potential investors to attract these investors’ financing to the firm's new investment project when investors disagree with the manager on investment strategy for the new project. This paper follows the representation theorem developed by Grossman (1981) and Milgrom (1981) and adopts probability theory to solve the model. Unlike Thakor (2015), we follow the market maker framework of Kyle (1985) and Admati and Pfleiderer (1988) and explore each participant's trading behaviors and analyze the more informed trader's strategic information disclosure when the more and less-informed traders disagree on a signal about the payoff of the traded asset.==== Thakor (2015) also argues that firms with higher levels of agreement disclose less information whereas differences in opinion considered in our model constitute just one of the key factors that contribute to the insider's choice of voluntary information disclosure. Our paper also relates to Basak (2000) and Scheinkman and Xiong (2003) in that traders “agree to disagree” on their expected values of fundamentals.====The rest of this paper is structured as follows. Section 2 presents the model setup and Section 3 explores the benchmark model when the informed outsider and insider have the same beliefs. Section 4 analyzes the equilibrium and insider's choice of information disclosure when the informed outsider and insider have different beliefs. Three extensions of the model are discussed in Section 5. Section 6 concludes.",Voluntary information disclosure with heterogeneous beliefs,https://www.sciencedirect.com/science/article/pii/S0165188921000166,3 February 2021,2021,Research Article,236.0
"Phaneuf Louis,Victor Jean Gardy","Department of Economics, Université du Québec à Montréal, Canada,Department of Credit Risk Modeling, Desjardins Group, Canada","Received 26 February 2020, Revised 18 January 2021, Accepted 19 January 2021, Available online 29 January 2021, Version of Record 12 February 2021.",https://doi.org/10.1016/j.jedc.2021.104076,Cited by (2),We compare the cyclical and welfare implications of positive trend ,"Nominal wage and price contracting has been a key ingredient of macroeconomic models with rational expectations for more than forty years now, the two most popular approaches being those proposed by Taylor (1980) and Calvo (1983).==== These models generally assumed zero steady-state inflation, perhaps due to perceptions that low levels of trend inflation like those experienced since the mid-1980s have small cyclical and welfare implications. Based on a comparison of the cyclical and welfare effects of positive trend inflation in dynamic stochastic general equilibrium (DSGE) settings embedding Taylor and Calvo contracts, we intend to show this is not the case.====A few studies compared the effects of positive trend inflation with Taylor and Calvo nominal contracts. These models generally combined sticky prices with perfectly flexible nominal wages. Kiley (2002) showed that the welfare costs of price rigidity are higher with Calvo contracts because they generate higher price dispersion. Ascari (2004) found that positive trend inflation has a stronger and more persistent effect on the impulse response of output to a monetary policy shock and generates higher steady state output losses with Calvo contracts.====More recently, Coibion and Gorodnichenko (2011) compared how trend inflation affects the prospect of indeterminacy in Calvo and Taylor price setting models. They showed that with Taylor contracts, determinacy can be achieved with a lower policy response to inflation. Coibion et al. (2012) compared how both types of contracts affect optimal inflation with positive trend inflation.====These studies have the following points in common. First, they assume that higher price dispersion is the key mechanism by which positive trend inflation has cyclical and steady state implications. However, the evidence in Nakamura et al. (2018) questions the existence of a positive relation between price dispersion and trend inflation. Using a sample of disaggregated data on price changes during the late 1970s and early 1980s when average inflation was high, they find no evidence that price dispersion is significantly higher when trend inflation is high. These findings lead them to conclude that inflation costs are “elusive” and that the standard New Keynesian model is “wrong”.====Second, previous studies that compared Taylor and Calvo contracting frameworks have assumed that the average frequency of price adjustment is equal in both models. However, Dixon and Kara (2006) questioned whether this is the correct basis of comparison between the two contracting frameworks. They showed that equating the average frequency of price adjustment in both models amounts to compare two different magnitudes: the average age of Calvo contracts and the average lifetime of Taylor contracts. They suggest that the right comparison is one between the ==== in both models. We keep this basis of comparison for the two contracting frameworks.====We propose a medium-scale DSGE framework that includes either Taylor or Calvo nominal wage and price contracts. Besides positive trend inflation and nominal rigidities, our model includes intermediate goods, real per capita output growth stemming from trend growth in total factor productivity (TFP) and investment-specific technology (IST), real adjustment frictions and monetary policy modeled as a Taylor-type of rule. We show that with sticky wages, the key relation driving the cyclical and welfare implications of positive trend inflation is the one between higher trend inflation and increased wage dispersion.====We report three main findings regarding the cyclical impact of trend inflation with Taylor or Calvo nominal contracts. First, when comparing the empirical relevance of both contracting models by their ability to match some standard business cycle moments, we find that Taylor contracts better match the correlations between nominal variables (inflation and the nominal interest rate) and real variables (output growth, consumption growth, investment growth and change in hours). While the contemporaneous correlations between nominal and real variables are mildly negative in the data, they are either of the wrong sign or significantly off the mark with Calvo contracts. By comparison, they are broadly consistent with the actual correlations with Taylor contracts. As we show, these findings prevail whether TFP or MEI (marginal efficiency of investment) shocks are most important in driving business cycle fluctuations.====Second, we find that Taylor contracts can generate persistent and hump-shaped responses of inflation to monetary policy and MEI shocks, and this without having to resort to the quarterly indexation of nominal wages and prices to past inflation. Calvo contracts do not.====Third, positive trend inflation has significant distorting effects on the shock impulse-responses of key variables with Calvo contracts, and this even if trend inflation is moderate. Therefore, omitting the distorting effects of trend inflation can lead to significant biases in the shock impulse-response functions in models with Calvo contracts. By contrast, impulse responses are almost completely immune to these distorting effects with Taylor contracts.====There are also striking differences concerning the welfare implications of positive trend inflation in both contracting models. We assess the severity of monopolistic distortions with Taylor and Calvo contracts. We provide a quantitative assessment of the impact of trend inflation on price dispersion, wage dispersion and the welfare costs of inflation.====First, for a plausible calibration of the Taylor and Calvo models, we provide evidence of a weak correlation between price dispersion and trend inflation. From this perspective both models are not inconsistent with the evidence offered by Nakamura et al. (2018).====Second, we show that trend inflation exacerbates wage dispersion. However, trend inflation has a much stronger distorting effect on wage dispersion with Calvo contracts. For example, with 4% trend inflation relative to a zero inflation, steady state wage dispersion increases by 33% with Calvo contracts and by 4% with Taylor contracts. The welfare costs of trend inflation are thus much higher under Calvo contracts than Taylor contracts.====Third, we identify the key factors behind the high sensitivity of wage dispersion and welfare costs to trend inflation with Calvo contracts. These factors are the average age of nominal wage contracts, real per capita output growth, the degree of substitution between labor skills, and the inverse Frisch elasticity of labor supply. Wage dispersion and inflation costs are much more stable to variations in these factors with Taylor contracts.====Fourth, we provide a quantitative evaluation of the welfare gains resulting from lower trend inflation during the Great Moderation. Based on Taylor nominal contracting, we find that the steady state welfare gains resulting from the reduction in trend inflation after the early 1980s range from 3====1% to 7====5%. Since the Fed is generally held accountable for the lower level of trend inflation recorded after the early 1980s, we argue the welfare gains arising from a lower level of inflation were potentially important.====The rest of the paper is organized as follows. Section 2 presents our medium-scale DSGE model with Taylor nominal contracts. Section 3 discusses issues related to the calibration of both contracting models. Section 4 looks at how both models fit some standard business cycle moments in the data. Section 5 looks at the welfare implications of positive trend inflation. Section 6 concludes with final remarks.",On time-dependent nominal contracting models with positive trend inflation,https://www.sciencedirect.com/science/article/pii/S0165188921000117,29 January 2021,2021,Research Article,237.0
"Clements Michael P.,Galvão Ana Beatriz","ICMA Centre, Henley Business School, University of Reading, UK,Warwick Business School, University of Warwick, UK","Received 15 September 2020, Revised 18 December 2020, Accepted 13 January 2021, Available online 28 January 2021, Version of Record 12 February 2021.",https://doi.org/10.1016/j.jedc.2021.104075,Cited by (10)," may never be observed. We show that expectations shocks – revisions in GDP expectations unrelated to changes in current economic fundamentals and orthogonalized to other, potentially related shocks – explain 7–8% of the two-year variation of output, investment, consumption and hours. This is similar to the proportion of business-cycle variation explained by monetary shocks, for example.","There is a venerable literature on the effects of changes in agents’ beliefs about the future on economic fluctuations dating back at least to Pigou (1927) and Keynes (1936). Changes in expectations may drive business cycles because they reflect aspects of some or all of the following: news about future fundamentals (Beaudry and Portier (2014)); fluctuations in sentiment (Milani (2017)); changes in beliefs (Angeletos et al. (2018)). Or they might constitute in part an independent source of fluctuation. To disentangle expectations-driven shocks from other fundamental shocks, a number of different identification strategies have been proposed in the literature, including for technology news shocks (Barsky and Sims (2011)), sentiment shocks (Levchenko and Pandalai-Nayar (2017), Fève and Guay (2016) and Lagerborg et al. (2020)), and confidence shocks (Barsky and Sims (2012)). Angeletos et al. (2018) evaluate the effects of confidence shocks using an empirical DSGE model. They claim that an empirical strategy based on vector autoregressive models and a measure of confidence would have difficulties in identifying confidence shocks because one needs to remove the effects of all anticipated future fundamental shocks.====We do not directly contribute to the literature on the ==== of shocks, but instead we propose a method to improve the ==== of expectations-driven business cycles by addressing two issues caused by data uncertainty.==== The first one is that agents only observe initial estimates of current and recent macroeconomic variables when they form their expectations. The second is that we wish to employ ‘true values’ on macroeconomic variables when computing dynamic responses but these ‘true values’ may be never be revealed, as national accounts data may be always measured with error.====Our solution is a two-step method. In the first step, we estimate the historical values of expectations shocks using US SPF (Survey of Professional Forecasters) forecasts of output growth in a real-time mixed-frequency VAR model. The real-time mixed frequency VAR model includes monthly economic and financial indicators and it is estimated using only data available at the time expectations were formed, or, in other words, using initial estimates of variables such as GDP and industrial production. We recover historical expectations shocks by applying a recursive identification approach supported by the timings of economic data releases to the Bayesian estimated real-time mixed-frequency VAR model.====In the second step, we compute dynamic responses of key macroeconomic aggregates such as consumption, investment and hours to expectations shocks assuming macroeconomic aggregates are measured using their ‘true’ values. As the observed data on the macroeconomic variables is contaminated by measurement errors, we show how to consistently estimate the dynamic responses under two assumptions about the measurement error. The first one is that after a number of rounds of revisions the ‘true’ values are revealed (as in, e.g., Cunningham et al. (2009), Kishor and Koenig (2012) and Garratt et al. (2008)), and the second is that ‘true’ values are never observed (as in, e.g., Jacobs and van Norden (2011)).====Our empirical results suggest that expectations shocks lead to business cycle comovement across macroeconomic variables. Expectations shocks explain 8% of the two-year forecast-error variance decomposition of output and investment even after purging the effects of technological news (Barsky and Sims (2011)), confidence shocks (Barsky and Sims (2012)), and monetary policy surprises (Gertler and Karadi (2015)). Our orthogonalized expectations shocks might capture non-technological news shocks, and surprises to sentiment.====We show that the autonomous changes in expectations, that is, expectations shocks, or their impact on the macroeconomy, will not be consistently estimated if data uncertainty on the economic activity variables is ignored. Data uncertainty not only makes it impossible to disentangle expectations shocks from fundamental shocks using VARs, but may also lead to inconsistent estimates of the parameters by OLS depending on the properties of data revisions. If data revisions add ‘news’ to the initial statistical office estimates (see, e.g., Mankiw and Shapiro (1986) or Clements and Galvão (2019)) then OLS estimates will be inconsistent.====The expectations shock can, however, be consistently estimated using real-time data. Using real-time data avoids ‘look-ahead’ bias. But we also need to use the key information the forecasters have access to when they form their expectations. Some studies based only on quarterly data neglect the higher-frequency data which in practice may be a valuable source of information. For forecasting GDP, for example, key monthly indicators such as industrial production and non-farm payroll employment for the first month of the quarter in question will generally provide relevant information, and reflecting this receive much media attention. Ignoring relevant information would lead to incorrect measurement of autonomous changes in expectations. To estimate expectations shocks, we consider mixed-frequency VAR (MF-VAR) models (see, e.g., Ghysels (2016)) enabling the monthly data to be included in a convenient way.====The appropriate method of estimating the response of key macroeconomic variables depends on whether we want to estimate the responses of initial estimates or ‘true values’. Responses for first releases can be obtained directly using the real-time mixed-frequency VAR. If, however, the researcher wishes to measure the macroeconomic responses using the “true’ values of the variables, then our suggestion is to add the historical values of the expectations shocks to a VAR where the variables are measured at their “true values”. Responses can then be calculated assuming recursive identification based on the timing of data releases. A remaining issue to address is how to obtain the “true values” of variables such as GDP, TFP, consumption, investment and hours. If we are willing to suppose that the true values of the variables subject to revision are eventually revealed, then we can simply discard the more recent observations from the latest data release. This will exclude the values still subject to revision, which if kept could result in inconsistent OLS estimates. Alternatively, if we assume that the true values are never revealed, an instrumental variable approach is required to accomplish the same end (as proposed by Croushore and Evans (2006)), and delivers consistent estimates under some weak assumptions on the nature of revisions, as we discuss.====The advantage of applying a VAR model to compute dynamic responses is that the computation of forecast error variance decompositions is straightforward. Qualitatively similar empirical results are obtained using instead a local projection approach. Dynamic responses can be computed by local projections under the assumption that “true values” are not observed, and the instrumental variable strategy also applies in this setting.====As well as setting out a valid approach when there is data uncertainty, we show that estimated responses may be affected both quantitatively and qualitatively if this approach is not followed. If real-time, monthly data is not used to measure expectations shocks, the dynamic effects on output, consumption, investment and hours will be incorrectly confined to the first year after the shock, and the responses of prices and the short-rate are found to be significantly positive. Estimating expectations shocks allowing agents to have observed the shocks that have taken place during the first month of the quarter is one reason why are findings are qualitatively different.==== In addition, data revisions to key macroeconomic data are far from negligible (see, e.g., Aruoba (2008)), and we show that whether ‘true” values are assumed to be eventually observed may alter our assessment of the long-run effects of expectations shocks.====We provide evidence that expectation shocks are a largely complementary source of business cycle fluctuation. Expectations shocks are mildly correlated with alternative belief-based shocks (technological news and confidence) and monetary policy surprises, and are not correlated with oil and defense shocks. Expectations shocks lead to significant cyclical comovements across macroeconomic aggregates even after they are made orthogonal to technological news shocks, confidence shocks, and monetary policy surprises. The proportion of the GDP growth variance decomposition explained by expectations shocks appears to be small (8%), but is in line with the findings of Bianchi et al. (2020). However, our measured expectations shocks may be proxying other expectations-based shocks, such as news shocks about alternative economic fundamentals, and sentiment shocks. Indeed, SPF forecasts have been employed previously in the estimation of DSGE models to pin down expectations and to help the measurement of news shocks (Miyamoto and Nguyen (2019)) and sentiment shocks (Milani (2017)). Empirically, we find that our economic activity responses resemble those for confidence shocks found by Angeletos et al. (2018) in a DSGE model, and for consumer sentiment shocks by Lagerborg et al. (2020).====The plan of the rest of the paper is as follows. Section 2 discusses the calculation of the expectations shocks using a real-time VAR. Section 3 explains how we determine the responses of the macroeconomy to the shocks, and presents the results. In Section 4 we explore the relationship between the expectations shocks and alternative belief-based shocks. Finally, Section 5 offers some concluding remarks.",Measuring the effects of expectations shocks,https://www.sciencedirect.com/science/article/pii/S0165188921000105,28 January 2021,2021,Research Article,238.0
"Li Yifan,Nolte Ingmar,Nolte Sandra","Alliance Manchester Business School, University of Manchester, Booth Street West, Manchester M15 6PB, UK,Lancaster University Management School, Lancaster University, Bailrigg, Lancaster LA1 4YX, UK","Received 26 August 2020, Revised 10 December 2020, Accepted 23 January 2021, Available online 28 January 2021, Version of Record 6 February 2021.",https://doi.org/10.1016/j.jedc.2021.104077,Cited by (2),"We develop a Markov-Switching Autoregressive Conditional Intensity (MS-ACI) model with time-varying transitional ====, and show that it can be reliably estimated via the Stochastic Approximation Expectation–Maximization algorithm. Applying our model to high-frequency transaction data, we detect two distinct regimes in the intraday volatility process: a dominant volatility regime that is observable throughout the trading day representing the risk-transferring trading activity of investors, and a minor volatility regime that concentrates around market liquidity shocks which mainly capture impacts of firm-specific news arrivals. We propose a novel daily volatility decomposition based on the two detected volatility regimes.","Since the seminal work of Engle and Russell (1998), a growing amount of literature has emerged on parametric modeling of intraday financial data. An important strand of this literature concentrates on the parametric modeling of intraday price volatility using point processes, including Gerhard and Hautsch (2002), Tse and Yang (2012), Li et al. (2019), Hong et al. (2020), etc. These papers show that point process based volatility estimators can provide valid intraday volatility estimates that are comparable to the popular Realized Variance (RV) estimates of Andersen et al. (2001).====An interesting feature of the point process-based approach is that its parametric structure provides a useful framework to examine intraday volatility interaction with market microstructure covariates without compromising the quality of volatility estimates. Existing frameworks are either incapable of incorporating other variables due to their non-parametric design (e.g. RV framework), or are considered inappropriate for intraday volatility estimation (e.g. intraday GARCH framework).====Under the point process-based framework, we propose a Markov-switching extension to the Autoregressive Conditional Intensity (ACI) model of Russell (1999) for the modeling of intraday volatility. To the best of our knowledge, we are among the first to develop such an extension to a conditional intensity model. In fact, only few studies consider Markov-switching extensions to autoregressive models for point processes, e.g. Hujer et al. (2002), De Luca and Zuccolotto (2006), and Gallo and Otranto (2012), which are all developed from the Autoregressive Conditional Duration (ACD) model of Engle and Russell (1998). We therefore fill this gap by providing an intensity-based autoregressive model with a Markov-switching feature for the modeling of point processes.====The lack of Markov-switching autoregressive models for point processes is possibly due to the fact that a non-linear autoregressive structure is required to ensure the positivity of the durations and the conditional intensity. This introduces a ‘path dependency problem’ in the construction of the likelihood function,==== which greatly complicates the estimation of such model. The most widely applied strategy to circumvent the path dependency problem is to approximate the observed likelihood function by a feasible version, for example Gray (1996), Kim (1994), Dueker (1997), Klaassen (2002), and Haas et al. (2004) in a MS-GARCH framework, and Hujer et al. (2002), De Luca and Zuccolotto (2006), and Gallo and Otranto (2012) for the MS-ACD model. However, these approaches do not solve the path dependency problem directly, and the quality of these approximations is difficult to verify empirically (Billio et al., 2014).====Distinct from the aforementioned studies, a direct solution to the path dependency problem typically relies on simulation and data augmentation techniques. For example, Bauwens, Dufays, Rombouts, 2014, Bauwens, Preminger, Rombouts, 2010 and Billio et al. (2014) develop Bayesian estimation techniques for the MS-GARCH model. Augustyniak (2014) introduces a Monte Carlo Expectation–Maximization (MCEM) algorithm (Wei and Tanner, 1990) for maximum likelihood estimation of the MS-GARCH model. Inspired by these approaches, we develop a maximum likelihood estimator of the Markov-switching ACI (MS-ACI) model based on the Stochastic Approximation Expectation–Maximization (SAEM) algorithm of Celeux and Diebolt (1992) and Delyon et al. (1999), which overcomes the path dependency problem and provides estimates of the variance–covariance matrix of the estimated parameter and the most probable state vector. Our approach can be computationally more efficient than the method of Augustyniak (2014) as the SAEM algorithm utilizes the simulated data more efficiently than the MCEM algorithm (Delyon et al., 1999). Via simulation, we show that our SAEM algorithm provides empirically feasible and reliable parameter estimates of the MS-ACI model.====We apply the MS-ACI model to the high-frequency Trade and Quote (TAQ) data of ten frequently traded securities (including a market index ETF, SPY) for the year 2016, timestamped at milliseconds. We model the dynamics of price durations, that is, the amount of time for the log-price to change by a given amount, and examine the contemporaneous relationship between price durations and the cumulative trading volume within each price duration. Our MS-ACI model detects two distinct regimes in the intraday volume-duration relationship for individual stocks: a dominant regime that is observable throughout the trading day in which the duration and volume exhibits a strong power law relationship, and a minor regime that concentrates around large bid-ask spread events with a much weaker connection between price duration and volume. However, the latter regime cannot be observed for SPY, the stock index ETF. These findings are further corroborated by empirical results based on an extended sample that covers all Dow Jones constituents in 2016, which we present in the Supplementary material of the paper.====Our empirical findings provide new insights into the dynamics of intraday volatility processes. Firstly, we show that intraday volatility processes for individual stocks exhibit regime-switching behaviour on an intraday level, which is likely to be caused by unpredicted shocks to the liquidity state of the market as a result of firm-specific information arrivals. Secondly, we conjecture that the dominant regime discussed above is likely to summarize risk transfers between market participants, and the power law relationship between volume and price duration may hold across assets and time as a result of the Market Microstructure Invariance hypothesis of Kyle and Obizhaeva (2016). Finally, we propose a novel decomposition of daily volatility into two components based on the two detected regimes, which allows us to disentangle different driving forces of volatility on a daily basis.====The rest of this paper is structured as follows: Section 2 introduces some basic point process theory and the original ACI model. In Section 3, the specification and estimation technique of the MS-ACI model are discussed. Simulation evidence is provided in Section 4, with the empirical application following in Section 5. Section 6 concludes.",High-frequency volatility modeling: A Markov-Switching Autoregressive Conditional Intensity model,https://www.sciencedirect.com/science/article/pii/S0165188921000129,28 January 2021,2021,Research Article,239.0
Bianchi Daniele,"School of Economics and Finance, Queen Mary University of London, Mile End Rd, London E1 4NS, UK","Received 30 March 2020, Revised 19 January 2021, Accepted 22 January 2021, Available online 27 January 2021, Version of Record 14 February 2021.",https://doi.org/10.1016/j.jedc.2021.104078,Cited by (6),I investigate the determinants of the commodity futures risk premiums for different maturities through the lens of a model of ,"The way in which investors form expectations about future commodity prices is of great interest to economists and market participants at least since Keynes (1930). Forward prices have been used extensively in economic models as an approximation of market beliefs.==== However, the forward curve includes not only investors’ expectations for the future, but also a component reflecting the compensation required by market participants for bearing the risk of uncertain fluctuations in spot prices, namely a risk premium. Whether this risk premium is positive, negative, or time varying and driven by changes in economic fundamentals has been controversial in the commodity markets literature.==== This controversy stems from the fact that investors’ expectations are difficult to measure.====In this paper, I contribute to this debate and examine to what extent investor expectations in commodity markets can be thought of as the result of a belief updating scheme in which expected future spot prices are revised based on past prediction errors and changes in aggregate economic fundamentals. Theoretically, the starting point is a simple extension of the Muth (1961) market model with inventory speculation. In particular, I extend the baseline model by introducing a persistent demand shock and a futures market, upon the process of price formation. That is, buyers and inventory holders are assumed to hedge their commodity positions by trading on futures, such that the effect of hedging vs speculation is considered in equilibrium. In addition, I assume that investors have incomplete information on the price dynamics and recursively learn about it over time (see, e.g., Evans, Honkapohja, 2001, Sargent, 2002, Cho, Williams, Sargent, 2002 and Williams, 2003, among others).====Empirically, I approximate the time varying ==== risk premiums as the spread between the futures price as of date ==== with maturity ==== and the adaptive expectations at time ==== on future spot prices over the same time horizon. Contrary to some of the existing empirical evidence based on survey forecasts (see, e.g. Greenwood and Shleifer, 2014 and Koijen et al., 2015), I show that the ex-ante risk premiums extracted based on adaptive expectations positively correlate with the one-step ahead realised returns across different commodities and maturities, although the predictive power is not perfect.==== In addition, the main results show that risk premiums are substantially time varying both across commodities and time horizons, and tend to be positive during expansions and negative during markets downturns and recessions, where the futures curve is predominantly in backwardation across markets. Finally, I show that the model-implied risk premiums reflect the quantity of risk and/or the market price for bearing risks, as proxied by the realised variance and realised skewness of the returns, consistent with existing theoretical evidence (see, e.g., Bessembinder and Lemmon, 2002).====The presence of a risk premium in commodity futures is often based on the assumption that market activity in futures markets is primarily affected by hedging vs speculation activities from commercial and non-commercial traders.==== For example, if there is a net-short aggregate futures position by natural hedgers, i.e. consumers and producers, futures prices need to be set below the expected spot prices to induce speculators – who do not have a commercial exposure to hedge – to balance the market by taking the opposing long position. Therefore, one can see the risk premiums in commodity markets being essentially a function of the profits that speculators expect to receive and the hedging needs of commercial traders. I directly test this assumption by using a set of panel regression models as well as a standard panel Vector Autoregressive (VAR) time series model.====The empirical results confirm that the sample variation of the ex-ante commodity risk premiums is significantly driven by risk sharing decisions and past performances, as proxied by hedging pressure and time series momentum, after controlling for a set of aggregate macroeconomic variables. On the other hand, aggregate market activity proxied by open interest does not have a significant explanatory power for the in-sample variation of commodity risk premiums. Interestingly, the estimates from a panel VAR model show that the role of time series momentum is predominant with respect to hedging pressure. This result is consistent with the evidence in Moskowitz et al. (2012), which show past performance represents a strong predictor in commodity futures markets. More generally, the panel VAR estimates confirm that there is a significant feedback effect between risk premiums, hedging pressure and time series momentum.====The longer term effect of time series momentum on commodity risk premiums is further highlighted by the cumulative impulse response functions from the panel VAR. While shocks to hedging pressure and open interest do not have a significant effect on risk premiums in the long run, time series momentum has a persistent effect on risk premiums which goes far beyond the short-lived impact which has been typically documented in the literature. This is possibly due to the high autocorrelation of time series momentum constructed based on a five-year performance window (see, e.g., Asness et al., 2013).====Finally, I report in the appendix a set of additional results to highlight the suitability of adaptive expectations to model investors’ beliefs. In particular, I take the consensus from the Bloomberg’s survey individual forecasts for a small set of available commodities, and (1) compare the consensus with the model-implied predictions and (2) test for the departure of the consensus forecast from strong rationality. The results show that the null hypothesis of adaptive expectations cannot be rejected since the average forecasts put positive weight on previous prediction errors, that is, expectations adapt to past information in a way that departs from strong rationality (see, e.g., Pesaran and Weale, 2006). Yet, the results show that the survey consensus forecast and the model-implied expected spot prices are broadly consistent over time and maturities.====This paper contributes to three main strands of literature. First, this work relates to a number of existing studies such as Nerlove (1958), Evans and Honkapohja (2001), Sargent (2002), Sargent and Williams (2005), Malmendier and Nagel (2015), and Bordalo et al. (2017), which consider recursive learning dynamics to model expected inflation, credit cycles, and expectations on general macroeconomic outcomes. In particular, Singleton (2014) suggests that trading activity is the result of an adaptive process in which hedgers and speculators learn about economic fundamentals, both from public information and market prices. Building on this intuition, this paper extends the existing literature by postulating an adaptive learning mechanism for future commodity spot prices which is consistent with a “learning from past errors” scheme.====Second, this paper relates to a recent literature that aims at understanding the main driving factors in commodity risk premiums such as Carter et al. (1983), Bessembinder (1992), De Roon et al. (2000), Bessembinder and Lemmon (2002), Acharya et al. (2010), Hong and Yogo (2012), Asness et al. (2013), Basu and Miffre (2013), Yang (2013), Hamilton and Wu (2014), Szymanowska et al. (2014), Bakshi et al. (2019), Koijen et al., 2018, Giampietro et al. (2018), Boons and Prado (2019), and Kang et al. (2020). Similar to Boons and Prado (2019), I show that the exposure to volatility captures a negative price of risk in commodity markets, which implies that investors are willing to pay for insurance against increases in volatility. I extend this result by showing that, instead, the exposure to skewness captures a positive price of risk, which suggests that downside risk has a significant role in pricing commodity markets (see, e.g., Bianchi, 2018). Yet, while Boons and Prado (2019) is primarily based on a non-parametric approach, in this paper I specify a full-blown learning dynamics of investors’ expectations to extract ex-ante risk premiums in the spirit of Singleton (2014). Similar to Kang et al. (2020), I show that there is a significant and positive relationship between hedging pressure and risk premiums. Yet, I show that hedging pressure and momentum can have non-overlapping information content for the dynamics of such premiums. However, I depart from Kang et al. (2020) along three main dimensions: first, I extract the ex-ante risk premiums vs looking at ex-post realised returns, which directly embeds the problem of how investors’ elicit their beliefs about and future spot prices; second, I provide a micro-founded characterization of such belief formation process; and third, I look at the long term, business cycle, variation in the effect of both hedging pressure and time series momentum on risk premiums.====Finally, the results in the paper can provide additional insights to understand which risk factors price the cross-section of commodity returns. While providing a joint pricing kernel specification across asset classes is beyond the scope of the paper (see, e.g., Giampietro et al., 2018), the main result that hedging vs speculation decisions – and their interplay with past performances – drive risk premiums, can shed further light on one of the controversies in the literature, that is whether hedging pressure primarily affect expected payoffs in commodity markets (see, e.g., Gorton et al., 2013).====The rest of the paper is organized as follows. Section 2 outlines the model of adaptive expectations. Section 3 describes the data and the variables of interest. Section 4 reports the main empirical results. Section 5 concludes. I leave to the appendix the model derivation and further results on investors’ beliefs and the comparison between the model-implied expectations and professional consensus forecasts.",Adaptive expectations and commodity risk premiums,https://www.sciencedirect.com/science/article/pii/S0165188921000130,27 January 2021,2021,Research Article,240.0
"Liu Keqing,Fan Qingliang","MOE Key Laboratory of Econometrics, The Wang Yanan Institute for Studies in Economics, Department of Finance, School of Economics, Fujian Key Lab of Statistics, Xiamen University,Department of Economics, 903, Esther Lee Building, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong","Received 4 May 2020, Revised 4 January 2021, Accepted 5 January 2021, Available online 15 January 2021, Version of Record 23 January 2021.",https://doi.org/10.1016/j.jedc.2021.104066,Cited by (3),"This paper studies the links among credit supply expansion, commercial bank asset account structures, and the housing boom preceding the 2007–2009 financial crisis. We propose a ====; and (4) an increase in the ratio of mortgages to firm loans in commercial bank asset accounts. In our model, a credit supply expansion to banks can also generate a boom-bust cycle through the collateral value channel via mortgage borrowers. Asset-side bank regulations that reduce excessive mortgage issuance during a credit boom can help to dampen the subsequent economic downturn.","Before the 2007–2009 financial crisis, the US economy experienced an unprecedented housing and mortgage boom. The following three empirical facts have been observed regarding this boom: first, housing prices increased dramatically; second, the mortgage-to-GDP ratio increased in parallel with house prices; third, the real mortgage interest rate decreased during the period (see Panels (a)-(c) in Fig. 1). The above stylized facts are more indicative of an increase in credit supply (Favara and Imbs, 2015). Although these facts have been well documented in the credit-cycle literature (e.g., Mian and Sufi, 2018 and Justiniano et al., 2019), most dynamic general equilibrium models have encountered difficulty in reconciling these facts.====This paper builds a dynamic stochastic general equilibrium (DSGE) model with financial intermediaries and a housing market to capture these facts. We attempt to link the housing boom with the bank credit expansion because banks played a central role during the housing boom. First, they were the main issuers of home mortgages to facilitate borrowers’ house purchases. Second, the rapid growth of mortgage securitization by banks attracted numerous investors to fund mortgage-backed securities (MBSs), which in turn fueled the housing boom. In addition, we observe a dramatic change in the structure of commercial bank asset accounts during the credit boom. Fig. 1 (d) shows the dynamics of the ratio of real estate loans to commercial and industrial loans of all US commercial banks during 1990–2010. The ratio almost doubled from 2000 to 2006 and remained at that level until immediately before the Great Recession started====. We show that our framework can capture the three stylized facts, as well as the change in bank asset account structures, during the boom.====In our model, banks absorb deposits from savers, lend funds to firms for production, and provide collateralized loans to borrowers for house purchases, during which processes the banks determine their asset account structures. Borrowers face borrowing constraints à la (Kiyotaki and Moore, 1997) and (Iacoviello, 2005) such that their borrowing is limited by the value of their houses. We further assume that the funds that banks can obtain from savers are limited by the banks’ internal net worth (Gertler and Karadi, 2011). Such setting is based on a more realistic assumption. More importantly, when banks face endogenous financial constraints and have limited funds to issue loans, their decisions about asset account structures have an impact on business cycles.====We show that our model can capture the above stylized facts when there is a credit expansion to banks (which we call “bank liberalization” hereafter). Bank liberalization mirrors the effect of securitization, that is, the pooling and tranching of bank assets to convert mortgages into mortgage-backed securities. These financial innovations, together with financial deregulation, reduce the financial friction between short-term investors and banks and allow banks to absorb more funds (Brunnermeier, 2009). Through bank liberalization, savers become willing to provide more funds to banks, so banks can invest more in both loans to firms and mortgages. This causes the return on loans to firms and the mortgage interest rate to decrease. Bank credit expansion also leads to a higher level of household borrowing and higher house prices. The increased house prices and decreased mortgage rates further relax home buyers’ borrowing constraints via the collateral value channel, so banks are inclined to lend more funds to mortgage borrowers compared to firms. As a result, the ratio of mortgage loans to firm loans increases. Our theoretical framework is, to the best of our knowledge, the first attempt to explain the change in bank asset account structures during the housing boom.====Moreover, the positive bank credit supply in our model causes nonmonotonic economic fluctuations, mainly through changes in the asset accounts of bank balance sheets. We show that the economy experiences a short-run boom and then suffers a downturn afterward when there is a positive credit supply to banks. Both the collateral constraints faced by borrowers and the financial constraints faced by banks play important roles in generating such boom-bust cycles. Since banks obtain more funds from a positive credit supply, additional funds are initially invested in loans to firms because borrowers face collateral constraints, leading to an increase in investment and output. However, once the increase in house values and the decrease in the mortgage rates relax borrowers’ collateral constraints, banks invest more in mortgages vis-====-vis their funding of firms. Since banks are financially constrained, the collateral value channel of mortgages has a “crowding out” effect on firms’ investment and capital purchases, predicting a subsequent decline in output. Our model illustrates a positive credit supply as a source of business cycles, consistent with the recent empirical findings of Mian and Sufi (2009), Glick and Lansing (2010), and Martin and Philippon (2017).====Bank credit expansions cause boom-bust cycles mainly through changes in the bank asset accounts, and this mechanism has important implications for policymakers aiming to limit economic recessions after booms. In our counterfactual experiments, we introduce an asset-side bank regulation: the policymaker taxes bank mortgage holdings and uses the revenue to subsidize loans to firms; the tax rate is proportionate to the mortgage-to-firm loan ratio. This regulation encourages banks to issue more loans to firms and fewer mortgages during the credit boom. The results show that such an asset-side bank regulation can mitigate economic decline at the cost of reducing the size of the preceding boom. The normative analysis shows that the asset-side bank regulation improves welfare by stabilizing credit cycles. Our results emphasize the importance of monitoring the structures of bank asset accounts during credit booms, which is complementary to macroprudential policies that focus on the other side of bank balance sheets, e.g., bank capital requirements or countercyclical capital buffers.====The rest of the paper is organized as follows. The remainder of the introduction focuses on a review of the literature. Section 2 describes our model, Section 3 shows the quantitative calibration of the model and the effect of an increase in credit supply, Section 4 conducts some numerical experiments for policies and welfare analysis, and Section 5 concludes the paper. The appendix contains a simple extension of our baseline model with housing investment and sensitivity analysis.","Credit expansion, bank liberalization, and structural change in bank asset accounts",https://www.sciencedirect.com/science/article/pii/S0165188921000014,15 January 2021,2021,Research Article,241.0
Manzan Sebastiano,"Bert W. Wasserman Department of Economics & Finance Zicklin School of Business, Baruch College, City University of New York, 55 Lexington Avenue, New York, NY 10010, United States","Received 30 January 2020, Revised 25 September 2020, Accepted 13 November 2020, Available online 30 December 2020, Version of Record 9 January 2021.",https://doi.org/10.1016/j.jedc.2020.104045,Cited by (4),I investigate how professional forecasters update their uncertainty forecasts of output and ,"The analysis of survey expectations shows that consumers and professional forecasters have diverging views about the future evolution of economic variables, which calls for a better understanding of the mechanism that agents use to form and revise their expectations (Mankiw, Reis, Wolfers, 2004, Mankiw, Reis, Wolfers, 2004, and Dovern, Fritsche, Slacalek, 2012, Dovern, Fritsche, Slacalek, 2012). Mankiw and Reis (2002) propose a theory of sticky information in which agents update their forecasts only occasionally due to the cost involved in processing the newly released information. This produces dispersion in forecasts since at any point in time there is co-existence of agents that incorporate the most recent macroeconomic information while others persist using outdated forecasts. An alternative argument for the existence of heterogeneous beliefs among agents is that they update their forecasts at every point in time, but are limited in their ability to process public information (Woodford, 2002, Woodford, 2002, and Sims, 2003, Sims, 2003). Andrade and Le Bihan (2013) and Coibion and Gorodnichenko (2015) provide empirical evidence that support the relevance of models with information rigidities based on survey expectations. Another argument for the existence of heterogeneous expectations is that agents use different models to form their expectations (Kandel, Pearson, 1995, Kandel, Pearson, 1995, and Brock, Hommes, 1997, Brock, Hommes, 1997, Brock, Hommes, 1998). Agents might produce different forecasts because they hold diverging prior views but also because, despite common priors, they interpret differently the relevance of the newly released information. There are several recent papers that try to disentangle these effects based on survey expectations. Lahiri and Sheng (2008) found that belief heterogeneity is largely due to differences in priors at long forecast horizons while it is driven by differential interpretation of news at short horizons. Using the same survey data but a different modeling strategy, Patton and Timmermann (2010) confirm that differences in priors represent the most important source of heterogeneity, although their results point to a minor role for the diversity in the interpretation of the signal. On the other hand, Manzan (2011) abstracts from the role of prior expectations to focus on the interpretation of news and finds evidence that forecasters are significantly heterogeneous in the way they update their forecasts. Overall, these papers suggest that forecasters are different in the way they form their prior expectations and in the way that they interpret the signal, although it is empirically difficult to disentangle the different effects without imposing any modeling assumption.====The aim of this paper is to investigate empirically what can be learned from ==== forecasts, rather than ==== forecasts, about the expectation formation process and the heterogeneity among forecasters. The density forecasts are obtained from the Survey of Professional Forecasters for the United States (US-SPF) and the Euro area (ECB-SPF). These Surveys collect expectations about the distribution of output growth and inflation by professional forecasters, in addition to point forecasts that have been used in some of the studies discussed earlier. The Surveys require participants to provide density forecasts from the first to the fourth quarter of a year with the goal of predicting the growth rate of the variable for that year. This structure thus allows to focus the analysis on the effect that macroeconomic news have on the revision of density forecasts since the forecast target remains constant while the horizon shortens over time. The empirical analysis is conducted adopting a simple Bayesian Learning Model (BLM) as the framework for the formation and revision of expectations. The model predicts that the posterior mean equals the weighted average of the prior mean and the signal contained in the new data releases, while the posterior precision (inverse of the variance) is given by the sum of the prior and the signal precisions. In particular, in this paper I test empirically the validity of the BLM prediction that the precision of density forecasts should not decline as the forecast horizon shortens. A forecaster that believes that the incoming data is uninformative about the future outcome of the variable will assign zero precision to the signal and thus expect that the posterior and prior precisions are equal. On the other hand, a forecaster that believes in the informational content of the data release will assign higher precision to the posterior, relative to the prior.====I empirical strategy consists of using the density forecasts to construct an observable measure of uncertainty for each forecaster at each point in time. This is done by calculating the variance of the individual densities which I then use to calculate the ratio of the prior and posterior precisions. The variance can then be used to evaluate if forecasts produced by professional forecasters are consistent with Bayesian updating of the precision and to what extent they are different across forecasters. The empirical evidence shows frequent violations of the BLM prediction since forecasters often provide density forecasts for output growth and inflation that are more uncertain (less precise) relative to the forecasts they produced in the previous quarter. This is inconsistent with Bayesian learning since the additional macroeconomic data released in the current quarter should increase their expected precision in case they consider the data informative or it should not change the prior precision in case they regard the news as uninformative. In addition, I find that the non-Bayesian behavior is common among most professional forecasters in the sample, although the frequency and magnitude of the deviations might be different.====The empirical analysis shows three main factors driving these findings. The first is a methodological issue with the administration of the Surveys. Forecasters are asked to assign probabilities to a set of intervals including two open intervals at the extremes of the grid. In most quarters, forecasters assign probabilities to the inner intervals of the grid, although occasionally they have assigned probability to the open intervals. An instance of this is the first quarter of 2009 when the rapid deterioration of macroeconomic conditions in Europe and the rest of the world caused many ECB-SPF forecasters to assign high probability for GDP growth to the lowest open interval of ====1% or lower. In the following quarter, the left open interval was shifted to ====6% or lower that allowed forecasters to assign most probability to the inner intervals. The truncation of the density forecasts in the first quarter of 2009 constitutes a problem when the analysis relies on extracting the mean and variance from the density forecasts. I solve this problem by constructing pseudo-density forecasts by exploiting the fact that forecasters are asked point forecasts, in addition to density forecasts. I assume that the point forecast represents the approximate center of the pseudo-density forecast and that the underlying distribution is triangular. In this way, I am able to produce pseudo-histograms that I interpret as the distribution that the forecaster would have submitted had the truncation not occurred.====Once this methodological issue is resolved, I find that individuals are likely to reduce the precision of their density forecasts, relative to their prior quarter forecast, when they receive a large surprise and as a reaction to having produced a very narrow distribution in the previous quarter. I define the surprise as the most recent data release of the variable being forecast, standardized with the mean and standard deviation of the individual prior density forecast. The evidence indicates that both negative and positive surprises have the effect of increasing the ratio of the prior to posterior precision. This suggests that, at times, forecasters react to unexpected news by increasing the dispersion of their density forecasts, relative to their prior expectation. The second factor that I find relevant in causing the non-Bayesian updating is the fact that forecasters sometimes concentrate their density forecast in a few bins. Due to the discreteness imposed by the histogram, a shift in the mean of a density that is highly concentrated in a few bins might cause the probability to be more disperse relative to the previous quarter. I find that the smaller the number of bins used in a quarter the more likely it is that in the following quarter the forecaster decreases the precision of the density. Finally, by using a panel group estimator we are able to identify groups of forecasters with an homogeneous response to surprises within the group, but heterogeneous across groups.====This paper is organized as follows. In Section (2) I introduce the BLM and discuss its implications for the updating behavior of professional forecasters. In Section (3) I discuss the density forecasts provided by the US-SPF and ECB-SPF and in Section (4) I conduct an exploratory analysis of the empirical support for the BLM prediction followed by a regression analysis to understand the determinants of the observed non-Bayesian behavior of forecasters. Finally, Section (6) draws the conclusions of the paper.",Are professional forecasters Bayesian?,https://www.sciencedirect.com/science/article/pii/S016518892030213X,30 December 2020,2020,Research Article,242.0
"Prein Timm M.,Scholl Almuth","Faculty of Social Sciences, Economics, University of Helsinki and Helsinki Graduate School of Economics, Finland,Department of Economics and Cluster of Excellence “The Politics of Inequality”, University of Konstanz, Germany","Received 23 July 2020, Revised 27 November 2020, Accepted 22 December 2020, Available online 29 December 2020, Version of Record 21 January 2021.",https://doi.org/10.1016/j.jedc.2020.104065,Cited by (1), risk. We consider a ==== in which the government has access to official loans conditional on the implementation of ,"Rising government bond spreads in the aftermath of the 2007/08 financial crisis forced the Greek government to turn to the International Monetary Fund, the European Commission, and the European Central Bank requesting financial assistance. The first bailout was granted in May 2010 and two further bailout programs followed in 2012 and 2015. In return, the government committed to implement pre-specified austerity policies. The implementation of the program conditions was accompanied by domestic protests and political unrest. Formerly small and newly founded parties opposing austerity massively gained votes, destabilizing the government and giving rise to doubts on the commitment to repay debt and fulfill conditionality.====The events in Greece raise several important questions: What is the impact of bailout programs on the risk of political turnover? How do sovereign default risk, bailouts, and political turnover interact and how are macroeconomic outcomes affected? How does stricter conditionality affect the risk of political turnover and sovereign default in the short run and in the long run?====To address these questions, this paper analyzes the interaction of sovereign default risk, bailouts, and political turnover in a politico-economic model of sovereign debt. The theoretical framework features endogenous default risk, endogenous participation rates in bailout programs as well as endogenous political turnover. We consider a small open economy that is inhabited by infinitely-lived households. The government finances a public good by raising taxes and by issuing external debt. International financial markets are incomplete and debt contracts are subject to default risk. In addition to debt provided by international private creditors, an (unmodeled) international financial institution provides official loans below the market rate and, in return, restricts the set of fiscal policies by imposing a target on the primary surplus. The government decides whether to fulfill its debt obligations or to default. Moreover, taking conditionality as given, the government chooses whether to make use of a bailout program.====There is a two-party system in which both parties care about the population’s welfare. Following Chang (2007), the parties differ in an exogenous one-time utility cost of default that can be interpreted as a personal cost of the policymaker due to a loss of reputation. Individuals are not affected by these utility costs, but differ in stochastic idiosyncratic ideological aspects, which are independent from economic policy. Political turnover is the endogenous outcome of the individual voting behavior, which is determined by the economic benefits from having the opponent rather than the incumbent in power as well as stochastic idiosyncratic ideological aspects. Risk-neutral international private creditors charge a premium that reflects the endogenous probability of a political turnover as well as the endogenous default risk.====In equilibrium, the probability of political turnover is a function of the productivity state and the debt policy. Implicitly, the turnover probability is affected by the bailout decision because the provision of official loans is conditional on the implementation of the pre-specified primary surplus target, which restricts the government’s borrowing choice. The policy functions suggest that the party with the lower utility cost of default is more likely to come into power when debt is high and is more willing to exit a bailout program by declaring a default. Instead, the party with the higher utility cost of default is more likely to be in power when debt is low and is more willing to make use of official financial assistance. To highlight the interaction between bailouts and political turnover, we provide a comparison with a counterfactual economy in which official loans are not provided. Our analysis reveals that if debt is high, bailouts foster the probability that the party with the lower utility cost of default comes into power, which, in turn, raises sovereign default risk.====In a quantitative exercise we apply our theoretical framework to the Greek economy and explore how the interaction between bailouts and political turnover affects macroeconomic outcomes. To this end, we simulate our model and study the dynamics of a bailout event. In the years before the bailout, our model predicts a low sovereign interest spread and a small probability of political turnover due to good economic conditions. Because of low credit costs, the incumbent government is not borrowing constrained and runs a budget deficit. The debt crisis is triggered by an adverse economic shock that reduces the ability of the government to repay the outstanding debt. Due to the increase in the sovereign interest spread, the incumbent government decides to enter a bailout program. Because official loans are conditional on the adoption of austerity policies, the incumbent government implements tax hikes and spending cuts, which raise the probability of losing power. Successively, the risk of political turnover elevates the sovereign interest spread. A comparison with the Greek bailout of May 2010 reveals that the model replicates the empirical pattern of output, consumption, and the sovereign interest spread quite well.====To quantify the impact of political risk on the sovereign interest spread during a bailout, we provide a comparison with a counterfactual economy in which political uncertainty is absent and the incumbent party remains in office forever. Our results suggest that the risk of political turnover increases the sovereign interest spread by 2 percentage points at the time of the entry into a bailout program.====In a policy analysis, we study the role of conditionality and vary the primary surplus target attached to the provision of official loans. We find that the frequency of political turnover is U-shaped in the strength of conditionality, which is driven by two opposing forces. On the one hand, fulfilling the primary surplus target is costly as it forces the incumbent to implement tax hikes and spending cuts, which foster the risk of losing power. On the other hand, a tighter fiscal constraint reduces debt in the economy, which decreases the probability that the party with the lower utility cost of default comes into power.====We show that while stricter conditionality raises the short-run risk of political turnover and sovereign default, in the long run, it mitigates sovereign default risk and political turnover if the fiscal constraint is not too tight. These findings highlight the tension that policymakers face when designing bailout packages: While stricter conditionality may improve fiscal sustainability and political stability in the long run, it fosters political uncertainty and sovereign default risk in the short run.====Our paper is related to three different strands of literature. First, our paper builds on the politico-economic literature that analyzes the interaction of political turnover and public debt, see, e.g., Alesina and Tabellini (1990), Persson and Svensson (1989), Aghion and Bolton (1990), and the overview in Persson and Tabellini (2000). While the aforementioned papers mostly consider two-period models, Battaglini and Coate (2008), Song et al. (2012), Müller et al. (2016) and Dovis et al. (2016) develop fully dynamic politico-economic theories of public debt, but abstract from sovereign default risk. Chang (2007) and Chang (2010) study the interaction between political crises and financial crises and focus on the role of self-fulfilling expectations. Our finding that conditionality may increase the incumbent’s probability of losing power is related to the recent work of Chatterjee and Eyigungor (2020) who argue that constraints on how much policies can change may generate an “incumbency disadvantage”.====Second, we build on the recent quantitative literature on sovereign debt that allows for default in equilibrium, see, e.g., Aguiar and Gopinath (2006) and Arellano (2008). Hatchondo et al. (2009) and Cuadra and Sapriza (2008) consider exogenous political turnover rates and show that political instability increases debt accumulation and default risk. Scholl (2017) introduces the probabilistic voting approach in a quantitative model of sovereign debt to analyze the impact of endogenous electoral outcomes on sovereign default risk. She shows that endogenous election probabilities increase the disparities between the parties’ debt and default policies. In a related study, Chatterjee and Eyigungor (2019) analyze the interaction of economic growth, election probabilities, and sovereign risk premia.==== Our paper is related to Andreasen et al. (2019) who highlight that the implementation of austerity policies is subject to political constraints. However, Andreasen et al. (2019) abstract from the role of official loans and conditionality, which is our focus here.====Third, this paper is related to the literature that studies the role of international financial institutions and the macroeconomic impact of bailouts and conditionality.==== Ardagna and Caselli (2014) discuss the politico-economic aspects of the Greek bailouts.==== Several papers analyze the impact of official loans on sovereign default risk using stochastic dynamic models of sovereign debt and abstracting from political uncertainty, e.g., Aguiar and Gopinath (2006), Boz (2011), Juessen and Schabert (2013), Kirsch and Rühmkorf (2017), Hatchondo et al. (2017), Pancrazi et al. (2020), and Roch and Uhlig (2018). Fink and Scholl (2016) show that bailouts prevent sovereign defaults in the short run, but may come at the cost of a larger default probability in the long run. Our analysis confirms their finding: The insurance character of bailouts induces the government to borrow more and the higher debt level raises the risk of a sovereign default. Fink and Scholl (2016) stress the opposing forces of conditionality. On the one hand, stricter conditionality reduces the probability that a government enters or remains in a bailout program such that sovereign default risk increases. On the other hand, tighter fiscal constraints lower the level of debt and mitigate sovereign default risk. In our model, similar mechanisms are at work. We contribute to this literature by studying the dynamic interaction between bailout, conditionality and political turnover. Our findings highlight that political turnover elevates the trade-offs of bailouts and conditionality in the short and long run.====The remainder of the paper is structured as follows. In Section 2 we consider Greece as a case study and provide narrative evidence on the link between sovereign interest spreads, bailouts, and political turnover. Moreover, we provide empirical evidence on the interplay between political outcomes and sovereign debt for a broader set of countries. In Section 3 we describe the theoretical framework. Section 4 presents the quantitative properties of the model and discusses the interaction between political turnover and conditional bailouts. Finally, Section 5 concludes.",The impact of bailouts on political turnover and sovereign default risk,https://www.sciencedirect.com/science/article/pii/S0165188920302335,29 December 2020,2020,Research Article,243.0
"Garriga Carlos,Kydland Finn E.,Šustek Roman","Research Division, Federal Reserve Bank of St. Louis, P.O. Box 442, St. Louis, MO 63166-0442, U.S.A.,University of California–Santa Barbara and NBER,Queen Mary University of London and the Centre for Macroeconomics, United Kingdom","Received 21 September 2020, Revised 8 December 2020, Accepted 12 December 2020, Available online 25 December 2020, Version of Record 9 January 2021.",https://doi.org/10.1016/j.jedc.2020.104059,Cited by (1),"There has been much interest recently in the role of household long-term, ====, debt in the transmission of ====. This paper offers a tractable framework that integrates the long-term debt channel with the standard New-Keynesian channel, providing a tool for ==== analysis that reflects the recent debates in the literature. As there is a nontrivial role in the model for both short- and long-term debt, it provides a laboratory to investigate the effects of monetary policy operating not only through its current short-term actions but also through expected, persistent, changes in its stance.","When a central bank decides on the next course of monetary policy, the whole machinery of the monetary transmission mechanism sets in motion (e.g., Bernanke, Gertler, 1995, Mishkin, 1995). The main lever of the mechanism in the current go-to models for monetary policy analysis is the presence of nominal price rigidities (sticky prices) in product markets, as embedded in the New-Keynesian Phillips curve. Recently, however, the literature started to pay increasing attention to the part of the mechanism that works through the exposure of households to long-term debt, typically mortgage debt (among others, Auclert, 2019, Cloyne, Ferreira, Surico, 2020, Di Magio, Kermani, Keys, Piskorski, Ramcharan, Seru, Yao, 2017, Doepke, Schneider, 2006).====This paper integrates both channels—the New-Keynesian and the long-term debt channel–in a tractable framework to provide a tool for monetary policy analysis that reflects the recent debates in the literature. While such endeavor may be interesting in its own right, it gains particular usefulness in light of another recent development in macroeconomics and finance. A stream of research documents that monetary policy surprises are more complex than traditional analysis suggested, consisting of not only current central bank actions but also of statements and signals about the expected future course of monetary policy (e.g., Campbell, Evans, Fisher, Justiniano, 2012, Gürkaynak, Sack, Swanson, 2005a, Nakamura, Steinsson, 2018).==== Furthermore, a large body of work in finance, starting with Ang and Piazzesi (2003), suggests that monetary policy contains a component that persistently affects expectations of future policy rates over time. While the current policy rate determines the cost of short-term debt, expectations about future monetary policy affect the cost of long-term debt (see, e.g., Hamilton, 2008).====Fig. 1 illustrates, with the help of an episode, the basic concepts that the model intends to capture. The figure plots movements of the ECB policy rate, together with movements of nominal mortgage interest rates, in a number of euro area countries around the time of the ECB monetary policy easing in 2008. Since in the case of long-term debt one needs to distinguish between new loans (flow) and outstanding debt (stock), each with potentially different interest rates, we separate the two. Furthermore, the countries are divided into two groups, those with fixed-rate mortgages (FRM) and those with adjustable-rate mortgages (ARM), based on the typical contract used. The two basic contracts are characterized by different pass throughs of the policy rate.====Without doubt, the cut in the ECB rate was partly transmitted into the economy through the standard New-Keynesian channel. The figure, however, shows that the ECB decision also affected households directly through their exposure to mortgage debt. Furthermore, the effects were not uniform across mortgage types and across new loans and outstanding debt. In the case of ARM countries, the interest rate on both new and outstanding debt declined more or less immediately and nearly by the full amount of the ECB rate cut. In FRM countries, however, while the interest rate on new loans also declined—though less then the ECB rate—the interest rate on outstanding debt remained essentially unchanged.==== Furthermore, the decline in the FRM rates on new loans was not only due to the cut in the ECB rate itself, but reflected, at least partially, expectations of its persistence. With the benefit of historical hindsight, we know the reduction in the ECB rate was very persistent indeed. Given that mortgage debt is on average around ==== of annual GDP in developed economies (International Monetary Fund, 2011), just how important is the long-term debt channel, when compared with the standard New-Keynesian channel, in transmitting monetary policy?====In the model, long-term debt plays a role by facilitating purchases of housing. Specifically, a fraction of new housing is financed through new ==== mortgages (either FRM or ARM), which are amortized over time in a way that mimics the payment schedule of a typical mortgage.==== There are two types of representative agents: “homeowners”, representing middle-class households, and “capital owners”, representing the top quintile of the wealth distribution (Campbell and Cocco, 2003). Both agent types supply labor. Capital owners can invest in capital used in production, whereas homeowners cannot. Capital owners also finance mortgages used by homeowners to purchase housing. Furthermore, the two agent types trade a noncontingent one-period nominal bond (unsecured credit), albeit homeowners only at a spread over the short rate, which increases with their outstanding credit position. The key distinction between the two agents is thus their access to capital and bond markets.==== This has consequences for their valuation of mortgage payments (cash flows) over the life of a loan and for their marginal propensity to consume. Due to the limited ability of homeowners to smooth out fluctuations in disposable income, they can be thought of as the “rich hand-to-mouth” consumers in the spirit of Kaplan and Violante (2014). The rest of the model has the standard New-Keynesian features: nominal price rigidities in product markets and a Taylor rule. The Taylor rule, however, includes shocks affecting expected future interest rates, in addition to standard policy shocks (the Taylor rule shocks are the only shocks considered).====When the measure of homeowners is zero, the model boils down to a representative agent New-Keynesian (RANK) model with capital. When mortgages are removed, the model becomes akin to a two-agent New-Keynesian (TANK) model, e.g., Debortoli and Galí (2018), with one-period debt. Introducing richer household-level heterogeneity would transform the model, at a significant computational cost, to a heterogenous-agent (HANK) model, e.g., Kaplan et al. (2018), with mortgages.====The main finding can be summarized as a near separation of the two channels. The New-Keynesian channel transmits mainly monetary policy shocks that are transitory, whereas the debt channel transmits persistent policy shocks. Furthermore, the effects of the New-Keynesian channel are predominantly aggregate, whereas those of the debt channel are redistributive. In a nutshell, the near separation result occurs for the following reason. The key friction for aggregate effects is nominal price stickiness. When a policy shock is temporary, the response of firms is through output. This is the standard New-Keynesian mechanism. However, when the shock is understood to be sufficiently persistent, inflation expectations shift and the aggregate response of firms is through prices, rather than output. The presence of mortgages has a quantitatively small effect on this basic mechanism, despite homeowners facing the bond market cost. When the shock is temporary, its effect on real mortgage payments—redistributing income between homeowners and capital owners—can be partially smoothed out by a bond trade between the two agents that is not too costly for the homeowner. The presence of mortgages thus has a small effect on the aggregate responses above and beyond the New-Keynesian channel. When the policy shock is persistent, its effect on real mortgage payments is persistent and thus significantly more costly for the homeowner to offset through bond trades. In this case redistribution is sizable and persistent but it has negligible consequences for output, as aggregate prices at that point adjust in line with the persistent change in monetary policy. Absent any further frictions or government policies, or heterogeneity in labor supply as in Doepke and Schneider (2006b), we expect this basic insight to hold in settings with richer income/wealth heterogeneity.====In relation to our previous work (Garriga et al., 2017), we extend the model of that paper to the conventional New-Keynesian setting. At the same time, however, we simplify the more general mortgage market structure of that paper by abstracting from optimal refinancing/prepayment and the choice between FRM and ARM. As we have shown in that paper, if FRM is refinanced every period when interest rates decline (which would be optimal in the absence of refinancing costs), FRM becomes equivalent to ARM along such an interest rate path. However, for calibration replicating the frequency of refinancing observed in U.S. data, it turned out that the equilibrium dynamics of an economy with refi-FRM were still closer to that with pure FRM than ARM.==== If refinancing also involves costlessly changing the debt outstanding (cash-in/cash-out), both FRM and ARM boil down to one-period loans. Again, for realistic calibration, this is not the case.====In terms of analysis, we add to our previous work in three ways. First, given the growing interest in the long-term debt channel, we compare its importance, in a common setting, with the New-Keynesian channel and explore any potential interactions between the two channels.==== Second, we clarify and contrast the following concepts, explored by the literature and discussed below, that are at work in the debt channel: (i) monetary policy operating through current cash flows v.s. the present value of the long-term debt position and (ii) the interest rate exposure v.s. the Fisher revaluation channel. In the case of ARM these concepts are subtle and not well understood, despite being increasingly used. Our model encompasses all four cases. We explain how they are underpinned by asset market incompleteness and how their quantitative importance is affected by policy shock persistence. Finally, we relate the policy shocks in the model to the yield curve concepts of slope v.s. level factors and, in the supplementary material, further to action v.s. statement policy shocks (Gürkaynak, Sack, Swanson, 2005a, Nakamura, Steinsson, 2018). These mappings help with both the structural interpretation of the shocks and calibration of their persistence.====The paper proceeds as follows. Section 2 reviews the streams of research that motivate some of our assumptions and computational experiments. Section 3 develops the model, Section 4 describes its calibration, and Section 5 reports the findings. Section 6 then explains the mechanism and connects it with the concepts in the literature. Section 7 summarizes the main lessons and concludes. A supplementary material contains details of the equilibrium, secondary derivations, and examples of two rotations of the basic policy shocks, which offer their alternative structural interpretations.",MoNK: Mortgages in a New-Keynesian model,https://www.sciencedirect.com/science/article/pii/S016518892030227X,25 December 2020,2020,Research Article,244.0
Guthrie Graeme,"School of Economics and Finance, Victoria University of Wellington, PO Box 600, Wellington 6140, New Zealand","Received 8 May 2020, Revised 8 December 2020, Accepted 13 December 2020, Available online 21 December 2020, Version of Record 30 December 2020.",https://doi.org/10.1016/j.jedc.2020.104057,Cited by (2),"This paper presents a real-options model of entrenchment in which a CEO chooses how much effort to put into boosting a firm’s productivity and the board and CEO bargain over executive-compensation and investment policies. The surplus that bargaining allocates derives from the reduction in value of the firm’s capital that occurs if the CEO is replaced. Even if the CEO has no ownership stake, she exerts effort in order to increase the value of the capital at risk. This increases the shared surplus, which increases the CEO’s current pay. Newly appointed CEOs are paid less and work harder than their entrenched counterparts. They exert more effort at firms where the CEO’s human capital is more important. In contrast, entrenched CEOs exert more effort at firms where their human capital is less important and turnover-induced disruption has a higher cost. Both types work harder when average productivity growth is higher and productivity growth is more sensitive to effort. The board and CEO will agree to accept a degree of investment inefficiency if this allows them to slow down the CEO’s entrenchment.","The list of factors that induce a chief executive officer (CEO) to exert effort that increases firm value is well known. CEOs are monitored by their boards of directors. They respond to direct financial incentives generated by performance-based pay and CEO ownership of securities issued by the firm. They also respond to indirect incentives generated by the threat of dismissal, the effect of reputation on future employment prospects, the desire to avoid intervention by activist investors, and, ultimately, the market for corporate control. This paper adds one more factor to this list: an incentive to increase the payoff from entrenchment. It shows that even if pay is not directly related to firm performance, and even if the CEO has no formal ownership stake in the firm, entrenchment generates an incentive to exert effort. Conventional measures of pay–performance sensitivity and ownership may therefore underestimate the strength of the CEO’s incentive to increase the value of her firm.====This paper develops this idea using a formal model of the board–CEO relationship. The ultimate source of the CEO’s entrenchment is the disruption that is caused if the CEO leaves the firm. The cost of disruption will depend on factors such as the amount of production lost during the transition in management, the loss of firm-specific managerial skill, and the interruption of personal ties between the firm’s CEO and its key suppliers, customers, and other stakeholders.==== The board’s desire to avoid incurring these costs gives the CEO bargaining power and forces the board to share decision-making with the CEO. This might involve explicit negotiations, but it is also possible when decision-making appears to be the domain of the board or the CEO alone.==== In this case, the formal decision-maker actually makes decisions subject to a tacit understanding about the set from which choices can be drawn. Such implicit bargaining is especially relevant for decisions that might be thought of as the board’s prerogative, such as board composition (Hermalin and Weisbach, 1998) or, as in this paper, investment. In this model, the board and CEO bargain over the CEO’s wage and the firm’s investment policy.==== Current investment generates future entrenchment and this entrenchment determines current and future negotiated pay.====An alternative framework, not pursued in this paper, assumes that the board chooses the CEO’s wage and the firm’s investment policy in order to maximize the firm’s market value subject to CEO participation and incentive-compatibility constraints.==== The efficient-contracting framework is appropriate if the board is so powerful that it can prevent the CEO from sharing in the surplus generated by the CEO’s employment by the firm. This paper, in contrast, considers situations where the CEO is powerful enough to extract some of this surplus. The board cannot simply make a take-it-or-leave-it offer to the CEO concerning the firm’s key compensation and investment policies. A complete explanation of the board–CEO relationship will probably involve elements of the two non-exclusive theories that currently dominate the field: efficient contracting and rent extraction. I adopt the latter framework in this paper and explore the implications of recognizing the bargaining power generated by entrenchment.====My main finding is that the CEO exerts effort to increase the firm’s productivity even though the model contains neither performance-based pay nor a formal ownership stake for the CEO. She receives no immediate direct benefit from exerting effort, but does so because this increases the value of the capital that will be lost now or in the future if the board and CEO cannot reach agreement. By making this capital more valuable, the CEO increases the size of the surplus from reaching agreement with the board, which in turn increases the level of pay she is able to negotiate with the board. All shareholders benefit from the CEO’s effort. The mechanism analyzed in this paper is similar to ownership-generated incentives in the sense that it generates managerial behavior that benefits all shareholders. However, there is a fundamental difference between standard ownership and the “pseudo-ownership” analyzed in this paper. The value to CEOs of standard ownership derives from the dividends paid out to the owners of the firm’s securities, whereas the value of “pseudo-ownership” derives from the pay that can be negotiated as a result of the underlying capital being lost in the event of CEO turnover.====CEO turnover potentially has a much greater adverse effect on the productivity of capital acquired during a CEO’s tenure than capital acquired when a previous CEO was in charge. This will be the case for firms that expand in lines of business closely aligned to the specialist skills of their CEOs or that operate in markets where their CEOs have strong ties to key stakeholders.==== Aligning investment with a CEO’s human capital can be a strategic decision made by a firm’s board or it can be the inevitable consequence of a CEO having more day-to-day involvement in a firm’s operations than the board. For example, in the latter case a self-interested CEO might skew investment proposals presented to the board towards her own human capital. She might implement the board’s investment decisions in ways that boost her importance to the firm. Regardless of the origins, if a firm’s investment is aligned with its CEO’s human capital, then capital acquired before the CEO arrived will be affected by her departure to a lesser extent than capital acquired afterwards.====In my main model I focus attention on the possibility that the alignment of investment with a CEO’s human capital is an inevitable consequence of the CEO’s close management of the firm. I incorporate this possibility by including two types of capital in the model. The first type (“generic capital”) was acquired by the firm prior to the current CEO’s appointment. In contrast, the second type (“manager-specific capital”) is acquired during the current CEO’s tenure. In order to highlight the role of entrenchment, I assume the two types of capital are perfect substitutes in every respect apart from how they are affected by CEO turnover: compared to generic capital, a larger proportion of manager-specific capital is lost if the current CEO leaves the firm. The composition of the firm’s capital stock therefore changes over time as the firm invests in new manager-specific capital, which strengthens the CEO’s bargaining position by making CEO turnover more costly to shareholders.==== All else equal, the surplus from reaching agreement grows over time as depreciating generic capital is replaced by manager-specific capital, so that the CEO’s pay increases as she becomes more entrenched. However, the higher pay increases the CEO’s opportunity cost of effort. I find this effect dominates the increased incentive to exert effort caused by the increased capital at risk of CEO turnover; that is, more entrenched CEOs exert less effort. Ultimately, however, which effect dominates is an empirical question.====I also consider the possibility that the firm can invest in both types of capital, with the board and CEO having to agree about the quantity and composition of investment. In this extension, manager-specific capital might be more cost effective than generic capital due to its alignment with the CEO’s human capital, in which case the board has to weigh the benefits of investing in a more profitable form of capital against the cost of a more entrenched CEO. Alternatively, manager-specific capital might be less cost effective than generic capital, but preferred by the CEO due to its greater entrenchment potential. The behavior of the firm’s total investment, the CEO’s wage, and her effort are very similar to their behavior in the main model. However, across all the parameter combinations I consider, the board and CEO agree to make manager-specific capital a smaller proportion of total investment than is required to minimize investment expenditure. There are two situations when manager-specific capital is especially out of favor. First, when profit per unit of capital is high, the resulting rapid investment means the CEO’s bargaining position is strengthening rapidly regardless of the type of capital acquired, so she is willing to agree to shift investment away from manager-specific capital. Second, when the CEO’s degree of entrenchment is low, her bargaining position is so weak that the board’s preference for an investment mix that slows down the rate of entrenchment dominates and the CEO agrees to shift investment away from manager-specific capital.====Returning to the main model, the degree of entrenchment affects how CEO behavior depends on firm and industry characteristics. As CEO–stakeholder relationships become more important, the effort exerted by a fully entrenched CEO falls on average, but a newly appointed CEO works harder. When the costs of CEO turnover are higher, an entrenched CEO works harder but a newly appointed CEO’s effort barely changes. However, in other respects their behavior is similar. For example, the amounts of effort exerted by both types of CEO are reasonably insensitive to the board’s bargaining power. When the firm’s underlying growth rate is higher, or effort has a larger effect on the firm’s expected productivity growth, both types of CEO exert more effort.====I assume that the board and CEO cannot commit to future policies, so that the CEO’s pay and the firm’s investment fluctuate with the amount of capital that would be lost in the event of CEO turnover. However, investment decisions are irreversible, which means that current investment decisions introduce a degree of stickiness into the CEO’s pay. Both parties recognise that the agreed current investment policy to some extent “locks in” future pay policy. At each point in time, the board and CEO are effectively bargaining over current pay and (via current investment) future pay—even though commitment is assumed to be impossible. This has interesting implications for executive compensation. For example, the CEO’s current wage is a non-monotonic function of the firm’s current profit per unit of capital: it is increasing in profit per unit of capital for low levels of profitability and decreasing in profit per unit of capital for high levels. The firm will not invest when profit per unit of capital is low, and a small increase in profitability will not change that. The CEO shares the resulting increase in firm value with shareholders by negotiating higher current pay with the board. In contrast, the board and CEO agree to substantial investment in new capital when profit per unit of capital is high, which both parties recognize increases the CEO’s entrenchment. A small increase in profitability will result in greater investment, hence greater entrenchment and higher future pay. The board and CEO agree to offset the persistent increase in future pay by lowering the CEO’s current pay. In short, the firm does not ==== to pay a higher wage currently because both parties know that it will ==== to pay a higher wage in the future.====This paper contributes to the literature examining the board–CEO relationship. In Hermalin and Weisbach (1998), as here, the board and CEO engage in a bargaining game, but in their case the board and CEO bargain over monitoring intensity (via board composition) and CEO pay. Investment does not play a role. Other authors investigate the board’s role in investment decision-making, but they typically assume that the board has all the decision-making power. In these papers the CEO is able to influence the board’s decision by deciding what information is made available to the board (Harris, Raviv, 2008, Harris, Raviv, 2010, Song, Thakor, 2006).==== In contrast, this paper models the repeated rounds of investment decision-making as an ongoing bargaining game involving the board and CEO. The ultimate source of the CEO’s bargaining power in future bargaining rounds is the investment chosen in the current round. As in Hermalin and Weisbach (1998), the CEO’s pay is determined simultaneously and is affected by the degree of the CEO’s entrenchment. However, the CEO in Hermalin and Weisbach (1998) becomes entrenched by reducing the board’s independence over time. In this paper, entrenchment occurs as the firm’s profitability becomes increasingly reliant on the CEO’s human capital.====A related literature shows how managerial rent extraction can give managers incentives to act in ways that benefit all shareholders. In my model, the prospect of capital being lost following CEO turnover allows the CEO to extract rents via the pay-setting process. This motivates the CEO to invest effort in boosting the firm’s productivity, which benefits all capital, not just the capital that will be lost. Much of the existing literature assumes that managerial rent extraction is a consequence of information asymmetry between a firm’s board and CEO. The CEO in these models is able to extract rents by tilting the firm’s investment policies towards projects with higher private benefits of control. This motivates the CEO to exert effort searching for attractive investment projects, which benefits shareholders provided that the profits and control benefits generated by projects are positively correlated. Firms can encourage this behavior in various ways. For example, they can maintain a diffuse ownership structure, which reduces shareholder pressure on the CEO (Burkart et al., 1997); they can select a busy or otherwise distracted board that is more likely to delegate investment decision-making to the CEO (Aghion and Tirole, 1997); or they can restrict the board’s ability to fire the CEO when a superior replacement becomes available (Almazan and Suarez, 2003).==== Although there are no ==== private benefits of control in the model in this paper, there are ==== benefits. The CEO’s ability to systematically replace obsolete generic capital with manager-specific capital introduces indirect private benefits of control, in this case the increased bargaining power that enables the CEO to extract a share of the cash flows that would otherwise flow to shareholders.====The paper also contributes to the theoretical literature on executive compensation. The workhorse of this literature is the efficient contracting hypothesis, according to which the board of directors (acting as the principal) designs a contract that induces the CEO (as agent) to work in shareholders’ best interests by basing pay at least in part on firm performance. The theory is well developed but its critics point to evidence that in practice executive pay appears to be based at least in part on factors that are not influenced by CEO effort (Bertrand and Mullainathan, 2001). They also claim that the efficient contracting hypothesis has difficulty explaining the extremely high levels of executive compensation observed in recent decades (Frydman and Saks, 2010). One explanation for high levels of pay is that they are the consequence of competition for talented managers, with the largest firms able to attract the services of the most talented CEOs. The theory of this market-based mechanism is well developed using competitive assignment models (Gabaix, Landier, 2008, Terviö, 2008), but its ability to explain trends in compensation levels in the mid-twentieth century is debated (Frydman and Saks, 2010). An alternative explanation, based on the belief that powerful executives are able to use their positions to extract rents from their firms, has also become popular. Known as the managerial power hypothesis, this explanation was developed by Bebchuk et al. (2002) and Bebchuk and Fried (2004), but the ideas can be traced back to Lambert et al. (1993). The evidence supporting the managerial power hypothesis is mainly empirical, with little supporting theory. The model in this paper focusses on the rent-extraction aspect of executive compensation. Although the CEO exerts effort that benefits shareholders, this is not the result of an incentive contract designed by the board. Rather, it is a consequence of the CEO’s ability to use her position to negotiate a share of the firm’s profits that exceeds her opportunity cost of working for the firm. The model in this paper therefore provides theoretical support for the managerial power hypothesis.====My results offer insights into the measurement of compensation. In particular, I show that the CEO’s current pay overestimates total compensation when profit per unit of capital is low: the firm is investing in insufficient new capital to replace the manager-specific capital lost to physical depreciation, which reduces the CEO’s entrenchment and therefore reduces the value of her future expected pay. In contrast, the CEO’s current pay severely underestimates total compensation when profit per unit of capital is high: the firm is investing rapidly, which increases the amount of capital lost in the event of CEO turnover and therefore increases the value of the CEO’s future expected pay. My results also offer insights into the relationship between pay and firm size. After controlling for firm profitability, this relationship becomes complicated. The CEO’s current pay is an increasing function of the amount of manager-specific capital and a non-monotonic function of the amount of generic capital. When the overall capital stock is small, the CEO’s current pay is an increasing function of the amount of generic capital. In contrast, when the capital stock is large enough, pay is decreasing in the amount of generic capital.==== Finally, the CEO’s total compensation is insensitive to the degree of entrenchment when profitability is high, but increasing in the degree of entrenchment when profitability is low. In the former case, total compensation is driven mainly by the firm’s investment, which is relatively insensitive to the degree of entrenchment. In the latter case, investment is negligible, so that total compensation is determined primarily by the current wage, which is increasing in the degree of entrenchment.====Finally, the paper makes a methodological contribution by embedding the Nash bargaining solution (Nash, 1950) in a real-options model of decision-making. The Nash bargaining solution has been used extensively to examine static problems in corporate finance and corporate governance. For example, it has been used to study the use of anti-takeover defenses (Harris, 1990), the use of capital structure to extract wage concessions (Perotti and Spier, 1993), interactions between the board of directors and the CEO (Hermalin and Weisbach, 1998), relations between entrepreneurs and venture capitalists (Hellmann, 1998, Inderst, 2004), the outcome of merger negotiations (Alvarez, Stenbacka, 2006, Thijssen, 2008), and the roles of angel and venture capital markets (Hellmann and Thiele, 2015). However, it is rarely used in dynamic settings. Sorger (2006) introduced the recursive Nash bargaining solution in the context of a deterministic discrete-time model. Schopf and Voss (2019) recently used Sorger’s approach to analyze environmental lobbying. The stochastic continuous-time formulation used here makes this solution concept applicable to a wide variety of problems in corporate finance and corporate governance. It is straightforward to implement in continuous time and accommodates stochastic state variables in a simple manner. In particular, the dynamic Nash bargaining solution can be calculated by solving a system with many similarities to the Hamilton–Jacobi–Bellman framework.====Section 2 sets up the model and Section 3 derives the conditions that the agreed investment and wage policies must satisfy, as well as the effort policy chosen by the CEO. This section also shows how the whole problem can be reduced to a simpler problem involving functions of just two variables, which measure the CEO’s entrenchment and the firm’s profitability. Section 4 analyzes a particular case of the model, uses it to demonstrate some important properties of the solution, and discusses the implications of investment-based entrenchment for executive compensation. Section 5 discusses the results of sensitivity analysis. Section 6 investigates the firm’s behavior when investment can be a mix of generic and manager-specific capital, with the board and CEO jointly deciding the mix and quantity of investment. Finally, Section 7 presents some concluding remarks as well as some suggestions for future research.",A dynamic model of managerial entrenchment and the positive incentives it creates,https://www.sciencedirect.com/science/article/pii/S0165188920302256,21 December 2020,2020,Research Article,245.0
Kim Jiwoon,"Hongik University, Republic of Korea","Received 28 January 2020, Revised 20 November 2020, Accepted 5 December 2020, Available online 11 December 2020, Version of Record 15 January 2021.",https://doi.org/10.1016/j.jedc.2020.104050,Cited by (0),"This study develops a business cycle search and matching model in which the volatility of employment closely matches that of the U.S. data without wage rigidity and the labor share overshoots in response to productivity shocks. To this end, I introduce an alternative mechanism of wage negotiations and bargaining shocks in an environment wherein a firm hires more than one worker and faces diminishing marginal product of labor (MPL). When Nash bargaining with a marginal worker breaks down, a firm negotiates wages with existing workers and produces with them. The breakdown in the negotiation with the marginal worker negatively affects the bargaining position of the firm with existing workers (one fewer workers) due to diminishing MPL because MPL is higher with one fewer workers. How much the firm internalizes this negative effect depends on the stochastic bargaining powers of existing workers, which can be identified through labor share data. The stochastic bargaining power of existing workers provides an additional margin to increase the volatility of labor market variables, including wages.","This study develops a business cycle search and matching model in which the volatility of employment closely matches that of the U.S. data without wage rigidity and the labor share overshoots in response to productivity shocks. In literature, two different bargaining protocols are used in the search and matching model wherein a firm hires more than one worker and faces diminishing marginal product of labor (MPL). One protocol is the Stole and Zwiebel (1996) type bargaining protocol as in Acemoglu and Hawkins (2014); Dossche et al. (2018); Elsby et al. (2013); Hawkins (2015), and Kudoh et al. (2019). In these papers, a breakdown in a negotiation with a marginal worker negatively affects the bargaining position of the firm with other workers (one fewer workers) because MPL is higher with one fewer workers. The other protocol is a standard bargaining protocol as in Andolfatto (1996); Merz (1995), and Cheron and Langot (2004). In these papers, a breakdown in a negotiation does not affect the bargaining with other workers because they implicitly assume that MPL does not change when the firm bargains with other workers. I interpret these two bargaining protocols as two extreme cases in terms of the relative bargaining powers between a firm and other workers. I will call other workers as existing workers. If existing workers have all the bargaining powers====, then the firm has to fully internalize the negative effects from the breakdown in the negotiation with a marginal worker. However, if the firm has all the bargaining powers====, then the firm does not internalize any negative effects from the breakdown by ignoring that MPL is higher with one fewer workers. Given the two extreme cases, I am looking at cases between the two extremes by introducing stochastic bargaining powers of existing workers.====In this study, when Nash bargaining with a marginal worker breaks down, a firm negotiates wages with existing workers. The bargaining power of existing workers differs from the bargaining power of the marginal (or new) worker and stochastically moves. The breakdown in the negotiation with the marginal worker negatively affects the bargaining position of the firm with existing workers (one fewer workers) due to diminishing MPL because MPL is higher with one fewer workers. How much the firm internalizes this negative effect depends on the stochastic bargaining powers of existing workers, which can be identified through labor share data. During expansions, hiring workers is relatively difficult for firms; thus, existing workers have relatively high bargaining power. If the firm fails to hire a marginal worker due to a breakdown in negotiations, then it has to pay higher wages to existing workers. Considering that the failure to hire marginal workers is more costly during expansions, the firm has more incentive to hire marginal workers by offering higher wages (and hours per worker) to forgo the higher cost associated with the breakdown. The stochastic bargaining power of existing workers amplifies this mechanism. The bargaining power of existing workers increases more during expansions through pro-cyclical bargaining shocks in addition to an improvement in the outside option of existing workers that occurs even when a fixed bargaining power of existing workers is assumed. During recessions, the opposite happens. The stochastic bargaining powers of existing workers provide an additional margin to increase the volatility of labor market variables through this mechanism. The stochastic bargaining power of existing workers simultaneously increases the volatility of employment, hours per worker, and wages. This mechanism does not rely on wage rigidity. Mechanically, the bargaining power of existing workers affects the outside option value for firms (i.e., cost associated with a breakdown in negotiations with marginal workers) in this study. The pro-cyclical bargaining power of existing workers results in the counter-cyclical firm’s outside option and the pro-cyclical firm’s surplus. Consequently, the pro-cyclical bargaining power of existing workers leads to more flexible wages. The calibrated model generates more volatile total hours, employment, hours per worker, and wages while labor share overshoots in response to productivity shocks as documented in Ríos-Rull and Santaeulalia-Llopis (2010). In particular, the volatility of employment in the model is similar to the actual U.S. data. In contrast to the prediction of Ríos-Rull and Santaeulalia-Llopis (2010), in which the effect of productivity shocks is dampened when labor share overshoots due to the huge wealth effects from the overshooting property of labor share, this study presents a model in which the labor share overshoots in response to productivity shocks and the volatility of employment closely matches that of the U.S. data without wage rigidity.====This study is related to several studies which can be classified into three groups. First, the baseline model is based on Andolfatto (1996). His model embeds search and matching framework into an otherwise standard RBC model, and has both extensive and intensive margins. The model improves the standard RBC model along several dimensions by incorporating search and matching framework in labor markets. However, the volatility of labor market variables is still far lower than that of actual data. The Andolfatto model also has highly pro-cyclical real wages and labor productivity, which have weakly pro-cyclical counterparts in actual data. Several papers have addressed these problems. Nakajima (2012) analyzes several volatility problems by explicitly distinguishing between leisure and unemployment benefits for the outside options of households. This distinction is consistent with the calibration proposed by Hagedorn and Manovskii (2008). However, the main focus of Nakajima (2012) is unemployment and vacancies than employment and hours per worker, which are my main interest. Cheron and Langot (2004) address the second failure of Andolfatto (1996) by using non-separable preference between consumption and leisure; accordingly, the outside options of households can counter-cyclically move. This proposal results in less pro-cyclical real wages and labor productivity. However, this study is not interested in the volatility of labor market variables in general.====The second branch of papers related to my study is the literature on the Stole and Zwiebel bargaining and its application to macroeconomics. Cahuc and Wasmer (2001); Ebell and Haefke (2003), and Cahuc et al. (2008) first incorporate the Stole and Zwiebel bargaining into a search and matching model in macroeconomics.==== Recently, several studies have been conducted on the application of the Stole and Zwiebel type bargaining to business cycle dynamics.==== Krause and Lubik (2013) incorporate the Stole and Zwiebel type bargaining protocol into a simple RBC search and matching model to evaluate the quantitative effects of the bargaining protocol on business cycle dynamics. They show that the aggregate effects of the bargaining protocol on business cycle moments are negligible. In contrast with Krause and Lubik (2013), this paper introduces the stochastic bargaining with existing workers when the match with a marginal worker fails, and the bargaining powers of existing workers stochastically vary. The time-varying incentive to hire workers for firms, resulting from the stochastic bargaining powers, provides a new margin to increase the volatility of labor market variables.==== Dossche et al. (2018) and Kudoh et al. (2019) examine the labor market effects of variable hours per worker (intensive margin) with the Stole and Zwiebel type bargaining. Dossche et al. (2018) find that the overhiring is larger when firms can bargain not only employment but also hours per worker with workers because hiring new workers decreases hours per worker for existing workers and lowers the values of the outside options and equilibrium wages for the workers. Kudoh et al. (2019) show that labor market fluctuations can be amplified even with a small Frisch elasticity when hours per worker are not negotiated and are determined by firms. Their model can successfully explain the labor market fluctuations in Japan.====Lastly, this study is also related to papers studying labor share dynamics. Ríos-Rull and Santaeulalia-Llopis (2010) document several properties of labor share dynamics on the basis of the U.S. data. In particular, they propose redistributive shocks that can be identified by using labor share data in the US and point out the importance of the dynamic property of labor share. They showed that labor share overshoots in response to productivity shocks (overshooting property), and the dynamic overshooting response of labor share drastically dampens the role of productivity shocks on labor markets due to huge wealth effects. My model also generates the overshooting property of labor share. However, the total hours, employment, and hours per worker are still more volatile than the benchmark Andolfatto model. In contrast with Ríos-Rull and Santaeulalia-Llopis (2010), the search and matching framework weakens the wealth effects from the overshooting of labor share and the pro-cyclicality of incentive for firms to hire workers is amplified by bargaining shocks and offsets the huge reduction of total hours. Colciago and Rossi (2015) and Mangin and Sedlacek (2018) develop a search and matching model that can generate the counter-cyclicality and overshooting property of the labor share. Colciago and Rossi (2015) reflect the strategic interactions among an endogenous number of producers in the model, which leads to counter-cyclical price markup. Mangin and Sedlacek (2018) introduce heterogenous firms competing to hire workers and investment-specific technology shocks to improve the labor share dynamics in the model. The main focus of these two papers is labor share itself than other labor market variables such as employment and hours per worker.====The main contribution of this study is as follows. First, this study presents a model in which the volatility of employment closely matches that of the U.S data without wage rigidity by incorporating the stochastic bargaining power of existing workers into the Andolfatto model.==== Jung and Kuester (2015) assume the stochastic bargaining power of marginal workers (not exisiting workers). The counter-cyclical bargaining power of marginal workers results in more rigid wages by construction and the amplification of labor market fluctuations.==== By contrast, this study introduces the pro-cyclical bargaining power of existing workers (not marginal workers) resulting in more flexible wages relative to the standard business cycle model with search frictions, such as that of Andolfatto (1996). Despite the more flexible wages, the volatility of labor market variables increases. The bargaining powers of existing workers can be time-varying because when labor markets are tighter, mostly in booms, existing workers are more valuable because the firm will have difficulty finding new workers. However, existing workers become less attractive to firms when the labor market is less tight, mostly in recessions, because new workers can be easily found. This reason makes the bargaining powers of existing workers possibly pro-cyclical with some lags. The stochastic bargaining power of existing workers amplifies the pro-cyclicality. The inclusion of the stochastic bargaining with existing workers improves the capacity of the standard RBC search and matching model, especially in the volatility of total hours, employment, hours per worker, wages and labor share.====Second, my model generates an overshooting property of labor share which is driven by exogenous bargaining shocks. However, the effect of productivity shocks on labor market variables are still significant in contrast with the prediction of Ríos-Rull and Santaeulalia-Llopis (2010). In their model, the effect of productivity shocks is dampened when labor share overshoots because of the huge wealth effects from the overshooting property. In contrast with their model, the baseline model has a search and matching framework. The nature of this framework weakens the wealth effects resulting from the overshooting of labor share. On top of these differences, the high incentive for firms to hire workers due to bargaining shocks offsets the huge reduction of total hours in booms.====The remainder of the paper is structured as follows. Section 2 introduces the baseline model with the stochastic bargaining powers of existing workers. Section 3 discusses the calibration of the baseline model. Section 4 shows the quantitative analysis of the model. Section 5 examines the robustness of the baseline model. Section 6 discusses the implications of the paper for the trend decline in labor share. Finally, Section 7 concludes and proposes the further research.",Wage negotiations in multi-worker firms and stochastic bargaining powers of existing workers,https://www.sciencedirect.com/science/article/pii/S0165188920302189,11 December 2020,2020,Research Article,246.0
Bruns Martin,University of East Anglia United Kingdom,"Received 23 July 2020, Revised 11 November 2020, Accepted 20 November 2020, Available online 5 December 2020, Version of Record 9 January 2021.",https://doi.org/10.1016/j.jedc.2020.104046,Cited by (2),"I propose a ====, I extract factors from a wide range of real and financial series and find that the effects of ==== shocks vary along the yield curve. In a second application to oil market shocks I add disaggregated US series to a standard model of the global oil market. I find that negative news about future oil supply have adverse effects on the US economy.","Proxy vector-autoregressive (Proxy-VAR) models are widely used to study the dynamic impact of structural shocks on the economy. Their popularity stems from the fact that they avoid the use of potentially non-credible timing restrictions implicit in conventional recursive identification schemes. However, typical Proxy-VAR models include only a small number of variables. This has two potential disadvantages: First, it is difficult to examine the impact of structural shocks at a disaggregated level in a single, unified model. Second, broad economic concepts need to be measured using individual, often narrowly defined, variables. These considerations motivate a transition to a data-rich environment.====A natural approach to summarise the information in rich datasets is provided by factor-augmented VAR (FAVAR) models. FAVAR models augment small-scale VARs with latent factors extracted from a large number of series. In so doing, they allow for a disaggregated analysis in a single, unified model. At the same time, FAVARs offer a way of controlling for abstract economic concepts such as “output”, “price level”, or “financial conditions”. They avoid having to associate these concepts to single, somewhat arbitrarily-chosen data-series. Instead, the approach employs information from a large number of series related to these concepts, e.g. financial spreads with different maturities, and summarises their joint behaviour in latent factors.====The first contribution of this paper is to propose a Bayesian Proxy Factor-augmented VAR (BP-FAVAR) model. This model offers a unified framework to combine a rich dataset with an identification strategy based on a proxy. It extends the approach proposed by Caldara and Herbst (2019) to allow for latent factors and accounts for their estimation uncertainty in a consistent Bayesian framework. The second contribution of the paper is to investigate the properties of the BP-FAVAR in a simulation exercise and in two applications for which the inclusion of factors is particularly natural and for which proxies are available.====The first application investigates the effects of monetary policy shocks. Caldara and Herbst (2019) revisit the question of which variables central banks’ policy decisions were based on during the Great Moderation in a small-scale Proxy VAR model. They rely on a high-frequency identification scheme and make the observation that central banks base their policy decisions not just on deviations of output, unemployment and inflation from their targets but also on financial conditions. In order to measure financial conditions, Caldara and Herbst (2019) include a corporate bond yield spread, the Baa spread, as an additional variable.==== They show that when including this variable, monetary policy shocks have large and persistent effects on real activity and prices. However, Baa-rated debt accounts for less than half of total corporate debt, making the Baa spread a potentially too narrow measure of financial tightness.==== This narrow definition could potentially lead to the model being non-fundamental for the shock of interest and restricts the analysis of the effects of monetary policy to this measure. Therefore, the inclusion of a broader measure of financial conditions is warranted. This need for a broader measure of financial conditions (and other economic concepts) leads me to transition to the framework by Bernanke et al. (2005) using a single observable factor, the policy rate, and extract factors from a large number of informational series, avoiding the need to measure abstract economic concepts using narrowly-defined variables. I exploit the BP-FAVAR model setup to investigate the reaction of financial spreads with different maturities to monetary policy shocks. I find that monetary policy shocks have an effect that varies substantially across the yield curve, especially in the medium run.====I then apply the model to oil market shocks. I revisit Känzig (forthcoming) who proposes a new proxy to investigate the effects of shocks in the global oil market on the US economy. Employing a small-scale Proxy VAR model, the author identifies oil supply news shocks in the global oil market and makes the observation that oil supply news shocks differ from conventional oil supply shocks in their effect on oil inventories. He then traces out the reactions of various US variables by estimating separate small-scale Proxy VAR models including various US variables of interest. This procedure omits cross-correlations among these US variables given that they enter in distinct models. For this reason, instead of estimating various distinct models, I propose to model the US variables jointly in the BP-FAVAR by adding latent factors from a large number of US series to an otherwise standard VAR model of the global oil market. I investigate the reactions of various US variables to the identified oil supply news shocks. I qualitatively confirm the findings in Känzig (forthcoming) about the adverse effects of negative news about future oil supply on US industrial production and unemployment rate. In addition, the data-rich BP-FAVAR allows to conclude that, consistent with economic intuition, the subcomponent of US industrial production related to fuel is most strongly affected and that US stock markets drop and rebound following an adverse oil supply news shock.====This study relates firstly to Bayesian FAVAR models such as Bernanke et al. (2005) who introduce the FAVAR model, Belviso and Milani (2006) who provide a structural interpretation of the latent factors, and Amir-Ahmadi and Uhlig (2015) who employ sign restrictions in FAVAR models. In addition, it broadly relates to early applications of Bayesian factor models such as Kose et al. (2003). Secondly, this study relates to the Bayesian Proxy VAR literature, most directly to the small-scale Bayesian Proxy VAR model by Caldara and Herbst (2019). Other studies employing external instruments in the Bayesian paradigm are Drautzburg (2020) who estimates a narrative DSGE-VAR model, Bahaj (2020) who applies high-frequency identification in a multi-country framework, and Arias et al. (2018) who propose a Proxy VAR framework amenable to an importance sampler. More generally, Stock and Watson (2012) combine factor models and proxies in a frequentist setting and Kerssenfischer (2019) investigates the relation between factor models and proxy identification. In parallel work to mine, Miescu and Mumtaz (2019) develop a Bayesian FAVAR model identified via a proxy and study its potential to address deficient information in small-scale models. Their model setup differs from mine along three main dimensions: First, they operate within a non-stationary factor model, while I employ data which are transformed to be stationary. Second, while I estimate latent factors using a Kalman filter, they use Principal Components (PC) analysis. Third, their focus is on the effect of latent factors on informational insufficieny issues of small-scale models. My focus is on an extension of the estimation algorithm, the choice of priors and a new posterior sampler. Mumtaz and Theophilopoulou (2019) apply a Baysian Proxy FAVAR to trace out the effects of UK monetary policy on disaggregated wealth inequality.====One challenge using factor-augmented VAR models is how to account for the estimation uncertainty of latent factors. This is technically challenging using bootstrap techniques in the popular frequentist approach of estimating factors via PC (see for example Yamamoto, 2019). In addition, there are no asymptotic results justifying the use of such techniques, as pointed out by Kilian and Lütkepohl (2017). To address this issue, I exploit the state-space representation of the model and employ the algorithm by Carter and Kohn (1994). This fully parametric approach treats the latent factors as random variables and samples from their posterior distribution. This procedure is included as an additional Gibbs step in the Metropolis-within-Gibbs sampler of Caldara and Herbst (2019).====The remainder of this paper is organised as follows: Section 2 introduces the model, Section 3 discusses the algorithm, priors, starting values, and a Monte Carlo exercise, Section 4.1 shows the monetary policy application and Section 4.2 discusses the oil market application. The last section concludes.",Proxy Vector Autoregressions in a Data-rich Environment,https://www.sciencedirect.com/science/article/pii/S0165188920302141,5 December 2020,2020,Research Article,247.0
"Colombo Luca,Labrecciosa Paola","Department of Economics, Deakin University, Burwood Campus, 221 Burwood Hwy, Burwood, 3125 VIC, Australia,Department of Economics, Monash University, Clayton Campus, Wellington Road, Clayton, 3800 VIC, Australia","Received 2 March 2020, Revised 5 November 2020, Accepted 8 November 2020, Available online 26 November 2020, Version of Record 18 December 2020.",https://doi.org/10.1016/j.jedc.2020.104030,Cited by (4),"In this paper, we present a ==== game model of duopolistic competition with ","In this paper, we extend the classical differential game model of duopolistic competition with sticky prices (Fershtman and Kamien, 1987) to an environment in which prices are not only sticky, but also stochastic.====Under the sticky price assumption, the current price does not adjust instantaneously to the level implied by current production. In the absence of uncertainty, at each point in time, firms face a price that they know will decline, but not instantaneously, if their joint output exceeds the level of demand at that price. When instead the evolution of the price is subject to stochastic disturbances, the price might increase over time even though firms’ joint output exceeds the level of demand at that price. The aim of this paper is to study how firms respond not only to the lag in the price adjustment, but also to the presence of uncertainty surrounding the price adjustment process. What are the (short- and long-run) implications of increased uncertainty for firms’ conduct in oligopolistic markets where prices are sticky and stochastic? What is the impact of uncertainty on the expected price? How about firms’ expected profits?====Intuition might suggest that increased uncertainty leads to a reduction in expected profits. We show that such an intuition might be misleading. Specifically, we show that uncertainty (modelled by means of a Wiener process affecting the evolution of the price) is beneficial to firms in terms of long-run expected profits and may be beneficial to firms in terms of discounted expected profits, depending on market size: if market size exceeds a certain threshold, then firms’ value functions increase as a result of an increase in the volatility of the process, otherwise, the opposite holds true. The expected price converges to a level that can be either higher or lower than the deterministic stationary price, depending on market size as well. We also find an interesting relationship between price fluctuations and long-run inflation (which could be empirically tested): price fluctuations and long-run inflation are negatively (resp. positively) correlated in relatively small (resp. large) markets. Furthermore, we show that the long-run stationary probability density of the market price can be computed explicitly, and corresponds to the Beta distribution.====Our game belongs to the class of differential games with white noise, in which uncertainty enters in the form of a Wiener process (see Dockner et al., 2000 and Haurie et al., 2012). Within this class, two subclasses can be identified that can be solved explicitly. The first subclass is that of stochastic games in which the HJB equations of the corresponding deterministic game, obtained by setting the diffusion coefficient equal to zero, have solutions for the value functions which are linear in the state. In this case, the same value functions are also solutions to the original (stochastic) HJB equations because the second-order derivative of the value functions vanishes. Examples are Sorger (1989) and Prasad and Sethi (2004). The second subclass is that of linear-quadratic games (to which our game belongs). The objective function and the dynamics of the corresponding deterministic game are quadratic and linear, respectively, and the square of the diffusion coefficient is either linear or quadratic. In this case, the guess of a quadratic value function leads to solutions of the HJB equations. An example is provided in Dockner et al. (2000, ch. 8).====Our paper contributes to the literature on sticky prices in oligopoly (e.g. Benchekroun, 2003, Dockner, 1988, Dockner, Gaunersdorfer, 2002, Fershtman, Kamien, 1987, Fershtman, Kamien, 1990, Simaan, Takayama, 1978, Tsutsui, 1996, Tsutsui, Mino, 1990, ====) by introducing uncertainty. The introduction of uncertainty makes it possible to study questions new to this literature. To the best of our knowledge, although several extensions to the seminal paper by Fershtman and Kamien (1987) have been proposed, including generalizing the number of firms to ==== (see Dockner, 1988), relaxing the infinite time horizon assumption (see Fershtman and Kamien, 1990), expanding the class of feedback strategies to include also nonlinear strategies (see Tsutsui and Mino, 1990), and allowing for capacity constraints (see Tsutsui, 1996), the existing sticky price game-theoretic literature has only considered deterministic games.==== With this paper, we aim to fill this gap in the literature, and provide a stochastic version of Fershtman and Kamien (1987) that can be used to improve our understanding of oligopolistic markets in the presence of price uncertainty.====Our paper also adds to the broad literature on stochastic dynamic games in oligopoly, which includes: (i) dynamic games of innovation (e.g. Chang, Wu, 2006, Choi, 1991, Dawid, Keoula, Kopel, Kort, 2015, Dawid, Kopel, Kort, 2013, Dockner, Feichtinger, Mehlmann, 1993, Doraszelski, 2003, Long, 2010, Malueg, Tsutsui, 1997, Reinganum, 1981, Reinganum, 1982, Smrkolj, Wagener, 2019, Weeds, 2002, ====); (ii) dynamic games of strategic investments under uncertainty (e.g. Baldursson, 1998, Boyer, Lasserre, Mariotti, Moreaux, 2004, Boyer, Lasserre, Moreaux, 2012, Dockner, Siyahhan, 2015, Genc, Reynolds, Sen, 2007, Huberts, Huisman, Kort, Lavrutich, 2015, Huisman, Kort, 1999, Huisman, Kort, 2015, Pawlina, Kort, 2006, Thijssen, Huisman, Kort, 2012, Billette de Villemeur, Ruble, Versaevel, 2019, ====); (iii) stochastic diffusion games in corporate finance (e.g. Aguerrevere, 2009, Bustamante, 2015, Carlson, Dockner, Fisher, Gianmarino, 2014, Grenadier, 1996, Grenadier, 2002, Lambrecht, 2001); (iv) stochastic fishery games (e.g. Haurie, Krawczyk, Roche, 1994, Haurie, Krawczyk, Zaccour, 2012, Jørgensen, Yeung, 1996, Kaitala, 1993, Wang, Ewald, 2010, ch. 11). In (i), innovation is modelled as a competitive process (race for technological breakthrough) among potential innovators that aim to be the first. The time of completion of the project is a random variable. In (ii) and (iii), the single investor framework of real options model (see Dixit and Pindyk, 1994) is extended to a strategic environment where the profitability of each firm’s project is affected by other firms’ decision to invest. The value of the investment project is subject to stochastic disturbances. This literature extends the classical timing games originating from the seminal paper by Fudenberg and Tirole (1985) to shed new light on preemptive investments, strategic deterrence, dissipation of first-mover advantages, and investment patterns under irreversibility of investments and demand uncertainty.==== In (iv), oligopolistic firms jointly exploit a common-pool renewable resource whose dynamics is governed by a stochastic differential equation. This literature studies, among other things, the impact of uncertainty on firms’ harvest strategies and profits.====Our analysis applies to all oligopolistic markets in which prices are sticky and uncertain. A prominent example is that of electricity markets. Indeed, electricity prices are sticky and uncertain. They are sticky because they do not vary with changes in overall supply and demand conditions, marginal costs, or wholesale market prices from either an ex ante or real time perspective (see Joskow and Wolfram, 2012).==== They are uncertain mainly because the costs of electricity generation vary over time in a way that can be predicted only to some extent. Changes in weather conditions, for instance, are one of the main drivers of changes in electricity prices, together with fluctuations of generation capacities and volatility of commodity prices. Consider, for instance, coal-fired power plants, which constitute a considerable part of generation capacities in many markets. The price of electricity produced by coal-fired power plants is affected by new coal-fired power plants as well as by changes of prices of coal and emission allowances. Another reason why price fluctuations are observed in electricity markets is that electricity is scarcely storable, therefore there is no arbitrage relation between spot prices and futures prices.====Fig. 1 above presents the quarterly volume weighted average spot electricity prices in each region of the National Electricity Market (NEM) in Australia in the time period 2014-2019.==== As can be seen in Fig. 1, spot electricity prices in Australia are highly volatile, exhibiting a mean-reverting behavior. Similar patterns have historically been displayed by spot electricity prices in deregulated electricity markets in the United States and Europe.====The remainder of this paper is organized as follows. The set-up is presented in Section 2. The equilibrium characterization is provided in Section 3. In Section 4, we deal with the impact of uncertainty on equilibrium strategies and expected profits. In Section 5, we compute the long-run stationary probability density of the market price. Concluding remarks are in Section 6.",A stochastic differential game of duopolistic competition with sticky prices,https://www.sciencedirect.com/science/article/pii/S0165188920301986,26 November 2020,2020,Research Article,248.0
"Zsurkis Gabriel,Nicolau João,Rodrigues Paulo M. M","Banco de Portugal and ISEG-Universidade de Lisboa, Portugal,ISEG-Universidade de Lisboa and CEMAPRE, Portugal,Banco de Portugal and Nova School of Business and Economics, Portugal","Received 24 April 2020, Revised 18 November 2020, Accepted 20 November 2020, Available online 26 November 2020, Version of Record 14 December 2020.",https://doi.org/10.1016/j.jedc.2020.104047,Cited by (0),In this paper we introduce a flexible framework to estimate the expected time (ET) an outcome variable takes to cross a threshold conditional on ,"The first hitting time or first passage time, i.e., the time a variable takes to reach a certain value, is a fundamental concept in stochastic analysis and represents an important modeling tool in fields such as economics, finance, biology and the life sciences.====Although there is a large literature in economics and finance addressing this topic (see, for instance, Durbin, 1971, Lo, MacKinlay, Zhang, 2002 or Giesecke, 2006), first-hitting time densities are mostly obtained for Wiener diffusion processes under the assumption of continuous-time, due to the tractability offered by the It calculus. However, this approach often requires strong computational efforts and closed form solutions are known only for some standard continuous-time models.====Since most economic and financial data is only available in discrete time, researchers usually opt for modeling duration time as a stochastic process instead of defining duration as the first time a stochastic process crosses a given threshold. Thus, continuous-time based first passage time densities and duration models (typically built in discrete time) have the same objective, namely, to characterize the length of time that separates different stochastic events. In fact, as illustrated by Whitmore (1986), duration models can be seen as reduced form representations of first passage time densities.====The extant literature on duration analysis is based on the specification of the hazard function, that is, on the conditional probability of exiting the initial state within a short interval having survived up to the starting time of that interval. Thus, the hazard function specification emphasizes the conditional probabilities==== Since a duration process can intuitively be associated with a dynamic sequence of conditional probabilities, the hazard-based approach is a convenient way to interpret duration data and can be sufficiently flexible to handle relevant issues such as the presence of censored observations and time-varying covariates. Parametric hazard models have been used in labor economics to examine duration dependence and the determinants of unemployment exit probabilities (see, for instance, Meyer, 1990, McCall, 1994 and Sueyoshi, 1995); in the analysis of firm survival (see, for instance, Audretsch and Mahmood, 1995 and Mata and Portugal, 2002); and in the analysis of duration dependence in economic cycles (see, for instance, Sichel, 1991, Ohn, Taylor, Pagan, 2004 and Berge and Pfajfar, 2019).====A closely related approach to duration dependence modeling is to treat the occurrence of a given event as a random variable which follows a point process==== A point process is a sequence of non-negative random variables representing the times at which events occur, which is defined as ==== with ====. A complete description of such processes is formulated in terms of the conditional intensity function which can, roughly speaking, be associated to the probability per unit of time of observing an event in the next instant==== Thus, different parameterizations of this function result in different point process models. Existing models can be grouped into two classes. The first class, formulated in calendar time, considers that the marginal effects of an event that has occurred in the past is independent of the intervening history; and the second class, focuses on the intervals between events and assumes that the duration between successive occurrences depends on the number of intervening events. The autoregressive conditional dynamic (ACD) model proposed by Engle and Russell (1998) is an important model of this class====.====In this paper, we focus on first hitting time processes. Thus, unlike point process models, we are not interested in the actual sequence ==== but only in the random variable associated with the time at which the event occurs for the first time (====). First hitting time problems have been mostly addressed in a continuous time context invoking Wiener processes, which involve complex mathematical concepts and can result in models which are difficult to estimate. Nicolau (2017) introduced an intuitive and easy to implement framework for estimating the first passage time probability function in a discrete time context. One of the main contributions of the present work is the development of a novel approach to estimate transition probabilities allowing for covariates, which corresponds to an important extension of this framework. To this end, we generalize the approach proposed by Islam and Chowdhury (2006) to estimate covariate-dependent Markov models of any order to the present context.====Understanding how a set of covariates influences the time a response variable takes to cross a fixed threshold may provide relevant insights on the potential causal relationships between economic variables. The proposed covariate-dependent expected time (ET) to cross a threshold estimator may also be a useful tool to support macroeconomic policy decisions, where there are desirable values or even formal targets for some key variables, such as, output growth, inflation, or unemployment. Thus, it is important to assess the effectiveness of the covariates in driving the outcome variable towards some preassigned values. In practice, the impact of covariates may not be symmetric and may depend on the distance between the starting point and the target value. Consider, for instance, the connection between monetary policy and real economic growth. Since both negative and above-trend growth rates are undesirable, monetary policy plays a key role in fostering a healthy level of economic growth. To this end, a tight monetary policy is adopted when rapid economic growth causes inflationary pressures and an easy one in a recession context in order to boost a rapid economic recovery. However, it has been shown in the literature that the responsiveness of the real economy to monetary shocks is different during recessions and expansions (see, for instance, Romer, Romer, 1994, Florio, 2004, Lo, Piger, 2005 and references therein). The framework introduced in this paper allows us to investigate these possible nonlinear dynamics by estimating ET conditional on different starting values. If, for a given starting value, changes in covariates are reflected in changes in the ET estimates, it suggests that the chosen covariates affect the movement towards a specified threshold in that specific situation. When other starting values are considered conclusions may however differ.====More specifically, the proposed methodology is also a relevant contribution to the vast literature on the effects of fiscal and monetary shocks on economic fluctuations (see Ramey, 2016 for a recent survey). It provides a useful complement to the traditional approach which is based on estimating macroeconomic shock multipliers by providing further information on the sensitivity of an economic variable to macroeconomic policy changes. Rather than measuring the average effect of the shocks during a given time horizon, the interest lies in inferring whether macroeconomic policy shocks promote a faster economic stabilization.====To further illustrate the usefulness of our approach an application to U.S. unemployment persistence is provided. Specifically, we analyse the response of U.S. unemployment persistence to monetary policy and government spending shocks. This is an important contribution, since with a few exceptions, such as e.g. Rossi and Zubairy (2011), most existing literature only focuses on one of these shocks at a time (see, inter alia, Romer, Romer, 2004, Blanchard, Perotti, 2002, Ramey, Zubairy, 2018, and Brinca et al., 2019). The framework introduced in this paper allows us to consider both monetary and fiscal shocks simultaneously, which provides relevant insights on the impact of the interaction between these two macroeconomic shocks on the dynamics of the unemployment gap.====As a measure of persistence, we consider the expected time the unemployment rate takes to return to its natural rate, where the labor market is in a sustainable equilibrium. Our results suggest that fiscal shocks are more effective than monetary shocks in stimulating a faster return of unemployment to the natural rate of unemployment in a context of labor market slack. These results are in line with recent findings pointing to the importance of fiscal policy for short-run economic stabilization (see, for instance, Romer, 2012). However, the effects of fiscal policy are highly regime dependent, for instance, Auerbach and Gorodnichenko (2012) show that the estimated fiscal multipliers of government purchases are larger in recessions. In our analysis, we consider positive starting values, i.e., ==== (==== was not addressed due to its limited economic relevance), which corresponds to a positive unemployment gap, which is typically associated with negative output gaps and recessions. Thus, indirectly, our focus centers on whether macroeconomic shocks stimulate a faster recovery from economic downturns. Our results also suggest that the relative effect of monetary policy on the ET is lower when ==== than when ====. As ==== suggests that we may be facing a persistent weak demand environment, these findings are in line with the limited effectiveness of expansionary monetary policy in the presence of bad expectations about the future reported, for instance, in Florio (2004) and Tenreyro and Thwaites (2016).====The remainder of the paper is organized as follows. Section 2 introduces the proposed methodology to estimate the conditional ET to cross a threshold (given a specific starting point). Section 3 investigates the finite sample properties of the parameter estimates that describe the relationship between ET and the covariates. Section 4 presents an empirical application which investigates the response of U.S. unemployment persistence to monetary policy and government spending shocks. Section 5 concludes and a Technical Appendix collects detailed proofs of the results presented in the paper.",The expected time to cross a threshold and its determinants: a simple and flexible framework,https://www.sciencedirect.com/science/article/pii/S0165188920302153,26 November 2020,2020,Research Article,249.0
"Acevedo Giancarlo,Bernales Alejandro,Flores Andrés,Inzunza Andrés,Moreno Rodrigo","Central Bank of Chile, Agustinas 1180, Santiago, Chile,Facultad de Economía y Negocios, Departamento de Administración, Universidad de Chile, Diagonal Paraguay 257, Santiago, Chile,Center of Applied Economics, DII, Universidad de Chile, Av. Beauchef 850, Santiago, Chile,Center for Energy and Environmental Policy Research at the Massachusetts Institute of Technology, 77 Massachusetts Avenue, E19-411, Cambridge, MA 01239-4307, USA,Facultad de Ciencias Físicas y Matemáticas, Departamento de Ingeniería Eléctrica, Universidad de Chile, and Instituto Sistemas Complejos de Ingeniería (ISCI), Av. Beauchef 850, Santiago-Chile,Imperial College London (Department of Electrical and Electronic Engineering), South Kensington Campus, London SW7 2AZ, UK","Received 27 August 2019, Revised 14 October 2020, Accepted 27 October 2020, Available online 19 November 2020, Version of Record 8 May 2021.",https://doi.org/10.1016/j.jedc.2020.104027,Cited by (2),We demonstrate that ,"“==== [...] ====” (June 2017 Statement by President Donald Trump on the Paris Climate Accord—White House.)====It is well-known that detractors of environmental policies—which promote energy production based on environmentally friendly technologies—argue that renewable energy production is not economically efficient from a private investing perspective. These views only analyze the economic perspective of pure cost minimization. This study demonstrates that environmental policies can also provide economic benefits in terms of decreasing risk when multiple renewable technologies are used simultaneously.====We use a model of optimal expansion planning for electricity production of a private investor, under environmental policies, where costs and risks are considered. In the model, new generating plants can be based on diverse renewable technologies (e.g. geothermal, small hydro, biomass, wind, hydro-reservoirs, run-of-the-river and solar technologies) and/or non-renewable technologies (e.g. coal, oil and gas technologies). We consider the investment costs of new generating plants, and their operational costs (i.e. maintenance and fuel price costs). Risks are not only related to economic uncertainty (e.g. volatility of fossil fuel prices and changes in energy demand), but also to unexpected financial costs associated with unsatisfied energy needs that are induced by renewable generation intermittency (e.g. penalties for unmet demand and the use of costly generation reserves to reduce generation intermittency).====The model also includes several characteristics of modern energy systems. First, we explicitly consider the penalties applied when electricity demand remains unsatisfied.==== Second, we include the use of (and costs associated with) generation reserves (i.e. backup generating capacity) to deal with the intermittency risk of renewable generation (including, as in reality, the reaction speed of such reserves). Third, we consider the possibility that an unexpected contingency may occur in the generating plants when using these reserves, such as the sudden failure of a generator, as modeled by Joskow and Tirole (2007) and Gowrisankaran et al. (2016). Fourth, we consider the use of demand-side services (DSS), in which the option exists for demand-shifting (i.e. moving a certain amount of demand from one hour during a day to another using economic incentives). Fifth, we allow for the possibility of forecasting weather conditions on an hourly basis to consider potential forecasting errors in using system reserves.====The model also characterizes a ==== diversification effect while considering diverse types of risks that may affect energy generation. If we decompose the electricity supply into different generating technologies, each technology can be observed as an asset within an energy-generation portfolio. Therefore, given that environmental policies induce an increase in the installed capacity and generation of ==== forms of renewable energy production (e.g. energy plants based on small hydro, solar photovoltaic (PV), wind, biomass, geothermal and concentrated solar power (CSP), amongst others), this induces an increase in the diversification of energy production. Thus, in a diversified energy portfolio, each generating technology can be used to ==== the risks of other technologies used for energy production, which induces a risk reduction in the whole energy system.====Consequently, we demonstrate that when implementing environmental policies provides a combination of traditional and ==== non-conventional technologies for energy production, this can decrease generation risks through basic diversification benefits. This result is highly relevant, as the implementation of environmental policies can be justified from both environmental and economic perspectives, given the decrease in risk for private investors.====We apply the model to a real case of electricity-production planning. The objective in applying this model is to provide an example of implementation and analyze potential expansion planning under environmental policies, which can be used as a guide in applying the model to other countries or regions. We use the Chilean Central Interconnected System (CIS) to determine optimal generation planning for the year 2025. In this implementation, we use information (and installed capacity) up to December 2014 from the Chilean CIS. We also use a unique dataset of ====, which are responsible for the renewable generation intermittency that affects electricity production.====As previously explained, our work indicates that when new forms of renewable power are used due to the implementation of an environmental policy, the intermittent generation of a given renewable technology can be hedged with other forms of renewable power. For instance, we observe that small hydro-power exhibits the lowest average production between 04:00 and 05:00, but high wind-generated energy is available at that time. Additionally, although solar PV generation is highly attractive when producing energy at noon, since solar radiation is at a maximum, no solar generation is available at night. Nevertheless, an increase in availability from small hydro and wind generation, which begins to increase after 18:00, compensates for a lack of nightly renewable production from solar PV plants. Further, biomass and geothermal energy-based plants generate stable electricity (e.g., Perez-Navarro et al., 2010; Heydari and Askarzadeh, 2016; Kulasekara and Seynulabdeen, 2019), which can also address the intermittency of other forms of renewable generation.====Thus, our study differs from previous literature, in which only a few renewable technologies are analyzed in terms of the correlation between their generated outputs. Generally, only the risk-reduction benefits from the interaction between wind and solar renewable generation are examined (Graabak and Korpås, 2016), instead of the hedging advantages gained when ==== renewable and non-renewable technologies produce energy simultaneously, as in our study.====We also demonstrate that increasing renewable technologies due to environmental policies can hedge changes in electricity demand, which is also stochastic. As an example of such hedging interaction in the implementation of the model, on a typical Chilean day, electricity demand is high at around 22:00, when there is also high generation availability from small hydro technology (rivers receiving snowmelt water from the Andean Mountains). In addition, on a typical Chilean day, there is, on average, high electricity demand between 12:00 and 14:00, which coincides with the highest availability of solar generation. These characteristics of renewable technologies (and other behaviors of renewable plants) can also be found in other countries and regions, and the model could thus also be implemented there.====Diverse environmental policies can be applied for energy production, including penalties and/or carbon taxes, among others (Stern, 2008; Harstad, 2012a, 2012b; Marron and Toder, 2014; Diaz et al., 2020). Although our work does not aim to analyze all policy tools, as a policy exercise we present the implementation of two of them, and evaluate them from the model's perspective. Thus, in this policy exercise, (i) we enforce a policy target of a minimum level of renewable generation through a strong penalty; and (ii) we impose a carbon tax. This policy analysis is relevant because private investors can evaluate the effect of regulations from two perspectives: the impact of environmental policies in terms of reducing the risk of energy production; and the financial costs associated with such regulations that are used to reach a given renewable generation goal.====The remainder of this paper is organized as follows: Section 2 presents a literature review, while Section 3 describes the model. Section 4 provides an example in which the model is implemented for optimal electricity-generation planning under environmental policies. Section 5 reports the main results from this implementation and a policy exercise; Section 6 concludes.",The effect of environmental policies on risk reductions in energy generation,https://www.sciencedirect.com/science/article/pii/S0165188920301950,19 November 2020,2020,Research Article,250.0
Zimmermann Paul,"IESEG School of Management, Department of Finance, 3 rue de la Digue, Lille 59000, France,LEM-CNRS 9221, 3 rue de la Digue, Lille 59000, France","Received 18 October 2019, Revised 8 October 2020, Accepted 15 November 2020, Available online 19 November 2020, Version of Record 7 December 2020.",https://doi.org/10.1016/j.jedc.2020.104033,Cited by (0),"Starting from the analysis of a levered firm’s capital structure, I show that corporate default risk becomes measurable through the leverage effect, i.e., the negative correlation observed between ","Merton’s (1974) structural model of credit risk implies that individual stock and corporate debt securities are reasonably close substitutes. The integration of credit and equity markets suggested by the standard financial theory incites rational investors to bet on the convergence of these two markets by arbitraging stocks against credit derivatives (Kapadia and Pu, 2012). The risk limits to capital structure arbitrage strategies being low,==== neither the equity market nor the credit market should dominate the price discovery of credit risk. However, recent studies have shown that insider trading may occur in the credit derivatives market and impound the price discovery process (e.g., Acharya and Johnson, 2007; Qiu and Yu, 2012; Kryzanowski et al., 2017). Such breaches to market efficiency raise the question as to which market attracts informed trading and arbitrage resources.====In this paper, I put forward a structural model of the leverage effect to interpret the dominance of credit markets in the price discovery process. In this model, the corporate leverage governs the transmission of price information between the credit and the stock market. When the firm’s financial leverage is low, the informational content of the credit market is low and produces low-intensity signals. As a result, credit traders are mostly noise or liquidity traders, and the bulk of the price discovery process primarily occurs in the stock market, in line with multiple empirical studies (e.g., Hilscher et al., 2015). Conversely, when the firm’s financial leverage gradually increases, rational and sophisticated credit investors acquire a gradual advantage in the gathering and the processing of information related to the firm’s credit quality. As credit traders tend to monopolize the incorporation of private information into prices, the transmission to the stock market intensifies due to the effect of the corporate leverage. The shift in intensity then pushes stock traders to chase the trend and morph into noise traders without their knowing. Everything happens as if the corporate leverage made informed trading migrate to the credit market.====My structural model for price transmission draws on the following insight. Relying on the economic concept of elasticity,==== I focus on the elasticity of default probabilities relative to stock prices. The intuition behind the credit-equity elasticity is that it conveys the joint correlation between changes in the firm’s market value and changes in the firm’s credit quality. Additionally, it delivers the optimal hedge ratio sought by capital structure arbitrageurs.==== The paper’s key result is then as follows. The structural framework proves to be rich enough to link the credit-equity elasticity to the elasticity of equity variance relative to stock prices. Simultaneously, the latter captures the so-called “leverage effect”==== as a linear function of the debt-to-asset ratio, one of the critical indicators of the firm’s financial health. Consequently, a simple function of the firm’s financial leverage turns out to encapsulate the signal for informed trading of the capital structure.====The model put forward offers far-reaching empirical implications. First, the credit-equity elasticity hypothesis brings support to the presence of a long-run equilibrium relationship between a firm’s credit spreads and equity prices. The intuition is that CDS and stock time series cannot drift too far apart from the equilibrium because capital structure arbitrageurs will act to restore the equilibrium relationship. By capturing the non-linear effect of the firm’s leverage, this co-integrating vector is distinct from the linear combination of the credit spread and the stock price already investigated in the literature (e.g., Kryzanowski et al., 2017, Narayan, Sharma, Thuraisamy, 2014).====Second, the model provides new testable hypotheses concerning the price discovery process at work in credit markets. If equity and credit prices are co-integrated, the permanent-transitory decomposition of Gonzalo and Granger (1995) ensures that they must track a common long-memory component, or efficient price (Hasbrouck, 1995). Meanwhile, an error-correction mechanism must absorb transitory shocks to reflect arbitrage across equity and credit markets. By specifying the co-integrated credit-equity system, my model allows computing the implicit efficient price of credit. Each market’s contribution to the price discovery process then becomes accessible.====Third, as already underscored in the literature (Choi and Kim, 2018, Kapadia and Pu, 2012), exogenous barriers to arbitrage such as funding constraints, liquidity risks, or short-sale impediments interfere with the co-movements in the equity and credit markets. This paper hypothesizes that the non-linear impact of the leverage effect may be one of the endogenous sources for the lack of integration between the credit and equity markets.====This paper uses a large dataset of S&P 500 firms and an extended timeframe (2008–2019) to examine the transmission of pricing information from the stock market to the credit default swap (CDS) market. By identifying the genuine price innovations arising in the stock market, I offer an empirical methodology to identify the non-linear impact of the financial leverage on the information flow transiting to the credit market. The paper’s main finding is the high statistical significance of this leveraged transmission mechanism to the CDS market. Uniform across firms, this newly-identified channel of transmission appears more intense than the direct unleveraged channel already investigated in previous literature.====Most firms in the sample reject the null hypothesis of no (leveraged-)co-integration between their equity and CDS markets. The empirical analysis shows that entities are more likely to be co-integrated when (i) they belong to a business sector perceived as more leveraged; (ii) they employ a higher debt-to-asset ratio; (iii) their CDS price is more volatile. These findings provide evidence that an increase in financial leverage ramps up market activity in capital structure arbitrage. One of the indirect market effects of corporate leverage is thus to intensify the integration between credit and equity markets.====For those firms which are significantly co-integrated, this study draws on the vector error-correction (VECM) approach of Gonzalo and Granger (1995) to identify the respective contributions of each market to the price discovery process. The CDS market share appears to be low and below 30% for the vast majority of firms, consistent with the CDS “sideshow” hypothesis (Hilscher et al., 2015). However, a small cluster of highly-leveraged firms exhibits an extremely dominant CDS market share close to 100%. This new finding provides reliable evidence for the role of the leverage effect in the price discovery process.====This paper relates to the vast empirical literature that investigates the price discovery process in credit markets. The conventional view states that credit pricing information primarily flows from stock markets to credit markets due to lower transaction costs (e.g., Hilscher et al., 2015). The alternative view underscores the role of private information in the flow of pricing information from credit markets to stock markets (e.g., Acharya and Johnson, 2007, Qiu and Yu, 2012). The most recent literature suggests that both credit and equity markets should potentially lead and lag the other market (Forte and Peña, 2009, Lee et al., 2018, Marsh and Wagner, 2016). By studying the endogenous, non-linear impact of the firm’s capital structure, this paper departs from a work of literature mainly focused on exogenous and linear transmission effects.====The article proceeds as follows. Section 2 contains the main theoretical contribution, while Section 3 discusses the economic implications of the theory. Section 4 describes the data used in the empirical analysis developed in Section 5. Finally, Section 6 concludes the article.",The role of the leverage effect in the price discovery process of credit markets,https://www.sciencedirect.com/science/article/pii/S0165188920302013,19 November 2020,2020,Research Article,251.0
"Georges Christophre,Pereira Javier","Department of Economics, Hamilton College, Clinton, NY 13323, USA","Received 11 January 2020, Revised 18 July 2020, Accepted 8 November 2020, Available online 12 November 2020, Version of Record 27 November 2020.",https://doi.org/10.1016/j.jedc.2020.104032,Cited by (2), is unlikely to entirely eliminate destabilizing speculation. They also accord well with the empirical “sparse signals” and “pockets of predictability” findings of Chinco et al. (2019) and Farmer et al. (2019).,"One source of mispricing in financial models is the overfitting of forecasting rules by market participants. We explore this mechanism in a simple agent-based model of a financial market. Our agents base their trading behavior on forecasts of future returns, which they update asynchronously. These forecasts then collectively drive market prices and feed-back on the data generating mechanism. It is well-known that under restrictive conditions, adaptive learning in such an environment can converge to a noisy version of the stationary rational expectations equilibrium (Honkapohja and Mitra, 2003). However, if agents are uncertain about the true model of the world in which they are operating, there is substantial room for learning to go further astray. In this context, the adoption of misspecified forecast rules can lead to additional excess volatility as well as market instability in the form of asset bubbles and crashes.====The models that economic agents entertain can be misspecified in various ways - e.g., the exclusion of relevant variables (underparameterization) (Evans and Honkapohja (2001)(Ch. 13), Branch and Evans (2006); Gabaix (2014); Hommes and Zhu (2014)), or the inclusion of intrinsically irrelevant variables (Branch, McGough, Zhu, Sunspots, 2019, Bullard, Evans, Honkapohja, 2008, Georges, 2008a, Georges, 2008b, Georges, 2015, Grandmont, 1998, LeBaron, 2012). In much of this literature, attention is given to the role of the misspecification in driving endogenous market dynamics, but it is often assumed that agents adopt misspecified rules uncritically and do not attempt to conduct model selection or other corrective methods in an effort to improve their forecasts. While fully rational expectations is empirically implausible, it is equally implausible that quantitative traders do not entertain the possibility that they are over- or underfitting the data. Indeed, this problem has generated a recent explosion of interest in predictive machine learning algorithms in financial markets.====Misspecification via overparameterization is particularly relevant in financial markets today. Financial market participants increasingly find themselves in data rich environments and have access to sophisticated computational resources. They face model uncertainty as always, but now have many possible predictors at their disposal and the machine learning tools to consider a wide variety of functional forms for combining these predictors. I.e., they can now entertain very substantial model complexity. Machine learning (ML) is a set of high dimensional statistical techniques for combining potentially large numbers of predictors in flexible ways to generate predictions. Much attention is given to the substantial scope, introduced by this model complexity, for overfitting the training data, i.e., for adopting overparameterized rules that perform well in the training sample but poorly out of sample. Consequently, machine learning typically employs methods for model regularization (parameter shrinkage), predictor selection, and out of sample testing to temper the model complexity and mitigate the risk of ovefitting. (E.g., Arnott et al. (2018); Gu et al. (2020); Hastie et al. (2009); de Prado and Lopes (2020)).====In the present paper, we focus on overparameterized forcasting rules and draw on the statistical and machine learning literature to endow our boundedly rational artificial traders with tools for model selection and regularization to adopt in their learning routines. These agents update their forecasts on two time scales. Every trading period they update their forecasts based on their current forecast rules and the most recent data. On a slower time scale, they perform forecast model selection and estimation based on a longer history of recent data. In our primary machine learning specification, the agents use the least absolute shrinkage and selection operator (LASSO), which performs both model selection and regularization using out of sample validation.====We work with a very simple artificial financial market based on Georges, 2008a, Georges, 2008b in which all traders are chartists who use nonlinear autoregressive (AR) rules for forecasting returns. These rules are over-parameterized but nest the fundamental rational expectations (RE) rule. Thus, traders are willing to extrapolate trends in the recent data that are not supported by rational expectations, but they could in principle learn not to follow such trends.====We compare market outcomes under LASSO learning with a corresponding baseline specification in which agents have fixed forecasting rules and update these with ordinary least squares (OLS). OLS learners overfit their forecasting rules to the data available to them. Trading on the resulting forecasts, they generate excess price volatility as well as market instability in the form of occasional explosive bubbles and crashes. This volatility and instability is increasing in the complexity of the agents’ forecasting rules.====We find that the addition of LASSO model selection and regularization methods to the traders’ learning algorithm substantially reduces, but does not eliminate, overfitting and the resulting excess volatility and market instability relative to the OLS baseline. Even though our LASSO learning traders are quite sophisticated about avoiding overfitting, they will still occasionally identify apparent predictability in the data, setting off a period of heightened volatility. These signals identified through LASSO tend to be sparse in the sense that only a subset of variables are identified as active predictors in a given episode. Interestingly, due to the intermittent nature of these discoveries of apparent predictability, LASSO acts as a powerful mechanism for generating fat tails and clustered volatility in returns, two near-universal empirical hallmarks of financial markets. The emergence of short-lived pockets of predictability with sparse signals is also supported by the empirical findings of Chinco et al. (2019) and Farmer et al. (2019) for U.S. equities. Further, while LASSO’s parameter shrinkage, variable selection, and out of sample validation cause estimated forecast rules to be both sparser and less extreme than rules estimated by OLS, we find that they are still extreme enough to produce occasional bubbles and crashes in the model. Our results suggest that even a high degree of attention to overfitting on the part of traders who are engaged in data mining is unlikely to entirely eliminate destabilizing speculation. We also consider several relevant machine learning alternatives to LASSO – stepwise selection, Ridge regression, and elastic nets – that similarly address the problem of overfitting and find similar results.====Our artificial traders can be thought of as model representations of either sophisticated human technical traders or purely algorithmic traders. While financial markets were once fully human operated, modern markets are hybrid spaces where computer and human trading coexist. The use of computers in asset markets now comes in many forms ranging from the simple support of human traders in the scheduling of buying and selling assets to the more sophisticated algorithmic traders which can learn and autonomously decide which assets they sell or buy (Kirilenko and Lo, 2013). According to Kaya (2016), high frequency trading – just one form of algorithmic trading accounted for almost 50==== of all the volume traded in the US equity markets in 2014. In a recent global survey including FinTechs and incumbent financial institutions (Ryll et al., 2020), 60 percent of surveyed investment managers indicated that they are already using artificial intelligence (AI) in their investment process, with 55 percent implementing AI for asset price forecasting. These practitioners also overwhelmingly reported expecting AI to become increasingly important in contributing to investment returns in the next five years.====A central issue with regard to algorithmic trading and other forms of digital automation is whether this activity has improved overall market quality, thus allowing investors to raise capital and manage risks more efficiently. Behavioral finance is often skeptical of the efficient market theory, suggesting that stock prices are to a certain extent predictable due to psychological and social aspects that lead to financial market inefficiencies (Shiller, 2003). Removing emotional entities from the market might thus be expected to improve market efficiency (Chaboud et al., 2014). However, algorithms that identify and trade purely on patterns in recent historical data may also deviate substantially from the efficient markets, rational expectations benchmark. Our results suggest that such deviations are unlikely to be fully eliminated as algorithms become increasingly sophisticated.",Market stability with machine learning agents,https://www.sciencedirect.com/science/article/pii/S0165188920302001,12 November 2020,2020,Research Article,252.0
"Bhattarai Saroj,Chatterjee Arpita,Park Woong Yong","2225 Speedway, Stop C3100, University of Texas at Austin, Austin, TX 78712, U.S.A.,UNSW Business School, School of Economics, University of New South Wales, Sydney, NSW 2052, Australia,Department of Economics and Institute of Economic Research, Seoul National University, 1 Gwanak-ro, Gwanak-gu, Seoul 08826, South Korea","Received 3 February 2020, Revised 4 November 2020, Accepted 5 November 2020, Available online 11 November 2020, Version of Record 26 November 2020.",https://doi.org/10.1016/j.jedc.2020.104031,Cited by (26),We estimate international ,"As a countercyclical response to the onset of the Great Recession in 2007, the US Federal Reserve drastically cut its conventional monetary policy instrument - the federal funds rate. Once the federal funds rate effectively hit the zero lower bound (ZLB) at the end of 2008, the Federal Reserve engaged in unconventional monetary policies to provide further stimulus. In particular, through the large-scale asset purchase (LSAP) program, it purchased long-term Treasury and agency bonds and mortgage backed securities. The main goal of the program, often referred to as quantitative easing (QE), was to lower long-term interest rates and thus spur economic activities in a situation where the short-term interest rate was stuck at the ZLB.====In this paper we evaluate the international spillover effects of the QE policy by the Federal Reserve on the emerging market (EM) economies. Massive capital has flowed into the EM economies since the Federal Reserve started its QE policy in 2008 and as a result, their local currencies appreciated substantially. These developments could have potentially significant financial and macroeconomic impacts on the EM economies. Our focus on the EM economies is also partly motivated by how popular media and policy-making circles around the world were rife with concerns about the spillover effects of the QE policy.====Our empirical strategy is to first identify the US QE shock, using a Federal Reserve balance sheet measure, and then assess its international implications using a monthly panel VAR for the EM economies. This allows us to document two features of international spillovers of US QE policy. First, the panel VAR model for the EM economies that treats the US QE shock as an exogenous shock allows us to estimate both macroeconomic and financial spillover effects of the US QE policy. Second our panel VAR approach, which allows for country specific effects, also allows us to assess heterogeneity in responses across different subgroups of the EM countries.====In particular, we estimate international spillover effects of the US QE shock on the following EM economies: Chile, Colombia, Brazil, India, Indonesia, Malaysia, Mexico, Peru, South Africa, South Korea, Taiwan, Thailand, and Turkey.==== A panel VAR for macroeconomic and financial variables of the EM countries is estimated with the US QE shock included as an exogenous regressor.==== We use a random coefficients panel VAR approach that partially pools the cross-sectional information across the EM economies.====There are statistically and economically significant effects on exchange rates, long-term bond yields, capital flows and stock prices of these EM economies. In particular, an expansionary US QE shock appreciates the local currency against the US dollar, decreases long-term bond yields, and increases stock prices of these countries. The impact effects on the nominal exchange rate is around 0.25%, on stock prices around 1%, and on long-term bond yields around 0.03% points. For the nominal exchange rate and stock prices, the peak effects are around three times as large as the initial effects and occur 5 months after impact. In addition, we find that more capital flows into the financial markets of these countries following an expansionary US QE shock. At its peak, capital inflows increase around 2%. This is a large effect. Using the average size of the capital flows in our data, this constitutes an average effect of about 3.9 billion dollars on the aggregate and 300 million dollars per country.====On the contrary, we interestingly find no significant and robust effects on output and consumer prices of the EM countries. These results are not necessarily surprising as capital inflows and exchange rate appreciations can have opposite effects on production. Net exports also do not respond significantly on impact but, after several periods, respond positively.====Next, we investigate if there are meaningful differences in responses across some subgroups of the EM countries. Motivated by the attention that Brazil, India, Indonesia, Turkey, and South Africa, which came to be known as the “Fragile Five,” received in the media due to the potential vulnerability of their economies to the US QE policy, we consider one group composed of these countries and another of the remaining eight countries. We indeed find that these Fragile Five countries respond more strongly and differently from the rest of the EM economies. This holds for all the financial variables that we consider, including capital flows. For example, the peak response of exchange rates and long-term bond yields is around four times larger for the Fragile Five countries and capital flows respond significantly only for the Fragile Five countries. However, we do not observe any significant heterogeneity in output and consumer price responses. Lastly, for net exports, the response is positive only for the Fragile Five group.====In a discussion of these heterogeneous effects across country groups, we document that the higher vulnerability of the Fragile Five countries we estimate can be related to high external and fiscal imbalances prior to the crisis. Moreover, we find that a sub-group that includes Mexico together with the Fragile Five countries shows particularly strong heterogenous spillover effects, which we again relate to its high external and fiscal imbalance prior to the US QE period.====Overall, our estimates from the EM panel VAR suggest two main results. First, there is evidence of much stronger spillover effects of the US QE policy on financial variables compared to macroeconomic variables. This result on financial variables is consistent with the narrative of US investors “reaching for yield” in emerging financial markets. Second, the effects on the Fragile Five countries (and Mexico) are larger compared to the other EM economies in our sample. This result is in turn consistent with the narrative of differential effects of US QE policy on the EM economies, which we relate to pre-crisis fundamentals.====This paper is related to several strands of the literature. There is an influential empirical literature, for example, Neely (2010), Gagnon et al. (2011), Krishnamurthy and Vissing-Jorgensen (2011), trying to assess the effects of the US QE policy on interest rates, expected inflation, and other asset prices such as exchange rates.==== A main approach in this literature is to assess the announcement effects of such policies - the response of high-frequency financial variables to the Federal Reserve’s announcements of policy changes within a very narrow time frame. By isolating the changes in these variables due to the announcement of the QE policy, this literature has shown that it contributed to lowering long-term interest rates and depreciating the US dollar.====We contribute to this literature by taking an alternative and complementary approach. Our results for the impact of QE on financial variables are consistent with the findings of the announcement effect literature, both domestically and internationally. We show this explicitly by comparing our spillover results with those obtained from using the identification approach of Inoue and Rossi (2018), which is based on yield curve changes around monetary policy announcement dates. We then extend the results from this literature by both assessing the impact on low-frequency macroeconomic variables that policy makers focus on, such as output, consumer prices, and external balances, as well as ascertaining the dynamic spillover effects of such policies. Incorporating macroeconomic variables into the framework and tracing out the dynamic, persistent effects of the QE shock cannot be easily addressed within the event study framework so our paper fills this significant gap in the literature by exploiting a VAR framework.====In taking a VAR-based approach to identify the US QE shock, our paper is related to Wright (2012), Baumeister and Benati (2013), Gambacorta et al. (2014), and Dedola et al. (2017). In particular, our approach is similar to that of Gambacorta et al. (2014) who focused on domestic macroeconomic implications of QE by several advanced countries using a central bank balance sheet variable as an instrument of policy. Gambacota et al. (2014) and Dedola et al. (2017) use sign restrictions to identify the monetary shocks. For example, the sign restrictions in Dedola et al. (2017) are based on the presumption that monetary shocks have the usual “textbook” effects, that is, contractionary shocks reduce output and inflation, while increasing a range of interest rates. As the US unconventional monetary policy during the global financial crisis was unprecedented, however, we use an approach that imposes much weaker identifying restrictions compared to such sign restrictions. It is a non-recursive identification method paralleling the identification for the conventional US monetary policy shock by Sims and Zha (2006a), suitably adjusted to reflect the nature of QE policy. We then focus on the effects of QE on the EM economies. Finally, Bluwstein and Canova (2016) also study spillover effects of unconventional monetary policy, but for a different set of countries. They use an agnostic approach for identification and estimate spillover effects of unconventional monetary policy by the European Central Bank on non-Euro area countries in Europe.====Our empirical strategy is also close to the literature that assesses the purchase effects of the US QE policy. For example, D’Amico and King (2013) use a cross-sectional instrumental variables estimation, where Federal Reserve asset purchases are instrumented to avoid endogeneity concerns, to study the effects of large-scale Treasury purchases on high-frequency Treasury yields. Similar to D’Amico and King (2013), we also estimate effects that arise when the actual operation of balance sheet policies by the Federal Reserve happens but our focus is on time series macroeconomic variation. We use a structural VAR to separate out unanticipated movements in the Federal Reserve’s balance sheet variables and trace out the dynamic effects of the QE policy on the EM economies. Our results on long-term Treasury yields in the US VAR are consistent with their findings.====Another measure of unconventional monetary policies by the Federal Reserve often used in the literature is the shadow rate proposed by, for example, Wu and Xia (2016) and Krippner (2016). Wu and Xia (2019) interpret the shadow rate as the target the Federal Reserve wants to achieve through purchasing and selling bonds and show that their shadow rate estimates are highly correlated with the QE-related asset holdings of the Federal Reserve. Unlike the shadow rate literature, we directly use the asset holdings of the Federal Reserve to identify the US QE shock in our baseline specification so that we can avoid having extra estimation for the shadow rate. A robustness exercise where we identify the US QE shock using the shadow rate however, shows that the main spillover result robustly holds with the shadow rate as well.====There is important work assessing the international effects of the US QE policy, for example, Glick and Leduc (2012), Glick and Leduc (2013) and Bauer and Rudebusch (2013). Our work is different from this research in that we focus on the EM economies. Chen et al. (2016) investigate the international spillover effects of US unconventional monetary policies using the global vector error correction model (VECM). We instead use a panel VAR model to pool cross-sectional information about the international effects of US QE policy.==== Overall, our evidence on the effects on exchange rates and long-term interest rates for these countries is complementary to the international effects documented by these papers on advanced economies. With this focus, using different methods, we are also contributing in the same vein as Eichengreen and Gupta (2013), Aizenman et al. (2014), Bowman et al. (2014), and Tillmann (2014). Our approach is different with respect to identification and the way we pool cross-sectional responses by the EM countries.====Our results on international capital flows are also related to Dahlhaus and Vasishtha (2014) and Lim et al. (2014), who analyze the effects of the US unconventional monetary policy on capital flows to developing or EM economies. The main difference is that our empirical specification uses data not just on capital flows, but also a range of other financial and macro variables jointly in a panel VAR framework. Finally, in using a VAR analysis to ascertain the effects of the US monetary policy on international capital flows and asset prices, this paper is also connected to Rey (2013) and Bruno and Shin (2015). We contribute by estimating heterogenous effects of US QE shock across emerging markets, as well as ascertaining effects jointly on a range of asset prices, such as exchange rates, long-term interest rates, and stock prices.",Effects of US quantitative easing on emerging market economies,https://www.sciencedirect.com/science/article/pii/S0165188920301998,11 November 2020,2020,Research Article,253.0
Gutkowski Violeta A.,Federal Reserve Bank of St. Louis,"Received 24 June 2020, Revised 4 November 2020, Accepted 5 November 2020, Available online 11 November 2020, Version of Record 30 November 2020.",https://doi.org/10.1016/j.jedc.2020.104029,Cited by (0),"This paper examines the importance of sovereign debt market liquidity in a New Keynesian environment with wage rigidities and financial frictions à la Kiyotaki and Moore (2012). The analysis implies that, independently of credit risk, a decrease in the liquidity of government bonds has significant detrimental effects on output, employment and investment. A shut down of sovereign debt market for one quarter generates a 7==== drop in output and investment as well as a 2==== increase in unemployment. Also, in a framework where the only sources of variation are private and public liquidity shocks, sovereign bond market illiquidity can account for most of the output drop in Italy between 2011q2 and 2013q1. These results suggest that European Central Bank’s (ECB) temporary policies taken in 2012 aimed at rising liquidity seem to have prevented a more prolonged and deeper economic downturn.","The relevance of private sector market liquidity in driving business cycles has been extensively studied by Kiyotaki and Moore (2012) (KM), Del Negro et al. (2017) (DEFK), Shi (2015), among others, in particular when analyzing the Great Recession. But how important is liquidity—easiness at which an asset can be sold and transformed into cash—in sovereign bonds market? In normal times, sovereign bonds are among the most liquid assets in the market. However, in turbulent times, they become harder to sell. Most of the sovereign debt literature has focused on the importance of default risk in explaining debt capacity, default frequencies and spreads. Liquidity premium is a substantial component of total spreads of sovereign bonds (Passadore and Xu (2015)) and in particular during crisis periods (Darbha and Dufour (2015)). But does sovereign market liquidity have any effect on output? This work draws the attention to sovereign debt market liquidity and asks to what extent can sovereign liquidity shocks affect macroeconomic outcomes.====This paper shows that independently of credit risk, a one-quarter shutdown of government bonds market generates a significant output drop through investment and unemployment. Also, while the Great Recession was mainly driven by ==== market illiquidity (as studied by DEFK and KM), I show that European Debt Crisis (EDC) can be related to a ==== market illiquidity. In my simulation, shocks to sovereign debt market liquidity can explain most of the Italian output drop during this crisis. In this analysis, sovereign bonds play a crucial role in relaxing the funding constraints of entrepreneurs by providing liquidity. Thus, even if these bonds are fully repaid in every state, a reduction in the ability to trade these assets lessens their liquidity provision and can have significant macroeconomic effects.====Sovereign bond market illiquidity spiked in mid-2011 in most European countries, with markets calming down by the end of 2012. The left panels of Fig. 1 show 10-year sovereign bid-ask spreads for periphery countries such as Italy, Spain, and Portugal—on the upper panel— and for core countries with stronger fundamentals such a Netherlands and Finland, Austria, France, and Belgium—on the lower panel. Higher bid-ask spreads feature lower market liquidity. Also, as we can see on the right-hand side panels of Fig. 1 both periphery and core countries, with weak and strong fundamentals respectively, faced a recession initiated in 2011. Moreover, as Fig. 2 suggests, those countries which experienced a larger fall in sovereign bond’s market liquidity have gone through a deeper recession. Fig. 2 plots the change in bid-ask spreads before and after the crisis to output drop from peak to trough during the crisis period, for each country.====Does the drying up of sovereign bond markets have any relevant implication for the real economy? I study the effects of liquidity shocks to sovereign debt markets in an economy with financial frictions embedded in a standard New Keynesian framework—with price and wage rigidities. Financial frictions include the standard borrowing and the resalebility constraints in the equity market as in KM, and a new friction that affects the market liquidity of sovereign bonds. In particular, financial frictions à la KM constrain the financing opportunities of investment in new capital. These frictions, not only affect the possibility of external financing through borrowing but also the possibility of fully liquidating current assets in order to fund the production of new physical capital. In this setting, sovereign bonds play a crucial role in relaxing these constraints by providing liquidity. Although government bonds offer lower returns than equity, in equilibrium they are held by households not only due to lower risk but also because of the liquidity they provide. Shocks affecting the resalebility of these government bonds, that is their liquidity in secondary markets, can have important macroeconomic effects. Funding ability of entrepreneurs becomes tighter when government liquidity deteriorates triggering a drop in investment and capital formation.====Thinking about a shutdown of sovereign debt market is a natural experiment to do given the almost null trading of sovereign bonds in Greece between 2011 and 2013, as well as the negligible turnover ratio for Italy in the same period (Fig. 3)====In compliance with European increase in sovereign bonds bid-ask spreads beginning in 2011 and almost null trading of some of these bonds, this paper shows that independently of credit risk, the liquidity of sovereign markets is meaningful. In a calibrated model where financial frictions parameters are set following DEFK, a one quarter shut down of sovereign bonds’ market leads to a ==== drop in output through a similar fall in investment and a ==== increase in unemployment. I then use corporate and Italian government bonds’ bid-ask spreads to draw the series of liquidity shocks affecting private and public debt markets respectively. I discipline the standard deviation of the private sector shocks such as to match the output drop during the Great Recession and obtain sovereign shocks’ volatility as a residual when targeting the output decline during European Debt Crisis. Finally, I simulate the model and show that government bonds market illiquidity can account for 86% of the drop in investment and output between 2011q2 and 2013q1 in Italy in an environment where the only sources of variation are private and public liquidity shocks. These results suggest that ECB’s temporary policies taken in 2012 aimed at rising liquidity in sovereign debt markets seem to have prevented a more prolonged and deeper economic downturn.====The structure of the paper is as follows. Section 1.1 discusses the related literature. Section 2 introduces the basic framework, and Section 3 presents the calibration and results. Finally, Section 4 concludes.",Sovereign illiquidity and recessions.,https://www.sciencedirect.com/science/article/pii/S0165188920301974,11 November 2020,2020,Research Article,254.0
Jeon Haejun,"Department of Management, Tokyo University of Science, 1-11-2 Fujimi, Chiyoda-ku, Tokyo 102-0071, Japan","Received 19 March 2020, Revised 26 August 2020, Accepted 1 November 2020, Available online 7 November 2020, Version of Record 21 November 2020.",https://doi.org/10.1016/j.jedc.2020.104028,Cited by (5),"In this study, we investigate optimal investment timing and capacity decisions in the presence of time-to-build and competition. Due to uncertain time-to-build, a leader, who invests first, may have its product enter the market after a follower’s. We show that a dominated firm with the longer time-to-build can become a leader by making the investment earlier than a dominant firm with shorter investment lags. The leader’s capacity choice increases with the dominated firm’s time-to-build, even if the dominated entity is the leader. This finding is consistent with the observation in the electric vehicles market in which a relatively new firm with little experience of mass production makes aggressive investment early on, while the biggest carmakers capable of mass production are timing their investment. With a welfare-maximizing policy, however, the dominant firm with the shorter time-to-build always becomes the leader. There is a significant loss of social welfare with the dominated firm being the leader, and the loss increases with the asymmetry of time-to-build.","Rome was not built in a day and neither were the products in the market. It usually takes time to make products available for sale from the time of initial investment, and the lags between investment timing and profit making are inherently uncertain, especially when they are of large-scaled or based on state-of-the-art technology. Further, the investment lags become more important when there is competition between firms because they are directly linked to the dynamics of the leader–follower relationship in the market.====In this study, we investigate firms’ optimal investment timing and capacity decisions in a duopoly market in the presence of time-to-build. The uncertainty in time-to-build changes the leader–follower relationship significantly; the leader, who invests first, may have its product enter the market after the follower’s. It also affects firms’ capacity decisions, which are associated with product price and consumer surplus. In particular, we allow the firms’ time-to-build to be asymmetric. This plays a crucial role in the derivation of the main results, namely, our investigation into how asymmetric time-to-build affects the firms’ investment strategies and the level of social welfare in a duopoly market, based on a continuous-time dynamic framework.====First, we show that a firm, which dominated in terms of time-to-build, can emerge as a leader by investing first in the market. Given the significant asymmetry of investment lags, a dominant firm has less incentive to invest first because its products can enter the market first even if it invests later. That is, a relative advantage of time-to-build weakens the incentive of preemption significantly, and the dominated firm with longer lags gets a chance to become the market leader, although there is no guarantee that the leader’s product enters the market first. This is a novel result in that the existing literature on the asymmetric duopoly market shows that a dominant firm with cost advantage always becomes the market leader (e.g., Huisman, 2001, Pawlina, Kort, 2006, Kong, Kwok, 2007). The novelty can also be found in the fact that we show a firm’s incentive to invest later in the absence of positive externalities or availability of new technology, which is in sharp contrast with the argument of a second-mover advantage in the existing literature (e.g., Dutta, Lach, Rustichini, 1995, Femminis, Martini, 2011, Hoppe, 2000, Hoppe, Lehmann-Grube, 2001).====We also show that the leader’s investment capacity increases with the size of the dominated firm’s time-to-build, even when the firm becomes the leader. A dominated firm invests aggressively not only by the timing, but also by the choice of capacity, as its time-to-build becomes longer. This finding, combined with the previous one, can explain automakers’ investment behavior in the electric vehicles market. Tesla, a relatively new entrant with no experience of mass production, has been investing aggressively in the electric cars sector, including the construction of gigantic battery factories and the installation of charging stations worldwide. However, the biggest carmakers capable of mass production and with shorter lags, such as Toyota and Volkswagen, have not made preemptive investment in electric cars, thereby timing their investments. Some might interpret this as a war of attrition in which there is a second-mover advantage, such as innovation in the future. We show that the investment behavior can be explained by the relative advantage of investment lags that allows a second-mover to enjoy the first-mover advantage.====Furthermore, we derive a welfare-maximizing investment policy in a duopoly market. The policy assigns the roles of a leader and follower to the dominant and dominated firms in terms of time-to-build, respectively. The firm with the shorter investment lags always becomes the leader in the context of this policy. We also show that the welfare-maximizing investments of the leader and the follower are made earlier and later than those chosen in a market equilibrium, respectively. In other words, market investments by the leader and the follower are made inefficiently late and early, respectively. This is in contrast with Huisman and Kort (2015) who showed that investment by both the leader and the follower is inefficiently early in the market. Socially optimal capacities for both the leader and the follower are much higher than those chosen in the market and the difference is more significant for the leader’s capacity. It is socially efficient to have products in the market as early as possible, and the social planner exploits the dominant firm’s shorter lags to maximize social welfare by emphasizing the dominant firm’s investment in terms of both timing and capacity.====Compared to a monopoly market, time-to-build has a more significant impact on both firms’ investment strategies and social welfare in a duopoly market. A monopolistic firm’s investment timing, which is delayed as the time-to-build increases, coincides with the welfare-maximizing one, and the capacity choices of both the monopolistic firm and the social planner are irrelevant to investment lags. In contrast, the presence of time-to-build can switch the roles of the leader and the follower in a duopoly market, and the investment timing of both is significantly different from the socially optimal ones; as mentioned earlier, investments made by the leader and the follower are inefficiently late and early, respectively. The capacities chosen by the firms and the social planner are also associated with the size of the investment lags. In this context, we clarify that time-to-build has a distinct impact in different types of market structures.====Several studies are devoted to investigating the effects of time-to-build. Majd and Pindyck (1987) pioneered this field of research by considering a project in which investment can only be made at a certain rate.==== Bar-Ilan and Strange (1996) elucidated this problem by assuming that a certain time must elapse to yield revenue and showed that lags can lead to earlier investment by reducing the value of wait. Bar-Ilan and Strange (1998) extended this research to the decisions on a two-stage investment. Some studies even considered debt financing for an investment project that takes time-to-build. Tsyplakov (2008) and Agliardi and Koussis (2013) studied the impact of time-to-build on investment and debt dynamics, and Sarkar and Zhang (2015) examined a two-stage investment with debt financing. Jeon (2021) endogenized capital structure and the timing of investment and default in the presence of time-to-build. These studies, however, were limited to analyzing a monopolistic firm’s decision, overlooking the effects of time-to-build in the presence of competition.====Grenadier (1996) examined a duopolistic real estate market with construction lags and found that declining demand can trigger simultaneous investments by firms. Grenadier (2000) derived competitive equilibrium in the presence of time-to-build and showed that current asset values depend not only on current supply and demand but also on the path of previous construction.==== Aguerrevere (2003) analyzed the impact of time-to-build and operating flexibility on investment decisions and found that increased uncertainty may lead to higher capacity. Pacheco-de Almeida and Zemsky (2003) studied multi-stage investment in a duopoly market with time-to-build and showed that lags can lead to an equilibrium in which firms make incremental investment. These studies, however, ignored the inherent features of uncertainty in time-to-build. Weeds (2002) incorporated uncertain time-to-build in the analysis of R&D competition, but only examined the timing decision in a winner-takes-all market scenario. None of these studies discussed asymmetry in time-to-build and its impact on firms’ investment decisions.====Setting aside time-to-build, we find several other studies on investment decision with competition. Nielsen (2002) examined a duopoly market not only with negative externalities but also with positive externalities. Huisman and Kort (2004) studied investment strategy in a duopoly market with future availability of new technology. Bouis et al. (2009) considered competition between three firms instead of two and found the accordion effect of investment triggers. Mason and Weeds (2010) showed that the leader’s investment threshold may not monotonically increase with uncertainty in the presence of preemption. Thijssen et al. (2012) adopted a general setup for analyzing a duopoly market and showed that the probability of both firms investing simultaneously notwithstanding their lower values in the case of sequential investments is non-zero. Huisman and Kort (2015) considered not only the timing decision but also capacity in a duopoly market and evaluated the effects of competition on social welfare. These studies, however, assumed homogeneity of firms in the market.====There are several other studies that introduced asymmetry between firms. Huisman (2001) provides a comprehensive analysis on duopolistic firms’ investment strategies for a wide range of setup. Lambrecht and Perraudin (2003) studied the preemption of a duopoly market in the presence of incomplete information about investment costs. Pawlina and Kort (2006) allowed cost asymmetry and showed that there can be an equilibrium in which one of the firms has no preemption incentive, and that identical firms can result in a socially less desirable outcome than if one of them had a cost advantage. Kong and Kwok (2007) incorporated asymmetry in both cost structure and revenue flow and presented a broader characterization of equilibrium. Casadesus-Masanell and Ghemawat (2006) studied a mixed duopoly in which a profit-maximizing firm was competing with a not-for-profit contender. Thijssen (2010) incorporated player-specific uncertainty in a duopoly market and showed that preemption did not necessarily occur.====There is a growing body of literature on the second-mover advantage in an oligopolistic market; in this market, the advantage can lead to an attrition game rather than a preemption one. Dutta et al. (1995) investigated quality competition in a duopoly market and addressed the second-mover advantage as an option to adopt new technology. Hoppe and Lehmann-Grube (2001) extended their work by incorporating R&D costs and showed that a second-mover advantage was superior when R&D costs were high. Hoppe (2000) and Femminis and Martini (2011) elucidated the second-mover advantage that resulted from spillover of innovation. Décamps and Mariotti (2004) studied the effects of positive externalities, considering private information on investment costs. Amir and Stepanova (2006) provided a broader characterization of equilibrium under a general framework, including asymmetric costs and various types of demands. These studies on the second-mover advantage involve either product innovation or process innovation. In contrast, our study shows a firms incentive to invest late in the presence of competition without any innovation or positive externalities. More specifically, our study shows that the relative advantage of investment lags allows a second-mover to enjoy the first-mover advantage.====The remainder of this study is organized as follows. To facilitate understanding, we examine a monopoly market first in Section 2. Section 2.1 discusses a firm’s optimal decisions on investment timing and capacity, and Section 2.2 derives an investment policy that maximizes social welfare. Based on this argument, we proceed to a duopoly market in Section 3. In Section 3.1, we assume that the roles of firms as a leader and a follower are exogenously given and investigate their optimal investment strategies. Section 3.2 endogenizes firms’ roles in the market, taking preemption incentives into account. In Section 3.3, we derive a welfare-maximizing investment policy in a duopoly market. Given these arguments, we present the results of comparative statics regarding time-to-build and discuss their economic implications in Section 4. Sections 4.1 and 4.2 are dedicated to discussing firms’ optimal investment strategies and welfare-maximizing policy, respectively. In Section 5, we demonstrate how the argument changes as we apply different types of demand functions: the additive demand function and the isoelastic demand function in Sections 5.1 and 5.2, respectively. Section 6 provides a summary of the study and discusses possible future work.",Investment timing and capacity decisions with time-to-build in a duopoly market,https://www.sciencedirect.com/science/article/pii/S0165188920301962,7 November 2020,2020,Research Article,255.0
"Kim Eungsik,Spear Stephen","Department of Economics, University of Kansas, Snow Hall 339, 1460 Jayhawk Blvd, Lawrence, Kansas, 66045 USA,Tepper School of Business, Carnegie Mellon University, Tepper Building 4102, 5000 Forbes Avenue, Pittsburgh, Pennsylvania, 15213 USA","Received 21 May 2020, Revised 18 October 2020, Accepted 19 October 2020, Available online 1 November 2020, Version of Record 22 January 2021.",https://doi.org/10.1016/j.jedc.2020.104023,Cited by (0)," and the model parameters. This analysis identifies the set of economies for each type of invariant measure. We show that the attractor of the singular invariant measure exhibits fractal patterns, which supports the proposition that self-affinity patterns in financial data can be explained via rational expectations equilibrium as an economic mechanism.","The overlapping generations (OLG) model, extending the seminal work of Samuelson (1958), has been widely used in macroeconomics, finance, and policy-making as an important workhorse model. As a part of the development of the model, there is now an extensive literature on the studies of equilibrium dynamics and associated invariant distributions in OLG models, which will be the focus of this paper. In deterministic OLG models, the invariant measure is simple and concentrated on a steady-state with probability one if the steady-state is saddle-path stable. However, the seminal paper of Benhabib and Day (1982) (and a follow-up paper Yokoo 2000) shows that an unstable steady-state can permit chaotic dynamics, which generates non-convergent, but non-wandering paths. The chaos in a deterministic model has an invariant distribution which is continuous on an uncountable subset of the state space. In stochastic OLG (SOLG) models, a stationary stochastic equilibrium has continuous invariant distributions even with a simple Bernoulli shock if the equilibrium is dependent on the full (infinite) history of shocks.==== In applied work, researchers calibrate a wide variety of OLG models by matching the simulated moments of the invariant measure of the models with their empirical counterparts.====Despite its importance, studies on the OLG model have been restricted in their attention to showing the existence, uniqueness, and stability of the invariant measure (see Benhabib, Day, 1982, Duffie, Geanakoplos, Mas-Colell, McLennan, 1994, Grandmont, Hildenbrand, 1974, Laitner, 1981, Morand, Reffett, 2007, Spear, Srivastava, 1986, Wang, 1993, Wang, 1994 and many others). None of the previous papers provide any characterizations of the nature of the equilibrium invariant measure. By the Lebesgue decomposition theorem, measures on Euclidean spaces can be decomposed into the sum of two measures: one absolutely continuous with respect to the Lebesgue measure and the other singular. A probability measure is ==== with respect to the Lebesgue measure if it assigns a positive probability to Lebesgue measure non-zero sets. The ==== distribution is defined as a probability measure that assigns probability one to a Lebesgue measure zero set. It is well-known that a simple shock process such as a symmetric Bernoulli distribution can produce either an absolutely continuous or a singular invariant measure in a stochastic dynamic system (see Barnsley and Demko 1985 or Guckenheimer and Holmes 2013).====Examining this feature is theoretically interesting since there exists a bifurcation into the two types of measure continuity, depending on the parameters of the models, even for models generated by small stochastic deviations from a deterministic model. Knowing the continuity feature of an invariant measure is also important in practice since absolutely continuous measures have density functions, whereas singular measures do not. One only needs to know the parameters identifying its density function to describe an absolutely continuous measure. On the other hand, representing a singular measure requires a large set of information: the value of the distribution for every point in its support.====However, there has been little attention paid in the literature to the problem of characterizing the continuity features of invariant measures in SOLG models (see Mitra et al. 2003 and Mitra and Privileggi 2009 for exceptions in the context of the stochastic neoclassical growth model). A key reason for this paucity of research is that identifying the equilibrium dynamics for the stochastic economy is daunting even in the models where one makes specific assumptions about preferences and other primitives. Furthermore, even if one is aware of the functional form of the equilibrium system, it is difficult to determine under what conditions singular or absolutely continuous measures arise and how they depend on economic environments unless the mapping is of a simple form.====Therefore, this paper pursues the characterization of the stationary stochastic equilibrium in SOLG models by analyzing the continuity properties of its invariant distribution and the relationship of the measure with the underlying dynamics of the model. For this analysis, we consider a Markov equilibrium (ME) where the asset holdings distribution is used as the set of lagged endogenous state variables. Woodford (1986) showed the existence of stationary stochastic equilibria using the infinite histories of exogenous shocks as state variables for general SOLG models. Spear (1988) showed a similar set of results using lagged prices as sufficient statistics for the infinite shock histories for a particular subset of SOLG economies. For general SOLG models, Duffie et al. (1994) showed that there exists a ME that requires asset holdings and other proper endogenous variables (possibly including lagged consumption allocations) as the sufficient statistics. Citanna and Siconolfi (2010) and Citanna and Siconolfi (2012) showed that it is generically sufficient to consider a ME in which only the asset holdings distribution is taken as the endogenous state variables.====We approach our research questions by working from a simple SOLG model to a general one and from an exchange economy to a production economy, step by step. In each model, we focus on stable dynamics around a deterministic steady-state under sufficiently small shocks because in this case, a simple linear iterated function system (LIFS) provides a good approximation for the ME via the Hartman-Grobman theorem. The linearized ME allows using the theory of LIFS to determine conditions for absolutely continuous and singular invariant measures. It also helps to identify relationships between the model parameters and the continuity feature of the invariant measures.====In a tractable three-period monetary SOLG model with logarithmic preferences, we provide a closed-form solution for the parameters characterizing its LIFS. This solution shows that the slope parameter of the LIFS is affected by only the endowment shares between ages. In addition, the Lipschitz constant of the LIFS belongs to ==== for the entire set of economies. A one-dimensional LIFS with this scaling factor satisfies the so-called “no-overlap” property. It generates a Cantor-like unique invariant distribution whose support is a Cantor-like invariant set. The Cantor-like distribution is singular with respect to Lebesgue measure if generated by a LIFS. Therefore, the simple monetary SOLG model always has a singular Markov invariant measure. The Cantor-like attractor also shows fractal self-affinity.==== This result shows that a multi-period SOLG model can generate fractal patterns in a rational expectations equilibrium.====In a three-period SOLG model with a Lucas tree and general preferences, we show that the slope parameter of the LIFS corresponds to the stable eigenvalues of the Jacobian matrix for the linearized price dynamics evaluated at the deterministic steady state if the aggregate shock is small enough. Based on this relationship, we numerically study how the model parameters affect the Lipschitz constant. We determine an open set of economies where the no-overlap property holds and a singular invariant measure arises and another open set of economies where an absolutely continuous invariant measure emerges. We find that a model with low risk-aversion and low dividend income share can generate a singular Markov measure. This result illustrates why a singular measure exists for the entire set in the simple monetary model, that is characterized by a risk-aversion set at one and a zero dividend.====We extend the analysis to a more realistic setting by lengthening lifetime, introducing cohort heterogeneity, and letting the states of nature be arbitrary (but finite). As in the simpler models, we also find that the affine matrix of the LIFS takes the stable eigenvalues of the Jacobian matrix for the linearized non-stochastic dynamic system at the steady-state as their own under small shocks. Although it is hard to fully classify all economies into the two continuity types with a high-dimensional LIFS, we give a sufficient condition for the model to generate a singular invariant measure. This condition implies that a trivial singular measure arises if the dimension of the essential state space for the LIFS is larger than or equal to the number of states of nature. To obtain a non-trivial singular measure, the number of shock states should be larger than the dimension of the essential state space and the spectral radius of the affine matrix should be small enough. We find a similar relationship between the spectral radius and model parameters as in the three-period model: low risk-aversion or low dividend share is required for generating a non-trivial singular measure.====Finally, we replicate the results from the exchange economies in a production model with a pay-as-you-go pension system. We numerically identify the set of parameters for singular versus absolutely continuous measures using the theory of LIFS. We find it robust that singular measures arise under a low capital income share and a low risk-aversion.====The empirical relevance of our results can be found in the literature on fractal phenomena observed in financial data. The self-affinity aspect of stock prices was one of the topics studied in both the non-linear dynamics and time-series econometrics literature over a decade between the mid-1980s and 90s. In the former literature, earlier researchers claimed to have found evidence for low-dimensional deterministic dynamic systems as data generating processes since the attractors of the chaotic dynamics can have self-affinity features (see Baumol, Benhabib, 1989, Scheinkman, Lebaron, 1989). However, later work with large data sets concluded that the previous results were misderived due to the paucity of data (see Vassilicos et al. 1993). The consensus now is that there is little evidence for low-dimensional deterministic systems as stock price generating processes (see LeBaron, 1994, Vassilicos, Demos, Tata, 1993). These results, then, require (at a minimum) high-dimensional chaotic systems or stochastic systems to account for the empirical features of the stock price data.====In the econometrics literature, many stochastic models have been developed based on the theory of fractional Brownian motion processes pioneered by Mandelbrot (1963) and Mandelbrot (1967). The time-series econometrics models capture the self-affinity properties observed in the financial data. However, they lack an economic mechanism behind the data-generating processes. To fill this gap, our paper shows how SOLG models can generate self-affine features on asset prices in their rational expectations equilibrium.====There are a few papers in the literature that also examine the continuity features of invariant measures and their relationship with equilibrium dynamics in dynamic stochastic general equilibrium models. Mitra et al. (2003) characterize the continuity property of invariant Markov distributions in a stylized one-sector stochastic growth model with logarithmic preferences and Cobb-Douglas production functions. As follow-up work, Mitra and Privileggi (2009) provide similar relationships with ours between the model parameters and the types of invariant measures in a more general one-sector model. Compared to these papers, we analyze the invariant measure in a variety of SOLG models. As far as we know, this is the first paper to study the continuity property of the invariant Markov measure in SOLG models. Our paper also incorporates the analysis of high-dimensional dynamic stochastic systems, which was not treated in the previous papers.====Although Gardini et al. (2009) also studied the limiting distribution of forward-looking dynamics in OLG models, the nature of the invariant measures of interest in their and our papers are different along with dimensionality. They consider the distribution derived by a sunspot shock under a backward bending offer curve in two-period deterministic OLG models whereas we examine the one driven by intrinsic shocks in high-dimensional SOLG models.====We organize this paper as follows. Section 2 examines a tractable three-period monetary SOLG model to get intuitions behind how the LIFS works. We study a three-period SOLG model with a Lucas-tree in Section 3. In Section 4, we extend the analysis to a general model. Section 5 provides the numerical accuracy of a linear approximation in SOLG models with small shocks. We replicate the results from the exchange economies in a production model in Section 6. Finally, Section 7 concludes this paper. Appendices include the mathematical proofs, basic concepts and results about the iterated function system, and algorithms for computing equilibria in the SOLG models.",A characterization of Markov equilibrium in stochastic overlapping generations models,https://www.sciencedirect.com/science/article/pii/S0165188920301913,1 November 2020,2020,Research Article,256.0
"Fontini Fulvio,Vargiolu Tiziano,Zormpas Dimitrios","Department of Economics and Management “Marco Fanno”, University of Padova, Italy,Interdepartmental Centre for Energy Economics and Technology “Giorgio Levi Cases”, University of Padova, Italy,Department of Mathematics “Tullio Levi Civita”, University of Padova, Italy,Department of Economics and Management, University of Brescia, C.da S. Chiara, 50, 25122, Italy","Received 15 September 2019, Revised 25 September 2020, Accepted 27 September 2020, Available online 30 September 2020, Version of Record 8 May 2021.",https://doi.org/10.1016/j.jedc.2020.104004,Cited by (5),"Reliability Options (ROs) are used to enhance the security of supply in electricity systems. When a power producer writes a RO, s/he agrees to set a cap on the price of electricity that s/he cashes. In return, the system operator, i.e. the party that is buying the option, pays to the option issuer a fixed premium. In this paper we analyze how ROs affect the timing and value of investments in the energy sector and we show under what conditions they can be used as investment stimuli. We prove that, contrarily to what is expected, ROs can potentially harm the security of supply by delaying the adoption of new capacity and by reducing the value of investing in it. To avoid such a result, a careful setting of the relevant parameters is needed.","As is well known in the theoretical literature of electricity economics, the equilibrium of perfectly competitive electricity markets delivers the efficient level of power capacity, that is, the level at which the marginal investment cost equals the marginal loss attributed to the energy demand that remains unserved.==== This implies that it would be unnecessary to provide an explicit remuneration for power capacity since the electricity market already provides enough incentives for new investments.====However, when moving from the first-best analysis to the real-world evaluation, several scholars (see e.g. Cramton, Ockenfels, 2012, Cramton, Ockenfels, Stoft, 2013, Cramton, Stoft, 2006, Joskow, 2007, Joskow, 2008, Joskow, Tirole, 2007, Joskow, Wolfram, 2012) have argued that there exist electricity-specific market failures that can impede reaching the first-best outcome. The limited and costly storability of electricity coupled with the need to maintain the system balanced in real time, imply that whenever there is not enough capacity to serve the load, the latter has to be shed. This means that customers might experience power outages even if they highly need power and express the willingness to pay for it. This is the so-called ==== problem.====Explicit remuneration of capacity through Capacity Remuneration Mechanisms (CRMs) can solve this problem.==== There exist several electricity markets in which CRMs of different forms are in use.==== For instance, ==== are explicit, administratively set, payments to power producers. ==== are procurement auctions through which the System Operator (SO) remunerates a given amount of generation capacity.==== ==== is the obligation for load serving entities to hold enough capacity to serve the load. ==== have to do with the withdrawal from the market, and attribution to the SO, of some physical capacity when needed. On the contrary, markets that have no CRMs are called ==== markets.====A CRM that is gaining momentum is a specific form of capacity auction in which power producers sell Reliability Options (ROs) to the SO. Originally proposed by Vázquez et al. (2002), Bidwell (2005), Oren (2005) and firstly implemented in Colombia (Cramton and Stoft, 2007), ROs are also adopted in ISO-New England (FERC, 2014), Ireland (SEM, SEM, SEM) and in Italy (Mastropietro, Fontini, Rodilla, Battle, 2018, TERNA).====In a nutshell, ROs have the form of swaps on power production. They are sold by power producers to the SO in exchange for a premium. In return, the sellers of the ROs agree to supply energy to the market and return to the SO the extra revenues that they obtain from prices rising above a predetermined level called the strike price of the RO. The return of these extra revenues, also called ====, discourages any opportunistic behavior on the producers’ side who might otherwise be tempted to withdraw capacity from the market in an attempt to benefit from price spikes.====Vázquez et al. (2002), Bidwell (2005) and Oren (2005) introduce ROs and show how they function. Cramton et al. (2013), Batlle et al. (2015) and Bhagwat and Meeus (2019) further discuss their features. However, all these papers present qualitative discussions on ROs. To the best of our knowledge, the only paper that quantitatively analyzes the properties of ROs is Andreis et al. (2020). In that work, the authors provide semi-explicit formulas to calculate the financial value of a RO. In order to do so, they consider a hypothetical existing power plant, assume that the overall amount of capacity is fixed and calculate the value of the RO as a financial option. They show how to calculate the arbitrage-free value of a RO that would emerge in a well diversified financial market.====Here, we tackle a different issue. For the power plant that is writing a RO, the RO is a tool to replace stochastic future cash flows, i.e. profits generated by selling electricity at a future time point, with deterministic present ones, i.e. the RO premium. While it is true that in any arbitrage-free financial market where ROs are correctly valued the power producer should be indifferent between participating in an ==== market and an energy market employing ROs, in a real-world setting, market failures can make the issuing of ROs an attractive alternative. In this case, the literature (Cramton, Ockenfels, 2012, Cramton, Ockenfels, Stoft, 2013, Joskow, 2007) claims that ROs should allow power producers to hedge their investment risk stimulating, as a result, the installment of new capacity.====In this paper, we assess whether ROs are effective in fostering investments in power supply, given that their price can differ from the arbitrage-free equilibrium value, and analyze to what extent their effectiveness depends on their parameters and on the power price. We do so by explicitly studying the impact of a RO scheme on the behavior of a potential investor who is contemplating entering the electricity market. In particular, we pose and answer two research questions: “How does the use of a RO scheme affect the timing of investment in new capacity?” and “What is the effect of the implementation of this scheme on the value of the option to make such an investment?”.====We do so by developing a real options model. This allows to properly account for sunk investment costs (e.g. the cost of setting up the power plant), uncertain future payoffs (e.g. volatile prices of electricity) and temporal flexibility (e.g. the option to postpone the investment). The real options approach builds on the idea that the option to undertake an investment project is analogous to an American call option on a real asset. Hence, when evaluating an investment option characterized by uncertainty and irreversibility, the potential investor needs to factor in that at the time of the investment s/he forgoes the option to reconsider the investment decision at some future time point when the uncertainty will be, naturally, partly resolved.====Within this framework, we find the following. First, we compute the optimal investment threshold and the value of the option to invest in a power plant when the RO scheme is in place. We show that the effect of the implementation of the RO scheme on the behavior of a potential investor depends on whether the electricity market is long or short in capacity at the time of the investment.====If the electricity market is long at the time of the investment, i.e. if the strike price of the RO is larger than the optimal investment threshold chosen by the potential investor, the application of a RO scheme is favoring the acceleration of the investment in new capacity. This happens because, on one hand, the timing decision of the potential investor is unaffected by the upper bound that the RO scheme is setting on electricity prices whereas, on the other, the investor is benefiting from the premium that the scheme is paying. Consequently, the potential investor opts for investing earlier.====As for the effect of the RO scheme on the value of the option to invest in new capacity, this is shown to be ambiguous. For a given strike price, we identify the minimum premium that would make the potential investor indifferent between participating in a RO scheme and abstaining from it. The non-monotonic effect of the implementation of the RO scheme on the value of the investment option has to do with the fact that, while the strike price of the RO is not binding when the investment takes place, it still constitutes an upper bound for the price of electricity. This means that, while in the short run the power plant can benefit from high prices, the strike price of the RO will be reached with a positive probability limiting the power plant’s profitability.====If the electricity market is instead short at the time of the investment, i.e. if the strike price of the RO is binding when the optimal investment threshold is reached, both the timing and the value effect are shown to be ambiguous. For a given strike price, we identify three possible cases. First, there is a lower region of RO premiums for which the investment takes place later and has lower value than an investment in an ==== market. In this region the combination of strike price and RO premium is such that a potential investor would choose not to participate in a RO scheme voluntarily or, alternatively, would opt for a later investment if the participation in the RO scheme is mandatory. Obviously, in this case the implementation of the RO scheme discourages investments in new capacity. Then, there is an intermediate region of RO premiums that guarantee the acceleration of the investment at the expense of the project’s value. The potential investor chooses to hasten the investment in order to start cashing the RO premium early on but this is proving costly in terms of project value. Last, there is a higher region of RO premiums that have a favorable effect both on the value and on the timing of the investment. In this case the RO premium is so large that it fully neutralizes the adverse effect related to the application of the strike price of the RO.====Our paper contributes to the extant literature in two ways. First, we show that a RO scheme does not always qualify as an investment stimulus. Second, we derive the conditions under which a RO scheme can have a positive effect on the value and the timing of an investment in new capacity. We conclude the paper presenting some numerical examples focusing on the effect of the price volatility on the investment thresholds and investment option values that we derive theoretically. Our interest in the volatility of the price of electricity has to do with the fact that the ==== problem is aggravated when the price of electricity is very volatile since, in that case, more capacity is needed to cope with increasing imbalances.====The rest of the paper is organized as follows. The investment problem in an electricity market without ROs is discussed in Section 2. In Section 3 we approach the same problem assuming that a RO scheme is in place. The effect of the RO scheme is discussed in Section 4. Section 5 discusses the impact of price volatility on the investment timing and the value of the investment. Conclusions follow in Section 6. All proofs are grouped in the Appendix.",Investing in electricity production under a reliability options scheme,https://www.sciencedirect.com/science/article/pii/S016518892030172X,30 September 2020,2020,Research Article,257.0
"Castellini Marta,Menoncin Francesco,Moretto Michele,Vergalli Sergio","Department of Economics and Management, Università degli Studi di Brescia, Via S. Faustino 74/B – 25122 Brescia, Italy,FEEM - Fondazione Eni Enrico Mattei, Corso Magenta 63, Milan, Italy,Department of Economics and Management, Università degli Studi di Padova, Via del Santo 33, Padova, Italy","Received 22 October 2019, Revised 20 May 2020, Accepted 31 August 2020, Available online 5 September 2020, Version of Record 8 May 2021.",https://doi.org/10.1016/j.jedc.2020.103988,Cited by (13),"The digitization of power system represents one of the main instruments to achieve the target set by the European Union 2030 climate and energy Agenda of affordable energy transition. During the last years, such innovation process has been associated with the Smart Grid (SG) term. In this context, efficiency and flexibility of power systems are expected to increase and energy consumers to be active also on the production side, thus becoming prosumers (agents that both produce and consume energy). This paper provides a theoretical ==== framework with the aim to model prosumers’ decision to invest in photovoltaic power plants, assuming that they are integrated in a SG. Our main focus is to study the optimal plant size and the optimal investment threshold, in a context where exchange of energy among prosumers is possible. The model was calibrated and tested with data from the Northern Italy energy market. Our findings show that the possibility of selling energy between prosumers, via the SG, increases investment values. This opportunity encourages prosumers to invest in a larger plant compared with the case without exchange possibility and that there is a positive relation between optimal size and (optimal) investment timing. The effect of uncertainty is in line with the literature, showing increasing value to defer with volatility. Our comparative statics stress the need for policies to push the PV efficiency.","In recent years climate change has become an important issue in the economic debate. The latest IPCC==== report (IPCC, 2019) underlines how important is to control temperature levels by reducing or limiting CO==== emissions. This could avoid the occurrence of irreversible effects. Some mitigation paths are characterized by the reduction in energy demand, the decarbonization of electricity and other fuels, and the electrification of the final use of energy. In this line, the European Union 2030 climate and energy policy has set three macro targets: (i) the reduction of 40% in greenhouse gas emissions (with respect to 1990 levels), (ii) 32% of energy coming from renewable sources, and (iii) an improvement in energy efficiency of 32.5%. In addition to that the European Union long-term strategy aims to reach a climate neutral economy within 2050.====Such policies require strong deployment of low carbon technologies as well as an adequate efficient environment.==== A central role is played by the definition of new emerging power system, required to be decarbonized, decentralized, and digitized. Decarbonization is also related to the diffusion of renewable energy plants. Instead, decentralization refers to the growing role of new many electricity producers, with small-scale, decentralized, and intermittent periods of overproduction of electricity, mostly photovoltaic (PV, hereafter). Finally, digitization implies the innovation of the power system, a concept that has also been associated in the last years with the Smart Grids (SGs, hereafter) that are ""robust, self-healing networks that allow bidirectional propagation of energy and information within the utility grid"".==== This last element plays an important role, since technological development enables also an affordable energy transition.====In this respect, the continuous integration of Distributed Energy Resources (DERs, hereafter), (Sousa et al. (2019); Bussar et al. (2016); Zhang et al. (2018)),==== along with the advance in Information and Communication Technology (ICT) devices (Saad al sumaiti et al., 2014), are inducing a transformation of a share of electricity consumers who ==== and ==== and share energy with other grid users. Such users are called “====” (Luo et al. (2014); Sommerfeldt and Madani (2017); Espe et al. (2018); Zafar et al. (2018)).====Smart grids actually introduce the possibility of adopting new behaviors: while traditional consumers are characterized by a passive behavior in buying and receiving energy from the grid, prosumers are proactive in managing their consumption and production (Zafar et al., 2018). Indeed, they can reduce their energy consumption costs, by self consuming the energy produced by their PV plants (Luthander et al. (2015); Masson et al. (2016)). In addition to that, Espe et al. (2018) remark the importance of prosumers participation to the smart grid as critical for both the sustainability and the long term efficiency of the energy sharing process.====Furthermore, SGs allow instantaneous interactions between agents and the grid: depending on its needs, the grid can send signals (through prices) to the agents, and agents can respond to those signals and obtain monetary gains as a counterpart. These two characteristics (self-consumption and possible return energy exchange with national grid) can add flexibility that, in turn, increases the value of the investment (Bertolini et al., 2018). A third important characteristic, that depends on the development of new technologies and digitalization, is the possibility to exchange energy also between agents (InterregEU (2018); Luo et al. (2014); Alam et al. (2017); Zafar et al. (2018); Zhang et al. (2018)), in a Peer-to-Peer (P2P, hereafter) energy trading or in developing energy communities (Sousa et al., 2019).====P2P energy trading represents ""direct energy trading between peers, where energy from small-scale DERs in dwellings, offices, factories, etc, is traded among local energy prosumers and consumers"" (Alam et al. (2017); Zhang et al. (2018)). Energy communities can involve groups of citizens, social entrepreneurs, public authorities and community organizations participating directly in the energy transition by jointly investing in, producing, selling and distributing renewable energy. This can introduce further flexibility to the investment that could add value, depending on the adoption costs of the new technology and the shape of the load (demand) electricity curve of agents. Therefore, it is interesting to study whether this additional flexibility may have value, how it could affect the investment decisions, and whether it may be supported by data.====In this paper, we examine how the connection to the SG and the possibility to exchange energy among agents, may increase the investment value in a PV plant (i.e., investment profitability) and influence decisions regarding the optimal size of the plant. We model the investment decision of two small (price-taker) end-user households. Each agent is a prosumer that have the non-excluding possibility to: (i) self-consume its energy production, (ii) exchange energy with national grid, and (iii) exchange energy with the other agent.====Due to the irreversibility and high uncertainty over the demand evolution, the technological advances, and the ever changing regulatory environment (Schachter and Mancarella (2015); Schachter and Mancarella (2016); Cambini et al. (2016)), we implement a real option model to determine the optimal size and the overall investment value of a PV system characterized by the features previously described.====Because of the many opportunities, SGs may generate managerial flexibility which prosumers can exercise optimally when deciding to invest. This flexibility gives them the option to decide strategically the optimal production/consumption energy pattern and can significantly contribute to energy saving and hedging the investment risk. To capture the value of managerial flexibility, we calibrate and test our model using data from the Italian electricity market.====In our work we combine decisions on irreversible investments under uncertainty with connections to an SG and with possibility of exchange between prosumers.====This paper contributes to both the SG and real option literature. The first strand studies technologies (Kriett and Salani, 2012), prosumers’ behavior in energy markets (Ottesen et al. (2016), Bayod-Rújula et al. (2017)), demand-side management (Oren (2001), Salpakari and Lund (2016)), demand-response (Schachter and Mancarella (2016), Sezgen et al. (2007)), P2P, and energy community.====On the side of the real option literature, we complement the studies about the energy sector (Kozlova (2017), Ceseña et al. (2013)) and in PV plants (Martinez-Cesena et al. (2013),Tian et al. (2017)) with a novel application in which we introduce prosumers sharing an initial investment and exchanging energy in domestic PV systems. Among these contributions, the closest to ours are: Bertolini et al. (2018) where the size of the optimal plant is identified through a real options analysis; Luo et al. (2014), in which exchange P2P is deepen in a Microgrid context under the assumption of storage possibility and its dynamics is simulated to understand the impact of cooperative energy trading on renewable energy utilization; Zhang et al. (2018) who investigates the feasibility of P2P energy trading with flexible demand and focusing on the energy exchange between the Microgrid and the utility grid; Gonzalez-Romera et al. (2019) where the case of two households prosumers is investigated, even though the focus is on energy exchange minimization instead of energy cost. In this context the novelties of our paper are: (i) the study of the value of flexibility introduced by P2P energy community, and (ii) the use of a real option approach.====Our findings show that at current prices, the introduction of the possibility of selling energy between agents encourages investment in larger plants. Moreover, both the exchange option and the investment deferral option have always a positive value. In addition to that, our results show a positive relation between plant optimal size and optimal investment timing (i.e., the greater the plant optimal size, the greater the investment deferral). About uncertainty, increasing volatility rises the option value to defer and, in turn, increases the investment value. At the same time, with high volatility, the PV plant is built for selling and not for exchanging energy. Thus, the energy community diffusion can be effectively pushed by stabilizing the energy prices’, and reducing their volatility.====The rest of the paper is organized as follows. Section 2 describes the model set-up. Section 3 introduces the calibration of the parameters, and Section 4 provides our main results and comparative statics. Section 5 concludes. Some technicalities are left to the appendices.",Photovoltaic Smart Grids in the prosumers investment decisions: a real option model,https://www.sciencedirect.com/science/article/pii/S0165188920301561,5 September 2020,2020,Research Article,258.0
"Irawan Denny,Okimoto Tatsuyoshi","Crawford School of Public Policy, Australian National University, Acton 2601, Australia,Research Institute of Economy, Trade and Industry (RIETI), Tokyo 100-8901, Japan","Received 13 September 2019, Revised 15 July 2020, Accepted 6 August 2020, Available online 12 August 2020, Version of Record 8 May 2021.",https://doi.org/10.1016/j.jedc.2020.103973,Cited by (8)," and overinvestment but a significantly positive relationship is found between global economic and country-level governance policy uncertainties and overinvestment. Lastly, the results suggest that the effect of overinvestment on firm performance after three years is positive, especially for firms in the mining sector.","Investment is an inherent component of business activities. By making investments, firms grow their capacity to increase output. However, the important economic notion of optimality also applies to firm investments. What if firms invest more than they should? Richardson (2006) defines this phenomenon as overinvestment. He proposes a relative measure that assesses the degree of over- and underinvestment using residuals from firms’ investment functions. Overinvestment can be considered as a result of firms’ risk-taking behaviour, providing higher firms’ performance sometimes but putting them in trouble other times. Therefore, if many firms in a sector or country overinvest, there may be too much risk for the sector or country. Therefore, from the policy-makers’ point of view, it is useful to identify the sectors and countries that tend to overinvest, as well as the possible causes of this overinvestment. Meanwhile, from the business-decision-makers’ point of view, it is useful to assess how overinvestment affects firm performance. Specifically, this study identifies overinvestment and its causes and assesses its impact on firm performance for resource companies from 32 G20-area countries.====From the macroeconomic point of view, overinvestment could have both positive and negative effects. When the business cycle is in a booming phase, general prices increase and thus induce firms to invest more to increase their production capacities (Kiyotaki, 2011). On a massive scale, this can expand aggregate overinvestment in the economy, which would further boost the economy, at least in the short term. However, the overinvestment would have an adverse inter-temporal impact and damage the economy in the long term. Firm overinvestment could also have both negative and positive side effects from the microeconomic point of view. For example, Kulatilaka and Perotti (1998), Weeds (1999), Chevalier-Roignant et al. (2011), and Henriques and Sadorsky (2011) suggest that increasing investment, which could be characterized as overinvestment, is a favourable strategy for firms under high uncertainty, as the overinvestment could improve their performance. On the other hand, Fu (2010), Liu and Bredin (2010), and Ling et al. (2016) argue that overinvesting has a significantly negative impact on the future performance of firms. Therefore, it is worthwhile empirically assessing the effects of overinvestment on firm performance, which this study performs for resource companies across a number of major countries.====In 2017, the total natural resource export from the sample countries accounted for more than 1.5 trillion USD.==== Russia and the United States are the world leaders in the export of natural resources, followed by other G20 countries, such as Australia, Canada, and Brazil. Furthermore, natural resource export plays a critical role in driving many leading economies. Natural resource export accounted for more than 50% of exports for Russia and Australia.==== For many other countries, natural resource export accounted for more than 20% of their total export (Brazil, Greece, Indonesia, Canada, and South Africa). Although the role of the resource sector in the overall economy (as a percentage of total value-added and total employment) might be decreasing, natural resource export still plays a vital role in maintaining macroeconomic performance for these countries, primarily through the export channel. These facts emphasize the importance of analysing overinvestment in the resource sector, providing a solid reason to focus on the sector. Specifically, we include all resource sectors available from our dataset source. These sectors are (1) alternative energy, (2) forestry and paper, (3) mining, and (4) oil and gas producers.====This study contributes to the literature by providing a comprehensive empirical exploration of overinvestment behaviour and its relation with business cycles and macroeconomic uncertainties among resource companies from 32 G20-area countries. Three analyses are conducted to clarify the characteristics of overinvestment and their effects on firm performance. First, this study examines overinvestment and underinvestment behaviour among the sample firms in each period using the framework developed by Richardson (2006). Second, this study examines whether business cycles and macroeconomic uncertainties play a significant role in explaining overinvestment behaviour. Third, this study examines how overinvestment affects firm performance.====This study considers the business cycle as a possible source of overinvestment. Specifically, this study employs a dual business cycle approach by considering the world business cycle and the home-country business cycle. This dual approach is effective in capturing the overall effect of business cycle fluctuations on companies’ overinvestment behaviour. In addition, commodity price uncertainty, global geopolitical uncertainty, and global economic policy uncertainty are considered in order to examine the relationship between overinvestment and uncertainty. Furthermore, a worldwide governance indicator is adopted as a proxy for country-level uncertainty.====This study offers several significant findings. The first analysis indicates that internal firm factors play a significant role in determining firms’ investment decision making and that the 2008 global financial crisis had a significant impact on overinvestment patterns in many countries. This result is also confirmed statistically by comparing estimated parameters from the investment function between before and after 2008. Also, the results suggest that the forestry and paper sector overinvests relative to the standard investment level predicted by the investment function regardless of the sample period, while the alternative energy sector tends to underinvest. Furthermore, many emerging economies, including Brazil, China, India, Indonesia, Russia, and South Korea, are found to have overinvested over the last three decades or so. The second analysis shows that commodity price inflation plays a more important role in inducing firms’ overinvestment than commodity price uncertainty does. The home country business cycle also significantly affects overinvestment, with signs alternating from negative to positive before and after the global financial crisis, while the world business cycle has no significant relationship with overinvestment. The finding also shows no significant relationship between global geopolitical risk and overinvestment but finds a significantly positive relationship between global economic and country-level governance policy uncertainties and overinvestment. Finally, the third analysis demonstrates that the effect of overinvestment on firm performance is positive, especially for firms in the mining sector.====The remainder of the paper is structured as follows. Section 2 reviews the literature and provides a theoretical background on overinvestment and discusses the empirical literature on the measurement of overinvestment and its relation to uncertainty and firm performance. Section 3 explains the study’s dataset, while Section 4 explains the study’s methodology. Section 5 presents the study’s empirical results and discusses them. Finally, Section 6 concludes the paper.",Overinvestment and macroeconomic uncertainty: Evidence from renewable and non-renewable resource firms,https://www.sciencedirect.com/science/article/pii/S016518892030141X,12 August 2020,2020,Research Article,259.0
"Picarelli Athena,Vargiolu Tiziano","Dipartimento di Scienze Economiche,Via Cantarane, Università di Verona Verona, VR 37129, Italy,Dipartimento di Matematica, Università degli Studi di Padova, Via Trieste 63, Padova, PD 35121, Italy","Received 15 September 2019, Revised 19 May 2020, Accepted 24 May 2020, Available online 10 June 2020, Version of Record 8 May 2021.",https://doi.org/10.1016/j.jedc.2020.103940,Cited by (11),"We present a novel technique to solve the problem of managing optimally a pumped hydroelectric storage system. This technique relies on representing the system as a stochastic ==== with state constraints, these latter corresponding to the finite volume of the reservoirs. Following the recent level-set approach presented in ====, we transform the original constrained problem in an auxiliary unconstrained one in augmented state and control spaces, obtained by introducing an exact penalization of the original state constraints. The latter problem is fully treatable by classical ==== arguments.","In the current transition to a low carbon economy, one of the most prominent issues is that the most used renewable energy sources (RES), i.e. photovoltaic and wind, are non-dispatchable: for this reason, there is a strong need to store the energy produced when they are available in order to use it when they are not.====As for today, the most cost-efficient method to store electricity is with pumped hydroelectric storage (PHS) systems, which in 2017 accounted for about 95% of all active tracked storage installations worldwide, with a total installed capacity of over 184 GW D. of Energy of the U.S.A. (2017). In brief, these systems consist of two or more dam-based hydroelectric plants, linked sequentially so that, while the water used to produce electricity in the lower reservoirs is lost as in traditional hydroelectric plants, upper reservoirs discharge their water in lower reservoirs, where it can be stored and possibly pumped back to upper reservoirs when the electricity price is low (typically in off-peak periods). In this way, a PHS can produce electricity in peak periods, i.e. when its price is high, and recharge the upper reservoir in off-peak periods. These systems can be efficiently coupled with non-dispatchable renewable energy sources (like photovoltaic or wind), thus providing a very effective way to store possible surplus renewable energy when the price is low, and to use it when needed. An alternative is of course to connect the system directly to the power grid: also in this case, PHS is likely to use renewable-generated electricity to pump back water in the upper reservoirs, given the very low marginal generating costs of renewable energy.====In the current literature, the mathematical treatment of PHS is usually done by computing the fair value as the sum of discounted payoffs when operated at optimum==== To perform this computation, one can find in literature two alternative approaches. The first one is via operation research techniques by formulating a linear/quadratic programming model, see e.g. Brown et al. (2008), Löhndorf et al. (2013), Vespucci et al. (2012). While these techniques allow to model potentially complex networks and constraints, the optimal exercise policy of the system within this approach turns out to be given only by the numerical solution of a linear/quadratic program, from which it is very difficult to extract the policy as a function of the relevant state variables (typically, the spot price of electricity and the reservoir levels).====The second approach is based on stochastic optimal control in continuous time, where with the dynamic programming approach one derives a partial differential equation called the Hamilton–Jacobi–Bellman (HJB) equation, i.e. a second-order partial differential equation, see e.g. Felix and Weber (2014), Shardin and Wunderlich (2017), Thompson et al. (2004), Zhao and Davison (2009), Chen and Forsyth (2008) and references therein. The HJB equation is usually nonlinear, and without an explicit analytical solution: thus, one should recur to numerical methods, and this limits the dimensionality that one can reach. Proof of this is that in all these papers (apart from the notable exception of Felix and Weber, 2014), the lower reservoir has infinite capacity (e.g. is a sea basin): this entails that their model has one less state variable and much easier state constraints. However, the main advantage of this technique is that one can obtain the optimal pumping/producing strategy as a feedback control, i.e. as a function of the relevant state variables (here being time, electricity price and water levels in the basins).====As said above, our approach is based on optimal stochastic control in continuous time and dynamic programming, leading to a HJB equation. Indeed, dynamic programming techniques are usually applied to prove that the value function associated to optimal control problems is the unique solution of the HJB equation in the viscosity sense and to characterize the optimal policy as feedback of the state variables. However, while this is a well-established research field for generic unconstrained optimal control problems, PHS has the peculiarity that, the reservoirs being finite, the state variables corresponding to their levels have to satisfy given constraints: thus, we have to formulate a state-constrained optimal control problem. There exists a huge literature on state-constrained optimal control problems and their HJB characterization. We refer the reader e.g. to Barles, Burdeau, 1995, Barles, Rouy, 1998, Ishii, Loreti, 2002, Katsoulakis, 1994 for stochastic control problems and to Capuzzo-Dolcetta, Lions, 1990, Ishii, Koike, 1996, Soner, 1986, Soner, 1986 for deterministic ones. In this case, the characterization of the value function as a viscosity solution of a HJB equation is intricate and usually requires a delicate interplay between the dynamics of the processes involved and the set of constraints, see e.g. Basei et al. (2014). First, some ==== (or ====) conditions have to be satisfied to guarantee the finiteness of the value function, second, specific properties on the set of admissible controls must hold to ensure the continuity of the value function and its PDE characterization. When the behavior at the boundary of the constrained region is clear, one could think in principle to use the same penalization techniques as in Basei et al. (2014). However, when (as in our case) one does not have natural boundary conditions, this approach would not characterize the solution uniqluely. This often makes the problem not tractable by the classical dynamic programming techniques.====In this paper we follow the alternative approach developed in Bokanowski et al. (2016) to provide a fully characterization of the value function and optimal strategy associated to the optimal control problem in a general framework. We pass by a suitable auxiliary reformulation of the problem which allows a simplified treatment of the state constraints. This is achieved by the use of the so called ====, built to permit a treatment of state constraints by an exact penalization technique. Initially introduced by Osher and Sethian (1988) to model some deterministic front propagation phenomena, the level-set approach has been used in many applications related to controlled systems (see e.g. Altarovici, Bokanowski, Zidani, 2013, Falcone, Giorgi, Loreti, 1994, Kurzhanski, Varaiya, 2006, Margellos, Lygeros, 2011, Soner, Touzi, 2002).====In our case, the level-set method allows to link the original state constrained problem to an auxiliary optimal control problem, referred as the ==== defined on an augmented state and control space, but without state constraints. This level-set problem has the great advantage of leading to a complete characterization of the original one and of being, at the same time, fully treatable by classical dynamic programming argument under very mild assumptions.====The rest of the paper is organized as follows. We introduce the optimal control problem and the main assumptions in Section 2. In Section 3 we provide a HJB characterization of the associated value function under suitable controllability conditions on the system dynamics. Then, under a simplified model, we discuss in Section 4 the main difficulties arising from the presence of state constraints when such assumptions are not satisfied. In Section 5 we present the level set method and provide the main results of the paper. A numerical validation of the proposed approach is provided in Section 6.",Optimal management of pumped hydroelectric production with state constrained optimal control,https://www.sciencedirect.com/science/article/pii/S0165188920301081,10 June 2020,2020,Research Article,260.0
"Antoci Angelo,Borghesi Simone,Galeotti Marcello,Sodini Mauro","University of Sassari, Italy,European University Institute and University of Siena, Italy,University of Florence, Italy,University of Pisa, Italy","Received 14 September 2019, Revised 29 April 2020, Accepted 10 May 2020, Available online 20 May 2020, Version of Record 8 May 2021.",https://doi.org/10.1016/j.jedc.2020.103929,Cited by (9),"Environmental problems are increasingly frequent, intensive and unpredictable. To protect from the observed environmental depletion, economic agents increasingly react by substituting previously free public environmental goods with costly private goods. This substitution mechanism, however, can contribute to enhance the indeterminacy of the possible consequences of mankind activity, further increasing the uncertainty on the future environmental trajectories. To investigate this issue, the paper proposes an intertemporal optimization problem in which agents derive utility from three goods: leisure, a public environmental good and/or private consumption that can be used as a substitute for the environment. The analysis shows that the economy may end up being trapped in the Pareto-dominated steady state and that both local and global indeterminacy may arise in the model. No indeterminacy, however, emerges if green technologies are used so that production has no negative effects on the environment.","Environmental problems rank progressively higher in the international political agenda attracting increasing attention from the public opinion. A growing number of people, scholars and international institutions worldwide (IPCC, 2014, IPCC, 2018, IPCC, 2019, Sachs, Schmidt-Traub, Kroll, Lafortune, Fuller, 2019, Climate Strike) call for immediate action to stop environmental degradation and its large negative effects. Mitigation activities are particularly needed since environmental problems are increasingly frequent, intensive and unpredictable. The combination of these three worrisome features increases the uncertainty on the environmental consequences of economic growth and of related anthropogenic activities, causing the indeterminacy of future environmental trajectories.==== As originally pointed out in the early literature on this issue by Pearce, Barrett, Markandya, Barbier, Turner, Swanson, 1991, Pearce, Markandya, Barbier, 1989, this might eventually lead the economy into a “grey zone” in which environmental consequences of human action are uncertain and possibly irreversible.====Along with the rise in environmental degradation and uncertainty, another (strictly related) phenomenon has attracted the attention of many scholars: the tendency to replace previously free public environmental goods with costly private consumption goods. In modern industrial economies one can identify a plethora of private goods and services that agents use to self-protect from environmental degradation. Some of the most typical and often-quoted textbook examples include air filters and water treatment plants, mineral water, double-glazing to reduce the acoustic damage from urban traffic, medicines against pollution-related diseases (e.g. asthma and skin diseases). In all these cases, individuals are now forced to pay for goods that were once freely available (i.e. clean air, clean water, silent cities etc.); by consuming these goods they try to restore the utility they used to enjoy from a pristine environment in the status quo (i.e. before environmental degradation took place). This substitution phenomenon – that was originally described in a seminal contribution by Hirsch (1976) who introduced the concept of “defensive consumption” – goes beyond these few textbooks examples and has become so pervasive in modern societies that it may account for up to several points of GDP.====In some cases this substitution mechanism may further increase environmental degradation. Thus, for instance, as reported by Sun et al. (2017) for China, the massive use of air filters to self-protect from outdoor air pollution may contribute to further worsen the air quality problem. The same occurs with the large increase in the use of air conditioning in response to heat waves and global warming. In these cases, therefore, the individual self-adaptation process tends to enhance and transfer the negative externalities to the other agents rather than filter them (Shogren and Crocker, 1991), what has been described with the term maladaptation in the literature on this topic (Antoci, Gori, Sodini, Ticci, 2019, Barnett, O’Neill, 2010, Schneider et al., 2001, UNEP, 2019).====This paper tries to relate the two phenomena mentioned above: on the one hand, the progressive substitution of public environmental goods with private consumption goods, and on the other hand, the increasing indeterminacy of environmental consequences of our activity. Our aim is to build a bridge between the two correspondent research lines. More precisely, the present paper shows that the substitution mechanism described above may cause indeterminacy, further increasing the uncertainty about future environmental trajectories.====Differently from previous studies in the literature on indeterminacy (see, for a review, Bella, Mattana, Venturi, 2017, Mino, 2017), in which indeterminacy is generated by positive externalities in the production process or in the accumulation process of human capital, in our model (local and global) indeterminacy results from negative externalities only, that are generated by the substitution process between the private good and the environmental good. More precisely, economic agents react to the depletion of the environmental resource by an increase in their labor input, which allows them to produce a higher quantity of private good which is consumed as substitute for the environmental resource. The consequent increase in production and consumption of the private good generates a further reduction in the stock of the environmental resource, and so on.====To investigate the issue described above, in this paper we analyze an economy with optimizing agents (the context is that proposed by Wirl, 1997) in which agents’ well-being depends on three goods: a produced (private) good, leisure, and a stock of a (free access) renewable environmental resource. The production activity of the private good deteriorates the natural resource, and individuals may defend themselves from environmental degradation by increasing consumption of the private good, which may be perceived as a “substitute” for the environmental resource.====In the proposed model, economic agents have to solve an intertemporal optimization problem in which the state variables are the stock of physical capital ==== accumulated by each agent and the stock ==== of a free access renewable environmental resource. The control variables are agents’ labor input ==== and consumption ==== of the produced good.====The analysis of the model shows that there exist at most two steady states, ==== and ====, the former being a poverty trap that is Pareto-dominated by the latter and has a lower level of the environmental resource. As it will be shown below, the poverty trap ==== can be an attractor only if the environmental good and the private consumption good are substitutes, namely, if the marginal utility of consumption increases as the environmental good decreases. Indeed, if that is the case, individuals have an incentive to work more and more as the environment depletes to afford higher consumption levels, but this leads the economy on a welfare-reducing trajectory.====When ==== is an attractor then the dynamics are ====, namely, given the initial conditions there exists a continuum of possible trajectories leading to ==== so that one cannot predict a priori how the economy will converge to ====. Furthermore, numerical simulations suggest that scenarios of ==== may occur, that is, given the initial conditions both steady states can be reached so that one cannot predict a priori where the economy will eventually converge to.==== This indeterminacy scenario may be observed because economic agents are unable to coordinate their choices, in that each of them takes the stock ==== of the environmental resource as exogenously given.====Finally, our findings show that indeterminacy may occur in the model if production has a negative impact on the environment. If green technologies are used, the model admits only one saddle point with two-dimensional stable manifold, therefore no indeterminacy occurs in that case.====The present work builds upon and extends previous studies (cf. Antoci, Galeotti, Russu, 2005, Antoci, Galeotti, Russu, 2007) in the research strand on the substitution of environmental goods with private consumption goods. However, it differs from such studies in two main respects. First, those works assumed an additively separable utility function so that the disutility of labor is not influenced by environmental quality and consumption, whereas here we assume that environmental quality affects the utility deriving from leisure and consumption (the utility function being multiplicative in consumption, leisure and the environment). Second, in the studies mentioned above global indeterminacy was absent (Antoci et al., 2007) or could be observed only assuming high positive externalities (Antoci et al., 2005), while here we show that it can occur also without any positive externality and assuming negative externalities only.====The paper will be structured as follows. Sections 2 and 3 define the set-up of the model and the associated dynamic system. Section 4 deals with the existence and local stability of steady states. Section 5 is devoted to numerical simulations of dynamics. Section 6 extends the model introducing output taxation to charge for the negative externalities generated by the production activity. Section 7 concludes the paper.","Living in an uncertain world: Environment substitution, local and global indeterminacy",https://www.sciencedirect.com/science/article/pii/S016518892030097X,20 May 2020,2020,Research Article,261.0
"Kollmann Robert,Lubik Thomas A.,Roeger Werner","Solvay Brussels School of Economics & Management, Université Libre de Bruxelles, Brussels, Belgium,Centre for Economic Policy Research (CEPR), London, United Kingdom,ERUDITE, Université Paris-Est Créteil (UPEC), Créteil, France,Federal Reserve Bank of Richmond, Richmond, USA,Deutsches Institut für Wirtschaftsforschung (DIW), Berlin, Germany,Europäisches Institut für Internationale Wirtschaftsbeziehungen (EIIW), Wuppertal, Germany,KUL-VIVES, Leuven, Belgium","Available online 6 August 2021, Version of Record 27 October 2021.",https://doi.org/10.1016/j.jedc.2021.104212,Cited by (0),None,None,"Secular stagnation, low interest rates and low inflation: Causes and implications for policy",https://www.sciencedirect.com/science/article/pii/S0165188921001470,6 August 2021,2021,Research Article,266.0
"Dawid Herbert,Kort Peter M.,Vergalli Sergio","Department of Business Administration and Economics and Center for Mathematical Economics, Bielefeld University, Bielefeld, Germany,Department of Econometrics and Operations Research & CentER, Tilburg University, The Netherlands and Department of Economics, University of Antwerp, Belgium,Department of Economics and Management, Università degli Studi di Brescia, Brescia, Italy,Fondazione Eni Enrico Mattei, FEEM, Milan","Available online 8 May 2021, Version of Record 8 May 2021.",https://doi.org/10.1016/j.jedc.2021.104126,Cited by (1),None,None,"Investments, Energy, and Green Economy",https://www.sciencedirect.com/science/article/pii/S0165188921000610,8 May 2021,2021,Research Article,271.0
"Georgiadis Georgios,Jančoková Martina","European Central Bank, 60311 Frankfurt am Main, Germany","Received 4 January 2020, Revised 13 October 2020, Accepted 27 October 2020, Available online 4 November 2020, Version of Record 18 November 2020.",https://doi.org/10.1016/j.jedc.2020.104025,Cited by (5),We hypothesise that New Keynesian ,"A salient feature of the global economy since the 1990s has been the dramatic rise of financial globalisation. Whether measured by (gross) capital flows or indicators reflecting the extent of legal capital account restrictions, economies’ financial markets have been exhibiting an increasing degree of integration. As a result, the global economy has progressively become subject to large cross-country spillovers through financial channels. Indeed, a growing body of empirical research provides evidence that financial interlinkages play a critical role in the transmission of shocks across economies. Similarly, several studies document the sizable impact of global factors on output and inflation in the rest of the world that materialises through financial channels (Ha et al., 2020). Some work even suggests that economies’ financial markets are subject to a global financial cycle, which is argued to materialise in variations in global risk aversion and to be driven by US monetary policy (Bekaert, Hoerova, Lo Duca, 2013, Bruno, Shin, 2015, Bruno, Shin, 2015, Ghosh, Qureshi, Kim, Zalduendo, 2014, Miranda-Agrippino, Rey, 2020, Passari, Rey, 2015, Rey, 2016).====At the same time, over the last two decades important advances in structural monetary modelling have been achieved, as reflected in the huge amount of work on New Keynesian dynamic stochastic general equilibrium (NK DSGE) models. While the first NK DSGE models focused on frictions in price setting and labor markets (Christiano, Eichenbaum, Evans, 2005, Smets, Wouters, 2003), the global financial crisis epitomised the role of frictions in financial markets for the propagation of shocks. The resulting wave of work has focused on introducing frictions in ==== financial markets (Christiano, Motto, Rostagno, 2014, Gertler, Karadi, 2011). Advances have also been made in generalising the initially closed-economy NK DSGE models to analyse the international transmission of shocks and policy design in open economies, giving rise to New Open-Economy Macroeconomics (Obstfeld and Rogoff, 1996). After two decades of continuous development and research it is fair to say that NK DSGE models have become standard elements of macroeconomists’ toolbox. In particular at central banks elaborate versions of NK DSGE models are routinely used, for example in order to determine what shocks have been drivers of recent business cycle movements. This is an important exercise, as the policy response to business cycle fluctuations depends on what type of shocks are driving the economy.====Against the background of the continuous strengthening of cross-border financial integration, it is noteworthy that powerful spillover channels based on frictions in ==== financial markets—for example involving cross-border interbank balance sheet exposures, collateral constraints and currency mismatches—are not routinely incorporated in NK DSGE models yet.==== Possible consequences of this particular discrepancy between empirics and theory have hardly been explored systematically so far in the literature. We aim to fill part of this gap with this paper.====For example, Justiniano and Preston (2010) as well as Alpanda and Aysun (2014) find that the standard open-economy NK DSGE model in the literature fails to replicate the large degree of cross-country business cycle co-movement in the data, and that it implies only an implausibly minor role of foreign disturbances for the evolution of domestic variables. More specifically, these studies find that the theoretical moments implied by standard NK DSGE models—which do not account for powerful financial spillover channels—are much closer to their empirical counterparts if it is assumed that the structural shocks are cross-country correlated.==== This suggests that standard NK DSGE models lack empirically relevant cross-border transmission channels for country-specific shocks or a global dimension that would allow to consider common shocks. In this paper we take the analyses of Justiniano and Preston (2010) as well as Alpanda and Aysun (2014) as a starting point to explore the degree to which standard NK DSGE models lack empirically meaningful ==== spillover channels.====In particular, in this paper we hypothesise that the standard NK DSGE models used in the profession fail to account for core-economy monetary policy spillovers that materialise through financial channels. We test this hypothesis by verifying—in a meta-study-like fashion—if several predictions are borne out by the NK DSGE models used in the literature: First, under our hypothesis we expect that monetary policy shock estimates of non-core economies are contaminated by a common component and are therefore positively cross-country correlated, the more so the more non-core economies are financially integrated and susceptible to spillovers through financial channels in the data. Second, we expect actual core-economy monetary policy to load positively on monetary policy shock estimates of non-core economies obtained from NK DSGE models, again the more so the more non-core economies are susceptible to spillovers through financial channels. And third, we expect the cross-country correlations between monetary policy shock estimates of non-core economies to be smaller when cleansed from actual core-economy monetary policy.====We provide empirical evidence that is consistent with the predictions from our hypothesis based on a dataset of monetary policy shock estimates for 29 economies obtained from more than 190 structural monetary models used in the literature. In particular, first, we document that the cross-country correlations between non-core—i.e. non-US—economies’ monetary policy shock estimates obtained from NK DSGE models are positive on average. This finding suggests that non-core economies’ monetary policy shock estimates are contaminated by a common component. Interestingly, we document that the contamination is as severe for monetary policy shock estimates obtained from elaborate NK DSGE models used at central banks and international organisations as for shock estimates obtained from stylised NK DSGE models used in academia. We also document that the cross-country correlations are larger for non-core economies which are more integrated with global financial markets in the data. This finding suggests that the existence of financial spillover channels in the data plays a role in the contamination of non-core economies’ monetary policy shock estimates obtained from NK DSGE models by a common component.====Second, we document that non-core economies’ monetary policy shock estimates obtained from NK DSGE models are positively correlated with changes in the US monetary policy rate, even after controlling for measures of global shocks such as changes in oil prices or the VXO. This finding suggests that the contamination of non-core economies’ monetary policy shock estimates obtained from NK DSGE models by a common component is at least in part driven by US business cycle shocks in the data and the endogenous response of US monetary policy spilling over to global financial markets. Again, we document that the loading of the US monetary policy rate on monetary policy shock estimates obtained from NK DSGE models is larger for non-core economies which are more strongly integrated with global financial markets in the data.====Third, we document that once cleansed from changes in the US monetary policy rate non-core economies’ monetary policy shock estimates obtained from NK DSGE models are significantly less cross-country correlated. Also, we show that cleansing from variables measuring global shocks such as changes in oil prices or the VXO alone does not achieve this. This finding suggests that the contamination of non-core economies’ monetary policy shock estimates obtained from NK DSGE models by a US monetary policy component does not merely reflect the latter’s response to global shocks.====We obtain some further evidence worth mentioning. First, we generalise the focus on US monetary policy and allow for the euro area to be the core-economy for European economies. And indeed, our findings regarding the contamination of non-core—i.e. non-US/non-euro area—monetary policy shock estimates obtained from NK DSGE models by a common component are at least robust if not strengthened when considering regional core economies. Second, we also obtain some tentative evidence that is consistent with the trilemma in international macroeconomics (di Giovanni, Shambaugh, 2008, Klein, Shambaugh, 2015, Obstfeld, Obstfeld, Shambaugh, Taylor, 2005): The contamination by a common component is less severe for monetary policy shock estimates for emerging market economies which impose capital controls and which feature flexible exchange rate regimes.====We would like to emphasise that our paper is not to be read as a general critique or dismissal of the use of NK DSGE models in the profession. Consistent with the view of Blanchard (2016), we believe that NK DSGE models “are eminently improvable and central to the future of macroeconomics”, and that whether specific elements—such as powerful financial spillover channels—are necessary depends on the purpose the models are used for. The conceptual analysis in our paper suggests that modelling of powerful financial spillover channels, at least in NK DSGE models that are used for policy analysis at central banks and international organisations, is important. And indeed, we find some evidence in our empirical analysis that the contamination by a common component is less severe if the shock estimates stem from NK DSGE models that do feature an explicit open-economy dimension and frictions in international financial markets.====More specifically, it would obviously be useful to identify the precise types of financial spillover channels that should be accounted for in NK DSGE models. In general, we think this depends on the economy in question. However, our evidence suggests that in particular currency mismatches on economies’ external balance sheets represent a powerful conduit for the transmission of core-economy monetary policy spillovers that underly the contamination of non-core economies’ shock estimates by a common component. Accounting for the role of foreign-currency borrowing thus seems to be a promising route for introducing powerful and empirically relevant financial spillover channels in NK DSGE models. Moreover, we find that the contamination of monetary policy shock estimates by a common component is particularly severe for non-core economies which are more financially integrated through international banking linkages. Accounting for the role of cross-border banking thus seems to be another promising route for introducing powerful and empirically relevant financial spillover channels in NK DSGE models. Interestingly, work on cross-border banking and currency mismatches in NK DSGE models is growing (Akinci, Queralto, 2019, Aoki, Benigno, Kiyotaki, 2018, Banerjee, Devereux, Lombardo, 2016).====It is important to explain why we focus on the contamination of ==== shock estimates. First, monetary policy shocks are the most commonly estimated shocks in structural and reduced-form macro-models; for example, while virtually all NK DSGE models feature a monetary policy shock, not all feature fiscal policy shocks. We can thus most efficiently test our hypothesis on the mis-specification of financial spillover channels using monetary policy shock estimates. Second, monetary policy shocks are the most straightforward—or maybe one should say least difficult—to interpret from a conceptual point of view. For example, the notion of demand and supply shocks is related to how preferences and technology are modelled; and the notion of fiscal policy shocks depends on the type of taxes and expenditures considered. In contrast, a monetary policy shock is typically introduced as a disturbance term that is tacked on to the Taylor-rule that determines the short-term interest rate. And finally, especially monetary policy has been associated with large spillovers through ==== channels in the empirical literature (Bekaert, Hoerova, Lo Duca, 2013, Miranda-Agrippino, Rey, 2020, Passari, Rey, 2015, Rey, 2016).====Our paper is related to the literature which is concerned with the role of financial spillover channels in structural monetary models for cross-country business cycle correlations (Akinci, Queralto, 2019, Aoki, Benigno, Kiyotaki, 2018, Banerjee, Devereux, Lombardo, 2016, Chin, Filippeli, Theodoridis, 2015, Iacoviello, Minetti, 2006, Nuguer, 2016, Ueda, 2012, Yao, 2019). Within this literature, our paper is most closely related to Justiniano and Preston (2010) as well as Alpanda and Aysun (2014). Their finding that standard NK DSGE models can only replicate international business cycle co-movement when structural shocks are allowed to be cross-country correlated is consistent with our finding that NK DSGE models that do not account for powerful financial spillover channels produce cross-country correlated monetary policy shock estimates. Finally, while the analyses of Justiniano and Preston (2010) as well as Alpanda and Aysun (2014) are based on counterfactual simulations of ==== structural models, in this paper we consider monetary policy shock estimates from more than 190 structural monetary models estimated for 29 economies.====The rest of this paper is organised as follows. In Section 2, we illustrate the mechanics of our hypothesis and derive testable predictions from a stylised counterfactual Monte Carlo experiment. In Section 3 we present our monetary policy shock database and test the predictions from our hypothesis derived in Section 2. Finally, Section 4 concludes.","Financial globalisation, monetary policy spillovers and macro-modelling: Tales from 1001 shocks",https://www.sciencedirect.com/science/article/pii/S0165188920301937,4 November 2020,2020,Research Article,277.0
Madeira Carlos,"Central Bank of Chile, Agustinas 1180, Santiago, Chile","Received 19 March 2020, Revised 22 August 2020, Accepted 27 October 2020, Available online 1 November 2020, Version of Record 9 November 2020.",https://doi.org/10.1016/j.jedc.2020.104026,Cited by (0),"Families’ ==== depend on beliefs about their children’s performance. I build a dynamic model of expectation formation to show how agents use both observable and unobservable information to predict their school scores. The model shows parents and students have substantial knowledge of unobservable factors affecting their performance, especially in middle and high school. Families are overconfident towards expecting higher grades and expectation formation differs by race. Families’ ability to predict future scores improved substantially during middle school due to several factors: lower bias and variance of the prediction errors, and a better use of past scores as predictors.","Predicting individual future performance is important in decisions with uncertain outcomes, such as starting a firm, paying loans (Madeira, 2019), raising children (Jervis, 2017), choosing a work career, applying to college or saving for retirement (Delavande and Rohwedder, 2011). Expectations are particularly relevant for academic choices, since many human capital decisions are made early in life (Cunha, Heckman, Lochner, Masterov, 2005, Delalibera, Ferreira, 2019). Economic models of education choice usually assume that agents are able to predict their academic performance and the expected returns of each option. This is a strong assumption, since a student’s performance may change substantially when arriving to a new grade or school level. It can be difficult for families to forecast their children’s academic achievement for several reasons: educators may favor different teaching methods, schools provide different social environments and curriculum materials may change between years.====This paper uses a publicly-available dataset, the Beginning School Study (BSS), to study how families use available information to form expectations of their academic achievement. The BSS elicited point predictions of academic outcomes for a panel of 825 parents and students from the children’s first-grade until their adult years. Previous work with the BSS data shows that families are systematically overoptimistic about their children’s school performance, although their predictions improved as the students aged (Madeira, 2018). To evaluate how respondents use available information I specify a model of student achievement and agents’ expectations, where respondents forecast their future scores based both on observable information (demographic characteristics and past performance) and private information of the agents==== that is unobservable to the econometrician. Respondents know the cut-off values used by the teachers to assign grades based on the students’ achievement. The model is distinct for parents and children, since the respondents within the same family do not necessarily share the same private information (Giustinelli and Manski, 2018): it is possible that the parents and children’ private information sets have a common component and are therefore correlated, but it is not necessary to specify and estimate this correlation because the dynamic model analyzes parents and children separately. Furthermore, in each period the respondents update both their observable (students’ past grades) and private information. The distribution of the respondents’ private information can be identified by the econometric model from two sources: 1) the heterogeneity of beliefs among agents with the same observable information, 2) the correlation of beliefs with the actual outcomes. I then compare the respondents’ predictions to those of a counterfactual rational agent that has access only to observable information and which therefore knows less than the respondents (which also have private information). The model can then be used to examine several sources of the agents’ prediction errors: 1) overconfidence (defined as a positive bias between the expected achievement and the respondents’ expectations), 2) inefficient use of the information observed in the student’s previous academic scores (defined as the average respondent reacting too little to the information in past grades relative to a rational agent’s predictions), or 3) noisy use of private information (defined as the excess variance in respondents’ prediction errors relative to those of a rational agent).====Using this model of expectation formation I find that in elementary school parents and students presented both a large bias and variance for their predictions, indicating that respondents are both overconfident and use noisy private information. The results suggest the overconfidence bias is smaller for families with higher education levels, older parents, and parents of girls. Respondents in elementary school react to differences in past academic achievement, but update their expectations more slowly than a rational agent would. Black families are more optimistic than average, but had prediction errors with a similar variance as white families. Over the years, however, black families made similar gains in their ability to predict performance as their white counterparts.====The model also shows that the role of parents and students’ private information about unobserved factors affecting school performance increased substantially during middle school and high school. Students in particular have a much higher degree of private information than their parents during high school. In middle school families’ bias and prediction variance show a strong decrease, in particular for students. In addition, I find that both parents and students made better use of the information available in past marks, updating recent information about school performance more quickly. Respondents’ ability to predict future scores improved substantially during middle school and high school. This improvement can be decomposed in terms of several components: lower bias and variance of the prediction errors, and a better use of past scores as predictors. Economic models of human capital decisions, therefore, should take into account higher degrees of private information as students grow older and that the unobserved information known by teenage students increases at quicker rates than for their parents. One potential implication for overcoming the information loss of parents in middle and high school is to increase the quality and frequency of the information sent to parents as children age (Bergman, 2020, Bergman, Edmond-Verley, Notario-Risk, 2018, Berlinski, Busso, Dinkelman, Martinez, 2016). Other studies are also taking into account that academic decisions are often a joint choice process made by parents and children (Attanasio and Kaufmann, 2014). One potential policy impact of considering parents and children’s joint roles in the education process is that incentives can be more efficiently targeted, with more capable parents managing better the resources for their children, while higher performing children can receive direct rewards for their achievement (Berry, 2015). Although there is a lack of quality survey data on parents and children’s expectations and their roles on the final academic outcomes (Giustinelli and Manski, 2018), some studies are dealing with how expectations of parents and children shape their schooling choices (Giustinelli, 2016). However, panel data on the joint expectations of parents and children is often still missing in this literature (Giustinelli, 2016, Giustinelli, Manski, 2018), therefore the BSS dataset and the dynamic learning model estimated for its respondents is a good complement to past studies.====Incorrect beliefs about achievement may lead families to make inefficient investments. For instance, overconfident students may put less effort in school. I find black parents and students are more optimistic about their academic scores even after several years, which may explain why they stay longer as students in school (Lang, Manove, 2011, Rivkin, 1995), despite having similar returns to education (Lang, Manove, 2011, Lang, Ruud, 1986).====This work is related to a large body of literature testing rational expectations, models of learning and updating beliefs (Attanasio, Almas, Jervis, 2020, Cornand, Hubert, 2020, Manski, 2018), both for continuous variables (Madeira and Zafar, 2015) and qualitative outcomes (Madeira, 2018). It is also related to the education process and expectations of disadvantaged families (Alexander, Entwisle, Thompson, 1988, Oyserman, 2015). Other work finds that incorrect beliefs about academic performance may explain inefficient education choices, such as college dropout decisions (Stinebrickner, Stinebrickner, 2014, Stinebrickner, Stinebrickner, 2012). Many empirical studies show that agents tend to overestimate their ability and their estimates do not improve significantly with feedback on the past performance (Hoelzl and Rustichini, 2005). Some laboratory experiments studied how agents update their beliefs with new information (Houser et al., 2004). However, lab studies may fail to replicate how agents learn over longer periods or in less standardized environments. Because few datasets follow the same respondents over many years, little is known about how agents adjust their beliefs (Conlon, Pilossoph, Wiswall, Zafar, van der Klaauw, 2012, Manski, 2018). This work fills some of that gap, since the extended time panel of the BSS dataset allows the researcher to observe how families change their beliefs as they age and learn more information (Conlon et al., 2018).====The paper is organized as follows. Section 2 describes the BSS data. Section 3 describes the structural model of expectation formation. Section 4 presents the estimation results of the structural model and explains the main changes in parents and students’ predictions of their academic performance. Finally, Section 5 presents a summary of the main results.",Learning your own ability,https://www.sciencedirect.com/science/article/pii/S0165188920301949,1 November 2020,2020,Research Article,278.0
"Guo Bin,Huang Fuzhe,Li Kai","School of Finance Nankai University, Tianjin 300350, China,Department of Applied Finance, Macquarie Business School Macquarie University, NSW 2109, Australia","Received 3 July 2020, Revised 12 October 2020, Accepted 25 October 2020, Available online 28 October 2020, Version of Record 9 November 2020.",https://doi.org/10.1016/j.jedc.2020.104024,Cited by (2),"This paper studies the impact of time to build on the term structure of interest rates in an otherwise standard (Cox et al., 1985a; Cox et al., 1985b, CIR) production economy. Due to time to build, production depends not only on the current business condition as in the original CIR, but also on past conditions over the production period. This causes equilibrium quantities, including the short rate, forward rates, and bond returns, to depend on the historical path of the production opportunities. Production delay that accumulates uncertainty over the time to build generates significant time variations in bond risk premia. Bond returns can be predicted by current forward rates, as well as their lagged values, since current market states not only affect the current short rate but also the short rate in a distant future. Due to the path dependence, risk premia cannot be fully spanned by current yields. We find evidence that time to build improves the ability of the CIR in generating empirical facts.","Built on a general production-based equilibrium framework of Cox et al. (1985a), the classical (Cox et al., 1985b, CIR) model of the term structure of interest rates has become one of the foundational tools for studies and practices in the fixed-income area. The CIR and its extensions are found to successfully generate many important empirical facts observed in bond markets.====However, the CIR struggles with matching the recent empirical evidence that the cross-section of yields does not span all information relevant to the forecasting of bond risk premia (e.g, Duffee, 2011, Joslin, Priebsch, Singleton, 2014, Ludvigson, Ng, 2009).==== Indeed, one assumption made by Cox, Ingersoll, Ross, 1985a, Cox, Ingersoll, Ross, 1985b is that the economy’s technology can transform new investment into the final good ==== (i.e., without time to build). This assumption, although it significantly simplifies the analysis, rules out the unspanned risk premia. In this case, all the variation in yield is due to variation in the future short rates. On the other hand, time to build has been extensively studied in macroeconomics and is found to significantly affect business conditions (e.g., Kydland and Prescott, 1982); however, very few theories study its implications on asset pricing, especially on the term structure. In this paper, we fill this gap and examine to what extent time to build can help explain the return predictability in an otherwise standard CIR model.====Due to time to build, the output of an investment is not realized until several periods later. This implies that production depends not only on the current states as in the original CIR, but also on past business conditions over the period of production, making it more difficult for the agent in the economy to use investment to smooth consumption. When making investment and consumption decisions, the agent needs to account for all historical market states over the time to build; thus, her optimal consumption depends on the entire historical paths of state variables. This directly leads to a path-dependent stochastic discount factor (SDF).====Our model augmented with time to build helps explain a number of empirical regularities observed in bond markets. Ample evidence challenges the expectations hypothesis that postulates a constant risk premium and implies that bond risk premia are unpredictable (e.g., Campbell, Shiller, 1991, Cochrane, Piazzesi, 2005, Fama, Bliss, 1987, Ludvigson, Ng, 2009). Our model helps resolve the expectations puzzle that cannot be explained by the original CIR model. Indeed, time to build causes the short rate to mean-revert to a weighted average of both current and past production growth rates. As a result, production delay increases risks in the economy by accumulating uncertainty over the time to build. This leads to significant time variations in risk premia.====The path-dependent SDF causes equilibrium quantities, including the short rate, forward rates, and risk premia, to depend on historical paths of the state variables characterizing production. In this case, both current forward rates (or yields) and their historical paths contain information of future bond returns since current market states not only affect current short rate but also the short rates in a distant future. As a result, bond risk premia can be predicted by both current forward rates (or yields) and their lagged values. This is consistent with the evidence documented by Stambaugh (1988) and Cochrane and Piazzesi (2005) that lagged forward rates help predict bond excess returns, even in the presence of current forward rates. More importantly, different from the conventional wisdom, rational expectations do not eliminate return predictability by past information in our model. This is ==== due to the non-Markovian feature. Our paper in turn provides insight into this fundamental notion and highlights the role of non-Markovian asset pricing.====Due to the path dependence, to define bond prices in a continuous-time framework, we need to specify a continuously infinite number of historical values of the state variables (over the time to build). Thus, bond prices are infinitely dimensional. This directly implies that excess bond returns cannot be fully captured by linear combinations of yields. This is consistent with the considerable evidence that a part of excess bond returns cannot be explained by the principal components of yields (e.g., Bauer, Rudebusch, 2017, Cieslak, Povala, 2015, Cooper, Priestley, 2009, Joslin, Priebsch, Singleton, 2014, Ludvigson, Ng, 2009).====Our model also shows that macro factors help predict future bond returns. Due to the path dependence, more state variables, in addition to the short rate, are needed to capture the dynamics of equilibrium. As a result, bond prices depend not only on the short rate but also on the state variables reflecting current and historical business conditions. This is consistent with the findings of Ludvigson and Ng (2009) that macro factors have important forecasting power for the risk premia of U.S. government bonds, above and beyond the predictive power of forward rates and yield spreads. In contrast, in the original CIR model, the short rate captures all information of production (macro factors) and hence in itself constitutes a sufficient statistic of the production equilibrium. In this case, future bond risk premia are completely determined by the current short rate, implying that macro factors do not help predict bond returns in the presence of the short rate. More broadly, the evident predictability by macro factors also challenges standard affine models in which the predictability of bond returns is completely summarized by the cross-section of yields or forward rates. Our model that incorporates time to build into the single-factor affine model of the CIR allows predictability by macro factors.====Time to build causes production to depend on historical business conditions and leads to intractability in general. Indeed, time to build is studied by typically using discrete-time models (e.g., Kydland and Prescott, 1982). In this setting, different lengths of delay lead to distinctly different systems with different dimensions: A larger delay involves more state variables and hence increases the dimensionality of the systems. It is tremendously difficult to numerically solve a high dimensional system. Further, because these systems are mathematically characterized by different sets of state variables, they have to be analyzed separately even though their economic difference lies only in the lengths of time to build. In contrast, we study a continuous-time model in which the length of time delay is simply measured by a parameter and because of this, we provide a unified study of time to build.==== Our setup allows tractability.====Most equilibrium pricing models are Markovian, in which equilibrium dynamics are completely characterized by the state variables underlying the original setting. This paper adds to the asset pricing literature by studying ====. In our model, time to build causes the dynamics of equilibrium to depend on its entire path over production delay, which has infinite dimensions. Therefore, equilibrium cannot be simply spanned by the original state variables as in a Markovian setting. To the best of our knowledge, there is no known examples of exact characterizations of non-Markovian equilibrium. In this paper, we explicitly solve for the equilibrium with production delay by applying the piecewise dynamic programming approach developed recently by Li and Liu (2018).==== In the spirit of Li and Liu (2018), we further devise a novel method to handle the problem that there is no Itô’s lemma for stochastic delay differential equations that characterize our economy. Our method allows for closed-form solutions that provide clear characterizations of the non-Markovian dynamics and effectively isolate the impact of time to build.====Our paper contributes to the large literature of term structure models of interest rates. Statistical models of the SDF are found to successfully match many empirical facts, e.g., Duffie and Kan (1996) and Dai and Singleton (2002). However, these reduced-form models do not speak to the underlying economic mechanisms. The aim of this paper is not to develop a model to better fit the data (as those statistical models) but to provide explanations for the economic mechanism in generating the stylized facts in the bond market. We show that time to build helps explain simultaneously a number of empirical facts in a parsimonious production economy.====Some stylized facts in the bond market, e.g., the expectations puzzle, have long posed a challenge for general equilibrium models of the term structure. Backus et al. (1989) show that a model with a representative agent and time-varying expected consumption growth cannot account for the expectations puzzle in a pure exchange economy. Indeed, only a few papers provide economic mechanisms in explaining the puzzle. For example, Wachter (2006) finds that habit formation helps explain the expectations puzzle. In her model, during recessions, the agent is more risk averse, leading to a high intertemporal substitution (high short rate). As a result, the short rate relates negatively to the surplus consumption ratio, generating positive bond risk premia. Different from Wachter, the agent in our model has a constant relative risk aversion. Production delay increases risks in the economy by accumulating uncertainty over the time to build, generating significant time variations in bond risk premia.====The literature also shows that learning (either Bayesian or non-Bayesian) can generate time-varying risk premia and return predictability, e.g., Timmermann (1993), Sargent (1999), Chakraborty and Evans (2008), Cho and Kasa (2017), and Li and Liu (2019a), amongst many others. There are two important differences between the dynamic equilibrium models with learning and our model. First, different from the learning models that focus on unobservable economic forces, in this paper, we study time to build that has been widely documented to radically affect business conditions but of which the effect on the term structure has not been studied. The agent in our economy observes the production process and is informed about the time to build; thus, there is no learning in our model. Second, and more importantly, time to build naturally leads to non-Markovian dynamics and we show that the non-Markovian feature plays the key role in generating the stylized facts in our model. In contrast, most learning papers study Markovian models, in which past information plays no role in price dynamics given all state variables.====Our paper also adds to the literature of production-based asset pricing models. Subsequent to Cox et al. (1985a), production economy has been used to study both the aggregate market and the cross section of asset returns, e.g., Jermann (1998), Gomes et al. (2003), Kaltenbrunner and Lochstoer (2010), and Croce (2014).==== These models study the effects of investment frictions (e.g., capital adjustment costs, investment irreversibility, and capital immobility) and stochastic productivity shocks on equity returns. Only a few papers study asset pricing implications of time to build, e.g., Boldrin et al. (2001) and Chen (2016). Chen (2016) is closely related to this paper. Chen considers both delays in transforming new investment into productive capital and making final outputs from productive capital. He shows that time to build helps generate a sizable equity premium, fit investment regressions, and explain the lead-lag patterns between asset prices and macroeconomic quantities. But Chen doesn’t explore the bond yields. This paper fills this gap in the literature.====The remainder of the paper is organized as follows. Section 2 presents some motivating regression evidence that historical information helps predict bond risk premia. Section 3 develops a CIR model with time to build and Section 4 solves for equilibrium. Section 5 examines model implications. Section 6 provides further empirical evidence of the failure of Markovian models and Section 7 concludes. Appendix provides the proofs.",Time to build and bond risk premia,https://www.sciencedirect.com/science/article/pii/S0165188920301925,28 October 2020,2020,Research Article,279.0
Kim Jiseob,"School of Economics, Yonsei University, 50 Yonsei-ro, Seodaemun-gu Seoul 03722, Republic of Korea","Received 13 April 2020, Revised 18 October 2020, Accepted 20 October 2020, Available online 27 October 2020, Version of Record 13 November 2020.",https://doi.org/10.1016/j.jedc.2020.104021,Cited by (1),I examine how the ,"There are several ways of extracting home equity through mortgage finance. Among them, this paper focuses on the mortgage refinance and the line of credit secured by home equity, which is called the home equity lines of credit (HELOC). During the housing boom, a significant number of households refinanced their loans and used HELOC, which could possibly have increased the mortgage debt before the crash and deepened the foreclosure crisis. In this paper, I quantitatively examine how refinancing and HELOC impact the increase in pre-crisis mortgage debt and post-crisis mortgage defaults. I then propose effective mortgage finance directions to mitigate mortgage defaults after the crisis.====HELOC is a secured junior loan where lenders agree to lend a maximum amount within an agreed period, collateralized by the borrower’s home equity. It works like a credit card loan where households can use the line of credit to borrow funds. Since HELOC is a second mortgage where the lender has the subordinate right to recover losses when the borrower defaults, its interest rate is usually higher than that for the conventional mortgage. HELOC has two phases: the draw period and the repayment period. During the draw period, which usually lasts for 10 years, homeowners can freely accumulate or decumulate loans by repaying interest only. After the draw period, the loan enters repayment, where the household cannot borrow additional funds and must repay both the principal and interest. The repayment period typically lasts for 20 years. Unlike the conventional (first) mortgage that is mainly used for buying homes, HELOC is widely used to borrow relatively small amounts to be repaid quickly for the purpose of home improvements and maintenance, living expenses, and personal loans.====Instead of using HELOC, households can also relax their budget tightness by refinancing the loan. Through refinancing, households can replace the original mortgage contract with a new one and cash out additional funds.==== Since the refinanced mortgage is the first loan, the borrowing interest rate is generally lower than that of HELOC. Though a significant origination cost is incurred, households that are not eligible to use HELOC refinance their mortgages. Therefore, both HELOC and (cash-out) refinancing provide means to smooth out disposable income and thus meet mortgage payments and avoid defaults.====On the other hand, both mortgage finance tools can also increase the mortgage default risk. Homeowners who temporarily face adverse income or consumption shocks are more likely to use such mortgage products. This in turn leads to higher repayment obligations in the future. In addition, households tend to save less financial assets by leaning on either refinancing or HELOC and thus are more vulnerable to adverse house price shocks.====In this paper, I examine the roles of refinancing and HELOC in improving households’ liquidity, accumulating household debt during the housing boom, and amplifying mortgage defaults during the financial crisis. To analyze this, I introduce a heterogeneous agent model where these mechanisms are embedded. When a renter decides to buy a house, she chooses the house size, takes out the long-term mortgage, and moves into the owner-occupied home. If the home value is higher than the outstanding mortgage, she is eligible to take out HELOC up to the home equity. When the household refinances loans, it repays the outstanding balance of loans, borrows again with new contract terms, and is allowed to switch the size of its home. The homeowner can also choose to sell the house or default on loans. When the household sells the house, it pays the remaining loan balance including the transaction cost. Mortgage defaulters cannot access the loan market and buy a new house for several periods as a default penalty. The mortgage and the HELOC interest rates are endogenously determined, reflecting the borrower’s default risk and the loan seniority. Specifically, financial intermediaries observe households’ optimal behavior, calculate the expected profit by making a household-by-household contract, and competitively decide the price of loans.====I calibrate the steady-state model to match the early 2000s US economy. I then consider the benchmark transition that reflects the pre- and post-financial crisis. In 2001, households expect that the average house price will increase year-by-year until 2007 and then remain steady. The rent-to-price ratio is also expected to change following the data path. Their ex-ante expectation and the ex-post realization of the house price and the rent-to-price ratio all coincide during the housing boom. At the start of 2007, the average house price unexpectedly declines for four consecutive years, mirroring the financial crisis. Consistent with data, the rent-to-price ratio simultaneously changes after 2007. Over the transition path, I adjust the cost of refinancing loans to match the actual refinance rate. Given this benchmark transition, I consider counterfactual transitions where the costs of refinancing or accessing HELOC increase over the transition path. By comparing the benchmark transition to experiment economies, I can analyze the macroeconomic implications of these mortgage finance tools. In addition, I examine effective policy directions for reducing post-crisis mortgage defaults by adjusting accessibility to refinancing or HELOC.====Main quantitative results are as follows. First, refinancing provides liquidity to financially troubled households but it simultaneously builds debt. Thus, in a downturn episode it contributes to an increase in the foreclosure rate. When the pre- and post-crisis refinancing cost exogenously increases — which halves the refinance rate compared to the benchmark over the transition — the aggregate mortgage debt decreases by 6.2 percentage points during the housing boom. This in turn leads to a decrease in the cumulative foreclosure rate between 2007 and 2010 by 1.3 percentage points. Hence, more debt does not solve debt problems.====Second, households save more when they know they will not have access to liquidity support from refinancing or HELOC. In other words, as the costs of refinancing or HELOC increase, it becomes expensive to cash out funds secured by home equity, which incentivizes households to increase financial assets. In turn, the foreclosure rate after the financial crisis under these experiment economies increases less than the benchmark. Therefore, policies to help ex-post can induce higher financial fragility ex-ante.====Third, low-income households relieve their budget tightness through HELOC, rather than through the cash-out refinance. In addition, the borrowing size through HELOC is smaller than that through the cash-out refinance. In turn, households that use HELOC less increase both consumption and future payment burdens than those using the refinance. This leads to a decrease in their marginal propensity to consume (MPC), while the MPC for households that refinance increases.====Fourth, HELOC, which is relatively small in its size, cannot significantly impact the increase in total household debt during the housing boom. However, households accumulate less financial assets and are less likely to take out long-term conventional mortgages. This in turn prevents them from spreading out the mortgage payment burden. Thus, HELOC contributes to an increase in the foreclosure rate in face of the adverse house price shock.====Lastly, limited access to HELOC after the crisis can effectively reduce the post-crisis foreclosure rate. However, adjusting refinance costs or improved access to HELOC negligibly impact the post-crisis foreclosure rate. Though an increase in the HELOC cost constrains financially troubled households more, it makes them to switch HELOC to the long-term mortgage. In turn, they are less vulnerable to consecutive decreases in the house price.====This paper is organized as follows. Section 2 reviews related papers. Section 3 presents the empirical motivation of this research. I introduce a quantitative model in Section 4 and present the calibration in Section 5. I analyze the benchmark steady state in Section 6. Subsequently, in Section 7, I present the main quantitative results. Section 8 concludes the paper.",Macroeconomic effects of the mortgage refinance and the home equity lines of credit,https://www.sciencedirect.com/science/article/pii/S0165188920301895,27 October 2020,2020,Research Article,280.0
"Mazzarisi Piero,Zaoli Silvia,Campajola Carlo,Lillo Fabrizio","Department of Mathematics, University of Bologna, Italy,Scuola Normale Superiore, Piazza dei Cavalieri, 7, Pisa (PI) 56126, Italy,Quantitative Life Science Section, The Abdus Salam International Center for Theoretical Physics (ICTP), Trieste, Italy,University of Zürich, Rämistrasse 71, 8006 Zürich (Switzerland)","Received 5 June 2020, Revised 14 September 2020, Accepted 21 October 2020, Available online 24 October 2020, Version of Record 16 November 2020.",https://doi.org/10.1016/j.jedc.2020.104022,Cited by (16),"Identifying risk ==== in financial markets is of great importance for assessing ==== takes place, and it is flexible enough for multivariate extension when more than two time series are considered in order to decrease false detections as spurious effect of neglected variables. An extensive simulation study shows the performances of the proposed method with a large variety of data generating processes and it introduces also the comparison with the test of Granger causality in tail by Hong et al. (2009). We report both advantages and drawbacks of the different approaches, pointing out some crucial aspects related to the false detections of Granger causality for tail events. An empirical application to high frequency data of a portfolio of US stocks highlights the merits of our novel approach.","The problem of inferring causal interactions from data has a remarkable history in scientific research and the milestone work of (Granger, 1969) represented the turning point in such study. According to Granger causality, given a couple of time series ==== and ==== it is said that ==== ‘Granger-causes’ ==== if the past information on ==== helps in forecasting ==== better than using only the past information on ====. Granger causality overcomes the philosophical question of properly defining what ‘true causality’ means, by limiting the study to systems whose state can be assessed quantitatively by means of time series and relying on the concept of ‘predictive causality’ (Granger, 1980). Within this framework, Granger causality is pragmatic, well defined, and has exhibited many successful applications in a variety of fields, from quantitative finance (Billio, Getmansky, Lo, Pelizzon, 2012, Corsi, Lillo, Pirino, Trapin, 2018) to transportations (Zanin et al., 2017) and neuroscience (Seth et al., 2015).====In time series econometrics, the most commonly used test of Granger causality for bivariate systems is the F-test originally proposed in Granger (1969). This test is sometimes referred to as causality ==== since the statistical testing procedure is based on the evaluation of the forecasting performances associated with the first moment of a time series. The most stringent assumption consists in considering the information on the two time series as all the ==== information when testing for Granger causality. This is a strong assumption because, in the case of a high dimensional system, a low dimensional subprocess contains little information about the true structure of interactions and some causal relations might be falsely detected as spurious effect of neglected variables (Lütkepohl, 1982). Starting with the seminal work of Geweke (1982), multivariate approaches have been proposed to correct these spurious effects by taking into account network effects in the statistical testing procedure. Finally, Granger causality has been investigated from a theoretical point of view moving from Econometrics to Information Theory and, in particular, the equivalence with transfer entropy was proved in the Gaussian case (Barnett et al., 2009), which is in turn equivalent to the log-likelihood ratio statistic (Barnett and Bossomaier, 2012). In practice, this implies, among other things, that the Likelihood-Ratio test is supported as statistical method for inferring Granger causality. The advantage relies on less stringent hypotheses about the statistical distributions with respect to the F-test.====Granger causality proved useful in monitoring systemic risk in financial markets. In fact, recent applications showed that it captures the network of risk propagation between market participants, and the degree of interconnectedness of this network can be used to define indicators of systemic risk. For instance, Billio et al. (2012) have adopted Granger causality ==== as a proxy for return-spillover effects among hedge funds, banks, broker/dealers, and insurance companies (==== see Battiston, Gatti, Gallegati, Greenwald, Stiglitz, 2012, Danielsson, Shin, Zigrand, 2012, Duarte, Eisenbach, 2014 for theoretical studies on spillover effects during financial crises), showing that the financial system has become considerably more interconnected before the financial crisis of 2007–2008 because financial innovations and deregulation had increased the interdependence of business between such investors.====The original Granger causality test evaluates the forecasting performance giving equal importance to the ability to forecast average or extreme values, negative or positive ones. However, when monitoring financial risk, extreme downside market movements are much more important than small fluctuations for spillover effects. A method specific for risk measures, in particular volatility, was introduced by Cheung and Ng (1996) with the concept of Granger causality ====, by extending the concept of causation to the second moment. Nevertheless, variance is a two-sided risk measure and it is not able to capture heavy tails, thus the causal relations between extreme events of two time series. To this end, Granger causality ==== can be defined. The concept was firstly introduced by Hong et al. (2009). In this work, the authors have proposed a kernel-based test to detect extreme downside risk spillovers with a statistical procedure in two steps: (i) measuring risk by the left (or right, depending on the application) tail of the distribution, ==== Value at Risk, and (ii) testing for non-zero lagged cross-correlations between the two binary time series representing the occurrences of extreme left (right) tail events, with a method based on spectral analysis. Based on this test, ====, Corsi et al. (2018) have studied the network of causal relations detected by Granger causality in tail for a bipartite financial system of banks and sovereign bonds and, combining measures of network connectedness with the ratings of the sovereign bonds, proposed a flight-to-quality indicator to identify periods of turbulence in the market. However, as we show below, the statistical test of Granger causality in tail by Hong et al. (2009) displays some sensitivity to both non-zero auto-correlation and instantaneous cross-correlation in the binary time series representing extreme events, resulting in an increased rate of false causality detections. Additionally, the test by Hong et al. (2009) is by construction a pairwise causality analysis, thus sensitive to false detections when variables of some importance, different from the two under investigation, are not considered.====In this paper we propose a different approach to identify Granger causality in tail, which overcomes some of the issues of the Hong et al. method. Differently from the latter, our approach is parametric and it explicitly models causality interactions between time series. Thus, it is less sensitive to other possible effects, such as autocorrelation, which may result in spurious detections as mentioned above, and can be generalized to multivariate settings. Moreover, having an explicit model of the causation process allows us to devise a method based on Likelihood-Ratio to test for Granger causality in tail. Specifically, the causation mechanism is captured by a process in which the extreme events are modeled according to a ==== process of order ==== namely DAR(p). The DAR(p) process, first introduced by Jacobs and Lewis (1978), is the natural extension of the standard autoregressive process for binary time series. We consider a multivariate generalization of the DAR(p) process, namely VDAR(p), where the binary variable ==== describing the occurrence of an extreme event for the underlying time series ==== can be copied from its past values or from the past values of the extreme events of another time series ====. The choice of this specific model is motivated by the fact that the Markov chain associated with the vector discrete autoregressive process of order one (VDAR(1)) can be interpreted as the maximum entropy distribution of binary random variables with given means, lag one auto-correlations, and lag one cross-correlations, as recently proved in Campajola et al. (2020a). Moreover, the same argument holds for the vector discrete autoregressive process of generic order, by noticing that a Markov chain of order ==== for ==== variables can be seen equivalently as a Markov chain of order one for ==== variables, under appropriate conditions for the transition matrix. By the principle of maximum entropy then the VDAR(p) model is the first candidate in building a parametric method to test for non-zero lagged cross-correlations (of binary time series) as signal of Granger causality in tail.====We first propose a statistical test based on the bivariate version of the model, for any ====. Then, to overcome the limit of pairwise analysis highlighted above, we also propose a statistical method for the multivariate case, with ==== (Markovian dynamics).====Our findings show that the detection of causality between extreme events is far from a trivial task, with a significant dependence on the adopted statistical procedure. In fact, we show that the proposed method and the current standard in literature represented by the test of Hong et al. (2009) differ under some circumstances. First, as mentioned above, the test by Hong et al. displays some sensitivity to auto-correlation and cross-correlation of the time series of extreme events, which are, on the contrary, naturally accounted for by our framework. We show numerically concrete examples of such behavior and the spurious effect of two-way causality detection in the presence of unidirectional relations, a drawback of the Hong et al.’s test which is solved by our method. As a consequence, we claim that our method should be preferable in such cases. At the same time, we present also cases when the approach by Hong et al. outperforms ours, for instance when the underlying dynamics of the time series is autoregressive and heteroskedastic, even if the discrepancy is typically quite small. In fact, in the case of model misspecification, a non-parametric approach should be preferable to a parametric method. Nevertheless, we show numerically also cases when the latter works better, even for misspecified data generating process. Second, we point out the importance of a multivariate approach, numerically showing the consequences of network effects in the spurious detection of Granger causality in tail relations when adopting a pairwise analysis. We then propose a possible solution within our framework. Finally, in an empirical application to high-frequency price returns of a portfolio of stocks traded in the US stock markets, we highlight that different methods yield different networks of causal interactions between stocks. First, while Hong et al.’s approach results in an almost complete graph, both the pairwise and multivariate versions of the statistical method we propose give much sparser networks. Hence, the measure of the level of causality in the system dynamics depends on the chosen approach. Additionally, the captured dynamics of the network evolution itself is quite different, in particular in relation to the presence of the financial crisis of 2007–2008: no patterns are recognized by using the test of Hong et al., while, with our method, a sharp transition characterizes the number of causal interactions between the stocks composing the financial sector and the others. In particular, we find that the financial sector started to be less ‘Granger-caused’ by the other stocks before the financial crisis, but it ‘Granger-causes’ more than the average over all the considered period. Thus, our findings open a discussion about the correct evaluation of a Granger causality relation between tail events and, in this paper, we highlight both advantages and drawbacks of the different approaches together with some signals associated with false detections.====The paper is organized as follows. Section 1 presents the general definition of Granger causality in tail and explains how to obtain the sequence of extreme events from data. We also discuss the importance of the multivariate approach to avoid false detections because of network effects. Section 2 introduces the novel methodology and describes how to construct the test statistics. Section 3 presents some Monte Carlo exercises to validate numerically the novel approach and to compare it with the test by Hong et al. (2009). Section 4 shows a financial application of the method. Finally, Section 5 concludes. Technical details are reported in Appendix A.",Tail Granger causalities and where to find them: Extreme risk spillovers vs spurious linkages,https://www.sciencedirect.com/science/article/pii/S0165188920301901,24 October 2020,2020,Research Article,281.0
"Candian Giacomo,Dmitriev Mikhail","Department of Applied Economics, 3000 chemin de la Côte-Sainte-Catherine, Montréal, QC H3T 2A7, Canada,Department of Economics, Florida State University, 255 Bellamy Building, Tallahassee 32306-2180, FL, USA","Received 26 May 2020, Accepted 6 October 2020, Available online 22 October 2020, Version of Record 3 November 2020.",https://doi.org/10.1016/j.jedc.2020.104011,Cited by (4), and leads to novel policy implications about the effectiveness of subsidies for liquidated assets.,"Default recovery rates for corporate bank loans in the United States are highly volatile and procyclical, ranging from 5% to 35% over the last 30 years. Models of financial frictions based on the costly enforcement approach à la (Kiyotaki and Moore, 1997) make no predictions on recovery rates, as they abstract from defaults in equilibrium. The literature that instead follows the costly state verification approach of Bernanke et al. (1999) focused mostly on the dynamics of spreads and defaults, putting little emphasis on the models’ predictions for the recovery rates associated with such defaults. We show that recovery rates in this class of models are almost flat over the cycle and rarely move by more than 2% from their average value. This finding suggests that the current framework understates the severity of financial frictions. We propose a mechanism that helps reconcile theory and empirics of recovery rates, and explore its consequences for business cycles.====The volatility and cyclicality of default recovery rates have an intuitive explanation. As pointed out by Shleifer and Vishny (1992), during a downturn it is harder for a bank to sell the assets seized from a firm in financial distress. First, the most productive use of these firm-specific assets would be exercised by similar businesses, which are likely to experience comparable financial difficulties. Second, in times of recession, other financial institutions are trying to sell similar assets due to widespread bankruptcies. For these reasons, foreclosed assets are sold at prices below their value in best use, and default recovery rates deteriorate sharply during economic downturns. In standard models of financial frictions, however, these channels are absent, as physical capital in default is assumed to be perfectly redeployable, and is therefore traded at the same price as new capital.====This paper contributes to the literature by developing and bringing to the data a theory of countercyclical liquidation costs based on Shleifer and Vishny’s (1992) idea of limited redeployability of capital. We formalize this notion in the context of a dynamic stochastic general equilibrium (DSGE) model that is closely related to BGG but that differs in the micro-foundations of the costs associated with defaults. In our model, defaults are costly because they entail the redeployment of foreclosed capital units that are firm specific. Non-defaulting entrepreneurs face idiosyncratic costs of redeploying this capital, capturing the idea that the most talented entrepreneurs are better managers of liquidated assets and can thus redeploy them more easily. At the same time, financial conditions play a crucial role because, when the best entrepreneurs are strapped for cash, the foreclosed capital may go to managers with a lower valuation of the asset. Thus, the distribution of wealth among non-defaulting entrepreneurs matters, because it endogenously determines the price of foreclosed assets. In recessions, when financial constraints are tight for all entrepreneurs, marginal liquidation costs rise because assets in default find a relatively worse alternative use and are traded at lower prices relative to new capital. The rise in liquidation costs results in a fall in recovery rates and an increase in the spread on borrowing rates, as creditors try to recoup their losses. Our mechanism thus endogenizes changes in bankruptcy costs or other costs of financial intermediation, which were identified by previous research as key drivers of business cycle fluctuations (Ajello, 2016, Christiano, Eichenbaum, Trabandt, 2015).====We embed this mechanism in a medium-scale New Keynesian DSGE model that we estimate using Bayesian techniques on U.S. macroeconomic and financial data to assess the quantitative relevance of our mechanism. As an original contribution to the literature that estimates DSGE models with financial frictions, we include the data on aggregate recovery rates together with more traditional evidence on corporate bonds spreads as observables in the estimation. We find that our framework can explain a considerable portion of fluctuations in recovery rates, including their sharp contraction at the onset of the Great Recession. Moreover, the presence of our mechanism improves the empirical fit relative to the state-of-the-art financial accelerator model. While the movements in recovery rates was the original motivation for developing our mechanism, we demonstrate that the fit to the data improves even when recovery rates are not used in the estimation. We show that the key ingredient behind these results is the interaction of our endogenous liquidation costs and financial disturbances, which we model as shocks to the net worth of entrepreneurs.====The intuition is as follows. The presence of endogenous liquidation costs significantly amplifies the effect of financial and monetary shocks on output and other key macro variables beyond the standard financial accelerator effect. When an adverse shock hits the economy, not only do markups go up and balance sheets deteriorate, but the liquidation value of assets in default plummets, as the marginal buyer of these assets is a less efficient one than in normal times. As a result, banks become more reluctant to lend to ==== entrepreneurs even if the latter have strong balance sheets, since, for the same probability of default, the potential recovery rate for the bank is now lower. Our mechanism thus delivers larger movements in spreads than traditional models for a given increase in the probability of default. We show that this amplified response of the spread allows the model to better reproduce the relative volatility of financial and macroeconomic variables, thus improving the overall empirical fit. In this way, we advance a previous literature that found exogenous changes in liquidation or bankruptcy costs to be essential to explain spreads dynamics (Fuentes-Albero, 2019, Levin, Natalucci, Zakrajsek) by providing an endogenous explanation for their fluctuations.====As a normative contribution, we show that policymakers might reduce the effect of fire sales of assets in default by subsidizing liquidated capital. In our model, the subsidy can be directed to the supply side of liquidated assets, represented by financial intermediaries, or to non-defaulting entrepreneurs on the demand side. If the market for liquidated capital were frictionless, the two subsidies would lead to the same allocation. This is not the case in our model because entrepreneurs are financially constrained. Paying the subsidy to entrepreneurs ex-post allows less efficient ones to buy assets in default. The resulting upward pressure on the price of liquidated capital ex-ante reduces the purchasing power of the most efficient entrepreneurs, mitigating the overall effect of the subsidy. Conversely, when the subsidy is directed to financial intermediaries, it directly increases their recovery value from defaults, thereby allowing them to charge lower interest rates on existing debt obligations. Lower debt repayments increase the wealth of non-defaulting entrepreneurs, allowing the most efficient ones to redeploy a larger share of foreclosed assets. The resulting lower liquidation costs generate a smaller deadweight loss and a stronger stabilizing effect on the economy. In other words, the subsidy to the banks is more effective because, instead of targeting the marginal buyer, it indirectly provides extra liquidity to the wealthiest entrepreneurs.==== Our paper is at the intersection of macroeconomics and finance, bridging two strands of the literature that have so far been relatively disconnected. There is a small empirical literature that has documented the relationship between the recovery rates at the aggregate (Mora, 2012) and industry level (Acharya et al., 2007). The finance literature has analyzed the joint behavior of defaults and recovery rates with credit risk or value-at-risk models (see Altman et al., 2005) or with more agnostic econometric models (Bruche and González-Aguado, 2010). We contribute to this literature by linking defaults and recoveries to macroeconomic fundamentals with a medium-scale New Keynesian general equilibrium model.====A large literature uses estimated DSGE models to study business cycle fluctuations (e.g., Justiniano, Primiceri, Tambalotti, 2010, Justiniano, Primiceri, Tambalotti, 2010, Del Negro, Giannoni, Schorfheide, 2015, Del Negro, Giannoni, Schorfheide, 2015). A common finding in this literature is that financial frictions play an important role in U.S. business cycle dynamics, and that financial shocks are essential to explain the Great Recession (Christiano, Motto, Rostagno, 2014, Christiano, Eichenbaum, Trabandt, 2015). When firms make price decisions in customer markets, models with financial frictions can also account for inflation dynamics during the recent financial crisis (Gilchrist et al., 2017). We propose to look at the dynamics of default recovery rates as additional piece of evidence that help quantify the importance of such financial aspects.====Ajello (2016) builds a model where entrepreneurs finance investment by trading financial claims via a banking sector, which is subject to shocks to its financial intermediation technology. When the model is taken to the data, these shocks are shown to explain 25% of GDP and 30% of investment volatility for U.S. data prior to the Great Recession. Our novelty is that that we treat intermediation costs as an endogenous object that depends on the developments of the market for liquidated assets. The endogenous movement in intermediation costs amplifies the effect of other financial shocks and leads to novel policy implications. One example of such implications is the differential effect of subsidies to the demand and supply side of assets in default.",Default recovery rates and aggregate fluctuations,https://www.sciencedirect.com/science/article/pii/S0165188920301792,22 October 2020,2020,Research Article,282.0
"Serletis Apostolos,Xu Libo","Department of Economics, University of Calgary, Canada,Department of Economics, University of San Francisco, U.S.A.","Received 11 October 2019, Revised 24 May 2020, Accepted 22 September 2020, Available online 16 October 2020, Version of Record 29 October 2020.",https://doi.org/10.1016/j.jedc.2020.103994,Cited by (9)," by investigating whether the ML and NQ functional monetary aggregates are of importance in resolving paradoxes associated with the measurement of money. In doing so, we also provide a comparison between the functional monetary aggregates and the Fed’s (broad) Sum M2 aggregate and the Center for Financial Stability (broad) Divisia M3 and Divisia M4 aggregates. Our detailed statistical analysis favors the functional monetary aggregates.","Currently, the common practice among central banks is to use ‘simple-sum’ aggregation to construct money measures from a list of possible components that are considered to be the likely sources of monetary services, as follows====where ==== is the monetary aggregate and ==== is one of the ==== monetary components of the monetary aggregate. However, (Friedman and Schwartz (1970), p. 151–152) dismissed simple-sum monetary aggregates, arguing that “this (summation) procedure is a very special case of the more general approach. In brief, the general approach consists of regarding each asset as a joint product having different degrees of ====, and defining the quantity of money as the weighted sum of the aggregate value of all assets, the weights for individual assets varying from zero to unity with a weight of unity assigned to that asset or assets regarded as having the largest quantity of ‘moneyness’ per dollar of aggregate value. The procedure we have followed implies that all weights are either zero or unity.”====Over the years, there have been many attempts at properly weighting the monetary components within a simple-sum aggregate, but without theory, any weighting scheme is questionable. Barnett (1980) argued instead for applying aggregation theory and statistical index number theory to monetary aggregation. He also argued [see (Barnett, 1980, p. 12)] that “[w]hile aggregation theory results in exact aggregator functions depending upon unknown (but estimable) parameters, statistical index number theory results in parameter-free approximations to aggregator functions. Index number theory provides the basis for the index numbers published by nearly every governmental agency in the world (other than the central banks).” In this regard, statistical index number theory provides a class of quantity and price indexes that can be computed from price and quantity data alone, thus eliminating the need to estimate an underlying structure==== Statistical indexes are mainly characterized by their statistical properties. These properties were examined in great detail by Fisher (1922) and serve as tests in assessing the quality of statistical indexes. While Fisher (1922) found the simple-sum index to be the worst known index number formula, the index that he found to be the best has now become known as the Fisher ideal index, which is the geometric average of the Laspeyres and Paasche indexes. Another index found to possess a very large number of such properties is the (Törnqvist) discrete time approximation to the continuous Divisia (1925) index; Divisia (1925) proposed the continuous time index for aggregating over goods.====Barnett, 1978, Barnett, 1980 proved how the Divisia approach to aggregation could be extended to include monetary assets and constructed monetary quantity indexes, now known as Divisia monetary aggregates. The Divisia index (in discrete time) is defined by====according to which the growth rate of the aggregate is the weighted average of the growth rates of the component quantities, with the weights being defined as the expenditure shares averaged over the two periods of the change, ==== for ==== where ==== is the expenditure share of asset ==== during period ==== and ==== is the user cost of asset ==== derived in Barnett (1978)==== which is just the opportunity cost of holding a dollar’s worth of the ====th asset. In the equation above, ==== is the market yield on the ==== th asset and ==== is the yield available on a ‘benchmark’ asset that is held only to carry wealth between multiperiods — see Barnett et al. (1992), Barnett and Serletis (2000), or Barnett (2012) for more details regarding the Divisia approach to monetary aggregation. Barnett has also extended the field of index number theory to include risk in Barnett, 1995, Barnett, 1997, and Barnett and Wu (2005). He extended index number theory to multilateral international financial aggregation in Barnett (2007), for multicountry economic unions. More recently, Barnett et al. (2016) further extended the Divisia monetary aggregates to the credit card-augmented Divisia monetary aggregates, which jointly account for the liquidity services provided by monetary assets and credit cards.====Over the years a large number of articles have shown that the use of the Divisia monetary aggregates can solve the “Barnett critique” — the measurement problems associated with the failure to find significant relations between money and key macroeconomic variables. See, for example, Barnett and Chauvet (2011), Hendrickson (2014), Serletis and Gogas (2014), Belongia and Ireland (2014), Belongia and Ireland (2015), Belongia, Ireland, 2016, Belongia, Ireland, 2018, Ellington (2018), Dai and Serletis (2020), and Dery and Serletis (2020), among others. In fact, (Belongia and Ireland (2015), p. 268) “call into question the conventional view that the stance of monetary policy can be described with exclusive reference to its effects on interest rates and without consideration of simultaneous movements in the monetary aggregates.” They argue that properly measured monetary aggregates, such as the new Center for Financial Stability (CFS) Divisia monetary aggregates, can and should play an important role (either as intermediate targets or indicator variables) for the conduct of monetary policy, in addition to that of the short-term nominal interest rate.====The fields of aggregation theory and statistical index number theory developed independently. However, Diewert (1976) provided the link between economic and aggregation theory and statistical index number theory by attaching economic properties to statistical indexes. These properties are defined in terms of the ability of a statistical index to approximate a particular functional form for the unknown underlying aggregator function. In fact, for a number of well known statistical indexes Diewert (1976) shows that they are equivalent to the use of a particular functional form. Such statistical indexes are called ‘exact.’ Exactness, however is not sufficient for acceptability of a particular statistical index when the functional form for the aggregator function is not known. In this case it seems desirable to choose a statistical index which is exact for a flexible functional form. Diewert termed such statistical indexes ‘superlative.’ Diewert also showed that the Divisia index is exact for the linearly homogeneous translog flexible functional form, and is, therefore, superlative.====The translog flexible functional form, introduced by Christensen et al. (1975), is a locally flexible functional form (a second-order local approximation to an arbitrary function), and as Caves and Christensen (1980) and Barnett and Lee (1985) have shown the regularity regions of locally flexible functional forms can be relatively small. However, over the years, an increasingly sophisticated literature has been under development on flexible functional forms. There is now a large number of other locally flexible functional forms, including the generalized Leontief (GL), introduced by Diewert (1973), and the Almost Ideal Demand System (AIDS), introduced by Deaton and Muellbauer (1980). There are also the effectively globally regular minflex Laurent (ML) models, which are based on the Laurent series expansion and were introduced by Barnett (1983) and Barnett and Lee (1985), the quadratic AIDS (QUAIDS) model of Banks et al. (1997), the general exponential form (GEF) of Cooper and McLaren (1996), and the normalized quadratic (NQ) models, introduced by Diewert and Wales (1988). Also, the globally flexible Fourier and Asymptotically Ideal Model (AIM) models, introduced by Gallant (1981) and Barnett and Jonas (1983), respectively, could be used.====This raises interesting methodological questions. If the Divisia index is exact to the linearly homogeneous translog flexible functional form, which has a relatively small regular region, can we use a statistical index from the statistical index number literature that is exact to a flexible functional form that has a larger regularity region? We are not aware of statistical indexes that can be shown to be exact to the effectively globally regular flexible functional forms or the globally flexible functional forms. For this reason, in this paper we build on a large body of literature, which Barnett (1997) calls the ‘high road’ literature, and take a microeconomic- and aggregation-theoretic approach to the demand for monetary assets and monetary aggregation. The approach that we take allows the estimation in a systems context assuming a flexible functional form for the aggregator function, based on the dual approach to demand system generation developed by Diewert (1974). We estimate two popular, effectively globally regular flexible functional forms, the minflex Laurent and normalized quadratic, to produce the ML and NQ functional monetary aggregates. We do so, in the context of highly disaggregated demand systems, encompassing the full range of monetary assets, unlike earlier work in this area that has generally been carried out in the context of small, highly aggregated demand systems.====In doing so, we pay explicit attention to theoretical regularity, since the usefulness of flexible functional forms depends on whether they satisfy the theoretical regularity conditions of positivity, monotonicity, and curvature, and in the older monetary demand systems literature there has been a tendency to ignore regularity. In fact, as (Barnett (2002), p. 199) put it in his ==== Fellow’s opinion article, without satisfaction of all three theoretical regularity conditions “... the second-order conditions for optimizing behavior fail, and duality theory fails. The resulting first-order conditions, demand functions, and supply functions become invalid.” Motivated by these considerations, we treat the curvature property as a maintained hypothesis in order to produce functional monetary aggregates consistent with neoclassical microeconomic theory and aggregation theory. We argue that unless regularity is attained by luck, flexible functional forms should always be estimated subject to regularity, as suggested by Barnett (2002).====We also address the issue of optimal monetary aggregation by (assuming that money has positive value in equilibrium, implying the existence of a monetary services aggregator function and) estimating large demand systems, encompassing the full range of monetary assets, to produce broad functional monetary aggregates. In this regard, Jadidzadeh and Serletis (2019) also address the issue of optimal monetary aggregation in the context of a large demand system. They provide evidence, based on disaggregated monetary demand responses, that the simple-sum monetary aggregates used by central banks around the world are inconsistent with neoclassical microeconomic theory. Their statistical tests also reject the necessary and sufficient conditions for all the money measures published by the Federal Reserve as well as a large set of null hypotheses that would be consistent with the existence of subaggregates of various subsets of liquid assets. Their tests support and reinforce (Barnett, 2016) assertion that we should use, as a measure of money, the broadest Divisia M4 monetary aggregate prepared by the Center for Financial Stability.====Finally, we highlight the influence of measurement on statistical inference by investigating whether the ML and NQ (broad) functional monetary aggregates are of importance in resolving paradoxes associated with the measurement of money, in solving the Barnett critique, and in understanding the effects of potential monetary policy actions. We do so in the context of two dynamic general equilibrium monetary business models, the Ireland (2004) and Andrés et al. (2006) models, both of which find a minimal role of money in business cycle analysis. We also test for Granger causality from the ML and NQ functional monetary aggregates to industrial production, using the conventional VAR approach as well as the Psaradakis et al. (2005) Markov-switching model with time-varying parameters. We also provide a comprehensive comparison between the functional aggregates, ML and NQ, and the Fed’s Sum M2 aggregate and the CFS broad Divisia M3 and Divisia M4 aggregates.====The rest of the paper is organized as follows. Sections 2 and 3 briefly sketch related neoclassical demand theory and aggregation theory. Section 4 presents the ML and NQ demand systems and discusses related econometric issues, paying explicit attention to the singularity problem and the imposition of global concavity. Section 5 discusses the data and presents the broad ML and NQ functional monetary aggregates. Section 6 presents the first group of empirical results and Section 7 presents the Granger causality test results. The final section concludes regarding the implications of our research for monetary theory, the conduct of monetary policy, and business cycle analysis.","Functional monetary aggregates, monetary policy, and business cycles",https://www.sciencedirect.com/science/article/pii/S0165188920301627,16 October 2020,2020,Research Article,283.0
McClung Nigel,"Research Unit, Bank of Finland, P.O. Box 160 Snellmaninaukio, Helsinki FI 00101, Finland","Received 12 August 2020, Revised 4 October 2020, Accepted 5 October 2020, Available online 12 October 2020, Version of Record 22 October 2020.",https://doi.org/10.1016/j.jedc.2020.104012,Cited by (5),"This paper examines E-stability, determinacy, and indeterminacy in a general class of regime-switching models with lagged ","Rational expectations (RE) models admit multiple equilibria, and economists frequently use two criteria to select an equilibrium. The first criterion (“determinacy”) emphasizes model restrictions that ensure the existence of a unique rational expectations equilibrium (REE).==== Alternatively, the adaptive learning approach of Evans and Honkapohja (2001) and many others uses “E-stability” (i.e. “expectational stability” or “learnability”) to select equilibria that emerge as the outcome of an econometric learning process involving imperfectly informed agents. These two criteria are distinct, but there is value in understanding the connections between them. If we can isolate conditions under which determinacy and E-stability both obtain, then we can dispense with sometimes burdensome E-stability computations. When the two criteria fail to select the same equilibrium, however, it should complicate our understanding of that equilibrium’s reasonableness. For example, we might choose to reject an E-unstable determinate equilibrium on the grounds that nearly-rational agents cannot generate the predicted REE dynamics. Additionally, we might give extra consideration to an E-stable equilibrium of an indeterminate model.====A growing body of work uses Markov-switching DSGE models to explain macroeconomic phenomena.==== These models feature stochastically evolving parameters, and are therefore well-suited for modeling environments with ongoing institutional, policymaking or structural changes that may affect expectations formation. The existing literature says relatively little about the multiplicity and learnability of equilibria of these non-linear models, particularly empirically-rich regime-switching DSGE models with lagged endogenous variables. This paper examines connections between determinacy, indeterminacy and E-stability in a general class of Markov-switching DSGE models with lagged endogenous variables. Our contributions are threefold. First, we demonstrate that determinacy conditions from Cho (2020) imply the E-stability of the unique mean-square stable REE if agents have current information and use one-period-ahead rules, such as Euler equations, in their decision-making. This contribution extends McCallum (2007), which finds that determinacy implies E-stability in a general class of linear models, to environments with time-varying parameters. This first result also complements a key finding from Branch et al. (2013)’s analysis of adaptive learning in ==== forward-looking Markov-switching models (i.e. models without lagged endogenous variables): an equilibrium that does not depend on extraneous past regimes (“regime-dependent equilibrium”) is E-stable if it is the unique regime-dependent equilibrium. However, conditions ensuring uniqueness of the regime-dependent equilibrium are not determinacy conditions. In contrast, we study determinacy and E-stability, and we consider a more general class of models.====Second, this paper addresses the existence of E-stable solutions of indeterminate Markov-switching models. E-stable non-fundamental (NF) or “sunspot” equilibria exist if an E-stable MSV solution also exists.==== These E-stable NF solutions depend on extraneous exogenous variables, including arbitrary lags of the Markov regime. We further show that indeterminate regime-switching models can have Iteratively E-stable (“IE-stable”) solutions, whereas indeterminate linear DSGE models do not admit IE-stable solutions under analogous informational and behavioral assumptions. Because E-stability and IE-stability conditions coincide in a wide variety of practical applications,==== this finding suggests a close connection between E-stability and determinacy conditions in linear models, but not in regime-switching models. It has been argued that IE-stable solutions have special properties: Evans, Guesnerie, 1993, Evans, Guesnerie, 2005 and Guesnerie (2002) associate IE-stability with “eductive” stability, or the ability of rational agents to coordinate on a REE using common knowledge of rationality, and Gibbs and McClung (2020) use IE-stability in a RE framework to select equilibria that do not feature the “forward guidance puzzle” (Del Negro et al., 2012), i.e. explosive or otherwise implausible economic responses to forward guidance announcements. Therefore, IE-stability may constitute a useful criterion for choosing equilibria of ==== regime-switching models.====Our third contribution explores the last point: we numerically show how to obtain IE-stable solutions of indeterminate New Keynesian models with persistent interest rate pegs. If we interpret these interest rate peg regimes as recurring zero lower bound (ZLB) episodes, these exercises furthermore reveal one way to construct an adaptive learning model with stable expectations at the ZLB.==== In special cases, the model’s IE-stability conditions coincide with the Long Run Taylor Principle (LRTP) of Davig and Leeper (2007), which has been incorrectly considered a determinacy condition in the past. Generally, our numerical results suggest that indeterminate regime-switching models easily admit IE-stable equilibria.====This paper is related to a vast literature that studies the relationship between determinacy and E-stability in linear models, including the above mentioned work by McCallum. Ellison and Pearlman (2011) studies “saddlepath learning”, a set of RE-consistent restrictions in agents’ learning rules that extend (McCallum, 2007)’s central result to linear DSGE models with lagged information. They find IE-stable indeterminate solutions when agents are saddlepath learning, but IE-stable indeterminate solutions do not exist in linear models under the more standard assumptions we consider here. Bullard and Eusepi (2014) study determinacy and E-stability in linear models under general assumptions about agents’ information sets and decision rules. They show that determinacy is not generally sufficient for E-stability; (McCallum, 2007)’s insight is sensitive to details of the learning specification. This paper examines whether McCallum’s insight is robust to details of the model structure.====Few papers examine determinacy and E-stability in regime-switching models. Most notably, Branch et al. (2013) studies adaptive learning in a class of purely forward-looking models. We build on their path-breaking work in several dimensions. First, we study a more general model class that allows for lagged endogenous variables. Therefore, this paper’s insights are applicable to empirically-rich models that feature endogenous sources of persistence such as habit formation, inflation indexation, capital, inertial policy, debt-financed government expenditures, etc. Second, we employ the mean-square stability concept, which is currently the most widely-used stability concept in the regime-switching literature. Third, we build on recent work that characterizes the full set of solutions of Markov-switching models, and comment on the existence of E-stable NF solutions, whereas Branch et al. (2013) focuses on specific examples of E-stable NF solutions. We discuss features of these special examples, and connections between their paper and our IE-stability analysis, below. Finally, we study models with persistent, recurring interest rate peg regimes. Other related work studies Bayesian learning in regime-switching models (e.g. Bianchi, Melosi, 2013, Bianchi, Melosi, 2018; Bullard and Singh (2012), Richter and Throckmorton (2015), and Foerster and Matthes (2020)), or the interaction of model misspecification, learning, and regime switches (Ozden and Wouters, 2020).====We also contribute to a literature that studies solutions of Markov-switching DSGE models. Cho (2016), Farmer et al. (2011), Maih (2015), and Foerster et al. (2016) provide solution techniques that build on the pioneering works of Davig and Leeper (2007), Svensson and Williams (2007), and Farmer et al. (2010), Farmer et al. (2009). Cho (2016) and Cho (2020) provide conditions for the uniqueness of mean-square stable solutions, while Barthelemy and Marx (2019) provide conditions for the uniqueness of bounded solutions. We do not address learnability of bounded solutions, but we briefly discuss potential connections between our work and Barthelemy and Marx (2019).====The paper is organized as follows. Section 2 introduces connections between determinacy and E-stability, with Propositions 1–2 presenting our first contributions. Section 3 examines the learnability of indeterminate solutions and introduces the concept of IE-stability; Propositions 3–4 and Corollary 4 present new findings. Section 4 examines the Iterative E-stability of solutions to indeterminate New Keynesian models with recurring passive monetary regimes. Section 5 concludes.",E-stability vis-à-vis determinacy in regime-switching models,https://www.sciencedirect.com/science/article/pii/S0165188920301809,12 October 2020,2020,Research Article,284.0
"Perras Patrizia,Wagner Niklas","Department of Business and Economics, University of Passau, Passau 94030, Germany","Received 28 February 2020, Revised 29 August 2020, Accepted 3 October 2020, Available online 8 October 2020, Version of Record 24 October 2020.",https://doi.org/10.1016/j.jedc.2020.104009,Cited by (6),"Motivated by Merton (1973), we propose a novel ","The crucial role of covariance risk in the pricing of assets can be motivated by the intertemporal capital asset pricing model (ICAPM) of Merton (1973). Reflecting investors’ demand to hedge against adverse changes in the investment opportunity set, the model implies that expected asset returns depend not only on conditional variance but also on conditional covariance with state variables that are linked to time-varying investment opportunities. We propose a bivariate asset pricing model where time-varying investment opportunities are mirrored by time-variation in the conditional covariance between equities and long-term government bonds. From a dynamic hedging perspective, this is intuitive. Given time-variation in conditional equity-bond covariance, which is due to differential exposures of equities and bonds to shocks that correlate with marginal utility of wealth, multi-period investors face dynamic hedging opportunities that matter for portfolio choice. Changes in conditional covariance that are related to the dynamics of investors’ hedging opportunities thus represent covariance risk that affects asset premia and hence expected returns.====The present paper aims to answer the intriguing question whether equity-bond covariance risk is jointly priced in stock and bond markets, and if so, what are the determinants that influence covariance risk? We propose a bivariate conditional pricing model that simultaneously relates expected equity and bond market returns to the variance risk component as well as to the equity-bond covariance risk component. Our study employs monthly U.S. market observations during January 1965 and December 2017 to capture dynamics in conditional market variance and covariance between equity and bond market returns. Model estimation results provide significant evidence that covariance risk is priced in both markets. Further, we can show that the proposed equity-bond pricing relationship is consistent with two important phenomena that are both observed under low levels of equity-bond covariance. (i) Dynamic asset re-allocations within ‘flight-to-quality’ from equities to bonds, and (ii) re-allocations to risky assets due to ‘fear-of-missing-out’.==== While fear-of-missing-out, so far, has not been extensively covered by academics in finance, flight-to-quality has. Recent research of Baele et al. (2020) identifies global flight-to-quality episodes in association with negative equity-bond correlation and significant bond-equity return differentials. As flight-to-quality effects are supposed to correlate with liquidity effects, Baele et al. (2010) find that illiquidity proxies play a role for equity-bond correlation dynamics. Connolly et al. (2005) show that a higher probability of observing a negative equity-bond covariance coincides with high stock market uncertainty. Motivated by these studies, we control for the market uncertainty level as proxied by realized equity market volatility. Our findings show significant differences in the response of the equity and the bond market to shocks in market illiquidity across uncertainty regimes, which support the evidence of flight-to-quality from equities to bonds. During flight-to-quality, high equity market uncertainty coincides with a decline of equity prices and an increase of bond prices and realized bond returns, raising the expected return on equities and depressing the expected return on bonds.====While the public media frequently refer to ‘fear-of-missing-out’ in association with rising equity markets, the academic literature so far provides no explicit definition of this phenomenon. We identify and characterize fear-of-missing-out episodes by the following criteria. First, they coincide with low equity market volatility and above average equity market returns. Second, they are accompanied by a contemporaneous price depression in the bond market and a negative equity-bond covariance. In contrast to flight-to-quality, fear-of-missing-out is characterized by evident flows of funds from bonds to equities rather than from equities to bonds. The fear-of-missing-out pressure leads investors to demand lower premia and hence, to accept lower expected returns on equities. The effect on expected returns implied by both phenomena, flight-to-quality and fear-of-missing-out, can be captured by our intertemporal model that maps the pricing relations between equities and bonds.====We further focus on the economic determinants that explain time-variation in equity-bond covariance and thus are potential drivers of flight-to-quality and fear-of-missing-out effects. Guidolin and Timmermann (2006) and Guidolin and Timmermann (2007), respectively, provide reliable evidence that linear models are not able to capture time-variation in the conditional moments of equity and bond returns. To account for regime-switching dynamics, we use a threshold vector autoregressive (TVAR) model in order to assess the determinants that are likely to affect equity-bond covariance. The TVAR setting comprises shocks to stock market illiquidity, shocks to bond market illiquidity and shocks to inflation as endogenous variables and allows to estimate parameters depending on threshold regimes of expected inflation. Estimating the model shows that shocks to inflation are likely to induce changes in equity-bond covariance. This can be related to the proxy hypothesis of Fama (1981) which states that inflation is closely tied to real economic activity and consequently, shocks to inflation may act as a signal for future growth. More recently, David and Veronesi (2013) and Dergunov et al. (2016) claim that inflation shocks can be either good or bad news regarding real growth expectations. Our results show that inflation shocks in fact can represent a positive or negative signal for future economic activity depending on the level of expected inflation. The observed asymmetric signaling effect directly affects equity-bond covariance. During periods of high expected inflation, a positive shock to inflation represents bad news for both markets and reduces expected equity as well as bond returns, thereby it induces an increase in the covariance of the asset returns. During periods of low inflation, positive shocks to inflation act as a signal for improved business conditions representing good news for the equity market. At the same time, positive shocks to inflation are bad news for the bond market and depress nominal bond returns. The differential response of the equity and the bond market tends to result in a reduced level of conditional equity-bond covariance. These findings overall underline the signaling role of inflation shocks which plays a major role for equity and bond markets and makes inflation to a key macroeconomic fundamental in explaining joint dynamics in equity and bond markets and thus, time-variations in equity-bond covariance.====The remainder of this paper is organized as follows. Section 2 presents the conditional pricing model and implications for expected equity and bond returns. Section 3 motivates variables that are likely to affect equity-bond covariance and describes the variable construction. Section 4 presents the empirical setting to identify variables that induce time-variation in equity-bond covariance. Section 5 presents the results regarding equity-bond covariance risk pricing and illustrates implications of the equity-bond pricing relations across flight-to-quality and fear-of-missing-out episodes. This section further identifies empirical determinants of time-variation in equity-bond covariance and thus, potential drivers of flight-to-quality and fear-of-missing-out effects. Section 7 concludes.",Pricing equity-bond covariance risk: Between flight-to-quality and fear-of-missing-out,https://www.sciencedirect.com/science/article/pii/S0165188920301779,8 October 2020,2020,Research Article,285.0
"Brotherhood Luiz,Delalibera Bruno R.","Universitat de Barcelona, Spain,Barcelona Economic Analysis Team, Spain,EPGE Brazilian School of Economics and Finance, Brazil","Received 26 May 2020, Revised 1 October 2020, Accepted 3 October 2020, Available online 7 October 2020, Version of Record 21 October 2020.",https://doi.org/10.1016/j.jedc.2020.104010,Cited by (2), over aggregate earnings and college attendance differences between the two countries.,"The public education system of many developing countries is characterized by a large gap between students’ socioeconomic status in schools and universities. Public school students tend to have less favorable economic backgrounds than those who attend private schools (PISA, 2012), while public university students belong to wealthier families if compared to young adults with no college education.==== Thus, public education expenditures affect different parts of the population depending on which stage they are allocated to.====At the beginning of the 2000s, governments in some developing countries, such as Brazil, Colombia, and Paraguay, allocated a high proportion of per-student education expenditures to public universities. This changed during that decade, when educational resources were redirected to students in basic education.==== Fig. 1 shows that, in 2002, for each dollar spent by the Brazilian government on a public university student, only 20 cents of dollars were allocated to a public school student. Ten years later, each dollar per university student corresponded to 83 cents per school student. Still, the Organisation for Economic Co-operation and Development (OECD) has recently recommended this project’s intensification in some developing countries, with the objective of raising resource allocation efficiency and promoting equality (OECD, 2018, OECD, 2018, OECD, 2019).====In this paper, we seek to answer the following research question: how should a government allocate public expenditures across schools and universities? To answer this question, we develop a general equilibrium model featuring heterogeneous agents, basic and higher education, public and private educational institutions, credit frictions, and complementarity between human capital inputs. We calibrate the model to Brazil and use it to simulate counterfactual experiments. We compute the optimal utilitarian policy that allocates per-student government expenditures across basic and higher education to maximize the average welfare across households. We also use our framework to understand how educational variables explain economic differences between Brazil and the United States.====In the model, each household is composed of a parent and a child. Parents make choices related to consumption, savings, and investments in their children’s education. A child’s human capital depends on her innate ability, basic and higher education expenditures, and parental education. Our human capital production function nests standard specifications, such as that in Cunha et al. (2010), but also allows for the existence of an optional educational stage.====There are credit frictions in the economy: households cannot take loans in the credit market, which generates the existence of poor parents with high ability children who are not able to borrow against their offspring’s future labor earnings. In such an environment, public educational policies may be beneficial to financially constrained households.====Public and private schools differ in two aspects. First, each student in public school receives educational investments from the government through public expenditures.==== Second, households’ education expenditures made through public schools have a different marginal return in terms of generating human capital gains for students than investments made through private schools. Public and private colleges differ in the same aspects as schools, with one additional distinction: public universities have a limited number of seats and admissions are based on a noisy measure of the applicants’ human capital, while vacancies in private universities are unlimited.==== Due to this congestion effect in public universities, access to public education expenditures is unequal across families.====There are two main general equilibrium components in the model. First, since vacancies in public colleges are limited, a grade point cutoff is determined in equilibrium to make the mass of students entering public college consistent with seats supplied by the government. Second, government expenditure per student is determined endogenously in both educational stages, taking into account the mass of agents choosing public educational institutions and the fact that the government’s budget must be balanced in equilibrium.====The model is calibrated to fit Brazilian data. Targeted statistics include the fraction of students in public and private schools and universities, households’ educational expenditures, cross-sectional variability of labor earnings, private school and college wage premia. The model fits the data well, and we validate the calibration strategy by investigating the model’s performance in fitting non-targeted statistics, such as percentiles of the wage distribution and educational choices by parental wage quartiles. We also replicate a quasi-experiment studied in Francis-Tan and Tannuri-Pianto (2018) as a validation exercise. Using a regression discontinuity approach, these authors find that, in Brazil, individuals admitted to an elite public university with scores close to the admissions cutoff have statistically significant higher wages than those who are barely rejected. We replicate this experiment in our model, running a discontinuity regression with simulated data, and obtain a coefficient estimate consistent with Francis-Tan and Tannuri-Pianto (2018) findings.====We use the calibrated framework to look for the optimal utilitarian policy, which is given by the allocation of per-student expenditures across schools and universities that maximizes the average welfare across households. Since our objective is to compare the welfare of the current generation under different policies, our computations take into account the transitions from the benchmark equilibrium to the steady states associated with each policy that we simulate (Kambourov, 2009).====We find that the optimal utilitarian policy is the one in which the annual basic-higher per-student expenditure ratio is equal to 0.96. That is, for each dollar spent in the education of a public college student, the government allocates 96 cents to a public school student. Note in Fig. 1 that this policy is similar to that followed by the United States during the middle of the 2000s.====The optimal policy benefits almost all households (96%) and delivers considerable welfare gains for families in the bottom wage quartile: such households would be willing to pay on average 0.82% of their consumption in all periods and states of nature to have the policy implemented. This experiment has positive and small effects on aggregate earnings, and it cuts down income inequality by 2% in the long-run.====Our welfare analysis indicates that the allocation of public expenditures across schools and universities observed in Brazil in 2002 was an inferior policy. It also suggests that the strategy followed by some countries in South America during the 2000s, in which per-student public resources were reallocated from higher to basic education, were effective policies, and not only should have generated positive impacts in the 2000s, but also shall produce further gains in the future.====We re-calibrate the model to fit U.S. data and use it to understand aggregate earnings and college attendance differences between the United States and Brazil. According to the calibration results, public education institutions are relatively more efficient (with respect to private institutions) in the United States than in Brazil. In particular, we find that public universities in the U.S. model are overall more efficient than private universities, while the opposite happens in the model calibrated to Brazil. These facts show that the two models depict substantially different economies.====In 2000, the United States was approximately four times richer than Brazil and had a college attendance 2.3 times higher. We find that differences in the supply of vacancies in public universities is the education policy aspect that, alone, has the highest explanatory power over aggregate earnings and college attendance differences between the two countries. Increasing the number of vacancies by a factor of 6.5 in Brazil, to replicate the U.S. case, leads to 2% higher aggregate earnings and 53% higher college attendance in the long-run. This result on college attendance is not mechanical because applying to public university is a choice that entails costs in the model.====To the best of our knowledge, this is the first paper to study from a quantitative macroeconomic perspective the optimal allocation of public expenditures across basic and higher education in an environment where households choose between public and private education institutions. Therefore, we contribute to a large literature that studies education policies using structural models (Abbott, Gallipoli, Meghir, Violante, 2019, Blankenau, Youderian, 2015, Caucutt, Lochner, 2020, Cubas, Ravikumar, Ventura, 2016, Delalibera, Ferreira, 2019, Epple, Romano, Urquiola, 2017, Epple, Romano, 1996, Epple, Romano, 1998, Erosa et al., 2010, Glomm, Ravikumar, 1992, Glomm, Ravikumar, 1998, Glomm, Ravikumar, 2003, Herrington, 2015, Lee, Seshadri, 2019, Manuelli, Seshadri, 2014, Restuccia, Urrutia, 2004, Cunha, Herskovic, Ramos).====Two papers that are close to ours are Caucutt and Lochner (2020) and Herskovic and Ramos (2017). Caucutt and Lochner (2020) study the role of several economic mechanisms in determining human capital investments in children at different ages. They use a rich dynastic household model and compute the optimal ratio of early to late government subsidies to education in the US. Our model is markedly different from theirs: we consider general equilibrium effects and endogenous choices among public and private education institutions, while they model aspects that we abstract from, such as unobserved costs of schooling (e.g., psychic costs). Another major difference is related to the classification of education stages: their “early education” stage ranges from early childhood to primary education, and “late education” includes secondary and tertiary education.====Herskovic and Ramos (2017) develop a model to study the economic implications of affirmative action in higher education in Brazil. They find that targeted quotas for public college admissions are a powerful policy to reduce intergenerational persistence of earnings and improve welfare and aggregate output. To answer our research question, we build on their model by adding new features, such as savings, dynamic complementarity across human capital stages, and a non-fixed government spending per student in public college.====The “hierarchical education” literature==== is also particularly close to our paper (Abington, Blankenau, 2013, Arcalean, Schiopu, 2010, Blankenau, Cassou, Ingram, 2007, Driskill, Horowitz, 2002, Lloyd-Ellis, 2000, Su, 2004). These papers try to understand the macroeconomic consequences produced by different allocations of public expenditures across basic and advanced education stages (“advanced” generally meaning higher education). Most papers in this literature are theoretical, with Arcalean and Schiopu (2010) being the only quantitative paper that we are aware of. We contribute to this literature by building and calibrating a rich quantitative model that encompasses several relevant features not considered by previous papers, such as heterogeneous agents, credit constraints, and differences between public and private education institutions.====The rest of this paper is organized as follows. Section 2 describes the main model. Section 3 presents the calibration strategy. Section 4 discusses the optimal utilitarian policy. Section 5 uses the framework to decompose economic differences between Brazil and the United States. Section 6 presents concluding comments.",Minding the gap between schools and universities,https://www.sciencedirect.com/science/article/pii/S0165188920301780,7 October 2020,2020,Research Article,286.0
"Conesa Juan Carlos,Li Bo,Li Qian","Department of Economics, Stony Brook University, United States,School of Economics, Peking University, China,Institute for Advanced Research, Shanghai University of Finance and Economics (SUFE), China,Key Laboratory of Mathematical Economics (SUFE), Ministry of Education, China","Received 8 April 2020, Revised 14 September 2020, Accepted 18 September 2020, Available online 1 October 2020, Version of Record 19 October 2020.",https://doi.org/10.1016/j.jedc.2020.103991,Cited by (6),We evaluate a reform of the US ==== system switching to consumption taxation instead of income taxation. We do so in an environment that allows for progressivity of ,"The standard argument in favor of a move towards increasing the reliance on indirect taxation is made on the grounds of efficiency considerations. The argument is that flat consumption taxes that are constant over time are less distortionary than the currently existing income tax code. While this is widely acknowledged among academic economists, it is usually perceived as a policy that increases inequality and reduces the progressivity of the tax system as a whole. We contribute to that literature by quantifying the impact of such reforms in an environment that accounts for differences in expenditure shares depending on earnings as observed in the data. While it is true that inequality would increase in the long run, the magnitudes are not that large. Nevertheless, such reforms only generate welfare gains in the partial equilibrium case.====We evaluate the welfare and macroeconomic implications of switching from income taxation to a system that exclusively relies on consumption taxation. In order to perform this policy exercise we propose a general equilibrium model with idiosyncratic uninsurable productivity risk in the spirit of Huggett (1996) and Aiyagari (1994). There are two crucial departures of our model: first, we distinguish between basic consumption goods, that are subject to a consumption floor, and the rest of consumption goods. Second, we introduce a stochastic discount factor, in the spirit of Krusell and Smith (1998), allowing the model to generate a distribution of wealth more in line with the data.====In order to discipline our distinction between different types consumption goods, we look at the expenditure shares relative to labor earnings. We follow the criterion of defining basic goods as those for which their expenditure share is decreasing in labor earnings. Based on this criterion we label as basic goods food at home, rent, utilities, prescription medicine, television and books. All other expenditure categories are labeled as non-basic consumption goods.====Table 1 reports expenditure shares as a function of earnings quintiles from the Consumer Expenditure Survey (CEX) 2015 (see Section 2 for details). We observe that the share of expenditure in basic goods (labeled as ==== in Table 1) is around 37–57% at the bottom of the earnings distribution, while it falls to less than 15% at the top. We carefully calibrate our economy to be consistent with both the observed distribution of earnings and the relationship between earnings and consumption shares across the two types of goods.====In an international comparison we observe that consumption taxes in the US are relatively low, especially when compared to most European economies. The Value Added Tax (VAT) in most European economies is above 20% and accounts for a substantial fraction of fiscal revenues, often close to the revenue generated by personal income taxes. In contrast, in the US sales taxes depend on the states and are subject to substantial differences in tax rates.====It is however a common feature to levy different tax rates on different types of consumption goods. Most notably, in the US grocery foods are exempted in 31 states of 46 states that have a state sales tax. Medical services and medicine are universally exempted. In contrast, clothing is exempted in only eight states, and the exemption does not apply to sport goods. A similar pattern emerges in Europe, where many goods are considered basic and therefore are subject to reduced rates. Our category of basic goods overlaps to a large extent with the categories that are exempt or subject to reduced rates in most tax systems around the world.====Our results suggest a strong rational for the widespread practice of taxing different goods at different rates. In fact, our quantitative results suggest that basic goods should be subsidized (a tax of ====), with a corresponding tax rate for non-basic goods of 68%. In contrast, if we were to impose equal rates across consumption goods, we would obtain a tax of 44%.====Such a reform implies that only the most productive households with limited wealth experience welfare gains. In contrast, the rest of the population experience welfare losses (that are increasing in wealth holdings). Along the transition, only 7.5% of the population in the benchmark economy are better off with the reform. Our results are consistent with Nishiyama and Smetters (2007), who argue that switching to consumption taxation would not be efficient because of decreased consumption risk-sharing. In our case we find welfare losses are the norm even though we allow for means-tested transfers and progressive consumption taxes, that could potentially ameliorate the distributive consequences of reforms.====Interestingly, when we perform the same policy exercise in a partial equilibrium setup, we find generalized welfare gains for all consumers. The key difference is that hours worked fall by 4% in the partial equilibrium case, while they increase and become more unequally distributed in the general equilibrium case.====There is a long tradition advocating for expenditure taxation instead of income taxation that goes back to at least Kaldor (1955). In the recent literature, quantitative macroeconomic models similar to ours have been used to quantify the impact of different types of reforms. Besides Nishiyama and Smetters (2007) mentioned above, exercises quantifying the macroeconomic impact and inequality implications of consumption taxation, flat-tax reforms in the spirit of Hall and Rabushka (1995), or other similar reforms such as negative income taxes, can be found in Krusell et al. (1996), Ventura (1999), Altig et al. (2001), Correia (2010) and Lopez-Daneri (2016) among others.====Except for Li (2020) who considers durable and nondurable consumption, none of the other standard papers in that literature distinguish between different types of consumption goods. Lopez-Daneri (2016) also incorporates into the analysis the distribution of transfers as a function of income, but the policy exercises are different. The importance of transfers is highlighted also by the analysis in Correia (2010). For us this is a key ingredient of the analysis, since transfers at the lower end of the income distribution play a key role in allowing individuals to meet the subsistence levels of basic consumption goods. Finally, in a follow up paper, Conesa et al. (2020), we use a life-cycle version of the model to think about the welfare implications and the alternatives to finance different scenarios of Universal Basic Income.====The rest of the paper is structured as follows. Section 2 describes the empirical facts regarding the existing consumption tax systems and consumption patterns in the US. Section 3 describes the benchmark model and the calibration strategy. The numerical experiments are carried out in Section 4. We present the steady state results, the transition dynamics, and then evaluate the impact of general equilibrium effects. Section 5 summarizes and concludes.",Welfare implications of switching to consumption taxation,https://www.sciencedirect.com/science/article/pii/S0165188920301597,1 October 2020,2020,Research Article,287.0
"Casares Miguel,Khan Hashmat,Poutineau Jean-Christophe","Departamento de Economía and INARBE, Universidad Pública de Navarra, 31006, Pamplona, Spain,Department of Economics, Carleton University, Ottawa, ON K1S 5B6, Canada,CREM, UMR CNRS 6211, Université de Rennes I, Rennes, France","Received 25 May 2020, Revised 7 September 2020, Accepted 21 September 2020, Available online 26 September 2020, Version of Record 7 October 2020.",https://doi.org/10.1016/j.jedc.2020.103997,Cited by (2), shocks.,", ==== and ====. Along this downward trend, this paper provides empirical evidence showing that the short-run fluctuations of US net business formation have turned more volatile, procyclical and persistent after the global financial crisis of 2007-08. Changes in both establishment entry and exit have contributed to these new cyclical patterns of business formation, making the role of the extensive margin much more relevant for US business cycles than what it was during the Great Moderation period.====Following ====, ====, ==== and ====, we consider a sticky-price model with variable number of firms, and extend the analysis to account for the above mentioned stylized facts.==== ==== However, as a common feature, these papers assume a constant rate of exit which leaves business formation mostly driven by fluctuations in the rate of entry. The variability in the rate of exit observed in the data is clearly at odds with the constant rate of exit rate formulation.====Hence, the first contribution of our paper on the modelling side is to introduce an ==== exit rate decision that takes into account the liquidation value of the firm and expected dividends. Remarkably, while the entry of new firms (or new varieties of consumption goods) has been widely considered in DSGE models, few papers have proposed an analysis of firm exit.==== Recently, ====, ====, ====, ====), and ==== have proposed endogenous exit of firms motivated by reasons different than ours.==== These attempts, however, turn out to be not satisfactory in addressing the stylized facts in the post-2008 financial crisis period. In ====, for example, the exit decision is based upon the current value of profits and it does not consider any liquidation cost associated with exit. This feature makes the exit decision ====. In another recent paper, ==== considers scrap values, which are time-invariant and firm specific, in the exit decision. ==== assumes that the liquidation value is zero. By contrast, we introduce heterogeneity through both firm-specific productivity and a time-varying liquidation value. This formulation has the advantage that we can assess how both the heterogeneity of incumbents and changes in the liquidation value can shape the intertemporal exit decision.==== In the opposite case, the business unit exits the industry, the production of its variety ends and there is business destruction.====For a quantitative evaluation of the role of the extensive margin for US aggregate fluctuations, we conduct a Bayesian estimation of our model with US quarterly data between 1993 and 2018. The extended model performs well on replicating business creation and destruction observed in recent US data. In particular, the posterior estimates generate model simulations that provide a good match on the second-moment ====Our estimation exercise also includes a variant of the model that assumes a constant exit rate. We find that the baseline model with both endogenous entry and exit outperforms the more standard constant exit rate model because it provides a better description of the joint dynamics of business creation and destruction. Besides, the overall fit of the estimated model to the data is superior when the exit behavior is endogenized.==== and ==== in the context of models without entry and exit. The importance of risk-premium shocks during the Great Recession was also established in ====. Additionally, we identify monetary policy shocks capturing the two waves of conventional and unconventional monetary expansions of the US Federal Reserve (in 2007-09 and 2013-15, respectively) with significant effects for GDP growth and business formation.====The rest of the paper is organized as follows. ==== provides the empirical motivation of our paper and outlines the key stylized facts on US business formation. ==== presents the extended DSGE model with a special focus on the processes of business creation and destruction. Section 4 introduces the Bayesian estimation strategy and provides the posterior estimates of the DSGE model with extensive margin for the US economy, including a validation exercise based on the empirical fit of second-moment statistics. ==== presents the main analysis and proceeds with the discussion of results. ==== concludes with the summary of the most relevant findings of the paper.",The extensive margin and US aggregate fluctuations: A quantitative assessment,https://www.sciencedirect.com/science/article/pii/S0165188920301652,26 September 2020,2020,Research Article,288.0
"Pham Manh Cuong,Anderson Heather Margot,Duong Huu Nhan,Lajbcygier Paul","Lancaster University Management School, Bailrigg, Lancaster, LA1 4YX, United Kingdom,Monash Business School, Monash University, Wellington Road, Clayton, VIC 3800, Australia","Received 23 January 2020, Revised 10 September 2020, Accepted 14 September 2020, Available online 24 September 2020, Version of Record 10 October 2020.",https://doi.org/10.1016/j.jedc.2020.103992,Cited by (3),"We compare trade size to the prevailing market depth at the best level in the limit order book to detect and account for zero impact trades in an immediate price impact model. Our model also incorporates standard trade attributes (trade size, ==== and volatility) in a dynamic setting. The incorporation of market depth information reduces the mean absolute/squared forecast error of an immediate price impact prediction by about 60%. After controlling for trade attributes, market depth, price impact dynamics and intra-and inter- day periodicities (in order of relative importance) all improve the prediction of a trade’s price impact. We demonstrate the value of our model by showing that splitting a big order into a series of smaller trades results in a reduction of between 60% and 82% of the immediate price impact cost of the big order. We also find that our depth indicator helps with the prediction of order flow and permanent price impact.","The ability to design optimal trades that minimize trading costs is of great interest to financial market participants. Explicit components of trading costs include bid-ask spreads and commission fees, but an implicit and much larger component of trading cost is market or price impact, which is the change in an asset price that results from a trade and is unknown ex-ante (Keim, Madhavan, 1996, Keim, Madhavan, 1998). Recent work on trading costs (e.g. Cont, Kukanov, Stoikov, 2014, Eisler, Bouchaud, Kockelkoren, 2012, Pham, Duong, Lajbcygier, 2017, Wilinski, Cui, Brabazon, Hamill, 2015, Zhou, 2012) has focused on the impact caused by a single trade ==== after its execution, and it demonstrates that trading volume, market capitalization and volatility are all important determinants of market impact. However, little effort has been made to explicitly incorporate market depth information into empirical models of immediate price impact. We study the empirical relationship between market depth and immediate price impact in this paper.====Our investigation is motivated by theoretical predictions and existing empirical findings on price impacts of trades. Theoretical models such as that by Kyle (1985) suggest that price impacts of orders are increasing in order size, as explored further by Foucault et al. (2013b). Theoretical and empirical work by Kraus and Stoll (1972) and Keim and Madhavan (1996) shows that price impacts are much larger for transactions that exceed available market depth at the best level. Knez and Ready (1996) find that the expected price improvement (measured as the difference between the transaction price and the prevailing bid or ask quote) is mostly dependent on “excess depth” (measured as the difference between quoted depth and order size). Expected price improvement is larger (in magnitude) the more negative “excess depth” becomes, but it essentially drops to zero once “excess depth” turns positive. Knez and Ready (1996) note that most of their observations fall into the latter category. Related later work such as that by Dufour and Engle (2000) has documented a high incidence of zero-impact trades.====The above cited work leads us to use information on quoted depth to identify zero-impact trades. We consider trades conducted in a limit order book market, and note that if a trade is smaller than the prevailing quoted depth at the best level on the opposite side of the order book, then it can be completely absorbed by the market depth at the best bid or ask price, and hence have zero immediate market impact. Trades that are larger than the available quoted depth will move the best bid or ask level after consuming all liquidity at the current depth, and will result in non-zero price impact. Thus, the quoted market depth information can be used in conjunction with trade size to form a zero-impact trade “detector”, that equals zero when market depth does not support price impact and equals one otherwise. We relabel this zero-impact trade detector as a market depth indicator, and argue that its inclusion in a price impact model that is applied to tick by tick data is intuitively justified. Further, we show that the use of this “indicator” in an empirical model improves the out-of-sample forecast accuracy of immediate price impact by reducing mean absolute/squared errors by about 60%. Such (statistically significant) reductions translate into a total decrease of approximately $AUD 100 million per annum in the forecast uncertainty of price impact costs, offering clear potential for more accurate projections of the costs and profits of trading strategies.====We then use price impact models that incorporate our market depth indicator to investigate and quantify the effects of order splitting strategies. Barclay and Warner (1993), Chakravarty (2001), Choi et al. (2019), Easley and O’Hara (1987), Kyle (1985) and others have argued that traders often split a large order into several smaller orders to hide their information and avoid adverse price impact against their large orders. Also, as discussed in Hasbrouck and Saar (2013), Hendershott and Riordan (2013) and O’Hara (2015), institutions now use algorithms to implement dynamic trading strategies that split and sequence orders so as to minimize execution costs. Along with this, Chordia et al. (2011) have documented that institutions now resort to splitting orders in response to decreased market depth at the prevailing quotes. In our analysis, we assess the effectiveness of order splitting strategies by comparing the price impact of a series of small trades to the immediate price impact of an artificial trade that aggregates these consecutive small trades.====Our empirical modelling framework is built on a threshold principle, that sets expected immediate price impact equal to zero whenever our market depth indicator is zero. When our indicator is equal to one, our model follows a specification that incorporates standard regressors such as the trade characteristics used by Lillo et al. (2003) and Zhou (2012). We include Corsi (2009) type heterogeneous autoregressive (HAR) variables to account for underlying dynamics that drive orders, trade and price impact, together with day of the week and diurnal variables as well. The “switch” between zero price impact and (potentially) non-zero price impact is accomplished by treating the product of the indicator and a linear specification of the other variables as “the model”, as we explain in further detail in Section 2.====We examine an Australian dataset of stocks drawn from the S&P/ASX200 index.==== We find that a model that incorporates the market depth indicator, other theoretically motivated variables (trade size, market capitalization and volatility), price impact dynamics, and time of day/day of the week patterns consistently outperforms all other models that we consider (including a naive model that always predicts zero market impact), based on both out-of-sample mean squared error (MSE) and mean absolute error (MAE). We further show that, in addition to traditional trade attributes, (1) the information from market quoted depth, (2) the dynamics of immediate price impact, combined with (3) intra- and inter-day periodicities (in order of relative importance) contribute to the performance of our favored immediate market impact model.====We assess the effectiveness of order splitting strategies by using the most accurate of our set of price impact models to estimate the immediate price impact of an artificial unobserved large trade that aggregates a series of consecutive same-sign small trades on the same trading day - and find that the observed price impact of these split small trades is much lower than that associated with a single big trade. More specifically, splitting a big order into smaller trades can reduce the immediate price impact of the former by between 60% and 82% on average. This result confirms the effectiveness of the order splitting strategies that may be employed by informed traders in order to hide their information and reduce their price impact costs, as suggested in the literature (e.g. Dufour, Engle, 2000, Easley, O’Hara, 1987, Kyle, 1985), or by institutional investors who use order-splitting strategies to minimize their trading costs while rebalancing their portfolios (e.g. Choi, Larsen, Seppi, 2019, Forsyth, Kennedy, Tse, Windcliff, 2012, Keim, Madhavan, 1995, van Kervel, Menkveld, 2019, Korajczyk, Murphy, 2019, O’Hara, 2015).====We also study the relation between market depth information and future order flows, and highlight the use of depth information to predict future order imbalance. This imbalance is an important aspect of the dynamics of incoming orders that provides a link between the immediate price impact studied here and the permanent price impact discussed in Bessembinder and Venkataraman (2010). Our analysis shows that the use of depth information also helps with the prediction of permanent price impact.====We contribute to the literature on immediate price impact (for example Lillo, Farmer, Mantegna, 2003, Wilinski, Cui, Brabazon, Hamill, 2015, Zhou, 2012) by demonstrating that the use of our proposed depth indicator in immediate price impact models enhances their forecast accuracy by large and statistically significant margins. We measure some of the economic implications of the use of our indicator in immediate price impact models and find substantial benefits, both in terms of the reduction in the forecast error of price impact models and in terms of potential savings made by splitting large orders. We also find that our depth indicator is informative about future order flows and the gap between immediate price impact and permanent price impact. Given the importance of minimising trading costs, such findings have considerable practical relevance.====The rest of the paper is organized as follows. Section 2 outlines our model of immediate price impact and then discusses our depth indicator and other aspects of model specification that lead to greater precision in immediate price impact estimates. Section 3 reviews our data, relevant institutional detail and our research methodology. Comprehensive out-of-sample comparative results are discussed in Section 4, followed by discussions of the economic implications of a more accurate immediate price impact model in Section 5, and some analysis of how well our depth indicator signals future order flow and permanent price impact in Section 6. We include additional discussion of results relating to a nonparametric extension of our model in an appendix, and Section 7 concludes.",The effects of trade size and market depth on immediate price impact in a limit order book market,https://www.sciencedirect.com/science/article/pii/S0165188920301603,24 September 2020,2020,Research Article,289.0
Scharfenaker Ellis,"Department of Economics, University of Utah, 260 Central Campus, Suite 4100, Salt Lake City, UT 84112, USA","Received 22 August 2019, Revised 17 August 2020, Accepted 11 September 2020, Available online 14 September 2020, Version of Record 19 September 2020.",https://doi.org/10.1016/j.jedc.2020.103990,Cited by (6),This paper explores the foundations and properties of the quantal response ==== of economic outcomes can be understood as arising from the unfulfilled expectations of entropy-constrained ====. This paper demonstrates the logic of the QRSE model in an application to US stock market data dating back to 1926. The model provides a parsimonious explanation for the distribution of rates of return on private equities as well as a behavioral foundation for asset price fluctuations.,"Many economic and social outcomes are the result of the actions of individuals interacting in an institutional structure. A significant share of economic theory tends to abstract away from the complex ways individual actions shape social outcomes by assuming that the individual actions have a zero impact on the outcome. For example, a theoretical cornerstone of Walrasian general equilibrium theory is that no individual firm or consumer can impact the equilibrium price system through their own buying and selling decisions. There are at least two obvious reasons for abstracting away from the ways individual actions shape social outcomes. First, is that the level of detail required to model and predict such a system becomes formidable when the number of individual participants becomes large. Second, information or data about individual behavior and actions tends to be either of a limited nature or is unobservable.====Situations in which the actions of individual participants have a non-zero impact on the social outcome tend to produce important qualitatively different equilibrium properties of the system under analysis. In particular, such systems tend to produce central tendencies in the distribution of the outcome as well as endogenous fluctuations around these tendencies that reveal information about the underlying forces that shape the system frequency distributions. This paper examines economic systems characterized by social interactions that are conditionally dependent on perceived social outcomes but that also shape those social outcomes through stabilizing and equilibrating effects.====The quantal response statistical equilibrium (QRSE) model developed by Scharfenaker and Foley (2017) applies maximum entropy reasoning for predicting the behavior of such systems when data provides only limited information about the system. When observations or statistical properties of social outcomes are available, but there is no direct information about the actions and behavior of individual participants that determine that social outcome, the problem of inferring the joint state space over the actions and outcome and thus predicting the configuration of the system is an ill-posed underdetermined problem.====While the unobservable component in such underdetermined models may in principle be observable, data rarely provides such information beyond the individual case-study level. Situations in which the macroeconomic outcomes are observable, but the details about individual actions that determine these outcomes are unobservable arise frequently in economics. For example, the buying/selling decisions of individual investors are determined by and determine observable equity prices, but are themselves unobservable at the system level. Firm decisions to allocate capital in particular sub-markets depend on the expected profit rate and determine the observable profit rate, but only aggregate firm balance sheet data is observable. Individual labor-market decisions depend on expected remuneration and shape the income distribution, but only income frequencies are observable.====In order to make inferences about the mutual dependence of individual actions and the resulting social outcomes requires we know the structure of a joint frequency distribution over the actions and outcomes. In most economic applications both dimensions of the problem are assumed to be known (e.g. Manski and McFadden (1981)) and the problem lies only in estimating the probability of individual actions through a parametric statistical model. If either dimension of the problem is unknown, however, we face the inverse problem of having to reconstruct the joint distribution based on whatever information we have about the system. Operationally, this requires that we impose constraints on the system in the form of a prior hypothesis. One of the most successful approaches to solving such underdetermined inverse problems is the Principle of Maximum Entropy (PME) inference championed by (Jaynes, 1979, Jaynes, 1982, Jaynes, 2003). The PME provides a straightforward and logical way to incorporate prior hypothesis in a model that produces the least biased estimate of such a probability distribution. A maximum entropy probability distribution is also a predictive statistical model that indicates whether or not the theoretical priors put into the calculation are relevant in the sense of providing a good fit to the data.====This paper details the logic of maximum entropy inference for underdetermined social interaction problems and examines some of the economic and statistical implications of the QRSE model. It turns out that many of the widely used statistical models used for modeling the frequency distributions of economic and social outcomes, including the Subbotin and Asymmetric Subbotin distribution, can be usefully understood as special cases of the more general behaviorally founded QRSE model. Thus, many social phenomena that give rise to macroscopic statistical regularities turn out to have a clear interpretation from the QRSE framework.====This paper also illustrates the logic of QRSE by extending the model to monthly firm-level equity price data dating back to 1926. The results provide a remarkable fit to the data as well as a clear behavioral interpretation of asset price fluctuations that may serve as an indicator of speculative activity.",Implications of quantal response statistical equilibrium,https://www.sciencedirect.com/science/article/pii/S0165188920301585,14 September 2020,2020,Research Article,290.0
Lappi Pauli,"University of Helsinki, Finland","Received 28 February 2019, Revised 12 August 2020, Accepted 31 August 2020, Available online 8 September 2020, Version of Record 16 September 2020.",https://doi.org/10.1016/j.jedc.2020.103987,Cited by (1),Polluting ,"Currently a problematic feature of many extraction operations around the world is that the monies intended for the reclamation of the polluted site after shut-down are not sufficient to return the site into a productive alternative use. More specifically, the reclamation bonds and reclamation trust payments by the firms tend to fall significantly short of the reclamation costs.==== For this reason, either taxpayer money must be used to finance the reclamation or the operation is not properly conducted. Failure to reclaim causes different kinds of problems ranging from acid mine drainage and groundwater contamination to loss of land, forests, grasslands and benefits from recreational activities. A plausible reason for insufficient bonding is asymmetric information related to reclamation costs.====This paper focuses on the hidden information aspect, and proposes a contract that includes a tax to control the environmental externality and a mechanism for screening the cost types that consists of a specified reclamation effort and a monetary transfer.==== In general, the mining firm has better information than the regulator about the characteristics relevant for regulation whether the regulation is rent taxation or environmental policy: the firm is better informed on the commercial circumstances at all stages of extraction (Boadway and Keen, 2010), on the geological properties at the mine site and on the firm’s technology (Collier and Venables, 2010), and, relatedly, on the development and operating costs (Osmundsen, 2010). The ways to exploit this better information are various and depend on the incentive structure presented to the firm. As Osmundsen (2010, pages 431–432) writes, the strategic reporting does not mean illegal activities, but instead, it means the transmission of information which is not based on the best understanding of the firm but on a selective data use and measurement best aligned with the firm’s interests. For example, in rent taxation the firm can use transfer pricing to influence the tax base (Boadway and Keen, 2010); in reclamation and closure operations, the relevant information includes for example the geological details, such as the potential of the waste rock to produce acid mine drainage, and technological details, such as the cheapest available technology to prevent or remedy the drainage. The best information on these is held by the mining company who has conducted surveying and owns the technology.==== The current reclamation policies allow the mining firms to use their information to their advantage. A typical approach for regulation is to require the firm to present a reclamation or closure plan, in which the firm must give an estimate of the future reclamation cost based on engineering cost considerations. In British Columbia the mining firm is required to present a detailed estimate of the reclamation cost to the regulator for inspection (Goverment of British Columbia, 2020), which allows the firm to employ the above mentioned tactics to transmit information on which the cost is based on. The situation is similar for example in Nova Scotia (Nova Scotia Environment, Stantec Consulting Ltd.), Ontario (Government of Ontario, Stantec Consulting Ltd.), Alaska and Nevada (NDEP, Stantec Consulting Ltd.). This means that the mining firm can hand in a report that portrays a picture about the costs that diverges from reality, which jeopardizes sufficient reclamation. The model of this paper follows these policies in that the mining firm has private information over the reclamation cost. Specifically, it has information on a parameter of the reclamation cost function and must hand over a cost parameter report to the regulator. However, in contrast to the above policy, the mechanism gives the firm incentives to report truthfully.====An alternative to relying on cost reports would be to use available data to estimate the reclamation costs, but this approach suffers from data problems and the disincentive of the mining companies to share their cost information. For example, Ho et al., 2018 estimate the well plugging costs using data from many U.S. states (note that there are for sure more wells than there are mines), and show that the bond amounts tend to be lower than the plugging cost. However, they say that one problem in their data is that it does not cover plugging done by the private sector. This sector is exactly the one in need of improved regulations, and which has little incentives to share their data to be used for policy review. The problem obtaining statistical estimates is even worse in mining, where mines are by far less numerous than the number of wells used in the above paper. In mining, the U.S. EPA proposed a rule (which it later reversed) for the financial responsibility requirements under CERCLA §108(b) for hardrock mining industry with the purpose to incentivise site reclamation and cleanup (U.S. EPA, 2017). Instead of relying on engineering cost information, the rule included an estimated cost formula that is used to form the size of the financial instrument based on firm’s report on, for example, the acre size of the tailings ponds. However, the formula will either over- or understate the true cost.==== Overestimation implies that too much firm’s resources are tied to the assurance and that the firm may not wish to participate. Underestimation means that the assurance is insufficient and it is possible that too costly mines are allowed to operate. To prevent this and to include the site-specific information held by the mining firm, the model of this paper uses a mechanism that guarantees participation.==== The main question is how to design a contract between the regulator and the mining firm, when the extraction operation produces a stock pollutant and leaves behind a polluted site that must be reclaimed at a cost which is known only by the mining firm? This question is analyzed in a set-up, where the regulator wants to obtain the highest net benefits from mining and reclamation, and proposes a take-it-or-leave-it contract to the mining firm. As there are two market failures - pollution externality and private information - the contract specifies a pollution tax to be paid during the extraction operation and a mechanism for cost information revelation that consists of a reclamation effort and a monetary transfer. In addition, the firm is required to pay the present value reclamation costs to a trust or as a bond before commencing the extraction. More specifically the research questions are: What kind of properties does the optimal contract have? How does the pollution tax under asymmetric information compare with the tax under complete information? When is it worthwhile for the regulator to exclude the most expensive firm types?==== Gaudet et al. (1995), together with Osmundsen (1995), is one of the first studies to introduce asymmetric information to exhaustible resource extraction literature.==== They analyze a model where the government offers a contract to an extracting firm in order to maximize weighted combination of the (royalty) revenue and the firm’s profit, when the extraction cost is private information for the firm. They find that the Hotelling rule is modified under the optimal contract. Osmundsen (1995) extends their model of cost asymmetry to cover more general extraction cost function.==== In Osmundsen (1998) this model is yet modified into a case with a stock dependent extraction cost and with asymmetric information on the initial amount of the resource. Also Martimort et al., 2018 analyze asymmetric information on the initial resource stock, but with an infinite horizon model, and they find among other things that optimal extraction rate is governed by ”virtual Hotelling rule”. The current model differs from these in that the extraction cost and the initial resource stock size are known both by the firm and by the regulator. Instead, the informational asymmetry manifests itself at the reclamation stage after the extraction has ended. Specifically, the firm has private information over a parameter in the reclamation cost function.====As the reclamation of the pollution stock is similar to abatement, the model is related to the literature on environmental policy under asymmetric information. The contract designed here must give the firm incentives to participate, which means that the total discounted value over extraction and reclamation stages must be non-negative. In principle, if the regulator is allowed to neglect the firm’s willingness to participate, it can set the following well-known contract that yields the complete information reclamation effort====: Regulator says to the firm that he must pay for every reclamation effort not performed the related marginal damage. However, when the firm can reject this contract (due to high reclamation costs related to the complete information reclamation effort), the contract may induce a welfare loss to the society, if the consumer surplus from the extracted good is high enough. This and valuable public funds are the reasons to study a second-best contract.====There exists a large literature on the regulation of a polluting exhaustible resource extraction, which includes among others models on carbon tax and exhaustible resources (Hoel and Kverndokk, 1996, Tahvonen, 1997, Ulph, Ulph, 1994), models related specifically to mining (Lappi, Ollikainen, 2019, Roan, Martin, 1996, White, Doole, Pannell, Florec, 2012), and models on exhaustible resource use and reclamation (Lappi, 2018, Sullivan, Amacher, 2009, Yang, Davis, 2018). Both Roan and Martin (1996) and White et al. (2012) assume that reclamation is done during the extraction operation, but in practice it is often needed after shut-down as assumed here. In addition, all of these models assume complete information with the exception of Yang and Davis (2018), who mention asymmetric information briefly when they propose a pollution stock tax, which requires the firm to pay for the marginal damages for every increment in the stock. They state that this regulation reaches the social optimum even if the reclamation cost is private information.==== However, this regulation neglects the possibility that the firm may not wish to participate at all, which may not be socially optimal if the consumer surplus is high enough (see above). In addition, a practical problem raised also by Yang and Davis (2018) is that the firm may choose bankruptcy instead of paying for the stock tax when the operation is shutdown.==== This can be avoided, if the monies for the reclamation are collected at the beginning of the extraction operation as is assumed here. But to calculate the correct amount one needs information on the reclamation cost.==== Section 2 develops the notation, the assumptions and the time-line of the model. Sections 3 and 4 characterize the optimal contract under complete information and under asymmetric information, respectively. In Section 5 the pollution tax under asymmetric information is compared to the tax under complete information (detailed calculations are presented in the online appendix), and in Section 6 the exclusion of the most costly types are analyzed. The final section offers discussion and some thoughts for further research. Proofs are relocated to the appendix.",On optimal extraction under asymmetric information over reclamation costs,https://www.sciencedirect.com/science/article/pii/S016518892030155X,8 September 2020,2020,Research Article,291.0
"Penalver Adrian,Hanaki Nobuyuki,Akiyama Eizo,Funaki Yukihiko,Ishikawa Ryuichiro","Banque de France, France,Institute of Social and Economic Research, Osaka University, Japan,Faculty of Engineering, Information and Systems, University of Tsukuba, Japan,School of Political Science and Economics, Waseda University, Japan,School of International Liberal Studies, Waseda University, Japan","Received 17 October 2019, Revised 3 July 2020, Accepted 20 July 2020, Available online 29 August 2020, Version of Record 9 September 2020.",https://doi.org/10.1016/j.jedc.2020.103978,Cited by (3),"We experimentally investigate the effect of a central bank buying bonds for cash in a quantitative easing (QE) operation. In our experiment, the bonds are perfect substitutes for cash and have a constant fundamental value which is not affected by QE in the rational expectations equilibrium. We find that QE raises bond prices above those in the benchmark treatment without QE. Subjects in the benchmark treatment learned to trade the bonds at their fundamental value but those in treatments with QE became more convinced after repeated exposure to the same treatment that QE boosts bond prices. This suggests the possibility of a behavioural channel for the observed effects of actual QE operations on bond yields."," In these circumstances, QE would just be an irrelevant shortening of the average maturity of net public debt.====There is, however, strong evidence that QE programmes have moved bond prices and yields, although the scale and duration of such effects is still being debated (====, ====, ====). The literature has focused on two departures from the textbook model to explain these effects. One theory is that central bank money and government bonds are not perfect substitutes (====) perhaps because markets are segmented due to investors’ ‘preferred habitat’ (====). QE reinforces the signal that the short-term rate will remain low for longer than a time-consistent policy rule would suggest. Lowering the expected path of short-term rates drags down long-term rates through the expectations hypothesis of the term structure.====This paper considers a different explanation. As currently implemented, QE is a commitment to buy relatively quickly a fixed value of bonds at any price. Indeed, since the intermediate objective of the policy is to lower bond yields, the greater the rise in bond prices is, the more “successful” the instrument is. The central bank is thus an unusual participant in the bond market because it is not deterred from buying by a higher market price (at least up to some point). If there were only one seller of government bonds, he or she could offer to sell at a price at which the central bank was just indifferent between buying and reneging on its commitment at a reputational cost. In other words, once the central bank has committed to buy, there is an exploitable opportunity for sellers collectively. However, in a completely competitive market with fully rational agents, common knowledge, no segmentation and no ability to buy enough of the market to become a monopoly seller, such an effect should not exist.====But what if some of these assumptions do not hold, for example, when people’s behaviors are better characterized by a level-==== type thinking (====, ====)? In such framework, if ==== players have an instinctive response that the presence of a large external buyer should raise the price, then this will influence the views of ==== players which together affect the expectations of ==== players and so on. As long as it is believed that there are enough low ==== players who believe prices will rise, even a very sophisticated player might be induced to make an offer above the fundamental price. This structure is similar to the guessing game of Ledoux made famous by ====.==== Level-==== models can account for many non-equilibrium behaviors observed in the laboratory experiments, including, various types of auctions (see, among others, ====, ====),==== Bertrand price competition (====, ====), and the travelers’ dilemma game (====).==== Recently, researchers have started to incorporate level-==== thinking in analyses of monetary policies. ==== construct a model of level-==== thinking applied to QE, and ==== use it, together with incomplete credit market, in explaining the “forward guidance puzzle.” We therefore anticipate that many participants will have an instinctive response that the presence of a large external buyer should raise the price.====The main purposes of the experiment presented in this paper are (a) to investigate whether interventions indeed raise the price of bonds, and (b) to investigate whether the observed effect of the interventions disappears with repeated exposure to the same treatment.====The environment which we consider is very simple, and is set out in more detail in ====. The experiment is designed to remove as many extraneous sources of uncertainty (such as stochastic dividend payments) and confusion (such as an inability to understand a declining fundamental value) as possible. At the start of the experiment, participants are given bonds and cash, which are essentially identical ways of transferring this endowment to the end of the experiment in the rational expectations equilibrium (REE). The REE – or fundamental – price is constant throughout the experiment. In the Benchmark treatment, participants can trade the bonds for cash among themselves over 11 periods. There is no reason to trade one asset for the other at this fundamental price and the volume of trade should be indeterminate.====In our first QE treatment (called Buy&Hold treatment), the central bank will buy a third of the bonds available in the market before periods four and five, using a discriminatory price auction. This will be described as a QE operation. The central bank holds these bonds until the end of the experiment. However, as this operation changes neither the value of the bonds at maturity nor the equilibrium cash flows, the auctions are competitive and nobody can become a monopoly seller, the market price for bonds in the REE does not change.====In our second QE treatment (called Buy&Sell treatment), the central bank again buys a third of the bonds available in the market before periods four and five, but sells them back again in a reverse discriminatory auction before periods eight and nine. Thus, the participants are back where they started and again there is no change in the fundamental price.====Despite its simplicity, we find statistically significant evidence of mispricing in the Benchmark treatment and in the two QE treatments when participants are first exposed to the experiment. And the degree of mispricing is higher in the two QE treatments in the periods after the first intervention takes place.==== The participants then repeat the same treatment to which they were originally exposed twice more. By the third round, as shown in the literature following ====, participants in the Benchmark treatment have clearly realized that prices should not deviate from the fundamental price and the median price is essentially flat across the 11 periods of trading at this level (====, Obs.1).====Throughout the experiment, we collect forecasts of future market prices from the current period onwards from each participant. Thus, we have a rich data set on the beliefs of the participants. We report a very tight link between median forecasts and market prices and between past prices and current forecasts. Forecast dispersion across participants in each group that trades together shrinks very rapidly and although the participants do not know this (others’ price forecasts are not revealed to participants), these common priors condition the evolution of future prices.====An interesting feature of both QE treatments is that participants begin to anticipate higher prices from the start, particularly by the third round. Thus, it is not an inability to think forward that prevents the ==== from emerging. Instead, everyone comes to expect that the bonds will be priced high in the auctions.====Our experiment is most similar in motivation to ====. That paper considered share buybacks and share issues in a setup in which the fundamental price of an individual share should be independent of the quantity of outstanding shares. Thus, in REE, the market price should not be affected by these treatment operations. Nevertheless, the authors found that share prices rise after a buyback and fall after an issuance of shares, as we do in our experiment. ==== proposes two possible reasons for this result: (i) the downward sloping nature of the demand schedule for the bonds, and (ii) the change in the traders’ beliefs about the intrinsic value of shares owing to the change in the quantity of shares. In our experiment, the market demand schedule is downward sloping by construction, thus the first reason certainly applies. For the traders’ beliefs, although we cannot analyze their beliefs about “intrinsic value” of bonds, we do show that the higher the average price paid during the buy operation in Round 1, the higher the period 1 price expectation in Round 2. Thus, the interventions do affect participants expectations about market prices. The latter analysis was made possible through our multiple rounds experiment with forecast elicitation.====Furthermore, we depart from the approach of ==== and ====. Furthermore, in our experiment, market transactions are organized according to a call market, instead of a continuous double auction and, importantly, market interventions take place outside of the usual trading periods, according to an auction rule, with subjects being clearly informed about the exact manner in which interventions will take place. Our experiment setting also focuses on the direct effect of the asset purchases on prices and price forecasts and their interrelationship. The similar effects of interventions observed both in ==== and in our study, despite of all these differences in the experimental setups, suggest that the initial effects (i.e., before the repetition) of interventions are robust to these differences.====The remainder of the paper is organized as follows. ==== describes the experiment in more detail, ==== presents the main results, and section 4 concludes.",A quantitative easing experiment,https://www.sciencedirect.com/science/article/pii/S0165188920301469,29 August 2020,2020,Research Article,292.0
"Augustin Patrick,Saleh Fahad,Xu Haohua","Desautels Faculty of Management, McGill University, and Canadian Derivatives Institute, 1001 Sherbrooke Street West, Montréal, QC H3A 1G5, Canada,Wake Forest University, 1834 Wake Forest Road, Farrell Hall, Building 60, Winston-Salem, NC 27109, USA,Desautels Faculty of Management, McGill University, 1001 Sherbrooke Street West, Montréal, QC H3A 1G5, Canada","Received 19 November 2019, Revised 13 August 2020, Accepted 18 August 2020, Available online 22 August 2020, Version of Record 1 September 2020.",https://doi.org/10.1016/j.jedc.2020.103977,Cited by (2),We show that existing metrics of ,"What defines a financial return? At first reflection, this question seems trivial, especially in the context of financial securities, such as stocks or bonds. But a more sincere assessment evokes the sensation that the answer to this question is not as straightforward as it may appear. The ambiguity surrounding the definition of a financial return is especially pronounced in the context of credit default swaps (CDS), for which there exists significant divergence in empirical applications. Surveying the literature, we find at least four different methodologies to compute the return from buying or selling a CDS. Against the backdrop of this disagreement, we show that results from academic research, both qualitative and quantitative in nature, largely depend on the type of definition used for the computation of CDS returns. We find this important to highlight, especially given the growing use of CDS time series in empirical research.====Our first objective is to clarify the concept of CDS returns, and to parallel the different computations applied in the literature. In that context, we provide a simple approximation to the true CDS return, using as inputs CDS prices, rather than the conventionally quoted break-even credit swap spreads. This has become especially important since the regulatory overhaul instigated by the 2009 Big and Small Bang Protocols, which prescribes standardized insurance premium payments together with up-front cash flows settled between protection buyers and protection sellers. The main concern is that the change in regulation has altered the cash flow structure of CDS transactions, which increases the need for a cash flow-based return measure.====Our second objective is to illustrate that commonly used approximations of CDS returns, such as simple changes in CDS spreads, or their log differences, poorly approximate true CDS returns. In fact, we show that the time series correlation between simulated time series of approximated CDS returns and true CDS returns based on prices, are often below 20%. The simple approximation of CDS returns we propose, on the other hand, has a time series correlation of at least 99% with the true return series. Such stark differences become of paramount importance in the examination of the relation between returns on stocks and securities subject to credit risk (such as bonds, for example), for which CDS are often used as a first best approximation. Thus, we illustrate that the relation between stocks and CDS returns varies substantially across different CDS return definitions, and such differences vary as a function of firm leverage and asset volatility.====Taken at face value, our comments indicate a criticism of prior empirical work. Does this mean that earlier findings in the literature based on approximations of CDS returns should be dismissed? Certainly not! However, our examination emphasizes an important distinction that needs to be made among notions of CDS returns. The natural question that is implied is about the appropriateness of various CDS return approximations in different contexts. In a third instance, we thus provide some guidance for when researchers can rely on simple or percentage changes of CDS spreads, and when it is necessary to compute true CDS returns. We argue that the computation of true CDS returns is particularly relevant for studies that examine investment strategies and return performance, which critically depend on the cash flows attached to the underlying securities.====This paper is organized as follows. Section 2 describes the structure of a plain vanilla CDS contract and parallels different methods for computing CDS returns. Section 3 introduces a practically useful metric that approximates true CDS returns. Section 4 examines the relation among simulated CDS return approximations, and their relation with equity returns. In Section 5, we revisit the evidence from existing papers that use CDS returns. Section 6 concludes.",CDS Returns,https://www.sciencedirect.com/science/article/pii/S0165188920301457,22 August 2020,2020,Research Article,293.0
"Basak Suleyman,Makarov Dmitry,Shapiro Alex,Subrahmanyam Marti","London Business School and CEPR, UK,HSE University, International College of Economics and Finance, Russia,New York University, USA","Received 5 June 2020, Revised 10 August 2020, Accepted 18 August 2020, Available online 22 August 2020, Version of Record 30 August 2020.",https://doi.org/10.1016/j.jedc.2020.103976,Cited by (3),"This paper provides a status-based explanation for convertible securities. An entrepreneur with status concerns inducing risk-taking decides how to ==== the firm and how to dynamically manage it. Solving analytically for the optimal security, we find that it is substantially similar to a convertible security. Our model can explain why convertible securities are mainly issued by start-ups and small firms, as we show that their salient characteristics, higher volatility and dynamic flexibility, accentuate incentives to issue convertible securities. We also provide analytical results relevant to quantifying how status concerns affect credit risk, an established factor behind security choice.","Financial securities play a fundamental role in the economy by facilitating interaction between entrepreneurs, those with project ideas, and financiers, those who wish to invest their resources. There is a voluminous security design and financial contracting literature examining how security choice depends on various considerations affecting the entrepreneur’s or the financier’s decision-making (see Biais et al., 2013, and Sannikov, 2012 for excellent literature reviews.) While this literature has made substantial progress, it appears that some salient factors affecting security issuance have not yet been identified.====This paper introduces into a security design setting a feature that is universally considered to be a defining characteristic of entrepreneurs—their willingness to take risks in some situations (supporting evidence is discussed below). Throughout our analysis, we adopt the interpretation from a classic paper by Friedman and Savage (1948) that this behavior arises due to ==== when an agent’s wealth lies between levels associated with low and high status.====The idea is as follows. Status tends to increase in a discrete step when an individual purchases a good associated with higher status (e.g., a house in an upscale neighborhood). If one’s wealth is below the target level at which one can afford a high-status good but not by much, the desire to possess the good induces risk-taking because higher wealth volatility implies a higher chance to exceed the target wealth level. An equivalent way to understand the emergence of risk-taking incentives is that they stem from an increase in the marginal utility around the level of wealth permitting the purchase of a status good (as elaborated in Section 2.2). Though status concerns are a commonly-noted factor behind risk-taking, especially in the context of entrepreneurial activity (as discussed later), other reasons could be behind this behavior such as concern for firm survival.====Our contributions are as follows. We develop an analytically tractable dynamic framework for examining security design under non-standard preferences that capture status concerns via embedding both types of risk attitudes, risk seeking and aversion.==== Our model provides a status-based explanation for convertible securities, as we find that a status-driven entrepreneur issues a “convertible-like” security to finance her firm.====The novel contribution of our work relative to existing theories of convertible securities is that our model provides an explanation for why convertible securities are mainly issued by riskier and more flexible firms, such as start-up and small firms. We show that incentives to issue convertible securities are positively related to firm riskiness and its dynamic flexibility. When the entrepreneur does not participate in the financing decision, we characterize analytically how status concerns affect the firm value dynamics and demonstrate that the effect can be substantial. This uncovers another channel—pertaining to credit risk—through which status concerns can affect security design, as well as be relevant for the pricing of convertible securities (Das and Sundaram, 2007).====We now preview our model and main results in more detail. We consider a continuous-time complete-information security design framework, as in Cadenillas et al. (2007), in which an entrepreneur chooses how to finance her firm and how to dynamically manage its operation. The first decision involves choosing a financial security to be issued to a financier, which is a risk sharing rule that specifies how the future risky firm value is shared between the two parties. The second decision involves dynamically choosing the expected growth rate (“return”) and volatility of the firm value process (“risk”). The entrepreneur has status concerns and so, as explained above, she seeks risk when her wealth is between levels associated with low and high status, and is averse to risks when her status is low or high. The financier is risk averse; she buys the security from the entrepreneur if it provides her with the required reservation level of expected utility.====We solve analytically for the optimal security and find that it is considerably similar to a convertible security, in that it features distinct equity- and debt-like components. The optimal security without status concerns is equity-like, and so it is the debt-like component that emerges due to status concerns. The reason is as follows. The risk-taking incentives arising due to status concerns result in the entrepreneur’s increasing the firm riskiness when high status is in sight. To insulate the risk averse financier from this risk, the entrepreneur introduces a debt-like segment. The entrepreneur essentially caters to the financier’s risk preferences when designing the security because of the need to satisfy the financier’s participation condition.====Researchers have been seeking to identify factors behind the decision to issue convertible securities. Given the evidence that start-up and small companies rely on such securities more often than other companies (see Section 3.4), the high risk of a firm is often viewed as a possible driver (Brennan and Schwartz (1998)). Our model provides formal support for this view, as we find that the incentive to issue a convertible security become more pronounced when the firm volatility increases. As discussed above, a key mechanism in our model generating a convertible security is the need to protect the financier from a status-induced increase in firm riskiness, and this need becomes stronger when the firm is more volatile.====Another characteristic of start-up and small firms that can be related to the use of convertible securities is their dynamic flexibility, the ability to adjust their characteristics over time at relatively little cost. Indeed, (Biais and Casamata, 1999) point out that firms relying on convertibles tend to be those for which “the ability to switch to riskier ventures is large.” Motivated by this consideration, we examine the importance of this dynamic flexibility by studying how its absence affects the optimal security. We find that the optimal security in the resulting static model, in which the entrepreneur is not able to switch firm riskiness, is no longer similar to a convertible security. Hence, our model is consistent with Biais and Casamata’s point: The optimal security in the static model features a segment providing a negative exposure to firm risk (i.e., a short position), instead of the debt segment in the optimal security of the dynamic setting. This segment allows the entrepreneur to satisfy her desire to take risks when approaching high status even though she is not able to achieve this by making the firm value riskier. Just as offering a positive stake in the firm allows the firm owner to reduce her risk (the classical risk sharing notion), offering a negative stake leads to the opposite result. Hence, the static and dynamic solutions are quite different.====We then consider a modified set-up in which the status-driven entrepreneur faces just one choice: how to manage the firm over time, and does not decide on what security to issue. This analysis can be applicable to firms, presumably larger ones, in which managing a firm and financing it are separate tasks undertaken in different divisions. We explicitly characterize the entrepreneur’s dynamic strategy and find that, with status concerns, the firm volatility can substantially vary over time, while it is constant when status concerns are absent. Understanding the implications of time-varying firm volatility has been attracting growing attention (Choi, Richardson, 2016, Du, Elkamhi, Ericsson, 2018). The implications for security design also seem clear. Firm volatility and its dynamics are key inputs in structural credit risk modelling, and credit risk is, in turn, a well-documented factor affecting the process of security design and issuance.====Non-standard preferences are, by definition, less understood than standard ones, which makes the robustness of our main results to be a natural concern. We devote considerable attention to this issue in the Internet Appendix, in which we argue that our main results remain valid under alternative ways of modelling status concerns and under parameter values different from those considered in the main body of the paper.====It has long been recognized that people care about their status in society, and in particular about financial status (Frank, 1985, Heffetz, Frank, 2011). Informally, how much someone cares about status is likely to be related to how actively she pursues opportunities that can propel her to a higher status and, by this measure, entrepreneurs’ concern for status appears to be rather pronounced. There is considerable evidence supporting this point. According to the 2011 High Impact Entrepreneurship Global Report, a comprehensive cross-country study of entrepreneurship, the idea that successful entrepreneurs have high status has wide support among both entrepreneurs and non-entrepreneurs. Becker et al. (2005) argue that entrepreneurship as an activity is especially appealing in countries in which entrepreneurial success leads to high status. Begley and Tan (2001) provide empirical support for this argument. It is generally accepted that another specific feature of entrepreneurs, besides status concerns, is their willingness to take risks. Begley and Boyd (1987) find that status concerns (in their language, “need for achievement”) and risk-taking propensity are two of the three features distinguishing entrepreneurs from the rest (the third feature is tolerance of ambiguity; overconfidence is another trait associated with entrepreneurship, see Hayward et al. (2006)).====The term “entrepreneur” in this paper can refer not only to an individual person but also to an established company considering how to finance its operations. In this case, it is not clear whether aggregating (possibly heterogeneous) status concerns of the company’s multiple shareholders would lead to the objective function of the form considered in this paper. However, our model remains applicable as long as the company’s risk-taking incentives are analogous to those of our entrepreneur, which seems to be the case empirically. There is extensive research on organizational economics initiated by the influential work of Cyert and March (1963). It challenges the view that all complex interactions within companies can be reduced to the standard assumption of profit maximization. It is argued that companies, when deciding how much risk to take, consider their current performance relative to a certain aspiration level, a target that a company tries to achieve (see Audia and Greve, 2006 and the literature review therein). A common argument in this literature is that “managers seem to feel that risk taking is more warranted when faced with failure to meet targets than when targets were secure,” and that “executives ... would not take risks where a failure could jeopardize the survival of the firm” (March and Shapira, 1987). This pattern—taking risks when below but near the target, and avoiding risks when either above or well behind the target—mirrors the idea of Friedman and Savage used in this paper.====Though this behavior may arise for alternative reasons, status concerns can well be a factor. Companies’ important decisions, such as security issuance, are ultimately made by CEOs, and CEOs are likely to have pronounced status concerns. In addition to the obvious point that someone with little concern for status is not likely to become a CEO in the first place, there is also evidence direct evidence supporting this point.==== This is also consistent with survey findings that wealthier people, such as those in charge of security issuance, tend to care more about status (Dynan, Ravina, 2007, McBride, 2001).====Our paper contributes to the literature aiming to explain the use of convertible securities. A common theme of existing works is that convertible securities help to mitigate various agency problems, which typically arise under asymmetric information. In particular, convertible securities are shown to mitigate the asset substitution problem (Green, 1984), window-dressing behavior (Cornelli and Yosha, 2003), moral hazard in the presence of renegotiation (Dewatripont et al., 2003), inefficient investment (Schmidt, 2003), the underinvestment problem (Lyandres and Zhdanov, 2014), and other asymmetric information problems (Constantinides, Grundy, 1989, Repullo, Suarez, 2004, Stein, 1992), Hellmann (2006), (Chakraborty and Yilmaz, 2011). Our analysis shows that convertible securities also have an economic role under full information, as is the case in our model. Several studies in this area, such as (Bolton, Harris, Cadenillas, Cvitanic, Zapatero, 2007, Larsen, 2005), and (Miao and Zhang, 2015) consider, like us, settings without asymmetric information, but they do not explain the use of convertible securities.====More broadly, our work also contributes to the growing literature investigating the role of status concerns in various areas of economics and finance. Examples include (Auriol, Renault, 2008, Becker, Murphy, Werning, 2005, Besley, Ghatak, 2008, Dijk, Holmen, Kirchler, 2014, Georgarakos, Haliassos, Pasini, 2014, Moldovanu, Sela, Shi, 2007, Roussanov, 2010), and (Hong et al., 2014).====The remainder of the paper is organized as follows. Section 2 describes the model. Section 3 characterizes the optimal security, describes how the entrepreneur manages the firm, and relates the findings to empirical evidence. Section 4 characterizes the optimal security in a static setting, solves a model without security issuance decision, and discusses limitations and robustness. Section 5 concludes. Appendix A presents all proofs. The Internet Appendix elaborates on several points related to the analysis in the main body of the paper.",Security design with status concerns,https://www.sciencedirect.com/science/article/pii/S0165188920301445,22 August 2020,2020,Research Article,294.0
Zheng Huanhuan,"Huanhuan Zheng is from Lee Kuan Yew School of Public Policy, National University of Singapore (NUS), 469C Bukit Timah Rd, 259772 Singapore","Received 23 February 2020, Revised 8 August 2020, Accepted 10 August 2020, Available online 13 August 2020, Version of Record 14 October 2020.",https://doi.org/10.1016/j.jedc.2020.103974,Cited by (7),"In a market with information friction, investors strategically consider the actions of others and evolutionarily switch between fundamental and technical strategies to maximize their payoffs. The collective actions of all investors exert feedback on asset price, which affects investors’ subsequent actions. We find that investors have the incentive to adopt fundamental strategy to restore ==== only if the mispricing is sufficiently large. When investors fail to coordinate on the fundamental strategy, market inefficiency increases, which blows the bubble. As market inefficiency grows, the coordination on fundamental strategy strengthens, which eventually bursts the bubble.","Investors are motivated to trade based not on what the asset is worth, but on what they think other players will think it is worth, because the asset price reflects the aggregate actions of all market participants. When the market is booming, well-informed investors follow the bullish crowd rather than betting against the trend, even if they know clearly that the asset price has far exceeded its reasonable value. After all, the price will rise when the market is dominated by bullish buyers, which benefits buyers and hurt sellers. Well-informed investors have no incentive to restore the market efficiency unless they expect sufficiently large number of investors to do so. We introduce coordination game to model such strategic trading behavior and explore its implications on financial bubbles and crashes.====In our model, a continum of investors trade on one risky asset in a market with information friction. Each investor has two investment options. The first is fundamental strategy, which buys the asset when its price falls below the value and sells it otherwise. The second is technical strategy, which trades based on the historical price pattern, i.e., buys when the price has increased and sells when the price has declined. The co-existence of fundamental and technical strategy in the choice set reflects the fact that both strategies have been made increasingly accessible to the public by the mass media and technology====. Investors consider actions of others and strategically choose the investment option that maximizes their expected payoffs. Because the payoffs depend on the collective actions of all players, even if investors realize that the market is inefficient, they may not necessarily act on such information. Instead, they do best by riding on the trend if technical strategy dominates, and by driving the price towards the value if fundamental strategy prevails. Investors essentially play a coordination game and receive a payoff from the outcome of the game. Due to information friction, investors do not have common knowledge about the fundamental value, but rather observe noisy private signals. Divergent opinions on the fundamental value lead to different expectations of others’ actions and disagreement on the relative attractiveness of each strategy. While some investors expect the fundamental strategy to yield a higher payoff in the trading coordination game, others expect the opposite, which results in heterogeneous trading behavior. The aggregate actions of all investors have a feedback effect on the asset price, which in turn affects subsequent investment decisions.====The dynamic interaction between strategic trading behavior and asset prices yields two important theoretical findings. First, an investor prefers the fundamental strategy when his expected market inefficiency, the absolute difference between price and value, is sufficiently large, and adopts the technical strategy otherwise. When the price is relatively close to the value, the trading power of fundamental strategy, an increasing function of market inefficiency, is either too weak to move the market or too small to generate good profit, which motivates investors to choose the alternative strategy. This explains why well-informed investors are not willing to bet against the trend when the market is on the early stage of booming. However, when the market is sufficiently inefficient, the trading power of fundamental strategy becomes so large that it increases the likelihood of moving the price towards the value, which motivates investors to act on their information and restore market efficiency. Second, the coordinated trading behavior shapes the cycles of bubbles and crashes. In particular, when market inefficiency is moderate, the coordination on technical strategy increases the market inefficiency under certain conditions, and, in extreme cases, generates asset price bubbles. However, as the market becomes increasingly inefficient, the coordination on fundamental strategy strengthens. When the market inefficiency exceeds certain threshold, the coordination on fundamental strategy accumulates such strong trading power that it moves the price towards its fundamental value, which restores the market efficiency, and in extreme cases, bursts the bubble.====Our work also sheds light on the prudential regulations in financial markets. First, we show that greater information transparency enhances the coordination on fundamental strategy when the market is sufficiently inefficient, which accelerates the bust of bubbles. It implies that increasing information noise in an excessively overpriced environment is helpful for a smooth transition from bull market to bear market. Second, we find that the bubble could be transformed into depression when there is a significant negative shock to the fundamental value. It implies that strong economic fundamental is the key for effective buffer against market crashes.====Our model utilizes insights from two strands of literature, one on heterogeneous agent models (HAMs) and the other on coordination games. HAMs that incorporate fundamental and technical strategies to highlight the differences of investors’ beliefs are powerful in explaining various market abnormalities that are difficult to be justified by efficient market hypothesis (EMH). In particular, they can (i) analytically explain bubbles, crashes, and volatility clustering (He, Li, Wang, 2016, He, Westerhoff, 2005, Huang, Zheng, 2012, Huang, Zheng, Chia, 2010, Lux, 1995); (ii) simulate data that match well with stylized facts (De Grauwe, Grimaldi, 2006, Huang, Zheng, Chia, 2012, Li, Zheng, Chong, Zhang, 2016, Zheng, Chen, 2019); and (iii) provide empirical specifications that outperform random walk and many existing models (Chiarella, He, Huang, Zheng, 2012, de Jong, Verschoor, Zwinkels, 2010).==== In these studies, investors do not consider the impact of other players on their investment performance. However strategic expectations are critical for investment decisions as the asset price reflects the aggregate actions of all market participants. Sticking to fundamental strategy may be both costly and risky as it is unclear how long it takes for the mispricing to be corrected (Shleifer, Summers, 1990, Shleifer, Vishny, 1997). A well-informed investors could be better off not acting on the belief that the price will converge to the fundamental value, when the market is dominated by chartists whose trading activities magnify the mispricing.==== To account for such strategic actions, we introduce coordination game to this strand of literature. Motivated the adaptive market hypothesis (AMH) proposed by Lo (2004),==== we adopt the framework of fundamental and technical trading strategies commonly used in HAMs. Both strategies available to each investor. Each investor strategically choose the strategy that maximize his expected payoff, taking into considerations actions of others and market conditions. When information is complete, there exist two equilibria: an efficient equilibrium where all market participants are fundamentalists and the price reveals its fundamental value, and an inefficient equilibrium where all market participants are subject to behavioral bias and the price can deviate infinitely away from its value. While some HAMs avoid the multiplicity issue by specifying whether an agent is a fundamentalist or a chartist ex ante====, others introduce information friction to the market structure to pin down a unique equilibrium (He and Zheng (2016); Shi and Zheng (2018)). We apply the coordination game in a market with information friction to solve the unique trading action, considering the strategic actions of others. It provides a microfoundation to endogenize the switching among heterogeneous trading behavior in Brock and Hommes (1997) and their followers. In our framework, investors take different trading actions because of dispersed private signals that lead to heterogeneous expectations on actions of others. Each investors’ trading behavior evolves adaptively in response to changing market conditions. Their collective behavior shapes the market that switch evolutionary between efficiency and inefficiency.====The coordination games have been applied in studying speculative attacks in the credit, currency, debt markets and in the banking sector (Angeletos, Werning, 2006, Bebchuk, Goldstein, 2011, Carlsson, Damme, 1993, Goldstein, 2005, Goldstein, Ozdenoren, Yuan, 2011, Morris, Shin, 1998, Morris, Shin, 2002). In a typical setup, many agents simultaneously and strategically choose action or inaction conditional on some noisy and private signals. Such strategic coordination leads to binary outcomes: the status quo is abandoned if a sufficiently large mass of agents take actions against it and is maintained otherwise. Ozdenoren and Yuan (2008) documents that investors have the incentive to coordinate on buying a pre-specified number of risky asset, which feedbacks on the asset price and leads to excess volaitlity. Han et al. (2016) apply the coordination game to endogenize the noise trader decision. In particular, an agent strategically decide whether to enter the market as a noise trader or remain out of the market. We extend the application of coordination game to model adaptive investment behavior highlighted in Lo, 2004, Lo, 2012. Our setup specifies not only whether an agent will buy, sell, or hold, but also how many shares of the risky asset will be traded in the form of fundamental and technical strategy. It captures how agents’ action set varies with changing market conditions. We can thus analyze whether the market is efficient as well as the degree of market inefficiency. In such a coordination game with multiple actions and multiple outcomes, we find co-existence of strategic complementarity and substitution. Applying coordination game techniques to solve a model with heterogeneous actions and heterogeneous payoff functions are generally complicated (Choi, 2014, Frankel, Morris, Pauzner, 2003, Sakovics, Steiner, 2012). We overcome these difficulties by incorporating multiple actions in two trading strategies and multiple outcomes in a dynamic price function.====In terms of bubble growth and crashes, the coordination on fundamental strategy in our model plays a similar role as the “synchronization” or coordinate selling in Abreu and Brunnermeier (2003), the failure of which will sustain the market inefficiency, and the restoration of which will burst the bubble. The main difference lies in the way that trading heterogeneity is modeled. Abreu and Brunnermeier (2003) emphasize the time-series heterogeneity that arises from investors’ sequential awareness of the market inefficiency, we focus on the cross-sectional heterogeneity that originates from agents’ strategic considerations on actions of others. Agents in our model have diverse information about the market efficiency, but they do not necessarily act on such information in maximizing payoffs. Instead they trade strategically based on what they think how other players will trade. Such cross-sectional heterogeneity releases two important assumptions in Abreu and Brunnermeier (2003). First, it explains why bubble arises and why some rational arbitrageurs ride the bubble even if they are well informed of the overpricing and the eventual busts of the bubble. In this sense, this paper also complements Harrison and Kreps (1978) and DeMarzo et al. (2008). Investors are willing to pay more than the fundamental value because of the opportunities to sell the stock to other class of traders at a higher price (Harrison and Kreps, 1978), the wealth level relative to cohort which affects the competition for future opportunities (DeMarzo et al., 2008), and the expectation that sufficiently large number of others are going to trade like them and benefit their transactions in this paper. Second, it endogenizes the threshold of cumulative selling power, exceeding which the bubble will bust, which is exogeneously given in Abreu and Brunnermeier (2003). Some bubbles are more resilient than the others as market structure and conditions shift. We allow such a threshold to vary with the market environment, which adds insight on the magnification of bubbles and the transformation to other market states.====The remainder of this paper is organized as follows. Section 2 develops the model that unifies insights from coordination games and disagreement models. Section 3 presents the theoretical findings on strategic actions and market efficiency. Section 4 discusses the extensions of the model and their impacts on main results. Section 5 concludes.",Coordinated bubbles and crashes,https://www.sciencedirect.com/science/article/pii/S0165188920301421,13 August 2020,2020,Research Article,295.0
Raveendranathan Gajendran,"Department of Economics, McMaster University, Hamilton L8S 4M4, Canada","Received 7 July 2020, Accepted 14 July 2020, Available online 18 July 2020, Version of Record 31 July 2020.",https://doi.org/10.1016/j.jedc.2020.103964,Cited by (2),"I propose a model of ==== lines and targeted search to analyze what accounts for the profitability of the U.S. credit card ====. My analyses lead to two main findings. First, the search friction has minimal impact on the level of profitability of the credit card ====. Most of the profitability is a result of the lender choosing the terms of contract. Second, improved information about consumers accounts for the fall in profitability since the 1980s. Consistent with the data, lenders respond to more information by increasing credit card limits and lowering markups.","The U.S. credit card industry is characterized by profits that are significantly higher than the banking industry average. Ausubel (1991) documented that credit card issuers earned profits that were 3–5 times of the banking industry average in the 1980s. Since then, the profitability of the credit card industry has declined. The difference between the return on assets for large credit card banks and the banking industry average declined by 32% between 1990 and 2008 (Grodzicki, 2019). Profits among VISA credit card issuers decreased from more than 5 to almost 3% of outstanding credit between 1983 and 2001 (Evans and Schmalensee, 2005).====The question of this paper is what accounts for the level and changes in the profitability of the U.S. credit card industry. It is important to answer this question for the following reasons. First, while the literature has hypothesized that search costs and adverse selection might account for trends in profitability, these explanations have not been quantitatively explored. Second, the credit card industry is subject to stringent regulation. For example, the Credit CARD Act of 2009 placed various restrictions on interest rates, credit card limits, and over limit fees. A theory that accounts for profitability will help us in understanding how issuers might re-optimize and respond to regulation.====To answer the above question, I build on the consumer credit models of Livshits et al. (2007) and Chatterjee et al. (2007). These models have successfully matched key patterns related to the expansion of revolving credit. However, these models cannot be used to analyze implications for profitability because they assume perfectly competitive zero profit lenders. The theoretical contribution of this paper is to modify the standard consumer credit model by incorporating revolving credit lines and targeted search. In my model, a revolving credit line is a long-term contract specified by a credit card limit and a credit card interest premium. In this model, credit card issuers have market power because of two ingredients. First, they pick the terms of contract. That is, they have all the bargaining power. Second, there is a search friction in the credit card market. This weakens the consumer’s outside option.====My analyses of the model lead to two main findings. First, the lenders’ bargaining power accounts for most of the profitability in the early 2000s. That is, the quantitative impact of the search friction on profitability is small. Second, improved information about consumers jointly accounts for the decrease in profitability and the expansion in revolving credit since the 1980s.====Calibrating the model to the 2004 U.S. economy, I first show that the model accounts for aggregate credit statistics including variables related to profitability. For example, the model not only accounts for the level of credit and bankruptcies, but also profit to outstanding credit and credit card spreads. The model also accounts for cross-sectional credit statistics by income and age. For example, the model accounts for the fact that high income consumers are more likely to hold a credit card, have higher levels of outstanding credit, higher credit card limits, and lower credit card interest rates. The model also accounts for the hump shaped life cycle profile of outstanding credit, the population with outstanding credit, and consumer bankruptcies.====Having accounted for key empirical patterns, I perform two exercises. To understand what accounts for the level of profitability in the early 2000s, I compare the benchmark model with a counterfactual where I severely diminish the search friction. To do that, I set the cost of sending credit offers to an extremely small number. In this exercise, profits fall from 3.44 to 2.97% of outstanding credit. Hence, through the lenses of my model, the impact of the search friction on the profitability of the credit card industry is small. Most of the profitability is a result of the lender choosing the terms of contract.====For the second exercise, I compare the benchmark model, which assumes perfect information, with a model with private information. In this exercise, I show that improved information quantitatively accounts for the decline in profit to outstanding credit in the U.S. credit card industry. Improved information decreases profit to outstanding credit from 5.14 to 3.44% in the model compared to 5.30 to 3.20% in the data. More information eliminates the adverse selection problem for the lenders. Hence, they respond by issuing more contracts with higher credit limits and lower spreads (adjusting for default risk). This mechanism is supported by the data. The (cross-sectional) average credit card limit to disposable income per capita in the model increases by 22.03 percentage points compared to 18.39 percentage points in the data. The (cross-sectional) average credit card spread (adjusting for default risk) decreases by 1.72 percentage points in the model compared 2.41 percentage points in the data. Furthermore, the model also accounts for the expansion in credit and other associated patterns.====Finally, I compare the model to existing models in the literature. In particular, I compare the model to the standard consumer credit model from Livshits et al. (2007) and Chatterjee et al. (2007). There are several differences. As mentioned above, my model has implications for profitability, spreads over default risk (markups), and credit limits, which the standard model abstracts from. My model also has implications for extensive margin credit statistics such as the population with credit cards and the population with outstanding credit. Comparing my model to the standard model, I find that the search friction is important to account for extensive margin credit statistics. In the data, low income consumers and young consumers are less likely to hold a credit card or carry a balance on their credit card. My model endogenously accounts for this feature of the data. Credit card issuers send fewer credit offers to low income consumers as they are less profitable. It takes time for young consumers to gain credit access because of the search friction. The standard model of consumer credit fails to account for this feature of the data as it abstracts from a search friction.==== My paper contributes to the literature that analyzes the rise in revolving (and/or unsecured) credit and consumer bankruptcies in the United States, the literature that studies profitability and market power in the credit card industry, and the literature that proposes models of credit lines and long-term contracts.====Several papers have rigorously analyzed various explanations for the rise in revolving credit. Livshits et al. (2010) argue that lower stigma and increased competition account for the rise in credit and bankruptcies. Livshits et al. (2016) find that both lower fixed costs and lower asymmetric information jointly account for the rise in credit, bankruptcies, and associated empirical patterns in the U.S. credit card market. Drozd and Nosal (2008), Galenianos and Nosal (2016), and Herkenhoff (2019) analyze the role of lower fixed costs (or increased efficiency) of sending credit offers. Athreya et al. (2012), Narajabad (2012), and Drozd and Serrano-Padial (2017) focus on explanations based on improved information. While these papers have analyzed the expansion in revolving credit, they have not analyzed implications for profitability. I contribute to this literature by jointly analyzing implications for the expansion in revolving credit and the fall in profitability.====As mentioned above, Ausubel (1991) documented that credit card issuers made profits that were signficantly higher than the banking industry average. Recently, Grodzicki (2019) documented the decline in profitability of the credit card industry. Herkenhoff and Raveendranathan (2019) propose a model with a finite number of non-atomistic credit card firms that issue non-exclusive credit lines and study the heterogeneous welfare implications of competitive reforms in the credit card industry. I contribute to this literature by quantifying the role of search costs and adverse selection for the profitability of the credit card industry.====Finally, I contribute to the literatre that studies credit card contracts as credit lines rather than bond price schedules. Mateos-Planas and Ríos-Rull (2013) propose a model of credit lines to study the implications of regulations on interest rate hikes imposed by the CARD Act of 2009. Drozd and Kowalik (2019) propose a model of credit lines with promotional rates and step up rates, and study effects of the fall in promotional offers since the Great Recession on aggregate consumption. Braxton et al. (2018) study optimal unemployment insurance in a model with credit lines. Raveendranathan and Stefanidis (2020) study regulations imposed the Credit CARD Act of 2009 and the fall in revolving credit in the last decade. I contribute to this literature by showing that these class of models can account account for income and life cycle profiles of various credit card market variables. In particular, they account for extensive margin credit statistics that the standard consumer credit model fails to account for.",Revolving credit lines and targeted search,https://www.sciencedirect.com/science/article/pii/S0165188920301329,18 July 2020,2020,Research Article,296.0
"BERTINELLI Luisito,CARDI Olivier,RESTOUT Romain","University of Luxembourg, DEM, Faculty of Law, Economics and Finance. 162 A, avenue de la Faïencerie, L-1511 Luxembourg,Lancaster University Management School, Bailrigg, Lancaster LA1 4YX England,Université de Lorraine BETA (CNRS UMR 7522), Université de Strasbourg, CNRS, BETA, Nancy, 54000, France","Received 26 October 2019, Revised 5 May 2020, Accepted 24 May 2020, Available online 26 June 2020, Version of Record 8 July 2020.",https://doi.org/10.1016/j.jedc.2020.103938,Cited by (0)," relative to non-tradables vary across time, space and stages of the business cycle. More specifically, our evidence reveals that ==== of the relative wage and relative price of non-tradables with respect to relative productivity of tradables increase over time. Our estimates also show that the fall in the relative wage is more pronounced whilst the appreciation in the relative price is less in countries where labor markets are more regulated and during periods of recession. To rationalize the evidence, we differentiate between ","According to the Balassa (1964) and Samuelson (1964) (BS henceforth) effect, higher productivity in tradables relative to non-tradables puts upward pressure on the relative price of non-tradables and appreciates the real exchange rate. Despite the fact that the link between the relative price and relative productivity finds some strong support in the data, estimates at an individual level documented by Canzoneri et al. (1999), Kakkar (2003) and Chong et al. (2012) reveal that this relationship varies greatly across OECD countries. This link also varies across time as estimates by Bergin et al. (2006) indicate that the BS effect has gradually strengthened over time. In this paper, we disentangle labor mobility costs across sectors from hiring costs and show that this distinction is crucial when it comes to explaining the variations of the relative price effects of a productivity differential across time, space and stages of the business cycle.====Our paper contributes to a growing literature which has recently put forward labor market frictions to rationalize the estimated effect of higher relative productivity of tradables on relative prices. To account for the link between sectoral productivity and relative prices as implied by the BS model, Berka et al. (2018) consider shocks to the labor wedge which fuel inflation of tradables. Beyond the fact that Berka et al. (2018) highlight the terms of trade channel while we focus on movements in the relative price of non-tradables, the major difference with our approach is that the previous authors treat shocks to the labor wedge (resulting from unexplained labor market frictions) and shocks to sectoral TFPs separately. We model instead labor mobility costs and allow for search frictions so that hiring costs are endogenously determined by both labor market policies and the state of the economy in the business cycle; such labor market frictions determine the magnitude of the appreciation in the relative price of non-tradables following higher relative productivity.====In this regard, our work is complementary to Cardi and Restout (2015) analysis which reveals that labor mobility costs tend to curb inflation of non-tradables. However, by abstracting from search frictions in the labor market, the authors cannot disentangle workers’ mobility costs from hiring costs and thus neither can account for the cross-country dispersion in the relative price effects of higher relative productivity of tradables nor the time-varying effects. Our key contribution is to show that time-declining labor mobility costs can account for the time-increasing effects of a productivity differential we document empirically, while international differences in labor market regulation (LMR henceforth) and variations of hiring costs along the business cycle can rationalize estimated cross-country and state-dependent effects, respectively.====By using a panel of eighteen OECD countries, our estimates reveal that an increase in the relative productivity of tradables lowers significantly non-traded relative to traded wages which is consistent with the presence of labor mobility costs. When estimating elasticities of the relative wage and relative price of non-tradables with respect to relative productivity in rolling sub-samples, we find that the former has increased over time from -0.32 to -0.15, while the appreciation in the relative price appears to be more pronounced. Concomitantly, the magnitude of labor reallocation across sectors following higher relative productivity has almost doubled over the same period which suggests that time-increasing estimated elasticities are driven by time-declining labor mobility costs.====Hiring costs which emerge naturally in an environment with search frictions vary with LMR and across stages of the business cycle. Using a set of indicators to capture the extent of LMR, the decline in the relative wage is found empirically to be more pronounced and the appreciation in the relative price to be less in countries where the unemployment benefit scheme is more generous or the worker bargaining power (measured by the bargaining coverage) is larger. While the relative wage also falls more in countries where legal protection against dismissals is stricter, we find empirically that the relative price appreciates by a larger amount. Furthermore, when we differentiate the effects of a productivity differential according to the state of the economy in the business cycle, our estimates reveal that the decline in the relative wage is more pronounced while the relative price appreciates less during periods of recession.====While matching frictions cause search unemployment, labor mobility costs lead sectoral unemployment to adjust at different rates across sectors. Our estimates show that an increase in the relative productivity of tradables lowers the unemployment rate of tradables more than that of non-tradables and this decline turns out to be less pronounced over time. By affecting hiring costs, search frictions matter as well as we find that the fall in the unemployment differential between tradables and non-tradables is amplified in countries where LMR is higher or during recessions.====In order to account for our evidence, we put forward a variant of a two-sector open economy model with tradables and non-tradables and search in the labor market along with an endogenous labor force participation decision in the lines of Shi and Wen (1999). Like Alvarez and Shimer (2011), workers cannot switch sectors without going through a spell of search unemployment which gives rise to labor mobility costs. Since the elasticity of labor supply at the extensive margin measures the extent of job search costs, it determines the degree of labor mobility across sectors.==== Labor mobility costs resulting from an endogenous sectoral labor force participation decision are pivotal to our work since standard search frictions are not sufficient on their own to account for the decline in the relative wage we estimate empirically. Conversely, hiring costs resulting from search frictions determine the magnitude of the relative wage decline which varies with labor market institutions and across stages of the business cycle.====One key feature of our open economy model with search frictions is its dynamic nature. When workers experience mobility costs, higher relative productivity of tradables leads traded firms to post more job vacancies than non-traded firms in order to encourage workers to shift toward the traded sector. Because search frictions make hiring costly and labor mobility costs amplify recruitment expenditure, higher hirings give rise to a current account deficit along the transitional path. As the country must fulfill the intertemporal solvency condition, net exports must increase in the long-run. Higher demand for tradables mitigates the appreciation in the relative price of non-tradables caused by the increase in traded relative to non-traded output. The rise in net exports also biases labor demand toward the traded sector which drives down non-traded relative to traded wages and generates a greater decline in the unemployment rate of tradables than that of non-tradables, in line with the evidence. The dynamic nature of our setup resulting from search frictions plays a pivotal role since keeping net exports fixed prevents the model from matching the evidence when traded and non-traded goods are complements in consumption. With an elasticity of substitution between traded and non-traded goods smaller than one (as our estimates suggest), higher relative productivity of tradables increases the share of non-tradables. Because labor demand is biased toward the non-traded sector, both the relative wage of non-tradables and the unemployment differential between tradables and non-tradables increase instead of declining.====When we calibrate our model to a representative OECD economy and allow traded and non-traded goods to be complements, our quantitative analysis reveals that the long-run increase in net exports driven by the accelerated hiring process more than offsets the rise in the share of non-tradables. Higher demand for tradables lowers both the relative wage of non-tradables and the unemployment differential between tradables and non-tradables while the appreciation in the relative price is mitigated in line with our estimates. If we shut down search frictions, hiring costs vanish so that net exports remain fixed, thus preventing the model to account for the evidence.====When we control for the variations of LMR over time, we find that time-declining labor mobility costs alone can account for the time-increasing effects of higher relative productivity we document empirically. Intuitively, lower labor mobility costs mitigate the rise in hiring costs resulting from search frictions so that demand for goods and labor turns out to be less biased toward tradables because net exports increase less.====While labor mobility costs create an asymmetry across sectors, search frictions play a crucial role by mitigating or amplifying this asymmetry in sector adjustment. More specifically, search frictions give rise to hiring costs which vary with LMR and across stages of the business cycle. In an economy where unemployment benefits are more generous or the worker bargaining power is higher or during recessions, demand for goods and labor is further biased toward tradables which amplifies the decline in the relative wage of non-tradables and mitigates the relative price appreciation, in line with our evidence. Intuitively, an economy with higher LMR or in recession has more unemployed workers and fewer job vacancies. Because a low labor market tightness makes hiring more profitable, recruiting expenditure increases more following higher relative productivity, thus amplifying the current account deficit and thus the long-run increase in net exports. Our quantitative results also show that the relative price of non-tradables appreciates more while the relative wage declines by a larger amount in countries with stringent employment protection legislation (EPL henceforth) in accordance with our empirical findings. Like Hopenhayn and Rogerson (1993) and Veracierto (2008), the strictness of legal protection against dismissals is modelled as a tax on reducing employment. While higher productivity causes a fall in labor supply due to the positive wealth effect, traded employment increases and non-traded establishments are shrinking since productivity gains are concentrated in the traded sector. Non-traded firms are thus subject to the firing tax which further biases labor demand toward the traded sector. The greater increase in traded relative to non-traded output results in a greater appreciation in the relative price.====To further assess the role of search frictions, we calibrate the model to country-specific data and investigate the implications of labor market institutions for the cross-country dispersion in estimated effects. While the model generates a wide dispersion in the relative wage and the relative price responses across countries, we find quantitatively that it can account for the larger decline in the relative wage and the smaller appreciation in the relative price in countries where labor market regulation is higher.==== Our cross-country analysis also reveals that a productivity differential of one percent results in a decline in the relative unemployment rate of tradables which appears to be insignificant in countries having more flexible labor markets but ranging between twofold and fourfold of that obtained for a representative OECD country in economies with higher LMR.====The remainder of the paper is organized as follows. In section 2, we document evidence on the long-run effects of higher relative productivity of tradables and contrast these effects across time, space and stages of the business cycle. In section 3, we develop an open economy version of the two-sector model with both imperfect mobility of labor arising from searching efforts and unemployment arising from matching frictions in both sectors. Section 4 derives analytical results to guide our discussion on the role of labor mobility costs and LMR. In section 5, we conduct a quantitative analysis to assess the ability of our model to account for the variations of the effects across time, space and stages of the business cycle. Section 6 summarizes our main results and concludes. The Online Appendix provides a description of the dataset along with additional empirical results, and shows robustness checks.====. Our paper is at the cross-roads of three strands of the literature investigating the adjustment of open economies to structural shocks. First, it is closely related to the BS theory which has been renewed by Bergin et al. (2006), Ghironi and Melitz (2005), and Christopoulos et al. (2012). Whilst the latter paper puts forward financial frictions as an explanation of the cross-country dispersion in the BS effect, the former two papers show that heterogenous productivity among firms and/or entry and exit of firms amplifies the BS effect. Recently, Cardi and Restout (2015) and Berka et al. (2018) have put forward labor market frictions to account for the BS effect found in the data. However, the two aforementioned works abstract from search frictions and thus cannot disentangle labor mobility from hiring costs which prevent the aforementioned works to account for the cross-country and state-dependent effects we document empirically.====Our paper also adds to a fast growing literature which contrasts empirically and theoretically the response of output and unemployment to fiscal or tax shocks across stages of the business cycle, see e.g., Auerbach and Gorodnichenko (2012), Michaillat (2014). By producing an asymmetry in the size of hiring costs across stages of the business cycle, our model with search frictions allows us to rationalize the state-dependent effects we estimate.====Third, our work is also related to the literature employing a multi-sector model with search frictions in the labor market and emphasizing the key role of the costs of sectoral reallocation in shaping the response of the economy to sector-specific shocks. As in Lilien (1982), labor mobility costs tend to increase search unemployment before labor fully adjusts following asymmetric shocks across sectors. In contrast to Lilien (1982), reduced search for a job caused by the positive wealth effect lowers unemployment in both sectors since we allow for the transition between leisure and labor force. Like Kehoe et al. (2019), we find that the response of sectoral labor is influenced by the elasticity of substitution between traded and non-traded goods together with the cost of sectoral reallocation. In the same vein as Kambourov (2009) and Cosar (2013), we investigate the quantitative implications of labor market policies when workers experience barriers to labor mobility. Beyond the fact that the authors focus on trade shocks, a key dimension of our setup which is absent from that of Kambourov (2009) or Cosar (2013) who assume that trade is balanced, is the dynamics of the net foreign asset position which brings about a change in the composition of the demand of goods and allows our model to generate productivity effects in line with our empirical findings.",RELATIVE PRODUCTIVITY AND SEARCH UNEMPLOYMENT IN AN OPEN ECONOMY,https://www.sciencedirect.com/science/article/pii/S0165188920301068,26 June 2020,2020,Research Article,297.0
Bornstein Gideon,"The Wharton School, University of Pennsylvania, USA","Received 1 February 2020, Revised 22 June 2020, Accepted 23 June 2020, Available online 26 June 2020, Version of Record 20 July 2020.",https://doi.org/10.1016/j.jedc.2020.103963,Cited by (4),I construct a continuous-time model of strategic default and provide a numerical algorithm that solves it. I compare the results and computation times to standard discrete-time models of sovereign debt. The method proposed here is faster than discrete-time computation methods while obtaining similar quantitative results. The few differences between the models can all be attributed to a ,"One of the promising developments in macroeconomics during the past few years has been the application of continuous time methods to incomplete-markets models. Such methods have been applied in Brunnermeier and Sannikov (2014) to study financial frictions in a macroeconomic model, in Gabaix et al. (2016) to study the dynamics of inequality, and in Kaplan et al. (2016) to study monetary policy with heterogeneous agents. This paper extends these methods to problems of strategic default on unsecured debt. The contribution of this paper is twofold. First, I construct and solve a continuous time model which expands the quantitative capabilities of strategic default models as its solution is faster and can accommodate a large number of state variables. Second, I show that the solution of the model is very similar to its counterpart discrete time model, and that the few differences between the two can all be attributed to a ==== feature.====The benchmark model considered is a short-term debt model, a continuous-time version of Arellano (2008). A benevolent sovereign faces fluctuations in its domestic output and can save or borrow using short-term debt contracts. The stochastic process for the sovereign’s output is not continuous over time but features jumps. At any point in time, the sovereign can choose to default on its debt obligations. If the sovereign defaults, it is excluded from financial markets for a stochastic period of time, during which it suffers an output loss. Risk-neutral international investors with access to a risk-free world interest rate buy the sovereign bond. In addition to the risk-free interest rate, the bond carries a premium corresponding to the risk of default. The sovereign takes into account the interest rate schedule when making its debt issuance decision. I propose a numerical method to solve the sovereign’s problem, study the business cycle statistics of the economy, and compare them to the solution of the discrete time version of the model. Finally, I show how to extend the analysis to different environments, e.g., allowing the government to issue long-term maturity bonds.====I calibrate the benchmark model according to Arellano (2008) and study the differences between the business cycle statistics of the model in continuous time to the ones in discrete time. Both the discrete-time and continuous-time short-term debt models rely on the assumption that debt cannot be diluted. That is, sovereign bonds mature before more bonds can be issued. However, as opposed to discrete time short-term debt models, where the researcher can control the maturity length of sovereign bonds by choosing the length of a period in the model, short-term bonds in continuous time mature in high-frequency. So one drawback of modeling short-term sovereign debt in continuous time is that it abstracts from the maturity length of bonds. In the calibrations considered, I find that this drawback is rather minor as the quantitative results of the continuous-time and discrete-time models are quite similar.====While business cycle statistics of the discrete-time Arellano (2008) model and its continuous-time counterpart are overall similar, there are two notable differences regarding the behavior of the trade balance and of spreads. In continuous time, the trade balance is less counter-cyclical and the average spread is lower. These differences are not driven by the different maturity length of sovereign bonds, but rather by one main feature of the continuous time model - painful deleveraging.====The main difference between discrete-time models of sovereign debt and the models presented in this paper is that in the latter, the deleveraging process is more costly. Consider a negative endowment shock which increases the sovereign’s risk of default, as well as the spread, holding fixed the sovereign’s debt level. In the discrete-time environment, the sovereign can immediately decrease its stock of debt which needs to be payed in the following period. This action decreases the probability of default, so that the sovereign does not face the high equilibrium spreads which it would if it kept its level of debt unchanged. In the continuous time version of the model, on the other hand, a negative jump in the stock of debt is not feasible. The negative endowment shock moves the sovereign into a region of higher spreads from which it cannot instantaneously escape. Deleveraging in continuous time is painful both because the sovereign faces higher spreads during the deleveraging process, and because quick deleveraging can only be done by cutting consumption considerably.====Painful deleveraging alters equilibrium outcomes along several dimensions. The continuous-time sovereign==== is worse off at any point on the state space in which the discrete-time sovereign chooses to decrease its debt obligations. Denote by ==== the set of points on the state space for which the sovereign is indifferent between defaulting and repaying its debt. Whenever the discrete-time sovereign is indifferent between defaulting and deleveraging, the continuous time sovereign prefers to default. For low and medium endowment levels, the discrete-time sovereign chooses not to deleverage on the default frontier, and is constrained to hold its level of debt fixed.==== For high endowment levels, the discrete-time sovereign chooses to deleverage when it is on the default frontier. Therefore, the default region of both sovereigns is similar for low and medium endowment levels but it is larger for the continuous-time sovereign for high endowment levels. In addition, policy functions of the sovereign in the two frameworks are remarkably similar for low and medium levels of endowment. For high levels of endowment and small levels of debt, both sovereigns choose to increase their debt. However, the discrete-time sovereign does so faster as the continuous-time sovereign has a lower incentive to reach a high level of debt associated with a high spread. So the continuous-time sovereign exhibits lower counter-cyclicality of the trade balance.====Since deleveraging is costly, the continuous-time sovereign tries to avoid regions of the state space associated with high spreads. This cautious behavior leads to a lower average spread in the continuous-time framework. However, negative endowment shocks do occasionally move the sovereign to regions of high spread from which it cannot immediately escape. So while the average spread is lower, the volatility of spreads is similar to the discrete-time environment. This implies that the coefficient of variation, the ratio between the volatility of the spread to its mean, is higher in the continuous-time environment. As Aguiar et al. (2016) explains, matching the high volatility of the spread in the data as well as the low probability of default is a difficult challenge for quantitative sovereign debt models. The continuous time environment helps overcoming the challenge for the same reason that deleveraging is costly. Avoiding high spreads following a negative endowment shock by instantaneously cutting the level of debt is infeasible.====It is important to note that the continuous-time version of the Arellano (2008) economy I study is not the limiting economy when taking the length of a period in the discrete-time model to zero. This is because I model the endowment process of the sovereign as a compound Poisson process rather than a Brownian motion. This modeling choice gives rise to defaults along the equilibrium path as in Arellano (2008).==== The methodological contribution of this paper is to construct a continuous-time model that can be solved in a short amount of time and that its equilibrium is similar, both qualitatively and quantitatively, to workhorse discrete-time sovereign debt models. Nonetheless, the numerical methodology accommodates also a Brownian Motion for the endowment process of the sovereign.====The numerical method proposed takes advantage of sparse matrix operations. It is between two to fifty times faster than discrete time solution methods, depending on the density of the grid. It’s comparative advantage is in analyzing a dense asset grid as it does not add many non-zero elements to the sparse matrix used in computations. There is an additional computational advantage when solving long-term debt models. In discrete-time models, it is notoriously difficult to compute the interest rate schedule as it involves solving a fixed point problem. Chatterjee and Eyigungor (2012) discuss this difficulty and show how in order for the value functions to converge one needs to add an additional state variable to the problem, a noise to the endowment process of the sovereign. In the continuous time framework, on the other hand, finding the interest rate schedule is a simple task as it boils down to a simple sparse matrix inversion. Finally, I show how one can use the Kolmogorov Forward equations to obtain the ergodic distribution of the model. The Kolmogorov Forward equations have an additional role in the context of heterogeneous agents, e.g., in models of consumer bankruptcy. They can be used to track the evolution of assets traded by all agents.====In the final part of the paper, I solve a continuous-time version of the long-term debt model of Chatterjee and Eyigungor (2012). The quantitative results of the continuous-time and discrete-time models are very similar. With long-term maturity bonds, deleveraging is less costly than in the benchmark model because acquiring back debt is cheap when the spread is high. While low prices of long-term debt in bad states provide a welfare benefit also in lower debt-issuance frequencies,==== this benefit is larger in the continuous-time framework. Under Chatterjee and Eyigungor (2012) calibration, I find that the optimal debt maturity in the continuous-time framework is slightly longer than in the discrete-time framework. In discrete time, the optimal debt maturity is the short-term one, a one-quarter bond. In continuous time the optimal maturity length of the bond is, on average, 1.2 quarters.====This paper is related to several strands of the literature. First, it is related to the theoretical and quantitative literature on sovereign debt, building on the seminal work of Eaton and Gersovitz (1981). It includes Arellano (2008) and Aguiar and Gopinath (2006) for short-term debt, and Hatchondo and Martinez (2009), Arellano and Ramanarayanan (2012), Chatterjee and Eyigungor (2012), and Sánchez et al. (2018) for long-term debt. This paper is also related to models of consumer bankruptcy such as Chatterjee et al. (2007), Livshits et al. (2007), and Mitman (2016). While focusing on sovereign debt, the methods presented in this paper can also accommodate general equilibrium models of consumer bankruptcy.====I build upon the work of Achdou et al. (2020), which shows how to solve incomplete-markets heterogeneous-agents models in continuous time. They study an environment without default and impose an exogenous borrowing limit, which is independent of the current endowment of the agent. In contrast, I study an environment with a strategic default decision that implies an endogenous borrowing limit on the sovereign.====This paper is not the first one to study models of sovereign debt in continuous time. Nuño and Thomas (2019) studies the effects of monetary policy in a sovereign debt model using a continuous time framework, and Tourre (2017) studies the behavior of bond spreads with a non-separable utility function. Both papers use a Brownian motion for the exogenous process of the sovereign’s endowment. While this assumption makes the analysis simpler, as a smooth pasting condition can be used on the default frontier, it makes the comparison to standard discrete time models difficult. For instance, these papers can only study long-term debt as there are no defaults in equilibrium if debt is of short maturity.==== The model presented in this paper can accommodate short-term debt, and the types of defaults are similar to discrete-time models. Both Nuño and Thomas (2019) and Tourre (2017) do not compare the results of their models to ones they would have obtained in a discrete-time environment. Compared to Nuño and Thomas (2019) and Tourre (2017), this paper can be seen as a bridge for understanding the underlying mechanism and differences between continuous- and discrete-time models of sovereign debt.====Aguiar et al. (2015) use a continuous-time environment to study fiscal and monetary policy in a monetary union with the potential of rollover crises on sovereign debt. As in the benchmark model I consider, they study short-term maturity bonds. Since no exogenous variable in their model jumps over time, in the absence of rollover risk the equilibrium does not feature sovereign defaults. They adopt the timing convention of Cole and Kehoe (2000), which gives rise to rollover crises–the source of sovereign defaults in their model. In contrast to them, I abstract from rollover risk and study strategic sovereign defaults in the spirit of Eaton and Gersovitz (1981).====Lorenzoni and Werning (2019) and Aguiar and Amador (2019) use a continuous-time environment to study the existence of multiple equilibria in the long-term sovereign debt model. Lorenzoni and Werning (2019) study self-fulfilling equilibria in which high interest rate spreads, due to the fear of a future default, lead to a gradual but faster accumulation of debt, ultimately validating investors’ fear. Aguiar and Amador (2019) analyze the necessary condition for such multiplicity to arise, and study whether “lender of last resort” policies can select a particular equilibrium. Similar to this paper, the income shock these papers consider includes a Poisson process.==== Differently from this paper, in Lorenzoni and Werning (2019), upon arrival of the Poisson shock, all uncertainty is resolved. And in Aguiar and Amador (2019), the sovereign is assumed to be risk-neutral.====The paper proceeds as follows. Section 2 presents the benchmark model of short term debt. Section 3 contains the numerical method that solves the model. The calibration of the model, its equilibrium, and the concept of painful deleveraging are presented in Section 4. Section 5 considers the two extensions of the benchmark model, world interest rate fluctuations and long term debt. Section 6 concludes.",A Continuous-Time Model of Sovereign Debt,https://www.sciencedirect.com/science/article/pii/S0165188920301317,26 June 2020,2020,Research Article,298.0
"Han Xing,Li Kai,Li Youwei","University of Auckland Business School, Auckland 1010, New Zealand,Department of Applied Finance, Macquarie University, NSW 2109, Australia,Institute of Financial Studies, Southwestern University of Finance and Economics, Chendu, China,Hull University Business School, University of Hull, Cottingham Rd, HU6 7RX Hull, United Kingdom","Received 31 December 2019, Revised 3 June 2020, Accepted 17 June 2020, Available online 20 June 2020, Version of Record 30 June 2020.",https://doi.org/10.1016/j.jedc.2020.103961,Cited by (19),"This paper documents a highly downward-sloping security market line (SML) in China, which is more puzzling than the typical “flattened” SML in the US, and does not reconcile with existing theories of the low-beta anomaly. We show that investor overconfidence offers some promises in resolving the puzzle in China: In the time-series dimension, the slope of the SML becomes more “inverted” when investors get more overconfident. This ==== overconfidence effect is intensified with biased self-attribution. As a general symptom of overconfidence in the cross section, high-beta stocks are also the mostly heavily traded. After accounting for trading volume, there is no longer the low-beta anomaly at both the firm and portfolio levels. Mutual fund evidence reinforces the view that institutional investors actively exploit the portfolio implications of a downward-sloping SML by shying away from high-beta stocks and betting on low-beta stocks for superior performance.","In theory, the Sharpe (1964) and Lintner (1965) capital asset pricing model (CAPM) posits an ==== security market line (SML). That is, differences in expected returns are compensations for different degrees of systematic risk (====, market beta). However, for decades, the empirical evidence has proven more complicated than the theoretical projection: Market beta is “unpriced” at the ==== level, reflecting a typical “flattened” SML in the US (Fama and French 1992).==== At the ==== level, low-beta stocks tend to outperform their high-beta counterparts on a risk-adjusted basis, leading to the so-called ==== (Friend and Blume 1970; Haugen and Heins 1975; Baker et al., 2011).====This paper examines the ==== in the context of the Chinese stock market, the largest emerging financial market in the world. Carpenter et al. (2020) highlight the importance of exploring China's stock market and its role in fuelling the growth of the world's second-largest economy (====, resource allocation). Liu et al. (2019) stress that it is crucial to allow for the unique features in understanding factor models in China. Following their leads, we shed new light on the CAPM model in China, the cornerstone of asset pricing. Estimating the (empirical) shape of the SML is not only crucial to our understanding of the finance theory, but also serves a number of practical purposes such as evaluating the cost of equity capital of a firm and developing investment strategies.====We document a striking “downward-sloping” SML in China, which is more puzzling than the typical “flattened” SML in the US. The slope coefficient of the SML in China has a negative value of −2.68, which is significant at the 1% level. Therefore, low-beta stocks outperform high-beta stocks on an ==== basis. The betting against beta (BAB) strategy is more profitable in China than in the US: The annualized Sharpe ratio of BAB is 0.99 in China, comparing to 0.66 in the US over the same sample period. Moreover, given the nature of an inverted SML, the BAB strategy is highly exploitable even for the most leverage-constrained investors in China (====, mutual funds and retail investors).====In principle, the strongly negative slope of the SML in China cannot be easily reconciled with existing theories of the low-risk anomaly that usually attribute to a certain type of constraints (Black 1972; Baker et al., 2011; Frazzini and Pedersen 2014; Schneider et al., 2015; Bali et al., 2017; Liu et al., 2018). These constraints can “flatten” the SML, but are unlikely to flip the “sign” of the slope of the SML (Black 1972; Jylhä 2018), implying that there might be other economic mechanism(s) at work which has not been fully explored.====Recently, Daniel and Hirshleifer (2015) conjecture that investor overconfidence, manifested by excessive trading, provide a natural explanation for the betting against beta effect. Similar notion is also expressed in Baker et al. (2011).==== Following these leads, we examine whether overconfidence can explain the puzzling SML in China. Intuitively, overconfidence may cause investors to underestimate risks and result in a higher-than-normal demand for speculative assets, especially for high-risk firms. Besides, there are also good reasons to believe that investor overconfidence is a key feature in the context of emerging markets: First, these markets are typically dominated by unsophisticated individual investors who tend to be too confident about their private information or trading skills (Han and Li 2017). Second, there are strong policy uncertainties caused by the regulatory body in emerging markets and such uncertainties tend to be underestimated by individual investors.====In the time-series dimension, we provide consistent evidence that investor overconfidence is able to resolve the negative slope of the SML observed in China, complementing the existing explanations for the beta anomaly. Using market turnover as a proxy for investor overconfidence and carefully controlling for other possible economic mechanisms, we find that following high degree of investor overconfidence, the slope of the SML becomes more “inverted” while the intercept of the SML gets more pronounced.==== We show that a one-standard-deviation shock in turnover ratio leads to a downward adjustment of 1.84% for the unit-beta portfolio (====, the slope of the SML), and an upward adjustment of 2.05% for the zero-beta portfolio (====, the intercept of the SML) after accounting for other economic mechanisms. Moreover, we also document a ==== overconfidence effect as investor overconfidence is amplified by self-attribution bias (Gervais and Odean 2001). Using prior market performance as a proxy for self-attribution bias, we find that the SML gets more inverted when investors become more overconfident due to biased self-attribution.====In the cross-sectional dimension, we further explore whether firm-level trading volume could also explain the low-beta anomaly in China. Prior empirical work suggests that investors who are overly confident about their information or skills tend to trade the most (Odean 1999; Barber and Odean 2000). These excessive trading, a symptom of overconfidence, leads to poor investment performance over time (Barber and Odean 2000; Grinblatt and Keloharju 2009). In that sense, stocks with the highest turnover ratio are mostly likely the assets that overconfident investors have “passions” about. Motivated by these works, we perform an extensive, firm-level “horse race” using the refined asset pricing test framework proposed in Hou and Loh (2016), which could differentiate the competing expiations on the low-beta anomaly. Results from the “horse race” seems to weigh more on the overconfidence-based explanation: Among all the (potential) economic mechanisms, the low-beta anomaly seems to be fully captured by the volume effect (====, turnover ratio). The findings are robust at the portfolio level as well: Once we control for stock turnover, there is no longer a low-beta effect (in the bivariate portfolio sorts).====Finally, we explore the low-beta anomaly with mutual fund data in China. A key difference between an inverted SML and a “flattened” SML lies in its portfolio implication. In a market with a “flattened” SML, more constrained investors tend to hold high-beta stocks (Frazzini and Pedersen 2014). In a market with a downward-sloping SML, however, even the most leverage-constrained investors (such as mutual funds) could exploit the low-beta anomaly by tilting towards low-beta stocks without suffering “benchmark as limits to arbitrage” (Baker et al., 2011). This is confirmed by our observations that the profits (====, alphas) of the BAB strategy in China stem mainly from the long leg of the portfolio.==== More interestingly, we find that professional fund managers in China actively engage in low beta strategy: After accounting for other well-known investment styles, it is clear that some fund managers actively exploit the low-beta anomaly by shying away from high-beta stocks and betting on low-beta stocks for superior performance.====The structure of the paper is as follows. Section 2 documents the sample data and data sources. Section 3 describes the empirical shapes of the SML in China and in the US. Section 4 presents an illustrative model augmented with investor overconfidence to explain the puzzling SML in China. Section 5 provides time-series evidence and tests the time-series predictions of overconfidence suggested by the theoretical model. Section 6 provides cross-sectional evidence, including the BAB strategy, firm characteristics, and the performance of the beta-sorted decile portfolios. Section 7 performs further analyses and robustness checks, including the firm-level “horse race” and bivariate portfolio sorts. Section 8 explores the portfolio implications of the negatively sloped SML in China with mutual fund evidence. Section 9 concludes.",Investor overconfidence and the security market line: New evidence from China,https://www.sciencedirect.com/science/article/pii/S0165188920301299,20 June 2020,2020,Research Article,299.0
"Lioui Abraham,Tarelli Andrea","EDHEC Business School France,Catholic University of Milan Italy","Received 4 February 2020, Revised 14 May 2020, Accepted 17 June 2020, Available online 20 June 2020, Version of Record 2 July 2020.",https://doi.org/10.1016/j.jedc.2020.103960,Cited by (4),"Anomaly-based long/short benchmarks are typically built from portfolios double-sorted on size and one additional characteristic, applying simple fixed-weights schemes. Characteristic-based portfolios show significant time variations of their abnormal returns (alphas) and market exposures (betas). While timing alphas is challenging, a long-term risk-averse investor benefits from implementing dynamic weighting schemes that account for the time variation of the betas of the portfolios. Particularly for long investment horizons, significant out-of-sample Sharpe ratio improvements and utility gains with respect to fixed-weights factor benchmarks are recorded using portfolios sorted on size, value, operating profitability, investment and momentum.","Since the seminal contribution of Fama and French (1993), the asset pricing literature has experienced an extensive activity aimed at discovering new pricing factors and market anomalies. Harvey et al. (2016) and Hou et al. (2020) examine hundreds of such pricing factors, criticizing on their actual economic magnitude. The abundance of factors has raised several concerns. The first relates to the incremental contribution of new anomalies, leading in the recent years to the development of new methodologies to discriminate across factors (e.g. Barillas and Shanken (2017), Barillas and Shanken (2018), Fama and French (2016), Fama and French (2018), Giglio et al. (2020), Gospodinov et al. (2017), Hou et al. (2015), Hou et al. (2019), Pukthuanthong et al. (2018)). Another issue is the resilience of the risk premia carried by the factors. McLean and Pontiff (2016), Jacobs and Müller (2020) and Hou et al. (2020) show that many anomalies are either disappearing or particularly sensitive to the economic conditions. Jones and Pomorski (2016) study how a myopic mean-variance investor can exploit decaying abnormal returns.====The potential for the timing of market anomalies, and the corresponding added value for investors, have been first highlighted in Guidolin, Timmermann, 2007, Guidolin, Timmermann, 2008. The time variation in factor expected abnormal returns (the CAPM ==== of these factors), as well as in their market exposure (the CAPM ====) is substantial. If the expected abnormal returns of anomaly-based portfolios are time varying, then, similarly to the equity market risk premium, rational investors should try to time the pricing factors. The following graphs show the CAPM 5-year rolling-window monthly ==== and ==== for the four long/short factors by Fama and French (2015) and the momentum factor by Carhart (1997).====As can be noticed, contrary to the widespread wisdom in the investment profession, factor investing is far from being an arbitrage opportunity, as the realized alphas show a significant time variation and are often negative. In the same way, factor returns are not nearly market neutral, showing large time variations of their market betas. This has consequences for the construction of anomaly-based portfolios, and the impact may be different for short- or long-term investors. This paper addresses these issues.====The construction methodology of the factors adopted by Fama and French (1993) and Fama and French (2015) is now standard in the literature. Stocks are double-sorted based on their size and another characteristic, and then long-short portfolios are constructed aiming at capturing the risk premium attached to the size effect and the characteristic. For example, the HML factor, also named the value factor, is obtained by double-sorting stocks into two quantiles of market capitalization (small and big) and three quantiles of book-to-market ratio (low, medium and high). HML is constructed by attributing a fixed long position of 50% to both the small and big portfolios with a high book-to-market ratio, and a short position of ==== to the small and big portfolios with a low book-to-market ratio. This fixed-weights approach does not exploit the time variation of the abnormal returns and market exposures of these portfolios. Our first objective is to build optimal long/short portfolios outperforming the fixed-weights factors from the perspective of a rational investor.====Another important drawback of the fixed-weights approach is that it ignores that agents may have different investment horizons. In other words, a long-term investor is affected by the time variation of the abnormal returns and the market exposures of the portfolios differently from a short-term investor. Should the same long/short factor portfolio fit all investors? Daniel et al. (2020) propose an asset pricing model based on investor psychology and augmented with factors that capture long- and short-horizon mispricings. We complement their behavioral perspective through an investigation of horizon-dependent rational factor construction.====Our contribution is both methodological and empirical. We set-up a factor-based dynamic asset allocation model with time-varying expected abnormal returns and market exposures, showing how to solve for the optimal dynamic portfolio strategy in the presence of weight constraints. From the empirical perspective, starting from portfolios sorted by characteristics, we use this framework to build optimal horizon-dependent long/short factor portfolios. We then compare their out-of-sample performance to the corresponding fixed-weights benchmarks.====In our setting, the market exposures (betas) and the abnormal returns (alphas) depend on the characteristics of the stocks underlying the investable portfolios. We document a significant degree of predictability of the betas of the characteristic-based portfolios by observing business-cycle related variables, which also entails that the volatility of portfolio returns is stochastic. While we observe some degree of predictability also for the alphas, this appears to be weaker. Using four sets of characteristic-based double-sorted portfolios, we build dynamic long/short strategies assuming constant alphas and betas (C====-C====), time-varying alpha and constant betas (TV====-C====), constant alphas and time-varying betas (C====-TV====), and time-varying alphas and betas (TV====-TV====). The strategy C====-C==== allows to better control the market exposures and expected abnormal returns than the fixed-weights factors, providing in most cases higher out-of-sample Sharpe ratios and certainty equivalents, in particular for long investment horizons (5 to 10 years). The strategy C====-TV====, by accounting also for the conditional time variation of the betas, outperforms the C====-C==== strategy across the board. The turnover of these strategies is in line with that of the long/short benchmarks. Conversely, and consistent with the intuition derived from the weak predictability of the alphas, the strategies accounting for time-varying alphas show large time variations of the portfolio weights and underperform the strategies based on the assumption of constant alphas in most cases. Our main finding is thus that strategies accounting for the time variation of the betas outperform the benchmarks based on one specific characteristic. The economic gain is especially large for two of the most traded anomalies: value and momentum. Additionally, our time-varying-beta strategies show solid performance also when the investable universe is enlarged to encompass portfolios sorted by different characteristics.====Our analysis is closely related to Guidolin, Timmermann, 2007, Guidolin, Timmermann, 2008 in that we also implement a dynamic allocation problem where the investor attempts to time anomaly-based portfolios. However, in their first contribution they consider long-only multi-asset-class portfolios with stocks sorted by size only, while in the second contribution they consider an investment in the market portfolio and two pre-built size and value factors. Differently, we focus on building enhanced long/short factor portfolios. In particular, we consider a long-term investor having access to a set of 6 portfolios double-sorted by size and one further characteristic (value, investment, operating profitability and momentum) and determine the optimal allocation, comparing it to the benchmark fixed-weights one. Another difference is that, while they use a regime-switching approach where the dividend yield can help predicting the current state of the economy, we employ two variables (dividend yield and default spread) to predict the conditional expected abnormal returns and market exposures of the assets. Both approaches entail that the asset volatility is stochastic, which is a significant technical challenge, but in their setting it stems from the existence of different volatility regimes, while in our setting it is due to the time variation of the market exposures. Finally, a common challenge is assessing the out-of-sample performance of the portfolio strategies. While they perform this analysis only for a buy-and-hold investor, we extend this analysis to a dynamic investor that rebalances the factor portfolios throughout the investment horizon due to: i) the information provided by the state variables; and, ii) the variation in hedging demands caused by the shortening of the investment horizon. We document that accounting for the first effect has a significantly higher economic value than for the second.====Our paper is related to the literature on conditional asset pricing models, where the factor loadings are allowed to be time varying. Some works assume that the time variation is not related to observable variables: for example Adrian and Franzoni (2009) and Koundouri et al. (2016) postulate auto-regressive processes for the betas, Bali et al. (2017) use a dynamic conditional correlation (DCC) approach, while Alexeev et al. (2017) use high-frequency data to distinguish time-varying betas associated with continuous systematic risk and jump risk. Lewellen and Nagel (2006) and Ang and Kristensen (2012) use non-parametric approaches to capture the time variation of the betas, finding that this is explained by business-cycle-related observable variables, such as the dividend yield and the default spread. These findings support a three-decade-long literature, starting from the early contributions by Shanken (1990), Ferson and Harvey (1991) and Jagannathan and Wang (1996), who explicitly condition the factor loadings on observable quantities. González et al. (2012) use a mixed-data sampling (MIDAS) approach to estimate conditional betas with respect to innovations in market returns, risk factors and macroeconomic variables. Baele and Londono (2013) use a DCC-MIDAS approach and also find evidence for a business-cycle exposure of industry betas. Fama and French (1997) instead condition the industry betas to observable firm characteristics, which can be time varying. Similarly, Kelly et al. (2019) focus on a wide set of observable firm characteristics, which drive the loadings onto a parsimonious set of latent factors, finding that only few characteristics are statistically significant. In our specification, which needs to be tractable in a dynamic asset allocation framework, the tradable portfolios are already sorted by stock characteristics. We allow for their market loadings and expected abnormal returns to time-vary linearly in business-cycle variables.====We contribute to the asset allocation literature by assessing the impact and the economic value of the predictability of alphas and betas for a long-term investor. While this issue has been addressed by several authors in a one-period setting (e.g. Avramov and Chordia (2006)), it has not been studied in the dynamic asset allocation literature (see the review in Wachter (2010) and the references in Branger et al. (2019)), where the processes of the asset returns do not typically consider an underlying factor model with time-varying loadings. This specification causes a technical complication due to the induced time variation of the volatilities of asset returns. Indeed, even if the underlying factors (e.g. market returns) have constant volatilities, the asset return volatilities are stochastic because of the time variation of the betas. It is well known that in this context analytical solutions are not available and that it is necessary to resort to approximations (e.g. Chacko and Viceira (2005), Hasler et al. (2019)) or numerical techniques (e.g. Brennan et al. (1997), Branger et al. (2019)). Because of the additional imposition of reasonable portfolio constraints, we adopt the second approach.====Finally, while out-of-sample performance evaluations are common in the one-period portfolio choice literature, as they are very informative and robust to model misspecifications and estimation errors, they are technically challenging and not usually performed in long-term dynamic portfolio choice works. Some remarkable exceptions can be found in Diris et al. (2014) and Lan (2015), who however consider limited asset universes (respectively 2 and 1 risky assets) and simpler frameworks. As we are particularly interested in the performances of the long/short optimal anomaly-based factors from the point of view of a real-time investor, we develop a fully-fledged out-of-sample analysis using 4 different datasets of characteristic-sorted portfolios.====The remainder of the paper is organized as follows. Section 2 introduces the dynamic asset allocation model and derives the optimal portfolio strategy. Section 3 describes the dataset and the estimation technique. Section 4 reports the main empirical findings. Section 5 concludes. A separate Internet Appendix contains all the mathematical derivations, the details of the numerical procedure implemented to calculate the optimal portfolio strategies, the identification procedure used to estimate the model parameters, as well as several additional empirical findings.",Factor Investing for the Long Run,https://www.sciencedirect.com/science/article/pii/S0165188920301287,20 June 2020,2020,Research Article,300.0
"Dur Ayşe,Martínez García Enrique","Department of Economics, North Carolina State University, 2801 Founders Drive, Raleigh, NC 27695, United States,Federal Reserve Bank of Dallas. 2200 N. Pearl Street, Dallas, TX 75201, United States","Received 1 April 2020, Revised 13 June 2020, Accepted 15 June 2020, Available online 20 June 2020, Version of Record 4 July 2020.",https://doi.org/10.1016/j.jedc.2020.103959,Cited by (1),"In many countries, ","Forecasting inflation—accurately and reliably—plays a critical role in policy-making and in the decisions of the private sector in undertaking their contractual and financial nominal commitments. In macroeconomic analysis and inflation forecasting, the New Keynesian Phillips curve has been widely used to capture the empirical relationship between inflation and the output gap, the unemployment rate, or even capacity utilization. However, the closed-economy Phillips curve seems to have flattened since the mid-1980s.==== And, as documented by Atkeson and Ohanian (2001), closed-economy Phillips curve-based models did no longer yield more accurate forecasts than a naïve, 4-quarter random walk benchmark. A survey by Stock and Watson (2009), motivated by the Atkeson and Ohanian (2001) puzzle and related literature, still finds that forecasts based on economic models, including the Phillips curve, only occasionally performed well.====A prominent explanation for the break in the forecasting performance of the closed-economy Phillips curve suggested in the literature is the role of globalization—that is, the greater integration of global markets in goods, labor, capital, and information. The literature has postulated the ‘global slack hypothesis’, that foreign slack, as well as domestic slack, drives cyclical domestic inflation, as a way to reconcile the Phillips curve relationship with the empirical evidence. The nexus between globalization, inflation, and monetary policy, in a relatively broad sense, has been discussed in academic and policy circles. Bernanke (2007), González-Páramo (2008), Mishkin (2009), Papademos (2007), Rogoff (2007), and Weber (2007) argued that globalization might have altered the inflation process.==== Former ECB president Draghi (2015) pointed out the importance of global factors in a speech as follows:====While Draghi (2015)’s speech seized on the idea of global inflation (see, e.g., Ciccarelli and Mojon, 2010, Kabukçuoğlu and Martínez-García, 2018), there is less work that has explored empirically how inflation is driven by global factors while bridging the gap with the New Keynesian theory behind it.==== In light of this, we study the modeling and forecasting of U.S. inflation based on an open-economy New Keynesian model. A major caveat with the open-economy Phillips curve that arises in this framework is the necessity to find a reliable measure of the unobservable global output gap. Even when consistent data is available, most foreign output gap series can be too short and the macroeconomic data may not be fully reliable (on this point, see Martínez-García, 2019). We argue that the lack of reliable and long enough global slack series might have shadowed the findings in the empirical literature, thereby clouding the role that globalization has on domestic inflation.====One of our key contributions precisely is to address the role of global slack on domestic inflation by suggesting other predictors with which the measurement problems that come with slack can be partly circumvented. We adopt a New Keynesian framework which can be viewed as an extension of the (closed-economy) money-in-the-utility New Keynesian model in Galí (2008) and Belongia and Ireland (2014) to the open-economy workhorse New Keynesian model of Martínez-García and Wynne (2010), Martínez-García (2019), and Kabukçuoğlu and Martínez-García (2018). In this setting, households can obtain liquidity services from real credit as well as real cash balances (although not necessarily as perfect substitutes of each other). We also introduce a stylized banking sector that levers the monetary policy transmission mechanism. All of this has the implication that fluctuations in money and credit do reflect, in equilibrium, the slack of the economy and, therefore, can be useful for inflation modeling and forecasting.====This framework brings a monetarist flavor into the open-economy New Keynesian model—an issue that had not been studied in earlier work by Kabukçuoğlu and Martínez-García (2018), as well as in other related open-economy New Keynesian models such as Clarida et al. (2002)—to study the linkages between inflation, global money growth, and global credit growth. In this setting, a key theoretical result is that no measures other than domestic and foreign output gap would help improve the forecasts of (changes) in U.S. inflation. In other words, the efficient inflation forecast for an open economy should be based on the domestic and foreign output gap. We then show with this model that the nominal measures of (i) global money gap and (ii) global credit gap contain information about the unobservable global output gap. This result turns out to be particularly useful for inflation forecasting given that the global output gap is notoriously hard to obtain or construct in practice.====The model posits that global slack is a better predictor of domestic inflation than domestic slack alone and makes the case for exploring the predictive power of global money and global credit in lieu of the unobservable global slack. To explore that, we test the pseudo-out-of-sample predictive performance of these alternative Phillips-curve-based predictors (global money and credit gaps) and document their strong predictive performance for U.S. inflation, especially since the mid-1980s, when the closed-economy measures performed poorly. In our empirical forecasting exercises, we adapt these theoretical insights from the open-economy New Keynesian model and follow in the footsteps of Stock and Watson (2003), and more closely, Canova (2007) and D’Agostino and Surico (2009). In particular, we rely on single-equation “economic models” to test the predictive performance of the global money and credit gap measures, using a simple autoregressive forecast of inflation as the benchmark to beat.====The global measures we use are simply the arithmetic averages of first-differenced money and nonfinancial credit series from G7 countries. Our empirical investigation involves five decades of quarterly data. We conduct pseudo out-of-sample forecasts for CPI and PCE measures of U.S. inflation at horizons 1, 4, and 12-quarters ahead. Our estimation and forecast periods include 80 quarters of data for each period. We go back as far as 1959:Q4 and perform forecasts under various subsamples shifting our starting date by one observation, until the last observation available in the our dataset (in general, until 2017:Q1).====Our metric for forecast accuracy is the MSFE of the open-economy Phillips-curve-based forecasting model with distributed lags of inflation and of the given predictors that we investigate, relative to the MSFE of the ‘restricted’ forecast derived from a nested univariate, autoregressive inflation process. We compute bootstrap standard errors for the MSFEs following Clark and McCracken (2006). We confirm the results of Atkeson and Ohanian (2001) with domestic slack (and similarly with global slack). We then document how the open-economy Phillips-curve-based forecasts with our measures of global money and credit perform better predicting future U.S. inflation, including in the post-==== recession period, while their closed-economy counterparts deteriorated since the mid-1980s.====Our results suggest that domestic credit growth helps forecast U.S. inflation only until the early 1990s, a pattern similar can be observed with U.S. money growth. In the Great Moderation era and onwards, the predictive accuracy of these domestic measures deteriorate significantly. The G7 averages of both money and credit growth, however, exhibit a better performance than the domestic measures in general from the earliest subsamples until the more recent subsamples. Moreover, the performance of G7 credit growth is clearly better for CPI inflation than PCE inflation, whereas G7 money growth performs well in forecasting both CPI and PCE inflation forecasts. The main conclusion we draw here is that domestic inflation becomes more connected with global liquidity and less connected with domestic liquidity over the same period that the U.S. economy has become more integrated with the rest of the world.====The credit measures that we evaluate have received very little attention in inflation forecasting so far. Watson (1999) evaluate the performances of only domestic credit measures, which are either subcomponents of the monetary aggregates (like the monetary base or reserves) or related to commercial and industrial loans. However, unlike us, they do not find evidence that such a channel works very well to help forecast inflation. Our work is more in line with that of Zanetti (2012) who considers a model augmented with the banking sector and finds that the linkages between inflation and the business cycle dynamics can be quite strong.==== The policy implication of our findings is that credit aggregates, not just monetary aggregates, deserve greater attention going forward for inflation forecasting and monetary policy-making.====We describe the theoretical foundations of the relationships between inflation and global liquidity in the open-economy New Keynesian model in Section 2. Our main theoretical result for forecasting, which suggests that global liquidity is related to global slack, is discussed in subsection 2.1. We then move onto our empirical model and findings in Section 3 and conclude in Section 4.",Mind the gap!—A monetarist view of the open-economy Phillips curve,https://www.sciencedirect.com/science/article/pii/S0165188920301275,20 June 2020,2020,Research Article,301.0
"Samano Mario,Santugini Marc","Department of Applied Economics, HEC Montreal, Canada,Department of Economics, University of Virginia, United States","Received 26 July 2019, Revised 10 February 2020, Accepted 3 June 2020, Available online 17 June 2020, Version of Record 3 July 2020.",https://doi.org/10.1016/j.jedc.2020.103943,Cited by (3),"We study the impact of standard-setting by introducing an externality that increases product compatibility in the presence of asymmetric returns to investment in a dynamic quality-ladder-type model. We classify the long-run, multi-modal probability distributions over different market structures that arise from this model. In some cases, the lagging firm may remain in the market in the long-run depending on the strength of the externality. In the case where only the laggard invests in compatibility, it is possible that the laggard becomes a monopolist if the leader has a relatively low R&D capability and the two firms are almost symmetric in this same regard. This variety of multi-modal long-run distributions may have important consequences for the estimation and the simulation of this class of dynamic models.","We study a class of business strategies in which a leading firm causes a positive externality on the perceived quality of the good produced by its competitors. For instance, the decision of a firm to increase the degree of compatibility with its competitor’s good, where the leading firm is the one with a higher capability of turning investments into larger installed bases. This strategy has been implemented in different industries in the form of standard-setting protocols.====Our research goal is to understand how the presence of this positive externality-due either to a firm’s unilateral decision or to regulation-affects the leading firm and the industry overall. To that end, we model the quality of a product as a function of the sum of its own installed base and the spillover, which is the degree of compatibility, with a fraction of the competitor’s installed base.==== This aggregate of the installed bases enters directly into the utility function, which captures higher compatibility among the products in the market. This modelling approach can be interpreted as an endogenous spillover process version of the model in Chen et al. (2009) with a cost to achieve compatibility and the decision can be unilateral. Firms decide the optimal amount of compatibility through a function that maps the competitor’s installed base into the amount of that base that can be added to its own. We embed the associated maximization utility problem into a dynamic quality-ladder model in which firms differ in their return to investment on their own installed bases. That is, for a given level of investment, one firm has a higher probability to increase its installed base size. We show that such a model can generate different types of long-run market configurations (market collapse, monopolies, duopoly, and combinations of these cases). We find the array of possible market structures that can arise from this game for different parameter values. Although we are not the first ones to acknowledge the multi-modal nature of these long-run probability distributions in a dynamic quality-ladder-type model, we are the first to classify them and to analyze the impact of compatibility externalities on them.====R&D spillovers have been studied in the context of process innovation. In this case, the externality directly decreases the marginal cost of production (see for instance D’Aspremont and Jacquemin (1988), Kamien et al. (1992), and Samano et al. (2017)).==== For product innovation, an important case is when the spillover increases the firm’s capacity to transform investment into a higher probability of achieving a higher product’s quality (see Goettler, Gordon, 2011, Song, 2011). In this paper we take a different approach and consider only the cases where the spillover causes an increase in the compatibility of the goods in the industry by increasing their perceived quality by consumers as it would occur in a standard-setting process and we do not consider reductions in production costs.====Empirical evidence on the existence of technological spillovers has been documented by Bloom et al. (2013). They separate the technology spillovers from the product rivalry effect of R&D and document that even when taking into account these two effects, industries such as pharmaceuticals, computers, and telecommunications exhibit technology spillovers. But even in the absence of positive externalities on quality, firms have different likelihoods of success of investment. Goettler and Gordon (2011) estimated a dynamic quality-ladder model for the computer processors industry. They find evidence for heterogeneity in the likelihood of success of investment, which can explain differences in the levels of investment and ultimately differences in the levels of quality between the goods.==== Another example of estimation of this class of models is found in Gowrisankaran and Town (1997). They consider two types of hospitals, for-profits and non-profits. The ratio of the number of these two types of hospitals is endogenous in their model. The parameter governing the probability of success of investment is restricted to be the same for the two hospital types, and yet, the observed market configurations in the data are not symmetric.====Motivated by those findings, we ask the following questions. What is the effect of heterogeneity in the firms’ ability to invest in their installed base on long-run market configurations? How does this form of heterogeneity interact with asymmetric compatibility spillovers? To answer them, we adapt the quality ladder model introduced by Ericson and Pakes (1995) and the algorithms to numerically solve for its equilibrium such as in Pakes and McGuire (1994) and in a particular case in Levhari and Mirman (1980) to the case of heterogeneous likelihood of success of investment with spillovers that affect compatibility.====We restrict our attention to the quality-ladder model without entry or exit. This is not a strong assumption since we allow for installed base levels of zero which are equivalent to exit. However, that does not prohibit the same firm from becoming active again if it achieves to increase its installed base to a positive level in the next period. We also note that in our motivating example from Goettler and Gordon (2011), they do not consider entry and exit since the industry they study does not exhibit such behavior during the time window in their data.==== In our second motivating example on the estimation of a quality-ladder model, Gowrisankaran and Town (1997) consider the possibility of entry and exit, however all hospitals belong to one of two firm types, and thus if all firms of one type exit, this is equivalent in our two-firm model to having an installed base of zero for one type of firm.====Heterogeneity in the quality-ladder dynamic models has been studied in the context of capacity games. Besanko and Doraszelski (2004) conclude that asymmetries of firm size can be due to the effects of price competition which in turn lead to long-run distributions that exhibit positive probabilities on outcomes that represent only one firm surviving.==== Their analysis keeps parameters symmetric across the two firms. We also find such configurations in cases of symmetric firms, but those configurations can arise from other parameter combinations as well. The asymmetries in price competition in their model arise because of small asymmetries in capacity accumulation that occur accidentally which makes one firm slightly dominant over the other, making the other firm to give up if investment is highly reversible. In Borkovsky et al. (2010) and Borkovsky et al. (2012), it is shown that the dynamic quality-ladder model can exhibit multiplicity of equilibria even in the absence of entry or exit if the investment is highly permanent. We take a different approach and allow firms to have different parameter values in their investment success function and study the long-run distribution over the installed base space given the unique equilibrium policies.==== We also abstract from collaborations in R&D such as research joint ventures that could lead to a different type of externalities (see Samano et al., 2017 and Cellini and Lambertini, 2009) and from interactions between free-riding firms (surfers) and R&D-oriented firms (see Abdelaziz et al. (2008)).====We find that asymmetries in the likelihood of success of investment can have relevant effects on long-run market configurations, which highlight the richness of the baseline model. Even though the externality may be beneficial to decrease the outside good market share, it could harm the leader and allow the lagging firm to remain in the market if the asymmetry in the externality is above certain level or even make the laggard to become the monopolist if the leader does not invest in increasing compatibility but the laggard does. Therefore, increasing compatibility can eliminate the advantage of the leader for a certain range of industry parameters. This is important since it shows that helping to increase compatibility with competitors is not always harmful and it explains some of the motivations behind the existence of standard-setting protocols. We show that when the laggard is the only firm that can invest in absorbing the competitor’s quality, the likelihood of observing duopolies in the long-run increases relative to the case of symmetric externalities. We also analyze the effects of depreciation rates that depend on the level of the installed base, the effects of the consumers’ sensitivity to price levels, and the interaction between the depreciation rate and the curvature of the utility function.====This paper also has implications for the simulation of this type of models. Typically, one obtains data from an industry, say in a duopoly, and assume this is the equilibrium. Then a set of parameters is obtained by estimating a dynamic model of competition that belongs to the class of models we analyze here. We show that it is possible that when simulating the industry with the estimated parameters, additional market structures arise in the long-run. If we only report expected values for the different outcomes of the model as it is the most common practice, it is possible that salient information is being masked since the multiplicity of modes in the probability distribution is not properly reflected in those expected values.====The remainder of this article has the following structure. Section 2 discusses some standard-setting examples. Section 3 introduces the model. In Section 4 we provide computational details and the parametrization of the model. Section 5 presents the main results. We discuss further connections to the literature and conclude in Section 6.",Long-run market configurations in a dynamic quality-ladder model with externalities,https://www.sciencedirect.com/science/article/pii/S0165188920301111,17 June 2020,2020,Research Article,302.0
"Kopp Thomas,Salecker Jan","The University of Siegen, Economics and Didactics, Siegen 57072, Germany,The University of Göttingen, Department of Ecosystem Modelling, Göttingen 37077, Germany","Received 21 February 2019, Revised 7 February 2020, Accepted 11 June 2020, Available online 15 June 2020, Version of Record 30 June 2020.",https://doi.org/10.1016/j.jedc.2020.103944,Cited by (8),"Marketing channel choices in agricultural trade networks affect the networks’ overall performance and influence rural livelihoods. This study identifies key determinants of these choices among natural rubber traders in Indonesia to evaluate four policy scenarios and their potential effects on rural incomes.====, better infrastructure and transportation capacity, and market information availability. The model is calibrated through a genetic algorithm which maximises the similarity between the simulated network and the actual network observed in the data.====Results indicate that sellers’ decisions on a buyer are primarily determined by debt obligations and past peer-interactions. The most influential sellers have a similar level of formal education as their peers and live in close physical proximity. Results of the policy scenario analysis suggests that policies aimed at reducing sellers’ dependence on credit from buyers and increasing education are the most effective policies for improving value chains and reducing ==== in the region under consideration.","Small-scale traders play a large role in agricultural value chains in economically less developed countries (Piyapromdee, Hillberry, MacLaren, 2014, Subramanian, Qaim, 2011). Their marketing channel decisions largely shape the structure of local trading networks, which in turn impacts the effectiveness of rural policies aimed at strengthening the performance of such networks (Sujarwo et al., 2014). Understanding the value-chain decisions of small-scale farmers is therefore crucial in enacting effective policies to develop strong trading networks in developing countries and improve the welfare of the rural poor (Barrett, 2008).==== However, the analysis of these processes is challenging, since models of such marketing decisions are characterised by feedback loops from repeated buyer-seller transactions and peer interactions, especially in small rural communities (Iftekhar and Tisdell, 2016). A prime example of these processes at work is in rubber market value chains in rural Indonesia. Here, small-scale village traders buy raw rubber from smallholder farms and sell at arms’ length to larger traders at the district level. Previous studies have shown that market imperfections exist at several stages of this value chain (Kopp, Brümmer, 2017, Martini, Akiefnawati, Joshi, Dewi, Ekadinata, Feintrenie, van Noordwijk).====In this paper, we simulate different rural development policies through a parameterised agent-based model (ABM) in order to evaluate their potential effects. The unique construction of this ABM allows for a representation of complex interactions among rubber traders in Jambi province, Indonesia, by modelling the effects of social closeness and peer interaction on participants’ marketing decisions. The model is calibrated to fit a rich data set gathered through in-field surveys conducted in the region.====Complex adaptive systems (CAS) exhibit evolutionary dynamics which arise from a) interactions amongst stakeholders, interactions between stakeholders and the environment, and a learning process through which the results of these interactions feed back into the decision-making processes in future periods. This results in a permanent adjustment of the matrix of combined decisions of all stakeholders to the environment (Potgieter, April, Bishop, 2005, Rammel, Stagl, Wilfing, 2007).====Feedback loops are a source of high-degree complexity in CAS and endogeneity is omnipresent (van den Bergh and Gowdy, 2003). This poses drawbacks to econometric approaches such as regression analysis in modelling these processes (Holland, 2006). Also, whenever observations are only available for one point in time it is difficult to assess how knowledge spreads between stakeholders. Process-based approaches are better suited to study CAS since they can capture endogenous processes and multiple feedback loops. To this end, agent-based modelling (ABM) approaches can circumvent endogeneity issues (Zhang and Brorsen, 2010).====One example of CAS at work in the field of economics is in the marketing networks of natural rubber traders in Jambi Province, Indonesia.==== In this region, rubber is predominately produced by small-scale farmers and then distributed via a network of agricultural traders to domestic processors, the crumb rubber factories. Traders vary in size and capacity; smaller village traders sell the rubber to larger district traders, who then sell either to a processor or to still another trader (Kopp and Brümmer, 2017). Several studies point to shortcomings in these value chains, primarily in terms of inadequate infrastructure (Martini, Akiefnawati, Joshi, Dewi, Ekadinata, Feintrenie, van Noordwijk, Peramune, Budiman, 2007), a lack of access to fair credit (Akiefnawati, Ayat, Alira, Suyitno, Joshi, 2010, Kopp, Brümmer, 2017), and a lack of market transparency (Peramune and Budiman, 2007). In Jambi, rural incomes lie significantly below the national average (Kopp et al., 2017). Given the importance of the rubber market in Indonesia’s national economy, policies aimed at improving the performance of the trade networks might be a promising way to effectively address these problems (Iftekhar and Tisdell, 2016). This paper seeks to determine ====To answer this question, while accounting for the complex and dynamic nature of a multi-agent decision-making environment, we develop the agent-based model RUBNET, which models each agent’s selling decision as a recursive process. The model is designed to allow for the simulation of previously developed policy scenarios. To take the CAS properties of the trading network into account, we employ an agent-based, pattern-oriented modelling approach to generate a hypothetical outcome under certain assumptions, represented as global model parameters (Grimm et al., 2005). RUBNET predicts trading connections of model agents based on these global parameter values and compares the emerging network to the network that was empirically observed in the field. The global parameters are then systematically changed in order to maximise the similarities between the simulated and the observed trading network. The predicted decisions in the model are influenced by individual buyer and seller characteristics, characteristics of bilateral relationships between sellers and buyers, as well as relationships between sellers. Data was gathered from a representative, multi-stage micro survey of small and medium-scale agricultural traders in Jambi Province, Indonesia in 2012. The parametrised model is then used to simulate the effects of different policy scenarios on the performance of the trading network.====The basic logic of the modelling process is as follows: A seller ranks all potential buyers based on individual characteristics, as well as their existing relationships to the prospective buyer and then selects the buyer with the highest rank.==== This procedure allows for heterogeneous effects based on individual characteristics. The selling decisions made by each seller’s peers also affect his or her decision-making process in future periods. To quantify the importance of peer influence on marketing decisions, a matrix of all other sellers’ decisions in the previous period enters the model as a possible determinant of the decision in the current period. The resulting seller-specific lists order all potential buyers according to each individual seller’s propensity to engage in trade with them. After each iteration, descriptive metrics of the predicted network are saved. This process is then repeated until the metrics converge. An optimisation algorithm is used to determine the values of the global parameters which maximise the number of correctly predicted trading links.====To the best of the authors’ knowledge, this is the first paper to employ an ABM approach based on the theory of CAS to predict agricultural traders’ marketing channel choices. The model includes an innovative approach of multiplying a social matrix with a weighting vector, to allow for heterogeneous effects based on individual characteristics. This enables the researcher to identify individuals whose decisions are disproportionally influential. The policy scenarios that are simulated based upon the parameterised model provide insights for policy makers on which policies are most effective in value chain improvement and subsequently poverty reduction in the region under consideration. The tools developed in this analysis are also applicable to similar situations in other countries and markets where the identification of drivers of trading network structures is desired. These include, firstly, the simulation of seller-trader relationships through agent-based modelling to predict agricultural traders’ marketing channel choices. Second is the systematic evaluation of policy scenarios based on the parameterised network. The study builds upon and shows the value of a unique data set of sellers’ and buyers’ data from a multi-stage field survey with agricultural traders.====The paper is structured as follows: the literature review in Section two gives an overview of ABM approaches used in the economics literature so far. Section three presents the ABM RUBNET, and applies it to the natural rubber trade network in Indonesia. Section four presents and discusses the results, and Section five concludes. An elaborate appendix lays out the details of the ABM developed in this paper.",How traders influence their neighbours: Modelling social evolutionary processes and peer effects in agricultural trade networks,https://www.sciencedirect.com/science/article/pii/S0165188920301123,15 June 2020,2020,Research Article,303.0
"Popoyan Lilit,Napoletano Mauro,Roventini Andrea","University of Naples “Parthenope”, Department of Business and Economics, Via Parisi 13, Naples 80133, Italy,Institute of Economics (LEM), Scuola Superiore Sant’Anna, Piazza Martiri della Libertá 33, I-56127 Pisa, Italy,Sciences Po, OFCE, and SKEMA Business School, Université Côte d’Azur, 60 rue Dostoïevski, 06902 Sophia Antipolis, France,Sciences Po, OFCE, 60 rue Dostoïevski - 06902 Sophia Antipolis, France,EMbeDS and Institute of Economics (LEM), Scuola Superiore Sant’Anna, Piazza Martiri della Libertá 33, I-56127 Pisa, Italy","Received 27 May 2019, Revised 8 February 2020, Accepted 24 May 2020, Available online 12 June 2020, Version of Record 27 June 2020.",https://doi.org/10.1016/j.jedc.2020.103937,Cited by (19), according to different types of ,"In this paper, we develop a macroeconomic agent-based model to study how financial instability can emerge from the co-evolution of interbank and credit markets and the possible policy responses to mitigate its impact on the real economy.====Crises in the banking sector are intermittent phenomena that generally appear after periods of intensive credit growth (Gourinchas, Obstfeld, 2012, Schularick, Taylor, 2012). They usually impose high costs not only to the financial sector but also to the economy at large (Reinhart and Rogoff, 2009). The damaging real effects and the feedback loops of the financial turmoil of 2008 also showed that systemic risks can arise during periods of apparent economic tranquility (Acemoglu, Ozdaglar, Tahbaz-Salehi, 2015, Battiston, Farmer, Flache, Garlaschelli, Haldane, Heesterbeeck, Hommes, Jaeger, May, Scheffer, 2016, Battiston, Gatti, Gallegati, Greenwald, Stiglitz, 2012), and that neither monetary policy nor micro-prudential regulations are sufficient to smooth systemic financial imbalances.====What are the transmission mechanisms between the banking sector and the real economy? On the one side, banking crises hit real economies via the financial accelerator (Bernanke, et al., 2007, Delli Gatti, Gallegati, Greenwald, Russo, Stiglitz, 2010, Gilchrist, Zakrajšek, 2012). On the other side, banking crises and credit supply also depend on liquidity freezes in the interbank market (Acharya, Merrouche, 2012, Angelini, Nobili, Picillo, 2011, Freixas, Martin, Skeie, 2011). Indeed, the facility through which banks can get liquidity affects their credit supply, thus dampening or magnifying the financial accelerator dynamics (Cornett, McNutt, Strahan, Tehranian, 2011, Iyer, Peydró, da Rocha-Lopes, Schoar, 2013).====For this reason, the evolving debate on interactions between monetary and macro-prudential policies (see Angelini, Neri, Panetta, 2014, Paoli, Paustian, 2017, Rubio, Carrasco-Gallego, 2014, among others) should also be focusing on the co-evolution of interbank and credit markets and their possible impact on financial stability and, more generally, on economic dynamics. For instance, one should better study how the Liquidity Coverage Ratio — one of the levers of the Basel III macro-prudential framework — affects liquidity risk and the supply of bank credit. More generally, macro-prudential and monetary policy should also consider active liquidity management. However, the growing body of literature emerging after the 2008 crisis partially overlooks this issue and does not provide models jointly accounting for liquidity crises, banking stability, and economic dynamics.==== In particular, do Basel III regulatory tools amplify or reduce the risk of liquidity crises? Can one design a macro-prudential framework that dampens instabilities in both liquidity and credit markets? Should monetary policy “lean against the wind”? What are the effects of this type of policy in the interbank liquidity market? Do we need other tools besides an interest-rate policy to avoid liquidity crises?====We address these questions extending the agent-based model developed in Popoyan et al. (2017) to include the interbank market.==== The model describes an economy composed of heterogeneous firms, banks, consumers, Government, and a Central Bank. Firms and consumers engage in trading relationships in decentralized goods and labor markets. Firms finance production relying on bank credit, whose supply is constrained by macro-prudential regulations. Banks engage in liquidity trading in the interbank market to satisfy liquidity needs arising from liquidity constraints. The Central Bank can supply liquidity in the interbank market; it performs monetary policy applying different types of Taylor rules and imposes a macro-prudential regulatory framework akin to either Basel II or III. Finally, the Government performs fiscal policy, bails out banks in case of a crisis, and eventually issues bonds to finance the deficit.====One reason why macro/micro-prudential policies, liquidity crises, and economic dynamics have not been analyzed under a common roof is the intractability that would result in standard macro models. This argument militates in favor of the pluralism of macro models (see the discussion in Haldane and Turrell, 2019). Among them, agent-based models (ABMs)==== are very well suited to study credit and liquidity market dynamics where heterogeneous agent-specific solvency and liquidity risks affect their interactions and endogenously lead to coordination failures, market freezes, and bankruptcy cascades.====We contribute to expanding the growing literature of agent-based macro-models integrating credit markets==== by analyzing the effects of interactions between macro-prudential regulation and monetary policy in a framework characterized by heterogeneous banks and firms and where banks can also exchange liquidity in an interbank market. Some recent contributions in agent-based models address the question of financial stability incorporating prudential instruments and by considering their interactions with monetary policy. However, most of those contributions have primarily focused on the analysis of either micro-prudential tools (mainly static minimum capital requirement, see Ashraf, Gershman, Howitt, 2017, Cincotti, Raberto, Teglio, 2012, van der Hoog, Dawid, 2017, Salle, Seppecher, 2018, Teglio, Raberto, Cincotti, 2012), or risk premia on loans (see Chiarella, Di Guilmi, 2017, Salle, Seppecher, 2018), while leaving macro-prudential measures and liquidity requirements out of sight. To the best of our knowledge, the only contribution that comes close to ours (even if without liquidity requirements) is Salle and Seppecher (2018). They present a macro ABM that accounts for static minimum capital requirements and their interaction with monetary policy rules (both classic and leaning-against-the-wind). The authors find that a monetary policy rule that targets the movements in the net worth of firms significantly dampens credit cycles and reduces the employment costs of financial crises. Additionally, they stress that the performance of the leaning-against-the-wind policy is more robust than the one obtained with a simple, dual-mandate Taylor rule. We obtain a similar result even if our model structure differs from Salle and Seppecher (2018) in that it incorporates a fully-fledged macro-prudential setup (both capital and liquidity requirements) an interbank market, which may introduce further channels of financial distress and we consider possible new venues of overgoing it.====Simulation results show that the model endogenously generates interbank market freezes wherein liquidity dries up, and interbank interest rates become significantly high. These anomalous situations, in turn, interact with the financial accelerator, possibly leading to firm bankruptcies, banking crises, and the emergence of deep downturns. The risk of market freezes in the interbank market requires the timely intervention of the Central Bank as liquidity lender of last resort to curb the negative impacts in other markets. Furthermore, we show that the joint adoption of a three mandate Taylor rule tackling credit growth and the Basel III macro-prudential framework is the best policy combination to stabilize financial and real economic dynamics. On the contrary, the Liquidity Coverage Ratio (LCR) spurs financial instability increasing the pro-cyclicality of banks’ liquid reserves. For this reason, we design a new macro-prudential tool that adds a counter-cyclical liquidity buffer to the LCR. The new enhanced LCR now contributes to stabilizing fluctuations in the interbank market. Relatedly, we find that active management of the width and symmetry of the interest-rate corridor by the Central Bank is a new unconventional monetary policy tool to dampen financial instability. Generally, our results support the Tinbergen principle: an adequate number of instruments is required to control inflation and to achieve stability in both interbank and credit markets. We then perform a detailed analysis of the different levers of Basel III, and we find that the combination of static and dynamic capital requirements is the most effective in dampening the pro-cyclicality of credit and in stabilizing the banking sector and the aggregate economy.====The rest of the paper is organized as follows. Section 2 describes the model. The results of policy experiments are reported in Section 3. Finally, Section 4 concludes.",Winter is possibly not coming: Mitigating financial instability in an agent-based model with interbank market,https://www.sciencedirect.com/science/article/pii/S0165188920301056,12 June 2020,2020,Research Article,304.0
"Niu Yingjie,Yang Jinqiang,Zou Zhentao","SHU-UTS SILC Business School, Shanghai University, China,Shanghai Key Laboratory of Financial Information Technology, School of Finance, Shanghai University of Finance and Economics, Shanghai Institute of International Finance and Economics, China,Economics and Management School, Wuhan University, China","Received 1 March 2020, Revised 5 June 2020, Accepted 6 June 2020, Available online 11 June 2020, Version of Record 23 June 2020.",https://doi.org/10.1016/j.jedc.2020.103942,Cited by (6),"We consider a robust contract with limited commitment in continuous time, in which the principal (firm) is ambiguity averse and seeks robust decisions. First, we find the existence of limited commitment not only attenuates the negative effect of model uncertainty on the firm value, but also makes the firm less pessimistic. Moreover, concerns regarding ==== motivate the firm to increase the worker’s exposure to the productivity shocks via the optimal contract. Finally, our theoretical model predicts that the combination of limited commitment and ambiguity generates both right skewness and decreases in the wage dynamics as the empirical data.","Classical contract theory assumes that the principal (firm) and the agent (worker) have the same beliefs about uncertainty. However, there are two reasons that motivate us to depart from this assumption. On the one hand, the Ellsberg (1961) paradox and related experimental evidence show that people treat risk and ambiguity in different ways. Ambiguity refers to the situation in which the probability distribution over the state of the world may be unknown, while risk refers to the case in which the distribution is known. On the other hand, Hansen and Sargent (2001) document that economic agents believe that the observed economic data come from a set of unspecified models. Concerns regarding model misspecification induce economic agents to make robust decisions.====This paper considers the robust contract by incorporating limited commitment to account for several stylized features of wage dynamics simultaneously. On the one hand, Lydall (1968) finds wage distributions are more skewed to the right than the underlying ability distribution. On the other hand, albeit the standard one-sided commitment model of Harris and Holmstrom (1982) does not predict any wage decreases, the empirical data show the opposite. Gottschalk (2005) estimates the annual probability of a wage decline to be between 4% and 5%. To address these issues, we consider two frictions in this paper: model uncertainty and limited commitment.====Our model has two essential building blocks: (i) the standard contracting problem with limited commitment; (ii) robustness and belief distortions. For the first building block, we adopt the exponential utility version of Grochulski and Zhang (2011), in which the worker is risk averse with limited commitment. For the second building block, we use entropy to measure model discrepancies. This approach is widely used in statistics and econometrics for model detection. More importantly, it is analytically tractable and suitable for our continuous-time setting.====Following Miao and Rivera (2016), we assume that the worker trusts the reference model of the productivity process. Due to the lack of information, the firm considers alternative models to protect itself from model misspecifications.==== Therefore, the firm designs a robust contract to maximize its value in the worst-case scenario. We model the firm’s objective as the multiplier preferences proposed by Anderson et al. (2003). Essentially, the firm solves a max-min problem.====To understand the interaction between model uncertainty and limited commitment, we consider three cases: (i) robust contract with full commitment, (ii) standard contract with limited commitment, and (iii) robust contract with limited commitment. For the former two cases, we obtain the closed-form solutions. When there are both model uncertainty and limited commitment, we solve the contracting model by using dynamic programming. We find the following main novel results. First, our model predicts that both limited commitment and model uncertainty erode the firm value and increase the firm’s effective risk aversion. However, these two forces play different roles in different states. Specifically, the effect of model uncertainty dominates limited commitment when the worker is far from the commitment constraint. When it is close to the participation constraint, limited commitment plays the major role and attenuates the negative effect of model uncertainty on the firm value.====Second, the existence of model uncertainty makes the worker’s wage to increase in response to a positive productivity shock in the interior since the ambiguity-averse firm tends to increase the worker’s exposure to the productivity shock. When there is only limited commitment or model uncertainty, both the drift and volatility of wage are constant. However, the drift and volatility become state dependent when there exist both forces. Furthermore, the drift and volatility of wage are increasing in the worker’s promised utility.====Finally, we discuss the model implications for wage dynamics and distributions. In empirical data, the wage distributions are more skewed to the right than the underlying ability distributions and the wage dynamics exhibit decreases. When there is only limited commitment or model uncertainty, it is impossible to generate both phenomena. With respect to the empirical facts, we consider both forces and find the wage dynamics and distributions in our model are consistent with the data. When the limited commitment constraints do not bind, the wage is time-varying and stochastic. Once the wage hits the lower bound, it increases in order to satisfy the participation constraint. Due to the existence of partial downward rigidity, the wage distributions are skewed to the right.====Our paper is related to a rapidly growing literature that introduces model uncertainty into the classical contract theory. Carroll (2015) studies a static contracting problem in which the principal has ambiguous beliefs about the agent’s possible actions. Miao and Rivera (2016) study robust contracts and focus on capital structure implementation and asset pricing implications. Liu et al. (2018) consider a principal-agent problem in which the agent is averse to ambiguity. Wu et al. (2018) study robust long-term contracting and focus on relative performance evaluation. Niu et al. (2019) extend dynamic agency and investment theory by incorporating model uncertainty. Szydlowski (2019) introduces ambiguity into a dynamic contracting problem in which the principal faces ambiguity regarding the agent’s effort cost. Different from these papers, we investigate how to design a robust contract under one-sided commitment.====Our article also contributes to the literature on optimal contracting with commitment frictions following Harris and Holmstrom (1982). Grochulski and Zhang (2011) study a continuous-time version of the optimal risk-sharing problem with one-sided commitment. Zhang (2013) provides a stopping-time-based solution to a long-term contracting problem with one-sided commitment. Ai and Li (2015) extend the neoclassical investment model to allow for limited commitment. Miao and Zhang (2015) propose a duality approach to solving contracting models with limited commitment in continuous time. Grochulski and Zhang (2017) study the interaction between moral hazard and limited commitment. Bolton et al. (2019) offer a new theory of corporate liquidity and risk management based on the inalienability of risky human capital. In contrast to these papers, our model considers the interaction between model uncertainty and limited commitment.====The remainder of the paper is organized as follows: Section 2 describes the model setup, which includes the contracting problem and model uncertainty. Model solutions are derived in Section 3. In Section 4, we illustrate the quantitative results and economic implications based on the optimal contract. Finally, Section 5 concludes the paper.",Robust contracts with one-sided commitment,https://www.sciencedirect.com/science/article/pii/S016518892030110X,11 June 2020,2020,Research Article,305.0
Kopiec Paweł,"Narodowy Bank Polski, Świętokrzyska 11/21, 00-919 Warsaw, Poland","Received 10 June 2019, Revised 21 May 2020, Accepted 23 May 2020, Available online 10 June 2020, Version of Record 23 June 2020.",https://doi.org/10.1016/j.jedc.2020.103941,Cited by (1),This paper studies a mechanism that amplifies the effects of a rise in government purchases through private consumption. ,"The Great Recession of 2008 accompanied by nominal interest rates close to zero gave rise to a lively discussion about the usefulness of fiscal policy for spurring a recovery. As a consequence, a growing body of research started to analyze the channels through which fiscal shocks are propagated to economy. This paper studies a powerful transmission mechanism of a rise in government expenditures that works through labor market and private consumption.====In particular, the channel analyzed in my paper is based on a combination of two empirically established premises. First, households are not able to insure against unemployment and thus they cut consumption expenditures in response to rising pessimism about future job prospects. For instance, Carroll (1992) presents evidence on the role of unemployment expectations in determining current consumption and finds that poor job prospects explain a substantial part of the weakness in consumption during recessions. Moreover, Kolsrud et al. (2018) analyze Swedish data and find that average drop in consumption expenditures during the first year of an unemployment spell equals 32%, which demonstrates household’s inability to insure against losing a job. Second, there is an ample evidence suggesting that fiscal packages have a large impact on job creation and employment. For example, Chodorow-Reich et al. (2012) and Serrato and Wingender (2016) find that $100,000 of additional government expenditures in the US generate 3.8 and 3.3 job-years, respectively. Moreover, using a structural VAR, Monacelli et al. (2010) show that an increase in fiscal purchases equal to 1% of GDP lowers unemployment by 0.6 percentage points and raises labor market tightness by 20%.====Motivated by these observations, I study a novel channel which amplifies the macroeconomic effects of higher government purchases through improvement in employment prospects. Fiscal expansion fuels aggregate demand and, due to price rigidities, leads to adjustment in the quantity of produced goods. The latter requires larger output capacity across firms and hence increases incentives to recruit additional workers. Higher demand for labor raises job-finding rates, which has two effects on aggregate consumption. First, at the individual level, it lowers the chance of being unemployed, which decreases precautionary motives and increases consumption. This force is referred to as the “reduced unemployment risk” channel in my paper. Second, at the aggregate level, a rise in job-finding rates leads to higher ratio between employed consumers and unemployed consumers. Due to imperfect insurance markets, the former spend more than the latter, so when the number of employed households grows then, automatically, aggregate consumption rises, too. This mechanism is called the “compositional” channel and jointly with the “reduced unemployment risk” channel gives rise to the “employment prospects” channel.====To quantify its magnitude, I use the standard Bewley-Huggett-Aiyagari model (BHA henceforth) extended to capture two ingredients. First of them are price rigidities, which guarantee that higher aggregate demand generated by fiscal package is not entirely absorbed by an upward adjustment in prices.==== Second ingredient is introduced to account for the idiosyncratic, endogenous unemployment risk faced by households. More formally, it is done by embedding the standard Diamond-Mortensen-Pissarides model of the frictional labor market into the BHA framework. I use the calibrated version of the model is applied to decompose the response of aggregate consumption to higher government spending and to isolate the impact of both the “reduced unemployment risk” channel and the “compositional” channel. In particular, I show that if the influence of both forces (summarized with the “employment prospects” channel) on aggregate private spending is shut off, then both the cumulative and the impact government expenditure multipliers shrink significantly. More specifically, the resulting drops in their values are between 32% and 38% under various monetary and fiscal policy scenarios.====A similar idea about the interaction between labor market and government purchases was described by Rendahl (2016) who analyzes the role of fiscal purchases in a liquidity trap. Rendahl (2016) uses a standard dynamic representative agent model combined with the Diamond-Mortensen-Pissarides framework and finds that even a short-lived shock to government expenditures can have a large impact on output because, in the presence of frictional labor market, it raises not only current employment but also its future levels.==== This, in turn, raises future path of aggregate incomes and, due to the consumption smoothing motives, stimulates current private spending, which propagates the initial fiscal shock. The main difference, in comparison to mechanism analyzed in my paper, is that Rendahl (2016) abstracts from agents heterogeneity and precautionary motives generated by incomplete insurance markets which are at the root of the “reduced unemployment risk” channel studied here. To articulate this difference, I compare the dynamics of aggregate private spending generated by fiscal expansion in the model with complete insurance markets with its behavior in the model with heterogeneous households when the “reduced unemployment risk” channel is closed. Interestingly, it turns out that the dynamics of both transition paths is similar. Moreover, it is shown that the magnitude of the “reduced unemployment risk” channel is substantial which means that the difference between the model with imperfect and complete insurance markets is quantitatively significant.====Furthermore, my paper is related to works studying the effects of fiscal policy shocks in models with heterogeneous households, in which a significant proportion of agents deviates from the consumption-savings behavior predicted by the permanent income hypothesis and thus exhibits relatively high levels of marginal propensity to consume (MPC). There are two groups of papers within that field: first of them focuses on the role of taxes and transfers (like McKay and Reis (2016), Den Haan et al. (2018)), and the second concentrates on the role of fiscal purchases (Navarro and Ferriere (2016), Brinca et al. (2017), Auclert et al. (2018), Kopiec (2019) and Hagedorn et al. (2019)).====McKay and Reis (2016) use a model with heterogeneous households and nominal rigidities to analyze the role of automatic stabilizers and find that taxes and transfers that affect inequality and social insurance can have a large impact on the volatility of macroeconomic aggregates. Similarly to my paper, Den Haan et al. (2018) study the influence of unemployment fears on precautionary motives and aggregate demand. In their work, a combination of uninsured unemployment risk, flexible prices of goods and nominal wage rigidities gives rise to deflationary spirals during recessions that squeeze firms’ profits, induce layoffs and, as a result, worsen idiosyncratic unemployment risk. This, in turn, spurs precautionary motives and lowers aggregate consumption that amplifies the drop in prices even further. Instead of analyzing the effects of fiscal spending (as it is done in this paper), they study the role of unemployment benefits in mitigating the dire consequences of deflationary spirals. The main technical differences between my paper and Den Haan et al. (2018) are: sticky prices of goods assumed here (which puts my work closer to the standard Heterogeneous Agent New-Keynesian framework) and idiosyncratic changes to worker’s productivity, which allow me to replicate both income and wealth heterogeneity observed in the data in a more accurate way. Moreover, to illustrate the interplay between uninsured unemployment risk and aggregate demand, Den Haan et al. (2018) compare the transition path of main aggregates during recession in the baseline model with the one generated by the model with full risk sharing. In my paper, additionally to comparing the effects of fiscal spending shock under both complete and incomplete insurance markets, I construct a decomposition method based on the one presented in Kaplan et al. (2018), that allows to isolate the sole impact of the deterioration in employment prospects which, in contrast to the comparison with a representative agent framework, keeps other elements of the heterogeneous agent model unchanged.====Let us turn to works that use models with heterogeneous agents to study the role of fiscal purchases. Brinca et al. (2017) construct a life-cycle, overlapping generations model to study the relationship between inequality (measured with the level of income risk faced by agents) and the effects of fiscal consolidations.==== They find a strong positive relationship between inequality and recessive impact of fiscal consolidation programs and propose a theoretical mechanism that is based on the reaction of labor supply of non-constrained agents to fiscal shocks. In contrast to Brinca et al. (2017), I concentrate on the extensive margin of employment by studying a model with frictional labor market which enables to capture the effects of fiscal shocks on aggregate labor market flows (with the associated “compositional” channel) and on idiosyncratic unemployment risk (with the “reduced unemployment risk” mechanism).====To investigate the transmission of higher fiscal purchases through private consumption in a disciplined way, Auclert et al. (2018) and Kopiec (2019) derive analytical characterizations of the multiplier in two different variants of the Heterogeneous Agent New-Keynesian model. Both works, however, abstract from labor market frictions and endogenous unemployment risk.====Navarro and Ferriere (2016) study the impact of changes in government expenditures in the standard BHA model with labor indivisibility and flexible prices. They find that only an increase in government spending that is accompanied by a rise in tax progressivity is able to generate a positive response in aggregate consumption. The reason for this fact is intuitive: when tax progressivity increases, authorities are able to decrease the average tax level because they tax top incomes at higher rates. Main beneficiaries of the associated tax cuts are agents with low labor income who exhibit relatively high MPC. In contrast to Navarro and Ferriere (2016), I consider a BHA model with frictional labor and sticky prices in which the government finances expenditures with a linear tax.==== The latter enables to separate the effects of a rise in government purchases from the transfer-like impact of changes in tax progressivity that accompanies fiscal expansion in Navarro and Ferriere (2016).====It seems that the closest work to mine is Hagedorn et al. (2019) who study the size of the fiscal multiplier using a version of the BHA model with price adjustment costs as in Rotemberg (1982) and decompose the reaction of private consumption to government purchases into several channels that provide better understanding of mechanisms that propagate fiscal stimulus. There are, however, two substantial differences between my work and the paper of Hagedorn et al. (2019). First, I consider a different specification of labor market which, in my case, features search frictions. This implies that shifts in public expenditures affect labor market flows and unemployment risk which is a prerequisite for studying the role of changes in employment prospects during fiscal expansions. Second, in contrast to Hagedorn et al. (2019), my decomposition method of aggregate consumption, which is crucial for the isolation of the “employment prospects” channel, is based on the total differentiation of aggregate consumption with respect to economic variables entering the household maximization problem.====The remaining sections of the paper are organized as follows. Section 2 presents the BHA model with frictional product market and sticky prices. In Section 3 I study the effects of an increase in government expenditures when monetary policy follows a standard Taylor type rule and stimulus is financed with taxes. Section 4 describes and quantifies the employment prospects channel. To check the robustness of its magnitude, in Section 5 I consider two alternative scenarios of expansion. First of them analyzes the stimulus during which monetary policy is not responsive to changes in macroeconomic environment (e.g., due to the ZLB constraint). Second scenario assumes that the increase in government purchases is financed with debt. Section 6 concludes.",Employment prospects and the propagation of fiscal stimulus,https://www.sciencedirect.com/science/article/pii/S0165188920301093,10 June 2020,2020,Research Article,306.0
"Fotiou Alexandra,Shen Wenyi,Yang Shu-Chun S.","Fiscal Affairs Department, International Monetary Fund, USA,Department of Economics, Oklahoma State University, USA,National Sun Yat-Sen University, Taiwan","Received 3 September 2019, Revised 30 December 2019, Accepted 5 February 2020, Available online 5 June 2020, Version of Record 23 June 2020.",https://doi.org/10.1016/j.jedc.2020.103860,Cited by (17),"Using the post-WWII data of U.S. federal ==== increase. Also, a capital income tax cut need not always have large revenue feedback effects as suggested in the literature.","Since the 1980s, corporate income tax rates have largely fallen, particularly in advanced economies. In the U.S., the Tax Cuts and Jobs Act (the 2017 Tax Act) reduced the federal statutory corporate tax rate from 35 to 21%. In Japan, a series of reductions since 2015 has lowered the statutory corporate tax rates from about 35 to 30% for large firms. While the literature has long recognized the supply-side benefits of capital income tax cuts,==== preliminary estimation finds small growth effects and reduced investment responses of the 2017 Tax Act (Gravelle, Marples, Kopp, Leigh, Mursula, Tambunlertchai). Since recent corporate tax cuts in the U.S. and Japan have been implemented amid high government debt with projected rising debt-to-GDP ratios,==== this paper asks whether the reduced expansionary effects of capital income tax cuts can be explained by a deteriorated fiscal state.====To proceed, we first provide empirical evidence of government debt-dependent capital income tax effects on output. Within a nonlinear smooth transition vector autoregressive model (STVAR), we estimate the macroeconomic effects of corporate income tax changes, conditional on government debt levels. We model the state of the economy as a continuous process dependent on a debt-transition variable. In this way, we incorporate regime switching behavior but do not restrict our sample in specific periods of high-debt dates.==== In addition, we compute generalized impulse response functions that give us the flexibility to allow the economy to endogenously move between high- and low-debt states. We find that a corporate income tax cut has a less expansionary effect on output in a high-debt state than in a low-debt state, and in some cases it can even turn contractionary. Considering the tax revenue change, the eight-quarter cumulative output multiplier is ==== when government debt is low and ==== when government debt is high.====Next, we use a neoclassical dynamic stochastic general equilibrium (DSGE) model with two policy regimes to explore the mechanisms that can drive fiscal state-dependent capital income tax effects. In the tax cut regime, the government does not pursue a fiscal adjustment, as often observed in reality. In the fiscal adjustment regime, the government increases primary surplus to stabilize debt. Under rational expectations, agents in the tax cut regime form expectations about regime switching. When a government is highly indebted, a deficit-financed capital income tax cut increases government debt further, inducing expectations of fiscal adjustments. Specifically, the regime switching probability is assumed to link to the current debt level and the government’s debt repayment capacity, captured by the fiscal limits of an economy in the sense of Bi (2012). The switching probability is assumed to be the probability that the current debt level can exceed a randomly drawn fiscal limit from a simulated distribution.====Our modeling differs from the common approach which uses a fiscal reaction function that implements fiscal adjustments periodically (e.g., Canzoneri, Collard, Dellas, Diba, 2016, Erceg, Lindé, 2014, Leeper, Plante, Traum, 2010, Traum, Yang, 2011, Zubairy, 2014). Fiscal adjustments in our specification are nonlinear and stochastic. In reality, although fiscal adjustments are often implemented when fiscal states change,==== there is no certain debt threshold triggering an adjustment. Fig. 1 plots the U.S. federal income tax legislation aimed at reducing deficits based on the tax chronology in Yang (2009): some legislation was enacted immediately following an upward trend in debt (e.g., the Tax Equity and Fiscal Response Act of 1982 and the Deficit Reduction Act of 1984), but there has not been such tax legislation since the latest upward trend starting in 2009. Our modeling implies that an adjustment is more likely to occur in a high debt regime, but timing is uncertain.====We simulate the effects of a persistent capital income tax cut, as of a similar magnitude to the reduction in the corporate income tax rate in the 2017 Tax Act. Fiscal adjustments in the baseline analysis are implemented through a transfer reduction and a policy reversal (an increase in the capital income tax rate). Adjustments in an alternative simulation are through consumption tax increases, reflecting the recent practice in Japan.====Simulation results show that the tax multipliers are fiscal state-dependent, whether the expected adjustments are through a policy reversal or a consumption tax increase. Conditional on a potential policy reversal and an initial U.S. federal net debt of 45% of GDP (the average federal debt held by the public from 1980 to 2018), the impact output multiplier is ==== and the five-year cumulative output multiplier is about ====. With the initial debt at 80% (roughly the net federal debt level in 2019), the stimulative effect diminishes: the impact and five-year output multipliers become ==== and ====. Moreover, the same tax cut turns contractionary when debt rises to 120% of GDP. Since a higher probability of policy reversal implies a lower expected after-tax return to current investment, this weakens the incentive provided by the capital income tax cut. Reduced investment lowers capital and the marginal product of labor, generating a smaller output multiplier in absolute value compared to the results with lower debt. When the expected adjustment instrument is through consumption taxes, the qualitative patterns of the multipliers with respect to debt are similar to those under a policy reversal. Note that during the analyzed horizon, fiscal adjustments do not occur. Thus, the differences of multipliers across various debt levels solely come from the expectation channel.====The literature on the macroeconomic effects of fiscal policy is voluminous, and our paper is related to both empirical and theoretical research.==== On the empirical front, the recent development has moved away from estimating single multipliers to state-dependent ones. Studies on business cycle state-dependent government spending effects include e.g., Auerbach, Gorodnichenko, 2012, Auerbach, Gorodnichenko, 2013, and Ramey and Zubairy (2018), and on business cycle or uncertainty state-dependent tax effects include Arin et al. (2015), Candelon and Liebman (2015), Demirel (2016), and Eskandari (2019).====As for the fiscal state-dependent fiscal policy effects, several empirical papers document more expansionary effects of government spending in low-debt than in high-debt states (e.g., Huidrom, Kose, Lim, Ohnsorge, Ilzetzki, Mendoza, Végh, 2013, Kirchner, Cimadomo, Hauptmeier, Nickel, Tudyka, 2014) and the government debt-dependent effects of fiscal consolidations through a mix of tax-based increases and government spending cuts (Alesina, Azzalini, Favero, Giavazzi Miano, 2018, Fotiou, 2019). Our analysis focuses on how fiscal states can affect capital income tax effects, both empirically and theoretically. To our knowledge, the literature has not explored whether tax policy effects also depend on the fiscal state.====On the theoretical approach, several papers have used DSGE models with regime-switching policy to study fiscal policy effects (e.g., Davig, 2004, Davig, Leeper, 2011; Davig and Foerster, 2019). Our paper is particularly related to Bi et al. (2013), which studies uncertain fiscal consolidations in the European countries. We follow their approach to link fiscal adjustment implementation with fiscal limits, but abstract from the uncertainty about the instruments to be used.====Finally, the key mechanism underlying our results is the policy expectations regarding potential fiscal adjustments. Bertola and Drazen (1993) model a “trigger point”—an upper bound of government spending-to-output ratio—to explain counterintuitive expansionary effects of fiscal consolidation. Sutherland (1997) also links debt levels to policy expectations to explain that a fiscal deficit may not have traditional Keynesian effects related to consumption increases, if the current generation expects that fiscal adjustments would occur within the same generation. With a linear fiscal reaction function of government spending to debt, Corsetti et al. (2012) find that private consumption can rise to a government spending increase when agents expect a policy reversal. Also, Bi et al. (2016) study how expectations of fiscal adjustment can drive debt-dependent government spending effects as found in the empirical literature. Our findings add to this literature by focusing on how expectations of future policy can matter for capital income tax effects, accounting for an increasing probability of fiscal adjustments as the fiscal state deteriorates.",The fiscal state-dependent effects of capital income tax cuts,https://www.sciencedirect.com/science/article/pii/S0165188920300300,5 June 2020,2020,Research Article,307.0
"Bian Zhicun,Liao Yin,O’Neill Michael,Shi Jing,Zhang Xueyong","School of Finance, Nanjing University of Finance and Economics, Nanjing, China,Department of Applied Finance, Macquarie Business School, Macquarie University, Sydney, Australia,Investors Mutual Limited (IML), Sydney, Australia,School of Finance, Shandong University of Finance and Economics, Jinan, China,School of Finance, Central University of Finance and Economics, Beijing, China","Received 5 November 2019, Revised 20 May 2020, Accepted 22 May 2020, Available online 4 June 2020, Version of Record 14 June 2020.",https://doi.org/10.1016/j.jedc.2020.103939,Cited by (6)," on the sample-based estimates to simultaneously mitigate these two issues. We investigate the performance of our proposed covariance estimator for MV portfolio construction using Monte Carlo experiments and empirical examples. We find that the resulting MV portfolio strikes a good balance between risk and turnover reduction, and produces more accurate equivalent returns after transaction costs are taken into account when compared to four other MV strategies.","Portfolio allocation is one of the most important investment decisions in financial markets. The minimum variance (MV) portfolio strategy has received growing attention over the last decade (e.g., DeMiguel et al., 2007) due to its capacity to achieve both lower risk and higher return, and to avoid the estimation of expected returns (Cai et al., 2020). To implement such a strategy, it is crucial to provide accurate estimations of the return covariance structure; however, time-varying and high-dimensional assets in the investment universe generally pose challenges to the covariance structure estimation process.====In general, the problem of estimating covariance for portfolio allocation can be approached from two perspectives: time series and the cross-section. In time series, the key to covariance estimation is to assess the time variation and allow for conditional heteroscedasticity. In the cross-section, the key is to correct the in-sample bias of the sample covariance estimate (Engle et al., 2019). A widely used and simple-to-implement approach is the rolling window-based sample estimator of the covariance matrix (DeMiguel, Garlappi, Uppal, 2007, DeMiguel, Garlappi, Uppal, 2009, Jagannathan, Ma, 2003, Kan, Zhou, 2007, Michaud, 1989); however, this estimator exhibits unsatisfactory performance or cannot even be estimated in the first place, when there are a large number of assets. The problem is often due to that the number of historical return observations per stock (that is, ====), which are not much larger than the number of assets ====, namely, the problem of ==== < ====, or the portfolio constituents are highly correlated stocks. Thus, unless ==== is substantially larger than ====, the sample covariance estimate will be subject to significant estimation error, an issue referred to as the “curse of dimensionality.” When ==== is large, the covariance estimation is computationally challenging using traditional methods (e.g., maximum likelihood estimation (MLE)), thus requiring a new estimation algorithms that can improve the efficiency of the estimation.==== On the other hand, in assessing the time variation of the covariance structure, the usual approach is to rely on rolling window analysis, which rolls the estimation sample ahead each time a new observation becomes available, and then repeat the estimation process with the new estimation window. In doing so, each estimation window is treated independently, artificially introducing variations between temporally adjacent sample estimates, a problem typically known as “time instability” (DeMiguel, Garlappi, Uppal, 2007, DeMiguel, Garlappi, Uppal, 2009, Jagannathan, Ma, 2003, Kan, Zhou, 2007, Kirby, Ostdiek, 2012, Kourtis, 2015, Michaud, 1989). In this study, we employ “regularization”, which is a recent advance in machine learning literature, to propose a doubly-regularized covariance estimator, which solves the two afore-mentioned issues in a joint manner.====Following window analysis is commonly used in portfolio allocation to assess the time variations of the return covariance structure (DeMiguel, Garlappi, Uppal, 2007, DeMiguel, Garlappi, Uppal, 2009, Goto, Xu, 2015, Kirby, Ostdiek, 2012, Kourtis, Dotsis, Markellos, 2012); however, several associated issues have impaired its practical use. First, the choice of optimal window length is challenging. While it is advisable to set the window length large enough to allow for a robust estimation, it should not be so large that it captures the short-term fluctuation. Hence, the optimal window length needs to strike a balance between the robustness in estimation and dynamic change detection. Furthermore, the rolling window approach treats each estimation window as independent without exploring their similarities. This is particularly cumbersome for consecutive estimates. Given the high sensitivity of portfolio weights to small changes in portfolio optimization inputs, the spurious time variations in the covariance estimates would largely exacerbate the portfolio instability problem (Kourtis, 2015, Michael, Robert, 1991). Therefore, the use of rolling window analysis in portfolio allocation must be accompanied by an additional mechanism that can determine significant variations in covariance estimates. To address this issue in standard rolling window analysis in portfolio allocation, we imposes a regularization in this paper to encourage “temporal stability” in covariance estimates. The regularization shrinks the covariance time variation towards zero, when the actual changes are small, and remain active when they are significantly large. This regularization encourages discrete or sparse time variations in the covariance matrix, making our approach distinguishable from the traditional dynamic conditional models, such as the Exponential Moving Average Model (EMAM) or the Dynamic Conditional Correlation (DCC) Model (e.g., Engle et al., 2019), in which the covariance is assumed to change continuously. The achieved sparsity in time variation of the covariance matrix shares the same spirit of a very recent literature on dealing with Bayesian time-varying parameter (TVP) models, e.g., Bitto and Frühwirth (2019) developed a shrinkage prior to regularize time-varying model for covariance estimation. Huber et al. (2019) propose an algorithm to estimate large Bayesian TVP vector autoregressions that contain a mixture of innovation components for each coefficient in the system, where the temporal evolution in the coefficients is governed by a latent threshold process. Huber et al. (2020) introduce a computationally simple method, along with global-local priors to both shrink and sparsify TVP models. However, we differ from them by achieving sparsity in time variation of the covariance matrix in a non-Bayesian way, and also, we explore its use in MV portfolio allocations.====Turning to the problem of the “curse of dimensionality,” a traditional way to deal with it is to use shrinkage estimators. The basic idea is to shrink the unbiased covariance estimator in the direction that reduces estimator errors. The shrunk estimator, therefore, strikes the optimal balance between model mis-specification and estimation errors. Ledoit and Wolf (2003), Kourtis et al. (2012), among others, shrink the sample estimate towards a more parsimonious target matrix (e.g., the identity matrix). Goto and Xu (2015) impose a penalty on the overall size of the covariance matrix off-diagonal elements, thus shrinking them toward zero. In this way, the estimator encourages both shrinkage and sparsity, thus achieving dimension reduction. Dimension reduction can also be achieved by using factor model-based estimators. Chan et al. (2015) impose a factor structure on the covariance matrix to improve the optimized portfolio’s out-of-sample performance. Fan et al. (2008) and Fan et al. (2019) also report the advantage of employing a factor structure in optimal large-scale portfolio construction. Kastner (2019) offered a Bayesian approach to dynamic covariance estimation by modeling the underlying co-volatility dynamics of a time series vector using latent time-varying stochastic factors. Other studies (e.g., Chan, Lakonishok, Swaminathan, 2007, Fan, Furger, Xiu, 2016) that a structural factor model based on economic intuition (e.g., industry classification) is more powerful in covariance estimation than statistical factor models. In addition to solving the statistical issue caused by the curse of dimensionality, other studies suggest powerful estimation methods to improve computational efficiency in covariance estimation. Pakel et al. (2020) and Engle et al. (2019), and among others, use a composite likelihood approach to estimate vast dimensional time-varying covariance models, which use the average of bivariate log-likelihood functions for a large selection of asset pairs. We follow the shrinkage approach to solve the cross-sectional high dimensional problem, and we impose “sparsity” regularization on covariance estimation over each rolling window.====In sum, we propose a new approach to estimate time-varying covariance using double regularization. Specifically, our new estimator is derived by minimizing a regularized loss function that contains a negative likelihood term and two penalty terms: cross-sectional sparsity and temporal stability. The first penalty is the same as the one used in Goto and Xu (2015), which encourages sparsity in off-diagonal elements of the covariance matrix. The second penalty imposes sparsity on temporal variation between estimates from two consecutive rolling windows, which facilitates temporal stability in the rolling window-based sample estimates. The regularized loss function is minimized by the alternating directions method of multipliers (ADMM) algorithm developed by Boyd et al. (2010). We refer to our new method as the doubly-regularized rolling window (DRRW) method hereafter.====Methodologically, our DRRW method is an extension of the standard rolling window approach, but it possesses several additional desirable properties. First, rather than focusing solely on the current estimation window, we employ the DRRW method to explore the common structure of the covariance matrix in related but possibly distinct consecutive rolling windows. The resulting estimator is able to strike a balance between estimation precision (by using more historical information) and short-term variation reflection. The latter property allows us to provide a novel toolkit for online detection of abrupt changes in high dimensional covariance structures. There are some new statistical and machine learning methods for change point detection in time series (e.g., Gibberd, Nelson, 2017, Ravikumar, Wainwright, Raskutti, Yu, 2011), which require storage of the whole sample period prior to the change point detection. Relying on the rolling window analysis, our DRRW method can detect change points in the covariance structure each time there is a new observation. This perfectly aligns with the real-time portfolio allocation exercise. Second, our DRRW method provides a more accurate estimation of the covariance matrix from either the time series or the cross-section perspective, and is the first to offer a unified rolling window-based toolkit to fulfil this task. Last, with the tuning parameter of regularization, we can control the degree of sparsity achieved by the estimator. More details on how to make practical choices for the regularization tuning parameters are presented in Section 2.3.====We conduct a Monte Carlo experiment and an empirical comparison to investigate the performance of our new DRRW estimator in MV portfolio allocation. In simulation studies, we experiment with three scenarios: 1). the real covariance matrix is time-invariant (or constant); 2). the real covariance matrix changes discretely with large magnitudes; and 3). the real covariance matrix changes continuously with small magnitudes. We form five MV portfolios using our new DRRW estimator, as well as other competing estimators. Our simulation results confirm that the MV portfolio using the DRRW covariance estimator outperforms other estimators regarding the out-of-sample portfolio risk. In the first scenario, the portfolio risk reduction is the highest when we use our DRRW covariance estimator, confirming the benefit of imposing temporal stability regularization to create a stable portfolio. The better performance of our DRRW covariance estimator in the second scenario highlights the gain from accurately detecting abrupt (or significant) time variations in the covariance structure. The favorable performance of our DRRW covariance estimator in the third scenario suggests that the benefit of pursuing a stable portfolio would outweigh the benefit of active trading even under variable market conditions.====In empirical analysis, we compare data from four well-diversified portfolios from Kenneth French’s website==== as well as 50 individual stocks randomly selected from the New York Stock Exchange (NYSE)/American Stock Exchange (AMEX) universe during the time period of 2003–2019. We find that the MV portfolio strategy using our DRRW covariance estimator achieves significant out-of-sample risk reduction and larger certainty equivalent returns after transaction costs in all the data sets.====Our new DRRW covariance estimator enables us to not only stabilize the MV portfolio performance, but also to assess when significant updates are required for portfolio weights. The resulting MV strategy strikes an optimal trade-off between rebalancing the portfolio to capture changing information reflected by the recent data and avoiding the impact of excessive trading with its associated transaction costs. In this sense, our research shares the same objectives as a few recent studies. In terms of change detection in the covariance structure, Arouri et al. (2019) proposed a cojump test for multiple asset prices, and study the use of the new technique in portfolio allocation. Arouri et al. (2019) successfully fulfil our second task but without examining applications on large-scale portfolio. In fact, the identification of co-jumps in Arouri et al. (2019) relies on high-frequency (HF) return data, which is not trivial in a high dimensional setting. Plachel (2019) introduces a unified model for regularized and robust large-scale portfolio optimization, which deal with dimension reduction and parameter uncertainty in covariance matrix estimation. Differing from Arouri et al. (2019), we achieve dimension reduction and change-point detection in covariance matrix estimation; however, parameter uncertainty is outside the scope of our research. In addition, HF data opens an additional channel to increase the precision of covariance estimates. For instance, Hautsch and Voigt (2019) study large-scale portfolio allocation under transaction costs and model uncertainty using HF data. Cai et al. (2020) propose a sparse covariance estimator that can deal with heteroscedasticity and possibly be contaminated by microstructure noise. Bu et al. (2019) develop a multi-scale approach using HF data to capture the dynamics of covariance matrix and tail risk across time horizons. Extension of our DRRW method using HF data is thus intrinsically interesting; however, we leave such an analysis for future research. In this study, we focus on improving the covariance estimation for standard MV portfolio construction. It is also interesting to see how our DRRW covariance estimator works along with other portfolio optimization specifications, such as an MV portfolio with short sale constraints, or a turnover penalized MV portfolio. Given that many studies have claimed improvement in large-scale portfolio allocation using a regularization portfolio optimization approach (e.g., Gullen, 2016, Hautsch, Voigt, 2019, Olivares-Nadal, DeMiguel, 2018). We expect combining the two aspects of regularization would provide incremental benefit to MV portfolio investors. We leave this for future study.====The rest of the paper is structured as follows. In Section 2, we describe the methodology and discuss the empirical implementation issues. In Section 3, we discuss the Monte Carlo simulation results. In Section 4, we provide a comparison of our DRRW covariance estimator and other popular competitors using real data sets. We conclude in Section 5.",Large-scale minimum variance portfolio allocation using double regularization,https://www.sciencedirect.com/science/article/pii/S016518892030107X,4 June 2020,2020,Research Article,308.0
"Lepetyuk Vadym,Maliar Lilia,Maliar Serguei","Bank of Canada, 234 Wellington St., Ottawa, Ontario K1A 0G9, Canada,The Graduate Center, CUNY, NY 10016, USA and CEPR,Santa Clara University, 500 El Camino Real, Santa Clara, CA 95053, USA","Received 20 September 2019, Revised 22 April 2020, Accepted 23 April 2020, Available online 21 May 2020, Version of Record 25 June 2020.",https://doi.org/10.1016/j.jedc.2020.103926,Cited by (12),.,"The Canadian economy did not experience a 2007 subprime crisis and was not initially hit by the Great Recession, unlike the U.S. and Europe; see a speech of the Bank of Canada Deputy Governor Boivin (2011). Nonetheless, after few months into the recession, Canada entered a prolonged episode of the effective lower bound (ELB) on nominal interest rates.==== In the paper, we investigate a hypothesis that the ELB crisis was contaminated to Canada from abroad, in particular, from the U.S.====Bank of Canada has a well-developed macroeconomic model of the Canadian economy called the Terms of Trade Economic Model (ToTEM); see a technical report of Dorich et al. (2013). That model is huge – 356 equations and unknowns and 215 state variables – and is analyzed exclusively by linearization-based methods. In the paper, we construct a scaled-down version of ToTEM, which we call a “baby” ToTEM. The model is still very large: it includes 49 equations and 21 state variables. To solve it, we introduce a deep learning (DL) projection algorithm – a combination of unsupervised and supervised machine learning techniques – capable of constructing global, fully nonlinear solutions.====We calibrate the bToTEM model by following the ToTEM analysis as closely as possible, and we check that our scaled-down model reproduces remarkably well the impulse response functions of the full-scale model. We conduct two empirically relevant policy experiments related to the ELB episode in Canada during the Great Recession. In the first experiment, we introduce into bToTEM a sequence of foreign shocks from ToTEM; and in the second experiment, we analyze a change in the inflation target from 2 to 3 percent.====Our analysis delivers several interesting results. First, we demonstrate that the international transmission of ELB is empirically plausible mechanism for explaining the Canadian ELB experience. To be specific, in the beginning of the Great Recession, Canada faced a dramatic reduction in foreign demand (in particular, in the U.S. demand), and it proved sufficient to produce a prolonged ELB episode in a realistic and meticulously calibrated bToTEM model of the Canadian economy.====Second, we demonstrate that it is relatively easy to generate realistic ELB (or ZLB) episodes in new Keynesian models via the foreign shocks calibrated from the data. In contrast, it is difficult to produce realistic ELB episodes via domestic shocks which lead to comovements that are inconsistent with basic business-cycle facts. Thus, generating an appealing domestic ELB scenario is a challenge for bToTEM, like it is for other new Keynesian models studied in the literature.====Third, we find that the Canadian economy would entirely avoid the ELB episode if the target inflation rate were 3 instead of 2 percent. However, this finding must be taken with caution. Our analysis abstracts from the issue of credibility of inflation targeting, so achieving a higher inflation target in the model is straightforward. However, it seems questionable that central banks could easily meet an increased inflation target, given persistently low inflation in many developed economies in recent years despite high degrees of monetary accommodation.====Fourth and contrary to what we expected, we find that the ELB constraint plays a relatively minor role in the bToTEM’s performance. In our baseline simulation, both local and global solution methods predict similar timing and duration of the ELB episodes. Furthermore, the presence of active ELB does not visibly affect the model’s variables other than the interest rate. We argue that a modest role of ELB is due to the presence of rule-of-thumb firms and wage-unions in bToTEM. Such agents dampen excessive responses of the economy to future shocks, ameliorating the forward guidance puzzle. The nature of the ELB irrelevance result in our model is different from the one advocated in Debartoli et al. (2019). In their case, the ELB constraint does not affect the economy because of the availability of unconventional monetary policies (forward guidance, quantitative easing, etc.) while in our case, such constraint is not quantitatively significant due to the presence of the rule-of-thumb agents.====Fifth, we discover that other nonlinearities – those not associated with ELB – can play an important role in the model’s predictions. In particular, when assessing the impact of a hypothetical transition from a 2 to 3 percent inflation target on the Canadian economy, we spot economically significant differences between the linear and nonlinear dynamics. We show that such differences are attributed to an uncertainty effect which implies large differences in steady states between linear and nonlinear solutions. The effect of uncertainty on the steady state is known in the literature but we show a simple way to control for such effect: if the initial condition for each solution is constructed in relation to its own steady state (which we view as a coherent approach), then the linear and nonlinear impulse responses look like vertical shifts of one another.====Finally and most strikingly, we find that the closing condition, used to ensure stationarity in open-economy models, plays an important role in the bToTEM dynamics. This finding is surprising because it is at odds with the well-known conclusion of Schmitt-Grohé and Uribe (2003) that closing conditions play virtually no role in the implications of the open-economy models. Our conclusion is different because the analysis of Schmitt-Grohé and Uribe (2003) relies on linearization while we focuses on nonlinear effects. In particular, in one experiment, we show that the closing conditions in linear and exponential forms lead to significantly different transitional dynamics. However, the linearized versions of these two closing conditions are identical, so the linearization analysis of Schmitt-Grohé and Uribe (2003) cannot differentiate between them. In turn, our nonlinear analysis reveals the importance of high-order terms, neglected by the linearization method.====The introduction of deep learning was critical for telling the nonlinear tale of the Canadian ELB episode.==== Models like bToTEM are intractable under conventional value function iteration and projection methods due to the curse of dimensionality. Our computational strategy differs from the conventional solution methods in three main respects. First, we introduce deep learning analysis into projection methods for analyzing dynamic economic models. Second, we combine supervised and unsupervised learning techniques into an effective computational strategy. Finally, we show how the new solution method can be used for analyzing large-scale, central banking models that are intractable up to now. There are other prominent recent papers on deep learning that had appeared while we were working on the present paper, including Duarte (2018), Villa and Valaitis (2019), Fernández-Villaverde et al. (2019), Maliar et al. (2019), and Azinović et al. (2019). We explain the relation of our work to that literature after we present our DL method.====The rest of the paper is organized as follows: In Section 2, we construct the bToTEM model. In Section 3, we describe the implementation of DL nonlinear solution methods. In Section 4, we use the bToTEM model to analyze the Canadian ELB episode. In Section 5, we assess the role of different types of nonlinearities in the bToTEM dynamics, in particular, we simulate a hypothetical increase of the inflation target from 2 to 3 percent and we analyze the role of the closing condition. Finally, in Section 6, we conclude.","When the U.S. catches a cold, Canada sneezes: A lower-bound tale told by deep learning",https://www.sciencedirect.com/science/article/pii/S0165188920300944,21 May 2020,2020,Research Article,309.0
Guthrie Graeme,"School of Economics and Finance, Victoria University of Wellington, PO Box 600, Wellington 6140, New Zealand","Received 7 November 2019, Revised 29 April 2020, Accepted 10 May 2020, Available online 20 May 2020, Version of Record 29 May 2020.",https://doi.org/10.1016/j.jedc.2020.103928,Cited by (5)," capacity on the incumbent’s assets-in-place is the same regardless of which firm invests. In intermediate cases, the incumbent is able to deter entry by making a smaller, earlier investment than the potential entrant. The smaller investment scale protects the incumbent’s assets-in-place, which offsets the incumbent’s cost disadvantage from investing in smaller steps than the entrant would choose. Nevertheless, the threat of entry constrains the incumbent’s investment behavior and limits its profitability. The model is solved using a combination of best-response iteration and the projected successive over-relaxation method.","In capital-intensive industries, incumbent firms are potentially able to deter entry by investing in additional capacity themselves. Perhaps the most famous example occurred in the U.S. titanium dioxide industry, when du Pont preempted its competitors by adding capacity in the 1970s (Ghemawat, 1984, Hall, 1990). Distinguishing investment intended to deter entry from investment for non-strategic reasons is difficult, partly due to the challenges in identifying entry threats (which will not always result in entry actually occurring). Some industry studies have made progress, however. For example, Cookson, 2017, Cookson, 2018 has recently shown that incumbents in the American casino industry are more likely to invest in additional capacity when threatened with entry. Dafny (2005) studied the response of hospitals to certain procedures becoming more profitable and found that the investment response was strongest in markets where potential entrants were most likely to be deterred by incumbent investment. Conlin and Kadiyali (2006) studied the lodging industry and found that firms in more concentrated markets invest in more idle capacity. There is thus empirical evidence that firms invest in capacity in order to deter entry. In this paper I provide theoretical evidence that incumbent firms can deter entry by exploiting the flexibility embedded in their own investment opportunities, even if entrants face (moderately) lower investment costs.====I develop a model with two competing firms that each have flexibility over the scale and timing of their investment activity. Only one firm initially operates in the industry, but there is scope for it and an entrant to add further productive capacity. This enables me to study the effect of the firms’ investment-cost structures on the incumbent’s ability to deter entry. I assume that, as in many industries, it is cheaper per unit of capacity to build a large plant than it is to add capacity in small amounts. The two firms weigh the cost savings achieved by investing in large steps against the risk that if demand grows only slowly then they will be left holding capacity they do not need. Each firm’s scale–flexibility trade-off is affected by the investment behavior of its rival.====After analyzing a wide range of possible cost structures, I find four types of equilibrium that occur. Entry occurs when the cost savings from investing in large steps are so substantial that both firms would choose to build a relatively large amount of capacity if they were able to invest first. Entry also occurs when the cost savings are so small that both firms would choose to build a relatively small amount of capacity if they were able to invest first. In both these types of equilibrium, the effect of increased industry capacity on the incumbent’s assets-in-place is the same regardless of which firm invests because both firms want to build the same amount of new capacity. However, this changes in the other two types of equilibrium. They occur when the cost savings from investing in large steps are large enough that the entrant would like to build a relatively large amount of capacity in one go, but small enough that the incumbent wants to build a relatively small amount of capacity first. The value of the incumbent’s assets-in-place will fall by less if the incumbent invests (and builds a relatively small amount of new capacity) than if the entrant invests (and builds a relatively large amount of new capacity). This offsets the incumbent’s cost disadvantage from not exploiting the cost savings from investing in large steps like the entrant. If the potential cost savings are small enough, then the incumbent’s cost disadvantage will be dominated by the reduced cannibalization of its assets-in-place. When this happens, the incumbent will invest first and the entrant will delay its own investment until some later date; otherwise the entrant will be able to invest as in the other two types of equilibrium.====In order to preempt entry, the incumbent has to invest so early that the value of the completed project is significantly lower than the required capital expenditure. The incumbent incurs this loss in order to protect the value of its assets-in-place: this is the price the incumbent pays to ensure that aggregate capacity increases by less than would be the case if entry occurred. At the level of the incumbent firm as a whole, this investment is still preferable to allowing the potential rival to invest. Overall, the threat of entry—even if that entry can be deterred—significantly reduces the profitability of investment.====The ability of the incumbent to deter entry depends crucially on its ability to choose the scale and timing of its investment. If the incumbent does not have the option to choose an investment scale that is smaller than the entrant’s then it cannot protect the value of its existing capacity by investing. Similarly, if the incumbent does not have the option to choose its investment timing then it cannot invest before the entrant. Thus, it is investment flexibility that is the barrier to entry in this model.====A third factor is needed in order for the incumbent to be able to deter entry: the costs of absorbing the new capacity into the incumbent’s existing operations cannot be too high. These costs arise in many ways and have a long history of appearing in investment models. For example, larger firms might incur higher decision-making costs if more decision-makers are involved or the board of directors needs to approve the expansion; existing capacity might have to be removed and sold at a discount before new capacity is installed (Grossman and Laroque, 1990); the investing firm’s operations might be disrupted for a fixed period of time while new capacity is installed, increasing operating costs and thereby reducing profits (Caballero, Engel, 1999, Caballero, Leahy, Cooper, 2006); existing capacity might be permanently reduced as a consequence of the expansion (Novy-Marx, 2007); expansion might divert managerial effort away from operating the existing assets. Large firms are affected more by these costs than small firms, so an incumbent faces higher absorption costs than an entrant. The papers cited in this paragraph incorporate these costs in models used for a variety of purposes, including pricing assets and understanding aggregate investment behavior. In contrast, I focus on their role in limiting entry deterrence. I show that the incumbent can often deter entry despite this cost disadvantage. However, the incumbent’s position becomes weaker as absorption costs increase, and eventually deterring entry is impossible. In situations where entry occurs, the incumbent’s threat to preempt the entrant’s investment imposes a binding constraint on the entrant’s investment timing if the absorption cost is especially low, otherwise the constraint is slack.====This paper contributes to three separate literatures, starting with work that combines real-options analysis and game theory to study firm behavior. Smets (1991) was the first author to use a real-options framework to study strategic interaction between firms. Firms were symmetric in the early literature, but Pawlina and Kort (2006) examined a duopoly in which firms incurred different costs when they invested. The firms were identical in all other respects, but the cost asymmetry meant the low-cost firm always invested first. Pawlina and Kort found that when the first-mover advantage is sufficiently strong, the low-cost firm cannot delay its investment without being preempted by its high-cost rival. In contrast, when the cost asymmetry is sufficiently large, this preemption constraint is no longer binding. Pawlina and Kort made two restrictive assumptions: the scale of investment was fixed (and the same for both firms) and each firm could invest at most once. The subsequent literature has relaxed these assumptions in various ways. For example, Huisman and Kort (2015) allow the two firms to choose the scale of their investments, but each firm can still invest only once. The low-cost firm invests first in the modified model as well, but now the low-cost firm builds more capacity than the high-cost firm. Huberts et al. (2019), Lavrutich et al. (2016), and Lavrutich (2017) also allow the firms in a duopoly investment game to choose the scale and timing of their investment, but the firms can still invest only once. Huberts et al. assume that one of the firms already has some capacity in place, Lavrutich et al. allow for a hidden third competitor to enter, and Lavrutich adds an exit option to the model. Boyer et al. (2012) consider a duopoly in which firms can invest multiple times; the firms have flexibility over when they invest, but the scale of each investment is fixed. At any stage of the investment game, the smaller firm has a greater first-mover advantage because its rival has more to lose from cannibalization and so waits longer before undertaking its own investment. In equilibrium, the smaller firm invests first.====The second literature to which this paper contributes examines what constitutes a barrier to entry. The model in this paper provides a more realistic alternative to analyses of entry based on the theory of contestable markets (Baumol et al., 1982). The barriers to entry in this model arise from the ability to choose the scale and timing of investment and absorb new assets into existing operations at low cost. Investment flexibility contributes to entry deterrence in two distinct ways. First, investment flexibility makes it possible for the incumbent to make a small, early investment, which keeps aggregate industry capacity as small as possible, while still deterring the entrant from investing before the incumbent. Second, investment flexibility prevents the potential entrant from committing to add only a small amount of capacity. If the potential entrant could make such a commitment, then the incumbent would allow it to invest, but instead the incumbent has to invest early to protect its assets-in-place.====Many authors have found that an incumbent can deter entry by over-investing in capacity and inducing the potential entrant to either under-invest or not invest at all, both in the old deterministic literature (Dixit, 1979, Dixit, 1980, Spence, 1977) and the more recent real-options literature. For example, Huisman and Kort (2015) find that an investment leader can over-invest in capacity to such an extent that the follower does not invest immediately after the leader.==== More recent work has shown that early under-investment can also deter entry. For example, Huberts et al. (2019) find that the incumbent invests first by investing early and building a small amount of capacity. Huberts et al. assume zero absorption costs and a constant variable cost of investment, so do not investigate the role of the scale–flexibility trade-off that is the underlying driver of the results in this paper.====Finally, the paper also makes a methodological contribution. Real-option games are usually solved by constructing closed-form solutions for the firms’ value functions for arbitrary investment policies and then using these value functions to identify equilibrium behavior.==== Even in simple cases, there can be many different permutations to consider, relating to which firm invests first, whether the follower’s preemption threat imposes a binding constraint on the leader, and the number of regions in which both firms delay investment. The underlying model structure therefore needs to be relatively simple in order for closed-form solutions for the value functions to be obtainable. Thus, the usual solution approach constrains choices such as the stochastic process driving the exogenous state variables and the players’ payoff functions. This paper shows how to find Markov perfect equilibria for real-option games using a combination of the projected successive over-relaxation (SOR) method to obtain the players’ value functions and best-response iteration to obtain equilibrium strategies. This combination of two numerical methods offers the possibility of greatly expanding the collection of real-option games that can be analyzed.====The key to using this approach is to express each firm’s optimal stopping problem as a linear complementarity problem (LCP). This approach has been used to value options since Brennan and Schwartz (1977) showed that the conditions determining the arbitrage-free price of an American put option could be formulated as an LCP. The theoretical foundations of this approach were subsequently developed by Jaillet et al. (1990). If the model structure is reasonably simple then it can be possible to find an exact solution to the LCP, as Bar-Ilan et al. (2002) did for their model of investment behavior featuring scale and timing flexibility, and Bar-Ilan and Sulem (1995) did for their model of inventory management. However, for many interesting problems it is necessary to use numerical methods to solve the associated LCP. A variety of numerical approaches based on finite difference methods have been used. For example, Conrad and Kotani (2005) and Guthrie and Kumareswaran (2009) use the projected SOR method to analyze oil drilling and forestry management, respectively. Chiarella and Kang (2013) use the same technique to value compound options when volatility and interest rates are stochastic. Insley (2002) and Insley and Rollins (2005) use a penalty method to solve a forestry management problem. Chang et al. (2015) use the finite volume method to analyze real options in the presence of climate change. These papers all use the LCP approach to solve real-options problems involving a single decision-maker, with no role for strategic interaction. In contrast, this paper shows how the LCP approach, in tandem with the projected SOR method, can be used to solve real-option problems involving strategic interactions between two decision-makers.====Section 2 describes the structure of the model of investment competition and Section 3 derives the conditions that define a Markov perfect equilibrium for the underlying investment game. I describe the numerical algorithm in Section 4 and use it to construct Markov perfect equilibria for several examples of the model in Section 5. In order to better understand the determinants of equilibrium investment behavior, I carry out sensitivity analysis in Section 6. I describe two extensions of the main model in Section 7, before concluding the paper in Section 8.",Investment flexibility as a barrier to entry,https://www.sciencedirect.com/science/article/pii/S0165188920300968,20 May 2020,2020,Research Article,310.0
"Giulietti Monica,Otero Jesús,Waterson Michael","School of Business and Economics, Loughborough University, United Kingdom,Facultad de Economía, Universidad del Rosario, Colombia,Department of Economics, University of Warwick, United Kingdom","Received 19 August 2019, Revised 18 March 2020, Accepted 29 April 2020, Available online 11 May 2020, Version of Record 23 May 2020.",https://doi.org/10.1016/j.jedc.2020.103927,Cited by (2),We assess the extent of inertia in grocery retail prices using data on prices and costs from a large supermarket chain in Colombia. Relative to previous work our ,"This paper contributes to the extensive macroeconomic literature on nominal rigidities by providing evidence on relative movements of price and cost changes for a wide variety of items, based on a detailed and unique micro data set of daily prices and costs from a major supermarket chain in Colombia, a developing country with relatively low levels of inflation.====According to the macroeconomic literature, nominal rigidities play an important role in explaining key economic processes, such as the dynamic relationship between money, real output and prices, and the evolution of retail price inflation over time; see e.g. Taylor (1999). Traditionally, nominal rigidities have been modelled in the macroeconomic literature as “sticky” prices, that is prices that respond slowly to exogenous shocks, such as unanticipated economic policy interventions or international events. In turn, a wide range of papers have debated the question of whether final goods prices are sticky or not.====In fact, there are two literatures concerning price flexibility that have been the subject of significant debate. One is the long-standing debate on flexibility of prices and its importance in macroeconomics to the presence of full employment, leading from Keynes (1936) criticism of the classicists. The other, also with a significant literature, is the debate regarding asymmetry of price changes to cost movements, surveyed in Meyer and von Cramon-Taubadel (2004). As the latter authors point out, there are at least three reasons to be concerned about possible asymmetry in this context. First, there may be gaps in economic theory (Peltzman, 2000), second there are potential welfare implications, and third, they may indicate market failures arising from monopoly power. However, the differences between these two literatures is arguably more apparent than real, since the effect of Keynesian price inflexibility is related to wage (i.e. in part cost) inflexibility and thus to the linkage between prices and costs. Given this context, we aim to contribute both to the literature on price flexibility or inflexibility in the longer term as well as the literature on the relationship of prices to costs. In doing this, we take account of the findings of Kehoe and Midrigan (2015) who show that although there is significant high frequency price flexibility, this is accompanied by substantial low frequency price stickiness. We do so by examining regularised prices (in the form of reference prices, as in Eichenbaum et al. (2011)) as well as actual prices.====The context of our paper is a major supermarket chain in Colombia, a country that has experienced significant periods of inflation in the past but over the period we are considering was under a strict central bank-imposed regime targeting control of inflation. Although we characterise the retailer as a supermarket, in fact the range of goods sold is extremely wide and our sample, while capturing only a small number of products, takes full advantage of this. A (probably different) supermarket chain was the target of a price-scraping exercise by Cavallo (2018). Price-scraping enables the capture of a very large number of prices but is not able to capture cost information.====There is a small previous literature on nominal price rigidities in Colombia. To our knowledge the first paper in this area is Jaramillo and Cerquera (1999), which provides evidence in support of the menu costs hypothesis during the years 1991-94, a period characterised by double digit inflation between about 20% and 30%. In 1991 a new political constitution radically modified the structure and functions of the central bank in Colombia (Banco de la República), with the purpose of creating an institution independent from the central government. Since the 1991 constitutional reform, inflation has been exhibiting a declining path to one-digit levels.====Studies that have been able to utilise cost information in examining price rigidity are very much in the minority. Salient exceptions are Eichenbaum et al. (2011), where an approximation to the replacement cost of an item by the retailer is constructed using data on sales and adjusted gross profit, and Anderson et al. (2017) who have data on “base wholesale costs” in their scanner-based analysis of price movements in response to cost movements, which they use to see whether prices or “regular prices” move with costs.==== Earlier, the famous paper by Peltzman (2000) discovered asymmetries in the movement on prices dependent on whether they were rising or falling. Nakamura and Zerom (2010) engage in a detailed investigation of the coffee market, in particular examining the relationship between commodity prices and wholesale prices and finding a role for local costs as well as markup adjustment. Our investigation, by contrast, has a wide range of products but is cleaner, or more limited, in examining only local cost movements. Sherman and Weiss (2015) look at competition in the small, between stallholders in a Jerusalem market, some of whom have nearby competitors whilst others do not. Chevalier et al. (2003) are also able to back out cost data in their study on Dominick’s food stores in the US but have a different research question in mind, understanding pricing behaviour in relation to public holidays.====The distinctive aspect of our sample is that we have scanner data on grocery prices and also on replacement costs from a broad-range major supermarket chain in Colombia. The data at our disposal are observed with daily frequency over a 2007-2010 study period, enabling us to see whether temporal aggregation conceals important movements. Although the sample period is not recent, a secondary benefit is that it nonetheless allows us to fill a gap in the existing literature through the examination of a developing country over a period of low and stable inflation (5.3% on average during the period under consideration). This is in marked contrast to most existing studies on developing countries which have focused on episodes of high inflation; see e.g. Ahlin and Shintani (2007) and Gagnon (2009).====On the one hand, one might expect to observe more prevalent price rigidities in the country as a result of the dominance of cash transactions.==== Indeed, Knotek (2011) finds evidence in support of the view that in economies where the use of cash is prevalent, prices which facilitate rapid transactions because they require few monetary units or little change in return help to generate price rigidity, as these prices are easier to remember. On the other hand, Colombian supermarket chains contribute to the development of new business formats by providing credit alternatives to customers for the acquisition of durable consumption goods, opening super-stores and express stores in low- and middle-income cities across the Colombian territory, and fostering the marketing of their own products. The introduction of these new business practices might be expected to change pricing structures through increased competition in the supermarket sector.====Following data description in section 2, in section 3 we carry out a range of investigations to establish the character of our data, in particular in terms of pricing patterns. Most studies of supermarket pricing, such as Nakamura (2008), Kehoe and Midrigan (2008), and Eichenbaum et al. (2011), hereafter EJR, have used filtering techniques to remove what appear to be short-lived price fluctuations creating excessive noise in the data. We choose to filter prices using the approach implemented in EJR, defining a reference price (cost) as the most frequently observed (modal) price (cost) within a given time period. In our case this period is one month, given the daily frequency of our data. We also examine the nature of price endings, heterogeneity in price movements and the consequences of aggregation.==== This examination confirms a general pattern of behaviour similar to that observed in existing studies on other countries.====In our subsequent analysis, in section 4 we use time series methods to investigate the tracking of prices to costs through the framework of a conditional asymmetric error correction model reminiscent of the modelling strategy adopted by Peltzman (2000), Nakamura and Zerom (2010) and Sherman and Weiss (2015). After finding support for the presence of stable long-run equilibrium relationships between prices and costs in the majority of products, and uncovering evidence of a long-run asymmetric response of prices to costs in several instances, in section 5 we investigate potential explanations of asymmetry based on imperfect competition models where measures of market structure play a key role. Section 6 concludes.",Rigidities and adjustments of daily prices to costs: Evidence from supermarket data,https://www.sciencedirect.com/science/article/pii/S0165188920300956,11 May 2020,2020,Research Article,311.0
Barrail Zulma,"Central Bank of Paraguay, Asuncion 1767, Paraguay","Received 12 March 2019, Revised 19 April 2020, Accepted 20 April 2020, Available online 28 April 2020, Version of Record 11 May 2020.",https://doi.org/10.1016/j.jedc.2020.103917,Cited by (4), suggest that the extended model outperforms the baseline emerging market model.,"During the period 1995–2010, emerging countries in Latin America were observing a significant increase in their middle income class population as poverty rates declined (Ferreira et al., 2013). This not only increased credit demand but also motivated the entry of new suppliers in the consumer credit market. According to Obermann (2006) and Montero and Tarzijan (2010), in countries such as Mexico, Colombia, Chile and Brazil, new providers from international and national-level retail chains emerged as main credit suppliers for “new middle class shoppers”. Indeed, many countries in the region experienced a credit boom in the second half of the 2000s (Hansen and Sulla, 2013). More importantly, these credit booms – as identified by the authors – were mainly driven by unsecured credit to households.====Both the increase in the fraction of population that is more likely to have access to financial services and the increase in unsecured credit supply particularly for lower income households suggest a rise in household credit market participation.====As a larger fraction of households can freely save and incur in debt to smooth income fluctuations, aggregate consumption would tend to be less volatile. However, for emerging economies, the empirical literature has found the opposite result. Greater ability to borrow and increased access to credit may be associated with either a short term boom in consumption (Fulford, 2013) or an increase in consumption volatility in emerging markets – see Basu and Macchiavelli (2015) and Bhattacharya and Paitnaik (2016). In this paper, I provide further evidence of this phenomenon and illustrate the role of financial frictions – acting through the interest rate channel – in the relationship between household credit market participation and business cycle dynamics. The paper contributes to the discussion by finding that the level of financial access is an additional channel through which the effect of interest rate fluctuations on business cycles may be amplified in an emerging economy.====To explicitly derive business cycle implications of rising household credit market participation and explore the role of financial frictions, a household sector financial constraint is added in an otherwise standard small open economy. In particular, Chang and Fernández (2013) model is extended with an exogenous fraction of rule-of-thumb consumers coexisting with households that are able to smooth income fluctuations. This model is chosen as baseline since it encompasses two alternative mechanisms typically used by the literature to explain business cycles in emerging markets: financial frictions and shocks to the trend. In the extended model, rule of thumb consumers are households that do not own any assets nor have any liabilities; they just consume their current labor income. While there may be several interpretations for this behavior, one is their lack of access to capital markets.====The model focuses on two types of financial frictions that have proved to be important when explaining empirical regularities in emerging markets.====The first relates to cyclical changes in access to international credit or in particular, the volatile and highly countercyclical interest rates that these economies face. This empirical regularity was documented by Neumeyer and Perri (2005) and Uribe and Yue (2006) and is usually attributed to countercyclical default risk. Real interest rates in these economies are sensitive to fluctuations in output as these influence international investors’ perception of country default risk; for its microfoundations see, for example, Eaton and Gersovitz, (1981), Arellano (2008) and Mendoza and Yue (2012).====The second financial friction is the working capital constraint introduced due to its importance in explaining output contractions in emerging countries (Chang, Fernández, 2013, Mendoza, 2005, Oviedo, Uribe, Yue, 2006). The presence of a working capital constraint introduces a direct supply side effect of changes in the cost of borrowing in international financial markets. This implies input demand is sensitive to the real interest rate and adds an indirect effect of the interest rate on consumption growth. Besides the direct effect proportional to the intertemporal elasticity of substitution, exogenous shocks moving the real interest rate would have an indirect effect through employment growth.====I find that estimation of the extended model still favors a high degree of the aforementioned financial frictions in an emerging economy. In such environment, rising household credit market participation yields greater consumption growth and trade balance volatility. In particular, the larger the fraction of households using financial services in an emerging country, the more amplified are the effects of shocks in the domestic economy, particularly those transmitted through the interest rate channel. The model also predicts that lessening financial frictions dampen the increase of consumption growth and trade balance volatility driven by a rise in household credit market participation.====This paper is related to a growing body of research that uses dynamic general equilibrium models to account for business cycles in small open emerging economies. When assessing the role of particular shocks in emerging market business cycles, many papers have found that financial frictions are significant and have an amplifying effect through the interest rate channel. Whether the shocks under study were productivity related (Aguiar, Gopinath, 2007, Chang, Fernández, 2013), external financial shocks (Akinci, 2013, Fernández-Villaverde, Guerrón-Quintana, Rubio-Ramírez, Uribe, 2011, García-Cicco, Pancrazi, Uribe, 2010, Lubik, Teo, Neumeyer, Perri, 2005, Uribe, Yue, 2006) or commodity prices (Drechsel, Tenreyro, 2018, Fernández et al., 2017, Fernandez, Gonzalez, Rodriguez, 2018, Shousha, 2016), the domestic real interest rate is a key channel through which the effect of these shocks are amplified in the economy. Unlike this paper, none of these works explicitly model household limited credit market participation. By doing so, this paper, illustrates an additional channel through which the effect of interest rate fluctuations on aggregate consumption may be amplified. As byproduct of the main research, the estimation of the extended model contribute to this literature in three ways. First, standard measures of predictive accuracy suggest that a model including rule of thumb consumers outperforms the baseline model. Second, when there is limited credit market participation by households, trend productivity shocks become a more relevant source of business cycle fluctuations. Finally, financial frictions acting through the interest rate channel remain quite significant after the inclusion of rule of thumb consumers in the model.====This paper also builds on a large literature on two-agent models: rule-of-thumb and unconstrained households or those with full access to financial markets. The simplistic two-agent model assumption is chosen not only for its tractability. The important lesson emerging from this literature is that allowing for simple deviations from the strict Ricardian behavior helps capturing the aggregate effect of policy shocks (Bilbiie, 2008, Bilbiie, Straub, 2013, Broer, Harbo Hansen, Krusell, Öberg, 2020, Galí et al., 2004, Galí, López-Salido, Vallés, 2007) and/or other sources of fluctuations (see Debortoli and Galí, 2017 and references therein).====A growing literature has emerged in recent years that aims at re-examining these important questions through the lens of richer models with heterogeneous agents and allowing the presence of occasionally binding borrowing constraints. However, Debortoli and Galí (2017) show that for the purpose of approximating the effects of aggregate shocks on aggregate variables or consequences of changes in the environment, a tractable two-agent model approximates reasonably well the predictions of richer models with heterogeneous agents. Based on these findings and the purpose of this paper, adopting the simple two-agent model assumption is an interesting first step to take in the literature studying business cycles in small open emerging economies. It is left for future research the exciting task of endogenizing the fraction of households with rule-of-thumb behavior using a richer model with occasionally binding borrowing constraints. A much richer model may not only assess more adequately welfare implications but also answer other interesting questions such as the effect of monetary or fiscal policy on income and wealth distribution.====The rest of the paper is organized as follows. Section 2 presents empirical evidence that an increase in indicators of the degree to which the public can access financial services would tend to amplify the responses of macroeconomic aggregates to a country interest rate shock in an emerging economy. Motivated by this empirical evidence, Section 3 embeds a household financial constraint in an otherwise standard open economy model with financial frictions. In Section 4, the extended model is taken to Mexican data and results regarding posterior distributions of key parameters of interest and model evaluation are presented. Section 5 illustrates the relationship between rising credit market participation and aggregate volatility of key aggregates and explores the role of financial frictions. In Section 6, I examine whether an increase in household credit market participation was behind the rise in consumption growth volatility observed in Mexico during the decade 2005–2014. Section 7 presents robustness checks. Finally, Section 8 concludes.",Business cycle implications of rising household credit market participation in emerging countries,https://www.sciencedirect.com/science/article/pii/S0165188920300853,28 April 2020,2020,Research Article,312.0
"Wei Bin,Yue Vivian Z.","Federal Reserve Bank of Atlanta, United States,Emory University, Federal Reserve Bank of Atlanta, and NBER, United States","Received 30 January 2020, Revised 8 April 2020, Accepted 17 April 2020, Available online 27 April 2020, Version of Record 4 May 2020.",https://doi.org/10.1016/j.jedc.2020.103916,Cited by (5),"Liquidity backstops can mitigate runs. In this paper we develop a dynamic model of debt runs based on He and Xiong (2012) to identify, both conceptually and quantitatively, the value of a liquidity backstop for its run-mitigating role. For the purpose of identification, we focus on the municipal bond markets for variable rate demand obligations and auction rate securities. Based on the run episodes in these markets during the financial crisis of 2007-09 and the calibrated model, we find that the value of a liquidity backstop is about 14.5 basis points per annum. Our findings have important policy implications regarding the effectiveness of liquidity backstops in ameliorating problems of financial instability.",".==== However, it remains a challenge to ==== the value of a liquidity backstop for its run-mitigating role. In this paper we propose a way to identify such value both conceptually and quantitatively by using certain run episodes during the recent financial crisis as a laboratory.====To conceptualize the value of a liquidity backstop, consider two otherwise identical money-like bonds A and B issued by the same issuer. The issuer pays a fee to acquire a liquidity backstop for bond A from a liquidity provider who serves as the “buyer of the last resort.” By contrast, bond B has no such liquidity backstop. As a result, all else being equal, the market for bond B is more fragile, subject to a higher run ====It is worthwhile to point out that the value of a liquidity backstop—the focus of this paper—is very different from liquidity premia which typically refer to the extra compensation to investors for holding illiquid assets. One prominent example of liquidity premia is the spread between on-the-run and off-the-run Treasury securities (====). In this case, on-the-run securities sell at a premium because they are more liquid relative to off-the-run securities. Moreover, the liquidity difference between these two types of Treasury securities always exists during calm or turbulent periods. By contrast, a liquidity backstop insures against liquidity shortage during market freezes. It enables an asset to be safe at crisis periods or crash-proof liquid (====). Broadly speaking, the value of a liquidity backstop can be considered as “safety premium”.====To quantify the value of a liquidity backstop is challenging for the following reasons. First, it is difficult to find such a pair of “twin” securities that are otherwise identical but differ only by whether a liquidity backstop exists or not. Second, severe liquidity shortages such as runs, which necessitate the usage of a liquidity backstop, are rare. The rare occurrence of runs makes it difficult to assess run probabilities in order to identify the value of a liquidity backstop.====In this paper, we overcome the above challenges by using as a laboratory the runs during the financial crisis on the municipal bond markets for variable rate demand obligations (VRDOs) and auction rate securities (ARS). VRDOs and ARS are both municipal bonds with nominal long-term maturities and floating interest rates that are reset typically on a weekly basis. They are both money-like securities and close substitutes, but differ along one important dimension: VRDOs are typically structured with liquidity backstop facilities ==== by banks serving as liquidity providers, but there are no such liquidity backstops in the ARS market.====These run episodes suggest that whether or not a liquidity backstop exists can lead to dramatically different dynamics in otherwise almost identical markets. This is the basis for our identification of the value of a liquidity backstop in a spirit similar to the difference-in-differences approach. Specifically, in mid February to March 2008, the different experiences in these markets—only the ARS market was under run (not VRDO)—helps identify the probability that an ==== liquidity support would fail. Moreover, the different experiences in the VRDO market in 2008—it was under run in September 2008, but not so in early 2008—helps identify the probability that a ==== liquidity support in the VRDO market would fail. The value of a liquidity backstop manifests itself in equalizing run probabilities in both markets, should the ARS market possess the same liquidity backstop as in the VRDO market.====, HX hereafter). Our model has two major departures from the HX model: (i) a floating interest rate, and (ii) modeling of committed versus uncommitted liquidity provision. The model is particularly useful. First, it accounts for the “dynamic” nature of the runs in these markets; that is, fear of possible ==== runs propels more creditors to run earlier on. Second, the model’s equilibrium is characterized by a unique “====We calibrate the model to the historical interest rate data for both markets. Using calibrated key model parameters, we are able to infer the unobserved fundamental process as well as the model-implied rollover thresholds in both markets. Consistent with our identification assumption that ARS investors have started to recognize the lack of a liquidity backstop since the onset of the financial crisis, the calibrated ARS rollover threshold has since then jumped to a higher level than the VRDO threshold. Based on the calibrated parameter values and the rollover thresholds, we quantify the value of a liquidity backstop by measuring the increase in the ARS rate needed to equalize the rollover thresholds in both markets.====The value of a liquidity backstop is identified to be about 14.5 bps per annum. Interestingly, our estimate seems quite compatible with the FDIC deposit insurance premiums that range from 1.5 to 40 bps. Furthermore, our notion of the value of a liquidity backstop is closely related to “all-in-spread-undrawn” (AISU) fees for credit lines that include fees paid on the entire (or unused) committed amount, which are about 12 to 21 bps in the data (see, e.g., ==== and ====). Our estimate of 14.5 basis point is largely in line with the fees on credit lines.====Our study has several important policy implications. First, the estimation results in this paper shed light on the value of a ==== liquidity backstop, for instance, the Federal Reserve’s emergency lending facilities established during both the financial crisis and the current COVID-19 pandemic.==== (====, ====). The value of a liquidity backstop, the key focus of this paper, speaks to the central difference between the shadow banking system and the traditional banking system and gives a direct measure of how ==== the shadow banking is.====We denote by ==== and ==== two real roots of the quadratic equation ==== ====where ==== ====, ====.====The following notation is used in determining equilibrium threshold",Liquidity backstops and dynamic debt runs,https://www.sciencedirect.com/science/article/pii/S0165188920300841,27 April 2020,2020,Research Article,313.0
Ladley Daniel,University of Leicester School of Business University of Leicester LE17RH UK,"Received 17 June 2019, Revised 3 April 2020, Accepted 7 April 2020, Available online 21 April 2020, Version of Record 1 May 2020.",https://doi.org/10.1016/j.jedc.2020.103912,Cited by (8),"Central to the ability of a high frequency trader to make money is speed. In order to be first to trading opportunities, firms invest in the fastest hardware and the shortest connections between their machines and the markets. Yet this is not enough: algorithms must be short, no more than a few instructions. As a result there is a trade-off in the design of optimal high frequency trading strategies: being the fastest necessitates being less sophisticated. To understand the effect of this tension a computational model is presented that captures latency, both of code execution and information transmission. Trading algorithms are modelled through genetic programming with longer programmes allowing more sophisticated decisions at the cost of slower execution times. It is shown that, depending on the market composition, short fast strategies and slower more sophisticated strategies may both be viable and exploit different trading opportunities. The relative profits of these different approaches vary, however, slow traders benefit and social welfare increase in the presence of HFTs. A suite of regulations are tested to manage the risks associated with high frequency trading, the majority are found to be ineffective, though constraining the ratio of orders to trades may be promising.","The phrase ‘time is money’ neatly captures the business model of high frequency traders (HFTs). These traders make money by being the first, whether that’s the first to trade against incoming orders or the first to revise stale quotes in the event of changing market conditions. Slower algorithms miss out on the best opportunities and bear more risk. As a result there is an arms race amongst HFT firms to create algorithms that can identify mispricings and execute new orders in the fastest time. The time to execute an order depends on several factors: the time taken for signals to travel between the exchange and the HFT computers, the computer hardware, and the algorithms that run on them. HFT firms pay to minimise the first two factors and not be at a disadvantage to their competitors. This includes co-locating of their hardware with that of the exchange==== and purchasing the fastest and most up to date computer hardware. The final component, the algorithms that govern trade, are the source of competitive advantage for HFT firms but also represents a trade-off. Longer trading algorithms take more computational cycles to execute and therefore result in slower actions. By reducing the length of an algorithm the HFT firm makes their order more likely to be first. Shortening algorithms, however, has a consequence - reducing the number of instructions reduces the information processing capacity of the algorithm - reducing the algorithm’s ability to identify profitable opportunities and avoid losses. This is a fundamental trade-off in the design of HFT algorithms. The shorter the algorithm the more likely it is to be first to act but the less sophisticated its strategy.====It is this trade-off that will be the basis of the investigation in this paper. A model is constructed of the behaviour of HFT algorithms subject to a speed/sophistication trade-off. HFT traders endogenously decide when to trade based on the actions of others and information arriving at the market. I analyse the problem faced by HFT strategy designers - what is the optimal strategy when speed may be traded-off against information processing ability? Whilst highlighted in the media (O’Brien, 2014) and by professionals (Sapir, 2019) this issue has not previously been considered in the scientific literature. Multiple papers have looked at trading speed in the face of technological costs (see for instance Biais et al., 2015 or Delaney, 2018), however, they have done so in an environment of perfect rationality. Similarly papers such as Huang and Yueshen (2018) and Bernales (2019) examine information acquisition and speed acquisition as separate dimensions. Here I make a new argument that these two are linked - the cost of speed is not just monetary but also in terms of cognitive (computational) sophistication, i.e. in order to increase speed, information processing capacity and therefore perfect rationality has to be sacrificed. The extent to which traders are willing to do this is not clear. For instance it is not inevitable that algorithms will be ever simpler and faster. Unsophisticated trading algorithms may leave money on the table that slower and more sophisticated HFTs may identify and capture.====In order to investigate this question it is necessary to have a representation of strategies where computational sophistication is related to time in a realistic manner. The natural choice for this is to use a computational approach in which high frequency traders are, like in real life, algorithms. Longer algorithms, as measured by the number of instructions, generally take more time to execute as the computer processor must step through each instruction in turn.==== The difficulty of this approach is then specifying the optimal trading algorithm(s). The relationship between code length, algorithm design and performance is complex and non-tractable. To resolve this I optimise the trading algorithms by competing them against each other within a market. This process maintains those that do well whilst continuously looking for modifications and improvements that will enhance performance. The approach of optimising programmes (as opposed to parameters) is referred to as genetic programming - essentially evolving algorithms to solve a task.==== It is particularly appropriate for this case as the structure of the algorithm itself is subject to modification - it may be made longer or shorter and the instructions within it changed. Other evolutionary techniques such as genetic algorithms work by optimising parameters within a specified algorithm which in this case would omit one of the key areas of potential differentiation between traders.====For this problem genetic programming creates an attractive analogy: a market of trading algorithms competing to make profits based on the speed and sophistication of their chosen algorithm with the most successful surviving and the losses being replaced. Given sufficient time this process will lead to the identification of a steady state in which important details of trading algorithms and market behaviour no longer change. It is this state, rather than the optimisation process which produces it, that I will analyse. Genetic programming has been used to simulate trading strategies of different sophistications before. Yeh (2008) uses a genetic programming model to show that greater intelligence improves market efficiency. While Ladley et al. (2015) uses a genetic programming model to investigate the relationship between skill and market fragmentation and show that large numbers of unskilled individuals make the market more susceptible to shocks. Manahov et al. (2014) show that varying the length of trading strategies impacts trader and market performance. Importantly, however, no work has looked at the trade-off between speed and sophistication.====Using the model, I show that despite competition to be fastest and therefore first to arrive at a trading opportunity, not all HFTs take this approach in equilibrium. Whilst increasing competition between HFTs does push the population towards shorter strategies this is not universal. Some traders adopt longer strategies and are shown to generate greater per order (and per trade) profits than those adopting the fastest algorithms. These traders identify and exploit trading opportunities that their faster competitors miss. As such there are multiple equilibria in the design of HFT trading strategies.====The presence of HFTs within the market is shown to be generally positive. Increasing numbers of HFTs increase social welfare, market quality through higher liquidity and lower pricing errors. At the same time they have relatively little effect on the overall profits of slow traders, instead their profits come through improved prices and reduced waiting costs for trade. The greater competition coming from higher numbers of HFTs leads to reducing profits for this group and other liquidity provides but no negative effects for other slower traders.====The effectiveness of a suite of regulations proposed to manage the impact of high frequency traders is considered. The majority, including minimum resting times, speed bumps and increased tick sizes are found to have little or no positive effect, often damaging market quality and increasing the returns of HFTs at the expense of slower traders. Transaction taxes are found to be particularly detrimental to both the market and traders. The only regulation which potentially has a positive effect are constraints on the ratio of trades to orders submitted by the HFTs. This regulation increases social welfare and the profits of slow traders relative to HFTs but at the expense of reducing market liquidity.====The remainder of the paper is organised as follows. Section 2 considers previous work looking independently at trading speed and the effect of cognitive ability on market performance together with HFT regulation. Section 3 presents a model of a market in which traders trade-off speed and strategic sophistication. Section 4 presents results showing optimal trading strategies and market quality whilst Section 5 looks at the effectiveness of regulations. Section 6 concludes.",The high frequency trade off between speed and sophistication,https://www.sciencedirect.com/science/article/pii/S0165188920300804,21 April 2020,2020,Research Article,314.0
Chen Chaoran,"Department of Economics, York University, 4700 Keele Street, Toronto, ON M3J 1P3, Canada","Received 27 October 2019, Revised 18 March 2020, Accepted 18 March 2020, Available online 13 April 2020, Version of Record 7 May 2020.",https://doi.org/10.1016/j.jedc.2020.103902,Cited by (11),"In the postwar U.S. economy, labor productivity has been growing faster in the goods sector versus the service sector. This paper argues that this sectoral labor productivity growth gap can largely be explained by the fact that capital intensity also increases faster in the goods sector. I build a two-sector ==== in which capital substitutes low-skilled labor but complements high-skilled labor, and the goods sector is more intensive in low-skilled labor relative to the service sector, as observed in the data. As capital becomes more abundant relative to labor along economic growth, low-skilled labor is substituted by capital, leading to faster growth of capital intensity and hence labor productivity in the goods sector. Using a calibrated model, I find that two thirds of the sectoral labor productivity growth gap can be explained by capital accumulation and its interaction with capital-skill complementarity.","Between 1947 and 2018 in the United States, employment share declined from 53.3% to 19.3% in the goods sector, which includes agriculture and manufacturing, and increased in the service sector. This is the well-known pattern of structural transformation. During this period, the goods sector’s labor productivity grew by 6.27-fold, much faster than the 2.88-fold of the service sector. Many studies take this labor productivity growth gap as exogenous and use it to explain the pattern of structural transformation (Baumol, 1967, Ngai, Pissarides, 2007). This paper instead aims to explain this sectoral labor productivity growth gap through the lens of capital-skill complementarity.====To motivate my research, I document two stylized facts about the postwar U.S. economy. First, although the nominal capital-output ratio is stable in the aggregate economy as one of the Kaldor facts, it increases over time in the goods sector and decreases in the service sector. This divergence in capital intensity helps to reconcile faster labor productivity growth in the goods sector. I further examine why capital intensity increases at different rates. I begin by documenting a second stylized fact: workers are predominantly low-skilled in the goods sector and high-skilled in the service sector, with skill intensity measured as the portion of manual versus cognitive occupations, routine versus non-routine occupations, or college-educated workers. Since it is well known in the literature that capital substitutes low-skilled labor and complements high-skilled labor, capital and labor are generally more substitutable in the goods sector, which relies more on low-skilled labor.==== When capital becomes relatively cheaper than labor (Greenwood et al., 1997), the capital-output ratio increases faster in the goods sector, contributing to its faster labor productivity growth.====Motivated by these facts, this paper addresses two research questions. First, given the literature’s estimate of capital-skill complementarity, can we use the differences in skill composition between sectors to explain their different trends in capital intensity over time? Second, how much can capital accumulation, especially the different trends in sectoral capital intensity, account for the labor productivity growth gap between the two sectors?====To answer these two questions, I build a neoclassical growth model with two sectors: goods and services. Both sectors produce their output using capital, high-skilled labor, and low-skilled labor, although the goods sector is relatively more intensive in low-skilled labor. Labor productivity is hereafter defined as output divided by total labor, which is calculated as the sum of high-skilled and low-skilled labor. I follow the literature and assume that capital substitutes low-skilled labor and complements high-skilled labor. This means that the elasticity of substitution (EOS) between capital and total labor at the sectoral level is endogenous and depends on the composition of labor: the goods sector is relatively more low-skill intensive, and hence its EOS between capital and total labor is higher than that of the service sector. In addition, I allow for exogenous total factor productivity (TFP) growth in both sectors as well as investment-specific technological change as in Greenwood et al. (1997), which implies that capital becomes more abundant and less expensive relative to both types of labor over time. This capital deepening is more substantial in the goods sector given its higher EOS. That explains the observed fact that capital intensity increases faster in the goods sector. I close the economy in a general equilibrium by assuming a representative household that makes decisions on consumption and investment in capital, while treating the endowment of high-skilled and low-skilled labor as exogenous.====I calibrate my model to postwar U.S. data. Particularly, the endowment of high-skilled labor over time is approximated by the portion of cognitive occupations in the economy. Most parameters governing technology and preferences are determined using moments solely from the initial year 1947 and are fixed thereafter. I then allow the exogenous sectoral TFP and the productivity of the investment-specific technology to grow over time. Note that I determine the growth rates of sectoral TFP such that, in equilibrium, exogenous sectoral TFP and endogenous capital accumulation together generate the observed labor productivity growth. The calibrated model matches the historical pattern of structural transformation and other data patterns whose change over time are not targeted in the calibration, such as the allocation of capital between sectors, the fractions of high-skilled labor in both sectors, the skill premium, and the relative price between goods and services. Importantly, the model replicates well the stylized fact that the nominal capital-output ratio increases (decreases) in the goods (service) sector, matching the magnitude of changes without requiring explicit targeting in the calibration. This means that the disparity in sectoral capital intensity trends can be almost fully explained by different skill composition between sectors and capital-skill complementarity.====I then decompose the sectoral labor productivity growth gap into a capital deepening component and a component unrelated to capital accumulation, consisting of mechanisms such as the aforementioned sectoral TFP growth. To do so, I shut down capital accumulation entirely in the model. The model then implies that the sectoral labor productivity growth gap shrinks substantially from 2.18-fold to 1.30-fold. The remaining ====-fold gap is then contributed by capital deepening and its interaction with capital skill complementarity, which is 65.9% or roughly two thirds of the entire gap (calculated as log (1.67)/log (2.18)). I further investigate this capital deepening component and quantify the portion of the entire labor productivity gap that can be directly attributed to changes in the sectoral capital-output ratio arising from capital-skill complementarity. I then design another experiment that allows for capital accumulation but restricts the allocation of capital between sectors such that their capital-output ratios remain proportional to the initial level, which is exogenously determined in my calibration. The predicted sectoral labor productivity growth gap is then 1.49-fold. This means the remaining ====-fold, or 48.4% of the entire gap, is directly contributed by endogenous changes in the sectoral capital-output ratio arising from capital-skill complementarity. I hence conclude that the sectoral labor productivity growth gap is not a mystery: capital accumulation and its interaction with capital-skill complementarity broadly comprise two thirds of this gap, and half of this gap is directly related to changes in the sectoral capital-output ratio.==== Differences in sectoral TFP growth are relatively less important.====The model also has other implications. With capital-skill complementarity, capital accumulation implies an increasing trend in skill premium as we observe in the data. In addition, since capital-output ratio, and hence capital income share, changes endogenously at the sectoral level, standard growth accounting should be implemented with caution when we quantify sectoral labor productivity growth over time. Lastly, the model endogenizes EOS between capital and total labor at the sectoral level, and the difference between goods versus service sector EOS is consistent with literature estimates.","Capital-skill complementarity, sectoral labor productivity, and structural transformation",https://www.sciencedirect.com/science/article/pii/S0165188920300701,13 April 2020,2020,Research Article,315.0
"Diem Christian,Pichler Anton,Thurner Stefan","Institute for Statistics and Mathematics, WU Vienna University of Economics and Business, Welthandelsplatz 1, A-1020, Austria,Complexity Science Hub Vienna, Josefstädter Straße 39, A-1080, Austria,Institute for New Economic Thinking, University of Oxford, Manor Road, OX1 3UQ, UK,Mathematical Institute, University of Oxford, Woodstock Road, Oxford OX1 3LP, UK,IIASA, Schlossplatz 1, Laxenburg A-2361, Austria,Section for Science of Complex Systems, Medical University of Vienna, Spitalgasse 23, A-1090, Austria,Santa Fe Institute, 1399 Hyde Park Road, Santa Fe, 87501 NM, USA","Received 15 May 2019, Revised 10 February 2020, Accepted 8 March 2020, Available online 12 April 2020, Version of Record 28 May 2020.",https://doi.org/10.1016/j.jedc.2020.103900,Cited by (39),We quantify how much ,"Increasing capital requirements for market participants is an obvious suggestion for improving the resilience of financial systems and, in particular, for reducing systemic risk in financial markets. Examples of innovative policy proposals, where capital requirements depend on macroprudential regulation are Cont et al. (2010), who propose capital requirements in relation to the Contagion Index values of banks, Gauthier et al. (2012), who suggest that bank capital buffers should correspond to their contributions to overall systemic risk, Markose (2012), who proposes a capital surcharge related to the eigenvector centrality of banks in the financial network, and Alter et al. (2015), who show that capital requirements based on eigenvector centrality can save up to 15% of total system losses. Moreover, in the classical risk measure literature, following Artzner et al. (1999) and Föllmer and Schied (2002), the risk of an asset is measured by the amount of capital that needs to be added to the position in order to make the position acceptable to the regulator or to the firm itself. This approach can be extended to determine the capital requirements for financial institutions to bring systemic risk to levels that are acceptable to the regulator; see for example, Feinstein et al. (2017) or Biagini et al. (2018).====In the recent past, after the last financial crisis, bank capital requirements have been adjusted upwards. In the Basel III Accord the regulatory minimum capital requirements for Common Equity Tier 1 (CET1) have been increased from 2% to 4.5%, and Tier 1 Capital from 4% to 6% (BCBS, 2011a). Additionally, a capital conservation buffer has been introduced by increasing CET1 and Tier 1 capital further to 7% and 8.5%, respectively. On top of this, national authorities can set an additional counter-cyclical buffer in the range between zero and 2.5% for phases of excessive credit growth. Global systemically important institutions have to meet additional CET1 requirements in a range of 1–2.5% (BCBS, 2011c).====Bank capital levels have been steadily increasing since the introduction of these new regulations. The monitoring report of the Basel Committee (BCBS, 2011b) shows that for a sample of 86 international banks with Tier 1 Capital larger than $3bn, the CET1 increased from 7.2% to 12.7% in the period from 2011 to 2018 (BCBS, 2011b, Graph 15). For Germany, Spain, France, and Italy, ECB data shows increases in Tier 1 capital ratios from 9.2%, 8.1%, 8.4% and 6.9% to 16.4%, 13.2%, 15.3%, and 14.4%, respectively, for the period from 2008 to 2017====. Nonetheless, some indicators of systemic risk suggest that systemic risk levels are not declining, but are still substantially higher than before the financial crises. A prominent example is the SRISK indicator of Brownlees and Engle (2016), which shows that the systemic risk level in Europe now is twice as high as it was before the crises====.====However, capital levels for absorbing shocks are only one part of the story in the context of systemic risk. The other essential component that determines systemic risk is the exposure network that is generated by contracts between financial agents. In particular, these networks capture the risks of potential cascading events that could threaten large fractions of financial markets with failure. This fact is reflected in a number of works such as in Allen and Gale (2000), Freixas et al. (2000), Eisenberg and Noe (2001), Boss et al. (2004a), Cont et al. (2010), Gai and Kapadia (2010), Battiston et al. (2012c), Markose et al. (2012) Thurner and Poledna (2013) and Glasserman and Young (2015).====It is therefore natural to ask what contributions to systemic risk originate specifically from networks and how their topology influences systemic risk. Indeed, many contributions to the systemic risk literature investigate the effect of network characteristics on systemic risk. Allen and Gale (2000) compare the effects of different network topologies, such as rings, fully connected graphs, and interconnected subgroups on interbank market stability. In Boss et al. (2004a) the role of scale- free network topologies in the context of systemic risk and stability is discussed. In Boss et al. (2004b) the betweenness centrality measure is introduced as a network-based measure for systemic risk. Nier et al. (2007) investigate the effects of network connectivity and concentration on contagious defaults. Gai and Kapadia (2010) employ a stylized analytical contagion model and look at the fraction of defaulting banks for given average degrees. Puhr et al. (2012) employ panel regressions to study the effects of network measures like Katz centrality on the number of defaulting banks, which are obtained from a simulation study. The concept of ==== is also part of this discussion and is investigated, for example, by Markose et al. (2012). Glasserman and Young (2016) dedicate a considerable part of their literature review to this topic. These and many more theoretical and empirical works indicate the possibility of using networks of financial connections as a leverage point for reducing systemic risk in a financial system as an effective alternative to costly capital requirements that have been shown to have limited effects on systemic risk reduction (Poledna et al., 2017). We also find in our current study that the reorganization of the interbank networks can yield lower levels of DebtRank than a Basel III-like equity increase. If systemic risk can be effectively reduced by altering the underlying exposure network characteristics, this should be prominently factored into financial market stability policies. It is therefore essential to systematically estimate the full potential for network-based systemic risk reduction.====In this work we propose a method for quantifying the systemic risk reduction potential in empirically observed direct exposure networks by employing standard optimization techniques. The systemic risk of a network is measured with the so-called DebtRank (Battiston et al., 2012c). The actual optimization relies on an approximation of the DebtRank, because the DebtRank is computed iteratively and is thus hard to use in optimization problems. The approximation is based on the direct impacts of defaulting banks on their neighboring nodes in the exposure network. We show how the systemic risk optimization can be solved as a mixed integer linear program (MILP) by standard reformulation techniques. The optimization problem can be solved by state-of-the-art optimization algorithms and could therefore also be easily implemented in practice. In the empirical part of this study we show the effectiveness of the proposed method by applying it to a data set containing ten quarterly observations of the Austrian interbank liability network from 2006 to 2008. Our findings for the 70 largest banks suggest that the DebtRank of individual banks can be reduced on average by a factor of 3.5 or 71%. This means sizeable reductions of the DebtRank for almost all of the 70 banks across the ten quarters with only a few exceptions. We evaluate the effectiveness of our optimization by calculating that bank equity would need to be increased by factors of between 2.38 and 4.26 to achieve the same level of DebtRank in the respective quarter. The average scaling factor is 3.32, thus on average 232% of the existing bank capital would need to be added to the banking system to reduce the DebtRank of the empirical networks to the level of the optimized networks. For comparison, the increase in Tier1 capital from 4% to 8.5% by Basel III corresponds to a scaling factor of 2.125. In comparison to this Basel III scenario, the optimization still reduces DebtRank by a factor of 1.61.====In practice, due to the current lack of incentive schemes for systemic risk management Leduc and Thurner (2017), financial networks do not evolve toward systemically optimal configurations, and obviously they do not result in any way from such optimization procedures. However, our study can give an estimate for the systemic risk reduction potential stemming from a specific reorganization of empirically observed networks. The same optimization algorithm can be used to compute network configurations that yield a maximum of overall systemic risk. In this way, for any observed financial network, the proposed optimization procedure yields a “range” of network structures, corresponding to minimal and maximum DebtRank. This allows us to identify network characteristics that are typical for low, medium, and high DebtRank.====Closely related studies include Poledna and Thurner (2016) and Leduc and Thurner (2017), which investigate how systemic risk can be reduced by changing the underlying networks when financial agents are incentivized to favor transactions with low systemic risk in the network. The idea of applying network optimization techniques that are commonly used in operations research to systemic risk reduction is relatively new. It has been pioneered in the specific context of overlapping portfolio and fire sales by Pichler et al. (2020) who find reductions in systemic risk of around 50% by rearranging the network structure of the 49 major european banks’ government bond portfolios. However, the optimization approach there – a quadratically constrained quadratic program (QCQP) – is substantially different from the one presented here. A recent paper by Krause et al. (2019) focuses on small homogeneous macroeconomic shocks affecting the assets of all banks simultaneously and how these shocks are amplified in the banking system. They show a Monte Carlo algorithm for finding minimal and maximal networks with respect to the amplification of such small homogeneous macro shocks. Another related study is Aldasoro et al. (2017). The authors employ a theoretical model of the interbank network where optimizing risk-averse banks invest in illiquid assets and lend to each other. In their model they account for contagion originating from liquidity hoarding, interbank interlinkages, and fire sales. Their model leads to a specific interbank network for which properties of the network topology are reported.====The paper is organized as follows. Section 2 presents our approach to quantifying systemic risk. In Section 3 we derive the optimization problem for reducing DebtRank. We discuss the data and the results of the application to the Austrian interbank market in detail in Section 4. We conclude in Section 5.",What is the minimal systemic risk in financial exposure networks?,https://www.sciencedirect.com/science/article/pii/S0165188920300683,12 April 2020,2020,Research Article,316.0
Tsiaplias Sarantis,"Melbourne Institute of Applied Economic and Social Research, The University of Melbourne, Australia","Received 2 July 2019, Revised 15 March 2020, Accepted 17 March 2020, Available online 10 April 2020, Version of Record 23 April 2020.",https://doi.org/10.1016/j.jedc.2020.103903,Cited by (5),The ,"Inflation expectations are pivotal in economics and finance, playing an important role in the price setting behaviour of firms, asset pricing and valuation, the aggregate price formation process and the effectiveness of monetary policy (Ang, Bekaert, Wei, 2008, Bernanke, 2007, Duffee, 2018, Fuhrer, Mishkin, 2007). This paper uses the inflation expectations of over 285 thousand individuals randomly surveyed over a period of 21 years to learn about the relationship between inflation expectations and aggregate price changes. In addition to examining the mean of inflation expectations, attention is paid to the higher-order moments of the cross-section of inflation expectations of which relatively little is known.====A key novelty of the paper is that it uses the higher-order moments of the cross-sectional distribution of inflation expectations to learn about the time-varying dispersion, asymmetry and extremeness of consumer price expectations. These measures examine different facets of time-varying consumer disagreement. The higher-order cross-sectional moments examined in this paper can be contrasted with the higher-order moments of beliefs at the individual level (e.g. what agents believe about the beliefs of other agents as in Coibion et al., 2018).====This paper draws on the work of Mankiw and Reis (2002), Carroll (2003) and Mankiw et al. (2004), but takes a broader view of disagreement. Rather than limiting the analysis to the central tendency and dispersion of inflation expectations, the entire cross-section is examined. Two key questions are addressed. First, is time variation in the level of consumer disagreement informative for future inflation? And two, does the data support the common assumption that the higher-order moments of the cross-sectional distribution of consumer inflation expectations constitute noise? Although the main focus of the paper is on the informativeness of the higher-order moments, the findings have implications for the diffusion of economic information (and the presence of information rigidities in consumer expectations) and these are also considered.====I find that there is more to the relationship between prices, inflation and inflation expectations than can be deduced by simply considering the first moment (or even the first two moments) of inflation expectations. Consistent with previous research (Andrade, Crump, Eusepi, Moench, 2016, Mankiw, Reis, Wolfers, 2004), the level of consumer dispersion observed in the dataset is time-varying. However, this paper also provides evidence that the higher-order cross-sectional moments exhibit ==== time-variation. This time-variation is not noise but is instead informative about future aggregate prices. Measures of consumer disagreement such as the dispersion and extremeness of inflation expectations are at least as important as the first moment in explaining aggregate price changes. This result is observed to hold both in and out of sample.====The analysis is undertaken using a novel individual-level dataset of the monthly inflation expectations of around 1200 individuals each month from 1995 to 2016. A benefit in using these moments relative to those obtained using the University of Michigan Survey of Consumers is that the typical monthly sample size is more than twice the size of the Michigan survey (which is based on 500 respondents for a population of over 300 million persons, relative to 1200 respondents for a population of less than 25 million for Australia). This distinction is important for the accurate measurement of the higher order moments and the capacity to reliably reflect the distribution of inflation expectations in the general population. A key issue here is that, by comparison to the first moment, the higher-order moments have relatively large standard deviations with the rate of convergence being slower for these moments.====After establishing a basic statistical relationship between the cross-section of inflation expectations and inflation, the paper examines the extent to which this relationship is contingent upon consumers forming expectations that are extrapolated using historical economic data. This relationship is estimated using a vector autoregressive approach that is unique in treating all four moments of the cross-section as potentially endogenous. Accordingly, the role of economic and financial data for each of the respective moments is estimated after accounting for the information already embedded in the lags of the entire distribution of consumer expectations.====The estimates indicate that the mean of inflation expectations is highly sensitive to historical economic and financial data. Consumers only partially adjust the mean of their inflation expectation in response to a change in the cash rate and therefore expect that tighter (looser) monetary policy will result in higher (lower) real interest rates. The higher-order moments, however, exhibit lower levels of sensitivity to lags of key economic and financial variables. Whereas the mean of inflation expectations is heavily extrapolated from existing economic data, the level of disagreement is only weakly associated with historical data.====To better understand these results, the paper investigates the extent to which unanticipated (or unpredictable) information in the cross-section constitutes noise. The results for the higher-order cross sectional moments show an insensitivity to much of the recent macroeconomic news, but the moments are clearly not noise. In effect, the higher order moments are neither conventional macroeconomic (or financial) news nor noise yet exhibit an association with future inflation that is both statistically significant and sizeable in terms of its economic impact.==== The higher-order moments appear to reflect unpredictable shocks (for example, tension in the Middle East) that are correlated with variables such as the volatility of commodity prices and inflation (both of which are significantly associated with the asymmetry and extremeness of inflation expectations). The findings contradict the conventional practice of trimming inflation expectations data to remove what is commonly deemed to be noise.====An additional benefit of using the empirical moments of consumer inflation expectations is that the relationship between the cross-section of expectations and realized inflation can be examined without imposing a priori restrictions on the expectation formation process (such as rational or extrapolative expectations). Rather, the actual expectations of consumers are used to derive the moments, and any deviation from rational expectations is reflected in these moments. The moments also reflect any inherent stickiness (or information rigidities) in the expectations of consumers and do not depend on assumptions regarding the extent to which consumers adjust their expectations from one period to the next. This is useful for obtaining data-driven measures of the information rigidities that influence the formation of consumer expectations.====Although the key literature on information rigidities estimates that consumers typically update their information sets about once a year (Carroll, 2003, Coibion, Gorodnichenko, 2012, Mankiw, Reis, 2002, Mankiw, Reis, Wolfers, 2004), the results in this paper tend to negate the existence of this archetypical consumer. The results suggest that heterogeneity in information rigidities is needed in order to reflect the cross sectional dynamics observed in consumer inflation expectations data (see, further, Baker et al., 2019). A clear bi-modality is observed in the distribution of the information rigidity parameter that is consistent with 25% of consumers updating their forecasts every quarter, with the remainder rarely updating their forecasts. This result has significant implications for the ability to anchor the inflation expectations of the majority of consumers.====Before presenting the key findings, Section 2 provides motivating evidence for why the moments of inflation expectations are potentially relevant for aggregate price changes. Section 3 discusses the inflation expectations dataset. The basic empirical relationship between the moments of inflation expectations and aggregate price changes is examined in Section 4. Section 5 considers whether the findings can be explained by the extrapolation of historical economic and financial data, and the extent to which unanticipated information in the higher-order cross-sectional moments constitutes noise. The implications of the results for consumer information rigidities are considered in Section 6, with Section 7 providing concluding remarks.",Time-Varying Consumer Disagreement and Future Inflation,https://www.sciencedirect.com/science/article/pii/S0165188920300713,10 April 2020,2020,Research Article,317.0
"Chassamboulli Andri,Peri Giovanni","Department of Economics University of Cyprus, Nicosia CY-1678, Cyprus,Giovanni Peri, University of California, Davis, One Shields Avenue, Davis, CA 95616, USA","Received 31 October 2019, Revised 4 March 2020, Accepted 5 March 2020, Available online 9 April 2020, Version of Record 9 April 2020.",https://doi.org/10.1016/j.jedc.2020.103898,Cited by (9),"In this paper we analyze the economic effects of different immigration policies using a model that incorporates economic and policy features crucial to understanding the migrant flows into the US. We differentiate among the most relevant channels of immigration to the US: family-based, employment-based and undocumented. Moreover we explicitly account for earning incentives to migrate and for the role of immigrant networks in generating immigration opportunities. Hence, we can analyze the effect of policy changes through those channels. In our simulations comparing long-run steady states, highly skilled employment and unskilled immigrants generate larger surplus to US firms than natives do. Hence policies restricting their entry either directly or indirectly have a depressing effect on ==== and, in turn, on native labor markets. Our analysis gives new insights into the effect of policies as it accounts for the endogenous immigration response which is overlooked by most existing models.","How would different immigration policies impact the US economy? This is an important and debated question. Economists have adopted, so far, rather simplified models to evaluate the consequences of changing immigration policies on the national economy and labor markets. Usually they have analyzed the consequences of a change in the number and in the composition of foreign-born as shifts in supply within a neoclassical model (e.g. Ottaviano and Peri, 2012, or Llull 2017). Actual policies, however, are provisions changing the conditions of entry through specific immigration channels or the degree of enforcement of those conditions. The number of immigrants and their composition are themselves equilibrium outcomes of these policies. To evaluate the effect of a policy change on immigration flows, and in turn of these flows on the economy, one has to account for the impact of policies on current and future incentives for immigration. Models that produce quantitative assessments of the impact of immigration on labor markets and other economic outcomes, have usually neglected the analysis of specific policies and their general equilibrium effects accounting for networks and incentives. To do this one has to model networks of job referrals and family unification opportunities, which may create the conditions for the so called “chain migration effects”. Analyzing and simulating how specific policies affect each channel of entry in the US and, in turn, the long-run immigration flow and labor market outcomes is the goal of this paper.====Sometimes changes in immigration laws have unintended long-run equilibrium effects. Through networks and family linkage effects they may increase substantially the immigration opportunities in the future. For instance, the Immigration and Naturalization act of 1965, supposed to be a change that could preserve immigrant composition while abolishing quotas, ushered a family-based immigration system in the US, and over time allowed the largest increase in immigrants in the US. Similarly the high tolerance for undocumented immigrants in the 1990s allowed in the US a large number of undocumented foreign workers also affecting labor markets and subsequent immigration opportunities.====The U.S. federal Government does not control directly the number of immigrants entering or staying in the country. Instead, it sets rules about their entry and their opportunities to remain and it decides the intensity of enforcement of these rules. Immigration policies are not usually quotas, but rather they specify different tracks of entry and different conditions for staying in the country. These rules, together with the incentives of immigrants and the effectiveness of their enforcement, generate the observed number of immigrants and shape their composition in terms of skills/productivity, bargaining power and expected duration of stay. These elements are key to our understanding of their labor market effects. While it will be impossible to mirror the complexities of the US immigration system, our model aims at capturing the main avenues of entry and stay in the US. No study, to the best of our knowledge, has so far incorporated the important interplay between immigration policies, immigrants’ economic incentives and the role of immigrant networks when analyzing the inflow of immigrants and their labor market effects.====We develop a two-country economy that represents the US and the rest of the World, and we model in detail each of three main ways of entry: Family, Employment and Undocumented immigration. The opportunities for legal entry through each specific channel, are affected by policies and by the existing networks (of family or potential co-workers). Similarly illegal entry is affected by the degree of enforcement. A change in one policy will change entry through that channel but also the size of the immigrant network with consequences on the opportunities for entry through other channels. It will also affect incentives of foreigners to migrate through labor market tightness and wages. In order to capture key features of the US labor market, we separate high (college-educated) and low (non college educated) skilled immigrants and we consider their different labor markets and opportunities for entry.====To reflect the current immigration system in the US we assume that the employment route for legal entry is only available to highly skilled, the family route is available to skilled and unskilled, in proportion of the existing family ties, and the illegal route is pursued only by less skilled. We represent the labor market using a search and matching model, which implies that firms post vacancies for skilled and unskilled workers and workers search for jobs. Native and immigrants fill those vacancies and, as they have different outside options, this reflects on their bargaining power and wages. Finally, we model the incentives to migrate so that changes in the wage and unemployment conditions in the US will affect the incentives to move: higher wages and lower unemployment conditions would attract more immigrants. To mirror the US experience in 1990-2015 we choose parameters that reflect the fact that during that period Mexico was the most relevant origin of low skilled immigrants and Asia (especially China and India) was the most relevant source region for high skilled immigrants. We solve the model and then we calibrate it to match aggregate labor market- and immigration statistics for the US, for the average of the 2010-2015 period. We use such an equilibrium as starting point to simulate alternative policy scenarios.====By capturing these important aspects of the labor market and of immigrant entry, this model allows us three insights into the effect of different policies that would not be present in models based on the canonical labor demand and exogenous supply of immigrants. First, the job creation effects of immigrants from any entry route (family, employment and illegal) are beneficial to natives, but for different reasons. Unskilled family and undocumented immigrants produce large surplus to firms because their wages per unit of productivity are lower than those of natives due to their worse outside options. High-skilled employment immigrants, instead, are selected on ability and increase the expected firm surplus from a match due to their higher productivity. Both effects generate more job creation and tighter labor markets. These job creation channels may attenuate or reverse the prediction on native employment and wages from a pure supply/demand model.====Second, while the family route allows unskilled workers it also generates family opportunities for high-skilled immigrants who come to the US to be employed. High-skilled individuals have larger incentives to migrate and generate “network” opportunities for other high-skilled individuals through job referrals. Symmetrically, the employment route allows only highly skilled workers but it also generates family opportunities for unskilled immigrants. Hence, in the long run, family reunification policies in the US have only a marginally smaller effect in increasing the skilled/unskilled ratio vis-a-vis employment-based policies. Restricting either of the two routes turns out to have similar negative job creation effects.====Third, given that the job creation effects of immigrants from any entry route are beneficial to natives, if the overall immigration policies are balanced between skilled and unskilled, then both groups of natives will be better off not only in terms of unemployment but also in terms of wages.====An innovation of this paper is that we can illustrate the impact on native wages and unemployment from changing one channel of entry at the time. For instance we can modify the approval rate of family admissions, so as to increase the number of family immigrants by a certain percent, leaving the other policies (relative to other channels of entry) unchanged. We can then analyze what would happen in equilibrium to the inflow of other groups of immigrants (also indirectly affected by incentives and networks) and to wages and unemployment of natives. We will use the model to evaluate the potential labor market effects of some recent policy proposals. One is the plan to reduce by 50% the family reunification immigrants (which captures the main provision of the RAISE act proposed in 2017 in the U.S. Senate). Another is the substantial increase in deportation rates to reduce (by 10 or 50%) the population of undocumented immigrants (this reflects the explicit goal of the Trump administration to increased intensity of deportations of undocumented). We can also analyze the plan to reduce the most prominent temporary visa program, the H1B, (which is under scrutiny and several bills proposed in the House in 2017, aimed at making it more restrictive). This would imply a smaller number of temporary high skilled employment immigrants. Our model will provide estimates of the potential effects of these measures on US labor markets, specifically on wage and unemployment rates of natives and immigrants. At the same time we will use the model to analyze the impact on migration flows of some important structural changes in the US, such as the increase in productivity of high skilled workers (skill biased technological change) and the increase in supply of highly educated workers.====Let us notice that, as we are considering different types of labor as the fundamental inputs in production we are implicitly assuming perfectly elastic capital response to changes in immigration. This is reasonable for two reasons. First, immigration is a slow phenomenon and rather predictable. Each year in the last two decades, the inflow of immigrants has been less than 0.4 percent of the labor force, implying that capital can adjust to it without deviating much from the optimal capital-labor ratio. Second, the analysis of our policy changes has to be understood to be long-run, including adjustments likely to take place in a decade. Short-run effects of immigration due to slow adjustment of capital are likely to be small nationwide, as shown by Ottaviano and Peri (2012).==== Let us also emphasize that we do not consider other possible impact of immigrants, besides their labor market effects, such as changing local norms or affecting local culture, which may, in the long run affect efficiency, but are harder to quantify.====The rest of the paper proceeds as follows. Section 2 reviews the relevant papers in the literature and the innovative content of this paper relative to those. Section 3 describes the main components of the model, its equilibrium conditions and provides an intuition of its key mechanisms, with a focus on incentive to immigration, network effects, and the working of the labor market. Section 4 describes the calibration and parameterization of the Model, targeting the US as home country and Mexico/Asia as foreign country, using as steady state the years 2010-2015. Section 5 describes the effects of immigration policies and indirect policies that change the tightness of each entry mechanisms and allow us to discuss the channels at work. It also describes the impact of some specific structural changes in the US, on migration flows, and in turn, on the labor market. Section 6 provides some concluding remarks.",The economic effect of immigration policies: analyzing and simulating the U.S. case,https://www.sciencedirect.com/science/article/pii/S016518892030066X,9 April 2020,2020,Research Article,318.0
"Alessandria George,Avila Oscar","University of Rochester and NBER, United States,University of Rochester and Central Bank of Colombia, United States","Available online 24 March 2020, Version of Record 13 June 2020.",https://doi.org/10.1016/j.jedc.2020.103871,Cited by (4),"We study Colombia’s trade integration over a 30 year period through the lens of a ==== model in which non-exporters have access to a risky exporting technology and exporters must invest in accumulating a better exporting technology. Our model is calibrated to match producer and exporter lifecycles and yields a novel estimate of the various costs of exporting. We find the up-front costs of starting to export are much lower than in previous analyses but that this technology is quite risky in that most firms that incur the cost do not end up exporting. We also find that for existing exporters, expanding exports requires sustained export-specific investments. We then examine the transition following Colombia’s 89-91 trade reform. We show that the relationship between the firm-level export intensity and aggregate export intensity disciplines the technology and policy changes accounting for this integration. We find that a common decline in tariffs can account for about 75 percent of the growth in exports as a share of manufacturing sales. We attribute the remaining 25 percent to an increase in the success of investments in export market access. About 10 percent of the increase in trade is accounted for by the endogenous accumulation of an improved exporting technology by existing exporters. These changes in policy and exporting technology boost welfare by about 7.1 percent. The transition following the reforms is characterized by an overshooting of output and consumption, with consumption peaking 15 years after the policy. Further tariff reductions are expected to increase welfare another 6.2 percent.","We study Colombia’s trade integration from 1981 to 2013. In this period, the Colombian manufacturing sector went from being relatively closed, exporting only 5.5 percent of its shipments in 1981 and 1982, to being substantially more open, exporting almost 15 percent of its shipments in 2006 and 2007. This near tripling of manufacturing exports involved a substantial change in the involvement of producers in trade and followed a range of policy reforms from changes in licensing, tariffs, taxes, subsidies, and more. Our primary goal is to identify the key changes in technology and policy that generated this expansion in trade and to then quantify the aggregate effects of these changes.====To organize our analysis, we examine Colombia’s integration through the lens of a dynamic general equilibrium model with heterogeneous firms that make forward-looking investments in export market access. In particular, we use a variation of the new exporter model of Alessandria et al. (2014). This model has a flexible specification of the exporting technology that can capture the salient features of the firm lifecycle and growth in the export market recently emphasized in the literature (see Ruhl and Willis (2017)). It captures the tendency for new exporters to be relatively small, export a small share of output, and be relatively likely to exit. It also captures the tendency of surviving new exporters to expand their export intensity gradually. These features are missing from previous analyses of export participation in Colombia (Roberts and Tybout, 1997), and Das et al. (2007) and suggests a very different structure for the investments in export market access than the traditional approach based on a sunk cost of exporting.====The first question we ask is: What was the technology for exporting in Colombia like at the start of the trade liberalization? This hinges on the assumptions of our model. We follow the literature and have firms make investments in export market access. Specifically, firms incur a sunk cost to start exporting and a different fixed cost to continue exporting. Additionally, these investments bear some risk in the sense that the investment in market access has an uncertain return. For our purposes, we assume the variable cost of shipping a good is uncertain and firm-specific. Over time, with sustained export market participation, we also find that the shipping technology improves. This introduces an endogenous component to the export intensity of an exporter and implies a long period of investment in building export sales.====We allow for firm-specific heterogeneity in the variable cost of shipping that depends on time in the market for a number of reasons. First, it is a natural generalization of the fixed-variable trade cost trade-off inherent in models of exporting to include time and risk. Second, it allows for heterogeneity in export intensity even in narrowly defined industries. Third, it is a parsimonious way to model the time path of profits from exporting while maintaining a representative agent on the consumer side that permits for easy aggregation.==== Obviously, models of consumer demand accumulation or habit could be developed to generate the same path of export sales upon entry. Matching the path of export sales and profits from a demand or supply mechanism would lead to similar entry and exit decisions.====We estimate the nature of the exporting technology for Colombian firms. We find that the up-front sunk cost to start exporting is quite low, only about 60 percent of the cost of staying in the market. In contrast, in a study of a subset of Colombian firms in three sectors over the same period, Das et al. (2007) find the up-front entry cost is nearly 10 times the cost of continuing in the export market. We also find that this investment is quite risky as it only yields exports about 15 percent of the time and that these new exporters use a technology that is about 9 times worse than the most efficient exporters. We also find that inefficient exporters have about an 8 percent chance of improving their exporting technology per year. In this way, we see that the typical new exporter expects it to take 5 to 10 years before it becomes as efficient an exporter as the typical continuing exporter. That is, exporters must make repeated investments in foreign market access to grow to scale in the export market.====The next question we ask is whether our dynamic model of endogenous export participation calibrated to Colombia’s pre-reform economy can capture the changes in the level of exports and distribution of exports from a single policy change, which we model as a change in the tariff. Perhaps surprisingly, given the various policy tools discussed and enacted in Colombia, we find that moving the economy from a 20 percent tariff to 7.5 percent tariff captures 75 percent of the overall change in exports and is consistent with the change in firm-level export intensity observed. That is, our model predicts the average export intensity among exporters, what we call the ====, will rise by about 46.9 percent while in the data the increase was about 45.2 percent. Our model also captures the aggregate change quite well as exports over total sales, what we call the ====, are predicted to grow by 74.3 percent while in practice the increase was closer to 95.5 percent. We use the model to decompose the change in the micro export intensity into a part from the policy change and a part due to exporters accumulating a better exporting technology. We find the investments in a better exporting technology magnify the micro export intensity by about 20 percent.====We then ask what other changes in the exporting technology could account for the gap between our model and the data on aggregate export flows. Here, we consider a variety of changes to the fixed trade costs and trade cost uncertainty. To generate the same macro export intensity while remaining consistent with the micro export intensity and the level of churning in the export market requires an improvement in the returns to investments in export market access by non-exporters. Specifically, we find that in the latter period 25 percent of investments in export market access yield exports compared to 15 percent in the earlier period. Achieving the same growth in the macro export intensity through changes in the fixed costs of exporting generates large changes in export churning, while changes in the cost of shipping or the rate at which new export intensity grow are inconsistent with the observed changes in the micro export intensity.====We then ask whether our finding that the change in the tariff is the key driver of export growth generalizes to the simpler models that abstract from new exporter dynamics. Here we find that models that generate exporter dynamics from a sunk cost only or exporter heterogeneity through a static decision from a fixed cost increases the macro export intensity by much less, between 64 to 66 percent. To get these models to generate a similar increase in trade then requires additional changes in the technology of exporting to capture the aggregate growth.====Our paper contributes to the recent literature that seeks to identify the effect of changes in trade barriers on trade and the distribution of activity within countries. Roberts and Tybout (1997) and Das et al. (2007) estimate a structural version of the Baldwin and Krugman (1989) and Dixit, 1989, Dixit, 1989 sunk cost of exporting model. Alessandria and Choi (2014) embed the canonical sunk cost model into a two country general equilibrium (GE) model.==== Alessandria and Choi (2014) use the GE sunk cost model to decompose the growth of US exports. They show the sunk cost model can explain the dynamics of the aggregate trade flows and the heterogeneous responses of trade from a decline in tariffs and iceberg costs. In particular, the model is consistent with the stronger long-run response of trade than the short-run response. They emphasize that GE models that abstract from these producer-level dynamics would infer substantial changes in the fixed costs of exporting. We find a similar result here, as these simpler models understate trade growth substantially. Our paper is also related to empirical work that seeks to sort out the dynamic impact of trade liberalization. A series of papers (Baier, Bergstrand, 2007, Buono, Lalanne), and (Baier et al., 2014) find trade agreements have gradual effects on trade flows. Baier et al. (2014) find that the trade growth from year 10 to 15 are almost as large as the growth in the first five years of a trade agreement and that much of the subsequent growth is related to changes in export participation.====A more recent literature pioneered by Eaton, Eslava, Krizan, Kugler, Tybout, 2009, Eaton, Eslava, Kugler, Tybout, 2008; Ruhl and Willis (2017) argues that the benchmark dynamic model of exporting with a sunk cost is inconsistent with observed dynamics of new exporters. Kohn et al. (2016) develop a partial equilibrium trade model consistent with these new exporter dynamics based on financial frictions. Alessandria et al. (2014) develop a general equilibrium model consistent with new exporter dynamics with a more flexible exporting technology and use it to evaluate the aggregate implications of a policy change. We apply this model to the often studied trade liberalization in Colombia (Eslava, Haltiwanger, Kugler, Kugler, 2004, Eslava, Haltiwanger, Kugler, Kugler, 2013, Fernandes, 2007). Unlike previous work we focus on a much longer period in a dynamic general equilibrium model.====In Section 2, we review some evidence on heterogeneity in exporting. In Section 3, we lay out a model consistent with these features, and in Section 4, we describe our strategy for calibrating the model and our main results. Section 5 considers the sensitivity of our results to assumptions of trade frictions and the dynamics of trade policy. Section 6 concludes.",Trade Integration in Colombia: A Dynamic General Equilibrium Study with New Exporter Dynamics,https://www.sciencedirect.com/science/article/pii/S0165188920300403,24 March 2020,2020,Research Article,319.0
"Abo-Zaid Salem,Kamara Ahmed H.","Department of Economics, University of Maryland, Baltimore County, 1000 Hilltop Circle, Baltimore, MD 21250, United States,College of Business, Texas A&M University-Corpus Christi, 6300 Ocean Dr., Corpus Christi, TX 78412, United States","Received 14 February 2019, Revised 10 March 2020, Accepted 17 March 2020, Available online 23 March 2020, Version of Record 23 April 2020.",https://doi.org/10.1016/j.jedc.2020.103901,Cited by (2),This paper studies the government spending multiplier in a quantitative model with ,"This paper studies the effects of credit frictions on the government spending multiplier during periods of liquidity trap, as may occur when the nominal interest rate reaches the zero lower bound (ZLB). The study extends a standard New Keynesian (NK) model with two types of households (constrained borrowers and unconstrained savers), in which credit frictions arise because borrowing is constrained by collateral.====We find that the liquidity-trap government spending multiplier with credit constraints is smaller than in a model that abstracts from such constraints. To see the intuition behind this finding, consider first a case in which the rise in government spending does ==== affect the tightness of the credit constraints (that is, the constraints were initially binding and remain binding to the same degree). Due to the credit constraints, the impatient households are unable to attain their desired level of consumption. Therefore, following a rise in government spending, their labor income rises, but the corresponding rise in consumption is smaller than it would have been if the impatient households had been able to borrow and smooth consumption freely.====The limited rise in consumption of the impatient households in the model with credit constraints weakens the response of total demand to a government spending shock. As a result, the increase in expected inflation is smaller than in a model with no credit constraints. When the nominal interest rate is fixed, the drop in the expected real interest rate is smaller. Consequently, consumption of ==== types of agents does not rise as much as it would have in the absence of these constraints, implying a smaller spending multiplier. This “neutral” case illustrates that the negative effect of credit constraints on the spending multiplier does not require credit constraints to become tighter following a rise in government spending. We then show that the rise in government spending could make the credit constraints tighter, which further weakens the response of total demand and makes the spending multiplier smaller than in the “neutral” case. Furthermore, the credit constraints may become looser but the spending multiplier with credit constraints would still be smaller than in a model without these constraints.====We also find that, with no credit constraints, the spending multiplier during liquidity traps is larger than the multiplier during normal times. However, with credit constraints, the liquidity-trap multiplier may become smaller than the normal-times multiplier. This finding challenges the perception that government spending is more effective in periods of slack.====Our finding regarding the behavior of inflation is significant: since the seminal work of Christiano et al. (2011), the quantitative literature on the spending multiplier has evolved around the behavior of expected inflation. This channel is often called the “expected inflation channel” of government spending whereby a rise in government spending generates a rise in expected inflation, which in turn reduces the expected real interest rate and boosts consumption. Christiano et al. (2011) show that the spending multiplier is large if the nominal interest rate does not respond to a rise in government spending. We show that this channel is markedly weakened or even fully eliminated by credit frictions.====This paper contributes to the recent quantitative literature on the spending multiplier. In a related study to ours, Carrillo and Poilly, 2013 find that financial frictions increase the spending multiplier, particularly at the ZLB. When the ZLB constraint binds, a rise in government spending reduces the real interest rate and allows for cheaper credit. Entrepreneurs use this time to accumulate more capital, which in turn increases their collateral and reduces future costs. By so doing, a rise in government spending encourages investment and leads to a larger spending multiplier. In our study, credit constraints lead to a weaker demand, a smaller rise (or even a decline) in the inflation rate, and a smaller fall (or a rise) in the real interest rate. Consequently, government spending is less effective than it would have been in the absence of credit constraints.====Using a New-Keynesian framework, Carlstrom et al., (2014) compare the spending multiplier when the monetary-fiscal expansion lasts a certain number of periods (“deterministic duration”) with the multiplier when the expansion is stochastic. They find the size of the stochastic multiplier to be larger than the deterministic multiplier. Zubairy (2014) studies the government spending multiplier in a stochastic general equilibrium model that features distortionary taxes and finds that the impact government spending multiplier is slightly above one. Bouakez et al. (2017) find a multiplier of 2.3 in a model with public investment. Woodford (2011) shows that a spending multiplier of well above one is possible under extreme circumstance as such in the Great Depression era. However, under less extreme circumstances, the spending multiplier may not be much greater than one, and it could be less than one or even negative if the persistence of the fiscal stimulus after the financial disturbance ends is sufficiently large. Leeper et al. (2017) show that the averages of short-run output multipliers are similar across regimes, but they are considerably larger after 10 years under the passive money/active fiscal regime than the active money/passive fiscal regime.====Dupor and Li, 2015 find that large spending multipliers require large responses of expected inflation to government spending, which does not align with their empirical evidence using U.S. data. Therefore, the expected inflation channel has no support in U.S. data or it has been too small to generate large fiscal multipliers. Our quantitative work suggests that credit constraints on households could be one explanation for the weak(er) “expected inflation channel”. Ramey and Zubairy, 2018 construct U.S. data for 1889-2013 to test whether government spending multipliers differ according to the amount of slack in the economy or being near the ZLB. They find that the amount of slack in the economy does not affect the size of the multiplier. Furthermore, their results for ZLB are mixed: the full sample indicates no larger spending multipliers near the zero lower bound, but a sub-sample that excludes the rationing periods of WWII indicates, in some cases, larger spending multipliers at the zero lower bound. Using a panel of OECD countries, Boehm (forthcoming) finds that, while the point estimates of the government spending multipliers differ between ZLB and non-ZLB episodes, the standard errors were large and, thus, equality between the multipliers at and away of the ZLB cannot be rejected. The model-based results that we obtain in this paper are largely consistent with these findings, which could suggest that the rise in the tightness of credit conditions in downturns reduces the effectiveness of fiscal policy.====Using Japanese data, Miyamoto et al., (2018), show that, at the ZLB, the government spending multiplier is considerably larger and the expected inflation rate rises by more than in non-ZLB episodes. Wieland (2019) finds that, in contrast to the predictions of the New Keynesian model, negative supply shocks are not expansionary at the ZLB (the analyses are based on the 2011 Great East Japan Earthquake and oil supply shocks). He then demonstrates that modifications of the model that are consistent with the empirical evidence would also overturn other unusual policy predictions at the ZLB, such as large fiscal multipliers.====More generally, our paper adds to the literature that questions the effectiveness of economic policies in the presence of credit frictions. Using a general equilibrium model, in which agents face uninsurable idiosyncratic income risk and borrowing constraints, McKay et al., (2016) show that, at the ZLB, forward guidance becomes considerably less powerful in stimulating the economy in the incomplete-market setup than in the standard macro model without borrowing constraints. Alpanda et al. (2019) find that the impact of monetary policy shocks on output and most other macroeconomic and financial variables is smaller during periods of economic ====, high household debt and high interest rates. They then show that a small-scale theoretical model that highlights the presence of collateral and debt-service constraints on household borrowing and refinancing could potentially rationalize these facts.====The remainder of the paper proceeds as follows. Section 2 presents simple aggregate demand-aggregate supply analysis. Section 3 outlines the model economy. Section 4 presents analytical analysis and Section 5 presents numerical results. Robustness analyses are presented in Section 6. Section 7 concludes.",Credit Constraints and the Government Spending Multiplier,https://www.sciencedirect.com/science/article/pii/S0165188920300695,23 March 2020,2020,Research Article,320.0
Heiberger Christopher,"Department of Economics, University of Augsburg, Universitätsstraße 16, Augsburg 86159, Germany","Received 18 December 2019, Revised 3 March 2020, Accepted 5 March 2020, Available online 19 March 2020, Version of Record 11 April 2020.",https://doi.org/10.1016/j.jedc.2020.103899,Cited by (2),"The present paper investigates: (i) the endogenous disaster mechanism in a textbook search-and-matching model when calibrated with a constant flow value of unemployment that is close to average labor productivity and (ii) its contribution to solve the ====.====I offer a simple explanation for the core of the disaster mechanism: the lower bound—say ====—on the annuitized costs the firm incurs from investing in a match lies sufficiently close to the body of productivity’s distribution. If labor productivity drops close to, or even below ==== of intertemporal substitution are both high, premia again collapse by an order of magnitude in the versions of the model without disasters.","Since the seminal paper by Mehra and Prescott (1985), the equity premium puzzle has spurred a great deal of research. The recent article by Jordà et al. (2019) echoes the finding of Mehra and Prescott (1985): there are sizable risk premia on stocks across countries and time periods that are hard to replicate within the framework of a risk-averse representative agent facing a stochastic stream of consumption.====A line of research initiated by Rietz (1988), taken up by Barro (2006), and extended to production economies by Gourio, 2012, Gourio, 2013 argues that the occurrence of rare but severe economic downturns as observed during the twentieth century is key to understanding the puzzle. Rietz (1988) and Barro, 2006, Barro, 2009 modify the consumption process in their models to account for those events, whereas Gourio, 2012, Gourio, 2013 considers rare but large negative shocks to productivity, which additionally destroy part of the capital stock. While successful in resolving the equity premium puzzle, these models are silent about the economic mechanisms causing infrequent economic crises and disasters are introduced exogenously to the models.====In a recent paper, Petrosky-Nadeau et al. (2018) (PZK) argue that the textbook search-and-matching model of Diamond, Mortensen, and Pissarides (DMP)==== can generate endogenous disasters (equal to or more than a 10% decline in output or consumption) in response to a standard productivity shock. Their model features a constant flow value of unemployment close to average labor productivity and has to be solved with global methods to detect the rare but large unemployment rates. Together with recursive preferences of the class introduced by Epstein and Zin (1989) and Weil (1990), the model also generates an equity premium of the empirically observed size.====The present paper answers four questions that emerge from the model of PZK. First, what mechanism underlies models driven by a single, standard productivity shock to generate rare economic disasters? Second, what features of the DMP model are necessary and sufficient to establish this mechanism? Third, do models with endogenous disasters help to solve the equity premium puzzle? Fourth, should we mistrust the results of numerous studies that embed the DPM framework and were solved with perturbation methods instead of global methods?====The core of the disaster mechanism in the textbook DPM model is amazingly simple and not specific to labor market models. In equilibrium, the firm’s decision to post vacancies must balance out the expected payoff with the expected costs from a match. The payoff is the stream of expected future productivity of a worker that is matched to a vacancy. The costs result from initial costs for posting the vacancy and hiring the worker and the stream of subsequent wage payments to the worker. Productivity is governed by an exogenous process, while the annuitized costs incurred from a match are bounded from below, say by ====. If ==== lies sufficiently close to the body of productivity’s distribution, it happens once in a while that the firm’s annuitized share in expected future productivity of a worker falls close to, or even short of, the threshold for consecutive periods. The firm’s payoff from a match can then break even the costs incurred from a match, either only if vacancies are filled with probability very close to one, or even no longer at all. In equilibrium, vacancies must drop (close) to zero, and the ongoing process of exogenous separations quickly increases unemployment.====Following Barro and Ursúa (2008) and defining disasters as series of periods with cumulative declines in output or consumption of at least 10%, the model generates disaster probabilities of 5% and 2.8%, respectively. During disasters, output collapses by 23% on average, and consumption shrinks by 27% on average. At first sight, it may seem that periods where vacancies fall (very close) to zero play only a minor role for the model’s disasters: only in approximately 7% of output disasters and 12% of consumption disasters do vacancies drop entirely to zero. In fact, even if all endogenous decisions of the model are shut down, so that output and consumption are driven solely by the exogenous process for labor productivity while vacancies are ==== to the stationary value, disaster probabilities of 2.7% (output) and 6.1% (consumption) can be observed. However, the model provides an internal mechanism that massively boosts disasters beyond the extent already introduced through the exogenous process; and periods with zero vacancies play an important role in delivering this endogenous mechanism. More specifically, if one only considers cumulative declines of at least 25%—approximately the mean disaster size in the model, but in excess of what can be observed for the exogenous process—vacancies drop to zero in approximately 50% of such large disasters.====The answer to the second question is that the constant value of the household’s outside option ==== in the wage bargain drives the result. The reasoning behind this proposition and the empirical content of a large and constant flow value of unemployment account for a substantial fraction of the paper. My arguments are as follows.====The costs the firm incurs from a match in the model of PZK result from initial costs for posting the vacancy and hiring the worker, and from the stream of subsequent wage payments. Moreover, the bargained wage is the sum of three elements: (i) a fraction of labor productivity the worker is always able to seize in the bargain, (ii) an additional fraction of the surplus of the match that the worker can seize depending on labor market tightness, and (iii) a constant component that the worker can impose on the firm because of a constant value of his outside option. The last component puts a lower bound on the wage. In consequence, the sum of the annuitized costs for posting the vacancy and hiring the worker, plus the constant component in the wage define the lower bound ==== for the annuitized costs the firm faces from a match. The sizes of the first two components are constrained by empirical evidence and, as they appear only once, they have less effect on the threshold ==== than the third component. Moreover, the average costs from filling a vacancy increase with labor market tightness. Any calibration of the respective parameters that raises these costs, therefore, also implies unreasonably mean unemployment rates. Hence, the disaster mechanism crucially depends on a large lower bound for the wage in order to sufficiently elevate ====. PZK achieve this by setting the value of the worker’s outside option ==== equal to 85% of mean labor productivity, and therefore shift ==== sufficiently into the body of the distribution of labor productivity.====Note that there exist alternative interpretations of the constant part of the wage. For instance, Hall (2005a) introduces a constant wage norm, and in the alternating offer bargain of Hall and Milgrom (2008), the worker can negotiate a wage which additionally includes costs the firm faces from delays in the bargain. As long as no other type of recurring costs from an employment is introduced to the model, what matters for disasters is that indifferent from the introduction to the model, the constant component of the wage is large relative to the firm’s share in productivity. Lowering these costs will quickly shift the level of productivity where the economy crashes far to the left tail of labor productivity’s distribution. The relationship is highly non-linear and even a moderate drop eliminates endogenous disasters entirely. The PZK model with a flow value of unemployment of 75% of mean labor productivity can no longer generate disasters. In fact, consumption disasters are then even dampened by the decisions that are endogenous to the model.====This result is well in line with those of Ljungqvist and Sargent (2017), who review matching models proposed in the literature to improve the response of unemployment to productivity shocks. They identify a common channel that operates on a diminishing fundamental surplus, being defined as productivity less an amount that cannot be allocated to vacancy creation. It is this non-allocatable amount that constitutes the large lower bound on the annuitized average costs and, therefore, shifts the level where vacancies become unprofitable. In the case of Nash-bargaining, the non-allocatable amount corresponds to the fixed flow value of unemployment ==== which enters the wage. The literature offers different calibrations of ====. Shimer (2005) identifies the flow value of unemployment with unemployment compensation and sets ==== to 40% of mean productivity to match the drop of income upon job loss under US replacement rates. Hagedorn and Manovskii (2008) estimate vacancy posting costs and the elasticity of real wages with respect to labor productivity from the data. To match both figures in their model requires ==== close to mean labor productivity of one and a bargaining weight of workers close to zero. Hagedorn and Manovskii (2008) defend the high value of ==== by arguing that the flow value of unemployment should not only reflect unemployment benefits but also other factors as, e.g., the foregone value from leisure over work effort or the value received through home production.====Chodorow-Reich and Karabarbounis (2016) show empirically that the flow value of unemployment is procyclical and volatile over the business cycle. This is also an endogenous feature of real business cycle models that embed the DMP framework, as, e.g., Merz (1995) and Andolfatto (1996), and arises from leisure in the utility function. Even when the flow value of unemployment is large on average so that the firm’s annuitized costs from a worker are large on average, the fact that the flow value of unemployment will drop together with the employment rate implies that there no longer exists a large lower bound for these costs. In consequence, the level ====, where the economy crashes, shifts far into the left tail of productivity’s distribution. I demonstrate this result within several well-known extensions of the DMP model. In particular, I consider leisure in the utility function, endogenous working hours, endogenous search of unemployed households, and home production. I calibrate all these extensions so that the average flow value of unemployment is equal to 85% of mean labor productivity. None of these models is able to generate endogenous disasters.====I move towards answering my third question. Despite the huge disasters in the benchmark DMP model calibrated by PZK, the equity premium remains too small with standard preferences. Different from the models of Rietz (1988) and Barro (2006), where disasters are the result of an exogenous shock causing a sudden and drastic drop in consumption, severe declines in consumption occur only gradually over an extended period of time with increasing unemployment. However, the stochastic discount factor under standard preferences only captures the lottery over next period’s consumption and does not account for the possibly large uncertainty regarding employment and consumption in the longer run. An important ingredient in models of asset prices with long-run risk are recursive preferences of the class introduced by Epstein and Zin (1989) (EZ) and Weil (1990). For example, Bansal and Yaron (2004) combine EZ preferences with long-run risk in consumption growth to generate high risk premia, and Rudebusch and Swanson (2012) show that EZ preferences help to explain asset prices under long-run inflation risk. Analogously, EZ preferences help to explain risk premia in the benchmark search-and-matching model. The benchmark model with EZ preferences with a coefficient of relative risk aversion of 10 and an elasticity of intertemporal substitution close to one, can replicate an equity premium of approximately 6.29% combined with a low risk-free rate of 1.95%. However, as lowering the flow value of ==== moderately already has substantial effects on the model’s disaster statistics, it also has substantial effects on the equity premium. More specifically, the equity premium in the model with EZ preferences and ==== drops by more than an order of magnitude to 0.40%, and can only be raised to 1.62% even if the coefficient of relative risk aversion is increased to a huge value of 50. As none of the models with endogenous flow value of unemployment is able to generate disasters, their equity premia, too, remain one order of magnitude below the premia in the data.====Finally, and with regard to my fourth question, perturbation methods deliver reliable solutions of models with labor market search-and-matching if either the flow value of unemployment is endogenous or not fixed at an extreme value relative to mean labor productivity. It is true that a second-order perturbation method fails to accurately display the dynamics in a standard real business cycle (RBC) model with labor market search, if the model is calibrated with a large and constant flow value of unemployment. However, my numeric studies show that a second-order perturbation, yet, yields a very good approximation, if the flow value of unemployment activities is constant but not larger than 75% of mean labor productivity. More importantly, it also provides a remarkably good approximation for models with procyclical flow value of unemployment, which is the dominant feature of RBC models with labor market frictions.====The remaining part of the paper is structured as follows. Section 2 presents the benchmark RBC model with labor market search. Section 3 analyzes the model’s disaster statistics and the mechanism that drives the results. Section 4 presents the model’s extensions and their implications for the endogenous disaster mechanism. Section 5 explores the equity premium in both the core model and its various extensions. Section 6 considers the accuracy of a perturbation solution and Section 7 concludes. More detailed derivations, including details of the numerical solution method, are found in the Online Appendix.","Labor market search, endogenous disasters and the equity premium puzzle",https://www.sciencedirect.com/science/article/pii/S0165188920300671,19 March 2020,2020,Research Article,321.0
Chodorow-Reich Gabriel,"Harvard University, USA","Available online 12 March 2020, Version of Record 13 June 2020.",https://doi.org/10.1016/j.jedc.2020.103875,Cited by (28),Cross-sectional or panel studies have joined time series techniques as an important element in empirical macroeconomists’ toolkit. The ,"Analysis of regional shocks and outcomes has joined time series techniques as an important element in empirical macroeconomists’ toolkit. Appendix A lists 50 studies published in general interest or macroeconomic field journals between 2012 and 2018 that use regional data to answer macroeconomic questions. The topics include estimation of fiscal multipliers, the impact of foreign trade on U.S. labor markets, the role of household deleveraging in the Great Recession, and the impact of bank credit on the real economy, to name a few. Yet, the econometric best practices for regional analysis and the aggregate implications of these studies remain active topics of research. In this article, I offer several pieces of advice for practitioners in this literature.====Each section of the article addresses a different aspect of regional data. Section 2 begins by casting regional analysis in a Rubin (1978) potential outcomes framework. Unlike in the canonical setting, in regional analysis the potential outcome in each region can depend on the treatments or shocks in all other regions and national variables can react to treatment applied to all regions. These differences determine relationships among several objects of interest to macroeconomists: the aggregate impact of an aggregate shock (====), the economy-wide impact of a local shock (====), the impact of a local shock on the treated region only (====), and the difference-in-difference estimator commonly used in practice (====).====The possible difference between ==== and ==== arises because the cross-sectional coefficient measures outcomes in the treated area relative to the “contaminated” untreated area. I refer to such contamination as a violation of the ==== no-interference Stable Unit Treatment Value Assumption (SUTVA-micro). The wedge disappears as the treated area becomes infinitesimally-sized relative to the untreated area. Using the multi-region New Keynesian model in Nakamura and Steinsson (2014), I show that in practical settings with geographic units of the size of U.S. states or smaller and demand shocks that do not induce factor mobility, SUTVA-micro violations should have minimal impact on regional estimates and usually may be safely ignored.====Section 3 addresses the difference between the local impact of a local shock ==== and the aggregate impact of an aggregate shock ====. Following arguments in Chodorow-Reich (2019), I argue for comparing regional estimates to a judiciously chosen aggregate benchmark in which monetary policy and other aggregate variables do not respond to the shock. While restrictive, this benchmark can also facilitate comparison to macroeconomic theory by holding fixed factors auxiliary to the shock under study. Next, even when a shock to a single region causes a sufficiently ==== spillover onto each other region as not to contaminate the estimation of the impact of the shock on the treated region, the spillovers may still aggregate to a magnitude that makes the economy-wide impact of the shock, ====, different from the impact on the treated region only, ====. I state conditions under which these spillovers make regional estimates a bound for aggregate effects and argue that, where applicable, such a bounding exercise provides a transparent and almost model-free approach to relating ==== to ====.====Sections 4 and 5 turn to common econometric issues that arise in the estimation of cross-regional regressions. Often in these settings, the exogenous variation affects a known subset of the total variation in the endogenous variable of interest. As one example, studies of regional fiscal multipliers often use excluded instruments that directly affect only part of government spending in an area. Section 4 presents Monte Carlo evidence that in these circumstances, standard instrumental variables estimation can perform poorly. Intuitively, if the excluded instruments explain only a small part of the endogenous variable, then conventional instrumental variables will suffer from a weak instruments problem. In contrast, imposing the researcher’s knowledge that the exogenous variation affects only a subset of the total can reduce bias and increase efficiency. An application to the 2009 American Recovery and Reinvestment Act (ARRA, or the “Obama stimulus”) and a contrasting set of results reported in Chodorow-Reich (2019) and Ramey (2019) illustrates the importance of this distinction.====Section 5 addresses another common econometric question: should researchers weight regional estimates by population? In the presence of heterogeneous treatment effects, weighting by population can yield a regression coefficient more representative of the population average treatment effect than unweighted regression. However, it can also produce a coefficient less representative than unweighted regression. Moreover, weighting can decrease efficiency and, with instrumental variables, increase bias, especially with skewed weights in small samples. I again use Monte Carlo evidence to assess the practical importance of these considerations and to relate to the literature studying the 2009 ARRA.====I recommend reading Section 2 before Section 3, and Section 4 before Section 5. However, readers interested solely in the econometric issues may skip directly to Section 4 with little loss of continuity.",Regional data in macroeconomics: Some advice for practitioners,https://www.sciencedirect.com/science/article/pii/S0165188920300440,12 March 2020,2020,Research Article,322.0
"Carta Francesca,De Philippis Marta","Bank of Italy, Dondena Gender Initiative (Bocconi University), Italy,Bank of Italy, Centre of Economic Performance (LSE), United Kingdom","Available online 10 March 2020, Version of Record 13 June 2020.",https://doi.org/10.1016/j.jedc.2020.103886,Cited by (0),None,None,Comments on “labor market trends and the changing value of time”,https://www.sciencedirect.com/science/article/pii/S0165188920300555,10 March 2020,2020,Research Article,323.0
"Aguiar-Conraria Luís,Martins Manuel M.F.,Soares Maria Joana","NIPE and Department of Economics, University of Minho, Portugal,Cef.up and Faculty of Economics, University of Porto, Portugal,NIPE and Department of Mathematics, University of Minho, Portugal","Received 26 June 2019, Revised 2 March 2020, Accepted 3 March 2020, Available online 9 March 2020, Version of Record 7 May 2020.",https://doi.org/10.1016/j.jedc.2020.103897,Cited by (21),"We present the first assessment of U.S. Okun’s Law across time and frequencies. We use a set of continuous wavelet tools that allows for estimating Okun’s coefficient and the lead/lag of output over unemployment at each moment and for each cyclical frequency. We find similar results for the gaps and the first differences specifications at business cycles frequencies, but not at lower frequencies. Okun’s coefficient has increased (in absolute value) since the mid-1960s, except in 1985–1995, and is not particularly sensitive to recessions. The lead of output varies considerably over time and also at different frequencies. We observe (especially with the gaps specification) that there are at least two cyclical processes relevant for the Okun’s relationship, one at the business cycle and another at lower frequencies. Methods that do not take this into account are bound to mix the information embedded in both cycles.","Okun (1962) uncovered a negative short-run relation between cyclical unemployment and cyclical real output, which proved to be such a remarkable stylized fact that came to be known as Okun’s Law. His two key econometric specifications, adopted in the subsequent literature, describe a relation either between levels or between the first differences of output and unemployment====in which ==== is the unemployment rate in period ====, ==== is the natural rate of unemployment, ==== is the logarithm of real output, ==== is the logarithm of potential output, and Δ is the first difference operator.====In Okun’s original estimates, a 1% deviation of real output from potential was associated with about a 0.33% deviation of unemployment from trend (levels or gaps specification), as a 1% change in real output was associated with about a 0.33% change in the unemployment rate (first differences specification).==== Okun’s view was that each percentage point of deviation of output from trend would be divided into a change of a third of a percentage point in productivity and a change of two thirds of a percentage point in aggregate hours, subdivided into a change of one third of a percentage point in the unemployment rate and roughly equal changes in labor force participation and hours per employee – see Gordon, 2010a, Gordon.====Subsequent empirical research on Okun’s Law established the rule of thumb that, in the U.S., each 1% deviation of real output from potential is associated with about 0.5% deviation of unemployment from the natural rate.==== Okun’s Law has been omnipresent in macroeconomics textbooks, macro models with unemployment, and applied business cycles analyses. Moreover, it has been the focus or a key analytical tool in a vast academic literature, recently earning a renewed interest – see e.g. Ball et al. (2017), Fernald et al. (2017), Kamber et al. (2018), and Arai (2016).====However, the stability of the coefficient of Okun’s Law has been challenged by a large literature in two ways: (1) the possibility of nonlinearities (meaning that the coefficient may be different at different phases of the business cycle); (2) time-variations due to possible structural breaks or changing trends. Moreover, some (smaller) literature has challenged the stability of the timing of Okun’s Law, raising issues related to its correct lag specification over time.====While Okun’s coefficient may have been unstable across time, it is likely that it has also been unstable for different frequencies. Not only there is evidence that cycles longer than those associated with business cycle frequencies display relevant information for macroeconomic analysis (Pancrazi, 2015), but there is also evidence of a lengthening of business cycles (Aguiar-Conraria, Martins and Soares, 2018, and Crowley and Hughes Hallett, 2018). If such volatility transfer has been caused by a change in policy preferences, then it very likely has also affected other main macroeconomic variables, such as the unemployment rate, and their relation with output. To test for this possibility, one has to scrutinize Okun’s Law at different frequencies, which the literature has not done so far.====Our paper adds to this important literature by using a technique that allows us to simultaneously deal with all these issues, namely, instability over time due to nonlinearities, time-variations, and changing lags specifications, as well as instability over frequencies.====We use continuous wavelet tools, with an approach consisting of a sequential analysis of wavelet coherencies, phase-difference diagrams, and wavelet gains, that provides estimates of the U.S. Okun’s Law coefficient simultaneously allowing for variations over time as well as variation in the timing of the relationship and variation across frequencies. More specifically, coherency indicates the strength and significance of the Okun’s relationship at each frequency for each moment of time, the gain provides an estimate of Okun’s coefficient at each frequency and moment of time, and the phase-difference indicates the sign of the estimate and the lead/lag of output relative to unemployment.====With this set of tools, this paper uncovers new stylized facts about the U.S. Okun’s Law in the past seven decades that would be difficult to detect either in the pure time-domain or in the pure frequency-domain. For completeness, we perform our analyses both for the levels (gaps) version and for the first differences version of the Okun’s Law.====The paper is structured as follows. In Section 2, we discuss the literature on Okun’s Law, clarifying our net contributions to the literature. In Section 3, we briefly describe our methodology. In Section 4, we present the data and perform a preliminary time-frequency analysis of each time-series. In Section 5, we present our results of estimation of Okun’s Law in the time-frequency domain. Both these sections focus on our preferred specification, relating the output and the unemployment gap. Section 6 concludes, presenting in a systematic way the stylized facts uncovered in the paper and comparing these with the current literature on U.S. Okun’s Law. In the Appendix, for robustness check and comparison with some literature, we present data and results for the growth rates specification.",Okun’s Law across time and frequencies,https://www.sciencedirect.com/science/article/pii/S0165188920300658,9 March 2020,2020,Research Article,324.0
"Lin Qian,Sun Xianming,Zhou Chao","School of Economics and Management, Wuhan University, China,School of Finance, Zhongnan University of Economics and Law, China,Collaborative Innovation Center of Industrial Updating and Regional Finance (Hubei), Zhongnan University of Economics and Law, China,Department of Mathematics, Institute of Operations Research & Analytics and Suzhou Research Institute, National University of Singapore, Singapore","Received 29 May 2019, Revised 26 February 2020, Accepted 2 March 2020, Available online 6 March 2020, Version of Record 8 April 2020.",https://doi.org/10.1016/j.jedc.2020.103896,Cited by (8),"In the presence of ambiguity about the driving force of market randomness, we consider a dynamic portfolio choice problem without any predetermined investment horizon. The investment criterion is formulated as a robust forward performance process, reflecting an investor’s dynamic preference adapted to the market evolution. We show that the market risk premium and the utility risk premium jointly determine the investors’ trading direction and the worst-case scenarios of the risky asset’s mean return and volatility. The closed-form solution for the optimal investment strategies is given in the special settings of the constant relative ==== (CRRA) preference. The resulting portfolio strategies highlight the effect of ambiguity on nonparticipation in the risky asset market from the forward performance point of view.","Dynamic portfolio choice problems usually envisage an investment setting in which an investor is exogenously assigned an investment performance criterion and stochastic models for the price processes of risky assets. However, by their nature, complex financial markets can confront investors with ambiguity about the future states of risky assets’ prices, beyond the risk (payoff uncertainty) characterized by stochastic models for risky assets’ prices.==== Ambiguity-averse investors may extemporaneously adjust their investment horizons and consistently adapt their preferences to the market evolution. Motivated by these investment realities, we study a robust portfolio choice problem, which provides time-consistent investment strategies independent of the investment horizon in a continuous-time framework. The resulting investment strategy is referred to as being horizon-unbiased.====The continuous-time portfolio choice problem pioneered by Merton (1969) is formulated as a stochastic control problem to maximize the expected utility at a fixed investment horizon by searching for the optimal strategy in an admissible strategy space. Although this problem formulation has been extensively used in the existing portfolio theory, it seems quite artificial to fix a prior investment horizon in many realistic investment circumstances. For instance, an investor may have no specific investment horizon in mind, but aims to have her portfolio’s value grow gradually over rolling horizons. In such a common case, an investor may revise the previous decisions and preferences in accordance with the accumulated market information. However, these considerations can hardly be handled within Merton’s framework, because the investor’s future preferences are pre-specified for each time point before the prior investment horizon. Note that if the investor has two candidate investment horizons ====, ==== with 0 < ==== < ====, the resulting optimal strategies associated with these two horizons are generally not consistent over the common time interval [0, ====] (==== ≤ ==== < ====) in Merton’s framework (see, e.g., Kim, Omberg, 1996, Musiela, Zariphopoulou, 2007, Wachter, 2002). It means that the investor could either continue to follow a suboptimal strategy or rebalance her portfolio at significant transaction costs, if she adjusts the investment horizon (from ==== to ====, or the other direction). In both cases, the investor will regret her previous investment decisions. To preclude such pathological regrets, the investor needs a time-consistent optimal strategy which is independent of the investment horizon and reflects her dynamic preference in terms of time and wealth. The horizon-unbiased utility or forward performance measure, independently proposed by Choulli et al. (2007), Henderson and Hobson (2007), Musiela and Zariphopoulou (2007), provides a portfolio framework satisfying the aforementioned considerations. In this framework, an investor specifies her initial preferences (utility function), and then propagates them ==== as the financial market evolves. From the methodological point of view, this striking characteristic contrasts the portfolio choice based on the forward performance measure with that in Merton’s framework, in which intertemporal preference is derived from the terminal utility function in a ==== way. Musiela and Zariphopoulou (2010b) specify the generic forward performance measure as a stochastic flow ====, taking time (====) and wealth (====) as arguments. The randomness of the forward performance measure is driven by the Brownian motion which is the same as the driving force of the randomness of asset price. This construction method implies that the driving force of market randomness is simultaneously embedded in the investor’s preference and the risky asset price process. It implicitly assumes that the Brownian motion represents the fundamental source of risk behind the financial market and the risky assets. In particular, the volatility of a forward performance measure reflects the investor’s uncertainty about her future preferences because of the randomness of the financial market states. However, due to the epistemic limitation or limited information, an investor may have ambiguity about the driving force of market randomness, and then cannot accurately assign a parametric stochastic process to the risky asset price (see, e.g., Carr, Lee, 2009, Epstein, Ji, 2013). This ambiguity inevitably affects the investor’s preference. A natural question is how the ambiguity about the driving force of market randomness simultaneously affects the preferences and investment strategy of an ambiguity-averse investor, who may adjust her investment horizon. We investigate this issue by proposing a robust forward performance measure, which captures an investor’s ambiguity about the driving force of market randomness.====The risky asset price is usually modeled as the solution to a stochastic differential equation (SDE) driven by Brownian motions, which represent the driving force of market randomness. Given this parametric model, an investor actually assigns a unique distribution over the possible outcomes of an investment strategy. The premise to set up such a parametric model is that the investor has full information on the driving force of market randomness. However, the financial markets are too complex for an investor to assign such a unique distribution, essentially due to the cognitive limitation or noisy information on the driving force of market randomness. This situation is referred to as ambiguity or Knightian uncertainty, while the former situation is referred to as risk (Epstein and Zhang, 2001). Ambiguity has attracted researchers’ attention in the area of asset pricing and portfolio management (see, e.g., Bossaerts, Ghirardato, Guarnaschelli, Zame, 2010, Brenner, Izhakian, 2018, Chen, Ju, Miao, 2014, Escobar, Ferrando, Rubtsov, 2018, Garlappi, Uppal, Wang, 2007, Guidolin, Liu, 2016, Jiang, Tian, 2017, Liu, 2011, Luo, 2017, Luo, Nie, Young, 2014, Maenhout, 2004, Wang, 2009, Yang, Liang, Zhou, 2019, Zeng, Li, Chen, Yang, 2018). In this paper, ambiguity about the driving force of market randomness takes the form of an investor’s ambiguous belief on the dynamics of the risky asset price. This ambiguous belief is characterized by a set ==== of probability measures defined on the canonical space Ω, the set of continuous functions. We incorporate the investor’s ambiguity about the risky asset price into her preference by defining the forward performance measure on the canonical space Ω.====In a similar idea of Epstein and Ji (2013), the set ==== of probability measures is first constructed such that the mean and volatility of the risky asset returns lie in a convex compact set ====. This formulation is obviously different from models capturing parameter uncertainty over the mean return or volatility. It generalizes the models with ambiguity about the distributions over the mean return or volatility. Note that ambiguity about the driving force of market randomness is the fundamental source of ambiguity about the dynamics of risky asset price. Our framework provides a unified framework to investigate an ambiguity-averse investor’s portfolio choice and dynamic preferences adapted to the market evolution.====We define the robust forward performance measure, by taking account of the investor’s ambiguity in the driving force of market randomness. In turn, we propose a method to explicitly construct such a robust forward performance measure for a given initial preference, and derive the corresponding investment strategy and conservative beliefs on the mean return and volatility of the risky asset. We show that the sum of the market risk premium and the utility risk premium determines the trading direction. Roughly speaking, the market risk premium corresponds to the premium associated with market risk (mean return and volatility), while the utility risk premium arises from investors’ uncertainty on her future preferences adapted to the market evolution, revealing the interplay between the investment universe and the risk attitude (El Karoui and Mrad, 2013). We further specify the initial preference of the CRRA type, and investigate the determinants of the conservative beliefs on the mean return and volatility of risky assets in three specific settings, i.e., ambiguity in terms of the mean return, volatility, and the structured ambiguity, respectively. When we consider ambiguity in terms of the mean return, we keep the volatility as a constant, and vice versa. The first two settings of ambiguity have been investigated in Merton’s framework (see, e.g., Lin and Riedel, 2014). The third setting is motivated by the fact that there is no consensus on the relation between the mean return and volatility of risky assets in the empirical literature (see, e.g., Bandi, Renò, 2012, Omori, Chib, Shephard, 2007, Yu, 2012), and investigated by Epstein and Ji (2013). We show that the sign of the total risk premium determines the conservative belief on the mean return in the first setting, while the risk attitude and the relative value of the utility risk premium over the market risk premium jointly determine the conservative belief on the volatility in the second setting. In the third setting, we do not derive the closed-form formula for the conservative beliefs, but show that the corresponding beliefs can take some intermediate value within the candidate value interval, as well as the upper and lower bounds. To our knowledge, these interesting results are new to the portfolio choice literature.====This paper contributes to the existing literature in three folders. ====, we propose a generic formulation of robust forward performance accommodating an investor’s ambiguity about the dynamics of risky assets or the driving force of market randomness. ====, we figure out the determinants of trading direction for an investor in a market with one risk-free asset and one risky asset. From the economic point of view, it is the sum of the market risk premium and the utility risk premium that determines an investor’s trading direction. These premiums provide a particular explanation for nonparticipation in the risky asset market from the forward performance point of view. ====, we show that the market risk premium, the utility risk premium, and the risk tolerance affect an investor’s conservative belief about the mean return and volatility. More specifically, if the maximum of the total risk premium is negative, an investor will take the maximum of the mean return as the worst-case value; if the minimum of the total risk premium is positive, an investor will take the minimum of the mean return as the value in the worst-case scenario; otherwise, the worst-case mean return lies between its minimum and maximum. The market risk premium, the utility risk premium, and the risk tolerance jointly determine an investor’s conservative belief on the volatility of risky assets. We emphasize that the conservative belief is related to the optimization associated with risk premiums, and these conservative beliefs may take some intermediate values within their candidate value intervals, as well as their boundaries.====. This paper is related to the literature on the effect of ambiguity on portfolio choice. Nonparticipation in the risky asset market is a stylized phenomenon documented in the finance literature (see, e.g., Campbell, 2006, Cao, Wang, Zhang, 2005, Dow, Werlang, 1992). Ambiguity aversion with ==== utility can lead to nonparticipation in the risky asset market (Epstein, Schneider, 2010, Lin, Riedel, Mukerji, Tallon, 2003). It cannot be generated within the framework of the smooth ambiguity model (Klibanoff et al., 2005) or the multipliers utility approach (Anderson et al., 2003), as shown by Epstein and Schneider (2010). In contrast to the backward utility framework used in these papers, we use the forward performance framework by following the rationale of ==== utility. This new point of view reveals that the utility risk premium contributes to nonparticipation in the risky asset market, as well as the market risk premium.====Most of the existing results on forward performance measures have focused on its construction and portfolio choice problems in the setting of risk, rather than ambiguity (see, e.g., Alghalith, 2012; Anthropelos, 2014, El Karoui, Mrad, 2013, Musiela, Zariphopoulou, 2010a, Nadtochiy, Tehranchi, 2017; Zariphopoulou and Žitković, 2010). As one of the few exceptions, Källblad et al. (2018) investigate the robust forward performance measure in the setting of ambiguity characterized by a set of equivalent probability measures. However, this approach fails to solve the robust “forward” investment problem under ambiguity in terms of volatility, which is characterized by a set of mutually singular probability measures (Epstein and Ji, 2013). We fill this gap by characterizing an investor’s ambiguity with a set of probability measures, which may not be equivalent to each other. Chong and Liang (2018) investigate robust forward investment under parameter uncertainty in the framework in which a unique probability measure is assigned to the canonical space. Different from this model setup, we consider an investor’s ambiguity on the driving force of market randomness, and assign a set of probability measures on the canonical space. This approach is not only technically more general than the approach with a set of dynamic models under a unique probability measure (as detailed in Remark 4 by Epstein and Ji, 2013), but also allows an investor to explicitly incorporate ambiguity about the risk source into her preferences. This is the key difference between our framework and the framework of Chong and Liang (2018). On the other hand, Chong and Liang (2018) construct the forward performance measure based on the solution to an infinite horizon backward stochastic differential equation (BSDE). Our approach associates the forward performance measure with a stochastic partial differential equation (SPDE), which provides the analogue of the Hamilton-Jacobi-Bellman equation (HJB) in Merton’s framework. For the reason of tractability, we limit ourselves to forward performance measures of some special forms, and investigate the corresponding robust investment strategy. It is outside the scope of this paper to investigate the existence, uniqueness, and regularity of the solution to the associated SPDE in a general setting. This simplified model setup and the corresponding results shed light on how ambiguity-averse investors dynamically revise their preferences as the market evolves.====The remainder of this paper is organized as follows. Section 2 introduces the model setup for robust forward investment. The construction of the robust forward performance measure is investigated in Section 3. In Section 4, we study the conservative belief of an ambiguity-averse investor with preference of the CRRA type. Section 5 gives a conclusion.",Horizon-unbiased investment with ambiguity,https://www.sciencedirect.com/science/article/pii/S0165188920300646,6 March 2020,2020,Research Article,325.0
"Slacalek Jiri,Tristani Oreste,Violante Giovanni L.","European Central Bank,European Central Bank and CEPR,Princeton University, CEBI, CEPR, IFS, IZA, and NBER","Available online 6 March 2020, Version of Record 13 June 2020.",https://doi.org/10.1016/j.jedc.2020.103879,Cited by (29),", collateral and labor income channels. The strength of these forces varies across households depending on their marginal propensities to consume, their balance sheet composition, the sensitivity of their own earnings to fluctuations in aggregate labor income, and the responsiveness of aggregate earnings, asset prices and ","In the last few years a new literature has flourished in macroeconomics. It embeds New Keynesian elements (namely nominal rigidities) into the workhorse incomplete market framework. A central contribution of this Heterogeneous Agent New Keynesian (HANK) literature is to revisit the transmission mechanism of monetary policy (Auclert, 2019, Kaplan et al., 2018, Werning, 2015). In Representative Agent New Keynesian models (RANK), monetary policy affects consumption expenditures because households substitute intertemporally between consumption and saving in the wake of unexpected changes in the interest rate. In HANK, this channel is small and coexists with a plethora of others. First, intertemporal substitution is not the only ====, i.e. partial equilibrium, effect. Changes in the interest rate affect household financial income differently depending on whether they are borrowers or savers (i.e., their net interest rate exposure). ==== effects operate through the general equilibrium responses of inputs and asset prices, notably labor income and house prices. After a monetary easing, the direct increase in households’ expenditure and firms’ investment stimulates output, employment and wages. The additional increases in expenditure induced by higher employment and wages are the essence of the indirect effect. The strength of these indirect channels crucially depends on the size of the marginal propensity to consume (MPC), which is an order of magnitude higher in HANK models relative to RANK ones.====The recent literature on HANK models has followed two parallel paths in order to quantify these various effects. Some authors have built rich DSGE models by adding a lot of realism on the household side to otherwise standard New Keynesian environment (Alves, Kaplan, Moll, Violante, 2019, Auclert, Rognlie, Straub, 2018, Auclert, Rognlie, 2018, Bayer et al., 2019, Debortoli and Galí, 2017, Gornemann et al., 2016, Hagedorn, Manovskii, Mitman, 2017, Kaplan et al., 2018, Luetticke, 2018, McKay et al., 2016, McKay and Reis, 2016, Wolf, 2019). Others have resorted to making simplifying assumptions that lead to analytical solutions (Acharya and Dogra, 2018, Auclert, 2019, Bilbiie, 2008, Bilbiie, 2020, Patterson, 2019, Ragot, 2018, Werning, 2015). These two approaches are complements. The first one provides a better framework for the analysis of business cycles and policy counterfactuals. The second one overcomes the computational complexity of the first and offers transparent insights about the economic forces at work.====In this paper, we follow this second approach and apply it to understand the relative strength of these numerous transmission channels of monetary policy to household spending in the euro area. In developing the theory, we closely follow Auclert (2019) and formulate an analytical decomposition that showcases the relevant ingredients needed for this type of back of the envelope calculation. We emphasize that the strength of all transmission channels to aggregate consumption depends on three key dimensions of households’ heterogeneity: their portfolios, their exposures to aggregate fluctuations, and their marginal propensities to consume. Distinctly from Auclert (2019), we argue that a useful way to summarize such complex distribution is grouping households between the non-hand-to-mouth, the wealthy hand-to-mouth and the poor hand-to-mouth (HtM) based on their holdings of liquid and illiquid assets as in Kaplan and Violante (2014) and Kaplan et al. (2014).====Our estimation consists of several steps combining aggregate and household-level data for the four largest euro area countries: Germany, France, Italy, and Spain. From structural VARs, we estimate how monetary policy affects inflation, asset prices, and aggregate employment. From the EU Labor Force Survey (EU-LFS), we estimate the differential systematic sensitivity of these three groups to cyclical fluctuations. Micro data on household balance sheets and income from the Household Finance and Consumption Survey (HFCS) gives us the distribution of household portfolios. From the existing literature, we borrow estimates of the MPC out of transitory income and capital gains. We then assemble all these ingredients and, guided by our decomposition, provide a back of the envelope calculation that quantifies the impact of a surprise change in interest rates on household expenditures, separating each channel. Using the estimated response of aggregate quantities and prices from the VAR to assess the magnitudes of general equilibrium feedbacks is another key difference between our approach and that of Auclert (2019) and Patterson (2019).====At the euro area level we find that about 60 percent of the total increase in nondurable consumption is due to the indirect income and housing wealth/collateral channels. Quantitatively, hand-to-mouth households adjust their consumption markedly after the shock. The initial 100 basis-point cut of interest rates (80 basis points over a one-year horizon) results in an increase of consumption of almost 1% for the poor hand-to-mouth and of 1.8% for the wealthy hand-to-mouth. Spending of the non-hand-to-mouth households rises by 0.5%. Given their disproportionate share in consumption (about 80%), the non-hand-to-mouth households dominate the aggregate consumption increase of 0.7%.====We find that the aggregate consumption response from this simple back of the envelope decomposition matches well the one independently estimated in our structural VAR.====The important role played by the indirect general equilibrium effects implies that a simple RANK model would underestimate the dynamics of aggregate consumption. The intertemporal substitution channel remains the prevalent one only for unconstrained households, and when we separate the very rich (top 10% ) from the rest of the unconstrained, even for them the indirect effects become dominant because of their extensive holdings of housing and stocks.====For the poor hand-to-mouth, all their increase in spending is due to the combination of three elements: (i) aggregate employment increases after the monetary easing, (ii) their labor income is especially sensitive to the cycle, and (iii) their MPC our of transitory income changes is large.====The wealthy hand-to-mouth gain from this same labor income channel. In addition, because they tend to be homeowners with large mortgages, they benefit for three reasons. After an interest rate cut, house prices rise. Also inflation rises, reducing the real value of debt. This last force, however, is quantitatively modest as a result of the well documented mild response of inflation to monetary policy shocks in the euro area. Finally, when they hold flexible rate mortgage contracts they also take advantage from the reduction in interest payments.====The differences in homeownership rates, mortgage market institutions (prevalence of adjustable-rate mortgages) and labor market institutions imply that the strength of the transmission channels varies considerably across our four euro area economies. We mostly contrast Germany and Spain, which represent polar cases in terms of these structural factors. For example, households in Spain are almost twice as likely to own their home and also hold much more adjustable-rate debt. The two countries also differ in the share of hand-to-mouth households. Over 17% of Spanish households are wealthy hand-to-mouth compared to 12% in Germany. In addition, we estimate a much stronger sensitivity of aggregate house prices and labor earnings to monetary policy in Spain than in Germany.====Because of all these differences, the income and the housing wealth/collateral effects in Spain substantially exceed their counterparts in Germany. Even the direct net interest rate exposure effect of a cut in the policy rate is large and positive in Spain, whereas it is small and negative in Germany since German households are net savers. When we add up to obtain the total effect, aggregate nondurable consumption responds very strongly in Spain (nearly 2%) and much less so in Germany (0.4 %).====The rest of the paper is organized as follows. Section 2 outlines our analytical decomposition. Section 3 explains how we empirically implement this decomposition. Section 4 describes our findings. Section 5 concludes.",Household balance sheet channels of monetary policy: A back of the envelope calculation for the euro area,https://www.sciencedirect.com/science/article/pii/S0165188920300488,6 March 2020,2020,Research Article,326.0
"Papp Tamás K.,Reiter Michael","Institute for Advanced Studies, Vienna, Austria,Institute for Advanced Studies, Vienna and NYU Abu Dhabi","Available online 6 March 2020, Version of Record 13 June 2020.",https://doi.org/10.1016/j.jedc.2020.103881,Cited by (3),"We develop a method to estimate heterogeneous agent models that uses not only time series of ==== aggregates, but can also incorporate micro level data (repeated cross-section or panel). The micro data may be collected at lower frequency and time-aggregated. The method is based on the linearization approach of Reiter (2009), combined with optimal state aggregation as in Reiter (2010). The model may contain decision problems with both continuous and discrete choice. Linearity of the model solution allows fast computation of second moments and likelihood. We discuss various computational devices to maximize the speed of the estimation.","With the rise of “HANK” (heterogeneous agent New Keynesian) models (Kaplan et al., 2018), heterogeneous agent models are now competing with standard DSGE models in macroeconomic policy analysis, and there is a need to estimate the parameters of this class of models. This appears to be very difficult: due to the high dimensionality of the state space, the solution of these models used to be very time-consuming, and estimation requires to solve a model many times. The early solution methods (Den Haan, 1997, Krusell, Smith, 1998) are nonlinear methods exploiting approximate aggregation to reduce the dimensionality. An alternative is the linearization approach of Reiter (2009), which allows for a high-dimensional state space by linearizing in aggregate variables. Linearization is convenient for estimation, because second moments and the likelihood can be computed in closed form.====It is conceptually straightforward to estimate linearized models using only aggregate data. Exploiting micro-level information in the estimation is less straightforward, in particular if these data are available at a lower frequency. Typically, aggregate data are quarterly, while household or firm data are available only annually. Time aggregation at the household level is not a simple exercise in the model, because the individual decisions can be highly nonlinear, especially when there is a discrete choice element. The existing literature uses micro data to calibrate the steady state of the model, while the properties of the fluctuations around the steady state are estimated with aggregate data, or perhaps with repeated cross-sections of data, but not exploiting the panel structure of existing data.====Our main contribution in this paper is to extend the linearization approach so as to include panel and mixed frequency data in the estimation. We handle the case where agents make both continuous and discrete choices. This is relevant both for households (labor participation, housing choice) as well as for firms (lumpy investment), but it raises specific problems in the linearization approach. The solution of this kind of models is explained in Reiter (2019), here we deal with estimation. Since the solution of heterogeneous agent models is still computationally costly, we pay special attention to the efficiency of the implementation.====The basic idea behind our approach is the following. We assume we have observations that contain information about a panel of individuals. We compute the theoretical counterpart of these observations in the model. For this purpose, we simulate a panel of individuals in the model over a given time period. The panel is initialized by the distribution of the stationary state without aggregate shocks. Each individual is hit by idiosyncratic shocks every period, but aggregate variables are always assumed to be at their steady state value. At the end of the sampling period, the observation is calculated by summing over the sampled individuals. In the stationary state, the panel observation is non-stochastic because of the law of large numbers. To compute the probability distribution of this observation in the model with aggregate shocks, which will give us the likelihood of the model parameters, we linearize the whole sampling procedure in the aggregate variables. An obstacle in this procedure is that the discrete choice of individuals and the stochastic transitions are not differentiable because of their discrete nature. We introduce smoothing operations such that the dependence on the aggregate variables is captured. We make the computation fast by using reverse-mode automatic differentiation.====This technique is useful not only for estimating a model, but more generally for the analysis of any properties of a model that are functions of individual histories. Such an analysis is made difficult by the fact that, in general, there is no straightforward way of simulating the trajectories of individual agents in a linearized model. Since individual decisions are nonlinear, but linearly approximated in aggregate variables, aggregate shocks of realistic size will cause the linearized decisions of a fraction of individuals to violate the model constraints, such as a borrowing constraint, or the requirement that probabilities are between 0 and 1. We circumvent this problem by simulating individuals in the aggregate stationary state, and compute any variable of interest in the stationary state. We then differentiate this variable with respect to aggregate states and shocks.====We show with an example that the use of micro data can greatly improve parameter identification. Since the focus of this paper is methodological, we illustrate this with simulated data. The example model is based on Takahashi (2019), which features time-varying uncertainty, which is hard to observe directly in the data. For this exercise, we use a maximum likelihood approach, because it is the most straightforward method and very powerful when the model is correctly specified. However, our method can just as well be used in combination with limited-information estimation methods, which we think are more appropriate for real world applications. The problem of mis-specification, which probably affects any general equilibrium model, is likely to be even more severe when the whole cross-sectional distribution is modeled.====Despite the methodological advances in recent years, the full model solution procedure, including the computation of the stationary state, is indeed time consuming. To make estimation feasible for interesting models, it is useful to partition the set of parameters into those that affect the steady state (in particular the invariant cross-sectional distribution), and those that do not. In the latter category there are the parameters that govern the dynamics of the exogenous shock process, including the shock variances, as well as the specification of measurement errors. They also include policy parameters relating to stabilization policy, such as the Taylor rule. If the first set of parameters is pinned down (calibrated) through the steady state properties of the model, the estimation of the second set of parameters can be reasonably fast. If one wants to treat the general case where all parameters are estimated, it would be necessary to have some kind of hierarchical algorithm that changes more frequently those parameters that can be easily computed.",Estimating linearized heterogeneous agent models using panel data,https://www.sciencedirect.com/science/article/pii/S0165188920300506,6 March 2020,2020,Research Article,327.0
Peri Alessandro,"University of Colorado Boulder, USA","Received 16 December 2019, Revised 26 February 2020, Accepted 27 February 2020, Available online 5 March 2020, Version of Record 20 April 2020.",https://doi.org/10.1016/j.jedc.2020.103894,Cited by (2),This paper proposes a novel approach for the computation of dynamic stochastic equilibrium models. We design an FPGA specialized in the computation of a ,"How do we get the most out of our scarce resources? We specialize. This paper illustrates an application of this principle to the field of computational economics.====From the genesis of the Internet of Things to the development of machine learning, the rise of the Information Age has witnessed an unprecedented increase in the demand for computational power. In order to cope with this technological challenge, major corporations - like Google, Intel and Microsoft - have recently stopped relying on commodity general purpose hardware (i.e. CPUs) and have instead started developing their own chips. In 2017, Google presented its custom accelerator for machine learning applications: the Tensor Processing Unit (TPU). Using the words of a distinguished hardware engineer at Google: “This is roughly equivalent to fast-forwarding technology about seven years into the future (three generations of Moore’s Law).”====The main contribution of this paper is to bring this ==== approach to the macroeconomists’ table. To this end, we present a class of chips that has been extremely successful in meeting the acceleration needs of a variety of sectors and fields in the last decade====: the Field-programmable gate array (FPGA). From genomics, medicine,==== and physics==== to banking and finance,==== the success of FPGA acceleration rests on a variety of reasons that are particularly appealing to both academic researchers and practitioners.====The foremost reason behind FPGAs’ diffusion ascribes to a distinctive feature of the chip: ==== FPGAs are integrated circuits with billions of tiny transistors organized in configurable logic blocks (CLBs). In contrast to CPUs and GPUs, the connections among FPGA’s transistors are not pre-designed by the manufacturer.==== On the contrary, hardware developers can use a description language==== to fully customize the routing network between the CLBs. This hardware flexibility allows the implementation of virtually any algorithm at the “transistor” level, with significant speed gains from hardware specialization.====A second set of reasons involves ====: FPGAs are easily programmable==== upon knowledge of the hardware design principles (which researchers can acquire by hiring/coauthoring with FPGA designers). In addition, recent developments in FPGA compilers are likely to ease the access to the technology even further, as discussed in the concluding remarks.====Another set of reasons relates to FPGAs’ ====. First, like GPUs, FPGAs are chips connected to a host. As a result, they save the infrastructure, energy consumption and maintenance costs of a cluster. Second, it is well established that FPGAs are more energy efficient than GPUs (for applications in finance, see De Schryver, 2015). Later in the paper, we discuss the savings associated with the usage of FPGA vis-á-vis GPU hardware in the context of our application.====Last but not least, recent technological advancements in cloud services may have non-trivial implications for the ==== of FPGA applications. Just like the fall in the cost of DNA sequencing has fostered breakthrough discoveries in genomics and medicine, the fall in the entry costs to the FPGA technology could provide an unprecedented opportunity for the research and development of high-speed algorithms.====In this context, the launch in 2017 of Unix Instances connected to FPGA chips (EC2 F1 Instances) on the Amazon AWS cloud platform could represent a cornerstone for the diffusion of FPGA-acceleration for at least two reasons. First, the cloud service dramatically reduces the entry barriers to the technology by allowing users to lease instead of buy the chip. Second, and more far-reaching, it facilitates the sharing of FPGA’s algorithms by allowing developers to upload their FPGA images to the cloud. For instance, the interested reader can deploy the FPGA solution discussed in this paper by launching our Amazon Machine Image (AMI) in Amazon AWS: the Value Function Iteration Accelerator - FPGA.====Although virtually any application can be implemented in an FPGA, not all applications benefit from FPGA acceleration. FPGAs are particularly apt for fine-grained parallelism: that is for accelerating algorithms that can be broken down to a large number of small tasks. Remarkably, the electronic engineering literature has already shown the benefits of FPGA acceleration for many algorithms that: ==== are integral parts of the economist’s toolboox (like maximum likelihood estimation and Monte Carlo simulations); or ==== have recently started receiving attention from economists (like neural networks, Fernández-Villaverde et al., 2019).==== In a related manner, Section 7 presents an interesting application in the context of risk management during the financial crisis. In particular, it discusses how FPGA accelerators helped JP Morgan drive down the time required to estimate the risk parameters of their derivative portfolio from ten hours (in a cluster of thousands of cores) to four minutes, making it possible to use these estimates to inform risk management and trading decisions in real time.====In the spirit of Aldrich et al. (2011), we illustrate the potential of the FPGA technology to solve a standard real business cycle (RBC) model. In particular, we design an FPGA specialized in the solution of a Bellman equation via value function iteration (FPGA approach). In doing so, we present a ==== parallelization scheme (the assembly line algorithm) and compare the speed gains vis-á-vis the GPU data-parallelization scheme proposed in Aldrich et al. (2011) (GPU approach). Our work documents  ~ 10-fold speed gains in the solution of the Bellman equation via value function iteration, on a state space with 65536 points in the capital grid and 4 points in the productivity shock grid. In addition, using the hourly rates charged by Amazon AWS on the two platforms, we find that solving the bellman equation in the FPGA is 67% cheaper than in the GPU.====The speed gains are a byproduct of hardware specialization. The FPGA approach grants access to two layers of parallelism, which are inaccessible to software developers: ==== instruction-level and ==== pipeline parallelism at the ==== (CLB) resources level. Intuitively speaking, the former determines the operations to be performed in parallel at every clock cycle,==== while the latter organizes their synchronous execution along an assembly line. Inspired by Ford’s assembly line, we refer to this parallelization scheme as the assembly line parallelism.====Fostered by the recent development of heterogenous agents models featuring nominal rigidities (Bayer et al., 2019) and a growing attention to the distributional effect of government policies, the FPGA approach could find promising application in the complex (and sometimes unfeasible) estimation of heterogenous agents models. This paper provides a first step in this direction, discussing the acceleration of one of the most expensive computational bottlenecks involved in this process: the solution of a Bellman equation.====By and large, these considerations suggest the presence of significant gains from hardware specialization, so far unexplored by the macroeconomic literature.",A hardware approach to value function iteration,https://www.sciencedirect.com/science/article/pii/S0165188920300622,5 March 2020,2020,Research Article,328.0
"Schnaubelt Matthias,Fischer Thomas G.,Krauss Christopher","Department of Statistics and Econometrics, University of Erlangen-Nürnberg, Lange Gasse 20, Nürnberg 90403, Germany","Received 19 September 2019, Revised 26 January 2020, Accepted 2 March 2020, Available online 5 March 2020, Version of Record 8 April 2020.",https://doi.org/10.1016/j.jedc.2020.103895,Cited by (16),"Most statistical arbitrage strategies in the academic literature solely rely on price time series. By contrast, alternative data sources are of growing importance for professional investors. We contribute to bridging this gap by assessing the price-predictive value of millions of tweets on intraday returns of the S&P 500 constituents from 2014 and 2015. For this purpose, we design a machine learning system addressing specific challenges inherent to this task. At first, building on the literature of financial dictionaries, we engineer domain-specific features along three categories, i.e., directional indicators, relevance indicators and meta features. Next, we leverage a random forest to extract the relationship between these features and subsequent ","Financial market predictions are a challenging task due to the high level of noise and the widely accepted, semi-strong form of market efficiency (Fama, 1970). Hence, there is a rising interest in alternative data and machine learning techniques to identify and exploit new predictive factors in academia (see, e.g., Bekiros, 2010, Fischer, Krauss, 2018, Huck, 2019, Xu, Chen, Coleman, Coleman, 2018) and in practice (see, e.g., Goldman Sachs, 2016, Morgan, 2018, Wilson, 2015).====A key alternative data source are news and text data on social media platforms. The corresponding literature can be clustered in three categories, building on the surveys of Nassirtoussi et al. (2014) and Oliveira et al. (2017) with more than 40 works. A first strand of literature focuses on the causal relationships between aggregated measures of sentiment and asset returns. For example, Antweiler and Frank (2005) use an aggregated bullishness measure for messages from Yahoo! Finance and Raging Bull to find an effect of sentiment on intraday market volatility and a small effect on returns. Subsequent works perform similar studies on stock message board posts (Das and Chen, 2007) and daily mood states from Twitter (Sprenger et al., 2013). Yet other studies focus on firm-specific stock returns of aggregated sentiment from news articles (Tetlock, 2007, Tetlock, Saar-Tsechansky, Macskassy, 2008) or Twitter messages (Sprenger et al., 2013). A second strand of literature focuses on the prediction of asset returns immediately following the release of single news items. For example, Schumaker and Chen (2009) and Schumaker et al. (2012) classify financial news articles using support-vector regression with regard to their effect on intraday stock returns. Similar studies with methodological focus examine corporate disclosures (Groth, Muntermann, 2011, Kraus, Feuerriegel, 2017), ad-hoc announcements (Hagenau et al., 2013) and tweets (Knoll et al., 2018). A third, more recent strand of literature focuses on the extraction of topics or domain-specific sentiment dictionaries using statistical methods: Huang et al. (2018) use topic modelling to compare analyst reports to earnings calls. Bybee et al. (2019) use topic modelling on business news to construct measures of topic-specific news attention, which they then relate to economical time series. Finally, Pröllochs et al. (2015) and Ke et al. (2019) develop statistical approaches to construct domain-specific sentiment dictionaries from stock returns.====To our knowledge, as of today, no study has comprehensively analyzed the intraday predictive value of twitter data for all S&P 500 constituents in a financial machine learning context using a rigorous event-based backtesting approach. With our work, we aim to fill this void. Specifically, we make the following contributions to the literature: (1) We develop a machine learning system that extracts a functional relationship between text messages and their subsequent stock returns in a low signal-to-noise setting. For this purpose, we use domain-specific features along three distinct categories (directional indicators, relevance indicators, and meta features). Once trained, our machine learning-based system is able to process a stream of noisy tweets, to extract information about the potential direction and relevance of the message, and to generate an actionable market signal for either trading or backtesting. As predictive models, we benchmark random forests against neural networks, a k-nearest neighbors model and a naïve, sentiment-based model. (2) We run an event-based backtest under realistic conditions, i.e., with execution constraints, trading costs and capital requirements. We find that our tweet-based trading strategy is able to generate statistically significant annualized returns of 6.4 percent with a Sharpe ratio close to 2.2, after accounting for transaction costs and limiting single-stock exposures to a reasonable level. Hence, our findings indicate that tweets may serve as an alternative data source with some untapped potential for excess returns. (3) Finally, we shed light into the machine learning black box and rigorously ask whether results are in line with economic rationale. Specifically, we show that returns are driven by the temporal distribution of tweets. The majority of profits stems from tweets clustered closely together in time, which corresponds to high-event situations for a stock that can often be profitably exploited. Second, we find that our features correspond well to subsequent market reactions. Tweets with words or n-grams with positive (negative) connotation are generally followed by an increase (decrease) of the price of the stock corresponding to the tweet. Third, we observe that medium market capitalization stocks rank higher in per-tweet profitability than high or low market capitalization stocks in the S&P 500 universe. Medium size stocks may provide a compromise between tweet coverage (sufficient number of tweets) and tweet relevance (large business-to-consumer stocks tend to show many noisy tweets). Fourth, we look into the assimilation of news into market prices by considering different holding periods. Results suggest a gradual diffusion of information, and a stronger and more persistent price reaction for tweets triggering short sales.====The remainder of this paper is organized as follows: In Section 2, we describe the data sets and software used for our work. Section 3 details all building blocks of our methodology, i.e., the generation of features and targets, the training of predictive models, the generation of trading signals, and finally the backtesting framework. Our results are presented in Section 4, and we conclude in Section 5.",Separating the signal from the noise – Financial machine learning for Twitter,https://www.sciencedirect.com/science/article/pii/S0165188920300634,5 March 2020,2020,Research Article,329.0
"Laussel Didier,Long Ngo Van,Resende Joana","Aix-Marseille University (Aix-Marseille School of Economics), CNRS & EHESS, France,Department of Economics, McGill University, Canada,Hitotsubashi Institute for Advanced Study, Hitotsubashi University, Japan,Cef.up, Economics Department, University of Porto, Portugal","Received 5 October 2019, Revised 17 February 2020, Accepted 23 February 2020, Available online 2 March 2020, Version of Record 16 March 2020.",https://doi.org/10.1016/j.jedc.2020.103869,Cited by (8),"We present a model of market hyper-segmentation, where a monopolist acquires within a short time all information about the preferences of consumers who purchase its vertically differentiated products. The firm offers a new price/quality schedule after each commitment period. Lower consumer types may have an incentive to delay their purchases until next period to obtain a better introductory offer. The monopolist counters this incentive by offering higher informational rents. Considering the dynamic game played by the monopolist and its customers, we find that there is always a Markov perfect equilibrium (MPE) in which the firm immediately sells the good to all customers, offering the Mussa–Rosen static equilibrium schedule to first time customers (and getting full commitment profits). However, if the commitment period between two offers is long enough, there is another MPE with gradual market expansion. Contrary to the Coasian result for a durable-good monopoly, we find that in both equilibria the profit of the monopolist increases (and the aggregate ==== decreases) as the interval of commitment shrinks. The model yields policy implications for regulations on collection and storage of customers information.","In recent years, the rapid growth of new-generation digital technologies has been reshaping firms’ business models in many economic sectors, such as software, retail, energy, automobiles, fashion and apparels, media and entertainment, and so on. On the one hand, these technologies allow firms to collect and treat very large volumes of (big) data about their customers, which enables them to get very accurate information on consumers’ true willingness to pay for their products. On the other hand, new-generation digital technologies (including machine learning, robotics, digital platforms, 3D printing, etc.) also enable firms to engage in innovative personalization strategies, through which they tailor their product/price offers to suit consumers’ individual preferences and needs, which the firms uncover within a short time.====The ability to process unprecedented amounts of real time data about their customers indeed permits firms achieve what is now known as hyper-segmentation of markets, increasing the scope of price and quality discrimination.==== As a result, in recent years, many firms are committing themselves to deliver personalized product-price offers to their customers at scale.==== Among a plethora of examples of this new trend, one can cite offers of personalized clothing items (e.g., Nike, Adidas, Longchamp, Gucci, Louis Vuitton and other luxury brands), the customizable Tesla dashboard, personalized TV scheduling (through on-demand video systems), personalized tourism experiences, customizable executive education programmes, and so on...====In this paper, we investigate the market dynamics that arise when a monopoly firm is able to: (i) collect Big Data about its customers (after their first purchase) and (ii) engage in hyper-segmentation strategies, making personalized quality-price offers to its returning customers. More precisely, we propose a dynamic extension of the static model of Mussa and Rosen (1978) on monopoly and product quality, extending the seminal Mussa–Rosen model in order to account for the consumers’ repeated purchases of a non-durable good for instantaneous consumption and the monopolist’s gradual information collection on the preferences of its set of heterogeneous customers. We assume that the monopolist cannot modify the price-quality schedules offered to its customers during an exogenous period of finite length, called the commitment period (which corresponds to the usual contractual commitment period in dynamic models investigating the Coasian conjecture). While in our formal model, for expositional reasons, the length of the commitment period is treated as a parameter, as we shall show, the monopolist has an incentive to make this length as close as possible to an irreducible minimum consistent with the state of information technology and the legal requirements that limit firms ability to collect and store consumers information. This result allows us to discuss the policy implications of our model.====At the beginning of each commiment period, the monopolist proposes to new customers a period-specific price-quality schedule intended to induce them to reveal their true type. Once a consumer has made her first purchase, the firm is able to collect “Big Data” to uncover her exact type, so that any possible misreporting (of consumers’ willingness to pay for the good) has no consequence beyond the period of the first purchase.====In other words, quality-price menus and data collection constitute complementary tools to implement personalization strategies within our model: the firm relies on quality-price menus to screen new customers, and as soon as they have made their first purchase, it exploits Big Data to uncover customers’ true willingness to pay. This allows the monopolist to offer to each returning customer the personalized quality-price deal that maximizes her potential surplus (which is fully extracted by the firm).====Thanks to accurate data collection (after the first purchase), former customers will be unable to misreport their type in subsequent periods.==== This means that consumers can cheat and misreport their types only in the period in which they purchase the good for the first time. As a result, only new consumers (who are buying the good for the first time) may earn information rents. Hence, the model combines second-degree discrimination with respect to new customers (whose preferences are unknown to the monopolist) with first-degree discrimination on former customers (whose preferences have become fully known to the monopolist after their first purchase). Consequently, the monopolist only needs to distort downward the quality offered to the successive rungs of the customers who buy the good for the first time.====Our main focus is on the case where the monopolist is unable to commit to future sales decisions. Under this scenario, we characterize the Markov Perfect Equilibria of the dynamic game played by the firm and its customer base (which consists of a continuum of consumer types). The firm makes decisions on (a) the quality-price schedules to be offered to new customers, and (b) personalized quality-price offers to old customers. Knowing that the firm will be able to track their true willingness to pay immediately after their first purchase, the targeted new consumers must decide on whether to make their first purchase now, or to delay it until the next period.====In a Markov Perfect Equilibrium, the monopolist quality/price strategy maximizes its discounted lifetime profit, given consumers’ expectations; and, given the monopolist’s sales strategy, consumers’ expectations are rational. Our first result for this model is that there exists a Markov-Perfect Equilibrium (which we will refer to as Equilibrium ==== from now on) in which the monopolist simply offers the static Mussa–Rosen equilibrium schedule in the initial period and all customers purchase the good for the first time in that period====. Afterwards, each consumer is offered a take-it-or-leave-it personalized price/quality deal which fully extracts her surplus (due to the firm’s data collection ability). At this Equilibrium ====, the monopolist obtains the Mussa–Rosen profits in the initial period, whereas it gets the maximum profit of a first-degree (perfectly discriminating) monopolist in all subsequent periods. This explains why the monopolist chooses to serve all the consumers in one go, offering the lowest quality at zero price to a subset of (bunched) consumers: by selling them the good immediately in the first period, the monopolist is able to collect data on their tastes, subsequently engaging in first-degree price discrimination (and collecting the corresponding profits).====Our second result is that there is generically another Markov-Perfect Equilibrium (which, from now on, we will refer to as Equilibrium ====) with completely different properties. In Equilibrium ====, successive rungs of customers purchase the good for the first time in consecutive periods, meaning that the monopolist is expanding the market gradually (in small steps) from high-type to low-type consumers. While it remains true that once a consumer has bought the good for the first time, she will be offered the personalized first-degree price discriminatory deal in subsequent periods, in Equilibrium ====, some customer types end up delaying their first-time purchase, which means that the monopolist must wait to discover their exact preferences (which can only be uncovered after consumers’ first-time purchase period, which is up to their rational decision).====The intuition behind the multiplicity of MPEs in this model is similar to the one underlying self-fulfilling expectations equilibria. When all customers expect that all of them or almost everyone (except possibly a set of measure zero) will buy in the initial period, no low-type customer can hope to gain anything from unilaterally deviating by delaying her first purchase: she anticipates that there will be no new offers to first-time buyers in any period in the future. Consequently, low-type customers do not require any informational rent to induce them to buy in the first period. Facing such expectations, the monopolist covers immediately the whole market. Thus, customers’ expectations are fulfilled.====In contrast, when consumers expect that the firm will not be able to sell the good in the initial period to a subset of consumers with a positive measure, they contemplate the possibility of gaining by delaying their first purchase in order to take advantage of the firm’s future attractive offers to recruit new customers. Therefore, in any given period, some set of consumers could be persuaded to become first-time customers only if they are offered strictly positive informational rents, and this applies even to the marginal customer of that subset. In turn, the consumers’ aspiration to get more informational rents makes the firm unwilling to sell immediately to all new customers. Thus, in Equilibrium ====, consumers expectations are also fulfilled.====Interestingly, the model exhibits non-Coasian dynamics. We find that as the length of the commitment period shrinks, for each of the two equilibria, the firm’s aggregate profit increases, contrary to the well-known “vanishing profit” result obtained in the standard Coasian model of durable good monopoly. The intuition behind our result is as follows. The monopolist’s discounted stream of profits is higher, the sooner it has sufficient information and ability to practice first-degree discrimination over consumers. An almost immediate consequence of this result is that the discounted stream of aggregate consumers’ surplus falls when the monopolist’s length of commitment period decreases. Moreover we show that, when the length of the commitment period falls below a threshold value, Equilibrium ==== (which is less favorable to the firm and more favorable to customers relative to Equilibrium ====) disappears, and only Equilibrium ==== remains. Accordingly, our analysis suggests that firms and customers’ interests are diametrically opposed regarding public policies aiming at reducing firms’ ability to collect information on consumers (e.g., the EU’s General Data Protection Regulation (GDPR) policy or the California Consumer Privacy Act of 2018====.). Such policies tend to increase the length of time needed to completely identify consumers’ tastes, thus resulting in greater consumer surplus and lower monopoly profit. We shall argue that our model may provide some hints as to why, relative to the USA, the European Union is more inclined to institute and enforce restrictive data protection laws which tend to hurt firms and benefit consumers.",Quality and price personalization under customer recognition: A dynamic monopoly model with contrasting equilibria,https://www.sciencedirect.com/science/article/pii/S0165188920300385,2 March 2020,2020,Research Article,330.0
"Grilli Ruggero,Tedeschi Gabriele,Gallegati Mauro","Department of Managment, Università Politecnica delle Marche, Ancona, 60122, Italy,Department of Economics, Universidad Jaume I, Castellon, 12071 Spain,Department of Managment, Università Politecnica delle Marche, Ancona, 60122,Italy","Received 8 October 2018, Revised 16 October 2019, Accepted 4 February 2020, Available online 29 February 2020, Version of Record 14 March 2020.",https://doi.org/10.1016/j.jedc.2020.103863,Cited by (9),"In this paper we characterize the evolution over time of a credit network in the most general terms as a system of interacting banks and firms operating in a three-sector economy with goods, credit and ====. Credit connections change over time via an evolving fitness measure depending from lenders’ supply of liquidity and borrowers’ demand of credit. Moreover, an endogenous learning mechanism allows agents to switch between a loyal or a shopping-around strategy according to their degree of satisfaction. The crucial question we investigate is how financial bubbles and credit-crunch phenomena emerge from the implemented mechanism.","The interdependence between real and financial markets has a long tradition in Economics. The literature has particularly focused on which of these two markets drives business fluctuations and economic growth. The main research question was to understand if it was the real economy to foster finance sectors or, alternatively, the credit market to stimulate real production. In other words, the economic research has always sought to capture the driving-force generating expansions and contractions of economic and financial cycles, that is to seize the direction of the causality nexus. Although theoretical and empirical studies diverge in identifying the direction of the causality nexus, with the pre-Lehman studies identifying the direction from real markets to financial sectors (see Bernanke, Gertler, 1989, Greenwald, Stiglitz, 1993, Kiyotaki, Moore, 1997), while the post-Lehman ones reversing it (see Brunnermeier, Eisenbach, Sannikov, 2012, Christiano, Ikeda, 2011), what is certainly undoubted is the self-reinforcing interaction between the two sectors which translates into booms followed by busts (see Borio, 2014). Yet, when credit boom bubbles go burst the macroeconomic consequences are severe (see Jordà, Schularick, Taylor, 2015, Mishkin, 2008). The interaction, in fact, is seen as the key ingredient in the cycles’ onset (see, Grilli et al., 2017, for a survey of the relevant literature). In whatever way this is modeled, either via balance-sheets interconnectedness among intermediaries (see Adrian, Shin, 2010, Adrian, Shin, 2010, Adrian, Shin, 2011, Geanakoplos, Gertler, Kiyotaki, 2010), or more sophisticated network theory tools (see Acemoglu, Ozdaglar, Tahbaz-Salehi, 2013, Bargigli, Tedeschi, 2014, Battiston, Delli Gatti, Gallegati, Greenwald, Stiglitz, 2007, Battiston, Delli Gatti, Gallegati, Greenwald, Stiglitz, 2012, Battiston, Delli Gatti, Gallegati, Greenwald, Stiglitz, 3., 2012, Glasserman, Young, 2015, Lageras, Seim, 2016), the interaction among banks and firms represents the channel to propagate/reduce financial frictions among market participants (see, Gertler and Williamson, 2015, for a collection of articles on this topic).====Following the pioneering financial accelerator framework by Bernanke and Gertler (1989) and Kiyotaki and Moore (1997), many attempts to model financial frictions and study their impact on financial and real markets have been proposed (see Bernanke, Gertler, Gilchrist, 1999, Carlstrom, Fuerst, 1997, among the first attempts). All these studies have identified some possible driving-force able to amplify and propagate the conventional transmission mechanism of real and monetary shocks through the endogenous emergence of limits on the available quantity of external finance. In this regard, an interesting branch of literature has focused on coordination failure mechanisms as a possible explanation of the emergence of credit frictions. Specifically, this literature has shown that the strategic behavior of heterogenous interacting agents competing for the achievement of scarce financial resources gives rise to spillovers and strategic complementarities leading to coordination failure phenomena (see Aikman, Haldane, Nelson, 2015, Bassetto, Cagetti, De Nardi, 2015, Rajan, 1994). Following this line of research, in this paper we are interested in identifying an endogenous mechanism able to generate coordination failure and analyzing its repercussions at micro and macro level. In this regard we build a three sector economy with goods, credit and interbank market, where agents, banks and firms, strategically compete for allocating their financial resources on an evolving credit network. On the one hand firms, operating as borrowers in the credit market, compete in seeking the best credit conditions in order not to be rationed. On the other hand banks, operating as lenders in the credit market and as lenders or borrowers in the interbank one, compete in offering the best credit conditions in term of interest rates and supply of liquidity when lenders, while they behave as the companies do when borrowers. The credit linkages between borrowers and lenders might change over time via a preferential attachment evolving procedure such that each borrower can enter into a lending relationship with lenders with a probability proportional to a compound fitness measure. This measure is a combination between lender attractiveness (i.e its fitness) in terms of supply of liquidity and interest rates and the borrower satisfaction (i.e its intensity of choice) in terms of credit requirements fulfillment. Lenders, therefore, can attract their customers by offering a high supply of liquidity associated with a low interest rate and borrowers reinforce the lender signal when their credit requirements are met. Moreover, the conduct of debtors which strengthen / weaken their creditors’ attractiveness on the basis of the granted loan, generates switching behavior in this group of agents. On the one hand, customers who meet their credit needs become loyal to their lenders and strengthen the intensity of choice parameter. On the other hand, those agents who are rationed become shoppers-around and weaken the intensity of choice parameter. The co-movement between the intensity of choice and the fitness is able to reproduce different credit network topologies ranging from the random graph to the scale-free one. This evolutionary framework allows us to emphasize the effect of different agents’ strategies and different network architectures on the business fluctuations.====In line with the results of the above mentioned studies on strategic complementarities, this work shows that a strong coordination in the agents’ behavior may generate a highly centralized credit network and this produces gridlock effects in the credit market and the emergence of credit crunch phenomena. An intuition of how the credit network architecture influences business fluctuation is as follows. Our system crosses several steps in the credit network evolution corresponding to the different strategies adopted by borrowers and lenders. There are times governed by the shopping around strategy, where the lender attractiveness is very low and, therefore, the credit network is random. Poor economic performances are associated with these periods because agents remain small in size and, consequently, the demand of credit and the production are low. As time goes by, some financial institutions gain credibility in the market and become more attractive in term of supply of liquidity and interest rates. This reinforces the fitness of these lenders which can attract several clients and ensure their loyalty. The heterogeneity of agents, therefore, increases the fitness signal and moves borrowers to choose the loyal strategy which is associated with a lower rationing. This process leads to the emergence of exponential or scale-free credit network topologies. At the same time, macroeconomic conditions improve, the credit allocation becomes more efficient and the production increases. Gradually, the system tends to create a large financial institution (i.e. hub) to which many loyal clients are connected. Obviously, this high centrality in the credit network, after an initial period where the hub can meet clients’ credit needs, leads to gridlock effects and credit crunch phenomena. The hub, unable to satisfy its clients loses its attractiveness, the credit bubbles go bust, the aggregate output collapses. Consequently, the system returns to a random credit network with shopping around agents.====Interestingly enough, our approach in explaining the materialization of financial frictions is very close in spirit to the Minskyan financial instability hypothesis, where endogenous shifts on the degree of financial fragility of banks generate business fluctuations (see Ferri, Minsky, 1992, Minsky, 1964). In fact, we show that swings in banks’ leverage are associated with the sequence of life stages that financial institutions undergo, from small decentralized entities to too-big centralized hubs. By combining the heterogenous agents approach with an endogenous mechanism generating time-varying financial relationships, we can show that the two main ingredients explaining the evolution of risk, namely the agents’ financial fragility and the direct or indirect interconnections among market participants, are far from being independent and, actually co-evolve.====Our stylized mechanism, designed to combine micro behaviors with meso financial interlinkages and macro performances, extends the theoretical literature dealing with the relationship between connectivity, business and financial cycles and systemic instability in dynamic credit networks (see, Grilli et al., 2017, for a survey of this literature). In this regard, the contribution of this work in respect to this literature is twofold. ====, to our knowledge, this is the only study introducing in a three sectors economy, the dynamic evolution of credit and interbank networks jointly. In fact, models with goods, credit and interbank markets just consider random graph credit systems where, by exogenously changing the degree of connectivity among agents, the system reproduces the well-known trade-off between systemic and sharing risk (see Grilli, Tedeschi, Gallegati, 2014, Grilli, Tedeschi, Gallegati, 2015, Tedeschi,. Mazloumian, Gallegati, Helbing, 2012). On the contrary, the literature dealing with evolutionary networks, just focuses attention on two-sectors’ systems composted by firm-bank, firm-firm or bank-bank credit relationships (see, for instance, Berardi, Tedeschi, 2017, Delli Gatti, Gallegati, Greenwald, Russo, Stiglitz, 2009, Delli Gatti, Gallegati, Greenwald, Russo, Stiglitz, 2010).====, among these last mentioned studies modeling dynamic credit linkages, the originality of this work is in the inclusion of a reinforcement mechanism between the lender fitness and the borrower intensity of choice. Specifically, the “endogenization” of the intensity of choice allows us, on the one hand, to solve the well-known calibration problems associated with this parameter (see Kukacka, Barunik, 2017, Recchioni, Tedeschi, Gallegati, 2015, for technical details), and, on the other hand, to introduce competition both on lender and borrower side.====Last but not least, the problems arising from credit market interconnectedness have also been highlighted by empirical studies which have emphasized the effect of preferential trading relationships on credit availability (see Boot, 2000, Ongena, Smith, 2000, Petersen, Rajan, 1994, Vidal-Tomas, Tedeschi, Ripolles, 2019). Although we do not want to enter into the debate on (dis)advantages of enduring credit relations, our results certainly support a vast literature sustaining privileged credit relationships as a tool to mitigate asymmetric information problems. However, we show that these relationships must be established with small local financial institutions (Avery, Samolyk, Berger, Black, 2011, Berger, Frame, 2007, Presbitero, Udell, Zazzaro, 2014, Strahan, Weston, 1998), and do not have to generate too-big or/and too-interconnected hubs.====The rest of the paper is organized as follows. In Section 2 we describe the model by analyzing the behavior of firms and banks and the dynamic of the credit and interbank network. In Section 3 we present the results of the simulations. Specifically, we proceed in two steps: firstly, we present the performances of agents and investigate how they influence the evolution of the financial network; secondly, we study the impact of the different network topologies on business fluctuations. Finally, Section 4 concludes.",Business fluctuations in a behavioral switching model: Gridlock effects and credit crunch phenomena in financial networks,https://www.sciencedirect.com/science/article/pii/S0165188918303476,29 February 2020,2020,Research Article,331.0
"Bolin Kristian,Caputo Michael R.","Department of Economics, Centre for Health Economics, University of Gothenburg, Gothenburg, Sweden,Department of Economics, University of Central Florida, 4000 Central Florida Blvd., Orlando, Florida 32816-1400, USA","Received 24 August 2018, Revised 14 February 2020, Accepted 25 February 2020, Available online 29 February 2020, Version of Record 14 March 2020.",https://doi.org/10.1016/j.jedc.2020.103893,Cited by (2),"The health capital model of Grossman (1972) is extended to account for uncertainty in the rate at which a stock of health depreciates. Two general versions of the model are contemplated, one with a fully functioning financial market and the other in its absence. The comparative dynamics of the feedback form of the consumption and health-investment demand functions are studied in these general settings, where it is shown that the key to deriving refutable results is to determine how a parameter or state variable affects the expected lifetime marginal utilities of health and ====. To add further reach to the results, a simplified stochastic control problem is explicitly solved, yielding estimable structural feedback demand functions.","The health-capital model developed by Grossman (1972) by necessity relied on a number of simplifying assumptions, “ … all of which should be relaxed in future work” (p. 247). In particular, Grossman (1972, pp. 247–248) argued that a more general model, most importantly,====The above admonishments of Grossman (1972) are taken seriously in what follows. In particular, the assumption of a known, constant rate of depreciation of health is dropped in favor of a time-varying stochastic rate of depreciation. At the same time, however, all the other essential features of Grossman's (1972) canonical model are retained. That way, new properties or complications that arise in an extended model can be fully attributed to the one change made, to wit, the introduction of a time-varying stochastic rate of depreciation.====Not surprisingly, Grossman's (1972) demand-for-health model has been extended in a few directions. A comprehensive survey of the extensions can be found in Grossman (2000), while a more up-to-date account can be found in Bolin and Lindgren (2016). It is worth noting that most of the previous theoretical treatments of the demand-for-health model are deterministic and focus on an open-loop solution of the control problem, as in, e.g., Ehrlich and Chuma (1990), Ried (1998), Eisenring (1999), Galama (2015), Laporte (2015), Strulik (2015), Bolin and Lindgren (2016), and Fu et al. (2016). In an open-loop solution, the values of the control variables are decided at the initial date of the planning horizon and are found by solving the Pontryagin necessary conditions. They are functions of the initial and terminal values of time and the state variables, as well as the parameters, in general. As discussed by Dockner et al., 217–18), open-loop solutions are not, in general, appropriate for control problems when the state variables are governed by stochastic differential equations. Consequently, any extension of the demand-for-health model that includes stochastic differential equations for the state variables, as is the case here, cannot employ open-loop solutions, but must instead employ feedback solutions, an important observation that will be revisited shortly.====One fundamental assumption of the demand-for-health model is that individuals are capable of influencing their stock of health by way of investments in health. Health investments are usefully thought of as health-related decisions made by individuals regarding, say, the extent of physical exercise, whether to smoke, the composition of one's diet, and the like. Formally, a stock of health evolves according to an equation of motion in which the stock of health at the next instant of time equals its current stock, plus investment in health, and minus depreciation. Although a stock of health can be influenced by way of investment in health, exogenous factors and uncertainty make its control less than perfect.====The evolution of a stock of health is stochastic for at least two reasons. The first reason stems from the fact that the impact of health investment on the evolution of a stock of health is uncertain. For instance, exerting specific amounts of physical exercise, or utilizing a specific amount of medical care, will not yield a certain increment in a stock of health. The second is that the rate of depreciation of health is itself stochastic, a fact recognized by Grossman (1972) nearly a half-century ago. This is the case if the emergence of a disease is characterized by an uncertain upward jump in the rate of depreciation of health. In particular, if an individual is struck by a serious illness, then a significant and persistent, but not fully certain, upward jump in the rate of depreciation occurs. If, however, the illness is less serious, then the upward jump is smaller and less persistent, but still not fully known. Ultimately, the degree of persistence and the size of the jump determine whether or not medical care is required.====In what follows, a distinction is made and maintained between curative and precautionary health investments. Curative health investments are defined to have direct effects on the stock of health or the rate of depreciation of health, or both, and are produced using medical-care goods and services. It is assumed that curative health investments are produced in a per-protocol fashion. As a result, curative health investments are not among an individual's choice variables. Accordingly, curative health investments are not considered in the formal models studied herein.====In contrast, precautionary health investments are defined as those that are under the control of an agent and indirectly affect the rate of depreciation of a stock of health by directly affecting the stock of health itself. That is, the current stock of health, which is directly influenced by precautionary health investments, determines the rate of depreciation of health. The view that precautionary health investments indirectly affect the rate of depreciation of health in the aforementioned manner is consistent with ample medical evidence. Said evidence suggests that health-related behaviors exert a direct influence on a health stock and thus indirectly affect the risk of various illnesses. As precautionary health investments are under the control of an individual, they are the focus of the stochastic optimal control models of health investment contemplated henceforth.====In light of the above and Grossman's (1972) advice, it is not surprising that a few extensions of the demand-for-health model that include stochastic elements exist. The extensions can be usefully divided along whether the model is formulated over two periods or many. The early work, typified by Dardanoni and Wagstaff (1987), Selden (1993), and Chang (1996), is of the two-period variety, and sought to determine the effect of an increase in initial wealth on investment in health when future health, or the return to health, is uncertain. A few multiple period stochastic models exist. Some have been formulated in discrete time, as in Picone et al. (1998) and Pelgrin and St-Amour (2016), but others have been formulated in continuous time, as in Cropper (1977), Liljas (1998), Ehrlich (2000), Laporte and Ferguson (2007), Hugonnier et al. (2013), and Burggraf et al. (2015). The papers closest in spirit to the present work are those by Ehrlich (2000) and Laporte and Ferguson (2007). Ehrlich (2000) investigated the comparative dynamics of a feedback solution of a health capital model where the time of death is uncertain under highly restrictive assumptions. In an attempt to capture a single serious illness occurring during one's life, Laporte and Ferguson (2007) formulated a stochastic control problem in which the stock of health evolves according to a Poisson process. They provided a qualitative characterization of the expected time-paths of health and health investment using a phase diagram, and compared them with their deterministic counterparts.====As noted earlier, when studying stochastic control problems, one must examine feedback solutions. A feedback solution for the control variables is found by solving the maximization problem that forms the Hamilton-Jacobi-Bellman (H-J-B) equation associated with the underlying stochastic control problem. In general, a feedback solution depends on the current value of the state variables and time, the parameters, and the terminal value of time. Thus, a feedback control for health investment will by construction provide an optimal decision rule for the rate of investment in health for whatever value the current health stock and parameters might take, at every point in time in the planning horizon. In particular, in the three versions of the health-capital model contemplated here, the optimal decisions for the rates of consumption and investment in health are only a function of the current values for health, wealth, the depreciation rate of health, and the parameters. Consequently, this is the approach followed in order to study the demand-for-health model when the rate at which a stock of health depreciates is stochastic. As explained in some detail in §6, the dependence of a feedback solution on current information—as opposed to past or future information—is particularly useful for empirical work.====In light of the preceding, the main objectives of the paper are to (i) develop three versions of Grossman's (1972) health-capital model that incorporate uncertainty along the aforesaid lines, (ii) derive the comparative dynamics of the feedback solution for each model, (iii) identify the fundamental objects that are key for deriving refutable comparative dynamics, (iv) derive an explicit solution for the feedback consumption and health-investment demand functions under a set of parametric assumptions on the model primitives, and (v) use the general comparative dynamics results to help identify why refutable comparative dynamics exist for the explicit solution. In passing, note that none of the aforesaid research address the set of objectives put forth here.",Consumption and investment demand when health evolves stochastically,https://www.sciencedirect.com/science/article/pii/S0165188918302471,29 February 2020,2020,Research Article,332.0
Bonfiglioli Alessandra,"Queen Mary University of London and CEPR, Mile End Road, London E1 4NS, UK","Available online 29 February 2020, Version of Record 13 June 2020.",https://doi.org/10.1016/j.jedc.2020.103872,Cited by (1),This paper provides a series of comments aimed mainly at addressing the empirical relevance of the proposed interpretation of the mechanism and partly at considering the possible interaction between the trade and ,"How did the trade liberalization reform of the early Nineties in Colombia affect welfare in the short and long run, once its effects on exporters’ dynamics and technological improvement are taken into account? Alessandria and Avila (2019) address this question quantitatively by means of a general equilibrium, heterogeneous firms model that is able to match macro and micro facts about Colombian export and exporters between 1989 and 2007. The main insight of their theory is that trade liberalization induces more firms to make the costly and risky investment required to improve their transportation technology and become exporters. It also makes incumbent exporters keep investing to maintain or improve their transportation technology and export with a higher intensity. Overall, the additional effects of trade on exporter dynamics and transportation technology amplify the welfare gains relative to the most common quantifications in the literature, especially in the medium run.====The idea that trade induces firms to engage in more risky investment to improve their performance as exporters is interesting and sensible. However, confining such improvement to transportation technology may downplay the importance of the proposed mechanism. The following comments are aimed mainly at addressing the empirical relevance of the proposed interpretation of the mechanism and partly at considering the possible interaction between the trade and labor market reforms that took place in Colombia in the same period. In Section 2, I highlight the stylized facts about Colombian exports and exporters that the model aims to theoretically reconcile and I briefly summarize the mechanism proposed in the microeconomic block of the theory. Section 3 focuses on two crucial assumptions made in the model; it proposes a complementary way of presenting the mechanism and proposes ways to evaluate the empirical relevance on the competing interpretations. Section 4 focuses on the broader economic and institutional background characterizing Colombia in the early Nineties and highlights the possibility that other reforms taking place in the same years may have interacted with trade liberalization in shaping firms’ distribution and export dynamics as well as welfare. Section 5 concludes.",Comment on “Trade integration in Colombia: A dynamic general equilibrium study with new exporter dynamics”,https://www.sciencedirect.com/science/article/pii/S0165188920300415,29 February 2020,2020,Research Article,333.0
"Boerma Job,Karabarbounis Loukas","University of Minnesota and FRB of Minneapolis, United States,University of Minnesota, FRB of Minneapolis, NBER, and CEPR, United States","Available online 28 February 2020, Version of Record 13 June 2020.",https://doi.org/10.1016/j.jedc.2020.103885,Cited by (12),"During the past two decades, households experienced increases in their average wages and expenditures alongside with divergent trends in their wages, expenditures, and time allocation. We develop a model with incomplete asset markets and household heterogeneity in market and home technologies and preferences to account for these labor market trends and assess their welfare consequences. Using micro data on expenditures and time use, we identify the sources of heterogeneity across households, document how these sources have changed over time, and perform counterfactual analyses. Given the observed increase in leisure expenditures relative to leisure time and the complementarity of these inputs in leisure technology, we infer a significant increase in the average productivity of time spent on leisure. The increasing productivity of leisure time generates significant welfare gains for the average household and moderates negative welfare effects from the rising dispersion of expenditures and time allocation across households.","Wages and expenditures increased substantially for the average household during the past two decades. At the same time, these gains were not distributed equally across households.==== The purpose of this paper is to develop a tractable framework that allows us to account quantitatively for the drivers of both average and divergent trends in labor market outcomes and to assess their welfare consequences.====Our framework is a general equilibrium model with incomplete asset markets and household heterogeneity in market and home technologies and preferences. Households have access to various home technologies that, following Ghez and Becker (1975), combine expenditures and time as inputs to produce final consumption goods. In the home sector, households are heterogeneous with respect to their preferences across goods and their productivity of time. Home production is not tradeable and storable, meaning that in every instance home production must be consumed, and not insurable, meaning there are no assets that households can purchase to explicitly insure against differences that originate in the home sector. In the market sector, households are also heterogeneous with respect to their productivity. Following the approach of Heathcote et al. (2014), the structure of asset markets allows households to insure against transitory shocks in their market productivity but not against permanent productivity differences.====We apply our framework to married households surveyed by the Consumer Expenditure Survey (CEX) and the American Time Use Survey (ATUS) between 1995 and 2016. We split the home sector into a non-market sector in which expenditures and time are substitutes in production and a leisure sector in which expenditures and time are complements in production. The non-market sector includes expenditures such as food and household services and time uses such as housework and child care. The leisure sector includes expenditures such as telecommunication and entertainment and time uses such as television watching and other recreational activities.====An appealing feature of the framework is the transparency and generality of the identification of the sources of heterogeneity across households. The model retains tractability because it features a no-trade result with respect to certain assets. Therefore, we can characterize the allocations of expenditures and time across sectors in closed form. Following the same approach as in our earlier work (Boerma and Karabarbounis, 2019), we use the analytical solutions to invert the equilibrium allocations and identify the sources of heterogeneity across households that perfectly account for the household-level data in any given point of time. Our exercise is to then shut off particular aspects of the evolution of the sources of heterogeneity over time. This allows us to assess the drivers of trends in sectoral expenditures and time allocation for the average household, the drivers of trends in the dispersion of sectoral expenditures and time allocation across households, and the welfare consequences of these trends.====We reach two main conclusions regarding the sources of heterogeneity that characterize households and their evolution of time. First, we infer that mean productivity of leisure time more than doubles between the beginning and the end of the sample. The key feature of the data leading to this inference is the dramatic increase in leisure expenditures relative to leisure time for the average household. The increase in expenditures relative to time is larger than the one predicted only by the decline in the relative price of leisure goods. Given that expenditures and time are complements in the production of leisure goods, we infer that the productivity of leisure time must have been increasing.====Second, the dispersion of the productivity of non-market and leisure time is larger than the dispersion of market productivity across households. Our inference of large uninsurable differences in home productivity follows from the observation that in the cross-section of households time spent either on the non-market or the leisure sector is weakly correlated with sectoral expenditures and market productivity. As a result, home productivity needs to be significantly dispersed in order to rationalize the variation of these observables. We document that the dispersion of the productivity of time inputs in home production has increased, paralleling the well-known increase in the dispersion of market productivity (wages) over time.====Our counterfactual analyses demonstrate the importance of market and home productivity and prices for the evolution of mean expenditures and market hours. Given the relative stability of market hours over time, the increase in mean market productivity accounts for most of the increase in mean expenditures over time. The increase in the relative price of non-market goods induces households to substitute away from non-market expenditures toward non-market time and the decline in the relative price of leisure goods induces households to complement leisure expenditures with rising leisure time. Changes in relative prices generate roughly 11 log points decline in market hours, with the majority of this decline accounted for by the increase in the relative price of non-market goods. This decline is offset by the rise of market and leisure productivities, which induce households to reallocate hours toward the market sector.====To assess the welfare effects of trends in labor market outcomes, we calculate consumption equivalent changes that arise from both changes in mean consumption and changes in the dispersion of consumption across households. By consumption we mean the final aggregator of the production process that involves aggregating sectoral goods produced with expenditures and time. A novel finding of our paper is to demonstrate that the rise of mean leisure productivity is quantitatively the most important driver of welfare changes over time. The increase in mean leisure productivity generates more than 30 log points increase in mean consumption over time. To put this number in context, the increase in mean market productivity contributes less than 10 log points increase in mean consumption. At the same time, the increase in mean leisure productivity, which affects all households equally, moderates the rise of consumption dispersion across households induced by changes in the variance of market and home productivities over time. The contribution of mean leisure productivity to welfare through the dispersion channel is roughly 10 log points of the consumption equivalent.====It is important to contrast our approach of assessing welfare effects through an equilibrium model to more descriptive approaches on the evolution of the dispersion of expenditures and time inputs.==== Similar to the distinction emphasized by Aguiar and Hurst (2005), in developing our welfare metric we distinguish between expenditures, which serve as an input in the production of final goods, and consumption, which is the result of a production process involving expenditures, time, and productivity. This distinction matters for our conclusions. For example, we find that the increase in the variance of the permanent component of market productivity is the most important factor accounting for the increase in the dispersion of total expenditures over time. However, this factor contributes significantly less to the welfare costs of dispersion once we recognize that these welfare costs are linked more closely to the consumption aggregator than to total expenditures.====We examine trends in labor market outcomes through the lens of a structural model, complementing earlier attempts to measure welfare effects from changes in the dispersion of observables. Attanasio and Davis (1996) is an early study that links the divergence of group wages to the divergence of group expenditures and argues that this departure from full insurance carries significant welfare costs. Heathcote et al. (2013) discuss the merits of structural approaches relative to statistical approaches when calculating welfare effects and estimate that, in response to the observed changes in the structure of wages, the welfare gains in terms of average consumption and leisure dominate the losses arising from increased dispersion. Relative to these papers, our paper incorporates multiple time uses and highlights the primary role of changes in leisure productivity in terms of understanding the welfare effects of recent labor market trends.====An emerging literature examines the role of shifts originating in the leisure sector for labor supply trends. Vandenbroucke (2009) adopts a quantitative Beckerian framework to study the driving forces behind the decline in working hours and their increased concentration over the first half of the 20th century. Accounting for the decline in market hours, he finds a primary role for increasing skilled wages and a limited role for the declining price of leisure goods. Bridgman (2016) develops a model with non-separable preferences that is able to accommodate the rise of average leisure and leisure inequality during the second half of the 20th century and Boppart and Ngai (2019) lay out conditions under which these trends are consistent with a balanced growth path. Like these papers, we are also interested in accounting for the evolution of the allocation of time. An important point of departure from this literature is that we incorporate micro-level data into our analysis of the heterogeneity in labor market trends across households.====Closest to our conclusions, Aguiar et al. (2018) infer a significant increase in the technological progress of recreational time of young men. Their inference comes from the observed increase in recreational computer time in excess of the predicted increase along a leisure demand system. Similar to them, we find a significant increase in leisure productivity over time. Under our maintained assumption that expenditures and time are complements in leisure technology, our inference comes from the observed increase in leisure expenditures relative to time in excess of the increase predicted by the decline in the relative price of leisure goods. Aguiar et al. (2018) do not map changes in leisure productivity to changes in welfare, whereas we uncover significant welfare effects from the rise of mean leisure productivity reflecting both an increase in average consumption and a moderation of consumption inequality.",Labor market trends and the changing value of time,https://www.sciencedirect.com/science/article/pii/S0165188920300543,28 February 2020,2020,Research Article,334.0
Laumer Sebastian,"University of Illinois at Urbana-Champaign, 1407 W Gregory Dr. Urbana, IL 61802, United States","Received 18 August 2019, Revised 4 February 2020, Accepted 18 February 2020, Available online 28 February 2020, Version of Record 4 April 2020.",https://doi.org/10.1016/j.jedc.2020.103868,Cited by (10)," factor-augmented VAR model. I use sign restrictions for identifying effects that both classes of models agree upon. This approach imposes a minimal set of restrictions on the empirical model, leaving the sign and magnitude of the effect of government spending on consumption free to be determined by the data. I find that: (i) government spending increases aggregate consumption; (ii) the estimated spending multiplier is close to 2; (iii) there exists heterogeneity even within durable, nondurable, and service consumption variables which is undocumented in the literature. Finally, I show that my identified structural shocks are not predictable by economic agents and are uncorrelated with traditional ==== shocks, suggesting that these effects are not confounded by ====.","What are the effects of government spending on the economy? How does it affect consumption? Theory does not have a conclusive answer to these questions. While neoclassical models predict that consumption decreases following a government spending increase (e.g., Aiyagari, Christiano, Eichenbaum, 1992, Baxter, King, 1993), new Keynesian models predict the opposite (e.g., Davig, Leeper, 2011, Galí, Vallés, López-Salido, 2007, Ganelli, Tervala, 2009, Zubairy, 2014). Furthermore, Leeper et al. (2017) point out that the sign of the consumption response depends on fine details of the theoretical model. These circumstances hand the question over to empirical research.====However, empirical studies also differ substantially in their conclusions. Within the VAR literature, the key debate concerns identification. Several papers apply the narrative approach developed by Ramey and Shapiro (1998) and Ramey (2011), and typically find a negative consumption response (e.g., Burnside, Eichenbaum, Fisher, 2004, Ramey, Shapiro or Ramey, 2011).==== Other papers use classical recursive identification along the lines of Blanchard and Perotti (2002). These analyses usually suggest a positive consumption response (e.g., Fatás and Mihov, 2001 or Galí et al., 2007). Additional papers that analyze the impact of government spending include Fisher and Peters (2010), Fragetta and Gasteiger (2014) and Ben Zeev and Pappa (2017), all of which find positive consumption responses. Mountford and Uhlig (2009) uncover ambiguous results. In my study, I focus on the consumption response. This focus reflects that consumption is the primary determinant of the size of the spending multiplier, which is the main object of interest in the literature.====My main contribution is to identify government spending shocks under relatively weak identifying assumptions via sign restrictions. This allows me to demonstrate that aggregate consumption increases for over one year after an expansionary government spending shock, consistent with new Keynesian models. I also uncover evidence about the disaggregated sources of this aggregate consumption response. Finally, I find that my structural shock series is not predictable by economic agents and that monetary policy shocks do not contaminate my findings and interpretations.====The empirical government spending literature has largely used identification strategies based on either Blanchard and Perotti (2002) or Ramey (2011). However, both approaches are related to strong identifying assumptions. Blanchard and Perotti (2002) find that government spending is contemporaneously unaffected by the business cycle. This finding can be implemented via a recursive VAR with government spending ordered first. Recursive identification requires one to apply strong zero assumptions for identification that lack theoretical foundations. It is widely known that results may change or vanish once those restrictions are replaced by weaker assumptions (Uhlig, 2005). In addition, Ramey (2011) shows that the resulting VAR shocks are predictable by economic agents whereas unpredictability is a key assumption about macroeconomic structural shocks (Ramey, 2016b).====This leads Ramey (2011) to create a narrative time series, her ====, to quantify agents’ expectations regarding future changes in government spending. This method assumes that the narrative time series is a direct measure of the latent shock series. The macroeconometrics literature has recently recognized that narrative time series, such as Ramey’s news defense shocks, are measured with error and should be viewed as instruments rather than as perfect measures of the latent structural shock series.==== Because Ramey (2011)'s approach accounts for fiscal foresight, it has become the dominating identification strategy in the empirical government spending literature. Fiscal foresight is a limited information problem that occurs because the econometrician cannot observe agents’ expectations about future government spending plans. This misalignment can result into misleading conclusions. The literature assumes that Ramey’s news shocks carry information about agent’s expectations which breaks the misalignment between the information sets.====My study builds on recent advances to address the issues described above. First, the literature argues that traditional VAR models suffer from the fiscal foresight issue and suggests using high-dimensional models with large information sets (Ellahie, Ricco, 2017, Forni, Gambetti). I employ a Bayesian FAVAR model with over 200 variables rather than the maximum ten that can be employed in VAR models. The model’s central assumption is that a small number of factors summarizes the dynamics of the large information set. The literature assumes that a large data set contains information about agents’ expectations that are then captured by the factors of the FAVAR model. Thus, high-dimensional models represent another way to break the misalignment between the information sets of economic agents and the econometrician, and overcomes fiscal foresight (Forni and Gambetti, 2010). I also include Ramey’s news shocks to control for information about agents’ expectations that are not already captured by the data. The FAVAR model also allows me to study the responses of disaggregated consumption variables which uncovers how government spending actually affects the economy.====Second, I employ traditional sign restrictions for identification. Traditional sign restrictions only restrict the sign of a subset of impulse response functions which represent the set of ==== predictions of theoretical models, and therefore, have strong theoretical foundations. I define a government spending shock as a shock that drives up output, employment, prices, government tax receipts, the government deficit and government spending. These are all joint predictions of neoclassical and new Keynesian models. In contrast, the responses of key variables are typically unrestricted. Thus, I do not impose any restrictions on the response of consumption and leave it free to be determined by the data. Traditional sign restrictions are less restrictive than the recursive or the narrative identification strategy and assign maximum weight to the data. This lets the data decide which theoretical predictions have more empirical support. The set of traditional sign restrictions is similar to that used in Forni and Gambetti (2010), except that I do not impose a sign restriction on interest rates. While there is no theoretical foundation supporting this sign restriction, I find that my results are not confounded by monetary policy shocks even without this additional sign restriction. Consequently, I leave interest rates unrestricted.====Third, I add narrative sign restrictions developed by Antolin-Diaz and Rubio-Ramírez (2018) to the set of identifying assumptions. Narrative sign restrictions differ from the narrative identification à la Ramey (2011) in the sense that they only restrict the sign of the latent shock series at particular events without imposing assumptions at any other points in time. In contrast, the narrative identification assumes that there exists a time series that is perfectly correlated with the latent shock series and is then interpreted as the shock series itself. I restrict the structural shock series to be positive at 1965Q1, 1980Q1, and 2001Q3. These dates have been identified by the literature as dates at which U.S. military spending increased surprisingly (Ramey, 2011, Ramey, Shapiro). The narrative sign restrictions help provide evidence that my identified structural shock series is not predictable by economic agents.====FAVAR models are used to study monetary policy (Bernanke et al., 2005), the U.S. housing market (Ng and Moench, 2010), fiscal policy (Fragetta and Gasteiger, 2014), global commodity prices (Lombardi et al., 2012) and unconventional monetary policy (Fiorelli and Meliciani, 2019). Sign restrictions on impulse response functions have been applied to analyze monetary policy (Uhlig, 2005), exchange rates (Scholl and Uhlig, 2008), fiscal policy (Mountford and Uhlig, 2009), oil prices (Baumeister, Peersman, 2013, Kilian, Murphy, 2012, Kilian, Murphy, 2014), optimism shocks (Beaudry et al., 2018) and uncertainty shocks (Shin and Zhong, 2018).==== Mumtaz and Surico (2009), and Amir Ahmadi and Uhlig (2015) combine the FAVAR model with sign restrictions on impulse response functions to study the effects of monetary policy shocks in a high-dimensional environment.====My analysis allows me to draw economically meaningful conclusions about government spending shocks without imposing strong and questionable identifying restrictions. First, I estimate the FAVAR model with aggregate consumption and 213 non-consumption variables. I find that aggregate consumption increases for more than one year after the shock and that the resulting spending multiplier exceeds one. These results are in line with new Keynesian models. However, my estimation also shows that the response of hourly earnings is mostly inconclusive. As a result, the sign restriction approach cannot fully rule out neoclassical predictions. The estimated long-run multiplier is close to two, which is toward the upper bound of what the VAR literature proposes. Comparing multiplier estimates from the FAVAR and traditional VAR models using different identification strategies, I find that multipliers are higher using sign restrictions than those from using the identification strategies of Blanchard and Perotti (2002) and Ramey (2011).====Next, I re-estimate the model, replacing aggregate consumption by 15 disaggregated consumption variables. This set represents the most disaggregated consumption variables available for quarterly frequency. The analysis reveals heterogeneous responses to an expansionary government spending shock, even within durable, nondurable, and service consumption variables. Households primarily increase consumption of services and a subset of durable goods. In contrast, the effect on nondurable goods is unclear. This heterogeneity has been previously undiscovered because the literature has only looked at the responses of aggregate, durable, nondurable, and service consumption. Moreover, this heterogeneity remains hidden if the researcher uses Ramey’s narrative approach.====Ramey (2011) argues that the consumption increase after a government spending shock following Blanchard and Perotti (2002)'s approach is just the recovery after an immediate drop that is not observed because the resulting VAR shocks occur several quarters too late and are predictable by economic agents. I conduct a Granger causality test to provide evidence that my identified structural shock series is not predictable by economic agents. For all except one specification, the pointwise median of the identified structural shock series is not Granger caused by professional forecasts. Using all draws of the structural shock series identified via traditional sign restrictions only, the share of Granger causality tests that fail to reject the null of non-Granger causality is between 75% and 87%. If I add the narrative sign restrictions, the share increases to 77% – 100%, thereby addressing (Ramey, 2011)’s critique. Finally, I compute the correlation between the structural shock series of my model and proxy variables that are used in the literature to identify monetary policy shocks. I find that the correlation is nearly zero. This indicates that monetary policy shocks do not contaminate my findings and interpretations.====The paper proceeds as follows. Section 2 introduces the FAVAR model. Section 3 explains the identification strategy and justifies the identifying restrictions. Section 4 describes the estimation procedure and data. Section 5 presents the results. Section 6 concludes.",Government spending and heterogeneous consumption dynamics,https://www.sciencedirect.com/science/article/pii/S0165188920300373,28 February 2020,2020,Research Article,335.0
Luetticke Ralph,"University College London, CEPR and CfM, England","Available online 28 February 2020, Version of Record 13 June 2020.",https://doi.org/10.1016/j.jedc.2020.103880,Cited by (0),None,None,Comment on “The Household Channel of Monetary Policy in the Euro Area: A Back of the Envelope Calculation”,https://www.sciencedirect.com/science/article/pii/S016518892030049X,28 February 2020,2020,Research Article,336.0
Nakajima Makoto,"Research Department, Federal Reserve Bank of Philadelphia, Ten Independence Mall, Philadelphia, PA 19106-1574, United States","Available online 27 February 2020, Version of Record 13 June 2020.",https://doi.org/10.1016/j.jedc.2020.103883,Cited by (8),"This paper quantitatively investigates capital income taxation in the general-equilibrium overlapping generations model with household heterogeneity and housing. Housing ==== rate is close to zero (1%), contrary to the high optimal ==== rate found with overlapping generations models without housing (31%). A low capital income tax rate improves welfare by narrowing a tax wedge between housing and non-housing capital; the narrowed tax wedge indirectly nullifies the subsidies (taxes) for homeowners (renters) and corrects over-investment to housing. Naturally, when the preferential tax treatment for owner-occupied housing is eliminated, a high capital income tax rate improves welfare as in the model without housing.","Whether the government should tax capital income in the long run has been an important question and one that has been answered under a variety of assumptions. Chamley (1986) and Judd (1985) argue that the government should not tax capital income, using a model with an infinitely lived representative agent.==== On the other hand, the optimal capital income tax rate is known to be different from zero in overlapping generations models. In particular, a recent study by Conesa et al. (2009) shows quantitatively that the optimal capital income tax rate is not only non-zero but also ====, using a calibrated overlapping generations model. What is missing in the discussion on optimal capital income taxation is housing, which consists of about one-third of the total capital of the U.S. economy and is the biggest single asset for the majority of U.S. households. Not only is housing large, but it is also different from non-housing capital and taxed very differently. The purpose of the paper is to revisit the optimality of capital income taxation, taking into account the unique characteristics of housing and housing tax policy.====How is housing different from non-housing capital? Notable differences are: (i) housing is held for the dual purpose of consumption and savings, (ii) housing can be either owned or rented, (iii) if owned, housing can be used as collateral for mortgage loans, and (iv) income from housing is taxed differently from non-housing capital income. In particular, in the U.S. there are two policies that favor housing, especially owner-occupied housing. First, imputed rents on owner-occupied housing are tax exempt. Second, the mortgage interest payment can be deducted from taxable income up to a certain limit. There are studies that investigate the implications of such housing tax policy, but mostly without a quantitative macroeconomic model. This paper is intended to bridge the gap between the literature on macroeconomic public finance, which typically ignores housing capital, and that on housing policy, where the quantitative general equilibrium model is rarely used.====In the U.S. and many other countries, owner-occupied housing enjoys various forms of implicit and explicit subsidies that non-housing capital does not enjoy. Rosen (1985) offers a good summary of the literature analyzing the effects of the government’s policy toward housing. However, analysis of housing taxation in a realistically calibrated general equilibrium model started to appear only recently. The pioneer work is Gervais (2002). He analyzes such welfare gains from eliminating the preferential tax treatment for owner-occupied housing, using a calibrated overlapping generations model. Díaz and Luengo-Prado (2008) study the effect of the preferential tax treatment of owner-occupied housing on homeownership. This paper will not provide a positive theory of housing taxation. Instead, housing tax policy is taken as given, and optimal capital income taxation conditional on different housing policies is explored.====I employ the Ramsey approach to the optimal taxation problem in a limited sense. In this approach, the size of government expenditures in every period is exogenously given, a set of available distortionary tax instruments is assumed, and the optimal tax system within the set is explored. For the baseline experiment, I assume (i) the preferential tax treatment for owner-occupied housing that is present in the U.S., (ii) progressive labor income taxation, with the progressivity mimicking that of the U.S. federal income tax, and (iii) proportional capital income taxation. Under these assumptions, the optimal level of the capital income tax rate is investigated while maintaining revenue neutrality. The assumption of the proportionality of the capital income tax is due to computational feasibility, but Conesa et al. (2009) find that the optimal tax system does not include progressive capital income tax in their model without housing.====There are three main findings. First, the optimal capital income tax rate is close to zero even in the life-cycle model, given the preferential tax treatment for owner-occupied housing. In the baseline experiment, the optimal capital income tax rate is found to be 1%. This is very different from 31%, which is obtained in the standard model without housing. The intuition is simple. When the imputed rents on owner-occupied housing are tax-exempt by assumption, lowering the capital income tax rate is equivalent to narrowing the tax wedge between housing and non-housing capital. There are two consequences. First, the narrowed tax wedge nullifies the subsidies to homeowners, who are typically higher earners, and taxes to renters, who are typically lower earners. Second, the narrowed tax wedge corrects the over-investment in housing capital. The numerical result shows that this simple intuition is actually very important in shaping the optimal capital income taxation. Second, when the preferential tax treatment for owner-occupied housing is eliminated, it becomes optimal to tax capital at a high rate again, as in the standard model without housing. In the baseline experiment, the optimal capital income tax rate is found to be 24%. When the tax wedge is eliminated by assumption, lowering the capital income tax rate no longer works to nullify the preferential tax treatment of owner-occupied housing. The two results above taken together suggest that housing tax/subsidy policy has a substantial effect on how capital income should be taxed. In other words, taxation of housing and non-housing capital should be considered as a package, because of the interaction between the two. Third, in either of the two cases discussed above, the welfare gain from moving from the baseline economy to the one with the optimal capital income tax rate is sizable: 1.2% of additional per-period consumption when the preferential tax treatment for owner-occupied housing is preserved, and 1.6% when the preferential tax treatment is eliminated. Consequently, implementing a high capital income tax rate, which is optimal in the model without housing, in the model with housing incurs a severe welfare loss.====To the best of my knowledge, Eerola and Maattanen (2013) are the only ones who study the optimal capital and housing taxation in a macroeconomic model. In particular, they investigate optimal housing taxation in the standard growth model with housing and non-housing capital. Using the standard Ramsey approach, they find that it is optimal to tax housing and non-housing capital at the same rate and close the tax wedge. It implies that, in the long-run, where it is optimal to have zero capital income tax as in Chamley–Judd, it is also optimal not to tax housing. Important differences from this paper are that my model features tenure decision between owning and renting, which is affected by preferential tax treatment of owner-occupied housing, market incompleteness that implies heterogeneity of households in various dimensions, and life cycle. The life-cycle aspect is especially important because Conesa et al. (2009) find that, in the model that features the life cycle, it is optimal to heavily tax non-housing capital. Relatedly, Erosa and Gervais (2002) and Garriga (2019) theoretically show that the optimal capital income tax rate is ====. Moreover, I find that the tenure decision is crucial for the main results of the paper, as the capital income tax indirectly redistributes income between homeowners and renters.====Implications of market incompleteness to optimal capital income taxation have also been studied. Aiyagari (1995) argues that, in the presence of market incompleteness, the optimal capital income tax is not zero in the long run. In the economy with uninsured idiosyncratic shocks to earnings, agents have a precautionary savings motive, which pushes the aggregate savings above the efficient level in the complete markets model. A positive capital income tax can fix the over-accumulation of assets by countering the incentive to hold precautionary savings. Domeij and Heathcote (2004) build on the model used by Aiyagari (1995) and investigate optimal capital income taxation in the model, which features a realistic degree of the wealth inequality due to market incompleteness. They find that, taking into account the welfare loss during the transition, implementing a zero capital income tax generates a welfare loss. According to their baseline experiment, the optimal capital income tax rate is 39.7%. However, the long-run optimal capital income tax rate without consideration of the cost of transition is still zero. Fuster et al. (2008) study how the strength of altruism affects the welfare gain from various tax reforms.====The model developed in the current paper is built on the literature that develops general-equilibrium models with uninsured idiosyncratic shocks. The classic papers are Aiyagari (1994) and Huggett (1996). The pioneer papers that introduce housing or durable assets into the standard general-equilibrium model with uninsured idiosyncratic uncertainty are Gervais (2002), Fernández-Villaverde and Krueger (2011), Díaz and Luengo-Prado (2010), Nakajima (2005), and Chambers et al. (2009a).====The rest of the paper is organized as follows. Section 2 sets up the model and Section 3 describes how the model is calibrated and numerically solved. Some of the details of calibration are found in Appendices A.1 and A.2. Appendix A.3 gives further details of the computational methods. The properties of the baseline model economy with housing are studied in Section 4. In Section 5, the methodology for counterfactual experiments is explained. Appendix A.4 provides some details about the welfare criteria used here. Section 6 presents the main results of the paper. Section 7 investigates the role of housing in shaping the main results. Section 8 extends the baseline model in three ways. Additional robustness analyses are offered in Section 9. Section 10 concludes.",Capital income taxation with housing,https://www.sciencedirect.com/science/article/pii/S016518892030052X,27 February 2020,2020,Research Article,337.0
Caggese Andrea,"UPF, CREI and Barcelona GSE, Spain","Available online 26 February 2020, Version of Record 13 June 2020.",https://doi.org/10.1016/j.jedc.2020.103874,Cited by (2),None,None,Comments on: “What drives aggregate investment? Evidence from German survey data”,https://www.sciencedirect.com/science/article/pii/S0165188920300439,26 February 2020,2020,Research Article,338.0
Ferriere Axelle,"Paris School of Economics, France","Available online 26 February 2020, Version of Record 13 June 2020.",https://doi.org/10.1016/j.jedc.2020.103884,Cited by (0),None,None,Comments on “Capital income taxation with housing”,https://www.sciencedirect.com/science/article/pii/S0165188920300531,26 February 2020,2020,Research Article,339.0
"Corbae Dean,D’Erasmo Pablo","Department of Economics, University of Wisconsin - Madison, United States,NBER, United States,Federal Reserve Bank of Philadelphia, United States","Available online 26 February 2020, Version of Record 13 June 2020.",https://doi.org/10.1016/j.jedc.2020.103877,Cited by (12), equilibrium model in which banks endogenously climb a funding base ladder. Rising concentration occurs along a transition path between two steady states after branching costs decline.,"Concentration of insured deposit funding in the U.S. has risen from 15% in 1984 to 44% in 2018 (when measured as the fraction of deposits held at the top 4 banks relative to deposits held at all U.S. bank holding companies), a roughly three-fold increase. Regulation has often been attributed as a factor in that increase. The Riegle-Neal Interstate Banking and Branching Efficiency Act of 1994 removed many of the restrictions on opening bank branches across state lines imposed under the McFadden Act of 1927 and other laws that attempted to address long-standing concerns about the concentration of financial activity. We interpret the Riegle-Neal act as lowering the cost of expanding a bank’s funding base. More generally, the methods in our paper can be applied to any government regulation that loosens or restricts the size of banks.====In this paper, we build an industry equilibrium model in which banks endogenously climb a funding base ladder along the lines of Ericson and Pakes (1995).==== Specifically, a bank can raise the mean size of its deposits at a cost. Idiosyncratic shocks to its deposits and to the success of its loan portfolio lead to an endogenous size distribution of banks. The specific method of (deposit) capacity accumulation we apply is due to Besanko and Doraszelski (2004).====Our paper is related to the delegated monitoring model of Diamond (1984). Diamond provides a framework where large banks arise to economize on the fixed costs of monitoring individual borrowers more efficiently than a large number of small depositors. Economies of scale in monitoring (decreasing average costs) induce size. The problem of monitoring the monitor is also solved by size; large diversified banks can offer non-contingent (and hence incentive compatible) deposit contracts. There are numerous empirical papers documenting the existence of scale economies in banking such as Berger and Hannan (1998) or Berger and Mester (1997).==== A large pool of depositors is also consistent with geographic diversification as described in Liang and Rhoades (1988).====Quantitative models of imperfect competition in the deposit market have been offered by Aguirregabiria et al. (2016), Egan et al. (2017), Drechsler et al. (2017) and Corbae and Levine (2019). In this paper we focus on imperfect competition in the loan market as in Corbae and D’Erasmo (2019).==== To solve the model, we use the computational methods in Weintraub et al. (2008). In particular, the approximation methods allow for there to be strategically important (dominant) banks. We think of rising concentration as occurring along a transition path between two steady states following a decline in branching costs.====The paper is organized as follows. Section 2 describes some aspects of the data that motivate our paper. Section 3 describes our environment while Section 4 describes the Markov Perfect Equilibrium of our model. Section 5 describes the parameterization of the model. Section 6 describes equilibrium properties of the model. Finally, Section 7 describes our main experiment; we lower the cost of expanding a regional bank’s funding base due, for example, to a regulatory change like Riegle-Neal and quantify the change in bank concentration. Section 8 provides directions for future research.",Rising bank concentration,https://www.sciencedirect.com/science/article/pii/S0165188920300464,26 February 2020,2020,Research Article,340.0
"Bachmann Rüdiger,Zorn Peter","University of Notre Dame, USA,LMU Munich, Germany,CEPR, Germany,CESifo, Germany,ifo, Germany","Available online 26 February 2020, Version of Record 13 June 2020.",https://doi.org/10.1016/j.jedc.2020.103873,Cited by (15)," (as much as approximately two thirds in both cases) is explained by aggregate demand shocks. Consistent with neoclassical views, however, technological factors are the most important investment determinant ====.","What drives aggregate fluctuations? This is a seminal question in macroeconomics (see Kocherlakota, 2009), to which a number of approaches have been brought to bear: structural vector autoregressions (to cite only a few seminal papers: Blanchard, Perotti, 2002, Blanchard, Quah, 1989, Christiano, Eichenbaum, Evans, 1996); dynamic factor models (e.g., Stock and Watson, 2012); estimated DSGE models (e.g., Smets and Wouters, 2007); business cycle accounting (e.g., Chari et al., 2007); and narrative approaches to identifying macroeconomic policy shocks, possibly in conjunction with structural vector autoregressions (see, for example, Mertens, Ravn, 2013, Romer, Romer, 2004, Romer, Romer, 2010).====This paper pursues a novel and complementary approach, using unique firm-level data from the Munich-based ifo Investment Survey (IS). The IS asks decision makers in German manufacturing firms about the importance of sales, technological factors, finance, return expectations, and macroeconomic policy as determinants for their investment activity in a given year. We propose a survey-based, narrative approach that estimates shocks from this subjective reasons data, including shocks potentially unrelated to macroeconomic policy. The approach in the literature closest in spirit to ours is Bewley (1999), who interviews managers in firms to investigate the sources of downward nominal wage rigidity. Using the firm-level survey responses, we first construct index measures of the importance of each investment determinant in the manufacturing sector (henceforth called aggregate investment determinant indices). In a second step, we recover orthogonal aggregate shocks from these index series. We finally compute the contributions of the identified aggregate shocks to the fluctuations of aggregate manufacturing investment growth (henceforth aggregate investment growth).====We argue that, in Germany, aggregate demand shocks and aggregate technology shocks shocks are the only plausible candidates for explaining aggregate investment fluctuations,==== and make them therefore the main focus of our strategy to identify orthogonal aggregate shocks from the narrative determinant series. To disentangle these two shocks in the data, we impose correlation restrictions that rely on the informational content of the narrative series and simple economic theory.==== Specifically, we require that the identified aggregate demand shocks are highly correlated with the sales investment determinant, and likewise for the identified aggregate technology shocks and the technological factors investment determinant. In addition, in many standard models aggregate demand shocks and aggregate technology shocks have different implications for prices. In particular, producer price inflation should be positively correlated with aggregate demand shocks while its correlation with technology shocks should be negative, and we impose these implications for identification.====We find that, consistent with neoclassical views, on average over time technological factors are the most important investment determinant. This is not the case for the ==== of aggregate investment growth (and also of aggregate output growth). First, a considerable portion, 81 percent, of the total variance of aggregate investment growth can be explained by our survey-based subjective investment determinants and the orthogonal shocks extracted from them. This result shows the high quality of these survey data, and, more generally, lends credence to our survey-based narrative approach. Second, in our baseline identification scheme, we find that approximately two thirds of the total variance of aggregate investment growth can be explained through aggregate demand shocks. Aggregate technology shocks play only a minor role.==== This finding is also confirmed in a counterfactual simulation of the post-reunification German aggregate investment rate: Almost none of the officially declared recession years for Germany would have experienced negative investment growth without the aggregate demand shocks, in contrast to the data. The reunification boom would not exist, neither would the prolonged slump in the early 2000s. There would have been no investment boom nor a rebound around the Great Recession. In other words, without aggregate demand shocks, the aggregate investment growth rate series would not align with the German post-reunification business cycle. Aggregate demand shocks are thus essential for our understanding of this business cycle.====What are these aggregate demand shocks that we identify as determining the bulk of aggregate investment fluctuations? In this paper, we provide a negative and a positive answer to this question.====First, using industry-specific capital expenditure and subjective investment determinant data and input-output matrices for the German manufacturing sector, we argue that our identified aggregate demand shocks are unlikely to be the result of misclassification of industry-specific technology shocks that spill over to other industries where they are merely perceived as demand shocks: For instance, one small manufacturing industry which procures large amounts of input goods from other upstream manufacturing industries could have a positive technology shock followed by increased input demand for other manufacturing industries, which would lead to higher demand-related investment there. Alternatively, consider a large manufacturing industry that makes an invention that is sold to downstream industries. It might classify a technological shock as increased demand for its products. We investigate both channels and conclude that none appears quantitatively relevant. Moreover, an industry-level analysis shows that in most manufacturing industries the aggregate pattern – demand shocks explain the bulk of investment growth volatility – replicates itself at the more disaggregate level, that is, our aggregate results are not a mere composition effect.====Second, we use manufacturing business sentiment data to provide suggestive evidence that our identified aggregate demand shocks, which correlate well with business sentiment, might be good candidates for sentiment or animal spirit shocks. By contrast, with the one and clear exception of the post-reunification monetary tightening in Germany by the Bundesbank and the immediate subsequent recovery, no obvious demand stabilization policy instrument has any association with our identified aggregate demand shocks.====Traditionally, there has been a branch of economics methodology that viewed subjective survey approaches, that is, asking economic agents what they did, what they expect, and why they did something, with some scepticism. Nevertheless, a growing number of economists has now made use of subjective survey data, mainly to study expectation formation and their rationality: Nerlove (1983), using business surveys from various countries, is a very early example; Bachmann and Elstner (2015) and Gennaioli et al. (2015) are more recent ones. Guiso and Parigi (1999), Bachmann et al. (2013) and Bachmann et al. (2017) have used expectation data from business surveys to study the impact of business uncertainty on economic activity. On the household side, Carroll and Dunn (1997), Souleles (2004), Bachmann et al. (2015) and Malmendier and Nagel (2016) are important examples, each with different research questions.====We view our approach as pushing one step further: If we can ask economic agents about their subjective expectations and gain useful economic insights, why not ask them about their subjective reasons for carrying out a particular economic action and use the answers for economic analysis?==== The advantage of our survey-based approach towards identifying shocks lies in this putative directness: The survey respondents report whether their investment activity in a given year was influenced by, for instance, technological factors, and, if so, how strongly. As a result, the narrative series constructed from the survey responses and used in shock identification are less prone to be confounded by other factors. In this regard, our approach is similar to other narrative methodologies that have been used in empirical macroeconomics to study the effects of macroeconomic policies (see Romer, Romer, 2004, Romer, Romer, 2010). Indeed, before we use the reasons data for shock extraction, one of the contributions of this paper is to show extensively that the investment determinants that we base our narrative approach on have highly plausible economic content: the sales investment determinant is highly correlated with new manufacturing orders; the technological investment determinant is correlated with the prevalence of restructuring and rationalization investment, investment with the purpose of cutting costs and making production more efficient, as well as process innovations carried out at the firm level; and the finance investment determinant is related to independent measures of external finance dependence at the firm level, and, in the time series, to credit spreads and proxies of idiosyncratic business uncertainty. Finally, we show that firms for which the sales determinant is important for their investment behavior, are more likely to increase prices and less likely to decrease them; and vice versa for the technological determinant.====The remainder of this paper is organized as follows. Section 2 introduces the survey data and presents the aggregate investment determinant indices. We also validate in great detail the survey data against proxy variables for aggregate demand, technology, etc. Section 3 lays out the empirical model for estimating the contribution of the aggregate investment determinants to aggregate investment growth fluctuations, and motivates the identifying assumptions. Section 4 presents the results, both for the manufacturing sector and disaggregated at the two-digit industry level and by German states. Section 5 summarizes the main findings and concludes.",What drives aggregate investment? Evidence from German survey data,https://www.sciencedirect.com/science/article/pii/S0165188920300427,26 February 2020,2020,Research Article,341.0
Den Haan Wouter J.,"Centre for Macroeconomics, London School of Economics and Political Science, Houghton Street, London WC2A 2AE, UK and CEPR, London","Available online 26 February 2020, Version of Record 13 June 2020.",https://doi.org/10.1016/j.jedc.2020.103882,Cited by (5),The techniques proposed in Papp and Reiter (2020) allow the use of cross-sectional and aggregate data observed at different frequencies in the estimation of dynamic stochastic ,"During the nineties, the first algorithms were developed to solve models with heterogeneous agents and aggregate uncertainty. There are now several algorithms.==== The method proposed in Reiter (2009) stands out in being much faster than other algorithms. As discussed below, the “Reiter approach” is not suitable for all models, but if it is then it comes with a massive computational advantage. Papp and Reiter (2020) extend the Reiter approach to make it suitable for estimation when cross-sectional information is not available at the same frequency as aggregate data, a situation that is quite typical.====In Section 2, I describe the Reiter method in the most straightforward way. This may be of some educational value, since the Reiter method is often combined with additional bells and whistles which obscure its intrinsic ability to obtain a numerical solution at low computational cost. This detailed discussion will make it easy to illustrate a weakness of the Reiter method and that is dealing with occasionally binding constraints. In the presence of occasionally binding constraints, it is important for the Reiter method – and more so than for other perturbation methods – that the fluctuations in aggregate uncertainty are really small. This weakness can be easily overcome by using penalty functions instead of inequality constraints, which often are actually more realistic than inequality constraints.====In Section 3, I discuss Papp and Reiter (2020) in detail and I will refer to my discussion in Section 2 and reiterate that the complexity can be reduced if penalty functions are used instead of inequality constraints.====The last section touches upon a more fundamental issue. Maximum Likelihood (ML), a full-information estimation method, is used by Papp and Reiter (2020) to estimate the model. ML and especially its Bayesian version are by far the most popular estimation procedures for stochastic dynamic macroeconomic models. Are these methods the right ones when we know that macroeconomic models are misspecified in at least some nontrivial dimensions? And the question should be asked whether the sophisticated extensions proposed in Papp and Reiter (2020) alleviate this fundamental problem or make it worse? That is, are computational techniques to solve models perhaps ahead of available empirical methodologies?",Discussion of “Estimating linearized heterogeneous agent models using panel data”,https://www.sciencedirect.com/science/article/pii/S0165188920300518,26 February 2020,2020,Research Article,342.0
Mankart Jochen,"Deutsche Bundesbank, Germany","Available online 26 February 2020, Version of Record 13 June 2020.",https://doi.org/10.1016/j.jedc.2020.103878,Cited by (0),None,None,Comments on “Rising Bank Concentration”,https://www.sciencedirect.com/science/article/pii/S0165188920300476,26 February 2020,2020,Research Article,343.0
Nguyen Thuy Lan,"Department of Economics, Santa Clara University, 500 El Camino Real, Santa Clara, CA 95053, United States","Available online 26 February 2020, Version of Record 13 June 2020.",https://doi.org/10.1016/j.jedc.2020.103876,Cited by (0),None,None,Comments on “Regional Data in Macroeconomics: Advice for Practitioners”,https://www.sciencedirect.com/science/article/pii/S0165188920300452,26 February 2020,2020,Research Article,344.0
Oldham Matthew,"Department of Computational and Data Sciences, George Mason University, 4400 University Drive, Fairfax, VA 22030, USA","Received 19 August 2019, Revised 12 February 2020, Accepted 13 February 2020, Available online 24 February 2020, Version of Record 6 March 2020.",https://doi.org/10.1016/j.jedc.2020.103864,Cited by (2),"Despite considerable efforts, the determinants of firm growth and financial market volatility have not been definitively identified, yet sets of stylized facts – most notably power-law distributions – relating to firm size and market returns suggest both evolve as part of a complex system. This scenario implies that a positive feedback loop between firms and investors exists and may be responsible for prejudicing the way management allocates their resources, with firm, and economic, growth adversely affected. Further, there are growing real-world concerns that the management of publicly listed firms is becoming too concerned with the movement of their firm's share price, which is adversely influencing resource allocation decisions. A related concern is that agents, in general, within financial markets are placing a disproportionate focus on short-term factors. To investigate the ramifications of the proposed feedback loop on firm growth and market volatility, this paper implements a novel agent-based artificial stock market where management can consider the movements of their firm's endogenously determined share price when allocating resources between sales and margin growth. The results highlight an inferior outcome regarding firm growth, and various other financial metrics, if management is overly concerned with share price movements. The growth of the firms (and market) is also affected by the mixture of the investor classes initiated due to the divergent levels of volatility they create. Additionally, the model presents insights into how and why the extent to which agents consider past outcomes in their decision-making process becomes influential. Notably, the model's results emulate an extensive set of global micro-level firm data. By providing significant insights on the effects of the stock market on management decision-making and its ramifications for firm growth, this paper provides crucial insights into the mechanisms responsible for inefficient behavior by market participants."," (Dimon and Buffett, 2018)====The statement from financial market heavyweights Jamie Dimon (Chairman and Chief Executive Officer of J.P. Morgan) and Warren Buffett (Chairman and Chief Executive Officer of Berkshire Hathaway) implies that there are serious concerns relating to the operational efficiency of global secondary equity markets. Policymakers and (most) market participants agree that financial markets should operate efficiently because they are an integral part of the modern economy, ensuring that firms can raise capital and investors can diversify their risks widely (Stiglitz, 1989). Further, the efficient pricing of assets is seen as a desirable outcome, as price efficiency guides real decisions (Bond et al., 2011). For example, stock market returns are a significant leading indicator of investment expenditure (an example of a real decision) by publicly listed companies, with the effects flowing into the broader economy (Barro, 1990). Therefore, if markets do not operate efficiently for an extended period, the economy will likely experience an over- or under-allocation of resources to particular sectors, with economic growth affected.====Efficient financial markets are meant to ensure the efficient allocation of resources because management is rewarded by investors for only implementing long-term profit maximizing strategies (Jensen and Meckling, 1976). However, per Dimon and Buffett (2018) short-termism – the behavior of allocating a disproportionate weight to short-term factors over long-term factors (Haldane, 2011) – on behalf of all firms and investors is disrupting the process and generating excessive volatility in financial markets (Davies et al., 2014). Given the shortfalls of existing economic theories to explain the dynamics in which short-termism affects financial market agents and the reasons that short-termism is able to persist, alternative theories are required. As such, it has become apparent the consideration of financial markets as an ecosystem populated with heterogeneous interacting agents (HIA) is now an appropriate approach. Kirman (1992) and Delli Gatti et al., 2005 present the rationale for utilizing an HIA approach in favor of those informed by the rational representative economic agent framework, the underlying basis of efficient markets.====From the various research possibilities related to short-termism, this paper implements a novel HIA model to identify whether, and why, the time horizon employed by financial market agents distorts the growth of firms. Furthermore, the model assesses how this dynamic may affect the behavior of financial markets in general. Consistent with the work of Chiarella et al. (2007), the model is populated with a mixture (either fundamental and/or trend) of investors who invest across multiple firms – an approach that extends the traditional agent-based artificial stock market approach of utilizing a single risky asset. The model's focus, and innovative step, is having management utilize the price signals from the stock market in their decision-making, which in turn affects the future earnings of the firms; thereby, the earnings mechanism becomes endogenous. By internalizing the earnings process of the firms it better reflects real-world markets and adds to the work of Dawid et al. (2019) in combining a financial market model with an industry model. Crucially, the paper introduces a set of global micro-level firm empirical facts (see Section 3) which are utilized to assess the fidelity of the model and its approach. The data provides significant insights into the characteristics of firm size, growth, and performance.====The model's results highlight an inferior outcome regarding firm growth, and various other financial metrics, if management is overly concerned with its firm's share price movements and too impatient in achieving internal growth expectation. A positive feedback loop fed by management trying to meet the expectations of their investors is identified as a likely source of this outcome. The growth of the firms (and market) is also affected by the investor class utilized as they are responsible for generating differing levels of volatility. Additionally, the model presents insights into how and why the extent to which agents consider past outcomes in their decision-making process becomes influential.====The remainder of this paper is laid out in the following manner: Section 2 expands upon the theoretical foundations of the paper; Section 3 explores the empirical facts used to inform the model; Section 4 contains the details of the models underlying this paper; Section 5 details the specifics of the implemented model, with the results presented in Section 6; Section 7 provides a discussion and concluding comments.",Quantifying the concerns of Dimon and Buffett with data and computation,https://www.sciencedirect.com/science/article/pii/S0165188920300336,24 February 2020,2020,Research Article,345.0
"Pancrazi Roberto,Seoane Hernán D.,Vukotić Marija","Department of Economics, University of Warwick, Coventry CV4 7AL, United Kingdom,Department of Economics, Universidad Carlos III de Madrid, Calle Madrid 126, Madrid, 28903 Getafe, Spain","Received 13 May 2019, Revised 17 December 2019, Accepted 18 February 2020, Available online 24 February 2020, Version of Record 14 March 2020.",https://doi.org/10.1016/j.jedc.2020.103867,Cited by (5)," that comes with imposed debt limits. The government endogenously asks for the bailout during recessions and repays it when the economy recovers. Hence, the bailout acts as an imperfect state contingent asset that makes the economy better off. The bailout duration is endogenous and increases with its size. The bailout size creates non-trivial tradeoffs between receiving a larger amount of relatively cheap resources precisely in times of need on the one hand, and facing longer-lasting financial constraints and accumulated interest payments, on the other hand. We characterize and quantify these tradeoffs and document that welfare gains of ==== are hump-shaped in the size of bailout loans.","In this paper we analyze the role of bailouts in economies exposed to sovereign default risk. When a government of a small open economy requests a bailout, it receives a one-time loan of size ==== from a third-party, an international financial institution (IFI, hencheforth). The loan comes with conditions attached to it: the IFI provides loans below the market rate but imposes debt limits on the government. The economy remains in the program until the loan is fully repaid. We address two main questions. First, what are the tradeoffs induced by these bailout policies and how do they vary with the exogenous bailout size, ====? Second, how do they affect welfare gains of bailouts?====To answer these questions, we develop a quantitative small-open economy model with endogenous default decision, financial frictions, and a non-trivial endogenous bailout choice. We first augment the standard endogenous default decision as in Arellano (2008) with an endogenous bailout decision. Specifically, the government decides to enter in the bailout program by requesting a loan from an IFI. The loan is of a fixed size, ====, and is non-defaultable. While in the program, the country pays a below-market interest rate in each period and faces borrowing restrictions. The restrictions reflect a fiscal conditionality clause imposed on its debt limit, i.e. while the country can still borrow from the market, it cannot increase its debt position. In order to exit the bailout program, the country has to fully repay the loan. Notice that while the country can default on its outstanding external debt, we assume that bailout loans are perfectly enforceable.==== Hence, in our setting, the bailout is equivalent to a long-term contract, in which the borrower receives all the resources ==== at the time she requests the loan, and repays an interest for the duration of the contract; the length of the contract is endogenous, as it depends on when the borrower fully repays the IFI loan.====Let us describe the implications of the existence of the bailout option. There are several forces at play. First, during crisis periods, when borrowing on the market is costly, the government asks for a bailout as it relaxes its budget constraints and allows the government to honor its debt and avoid default. Therefore, in the presence of bailouts, when the country is outside the program, international investors recognize that default risk is smaller and offer a higher price on sovereign bonds. As a consequence, the government can borrow more. Second, bailouts are requested during crisis times, because it is precisely during these periods that they are relatively cheaper than the outside option, market debt, and because they are particularly beneficial to the risk-averse government as they relax its budget constraint. Third, entering the bailout program implies periods of financial constraints and continuous interest payments. The duration of these constraints is directly linked to the bailout loan size, ====. On the one hand, if the bailout loan is small, the intervention period is relatively short, because it is relatively easy to repay this small loan when the economy exogenously recovers. On the other hand, if the bailout loan is large, the intervention period is relatively long, because it is much harder to repay the loan and it takes a much better realization of the exogenous economic conditions for the government to find it optimal to repay it. Notice that, in any case, the bailout mimics, albeit imperfectly, a state-contingent asset: it provides resources in bad times, and is repaid in good times.====In summary, a larger bailout size ==== is associated with: (i) better borrowing conditions when outside the program, as investors expect that the government avoids default in crisis times by requesting bailouts; (ii) a larger amount of resources available in times of need; (iii) a longer bailout program, which implies longer-lasting borrowing constraints and higher overall interest payments; (iv) a larger amount of resources to be repaid. While the first two features are welfare beneficial, the last two are not. The overall welfare effects of the bailouts depend on the magnitude of each of these forces, which are driven by the equilibrium behavior of the economy. We quantify these tradeoffs in a calibrated model.====The forces described above are a function of the fact that, as mentioned, a bailout in our setting is equivalent to a long-term contract between the IFI and the borrower and that its duration is endogenous. This feature is a key difference with respect to Fink and Scholl (2016). In their setting the country decides in each period whether to borrow from the IFI and how much; instead, in our setting, given the amount of the loan, ====, the government decides whether to borrow from the IFI and how long to remain in the program. In other words, while in Fink and Scholl’s framework a government decides the size of the IFI loan taking the duration of the program as given (one period), in our framework a government decides the duration of the program taking the size of the IFI loan as given. The fact that the duration is chosen endogenously and depends upon the size of the bailout deeply affects the welfare implications of bailouts by creating non-trivial tradeoffs between receiving larger loans and being locked in the constrained program for longer.====In order to give the model the best chance to incorporate various costs of default, we also augment the model with a financial intermediation channel in order to account for the observed relationship between sovereign spreads and domestic private credit conditions.==== In particular, we assume that firms face a working capital constraint on their wage bill as in Mendoza and Yue (2012). To meet the constraint firms must borrow funds from international financial intermediaries (banks) that buy government bonds on the secondary market. These sovereign bond holdings on the banks’ balance sheet generate an endogenous relationship between the price of government debt and the interest rate that banks charge to firms, as pointed out by Gennaioli et al. (2018). The model is able to replicate the data in several dimensions, namely default frequency, average sovereign spreads, average private credit rates, etc. As such, the model is suitable for evaluating the effects of bailout interventions, also accounting for how they affect the private credit market.====We show that while different bailout sizes do not alter the long-run average levels of consumption (and labor), they affect their standard deviations. Therefore, the welfare properties of bailouts are mainly driven by how they alter the second moments of consumption. Our main result is that the relationship between welfare gains of bailouts and the bailout size ==== is non-monotonic. For relatively small values of ==== welfare gains increase substantially with ====. The existence of bailouts improves market conditions, allowing the economy to borrow more and to use asset market as a good buffer against shocks. In addition, during crisis times, small bailouts allow for a good consumption smoothing profile, while not constraining the economy too much as they are easier to repay. In contrast, when ==== becomes relatively large, bailouts are still beneficial, but welfare gains decline. As bailouts are generous and cheap, the economy substitutes debt from the market with the IFI loans; however, as it is harder to repay these loans, the economy will be locked in the bailout program for a longer time, which limits its consumption smoothing ability.====In the process of decomposing this result, we show how the peak of the welfare gain, as a function of the bailout size, is associated with: (i) the peak of average borrowing from the market; (ii) the trough of the standard deviation of consumption; and (iii) the peak of the unconditional default frequency. The explanation for these links is the following. The existence of small bailouts leads to a relaxation of the endogenous borrowing limit that characterizes endogenous default models: when bailout option is present, investors understand that, if the government has not yet asked for a bailout, their investment has higher chance to be repaid and, therefore, they offer better financing conditions to the small open economy, which can, then, increase its borrowing. A higher level of borrowing has two implications: first, the borrower can use financial markets at a larger extent for consumption smoothing purposes, and, second, the borrower is more prone to defaults if the economic outlook becomes grim. It turns out that, in our setting, the gains of the former effect are larger than the costs of the latter.====Finally, we provide back-of-the-envelope calculations regarding the size of the amounts offered by the European Financial Stability Facility (EFSF) and European Stability Mechanism (ESM) to Ireland, Portugal and Greece in the recent sovereign crisis. These programs were different from the traditional International Monetary Fund (IMF) loans in that they were provided at significantly lower rates and longer maturities. In addition, most of the disbursements were front loaded, making our modelling choice of receiving funds all at once at the beginning of the program quite reasonable. The implied ranges for bailout sizes are in line with the values that we consider in our welfare analysis. While for Ireland and Portugal the size of the bailout is rather small, in a range close to the peak of the welfare function, the bailout size for Greece, if read under the lens of our exercise, might have been too large as Greece might be under the conditionality clause for a long time before being able to repay the loan.====Our paper relates to the growing literature on debt crisis and policy interventions by international financial institutions. In addition to the already mentioned work of Fink and Scholl (2016), our paper relates to Boz (2011) who investigates how the presence of the IFI loans affects the decisions of the sovereign, in a framework where the government can repeatedly borrow from these institutions even when it does not honor its debt to private international creditors. In her framework, fiscal conditionality is accounted for by a higher discount factor in periods when the sovereign is indebted to the international institutions. As in both Fink and Scholl (2016) and Boz (2011), we abstract from the decision-making process of the creditor. This is also similar to Aguiar and Gopinath (2006) who model bailout originating from an unmodeled third party. In their setup, however, the bailout comes in the form of an unconditional transfer, while in our setup bailout comes in the form of a non-defaultable loan and with imposed borrowing regulations, leading to more relevant trade offs. Moreover, a novel finding of our paper relates to the ex-post consequences of a bailout program conditional on the economic performance upon its implementation.====This paper is also related to the work of Guler et al. (2014) and Hatchondo et al. (2017). While the first paper studies bailouts in a two country version of the (Eaton and Gersovitz, 1981) model, the second paper considers the impact of introducing non-defaultable debt in a standard open economy endogenous default model; while that paper looks at the welfare consequences at the time of an unexpected introduction of bailouts (ex-post analysis), we are concern on the welfare consequences as a function of the bailout size, assuming that there is always full knowledge of the bailout possibility (ex-ante analysis).====Our work also relates to the work of Corsetti et al. (2006), Roch and Uhlig (2018), Bianchi and Mendoza (2011), Pancrazi and Zavalloni (2019), and Kirsch and Rühmkorf (2015) who study the role of official lending in various settings. We also related to Jeanne and Zettelmeyer (2001) who, in a policy oriented paper, discuss the implications of international bailouts and conditionality in relation to moral hazard. More generally, this paper relates also to the literature on strategic default such as (Eaton and Gersovitz, 1981), (Arellano, 2008), (Aguiar and Gopinath, 2006) and (Mendoza and Yue, 2012), and to papers that study the impact of different types of debt on the dynamics of sovereign spreads such as Hatchondo et al. (2017). To the best of our knowledge, our paper complements existing literature by analyzing welfare implications of bailouts in economies subject to financial constraints considering realistic tradeoffs arising from fiscal conditionality.====We relate to the literature that studies the impact of spread changes in high leveraged economies, such as (Fernández-Villaverde et al., 2011) and others. In particular, we highlight the interaction between financial intermediaries and sovereign risk, such as (Bianchi, 2016) and (Acharya et al., 2014). Also, sovereign default and banking crisis have raised the attention of many recent papers in the international macroeconomics literature. For instance, (Reinhart and Rogoff, 2011) and (Gennaioli et al., 2018) conduct empirical studies to uncover the relationship between sovereign debt and banking crisis, using aggregate and cross-country panel data on banks, respectively. In addition, (Sosa-Padilla, 2018) develops a model where banks are exposed to the risk of sovereign default as they lend both to the government and to the corporate sector. Mallucci (2013) uses a similar model with wholesale funding to study the implications of a relaxation of collateral eligibility requirements by a central bank.====The remainder of the paper is as follows. Section 2 describes the model. Section 3 discusses the calibration and the empirical strategy. Section 4 evaluates the welfare properties of bailouts. Section 5 concludes.",Welfare gains of bailouts in a sovereign default model,https://www.sciencedirect.com/science/article/pii/S0165188920300361,24 February 2020,2020,Research Article,346.0
"Petrović Marko,Ozel Bulent,Teglio Andrea,Raberto Marco,Cincotti Silvano","Universitat Jaume I, Dept. of Economics. Campus del Riu Sec, Castellón 12071, Spain,Università Ca’ Foscari Venezia, Dept. of Economics. San Giobbe 873, Venice 30121, Italy,DIME - University of Genoa. Via Opera Pia 15 I-16145 Genova, Italy","Received 14 September 2018, Revised 28 November 2019, Accepted 4 February 2020, Available online 21 February 2020, Version of Record 18 March 2020.",https://doi.org/10.1016/j.jedc.2020.103866,Cited by (4)," between countries, increasing the sustainability of the monetary union.","The recent history of the European Union (EU) highlighted the critical importance of a proper political and economic architecture in order to take full advantage of the unification and to withstand exogenous negative shocks. The global financial crises of ==== had a pervasive impact on all the leading economies in the world, but the place where it might have been more disruptive is the European Union, which revealed structural fragility and inadequacy to tackle some of the main challenges ahead. The crisis in the Middle East and North African countries, exacerbating migration flows towards Europe, represents another shock that disclosed the lack of coordination among countries in the EU. The geographical, cultural, and economic diversity among these countries has not been harmonized in a well-balanced and convincing project, thus exposing the union to frequent confrontations and conflicts at different levels, i.e., North versus South, Core versus Periphery,==== U.K. versus continental Europe. Brexit and the recent electoral success of euro-skeptical parties across Europe, have shown both the vulnerability of the current project and the need to revise it. A rich debate flourished among scholars, politicians, and observers, about the needed steps to improve the prosperity and the stability of the European project.====The aim of this paper is to propose a computational model, flexible enough to tackle several of the main topics that are emerging within the European Union and beyond. The idea is to use the agent-based methodology to reconsider some of the basic results in international economics, taking advantage of the powerful and peculiar features of this approach, like heterogeneity and local interactions, which are able to generate endogenous business cycles. We address very general and essential questions, as (i) measuring the difference between open and closed economies in terms of economic performance, (ii) how this performance changes according to different assumptions about relevant variables, as workers mobility or productivity gap, (iii) how inequality between countries evolves in different scenarios, (iv) if fiscal policy coordination can improve the inequality and/or the performance of the union. In order to investigate these topics we transform the single-country Eurace model==== into a system of interacting economies. Therefore, the core simulation element evolves from a single closed economy to a multiplicity of open economies that interact in different markets and with different strengths.====In particular, we study the economic consequences of becoming part of a union, whose architecture is inspired from the EU, i.e, it includes international labor, goods and capital markets, along with a common currency. We start with a very simple and general case, where we compare the performance of two identical countries belonging to the union with two equivalent closed economies. We then extend the first experiment to the case of countries endowed with different productivity levels, examining the conditions under which two different countries can benefit from becoming members of a union, and considering the main problems that might emerge. The analysis of a wide range of macroeconomic variables allows us to figure out the main determinants of the observed outcomes. The core results of the paper emerge as a consequence of the idiosyncratic dynamics of the countries, and in particular of the endogenous and asynchronous business cycles, typical of the agent-base approach, which does not rely on external shocks to generate business cycle fluctuations. We find that countries belonging to a union perform better than their isolated counterparts, mainly because of trade and workers mobility, which smooth out business cycles asynchrony between countries, enhancing sales and employment. However, in the case of high productivity gap and low mobility frictions, the union can even exacerbate the inequality between countries because an excessive emigration impoverishes countries with low productivity. Labor mobility in this case is no longer an adjustment mechanism to even out business cycle shocks across countries, but leads to permanent changes in the structure of countries’ economies. In order to tackle the problem of inequality, we design a final experiment where we test the possible mitigation effect of a redistributive fiscal policy at the union level.====There are a few works in the agent-based literature dealing with multi-country settings. Dawid et al. (2012) use a two-region model to study the effects of labor market policies on output and convergence, while Dawid et al. (2014) study the impact of human capital and technology policies designed to foster convergence. Using a similar setting, Dawid et al. (2018a) find that technology policies reduce income inequality in the economy, especially helping the developing region to increase output. Furthermore, Dawid et al. (2018b) test the effects of a set of inter-regional fiscal policies on economic growth, finding that fiscal transfers have a positive effect on the weaker (periphery) region. In general, our results are in line with the work of Dawid and co-authors, however we stress in this paper the nexus of productivity gap and emigration====, which has not been fully considered in their work. Another conceptual difference between our work and the ones of Dawid et al. is that we do not focus on convergence, but we aim at understanding the conditions under which two countries with a permanent productivity gap can take advantage of joining a monetary union. This choice is justified by the empirical literature casting doubt on the productivity convergence process in Europe. Many studies, from Tsionas (2000), and Boldrin and Canova (2001), to Aiello and Pupo (2012), Monfort et al. (2013), and Sondermann (2014), show from different perspectives a lack of significant convergence, therefore highlighting the relevance of studying if and when it is convenient for countries with different productivities to join in unions. Actually, non-convergence can stem from different economic traditions and cultural traits that might be important to acknowledge or even protect. In this perspective, the union should be the place allowing for the successful cohabitation==== of countries with different economic strength and properties: Germany and Greece can serve as an example. Moreover, a recent work of Dosi et al. (2019) simulates a world economy with international technology flows, trade interactions and exchange rate dynamics, showing a persistent income divergence that leads to polarization and club formation among countries. Therefore, productivity is considered as fixed in our model and it mainly changes the size of the economy and the wage structure, thus allowing us to focus on the conditions under which two or more countries can benefit from being part of a union.====Caiani et al. (2018) find in a European-like ABM economy that fiscal austerity raises public debt-to-GDP ratio. They focus attention on the design of international trade, omitting the international labor market, which is, on the contrary, quite central in our work (details about the framing of labor mobility in our paper are provided further on). Furthermore, Wolf et al. (2013) designed a flexible multi-regional model, able to address research questions that range from theory generation to policy analysis. The authors are especially interested in the study of different growth paths in the context of climate policy. More in general, the model presented in this paper belongs to the recent tradition of macroeconomic agent-based models, which includes Dosi et al. (2015), Caiani et al. (2016), and Ashraf et al. (2017), as a non-exhaustive sample.====The contribution of our paper is not limited to the agent-based literature but interacts with other fields of studies, such as ====ptimal ====urrency ====reas (OCA) theory and labor market effects of immigration. The results of the paper are relevant to OCA theory, pioneered by Mundell (1961), which investigates costs and benefits of being a part of common currency areas.==== According to the theory, a currency union will be less costly for countries that (i) share higher co-movements of economic variables, (ii) have high mobility of workers and (iii) have high degree of wage and price flexibility, since these conditions facilitate the adjustment process and full employment. In addition, costs will decrease with fiscal integration, which smooths out diverse shocks through regional transfers (Kenen, 1969), and in particular smooths consumptions when countries face idiosyncratic shocks (Mundell, 1973, Buiter, Sibert, 2008). Among the benefits for the union, there are (i) reducing transaction costs and exchange rate uncertainty, (ii) increasing price transparency and trade====, and (iii) a higher commitment to monetary policy, with a more efficient inflation supervision (Chari et al., 2019). In the perspective of our paper, it is useful to point out that the OCA criteria are endogenous and that once a country joins the union, the optimality conditions are likely to be affected (Mundell, 1961 and Krugman, 1993). For instance, Frankel and Rose (2001) find that trade increases the synchronization of business cycles while Alesina et al. (2002) and Barro and Tenreyro (2007) show that the currency union affects the degree of co-movement of price and output shocks across countries.====We find similar results in our model, in a framework where we do not need asymmetric shocks but asynchronous business cycles can arise endogenously. Labor mobility is an important adjustment mechanism in a monetary union, which smooths cycle asymmetries among countries, allowing the achievement of full employment and price stability. Our results validate the view of Kenen (1969) that the fiscal transfers smooth out diverse shocks and improve synchronization among union members. The benefits of a fiscal union are larger the stronger the asymmetries among the union members, which is in line with the results of Farhi and Werning (2017). However, in our paper productivity gaps are unrelated to monetary policy responses, and instead create systematic wage differences which drive labor mobility. Therefore, unlike in OCA theory, where labor mobility should improve performance because of its ability to mitigate business cycle differences, in our paper labor mobility, under certain conditions, can worsen the union’s performance because it creates polarization. We show that high productivity gaps, combined with excessive mobility, may lead to a suboptimal outcome in the union, reducing the aggregate performance of the countries due to strong emigration and inefficient use of capital and labor.====The effects of labor mobility on the macroeconomic aggregates have been widely examined in the theoretical literature, mainly using the computable general equilibrium (CGE) framework, for instance in the context of the EU Eastern enlargement. This type of studies allows the analysis of the interaction between migration, capital movements, and trade which is also addressed in our study. Overall, the CGE literature finds stronger implications of migration on wage and unemployment than those found in the empirical literature. The negative effects of immigration, in particular for low-skilled workers, are outweighed by positive effects coming from the integration of goods markets (e.g. Baldwin-Edwards, 1997). Therefore, most of the models predict that the EU Eastern enlargement results in higher wages and lower aggregate unemployment in both receiving and sending countries, which is in line with the results that we provide in this paper. Besides, CGE models predict an increase of GDP in the receiving country and in the EU. This effect is even amplified if the creation of new trade between existing and new member states is taken into account (Boeri and Brücker, 2005). However, the gains in aggregate and per capita income can be reduced due to labor market rigidities (see for instance Palmer and Pytliková, 2015; and Baas and Brücker, 2008 who studied the effects of immigration to Germany and UK). In contrast, our study shows that labor market rigidities not necessarily harm the aggregate income, but the final outcome will also depend on the country diversity in the union. Our model allows to consider both the performance of the union as a whole and the performance of the single countries of the union with respect to their identical counterparts as isolated countries. Therefore, it allows to study not only the impact of immigration==== on the hosting country, but also the distribution of the population in the union. We find that excessive labor mobility can harm the lower productivity country, as many workers, attracted by higher real wages, emigrate to the high tech country, provoking the disarray of the government budget and of the whole economy. On the other hand, we find that labor mobility is able to foster employment and income in the union, especially when the productivity gap between countries is not too high.====Finally, we spend some words on the fiscal integration mechanism==== that we call “fiscal pool”, consisting of a centralized deposit account where member countries of the union are obliged to put a part of their budget surplus (if any). On the other hand, countries that need to finance their budget deficit can ask (and obtain) money from the fiscal pool. Since the governments finance their budget deficits by raising new public debt, the “fiscal pool” should enable a better use of the income surplus in the union, thus improving the budget balance and reducing the public debt of the union members. The governments should afford higher spendings, providing also higher transfers to households which would increase their total income. In turn, this should decrease the likelihood of migration since households will be willing to stay in their home countries if they can earn competitive incomes. In addition, note that the outflow of workers may act as a sort of the accelerator mechanism. With a lower amount of production factors (workers) the economic activity declines, and so the tax revenue. If the public expenses (e.g. interest on the sovereign debt, public employees, etc.) are downward rigid in the short run, the government may be forced to raise new public debt and try to decrease some of the public expenditures (contractionary fiscal policy) which will further slow down the economic activity. On the contrary, expansionary fiscal policies generally improve the performance of the countries in the union. For instance, an increase in the maximum deficit-to-GDP ratio (expansionary policy) improves the dynamics of GDP, labor productivity, and employment, however, at the cost of a higher level of public debt and inflation (Caiani et al., 2018). In addition, diverse shocks that may hit the members of common currency areas can be mitigated through the fiscal integration among countries. The fiscal transfers between regions can absorb the impact of asymmetric shocks and improve the efficiency of the common monetary policy, as in Kenen (1969).====The paper proceeds as follows. The next section presents the model. Section 3 describes computational experiments and presents results. The paper ends with conclusions in Section 4.",Should I stay or should I go? An agent-based setup for a trading and monetary union,https://www.sciencedirect.com/science/article/pii/S0165188918302720,21 February 2020,2020,Research Article,347.0
"Kraft Holger,Meyer-Wehmann André,Seifried Frank Thomas","Faculty of Economics and Business Administration, Goethe University, Frankfurt am Main, Germany,Department IV – Mathematics, University of Trier, Trier, Germany","Received 16 May 2019, Revised 16 December 2019, Accepted 30 January 2020, Available online 19 February 2020, Version of Record 2 March 2020.",https://doi.org/10.1016/j.jedc.2020.103857,Cited by (9),"In dynamic portfolio choice problems, stochastic state variables such as ==== lead to adjustments of the optimal stock demand referred to as hedge terms or Merton-Breeden terms. By deriving an explicit solution in a two-agent framework with a stochastic opportunity set, we show that relative ==== concerns give rise to new hedge terms beyond the ordinary ones. This is because the agents hedge against both exogenous changes in the state variable and endogenous decisions of the other agent. Depending on the parametrization of the model, these new terms can significantly change the investors’ hedging demands. We also show that both heterogeneity in ","In his seminal work on dynamic portfolio choice, Merton, 1969, Merton, 1971 shows that stochastic state variables such as stochastic volatility lead to adjustments of the optimal stock demand referred to as hedge terms or Merton-Breeden terms. The myopic demand of a setting with constant investment opportunity set is changed according to these additional terms. For instance, a counter-cyclical volatility process increases the optimal stock demand relative to the myopic demand (see, e.g., Liu, 2007).====Our paper contributes to this literature by studying a framework where investors have relative wealth concerns, i.e., investors derive utility not only from maximizing their own wealth, but also from performing well relatively to their peers. Basak and Makarov (2014), among others, show that relative wealth concerns give rise to additional myopic stock demand if the opportunity set is constant.==== We focus on a setting with a stochastic opportunity set where the state variable is not necessarily spanned, i.e., the market is in general incomplete. This setting includes the model by Chacko and Viceira (2003) and the Heston model studied in Liu (2007) as special cases.====By deriving an explicit solution for a two-agent framework and this stochastic opportunity set,==== we show that relative wealth concerns give rise to new hedge terms beyond the usual Merton-Breeden terms. This is because the agents do not only hedge against exogenous changes in the state variable, but also respond to the endogenous hedging activities of other agents.====Therefore, the investors’ stock demands consist of four parts: an ordinary myopic term, a myopic term stemming from relative wealth concerns, an ordinary hedge term, and a hedge term resulting from relative wealth concerns. Depending on the parametrization of the model, the new hedge terms can significantly change the hedging demand of every investor. We find that wealth concerns give rise to a hedging demand that is of a similar order of magnitude as the ordinary hedging demand when compared to the corresponding myopic demands (up to about 15% to 20% of the corresponding myopic demands). We also show that both heterogeneity in risk aversion or relative wealth concerns can have similar effects on the heterogeneity in portfolio decisions. Besides, we prove that in a setting with a stochastic state variable and relative wealth concerns only, all agents invest in the growth-optimal portfolio, independent of their relative risk aversion coefficients.====Relative wealth concerns can be studied in a multi-agent framework with endogenous benchmarking. Formally, this leads to a non-cooperative, non-zero sum stochastic differential game in the sense of Isaacs (1965), Friedman (1972), and Nisio (1988), among others. Our paper provides an explicit solution to this game in a setting with an unspanned state variable. It thus generalizes results in Liu (2007), among others, who studies the case of a single investor, and findings in Basak and Makarov (2014), among others, who study a related game without a state variable. Additionally, we prove a verification theorem for this incomplete-market framework showing that the solution of the system of Hamilton–Jacobi–Bellman–Isaacs equations is the optimal solution of the problem.====There are several potential reasons why relative wealth concerns can influence decisions of investors. First, the compensations of some agents (e.g., fund managers) are tied to their performances relative to their peers. To see why relative wealth concerns might be relevant in such a situation, notice that the performance of actively managed funds can be evaluated in two dimensions: First, one can measure the relative performance with respect to a passive benchmark such as the S&P 500 that is exogenously given. Second, one can compare the fund performance with funds in the same category. The latter has become increasingly important since the mid of the 90s when, among others, Morningstar Inc. introduced peer-group based fund rankings.==== Consequently, funds in the same category (e.g., funds with benchmark S&P 500) are compared to one another. For two funds in the same category that are similar in the relevant peer-group ranking, relative wealth concerns can thus proxy for the effect that is induced by the relative evaluation of a fund with respect to its peer group. Following these ideas, we consider an application of our framework where an investor delegates his portfolio decision to a fund manager who has social preferences. For different levels of risk aversion and strengths of social preferences, we quantify the costs of delegation in closed form.====Second, relative wealth concerns might result from ==== and our paper also contributes to this strand of literature. Early papers include Duesenberry (1949), Hirsch (1976), Frank (1985), and Hopkins and Kornienko (2004). Bakshi and Chen (1996) build a model in which agents derive utility from both own consumption and relative social standing. Social standing furthermore depends on the agent’s own absolute wealth as well as a so-called social-wealth index where the social wealth index is ==== given and not determined within the model. In Bakshi and Chen’s model the social-wealth index can vary among agents since agents have different reference groups. They find that the additional state variable, the level of the social-wealth index, induces agents to alter their portfolio decision in a more conservative manner compared to Merton, 1969, Merton, 1971.====Roussanov (2010) picks up on the specification used by Bakshi and Chen (1996) and defines social status as household wealth divided by per-capita wealth in the economy. In Roussanov’s model, the agent maximizes CRRA utility from consumption and relative wealth (benchmarked against aggregate wealth). Roussanov interprets the ordinary consumption term as necessary consumption and the relative wealth term part as luxury consumption in the sense of Wachter and Yogo (2010). The model developed by Roussanov is able to match empirically observed heterogeneity in portfolio holdings, in particular the discrepancy between more affluent and poorer households.====Furthermore, there is a recent literature on portfolio choice problems with social interactions. Basak and Makarov (2014) study the strategic interaction among money managers competing for fund flows. Formally, they analyze a stochastic differential game with social preferences in a canonical Merton, 1969, Merton, 1971 setting, i.e., there are no state variables and the investment opportunity set is thus constant. Curatola (2017) studies the implication of social preferences on portfolio choice and asset pricing, but assumes log-linear preferences. Additionally, there are more formal treatments of such problems: Geng and Zariphopoulou (2017) study a portfolio choice problem in a Merton, 1969, Merton, 1971 setting involving social preferences. Lacker and Zariphopoulou (2017) analyze the same framework with multiple agents. Espinosa and Touzi (2014) provide general conditions for the existence of Nash equlibria in such portfolio games in complete markets or in specific settings with exponential utility. Browne (2000) studies alternative versions of portfolio games. However, all these papers do not include (unspanned) stochastic state variables.====The remainder of the paper is structured as follows: Section 2 formulates and solves the stochastic differential game of two (potentially) heterogenous investors with relative wealth concerns. The solution is explicit up to solving a system of ordinary differential equations. Section 3 shows that the solution becomes fully explicit in a setting with homogenous agents. Section 4 studies the economic implications on the optimal stock demands. Section 5 discusses an application of the framework where an investor with CRRA preferences delegates his portfolio decisions to a fund manager with social preferences. We provide a closed-form solution for the costs of delegation in our framework. Section 6 concludes. All proofs and some auxiliary results as well as the estimation of the different models can be found in the Appendix.",Dynamic asset allocation with relative wealth concerns in incomplete markets,https://www.sciencedirect.com/science/article/pii/S0165188920300270,19 February 2020,2020,Research Article,348.0
"Alfeus Mesias,Grasselli Martino,Schlögl Erik","Dipartimento di Matematica, Università degli Studi di Padova, Italy,Léonard de Vinci Pôle Universitaire, Research Center, Finance Group, Paris La Défense Cedex 92 916, France,University of Technology Sydney, Quantitative Finance Research Centre, Australia,African Institute of Financial Markets and Risk Management, University of Cape Town, South Africa,Faculty of Science, Department of Statistics, University of Johannesburg, P.O. Box 524, Auckland Park, 2006, South Africa","Received 27 July 2019, Revised 20 November 2019, Accepted 7 February 2020, Available online 19 February 2020, Version of Record 8 April 2020.",https://doi.org/10.1016/j.jedc.2020.103861,Cited by (7),"Starting from the observation that single-currency swap basis spreads contradict classical arbitrage arguments, we construct a framework where this basis arises due to the presence of “roll-over risk.” This risk consists of two components: (1) facing a higher credit spread (e.g. due to a credit downgrade) when rolling over short-term borrowing (2) heightened borrowing costs due to an absence of market liquidity. The model simultaneously fits OIS, ","The phenomenon of the frequency basis (i.e. a spread applied to one leg of a swap to exchange one floating interest rate for another of a different tenor in the same currency) contradicts textbook no-arbitrage conditions and has become an important feature of interest rate markets since the beginning of the Global Financial Crisis (GFC) in 2008. As a consequence, stochastic interest rate term structure models for financial risk management and the pricing of derivative financial instruments in practice now reflect the existence of multiple term structures, i.e. possibly as many as there are tenors. While this pragmatic approach can be made mathematically consistent (see Grasselli and Miglietta, 2016 and Grbac and Runggaldier, 2015 for a recent treatise, as well as the literature cited therein), it does not seek to explain this proliferation of term structures, nor does it allow the extraction of information potentially relevant to risk management from the basis spreads observed in the market.====In the pre-GFC understanding of interest rate swaps (as explained in, e.g., Hull (2008)), the presence of a basis spread in a floating-for-floating interest rate swap would point to the existence of an arbitrage opportunity, unless this spread is too small to recover transaction costs. As documented by Chang and Schlögl (2015), post-GFC the basis spread cannot be explained by transaction costs alone, and therefore there must be a new perception by the market of risks involved in the execution of textbook “arbitrage” strategies. Since such textbook strategies to profit from the presence of basis spreads would involve lending at the longer tenor and borrowing at the shorter tenor, the prime candidate for this is “roll-over risk.” This is the risk that in the future, once committed to the “arbitrage” strategy, one might not be able to refinance (“roll over”) the borrowing at the prevailing market rate (i.e., the reference rate for the shorter tenor of the basis swap). This “roll-over risk,” invalidating the “arbitrage” strategy, can be seen as a combination of “downgrade risk” (i.e., the risk faced by the potential arbitrageur that the credit spread demanded by its creditors will increase relative to the market average) and “funding liquidity risk” (i.e., the risk of a situation where funding in the market can only be accessed at an additional premium above the benchmark reference rates).====Thus, arguing explicitly that the phenomenon of the frequency basis can only persist (as empirically observed) if the putative “arbitrage” channel is closed off by the presence of this (new) risk, we propose a model in which roll-over risk endogenously leads to the presence of basis spreads between interest rate term structures for different tenors. This is in essence the “reduced-form” or “spread-based” approach to multicurve modelling, similar to the approach taken in the credit risk literature, where the risk of loss due to default gives rise to credit spreads. The model allows us to extract the forward-looking “market’s view” of roll-over risk from the observed basis spreads, to which the model is calibrated.====In this model, the spread between LIBOR (London Interbank Offer Rate) and OIS (overnight index swaps) becomes a special case of the modelled frequency basis, i.e. the spread between a rate for borrowing “in one go” over, say, three months (LIBOR), versus rolling over borrowing daily (the rate underlying OIS). This provides a theoretical framework for the relationship between the frequency basis and what the literature commonly calls “term premia” or “term funding risk”, i.e. it makes explicit how the LIBOR/OIS spread represents a premium paid by the borrower at LIBOR to avoid term funding risk over the length of the LIBOR borrowing period.====The bulk of the literature on modelling basis spreads is in a sense even more “reduced-form” than what we propose here, in the sense that basis spreads are recognised to exist, and are modelled to be either deterministic or stochastic in a mathematically consistent fashion, but there are no structural links between term structures of interest rates for different tenors (in a sense, the analogue of this approach applied to credit risk would be to model stochastic credit spreads directly, without any link to probabilities of default and losses in the event of default). This strand of the literature can be traced back to Boenkost and Schmidt (2004), who used this approach to construct a model for cross currency swap valuation in the presence of a basis spread. This was subsequently adapted by Kijima et al. (2009) to modelling a single-currency basis spread. Henrard (2010) took an axiomatic approach to the problem, modelling a deterministic multiplicative spread between term structures associated with different tenors. Initially, these models were not reconciled with the requirement of the absence of arbitrage. Subsequent work, however, such as Fujii et al. (2009), gave explicit consideration to this requirement. This pragmatic way of modelling interest rates in the presence of spreads between term structures of interest rates for different tenors has been pursued further in a number of papers, including Mercurio, 2009, Mercurio, 2010 in a LIBOR Market Model setting, Kenyon (2010) in a short-rate modelling framework, stochastic additive basis spreads in Mercurio and Xie (2012), and Henrard (2013) for stochastic multiplicative basis spreads. Moreni and Pallavicini (2014) propose a model of two curves, riskfree instantaneous forward rates and forward LIBORs, which is Markovian in a common set of state variables. Macrina and Mahomed (2018) construct a pricing kernel framework for multicurve models (be it discount count curves in different currencies, or real vs. nominal interest rates, or for different tenors), but again this approach does not attempt to model the structural links between different term structures.====Early work incorporating some of the potential causes of basis spreads into models of the single-currency “multicurve” environment post-GFC includes Morini (2009) and Bianchetti (2010b), who focus on counterparty credit risk. The model of Crépey (2015) links funding cost and counterparty credit risk in a credit valuation adjustment (CVA) framework, but does not explicitly consider spreads between different tenors arising from roll-over risk.====There is an emerging view that “roll-over risk” is what prevents pre-crisis textbook arbitrage strategies to exploit the basis spreads between tenors, and that modelling this risk can provide the link between OIS, the XIBOR (e.g. LIBOR, EURIBOR, etc.) style money market, the vanilla swap market, and the basis swap market. A key contribution in this vein is Filipović and Trolle (2013), who estimate the dynamics of interbank risk from time series data from these markets. They define “interbank risk” as “the risk of direct or indirect loss resulting from lending in the interbank money market.” Decomposing the term structure of interbank risk into what they identify as default and non-default (liquidity) components, they study the associated risk premia. Filipović and Trolle interpret the “default” component in terms of the risk of a deterioration of creditworthiness of a LIBOR reference panel bank resulting in it dropping out of the LIBOR panel,==== in which case this bank immediately would no longer be able to roll over debt at the overnight reference rate, while the rate on any LIBOR borrowing would remain fixed until the end of the accrual period (i.e., typically for several months). In their analysis, this differential impact of downgrade risk on rolling debt explains part of the LIBOR/OIS spread; the residual is labelled the “liquidity” component.==== It is important to note that both components manifest themselves in the risk of additional cost when rolling over debt, i.e., “downgrade risk” and “funding liquidity risk”==== combining to form a total “roll-over risk.”==== In our framework below, we go further by modelling both components as spreads applied to the overnight borrowing cost, showing how these block potential naive “arbitrage” strategies to take advantage of the LIBOR/OIS spread and/or the frequency basis, thus permitting a more explicit analysis.====An alternative approach at the more fundamental end of the modelling spectrum is the recent work by Gallitschke et al. (2014), who propose a model for interbank cash transactions and the relevant credit and liquidity risk factors, which endogenously generates multiple term structures for different tenors. In particular, they explicitly model a mechanism by which XIBOR is determined by submissions of the member banks of a panel, which adds substantial complexity to the model.====We construct a consistent stochastic model encompassing OIS, XIBOR, vanilla and basis swaps in a single currency.==== Our “reduced-form” approach explicitly models both the credit and the funding component of roll-over risk to link multiple yield curves. In that, it is more parsimonious than the “pragmatic” way of modelling extant in the literature (reviewed above), where stochastic dynamics for basis spreads are specified directly without recourse to the underlying roll-over risk. In particular, this allows the relative pricing of bespoke tenors in a model calibrated to basis spreads between tenors for which liquid market data is available. It does not require the introduction of a new stochastic factor (or deterministic spread) for each new tenor. However, the approach is “reduced-form” in the sense that it abstracts from structural causes of downgrade risk and funding liquidity risk – in this sense, our approach is closest in spirit to the “reduced-form” models of credit risk, doing for basis spreads what those models have done for credit spreads. This framework is presented from a cross-sectional, “mark-to-market” calibration point of view, and as such represents a proposed replacement for the more “ad hoc” multicurve modelling approaches currently prevalent in industry practice.====The remainder of the paper is organised as follows. Section 2 explains how the presence of roll-over risk “breaks” the naive arbitrage opportunity arising from the frequency basis and expresses the basic instruments in terms of the model variables, i.e., the overnight rate, credit spreads and a spread representing pure funding liquidity risk. This section abstracts from any specific assumptions about the nature of the roll-over risk dynamics. Section 3 calibrates a concrete specification of the model in terms of multifactor Cox/Ingersoll/Ross-type dynamics==== to market data for OIS, interest rate and basis swaps – this is the version of the model focused solely on the frequency basis. In order to separate roll-over risk into its credit and liquidity component in calibration to market data, we need to include credit default swaps (CDS) in the set of calibration instruments – this is done in Section 4. Section 5 concludes.",A consistent stochastic model of the term structure of interest rates for multiple tenors,https://www.sciencedirect.com/science/article/pii/S0165188920300312,19 February 2020,2020,Research Article,349.0
"Kim Soyoung,Yim Geunhyung","Department of Economics, Seoul National University, San 56-1, Sillim-Dong, Seoul 151-746, Gwanak-Gu, Republic of Korea,Monetary Policy Department, Bank of Korea, 67, Sejong-daero, Seoul 04514, Jung-Gu, Republic of Korea","Received 17 March 2019, Revised 29 January 2020, Accepted 3 February 2020, Available online 18 February 2020, Version of Record 17 March 2020.",https://doi.org/10.1016/j.jedc.2020.103858,Cited by (2), and the inflation rate.,"Since New Zealand adopted inflation targeting in 1990, an increasing number of countries have adopted this policy as well. As a result, actual inflation rates in many economies, even emerging ones, have decreased sharply after the inflation targeting system was introduced. The inflation rate in our sample of 19 countries dropped by 9.3% points on average in 5 years after adopting inflation targeting.====Although inflation targeting has been successful in reducing the inflation rate, the inflation-targeting central banks have not always been successful in meeting the inflation target. In our sample of 19 countries, the absolute value of deviation of the actual inflation rate from the target is 2.0% points at an annual average. This study aims to find out how inflation-targeting central banks behave when they miss the target. Particularly, do inflation-targeting central banks respond to misses on their targets by adjusting their targets to close the gap between the actual inflation rate and the inflation target?====For example, in 2004, the Bank of Indonesia set the following three-year inflation target: 6 ± 1% (2005), 5.5 ± 1% (2006), and 5 ± 1% (2007). However, when the inflation rate departed from the upper bound of the target (10.5% in 2005), the Bank of Indonesia revised upwards the mid-point of the target by more than 1% point to 8 ± 1% (2006), 6 ± 1% (2007), and 5 ± 1% (2008) in 2005.====Similarly, the Central Bank of Colombia decreased the annual inflation target gradually from 22% (1993) to 17% (1996) when the inflation rate declined from 27% (1992) to 20.9% (1995). However, the central bank raised the target by 1% point to 18% in 1997 because the inflation rate did not substantially drop in 1996 (20.8%) and exceeded the inflation target by 3.8% points. However, the central bank lowered the target to 16% in 1998 after confirming that the inflation rate decreased to 18.5%; the difference between the actual inflation rate and the target shrank to 0.5% point in 1997. Conversely, in 1999, the inflation rate suddenly dropped to 10.9% from 18.7% in the previous year and fell short of the target by 4.1% points. The bank revised the target downward by 5% points from 15% (1999) to 10% (2000) in this case. Moreover, the Central Bank of Colombia adjusted its target rate even in recent years because the inflation rate deviated from the target substantially. As the inflation rate sharply increased from 5.5% in 2007 to 7% in 2008, missing the target by 3% points, the central bank revised the target upward by 1% point from 3.5–4.5% (2008) to 4.5–5.5% (2009). The inflation rate then dropped in 2009 (4.2%), causing the target to be missed by –0.8% points. Subsequently, the target for 2010 was reduced to 3 ± 1%.====Under inflation targeting, central banks often face immense pressure to keep the actual inflation rate within a target range. Thus, when the actual inflation rate deviates from the target, central banks may decide to change the inflation target to close the gap in the next period. In other words, central banks ideally set an inflation target first and then make the actual inflation rate adjust to the target. However, central banks may adjust an inflation target to the actual inflation rate instead, especially when meeting the target is difficult.====With such behavior, inflation targeting may seem successful even when it is not because the actual inflation rate is close to the target. More importantly, such behavior may weaken the stabilizing role of an inflation targeting framework. Under the inflation targeting framework, the central bank is supposed to help stabilize inflation by setting the target, trying to achieve the target, showing effort to economic agents, and leading economic agents to set inflation expectations close to the target. However, if the central bank changes the target to close the gap between the target and the actual rate, then the inflation expectations of economic agents and the actual inflation rates may not be stabilized because deviations of the inflation rate from the target will be resolved by adjustment of the target and not by changes in actual inflation rate with monetary policy actions. In this case, the inflation target adjusts to the inflation rate; therefore, the inflation target may not work as an anchor for the inflation expectations of economic agents.====We run panel regressions by using the data of 19 inflation-targeting countries. The empirical results suggest that the adjustment of inflation targets from the previous period significantly and positively depend on the deviations of the actual rate from the target. In other words, when central banks miss the target, the central banks adjust their inflation target in the next period to close the gap between the actual rate and the target, given the high persistence of the inflation rate. These results robustly stand against various modifications of the empirical model, such as considering reverse causality and reducing the sample period. We also divide the sample countries into two groups, namely, high- and low-performance groups, based on the performance of their central banks in meeting the inflation target. The results show that countries in the low-performance group actively adjust inflation targets to meet the target compared with the high-performance group. This result may suggest that central banks with low performance have further incentive to adjust an inflation target to reduce the gap between the actual inflation rate and the target.====We construct a standard New Keynesian model to illustrate the consequences of the behaviors of central banks. We show that equilibrium is undetermined under such behavior, and thus reveal an ironic result. When the actual inflation rate deviates from the inflation target, but the inflation target is difficult to achieve, central banks may consider adjusting the inflation target to close the gap between the actual inflation rate and the target in the next period. This action aims to enhance the credibility and stabilize the inflation rate but ends up with equilibrium indeterminacy by destabilizing the inflation expectation and the inflation rate.====The rest of our paper is organized as follows: Section 2 explains the data and presents the empirical results. Section 3 provides the standard New Keynesian model to illustrate the consequences of central banks’ adjustment of inflation targets to meet the target. Section 4 concludes the study with a summary of the results.",Do inflation-targeting central banks adjust inflation targets to meet the target?,https://www.sciencedirect.com/science/article/pii/S0165188920300282,18 February 2020,2020,Research Article,350.0
"Janssen Dirk-Jan,Li Jiangyan,Qiu Jianying,Weitzel Utz","Institute for Management Research, Radboud University, Heyendaalseweg 141, Nijmegen, the Netherlands,School of Finance, Laboratory of Experimental Economics, Dongbei University of Finance and Economics, Jianshan Street 217, Shahekou District, Dalian, China,Vrije Universiteit Amsterdam & Tinbergen Institute, De Boelelaan 1105, 1081HV, Amsterdam, the Netherlands","Received 18 July 2019, Revised 29 January 2020, Accepted 5 February 2020, Available online 13 February 2020, Version of Record 29 February 2020.",https://doi.org/10.1016/j.jedc.2020.103856,Cited by (5),"We examine the role of the disposition effect in ==== following the arrival of private signals to a small group of informed traders. Subjects trade an ambiguous asset via a computer-based double auction. Using a 2 × 2 × 2 design, we endow two types of signal, i.e., positive vs. negative, to informed traders with two different levels of the disposition effect, i.e., high vs. low, that are measured in two domains, i.e., gain vs. loss. We find that (1) the disposition effect measured in the gain domain has qualitatively different implications from the disposition effect measured in the loss domain; (2) following a favorable signal, informed traders with high disposition effect levels are more likely to sell and less likely to hold the asset while following an unfavorable signal, the opposite is true; (3) there is some evidence of stronger price underreaction in markets with informed traders with high disposition effect levels than in markets with informed traders with low disposition effect levels, but the effect is overall relatively weak; and finally and most importantly (4) the above results hold only when the sign of the signal matches the domain that the disposition effect levels of the informed traders are measured in.","Aggregating diversified private information into prices is perhaps the most important function of markets (Hayek, 1945). Much effort has been devoted to the design of market mechanisms that could facilitate this function. However, markets are never perfectly efficient (Grossman, Stiglitz, 1980, Lo, 2004). Many factors could affect the speed and the scope with which private information is aggregated and transmitted to prices (see e.g., Bao et al., 2019, Barber, Odean, 2007, Bekaert, Harvey, Lundblad, 2007, Chen, Goldstein, Jiang, 2006, Garleanu, Heje, 2018, Merkley, Michaely, Pacelli, 2017). Identifying those factors and examining their exact implications has been an important agenda for many researchers for decades. In this paper, we consider the disposition effect.====The disposition effect, as first coined in Shefrin and Statman (1985), is defined as the tendency of investors to hold stocks with capital losses too long and to sell stocks with capital gains too soon. The disposition effect is perhaps “one of the most robust facts about the trading of individual investors” (Barberis and Xiong, 2009). It has been observed in stock markets (Chong, 2009, Hur, Pritamani, Sharma, 2010, Odean, 1998), futures markets (Choe, Eom, 2009, Chou, Wang, 2011, Frino, Johnstone, Zheng, 2004, Li, Yang, 2013), mutual fund markets (Cici, 2012, Frazzini, 2006, Singal, Xu, 2011), and experimental asset markets (Chang, Solomon, Westerfield, 2016, Cueva, Iturbe-Ormaetxe, Ponti, Tomás, 2019, Da Costa Jr, Goulart, Cupertino, Macedo Jr, Da Silva, 2013, Fischbacher, Hoffmann, Schudy, 2017, Frydman, Rangel, 2014, Hermann, Mußhoff, Rau, 2019, Jiao, 2017, Pelster, Hofmann, 2018, Rau, 2015, Weber, Camerer, 1998, Weber, Welfens, 2007, Weber, Welfens, 2007).====The possibility that the disposition effect could affect market efficiency has been well recognized (Frazzini, 2006, Grinblatt, Han, 2005, Kaustia, 2004, Weber, Welfens, 2007). For a piece of private information that only a particular trader possesses to be incorporated into market prices, the trader must initiate trades. Following positive (or negative) information, there must be sufficient buying (or selling) activities to push up (or down) prices. However, traders exhibiting the disposition effect are likely to distort this process. They sell assets following capital gains and hold assets following capital losses. In a market in which many traders are affected by the disposition effect, together with limits of arbitrage (Shleifer and Vishny, 1997), those distortions might result in insufficient buying (or selling) pressure to push the price to the correct level. As a consequence, price underreaction and subsequent return momentum arises (Dacey, Zielonka, 2008, Frazzini, 2006), and markets become less efficient.====We experimentally investigate the role of the disposition effect in market efficiency following the arrival of private information to a small group of informed traders. Our contribution is mainly threefold. First, we make an explicit distinction between the disposition effect in bull markets, where most traders experience capital gains, and the disposition effect in bear markets, where most traders make losses. For this purpose, instead of using one general measure, as in Weber and Camerer (1998) and Weber, Welfens, 2007, Weber, Welfens, 2007, we measure the disposition effect separately in the gain domain and the loss domain. Our measurement is built on prospect theory, arguably the most popular explanation of the disposition effect. Traders behaving consistently with prospect theory use the purchasing price of an asset as the reference point and code prices above it as gains and below it as losses. The S-shaped value function predicts a higher propensity to sell the asset with gains due to risk aversion in the gain domain and a stronger willingness to hold the asset with losses due to risk seeking in the loss domain, resulting a trading pattern that is consistent with the disposition effect. If prospect theory is indeed a driving force behind the disposition effect, in bull (or bear) markets where most traders have capital gains (or losses), only the disposition effect driven by the value function in the gain (or loss) domain is relevant. After all, decisions to sell winning stocks and hold or buy losing stocks are, from a behavioral point of view, fundamentally different. There is no reason to believe that traders affected by the disposition effect in bull markets are the same ones affected in bear markets. If the disposition effect affects different populations in bear markets than in bull markets, it might result in different market dynamics. Indeed, on the individual level investors “exhibiting a strong tendency to quit winning investments quickly are not necessarily the same investors who stick to their losing ones” (Weber and Welfens, 2007a, pp. 25); on the aggregate level, prices underreact strongly after a positive shock, while prices underreaction is much less pronounced following negative shocks (Weber, Welfens, 2007). Further evidence can be found in Frazzini (2006) and Weber and Welfens (2008). Consequently, it is important to distinguish the disposition effect in the gain domain and the loss domain.====Second, we consider the relationship between the disposition effect and market efficiency using private instead of public signals. The link between the disposition effect and price underreaction has been shown in, among others, Grinblatt and Han (2005), Frazzini (2006) and Weber, Welfens, 2007. The focus of those studies has been on the arrival of public information. While those studies provide important insights on the existence and implications of the disposition effect, they neglect some important points. First, as we point out in the beginning of the introduction, a primary function of markets is to aggregate diversified private information rather than public information. Hence, it is essential to know the role of the disposition effect in the translation of private information to market prices. Second, in studies relying on public signals, it is not entirely clear whether and how the disposition effect directly affects market efficiency. Following public signals, market prices generally need some time to fully reflect new information, instead of immediately jumping to the new equilibrium level, as predicted by the efficient market hypothesis. However, it is unclear what exact role the disposition effect plays in this process. The disposition effect is not defined by the ability to absorb information, and traders affected by the disposition effect might process information and adjust their trading prices as quickly as other traders. Therefore, the underreaction of market prices to public signals per se might come from factors other than the disposition effect. The results found in works such as Weber and Camerer (1998), Weber, Welfens, 2007, Weber, Welfens, 2007 might merely reflect a correlation of those factors with the disposition effect. With private information, market prices adjust slowly, and the disposition effect has a clear role in this process. Moreover, different types of signals could imply different trading dynamics. Unlike in the case of public information, the informed traders have a distinct informational advantage over the uninformed traders. They might construct trading strategies that could hide their information and exploit the information advantage for as long as possible (e.g., Buffa, 2013). Those trading strategies might lead to a potentially stronger underreaction when signals are private rather than public. It is interesting to see if and to what extent the disposition effect plays a role in those dynamics.====Finally, our investigation relies on experimental asset markets instead of empirical data. An experimental approach, in contrast to empirical methods, provides us with full control over the fundamental value of the asset, the information structure of signals, and the behavioral trait – the level of the disposition effect – of the informed traders receiving signals. Those tight controls allow us to directly test the role of the disposition effect in market efficiency without having to worry about other confounding factors, such as mean reversion in beliefs, portfolio re-balancing, and trading costs. Furthermore, by examining detailed micro-structure trading data, we are able to get a comprehensive view of the whole translation process from signals to prices and to pinpoint the steps where things go wrong.====Our experiment consists of two main parts. In the first part, we measure the level of the disposition effect separately in the gain and loss domains using a novel method. The basic task in our measurement method consists of playing an ambiguous lottery for a maximum of four rounds. The ambiguous lottery is the same in all rounds and offers either a gain or loss of 400 Experimental Currency Units (hereafter ECU). The task ends once subjects decide not to play the lottery. Consistent with the empirical definition of the disposition effect, subjects who experience a gain in playing a lottery should become less likely to proceed to next rounds, while subjects who experience a loss in playing a lottery should become more likely to proceed to next rounds. To more accurately and effectively measure the level of the disposition effect, we manipulate the outcomes of the ambiguous lottery in two out of six tasks, such that subjects face four continuous gains in one manipulated task and face four continuous losses in the other, if subjects play to the end.====In the second part of the experiment, nine subjects compose a market and trade an asset via a computer-based double auction. The final value of the asset depends on the color of a randomly drawn ball from an ambiguous bowl. It is common knowledge that three of the nine subjects are informed traders. Each market trading is divided into two phases. Subjects trade the asset for two minutes in the first phase, then three subjects – informed traders – receive the same private signal, and trading continues for another two minutes. Our central focus is on the implications of the informed traders disposition effect levels on their individual trading behavior and aggregate market performance. For this purpose, we appoint informed traders according to their levels of the disposition effect; i.e., informed traders are three subjects either with the highest or the lowest levels of the disposition effect in a market. We consider both the disposition effect in the gain domain and that in the loss domain. The signals that informed traders receive are of two types. A positive signal excludes a bad state and implies the value of the asset is more likely to be high, whereas a negative signal excludes a good state, and thus the value of the asset is more likely to be low. In the experiment, we deliberately construct markets in which the sign of the signals (positive/negative) either matches or does not match the domain in which the levels of the disposition effect are measured. This allows us to check whether the disposition effect measured in the two domains indeed has different implications. To cover the whole spectrum, we run eight treatments in total with a within-subject design so that we have: two levels (high/low) of the informed traders disposition effect levels  ×  two domains (gain/loss) in which the disposition effect levels are measured  ×  two types (positive/negative) of private signals.====Our results show that disposition effect levels measured in the two domains are not correlated, suggesting individuals exhibit higher levels of the disposition effect in bear markets than in bull markets. In line with the disposition effect, we find that informed traders with high disposition effect levels exhibit a significantly greater (or lower) willingness to hold the asset following a negative (or positive) signal than informed traders with low disposition effect levels. This willingness is evident both from the numbers and the prices of (submitted and accepted) bids and asks. Furthermore, the differences in the trading behavior result in substantially different asset holdings between informed traders with high and low disposition effect levels. At the aggregate market level, we observe significant underreaction following both types of private signals, and there is no indication that prices are approaching a new fundamental value within the trading round of the double auction market. We find some evidence that markets with informed traders with high (low) disposition effect levels experience stronger (weaker) price underreaction. This is true following both a positive and negative signal, although the difference is more obvious following a negative signal. Finally and most importantly, the above results hold only when the sign of private signals (positive/negative) matches the domain in which the disposition effect levels are measured.====The rest of the paper is structured as follows. Section 2 presents the experimental design, procedures and hypotheses. In Section 3, our results are presented and discussed. Finally, Section 4 concludes.",The disposition effect and underreaction to private information,https://www.sciencedirect.com/science/article/pii/S0165188920300269,13 February 2020,2020,Research Article,351.0
Platt Donovan,"Mathematical Institute, University of Oxford, Oxford, United Kingdom,Institute for New Economic Thinking (INET) at the Oxford Martin School, Oxford, United Kingdom","Received 22 February 2019, Revised 30 January 2020, Accepted 2 February 2020, Available online 13 February 2020, Version of Record 3 March 2020.",https://doi.org/10.1016/j.jedc.2020.103859,Cited by (38),"Despite significant expansion in recent years, the literature on quantitative and data-driven approaches to economic agent-based model validation and calibration consists primarily of studies that have focused on the introduction of new calibration methods that are neither benchmarked against existing alternatives nor rigorously tested in terms of the quality of the estimates they produce. In response, we compare a number of prominent agent-based model calibration methods, both established and novel, through a series of computational experiments in an attempt to determine the respective strengths and weaknesses of each approach. Overall, we find that a simple, likelihood-based approach to ==== estimation consistently outperforms several members of the more popular class of simulated minimum distance methods and results in reasonable parameter estimates in many contexts, with a degradation in performance observed only when considering a large-scale model and attempting to fit a substantial number of its parameters.","Recent advances in computing power, along with successes in other domains such as ecology, have resulted in the emergence of a growing community arguing that agent-based models (ABMs), which simulate systems at the level of individual agents and the interactions between them, may provide a more principled approach to the modelling of the economy, which has traditionally been studied through the lens of general equilibrium theory and a set of restrictive and often empirically-inconsistent assumptions (Fagiolo, Roventini, 2017, Farmer, Foley, 2009, Geanakoplos, Farmer, 2008). Indeed, recent decades have seen the emergence of a wide variety of economic and financial ABMs that largely dispense with the unrealistic assumptions that characterise traditional approaches in favour of more realistic alternatives rooted in empirically-observed behaviours (Chen, 2003, LeBaron, 2006). This paradigm shift has ultimately resulted in a degree of success, with ABMs becoming well-known for their ability to replicate empirically-observed stylised facts: qualitative properties that appear consistently in empirically-measured data and are not readily recovered using traditional approaches (Barde, 2016, LeBaron, 2006).====Despite the aforementioned successes, ABMs face strong criticisms of their own, focused particularly on the inadequacy of current validation and calibration practices (Grazzini and Richiardi, 2015). In the vast majority of studies, particularly those that introduce large-scale models, validation procedures are qualitative in nature and seldom venture beyond the demonstration of a candidate model’s ability to reproduce a set of empirically-observed stylised facts (Guerini, Moneta, 2017, Panayi, Harman, Wetherilt, 2013). Calibration in such investigations is equally rudimentary, and typically takes the form of manual parameter adjustments or ad-hoc processes that aim to select parameters that allow the model to reproduce the set of stylised facts considered during validation.==== While such stylised fact-centric methodologies may seem reasonable at first glance, the very large number of models able to recover a similar number of stylised facts, the wide variety of behavioural rules employed in these models, and the difficulty experienced in attempting to identify the causal effects of many behavioural rules on emergent dynamics, renders robust model comparison an impossibility when using qualitative, stylised fact-centric methods (Barde, 2016, Lamperti, Roventini, Sani, 2018, LeBaron, 2006). This leads to what is often referred to as the “wilderness of bounded rationality” problem (Barde, 2016, Geanakoplos, Farmer, 2008).====In response to these criticisms, a small, but growing literature dealing with more sophisticated quantitative validation and calibration techniques has emerged (Fagiolo et al., 2017). While significant progress has been made, particularly in the last three years, this literature still suffers from a number of key weaknesses. Firstly, it is overly-compartmentalised. By this, we mean that most studies within this research area focus on the proposal of new methods and seldom compare the proposed techniques to other contemporary alternatives. This, combined with the fact that the theoretical properties==== of many of these new techniques are not well understood (Grazzini et al., 2017), leaves the modeller with a difficult choice between a large number of methods with no obvious reason to favour one approach over another. Secondly, severe computational limitations have resulted in most techniques only ever being applied to highly-simplified models==== that are several decades old and no longer a good representation of the current state of economic agent-based modelling (Fagiolo, Guerini, Lamperti, Moneta, Roventini, 2017, Lamperti, Roventini, Sani, 2018). This leads to significant doubt regarding the applicability of current methods to the large-scale models that now dominate the literature.====We therefore aim to address the above using a principled, yet practical approach. Specifically, we compare a number of prominent ABM calibration methods, both established and novel, through a series of computational experiments involving the calibration of various candidate models in an attempt to determine the respective strengths and weaknesses of each approach. Thereafter, we apply the most promising of the considered calibration techniques to a large-scale ABM of the UK housing market in order to assess the extent to which the performance achieved in the context of simple models is maintained when confronting state-of-the-art ABMs.",A comparison of economic agent-based model calibration methods,https://www.sciencedirect.com/science/article/pii/S0165188920300294,13 February 2020,2020,Research Article,352.0
"Lütkepohl Helmut,Woźniak Tomasz","DIW Berlin and Freie Universität, Berlin,Department of Economics, University of Melbourne, FBE Building, 111 Barry Street, Carlton, VIC 3053, Australia","Received 30 September 2019, Revised 9 February 2020, Accepted 10 February 2020, Available online 13 February 2020, Version of Record 4 March 2020.",https://doi.org/10.1016/j.jedc.2020.103862,Cited by (10)," in which the structural parameters are identified via Markov-switching heteroskedasticity. In such a model, restrictions that are just-identifying in the homoskedastic case, become over-identifying and can be tested. A set of ==== restrictions is derived under which the structural matrix is globally or partially identified and a Savage–Dickey density ratio is used to assess the validity of the identification conditions. The latter is facilitated by analytical derivations that make the computations feasible and numerical standard errors small. As an empirical example, monetary models are compared using heteroskedasticity as an additional device for identification. The empirical results support an identified ==== reaction function with money.","A central problem in structural vector autoregressive (SVAR) analysis is the identification of the structural parameters or, equivalently, the identification of the structural shocks of interest. The identifying assumptions are often controversial. In order to avoid imposing unnecessarily many restrictions, typically only just-identifying restrictions are formulated. In that case the data are not informative about the validity of the restrictions and they cannot be tested with statistical methods. Moreover, even if over-identifying restrictions are imposed, they can only be tested conditionally on a set of just-identifying restrictions. This state of the art has led some researchers to extract additional identifying information from the statistical properties of the data. Notably heteroskedasticity and conditional heteroskedasticity of the reduced form residuals have been used in this context (Rigobon, 2003). Using such additional information may enable the researcher to make the data speak on the validity of restrictions that cannot be tested in a conventional framework.====One model that has been used repeatedly in applied studies lately to capture heteroskedasticity is based on a latent Markov process that drives the changes in volatility. The model was first proposed by Lanne et al. (2010) for SVAR analysis with identification through heteroskedasticity and it was further developed by Herwartz and Lütkepohl (2014). The SVAR model with Markov-switching heteroskedasticity (SVAR-MSH) is in widespread use (see, e.g., Chen, Netšunajev, Lütkepohl, Netšunajev, 2014, Lütkepohl, Netšunajev, 2017, Lütkepohl, Velinov, 2016, Netšunajev, 2013, Velinov, Chen, 2015 and Kilian and Lütkepohl, 2017, Chapter 14). Some Bayesian methodology has been developed for its analysis by Kulikov and Netšunajev (2013), Lanne and Luoto (2016) and Woźniak and Droumaguet (2015). Apart from Woźniak and Droumaguet (2015), all Bayesian approaches base inference for these models on draws from the posterior of the reduced form parameters and transform this output into the posterior draws of the structural model identified through heteroskedasticity. Hence their methodology can only be used to generate posterior draws for just-identified structural parameters which limits its applicability when over-identifying restrictions are of interest. Woźniak and Droumaguet (2015) focus on a locally identified SVAR-MSH model and develop methods for drawing from the posterior of the structural parameters. The posterior distribution of the parameters of a locally identified model is multimodal, however, which allows a statistical model comparison, but severely limits the analysis of the structural parameters.====In the present study, a full Bayesian analysis framework is presented based on a SVAR-MSH model where some or all equations are identified. The setup facilitates both of the objectives mentioned above. We emphasize that our setup allows for the possibility that only some of the structural equations and associated structural shocks are identified. In the SVAR literature it is not uncommon that only the responses to a single shock or a small set of shocks are of interest. For example, in a monetary model, the monetary policy shock is often of primary interest. In that case, it makes sense to focus on the structural parameters associated with that shock only. Our approach allows us to handle that situation even if the other shocks are not properly identified.====Our main additional contributions to the SVAR-MSH literature for identification through heteroskedasticity are as follows:====The methods are illustrated by applying them for an empirical analysis of the role of a Divisia money aggregate in a monetary policy reaction function. In a frequentist SVAR analysis, Belongia and Ireland (2015) find support for the hypothesis that Divisia monetary aggregates are important variables in the monetary policy rule. In their conventional SVAR models without accounting for heteroskedasticity they can only test over-identifying restrictions to validate their hypotheses. Using Belongia and Ireland (2015) as a benchmark, the Bayesian methods developed in the current study for the SVAR-MSH model are applied for a broader statistical analysis of the identifying restrictions even for models that are not identified in Belongia and Ireland’s framework. We find evidence that a money aggregate is an important factor determining the monetary policy.====The remainder of this study is organized as follows. The next section presents the basic model framework and derives conditions for identification of the structural parameters. Section 3 discusses the prior assumptions used for the structural parameters. The SDDR procedure for investigating the conditions for identification of the structural parameters obtained from the volatility model is presented in Section 4 and the empirical illustration is discussed in Section 5. Conclusions follow in Section 6 and, finally, the proof of a result regarding identification through heteroskedasticity is given in Appendix A, the computational details of the Gibbs sampler and the estimation of the marginal data densities are presented in Appendix B, while Appendix C contains more details on the distribution used in the SDDR procedure. Additional empirical results on the precision of our estimates are presented in Appendix D.",Bayesian inference for structural vector autoregressions identified by Markov-switching heteroskedasticity,https://www.sciencedirect.com/science/article/pii/S0165188920300324,13 February 2020,2020,Research Article,353.0
"Kukacka Jiri,Kristoufek Ladislav","The Czech Academy of Sciences, Institute of Information Theory and Automation, Pod Vodarenskou vezi 4, 182 00 Prague 8, Czechia,Faculty of Social Sciences, Institute of Economic Studies, Charles University, Opletalova 26, 110 00 Prague 1, Czechia","Received 25 July 2019, Revised 15 December 2019, Accepted 26 January 2020, Available online 7 February 2020, Version of Record 24 February 2020.",https://doi.org/10.1016/j.jedc.2020.103855,Cited by (39),"Agent-based models are usually claimed to generate complex dynamics; however, the link to such complexity has not been subject to rigorous examination. This paper studies this link between the complexity of financial time series—measured by their ==== properties—and the design of various small-scale agent-based frameworks used to model the heterogeneity of financial markets. Nine popular models are analyzed, and while some of the models do not generate interesting multifractal patterns, we observe the strongest tendency towards multifractal behavior for the Bornholdt Ising model, the discrete choice-based models by Gaunersdorfer & Hommes and Schmitt & Westerhoff, and the transition probabilities-based framework by Franke & Westerhoff. Complexity is thus not an automatic feature of the time series generated by any agent-based model but generated only by models with specific properties. In addition, because multifractality is considered a financial stylized fact, its presence can be used as a new means to validate such models.","Financial markets have proven to be very complicated systems of competing and interacting economic agents with heterogeneous motives, strategies and investment horizons. Such interplays create highly complex dynamics that are hard to model and predict with the linear models that play an essential role in financial economics and related fields (Chen, Chang, Du, 2012, Hu, Wen, Rahmani, Yu, 2019, Shang, 2014). Nonlinear agent-based models, usually built on simple heuristic rules of agents’ behavior, have become popular in various fields of the social sciences over the last two decades, as they provide a more realistic description of society and its functioning. In economics and particularly in finance, these models have been shown to successfully replicate the traditional stylized facts of financial markets (Cont et al., 2007). Not only are financial agent-based models able to mimic the statistical and dynamic properties of financial time series; they also allow for a more detailed discussion of the connection between their construction design and the system dynamics they produce, potentially including multifractal behavior of financial time series.====Multifractality is a statistical characteristic of a sequence or time series connected to its complex scaling properties. Extending the fractality characteristic by describing scaling properties of the series with a single exponent, multifractality in a series explains complex behavior by a spectrum of exponents. This leads to possibly very complex dynamics of the series that are difficult to describe but interesting for a broad range of disciplines, including hydrometeorology (Veneziano et al., 2006), medicine (Lopes and Betrouni, 2009), image processing (Arneodo et al., 2000), geochemistry (Zuo and Wang, 2016), geology (Dellino and Liotino, 2002), climate change (Ashkenazy et al., 2003) and others. One of the very fruitful fields of study with respect to multifractality is finance. In addition to many empirically oriented studies (Barunik, Aste, Matteo, Liu, 2012, Di Matteo, 2007, Di Matteo, Aste, Dacorogna, 2003, Di Matteo, Aste, Dacorogna, 2005, Liu, Matteo, Lux, 2007, Lux, Kaizoji, 2007, Zhou, 2009), there have been many statistically oriented ones focusing on possible sources of multifractality in financial time series including stock markets, foreign exchange rates, interest rates, commodities, and others, specifically their returns, volatility, volume, and liquidity.====In this vein, it has been shown that various statistical and dynamic characteristics can be identified as sources of ‘real’ or ‘apparent’ multifractality. Among these, there are fat tails (Chechkin, Gonchar, 2000, Nakao, 2000), distribution broadness (Zhou, 2009), finite sample size (Grech, Pamuła, 2012, Grech, Pamuła, 2013, Pamuła, Grech, 2014), and various correlation structures (Buonocore, Aste, Matteo, 2016, Kantelhardt, 2009, Kantelhardt, Zschiegner, Koscielny-Bunde, Bunde, Havlin, Stanley, 2002, Theiler, Eubank, Longtin, Galdrikian, Farmer, 1992). Much space has thus been given to exploring statistical and econometric sources of multifractality, and these sources can be seen as well understood, as documented in a recent review by Jiang et al. (2018). Multifractality has actually been labeled a new stylized fact of the financial markets (Lux and Segnon, 2018) in addition to the standard stylized facts listed by Cont (2001) such as the vanishing of the returns autocorrelation, volatility clustering, or non-Gaussian and fat-tailed distributions of the returns. However, little to no attention has been given to structural, qualitative sources of multifractality in financial time series.====As the multifractal framework has served as a basis for various models (Bacry, Kozhemyak, Muzy, 2008, Calvet, Fisher, 2001, Lux, 2008, Mandelbrot, Fisher, Calvet, 1997), we suggest that the presence of multifractality can potentially be used as one of the new means of validation of financial agent-based models in general (Fagiolo, Guerini, Lamperti, Moneta, Roventini, 2019, Lux, Zwinkels, Hommes, LeBaron, 2018, Platt, 2019, Windrum, Fagiolo, Moneta, 2007). This paper thus focuses on a connection between multifractality and complexity of financial time series and examines multifractality as its measure. As complexity is a very broad term in the context of time series modeling, we here specify it as a complicated nonlinear correlation structure originating from the heterogeneity of financial markets. Heterogeneity of markets is here synonymous with heterogeneity of market participants, specifically their trading strategies, investment horizons and sensitivity to local and/or global influences.====Nine popular elementary financial agent-based models are analyzed. The list of the ‘model zoo’ begins with one of the oldest small-scale models in the literature introducing the concept of behavioral heterogeneity among financial market participants, the cusp catastrophe model by Zeeman (1974); this model is followed by the seminal Brock and Hommes (1998) heterogeneous agent model that is further extended by Gaunersdorfer and Hommes (2007), which dynamics are governed by the multinomial logit discrete choice formula for the adaptive switching of economic agents between available trading strategies. The spin model by Bornholdt (2001) parallelizes a financial market to the ferromagnetic model, and the model by Gilli and Winker (2003) built in the ‘ant dynamics’ tradition of Kirman (1993) introduces the concept of transition probabilities. Models by Alfarano, Lux, Wagner, 2005, Alfarano, Lux, Wagner, 2008 provide extended concepts of modeling financial herding based on the asymmetric transitions probabilities and the Langevin equation approximation. Finally, we study the recent concept by Franke and Westerhoff (2011) that extends the famous ‘interactive agent hypothesis’ with a structural volatility component; one of the most recent approaches by Schmitt and Westerhoff (2017) is a computationally simplified version of a large-scale ‘structural volatility’ model with mutually dependent individual stochastic terms and the occurrence of sunspots that may trigger extreme events. By connecting these models with the multifractal framework, we verify the existence of multifractal behavior within artificial agent-based financial markets. Moreover, using predominantly the simulation-based approach, we elegantly bypass complications with parameter estimation (Grazzini, Richiardi, 2015, LeBaron, Tesfatsion, 2008, Recchioni, Tedeschi, Gallegati, 2015).====The model contest paper by Franke and Westerhoff (2012) might, to some extent, be considered a conceptual precursor to our multifractal analysis. The authors, however, concentrate specifically on so-called ‘structural stochastic volatility’ models under the transition probabilities-based and the discrete choice-based approach. The main goal of the analysis then lies in their replication capability of the important stylized facts and empirical moments within the framework of the simulated method of moments.====An extensive Monte Carlo simulation study utilizes the toolkit of the multifractal detrended fluctuation analysis (MF-DFA), the multifractal detrended moving average method (MF-DMA), and the generalized Hurst exponent method (GHE) estimation. To distinguish whether a potential multifractality is due to agent-based linear and nonlinear correlation structure or distributional properties of the underlying process, we compare the result of the original series with the randomly shuffled series. While some models mostly appear to suffer from a small-size effect potentially influencing multifractal properties and do not generate other interesting multifractal patterns, we observe the strongest tendency towards multifractal behavior for the Bornholdt (2001) Ising model, the discrete choice-based models by Gaunersdorfer and Hommes (2007) and Schmitt and Westerhoff (2017), and the transition probabilities-based framework by Franke and Westerhoff (2011).",Do ‘complex’ financial models really lead to complex dynamics? Agent-based models and multifractality,https://www.sciencedirect.com/science/article/pii/S0165188920300257,7 February 2020,2020,Research Article,354.0
"Chorro Christophe,Ielpo Florian,Sévi Benoît","University Paris 1 Panthéon-Sorbonne - CES, Maison des Sciences Économiques, 106-113 Boulevard de l’Hôpital, Paris 75013, France,Centre d’Économie de la Sorbonne, Paris 1 University and Labex Réfi, France,Unigestion SA, Avenue de Champel 8C, Geneva 1211, Switzerland,LEMNA, Université de Nantes, France","Received 16 July 2019, Revised 11 November 2019, Accepted 13 January 2020, Available online 5 February 2020, Version of Record 4 March 2020.",https://doi.org/10.1016/j.jedc.2020.103853,Cited by (1),"Recent contributions highlight the importance of intraday jumps in forecasting realized volatility at horizons up to one month. We extend the methodology developed in Maheu and McCurdy (2011) to exploit the information content of intraday data in forecasting the density of returns. Considering both intra-week periodicity and signed jumps, we estimate two variants of a ==== model of returns and volatilities where the jump component is independently modeled. Our empirical results for four futures series (S&P 500, U.S. 10-year Treasury Note, USD/CAD exchange rate and WTI crude oil) highlight the importance of considering the continuous/jump decomposition of volatility for the purpose of density forecasting. Specifically, we show that models considering jumps apart from the continuous component consistently deliver better density forecasts for horizons up to one month and a half and, in two cases out of four, for horizons up to three months.","The extraction of the jump component in the dynamics of asset prices has witnessed a considerably growing body of literature. Of particular interest is the decomposition of realized volatility between its continuous and jump components. In a reference paper, (Andersen et al., 2007a) provide convincing empirical evidence that disentangling jumps from the continuous component significantly improves realized volatility point forecast at horizons up to a trading month. The explanation for this result lies in the strong persistence in the continuous component and the absence of autocorrelation in the jump component.====Forecasting realized volatility has shown to be critical in empirical finance applications such as portfolio choice (Chou, Liu, 2010, Fleming, Kirby, Ostdiek, 2003, Liu, 2009, Liu, Maheu, 2009), risk management (Bali, Weinbaum, 2007, Clements, Galvao, Kim, 2008) or derivatives pricing (Alitab et al., 2019, Christoffersen, Feunou, Jacobs, Meddahi, 2014, Corsi, Fusari, La Vecchia, 2013). Useful for these applications, volatility point forecasts, the often traditional focus, are better seen as the central points of ranges of uncertainty. Consequently, to provide a comprehensive description of the uncertainty associated with the point forecast, many professional forecasters and central banks now publish density forecasts, or more popularly fan charts. In contrast to interval forecasts, which give the probability that the outcome will fall within a stated interval, density forecasts provide a full description of the uncertainty associated with the forecast.==== Recently, Hansen et al. (2011), Maheu and McCurdy (2011), Shephard and Sheppard (2010) and Liu and Maheu (2018) have suggested “complete” models of returns and volatility. In these models, returns and volatility are modeled simultaneously, thereby allowing density predictions for returns once assumptions are made about the conditional distribution of returns. In particular, Maheu and McCurdy (2011) propose a bivariate specification of returns and volatility and confirm, in the density forecasting context, the overall finding that intraday data enhance predictions.====Our paper is the first to merge these two strands of the recent literature on realized volatility and investigates whether the separation between the continuous and the jump components is of importance in predicting the density of returns. By using intraday data, it is possible to detect and extract jumps as the difference, when statistically significant, between realized volatility and a measure of realized volatility that is robust to jumps. This decomposition enables to embed jumps for forecasting purposes. We rely on the link between the conditional variance and the realized volatility provided in Andersen et al. (2003) to estimate five different nested models. The motivation behind considering jump-robust measures for realized volatility is that they simply have better predictive properties than non-jump-robust ones (see Shephard and Sheppard, 2010). Our empirical results for futures series of four different asset categories (financial index, sovereign bond, exchange rate and commodity) strongly argue in favor of separating and then modeling the two components when forecasting the density of returns up to three months. As such, we show that disentangling jumps from the continuous component do help in forecasting the density of returns.====In the general empirical framework developed in Maheu and McCurdy (2011), our approach builds on the parsimonious Heterogeneous Autoregressive (HAR) model developed by Corsi (2009) to capture the well-known long-memory dependence in volatility. The model has shown to be very successful in numerous applications dealing with the dynamics of the realized volatility (see Corsi et al., 2012). As for the detection of jumps, we proceed with the test in Huang and Tauchen (2005) that we adapt to the median realized volatility (MedRV) proposed in Andersen et al. (2012) which is a jump-robust measure of realized volatility. Our choice for the MedRV measure is motivated by the empirical work in Theodosiou and Zikes (2011) and Dumitru and Urga (2012) who show that MedRV is, in most cases, a good alternative to the well-know bipower variation suggested earlier in Barndorff-Nielsen and Shephard (2004) which suffers from lower performances in finite samples. We extend the bivariate model in Maheu and McCurdy (2011) in allowing for a leverage effect whose usefulness is empirically demonstrated in our application. Finally, the comparisons between the different bivariate specifications for daily returns and realized volatilities are conducted using the predictive likelihood tests of Amisano and Giacomini (2007) which has the interesting property to focus on the whole distribution and provides, as such, a general measure of the density forecast soundness.====This paper makes essentially two contributions to the literature. First, we extend the framework of Maheu and McCurdy (2011) to show how jumps can be accounted for in their bivariate model. Critical to our approach is the adequacy of the conditional jump distribution used for forecasting purpose with the empirical distribution of jumps. Second, we investigate the information content of intraday jumps when it comes to forecasting the density of returns. We do so primarily through a thorough comparison of the performance of models based on jump-robust and non-jump-robust (naïve) measures of realized volatilities. Compared to the naïve measure of realized volatility, recognizing jumps provides a statistically significant improvement for horizons up to 30 days for the four futures series. We thus extend the seminal results in Andersen et al. (2007a)==== showing the importance of disentangling jumps from the continuous component to forecast not only the realized volatility but also the distribution of returns. Hence, our results potentially have practical implications for activities such as portfolio choice, construction of risk measures and derivatives pricing as these activities might benefit from improved return density forecasts. As an illustration, we provide in the penultimate Section a forecasting exercise of the daily Value-at-Risk (VaR) for the S&P 500 futures and provide evidence that models including a specific jump component perform significantly better in this context.====The remainder of the paper is organized as follows. Section 2 summarizes the construction of the volatilities and jumps data sets. Section 3 details our choice of an adequate distribution for jumps. In Section 4, we present our modeling strategy as well as the methodology used to compare density forecasts. Section 5 discusses the empirical results, which are used in a short experiment to gauge the economic value or our findings through a VaR forecast exercise in Section 6. Finally, Section 7 provides concluding remarks.",The contribution of intraday jumps to forecasting the density of returns,https://www.sciencedirect.com/science/article/pii/S0165188920300233,5 February 2020,2020,Research Article,355.0
"Bognanni Mark,Zito John","Research Department, Federal Reserve Bank of Cleveland, PO Box 6387, Cleveland, OH 44101-1387, USA,Department of Statistics, Rice University, PO Box 1892, Houston, TX 77251-1892, USA","Received 8 August 2019, Revised 2 January 2020, Accepted 6 January 2020, Available online 5 February 2020, Version of Record 26 March 2020.",https://doi.org/10.1016/j.jedc.2020.103851,Cited by (4),We develop a ==== with ,"Over the last 40 years the vector autoregression (VAR) has become a benchmark tool in empirical macroeconomics. Among the various extensions to the VAR, stochastic volatility (SV) has perhaps been the most robustly successful at improving forecasting and model fit. The demonstrated preference of standard macroeconomic time-series for including some form of SV in the VAR has been evident at least since Primiceri (2005) and Sims and Zha (2006).==== In a particularly thorough recent example, Clark and Ravazzolo (2015) show that a wide variety of stochastic volatility specifications robustly improve VAR forecasting performance.==== However, while the gains from including stochastic volatility in the VAR are now well documented, so too are the challenges of VAR-SV model estimation. Most practitioners analyze the model from a Bayesian perspective, but unlike with a constant-parameter VAR, the Bayesian posterior distribution of the VAR-SV cannot be fully characterized analytically. Simulation-based computational methods are then the key tool for posterior inference.====This paper’s primary contribution is to develop a parallelizable algorithm for sequential Bayesian estimation of VAR-SV models. By “sequential” we mean that when new data are incorporated into the model estimates, an approximation to the new posterior adapts the approximation to the old posterior. Importantly, our proposed approach remains fully Bayesian in the sense that inference is based on the model’s full likelihood function and the algorithm can estimate posterior moments arbitrarily accurately in the limit of certain algorithm settings. The upshot of our estimation algorithm is that, given an approximation of the posterior at time ====, the approximation of the posterior at time ==== can be obtained an order of magnitude faster than under the extant approach in the literature. We demonstrate the effectiveness of our estimation approach by applying it to a seven-variable “medium scale” VAR-SV, in which our sequential algorithm yields more precise estimates than when using the extant Markov chain Monte Carlo (MCMC) algorithm alone, while also dramatically decreasing computation time.====Our estimation method is fundamentally a sequential Monte Carlo (SMC) algorithm. Sequential Monte Carlo algorithms are particle-based methods that approximate a sequence of densities of interest with discrete approximations based on weighted samples. In our setting, the relevant densities to approximate are the sequence of VAR-SV posteriors as the available time-series of data expands. An important practical aspect of SMC, and one that we heavily exploit, is that the most computationally intensive parts of the algorithm can be executed in parallel.====Our SMC implementation leverages the known structure of the VAR-SV posterior in two important ways in order to tackle the notoriously difficult problem of simultaneous sequential inference for both static parameters (the linear VAR coefficients) and dynamic parameters (latent volatility states). First, when updating the particles from one stage to the next, we use the fact that one can analytically marginalize the static parameters, at which point we are effectively executing the algorithm in a parameter space of reduced dimension while still accounting for the posterior of the static parameters.==== Second, when rejuvenating particle diversity, a stage of SMC called “mutation,” we use a known MCMC algorithm specifically tailored to the VAR-SV. The MCMC algorithm is of a relatively efficient variety known as a Gibbs sampler, which makes our algorithm considerably more effective at rejuvenating meaningful particle diversity compared to more naive alternatives. The potential utility of such approaches has long been discussed within the SMC literature, in which SMC and MCMC are sometimes referred to as “complementary.”==== In practice, however, SMC algorithms used in the literature are usually implemented with only relatively naive mutation kernels. Our paper shows, in a practical and empirically relevant example, the usefulness of folding a relatively more effective MCMC kernel into SMC when such a kernel is available.====To contextualize our contribution, it is helpful to understand the standard method of Bayesian inference for the VAR-SV. To date, fully Bayesian inference in the VAR-SV is conducted by means of simulating random samples from a Markov chain Monte Carlo (MCMC) algorithm. The key strength of the MCMC algorithm is that it “works” in the formal sense that, asymptotically in the number of iterations of the Markov chain, posterior moments of interest can be estimated arbitrarily accurately. Indeed this fact motivates our inclusion of the MCMC algorithm into our approach as the mutation kernel. However, MCMC also has two key weaknesses that our SMC algorithm addresses. The first weakness is that MCMC samples must be generated serially. This is a simple consequence of the fact that the distribution of the ====th random sample from a Markov chain conditions on the ==== random sample. Hence, the MCMC algorithm cannot be cleanly parallelized, and the only way to increase its estimation accuracy is to simply let it run longer. In contrast, key steps of our SMC algorithm can be executed in parallel. This means the SMC algorithm’s accuracy can be improved by making use of additional CPUs ====.====The second weakness is that any change to the information set, such as the arrival of a new data point, alters the joint posterior distribution of all model parameters, but the MCMC algorithm has no notion of an incremental update to existing posterior samples. Rather the MCMC algorithm must simply be rerun from scratch using the new data set.==== In a production environment, the model parameters would ideally always be estimated from the most recent, and most complete, information set to yield the best possible forecasts and analysis. Hence, it would be a substantial nuisance that a satisfactory run of the MCMC algorithm for the seven-variable model takes three hours. In contrast, the key benefit of our algorithm is that it can rapidly update estimates from a previous period.====Lastly, this paper also contributes by demonstrating the practical viability of using publicly available resources to implement high performance computing tasks. This is important because our algorithm performs best when using computing hardware that few researchers have physically available in their offices. However, our implementation uses only resources, including the computing hardware, that are readily accessible to every researcher with an internet connection. To be more specific, the key features of the computational environment we use are threefold: (1) the computer hardware was remotely provisioned from a public cloud service, namely, Amazon Web Services; (2) our programs are written in the ==== language, which is open source and distributed under the MIT License; and (3) the parallelism is implemented using only “high level” ==== commands.==== With regard to the estimation algorithm, our paper clearly builds off of the vast literatures on SMC and particle filters. One can find excellent overviews of SMC methods in Doucet et al. (2001) and Doucet and Johansen (2011). With discussions more targeted to the interests of economists, Creal (2012) and Herbst and Schorfheide (2015) also provide thorough overviews. Our algorithm is also closely related to the sequentially adaptive Bayesian learning (SABL) algorithm, which is also based on SMC, used in Durham and Geweke (2014) and Durham et al. (2019).====Somewhat more specifically, our interest in sequential estimation of both static and dynamic unobservables is related to the work of Storvik (2002), Fearnhead (2002), and Djurić and Miguez (2002), all of which are also aimed at circumventing the poor performance of a more naive SMC approach of simply augmenting the state vector with the unknown static parameters and use a particle filter to estimate the augmented state. Such an approach is known to degenerate rapidly.==== Our approach differs from these by integrating Rao–Blackwellization into a key stage of the algorithm.====From here the rest of the paper proceeds as follows. In Section 2 we introduce the VAR-SV model and describe some of its key analytical properties. In Section 3 we describe our sequential Monte Carlo. In Section 4 we apply our estimation method to a seven-variable VAR-SV and rigorously document the algorithm’s performance relative to using MCMC alone. In Section 5 we describe some additional details of our computational environment. In Section 6 we conclude.",Sequential Bayesian inference for vector autoregressions with stochastic volatility,https://www.sciencedirect.com/science/article/pii/S016518892030021X,5 February 2020,2020,Research Article,356.0
"Gomes Diego B.P.,Iachan Felipe S.,Santos Cezar","Alberta School of Business, University of Alberta, Canada,FGV EPGE–Escola Brasileira de Economia e Finanças, Brazil,Banco de Portugal, Portugal","Received 25 September 2019, Revised 8 January 2020, Accepted 17 January 2020, Available online 30 January 2020, Version of Record 15 February 2020.",https://doi.org/10.1016/j.jedc.2020.103854,Cited by (9),"We study labor earnings dynamics in a developing economy with a large informal sector. We use nationally representative Brazilian panel data that cover both formal and informal workers. We document large disparities in earnings fluctuations faced by these segments of the labor market, as well as the high frequency of transitions between them. Informality is associated with more volatile earnings, while workers in the formal sector are subject to significant downside risk. Transitions between formal and informal employment bring large asymmetric earnings shocks and have a frequency that depends on age and the initial earnings level.","Uninsurable earnings risk influences many individual decisions and takes center stage in contemporary Macroeconomics. In the volatile economies of developing countries, it is reasonable to expect these risks to be large and impactful. While a good match with an expanding firm can translate into accelerated accumulation of experience and wage gains for a worker, a negative shock, such as a layoff from a company experiencing waning economic activity, can lead to a long-term loss of formal employment and a difficulty in ever recovering the same level of income.====Recent research with large panel datasets has documented a rich set of features for the empirical behavior of labor earnings risk.==== Income risk is pervasive, but the availability of appropriate data has been constrained most learnings to a select group of developed economies. In this paper, we document key facts about labor earnings dynamics in a developing economy. We use Brazilian panel data from the first quarter of 2012 through the second quarter of 2018 that cover the whole country. As in countries with similar income levels, informality plays a large role in the labor market, and a failure to account for it leads to a biased market description.====An informal worker is defined as someone whose employment record is not registered through the country’s social security systems.==== As such, there is no compliance with statutory labor rights and obligations. For example, there is no enforcement of the mandated employer contributions to social security, no layoff compensation, no access to unemployment insurance upon job separation, and, potentially, no paid time off. While on the one hand, conditions for informal workers are generally considered to be worse than in the formal sector, on the other hand, informal employment can offer an alternative which is especially valuable to some groups of workers, such as young people entering the job market or recently laid-off workers.====We first document the high transition rates between the formal and informal sectors. We also show how these transitions vary with worker characteristics. For instance, young workers originally employed in the informal sector are more likely to find a formal job compared to older workers. We then study several higher-order moments of the distribution to account for heterogeneity in labor earnings risk across age groups, current income level, and status of employment, and to describe the likelihood of transitions between these. We find many striking differences between the nature of the earnings risk faced by workers under formal and informal labor arrangements.====Formal sector workers typically enjoy both higher levels of earnings and less volatile innovations. The standard deviation of log-earnings innovations==== is 0.46 log-points for workers who are initially in the formal sector, while it is 0.73 for workers who are in the informal sector. However, the nature of the risks each set of workers faces is markedly different.====A transition from the formal to the informal sector is a significant negative shock: on average, a worker who switches to informality loses 0.28 log-points of net earnings from one year to the next. These innovations are reasonably heterogeneous (a conditional standard deviation of 0.70) and also significantly left-skewed (-0.74), emphasizing the role of further downside risk even within the group that suffers the negative shock. In the other direction, a switch from the informal sector to formal employment presents a somewhat mirror reflection of these shocks. A worker typically gains 0.18 log-points, with a distribution that has a conditional standard deviation of 0.67 and is positively skewed (0.51).====We also document that earnings innovations show large asymmetries and kurtosis and study how the distribution of these shocks depends on individual characteristics, such as the level of income and age. Our main finding is that transitions across sectors are very frequent (approximately 15% of workers change their employment form from one year to the next) and age dependent. Young workers are much more likely to transition between sectors. Also, it is typically the low-earning workers from the formal sector who fall into informality most easily, while high-earnings workers from the informal sector that most frequently switch to formal employment.====Finally, we extend the analysis to study unemployment and business cycle patterns. We find evidence that informal employment acts as a buffer against unemployment: transitions between unemployment and the informal sector are more likely than between unemployment and the formal sector. This is especially true during recessions. However, even though these transitions exhibit business cycle variation, conditional on the sector, the distributions of labor income innovations are quite acyclical.====This paper connects to four strands of the literature. First, there are studies documenting labor earnings risk in a variety of countries. Meghir and Pistaferri (2011) provide a survey of the literature studying the United States, the country with the absolute majority of articles. Other developed countries have also been studied, like the U.K. (Dickens, 2000), Canada (Baker and Solon, 2003), Italy (Cappellari, 2004), Spain (Alvarez, 2004), and Germany (Krebs and Yao, 2016). Given the context of these economies, these works do not focus on the differences between formal and informal sectors.====A second set of recent papers uses rich micro data sets to study nonlinear and higher-order aspects of labor earnings risk, such as Arellano et al. (2017), De Nardi et al. (2019), and Guvenen et al. (2019). We also document high-order risk, but add to this literature by focusing on the importance of both within-sector earnings risk and frequent risky sectoral transitions.====There are also papers that work with Brazilian data specifically, as we do here. Alvarez et al. (2018) study the recent decrease in labor earnings inequality among formal workers in Brazil. Engbom and Moser (2018) relates the rise of the minimum wage in Brazil to the decrease in income inequality. Menezes-Filho et al. (2008) documents changes in wage compensation in the country. Some papers also look at the relationship between the Brazilian trade liberalization episode and income inequality; e.g., Adão (2016) and Dix-Carneiro and Kovak (2015). We contribute to this literature by documenting differences in wage shocks experienced by both formal and informal workers.====Finally, there is also a literature that focuses directly on informality. Meghir et al. (2015) studies informal firms in Brazil. Ulyssea (2018) allows informal firms to coexist with formal firms that may hire informal workers. Both papers include workers who are not subject to income shocks. Other papers focus on the relationship between informality and trade (Coşar et al., 2016), tax collection and productivity (Ordóñez, 2014, De Paula and Scheinkman, De Paula, Scheinkman, 2010, De Paula, Scheinkman, 2011), job creation and destruction (Bosch and Esteban-Pretel, 2012), economic development (La Porta and Shleifer, 2008), and regulation (Rocha et al., 2018). None of these papers focuses on differences in labor earnings risk across formal and informal sectors, as we do.====This paper is organized in four additional sections besides this introduction. Section 2 describes the data and the construction of the earnings measure. Section 3 discusses the empirical findings, and Section 4 reports additional analyses regarding unemployment and business cycle variation. Finally, Section 5 concludes.",Labor earnings dynamics in a developing economy with a large informal sector,https://www.sciencedirect.com/science/article/pii/S0165188920300245,30 January 2020,2020,Research Article,358.0
Horvath Jaroslav,"Department of Economics, University of New Hampshire, 10 Garrison Avenue, Durham, NH 03824, USA","Received 6 August 2019, Revised 6 December 2019, Accepted 6 January 2020, Available online 30 January 2020, Version of Record 12 February 2020.",https://doi.org/10.1016/j.jedc.2020.103852,Cited by (0),"Not necessarily. I provide evidence that advanced countries’ equity premium and consumption growth differ significantly from those of emerging countries. I then estimate distinct disaster risk parameters for these two country groups. My ==== demonstrates that in some aspects advanced countries are more exposed to disaster risk, while in others their exposure is smaller. Disasters are estimated to be more severe and uncertain in advanced countries, but are on average less persistent. Advanced countries are also more likely to experience a global disaster, whereas disasters in emerging countries tend to be more idiosyncratic. I show that country-group heterogeneity in disaster length and magnitude has the largest impact on equity premium.","Rietz (1988), Barro (2006), and Barro and Ursua (2008a), Barro and Ursua (2008b) report evidence that the long-standing equity premium puzzle, discovered by Mehra and Prescott (1985), can be explained as compensation for the risk of rare events that lead to “disastrous” falls in income and consumption. The work of Barro (2006) and Barro and Ursua (2008a) Barro and Ursua (2008b) assumes that the fall in output and consumption during disasters is permanent and instantaneous – occurs over one period (year). Nakamura et al. (2013) extend their work by allowing for a multi-period nature of disasters and for disaster recoveries, and conclude that disaster risk is still important in resolving the equity premium puzzle. Their analysis delivers a coefficient of relative risk aversion that is lower than in models without disaster risk, but at 6.4, is higher than in Barro (2006) and Barro and Ursua (2008a) Barro and Ursua (2008b).====In this paper, I extend the disaster risk framework, in particular the model of Nakamura et al. (2013), to address another concern: it assumes a common risk premium and a common probability and magnitude of disasters across all countries. The extension is motivated by documenting that, in the long-run international data, the consumption growth and equity premium in emerging market economies (EMEs) are significantly different from their counterparts in advanced economies (AEs).==== More specifically, consumption growth in AEs exhibits, on average, larger kurtosis (“fatter tails”), while the average equity premium is found to be considerably larger in EMEs – I find the average equity premium to be 4.9 and 9.7 percent in AEs and EMEs, respectively.====This evidence leads me to estimate distinct disaster risk parameters for these two country groups. Like Nakamura et al. (2013), I use a Bayesian analysis and allow for both permanent and temporary disasters that can unfold over multiple years. But, my analysis uses a larger and longer sample of countries and estimates group-specific disaster magnitudes and probabilities.==== In addition, my model accounts for correlation in the timing of both permanent and temporary disaster drops in consumption within each group of countries.====The Bayesian analysis reveals that in some dimensions AEs are more exposed to disaster risk, while in other dimensions their exposure is smaller. More specifically, my empirical estimates demonstrate that disasters are larger (in magnitude) and more uncertain in AEs. However, EMEs face, on average, a higher probability of entering a disaster on their own – they are six times more likely to enter an individual disaster – and exhibit a larger disaster persistence – a typical disaster lasts on average 50 percent longer. In addition, the Bayesian estimation finds a large correlation of permanent consumption drops in disasters across AEs and of temporary consumption drops in disasters across EMEs. Overall, the estimation results show that AEs tend to enter global events with more disastrous consequences such as the Great Depression, while EMEs more frequently experience isolated disasters, such as civil wars and coups, which are usually of a smaller magnitude.====I show that the distinct exposure of AEs and EMEs to disaster risk has several implications for asset prices. As in earlier studies, I adopt an endowment economy with a representative consumer that has Epstein–Zin–Weil (EZW) preferences in each country group. Given the estimated disaster parameters, matching the equity premia in the group of advanced and emerging countries requires a coefficient of relative risk aversion (CRRA) of 4.4 and 6.2, respectively. The lower CRRA value in AEs than in EMEs is in line with empirical evidence provided by Falk et al. (2018), who use “an experimentally validated survey data set” to measure risk taking behavior in individual countries. Nakamura et al. (2013) arrive at a CRRA of 6.4 to match the mean of the observed equity premia across all countries in their sample. Barro and Jin (2016) require a CRRA value of 5.9 when jointly estimating disasters and long-run risks, first introduced by Bansal and Yaron (2004), in a unified framework. Different from previous studies, my work allows for disaster contagion and importantly for country-group heterogeneity in disaster parameters and equity premia. This yields the fact that to match the equity premium the model requires a lower CRRA for AEs, compared to EMEs and to other related works that allow for recoveries in consumption during disasters.====My framework also allows me to quantify the contribution of the differences in group-specific disaster parameters to the equity premium. I find that differences in disaster duration and in permanent disaster shocks to consumption contribute the most to the equity premium in both groups of countries. Specifically, for AEs it is the difference in disaster duration that leads to the largest change in the equity premium. Setting the mean AE disaster length to the mean EME disaster length of six years (instead of the estimated four years for AEs) increases the equity premium for AEs by approximately 35 percent, relative to its observed and baseline model value. For EMEs, it is the difference in the long-run disaster impact on consumption that contributes the most to the equity premium. The EME equity premium increases by 29 percent when EMEs face the long-run disaster shocks estimated for AEs. Lastly, in line with data, the model generates higher correlations of cross-country consumption growth, equity returns, and risk-free rates for AEs than for EMEs. This result is driven by the more “global” nature of disasters in AEs (for example, the World Wars) compared to the more “idiosyncratic” nature of disasters in EMEs (for example, the Asian Financial Crisis).====To maintain accuracy of the parameter estimation, the probability of disasters is assumed to be constant over time in each country group. Gourio (2012) calibrates a time-varying disaster probability in a real business cycle framework to account for the co-movements of asset prices and macroeconomic aggregates in the United States. Similarly, Wachter (2013) models a stochastic disaster probability to account for several U.S. asset pricing features such as the long-run predictability and high volatility of stock returns. In contrast, my research targets distinct equity premia in AEs and EMEs, and uses Bayesian analysis to estimate a non-stochastic disaster probability and other group-specific disaster risk parameters for the two country groups.====My paper contributes to the vast literature trying to explain the equity premium puzzle. Instead of trying to provide an exhaustive list of possible explanations for the puzzle, I focus on listing a few seminal works including the literature on habit formation, long-run risks, disasters, agent heterogeneity, and behavioral explanations. The habit formation work is exemplified by Campbell and Cochrane (1999), who, building on the work of Abel (1990) and Constantinides (1990), show that their model can replicate the observed equity premium under the assumption that agents care about the value of their current consumption relative to its past. A possibility that the consumption growth contains a small persistent growth component and stochastic volatility has been put forward by Bansal and Yaron (2004), later followed by Hansen et al. (2008), Bansal et al. (2010), and Constantinides and Ghosh (2011). In contrast, Constantinides and Duffie (1996) argue that consumption heterogeneity across agents is key in resolving the equity premium puzzle, while Barberis et al. (2001) utilize prospect theory to offer a behavioral explanation – agents’ risk aversion varies based on past outcomes – to the puzzle. My work is in line with the disaster literature introduced by Rietz (1988) and later extended by Barro (2006) and Nakamura et al. (2013), which states that the equity premium is a compensation for infrequent, but disastrous events that lead to large consumption drops. However, in contrast to the previous disaster studies, my work estimates group-specific disaster parameters and quantifies the impact of differences in countries’ disaster parameters on the distinct equity premium in advanced and emerging countries.====The rest of the paper is organized as follows. Section 2 describes the data and stylized facts of consumption growth and equity premium in advanced and emerging countries. Section 3 lays out the empirical model of consumption disasters. The Bayesian estimation method is outlined in Section 4. Section 5 presents the estimates of group-specific disaster parameters. Section 6 analyzes the asset pricing implications of the differences in countries’ disaster parameters on the equity premium. Section 7 concludes.",Macroeconomic disasters and the equity premium puzzle: Are emerging countries riskier?,https://www.sciencedirect.com/science/article/pii/S0165188920300221,30 January 2020,2020,Research Article,359.0
Paul Pascal,"Federal Reserve Bank of San Francisco, United States","Received 7 March 2019, Revised 8 November 2019, Accepted 20 December 2019, Available online 20 January 2020, Version of Record 10 February 2020.",https://doi.org/10.1016/j.jedc.2019.103830,Cited by (16),"Financial crises occur out of prolonged and credit-fueled boom periods and, at times, they are initiated by relatively small shocks that can have large effects. Consistent with these empirical observations, this paper extends a standard ==== to include financial intermediation, long-term loans, and occasional financial crises. Within this framework, intermediaries raise their lending and leverage in good times, thereby building up financial fragility. Crises typically occur at the end of a prolonged boom, initiated by a moderate adverse shock that triggers a liquidation of existing investment, a contraction in lending, and ultimately a deep and persistent recession.","The 2007–2009 financial crisis revealed the need for macroeconomic models to incorporate connections between the financial sector and the macroeconomy that can amplify economic shocks and lead to occasional deep economic downturns. Over the past few years, rapid advances have been made to extend standard macroeconomic models and include financial intermediation to account for episodes of severe financial distress.====At the same time, a quickly growing empirical literature has revealed several stylized facts about financial crises. Crises are rare events that are usually preceded by prolonged boom periods and a buildup of macro-financial imbalances. For example, in the run-up to crises, credit usually rises rapidly, and credit growth is a robust early-warning indicator of crises (e.g., Schularick and Taylor, 2012; see also Fig. 13 in Appendix A.4.2).====Financial crises are associated with severe recessions that are typically deeper than normal recessions, particularly if they are preceded by a buildup of credit (Jordà et al., 2013). However, the ultimate triggers of crises can be relatively small. With respect to the 2007–2009 financial crisis, Gorton and Ordoñez (2014) argue that losses from mortgage-backed securities - the relevant shock for the financial sector around that time - were actually quite modest (see also Ospina and Uhlig, 2018).====These empirical facts about crises pose challenges to current macroeconomic models. Why does financial fragility build up in good times? What is the propagation mechanism that turns shocks that are not particularly large into severe macroeconomic events? In this paper, I develop a quantitative macroeconomic model that addresses these questions. In my model, crises are as frequent and severe as in the data, financial fragility endogenously builds up during booms when credit expands, and crises are usually initiated by a moderate adverse shock.====In typical macroeconomic models, two features generally work against a buildup of financial instability in good times. First, agents are risk-averse and therefore prefer to smooth their consumption. When their income temporarily increases, then agents want to save part of it and any prior level of borrowing therefore decreases. Second, in good times, asset prices increase, generally resulting in countercyclical leverage. However, in the model that I consider, these forces are overturned due to agent heterogeneity and limited asset market participation.====At the heart of my model are households, financial intermediaries, and a corporate sector. Financial intermediaries represent the whole modern financial intermediation sector, but I refer to them as banks for short. Households and banks are both risk-averse and consume. However, they differ in their degree of patience as in Kiyotaki and Moore (1997). Banks are less patient and therefore borrow from households in a short-term bond market. Moreover, only banks undertake risky investments in the corporate sector by issuing long-term and defaultable loans.====Within this framework, I show that bank leverage can be procyclical. In good times, it becomes more profitable to invest in loans, and banks therefore increase their lending. They finance their investment by raising additional borrowing from households. Depending on the calibration, the increase in debt may outweigh the rise in the value and the amount of loans, resulting in procyclical leverage. I show that leverage measured at market values of U.S. financial intermediaries is mildly procyclical in the data, and I calibrate the model to this evidence.==== Moreover, I show that this calibration also matches the empirical impulse response of leverage to a technology shock - the only aggregate shock in the model.====Given the calibrated model, banks enlarge their balance sheets and raise their leverage during prolonged boom periods. However, by leveraging up in good times, banks may also increase the risk of funding restrictions by creditors, once an adverse shock hits the economy. The model considers this trade-off explicitly by introducing occasional financial crises that are triggered if bank leverage goes above a certain threshold. The increase of leverage in good times moves banks closer to this “cliff”, thereby building up financial fragility.====Beyond the cutoff point for leverage, banks face the risk of a creditor run. To avoid a run and therefore insolvency, banks inefficiently liquidate a fraction of their long-term loans. The early liquidation of loans gives banks additional liquidity and eliminates the possibility of a bank run. However, this process is also particularly costly for the economy since ongoing investment projects are stopped and a fraction of capital is lost. In this way, the model is able to account for the sharp contraction of output during financial crises.====While a bank run in the end never materializes, it is the occasional threat of a run that leads to a severe disruption of financial intermediation. The model can therefore account for the fact that most financial intermediaries do not actually experience a bank run that leads to insolvency during a typical financial crisis. Instead, as shown by Chodorow-Reich and Falato (2017) with respect to the 2007–2009 financial crisis, banks sharply reduce existing credit to obtain liquidity and thereby avoid a bank run and insolvency. They find that total long-term credit and commitments outstanding contracted by 5.8% in 2008 and 5.9% in 2009 because unhealthy banks renegotiated the loan terms or accelerated repayment by a borrower that violated a covenant, similar to the mechanism here. This channel accounts for roughly two-thirds of the total credit reduction by unhealthy banks during the 2007–2009 crisis, thus is the dominant channel through which credit contracted.====Taken together, the model includes standard business cycle dynamics, a realistic representation of the financial sector’s balance sheet, and endogenous financial crises. I calibrate the model to match both the frequency and the severity of crises in the data. In this calibrated version, I find that the typical path leading to a crisis is characterized by a prolonged and credit-fueled boom, followed by a sudden bust that is triggered by a relatively moderate adverse shock.====As in the data, credit growth is a robust predictor of crises (e.g., Schularick and Taylor, 2012). Financial recessions are typically deeper than nonfinancial recessions, in particular if they are preceded by an unusually large buildup of credit, confirming existing empirical evidence (e.g., Jordà et al., 2013). In addition, credit spreads predict the severity of crises (Krishnamurthy and Muir, 2017). The behavior of the economy around nonfinancial recessions is different, since they are not preceded by an expansion of banks’ balance sheets, a credit boom, or a buildup of leverage. In addition, I show that the model replicates the occurrence of the 2007–2009 financial crisis when confronted with a historical series of structural productivity shocks for the U.S. economy.====For the main quantitative analysis, I treat crises as unanticipated events, as in the benchmark model by Gertler and Kiyotaki (2015). That is, when solving their decision problems, agents do not take into account that crises may occur in the future as a result of future shocks. In an extension, I relax this assumption and consider a slightly modified version that allows for crises to be anticipated. In future work, it would be interesting to take the model with anticipated crises further and consider macro-prudential policy interventions that aim to reduce the likelihood of crises.====Long-term loans are an important ingredient of the model. The illiquidity and possible liquidation of long-term loans determine how often crises occur and how severe they are. To speak to these mechanisms, I model long-term defaultable debt in a novel and tractable way (see, e.g., Chatterjee and Eyigungor (2012), Gomes et al. (2016), and Elenev et al. (2018) for alternative approaches). There is a distinction between newly issued and existing credit, and the two can differ in their riskiness and chance of default, particularly if existing credit is liquidated early. The model can also match the empirical evidence by Demyanyk and Hemert (2011) and Justiniano et al. (2017), who show that loans that were issued closer to the 2007–2009 financial crisis in the United States were of lower quality and had higher default rates ex-post.==== This paper builds on a vast literature about financial frictions within macroeconomic settings. Among the seminal contributions in this field are Bernanke and Gertler (1989), Kiyotaki and Moore (1997), Carlstrom and Fuerst (1997), and Bernanke et al. (1999). Before the 2007–2009 financial crisis, financial frictions were mostly considered with respect to the balance sheets of nonfinancial firms (e.g., Bernanke et al., 1999). Since then, the literature has quickly progressed. The focus shifted towards modeling financial intermediaries and introducing occasional financial crises explicitly. Without providing a full overview, contributions to this literature include Adrian and Boyarchenko (2012), Akinci and Queralto (2017), Benigno et al. (2018), Bianchi (2011), Bocola (2016), Boissay et al. (2016), Brunnermeier and Sannikov (2014), Elenev et al. (2018), Faria-e-Castro (2017), Gertler, Kiyotaki, 2010, Gertler, Kiyotaki, 2015, Gorton, Ordoñez, 2014, Gorton, Ordoñez, 2016, He and Krishnamurthy (2014), Martinez-Miera and Suarez (2012), and Mendoza (2010).====Several papers have shown that aggregate lending and investment can suddenly contract when the financial sector’s net worth or risk-bearing capacity is reduced (e.g., Gertler and Kiyotaki, 2010). Other contributions have particularly highlighted the nonlinear nature of such mechanisms. An adverse shock can have substantially worse effects if the financial sector is already at or close to its limits on how much funding to raise (e.g., Brunnermeier and Sannikov, 2014).==== In addition, when the economy enters a recession, borrowing restrictions for households, firms, and financial institutions may bite at the same time, and the different sectors may pull each other down (e.g., Elenev et al., 2018). A common theme across these papers is therefore that they all provide mechanisms through which the effects of adverse shocks get amplified. The size of the disturbance that is needed to explain the severity of crises in the data may therefore be strongly reduced, however, it may still be large (see, e.g., Gertler and Kiyotaki, 2010). Moreover, in these frameworks, crises are not more likely to occur out of prolonged and credit-intensive booms, as in the data. The goal of this paper is to address these two challenges.====Only a few papers have attempted to provide a theoretical account of both the buildup of fragilities during the boom period that usually precedes crises and the eventual crash. One early contribution that could be interpreted in this way is Lorenzoni (2008). In contrast to his paper, I provide a quantitative analysis of financial crises within a macroeconomic setting. This is also the difference to Gorton and Ordoñez (2014) and Gorton and Ordoñez (2016), who build on the idea that debt is informationally insensitive during booms but can suddenly turn informationally sensitive even after small shocks and therefore lead to a contraction in lending.====The closest paper is Boissay et al. (2016), in which crises also follow credit booms and are initiated by moderate adverse shocks. In their paper, households accumulate bank debt during a boom, giving banks incentives to engage in risky activities, which can result in a collapse of the interbank market. In contrast, I present a framework that features a realistic banking sector and the mechanisms through which systemic risk builds up and the eventual crash occurs are both matched to the data. The model replicates the empirical behavior of intermediary leverage, which reflects the financial sector’s risk-taking behavior, and determines the buildup of financial crisis risk.==== Moreover, the model features long-term loans and therefore a maturity mismatch for banks. Crises are particularly costly because of the early liquidation of legacy loans. Again, I show that this mechanism is supported by the data, given the evidence in Chodorow-Reich and Falato (2017).====The paper is similar to Gertler and Kiyotaki (2015) and Gertler et al. (2017) in that banks are subject to rollover crises. Depending on macroeconomic fundamentals, a run equilibrium occasionally arises. In contrast to these papers, however, a run never materializes since banks engage in an early liquidation of their loans to avoid a run. Such liquidations are costly and result in a sharp contraction in aggregate output, capturing the discrete nature of crises. Gertler et al. (2017) and Bordalo et al. (2018) show that models in which agents have optimistic beliefs can also replicate the boom-bust patterns. Boz and Mendoza (2014) demonstrate that the same can be achieved if agents learn about a new financial environment. Here, I abstract from learning mechanisms and optimistic beliefs.====. The next section outlines the model. The model is calibrated to the data as explained in Section 3. Based on this calibration, the model is analyzed in Section 4. Section 5 concludes.",A macroeconomic model with occasional financial crises,https://www.sciencedirect.com/science/article/pii/S0165188919302258,20 January 2020,2020,Research Article,360.0
Alam M. Jahangir,"Department of Economics, Truman State University, United States","Received 7 December 2018, Revised 21 December 2019, Accepted 24 December 2019, Available online 5 January 2020, Version of Record 4 February 2020.",https://doi.org/10.1016/j.jedc.2020.103831,Cited by (6),"Capital misallocation can lower aggregate total factor productivity, but much less is known about its cyclicality. Using European firm-level data for 2005 to 2014, I establish that capital misallocation, as measured by the dispersion of returns to capital, is higher during recessions and lower during booms. This result is robust to using a much longer dataset from Compustat for the United States and Canada. I also find that firms’ net worth, measured as the difference between total assets and liabilities, relative to sales, can explain more capital misallocation than all the other examined firm-level factors combined. Furthermore, my results suggest that firms’ net worth explains approximately 10 percent of capital misallocation and 30 percent of its cyclicality.","Capital misallocation, the allocation of capital to plants with lower rather than higher returns to capital, can reduce aggregate total factor productivity (TFP), explaining a large part of cross-country TFP differences.==== Although this relationship between capital misallocation and aggregate TFP has been well documented (Bartelsman, Haltiwanger, Scarpetta, 2013, Hsieh, Klenow, 2009, Restuccia, Rogerson, 2008), the cyclicality of capital misallocation is studied much less frequently (Bloom, Floetotto, Jaimovich, Eksten, Terry, 2018, Kehrig, 2015), as a recent survey by Restuccia and Rogerson (2017) also mentions. This cyclicality is important because it may enhance the cyclicality of TFP. Identifying the sources of and factors in capital misallocation and its cyclicality is also important for mitigating TFP loss during recessions.====Following the literature, I measure capital misallocation as the dispersion of the marginal revenue product of capital (MRPK) across different production units. Specifically, I measure the MRPK by applying a scaling factor to the average revenue product of capital (ARPK), which is measured by sales per unit of capital stock. In a static and frictionless environment, this measure should equal the gross interest rate. Thus, the dispersion of the MRPK should indicate resource misallocation or lower production efficiency conditional on resources. However, in a world with dynamic capital accumulation or other frictions, MRPK dispersion should emerge naturally, as such environments generally prevent units of production from immediately adjusting capital to accommodate different productivity shocks. These frictions may include capital time-to-build, adjustment costs in investment, or financial constraints.====To examine the cyclicality of capital misallocation, I use a rich firm-level dataset from Amadeus that covers firm balance sheet information for a set of European countries from 2005 to 2014. Using measures of firm sales and capital stock, I establish that capital misallocation is countercyclical (i.e., higher during recessions and lower during booms). This result is robust to using a much longer dataset from Compustat for the United States and Canada (including data from 1961 to 2011). To document this countercyclicality of capital misallocation, I use graphs of the business cycle and the correlation between the cyclical components of capital misallocation and gross domestic product (GDP).====To identify whether recessions are associated with higher capital misallocation, I pool the calculated dispersion of the MRPK within industries. Specifically, I estimate the effect of business-cycle intensity, measured as the number of months of recession in a given year as well as output gap, on capital misallocation. Because the uncertainty shocks that are associated with a greater dispersion of revenue TFP (TFPR) are countercyclical (Bloom et al., 2018) and the dispersion of the MRPK is associated with the dispersion of TFPR (Asker et al., 2014), I control for the year-industry-country standard deviation of the TFPR firm shock, among other controls. Otherwise, these two properties can generate cyclicality. Using this specification, I establish that capital misallocation is indeed countercyclical, even at the industry level.====Much of the aggregate countercyclicality of capital misallocation may be driven by a few specific industries. To understand the cyclicality of misallocation at an industry level, I show that sectors associated with large external finance dependence exhibit higher cyclicality, as in Rajan and Zingales (1998). This finding suggests that financial factors or volatility associated with productivity/demand shocks (TFPR) can potentially contribute to misallocation.====To understand the sources of capital misallocation, I decompose it into that due to variation between industries, that between firms within industries, and that within firms over time. I estimate that more than 50% of capital misallocation is due to variation between firms within industries. Because variation between firms within industries accounts for most capital misallocation, I focus on estimating the relative importance of several firm-level factors associated with capital misallocation. The results show that firms’ net worth, measured as the difference between total assets and liabilities, relative to sales can explain more capital misallocation than all the other examined firm-level factors (i.e., firm age, number of employees, leverage ratio, input growth, and TFP shocks) combined. Overall, firms’ net worth explains approximately 10% of the capital misallocation within industries. These results are consistent with the fact that the financial frictions associated with credit constraints may play an important role in firms’ investment decisions.====To clearly identify the capital distortions that emerge from financial frictions instead of other firm-level factors, I evaluate net worth relative to sales. Since net worth might capture unobserved variables, in this paper, I document the conditional relationship between MRPK and net worth. My results suggest that controlling for net worth explains approximately 30% of the cyclicality of capital misallocation and 30% of the cyclicality of TFP.==== These facts allow me to conclude that capital misallocation is indeed countercyclical and that a major component of the misallocation is accounted for by firm-level net worth; these results are consistent with the fact that financial frictions may play an important role in firms’ investment decisions. The cyclicality of capital misallocation comes from the cyclicality associated with credit constraints, that is, contractions associated with lower net worth imply that more firms are credit constrained and, thus, are less able to adjust their capital to the desired levels. Thus, a firm with a lower net worth may face a higher cost of borrowing and even tighter credit constraints during recessions than a firm with a higher net worth. These results support the analysis of Bernanke and Gertler (1989) on the effect of borrower net worth on the business cycle.====The remainder of this paper is organized as follows. Section 2 presents the literature review. Section 3 describes the databases, data cleaning, and sample selection as well as the methods used to estimate MRPK and TFP. Section 4 explains my results. Section 5 concludes.",Capital misallocation: Cyclicality and sources,https://www.sciencedirect.com/science/article/pii/S0165188920300014,5 January 2020,2020,Research Article,361.0
"Heiberger Christopher,Maußner Alfred","Department of Economics, University of Augsburg, Universitätsstraße 16, Augsburg D-86159, Germany","Received 6 August 2019, Revised 20 November 2019, Accepted 3 December 2019, Available online 27 December 2019, Version of Record 19 February 2020.",https://doi.org/10.1016/j.jedc.2019.103819,Cited by (4)," driven by an iid sequence of draws ϵ====. Yet, widely available formulae to compute perturbation solutions are based only on the perturbation of ====ϵ==== and do not adequately capture the additional deviation ","Would economic agents benefit, if the business cycle could be removed? And if so, by how much? Lucas (1987) started this discussion by arguing these benefits are negligible. He considers a representative, risk-averse consumer facing a stochastic consumption stream. The consumer values this stream according to an additively separable intertemporal utility function with iso-elastic period utility. The expected mean of the stream grows at a constant rate and the fluctuations around this trend match the variance of trend deviations of quarterly real U.S. consumption. For plausible values of the coefficient of relative risk-aversion (his Table 2 considers values between 1 and 20) he estimates a welfare gain not exceeding 0.1 percent of annual consumption, i.e., about $ 8.5 in 1983.====Since then, many researchers have estimated the welfare costs of business cycles in more elaborate models.==== These include departures from the specification of preferences and of the consumption process (as, e.g., Barro, 2009, Dolmas, 1998, Obstfeld, 1994, Tallarini, 2000), models with uninsurable idiosyncratic risk (as, e.g., De Santis, 2007, Heathcote, Storesletten, Violante, 2008, İmrohoroğlu, 1989, Krebs, 2003, Krusell, Mukoyama, Şahin, Smith, 2009), the consideration of nominal frictions (as, e.g., Cho, Cooley, Phaneuf, 1997, Galí, Gertler, López-Salido, 2007), endogenous growth (as, e.g., Barlevy, 2004, Heer, Maußner, 2015), and model uncertainty (Barillas et al., 2009). The range of estimates is wide, from being close to Lucas’ 0.1 percent to several orders of magnitude beyond. For instance, İmrohoroğlu (1989), p.1378 estimates 0.3 percent of average consumption, Krebs (2003), p. 862 finds 7.48 percent, Tallarini (2000), Table 3 calculates costs between 2.1 and 12.6 percent, and in the model of Barro (2009) the society would be willing to reduce GDP by about 20% to eliminate rare disasters.====The focus of this paper is on welfare computations within dynamic stochastic general equilibrium (DSGE) models. This class of models encompasses real business cycle models as presented, e.g., in King, Plosser, Rebelo, 1988a, King, Plosser, Rebelo, 1988b as well as New Keynesian monetary models in the spirit of Christiano et al. (2005) and Smets, Wouters, 2003, Smets, Wouters, 2007. Since its origins in the 1980s DSGE models have become the workhorse of macroeconomic research.====The most basic source of uncertainty and the driving force of business cycles in these models are shocks to total factor productivity (TFP), say ====. Commonly, the log of TFP is assumed to be normally distributed, i.e. ln ==== ~ ====(====).==== Since ==== provides a measure for the degree of uncertainty in the model, the most natural way to assess the effects of uncertain shocks on the welfare of economic agents is to compare the agents’ welfare for different values of ====. Moreover, an adequate estimate of the costs of risk should leave the mean of ==== unchanged when varying ====, i.e. for an adequate estimate of the costs of business cycles the TFP shock must have the mean preserving spread (MPS) property. Since the mean of the log-normally distributed random variable ==== is increasing in ====, i.e. ==== a MPS analysis demands that ====.====Different from the example of Lucas (1987), DSGE models usually admit no closed form solution. The most wide-spread approximation technique for DSGE models are perturbation methods. From the perspective of the user, they are easy to apply and able to handle even large scale models. Perturbation methods start with the solution of the model’s deterministic version. The deterministic solution is subsequently perturbed with respect to the uncertainty parameter ==== and an approximation to the stochastic model, which additionally captures the effects of ==== up to an arbitrary order, is constructed. Formulae for perturbation solutions to DSGE models are widely available, see e.g. Schmitt-Grohé and Uribe (2004b), Gomme and Klein (2011), Andreasen (2012), and Binning (2013). However, the canonical framework on which they rest, assumes that only the variance of the model’s shocks depends on the perturbed parameter ====. While shocks with non-zero means are covered under this framework, the crux of MPS is not that the mean ==== is non-zero but the fact that it is coupled to the perturbation parameter ====, e.g. by ====. When the effect of ==== on the model solution is derived, e.g. by taking derivatives of the model’s equilibrium conditions with respect to ====, the effect of a simultaneously changing mean is neglected, i.e. the derivative ====/==== is not taken into account. The same is true for higher-order effects as well as for models with multiple shocks to other variables. Hence, the MPS property is not covered under the canonical framework and, to the best of our knowledge, no formulae which consider the additional effects are available. In consequence, welfare measures which rest on available formulae are (potentially seriously) biased.====The contribution of our paper, therefore, is first and foremost methodological. We extend the canonical DSGE model of Schmitt-Grohé and Uribe (2004b) to allow for stochastic processes that have the MPS property. For this extended model we derive the second-order perturbation solution and provide Matlab code that implements this solution.==== Our solution appropriately adjusts the level effect of the perturbation parameter. Thus, it also shifts the agent’s value function required for welfare comparisons. We distinguish between ==== and ==== welfare measures. The former compares two economies, A and B, say, which start at the same economic state. Economy A remains in this state forever while economy B is hit by exogenous shocks that drive the business cycle. The unconditional measure integrates out the effect of a specific starting point.====Even when the model solution is computed accurately under the MPS property, the total effect of shocks on the welfare of economic agents within a production economy can be decomposed into two conceptually different components. Cho et al. (2015) argue that on the one hand, risk-averse agents, having concave utility functions, dislike fluctuations: according to Jensen’s inequality the expected utility of a lottery does not exceed the utility obtained from the expected outcome of the lottery. Therefore, these agents will benefit, if the lottery is replaced by a certain stream of consumption and leisure equal to expected consumption and leisure from the lottery. On the other hand, in a production economy, expected consumption and leisure are not exogenously given even when the mean of shocks is fixed to a specified level. For example, in a two-country model with perfect capital markets and where only the TFP of the domestic country is subject to shocks, a social planner can optimally shift capital between the two countries in response to shocks to domestic TFP. In consequence, one can expect that in a world with shocks mean total output exceeds mean total output from a world without shocks even when TFP shocks have the MPS property. Hence, in production economies uncertainty may produce an additional, ==== mean effect from the optimal adjustment of agents and this mean effect may very well be positive.==== Yet, with the available formulae for perturbation solutions which lack correction for the MPS, this mean effects will be systematically overestimated so that the costs of business cycles are systematically underestimated.====We apply our techniques to a standard real business cycle model and its extension to a one-good, two-country model with perfect international capital markets. We calibrate the first model as in Cho et al. (2015) so that we can compare our results to those presented in their Figure 2.==== We show that overestimation of the mean effect under the available standard perturbation solution is misleading and suggests positive welfare effects from business cycles, whereas removing the business cycle turns out always beneficial under our proposed solution.====We confirm the logic behind our method both analytically and numerically. Our analytic derivation is limited to a toy model that admits a closed form solution. Numerically we compare the results from our perturbation method with the results obtained from two different weighted residuals methods where the MPS can be implemented trivially. In the two-country model, the extended potential for risk-sharing implies negative welfare effects of removing the cycle for low degrees of risk-aversion. However, the standard perturbation solution still systematically underestimates the welfare costs of uncertainty while the accuracy of our proposed method is again confirmed by the weighted residuals methods.====From here we proceed with a sketch of the toy model in Section 2. In addition, we briefly describe the benchmark real business cycle model and its two-country extension. These models serve as examples of the canonical DSGE model presented in Section 3.1 and as a framework to illustrate the computation of conditional and unconditional welfare measures in Section 3.2. In Section 3.3 we sketch the solution of the one-country model via two weighted residuals methods. Section 4 presents our quantitative results. In particular, we provide conditional and unconditional welfare gains from removing fluctuations from the models of Section 2. Section 5 concludes. The Appendix covers additional material. In particular, it presents the full sets of equations behind our models, covers the derivation of our second-order perturbation solution, describes in more detail our weighted residuals methods, and supplies the tables that underly our graphical presentation of the results.",Perturbation solution and welfare costs of business cycles in DSGE models,https://www.sciencedirect.com/science/article/pii/S0165188919302143,27 December 2019,2019,Research Article,362.0
"Luo Pengfei,Tian Yuan,Yang Zhaojun","School of Finance and Statistics, Hunan University, Changsha, China,Faculty of Economics, Ryukoku University, Kyoto, Japan,Department of Finance, Southern University of Science and Technology, Shenzhen, China","Received 5 May 2019, Revised 5 October 2019, Accepted 17 December 2019, Available online 26 December 2019, Version of Record 9 January 2020.",https://doi.org/10.1016/j.jedc.2019.103829,Cited by (4),This paper utilizes a ,"The real options theory has a long research line since Myers (1977) and more often than not, the literature does not take competition into account. Actually, most firms operate in a competitive environment and their investment opportunities are not exclusive. Therefore, it is more interesting to examine how strategic interactions in a competitive environment affect the timing and pricing of the option to invest.====Smets (1991) firstly develops a real options model to examine irreversible market entry for a duopoly facing stochastic demand. Grenadier (1996) provides a general and tractable approach for deriving the strategic exercise of options in real estate markets. Weeds (2002) analyzes irreversible investment in competing research projects with uncertain returns under a winner-takes-all patent system. Huisman and Kort (2003) utilize a real option game approach to determine when and which technology should be adopted in a duopoly framework, which is further extended by Huisman and Kort (2004) to take into account technological progress. Mason and Weeds (2010) examine the impact of preemption on the relationship between uncertainty and investment. Nishihara and Shibata (2010) consider the investment and financing policies in a duopoly model, and study the interactions between preemptive competition and financing constraint. All of these papers have strengthened the importance of extending real options analysis by including strategic interactions among agents. Lambrecht and Perraudin (2003) incorporate incomplete information into preemption investment where the agents’ costs are private information. Hsu and Lambrecht (2007) incorporate asymmetric information into a model by assuming that the challenger has complete information about the incumbent whereas the incumbent does not know well the challenger. Kong and Kwok (2007) further investigate the strategic investment decisions and option values under asymmetry on both the sunk cost of investment and revenue flows.==== Shibata and Yamazaki (2010) examine the impacts of an asymmetric access charge regulation on competitive investment strategies in a liberalized telecommunication market.====To the best of our knowledge, all the papers in the literature on preemptive investment assume an exponential discounting, i.e., a constant rate of time preference. However, agents may have time-inconsistent preferences, which can arise in practice as shown in several empirical studies, see, e.g., Thaler (1981) and Loewenstein and Prelec (1992).====Recently, time-inconsistent behavior of agents has received increasing attention. Grenadier and Wang (2007) consider a monopolistic investment problem under uncertainty and time-inconsistent preferences. Lien and Yu (2014) develop a discrete model to study the interplay between firm investment and cash flow hedging decisions under time-inconsistent preferences. Tian (2016) extends Grenadier and Wang (2007) to provide an analytically tractable framework for a monopolistic entrepreneurial firm’s capital structure and investment decisions under time-inconsistent preferences. However, all the papers above focus on decision-making problems of a single agent and emphasize that it must be interesting to extend the model to a duopoly case and explore how time-inconsistent agents would interact with time-consistent agents. Along the research line, we aim to fill this gap.====. The objective of our paper is to integrate the two strands of literature: preemptive investment and time-inconsistent preferences. We provide an analytically tractable game-theoretical real options framework taking into consideration time-inconsistent preferences. We fix the leader’s value function and follower’s option value and define preemption in general. We find that the key determinant of the Nash equilibrium is the relative time-inconsistent degree between two competitors. Therefore, we further investigate two cases in particular: a homogeneous (symmetric) case, i.e. the situation where both players have the same time-inconsistent preferences, and a heterogeneous (asymmetric) one, i.e. the situation where one player is time-inconsistent and the other is time-consistent.====The main contribution of our paper is to demonstrate how time inconsistency affects preemptive investment. First, in the homogeneous case, we find that time inconsistency delays preemptive investment. As a result, the inefficiency caused by preemptive competition is mitigated. Our analysis complements the monopolistic investment literature under uncertainty and time-inconsistent preferences. Second, in the heterogeneous case, we show that the time-consistent agent becomes the leader when facing a time-inconsistent rival. In particular, if the degree of the rival’s time-inconsistent preferences is weak, the time-consistent leader must accelerate investment to take the preemptive investment threshold chosen by the time-inconsistent rival. However, if the degree of the rival’s time inconsistency is strong, the time-consistent leader does not need to take an inefficient preemptive investment as if the competition disappeared, i.e., the optimal investment in a non-strategic setting is realized. As a result, we demonstrate that the inefficiency of investment caused by preemptive competition is even eliminated if the difference between the two rivals’ time-inconsistency degrees is sufficiently high. Third, it is well known in the existing literature on competition that the leader’s value function as well as the entry threshold depends on the follower’s entry threshold. Going one step further, we clarify that in the heterogeneous case, the time-consistent agent’s entry threshold also depends on the time-inconsistent rival’s preemptive threshold, the optimal threshold in a non-strategic setting and their ordering as well. As far as we know, this finding is brand new to the literature on preemptive investment and well answers the questions raised by the literature on time-inconsistent preferences that how time inconsistency works in a duopoly case and how time-inconsistent agents would interact with time-consistent agents. Forth, we find and verify that the preemptive investment threshold under the time-inconsistent case is higher than that derived from the classical time-consistent model. Last, our theoretical results provide a behavioral explanation for why preemption occurs in some markets but is not present in others. For example, this phenomenon is observed in a real estate market, which is reported by Grenadier (1996).====Our paper is most closely related with Grenadier and Wang (2007) and Chapter 9 of Dixit and Pindyck (1994). We extend the former into a duopoly model addressed by the latter among many others. There are however several major differences shown below.====First, the former’s equilibria depend on the intra-personal competition between the current self and future self due to the time inconsistency, while the latter’s equilibria arise from the duopoly competition without considering time- inconsistent preferences. In this paper, we combine both to provide novel insightful findings, which are never discussed before in the literature to our knowledge.====Second, we assume that the two players (agents) have heterogeneous beliefs, which are a novel and realistic feature from behavioral finance. Dixit and Pindyck (1994) and Grenadier and Wang (2007) only consider that the agents have homogeneous beliefs. In the heterogeneous case, we find that the leader’s investment policy depends on the time-inconsistent difference between the two players. Our conclusions produce an explanation for an empirical fact.====Third, we simultaneously consider the time-inconsistent effect of Grenadier and Wang (2007) and the first-mover advantage effect of Dixit and Pindyck (1994). In the duopoly market, from the view of a rational marketer, who is time-consistent, the time-inconsistent effect can mitigate and even eliminate the investment distortion arising from the first-mover advantage effect, implying that the time-inconsistent effect improves the effectiveness of the market in the duopoly competition. By contrast, Grenadier and Wang (2007) state that the time-inconsistent effect leads to the ineffectiveness in a monopoly market.====Last, we examine the duopoly competition while Grenadier and Wang (2007) consider a monopoly market. Our model instead of the latter can capture preemptive action and its effects on the pricing and timing of the option to invest.====The structure of the paper is as follows. Section 2 describes the setup of the model. Section 3 briefly reviews the time-consistent benchmark. As the main part of this paper, Section 4 considers the strategic investment under time-inconsistent preferences. In particular, we consider two cases: One is homogeneous and the other is heterogeneous. Section 5 examines the implications of the model by providing numerical examples. Section 6 concludes.",Real option duopolies with quasi-hyperbolic discounting,https://www.sciencedirect.com/science/article/pii/S0165188919302246,26 December 2019,2019,Research Article,363.0
"Mundt Philipp,Alfarano Simone,Milaković Mishael","Department of Economics, University of Bamberg, Feldkirchenstraße 21, Bamberg 96052, Germany,Department of Economics, University Jaume I, Campus del Riu Sec, Castellón 12071, Spain","Received 2 April 2019, Revised 9 December 2019, Accepted 10 December 2019, Available online 13 December 2019, Version of Record 30 December 2019.",https://doi.org/10.1016/j.jedc.2019.103820,Cited by (7),.,"Accurate forecasts of profitability are relevant for investment decisions and provide valuable information for investors, managers, and other groups of corporate stakeholders. Since refined estimates of profitability improve firm valuation, they are also of interest to financial analysts and traders.==== Competition tends to equalize profit rates, or in more modern parlance the return on assets (ROA), through the process of capital reallocation that is subject to all types of real frictions.==== So, it is not overly surprising that profit rates are to some extent predictable in the sense that their time series are persistent and mean-reverting (e.g. Nissim, Penman, 2001, Stigler, 1963), which is also true for other accounting ratios such as measures of leverage, liquidity, and operating efficiency (Gallizo et al., 2008). What sets profit rates apart is that their cross-sectional distribution is stationary for surviving corporations and well approximated by a symmetric Laplace density (Mundt et al., 2016). Here we show that a parsimonious diffusion process (or stochastic differential equation) that accounts for all the above statistical regularities in corporate ROA, first suggested by Alfarano et al. (2012), outperforms previously proposed time-series and cross-sectional models in terms of their out-of-sample forecasting performance across different time horizons. To the best of our knowledge, the diffusion process is the only model so far that is consistent with both the persistent mean-reversion of individual ROA time-series and the cross-sectional distribution of ROA. It is probably best understood as a reduced-form model of economic frictions in the process of capital reallocation. Its distinct feature is that it dictates a precise constraint on the strength of mean-reversion that is derived from the observed cross-sectional distribution.====Our main idea is to exploit the notion of ergodicity in the profitability of surviving corporations. Ergodicity refers to a situation where the unconditional moments of individual time-series converge to the moments of the stationary cross-sectional distribution. Put differently, if a system is ergodic then the cross-sectional outcome at a given point in time will convey the same statistical information as the time-series of individual destinies; in the natural sciences such a situation is often referred to as a statistical equilibrium (see, e.g., Garibaldi and Scalas, 2010). This is particularly helpful when the number of cross-sectional observations (here several hundred surviving corporations) is larger than the number of observations in the time domain (here several decades of annual data for individual corporations). Our gains in predictive performance, therefore, originate essentially from exploiting ergodicity both with respect to the law of large numbers, and with respect to the conscientious specification of mean-reversion in individual profit rate series that is prescribed by their cross-sectional distribution. The notion of ergodicity is generally only sensible for surviving entities because in the presence of ruin or corporate death it does not make sense to postulate that cross-sectional or ensemble averages are representative of the time-series averages of individual entities (see, e.g., Peters, Gell-Mann, 2016, Taleb, 2018). The focus on surviving corporations is, however, less restrictive than it seems at first. After all, if one is not willing to part with concerns of survivorship bias, our results can simply be stated as being conditional on survival. Yet the vast majority of corporate “deaths” are actually caused by transfers of ownership and not by bankruptcy and liquidation, which historically account for a very small fraction of corporate mortality (Daepp et al., 2015). Survivors accordingly carry an enormous amount of incorporated capital through time and also represent macroeconomically crucial “granular” entities in the jargon of Gabaix (2011), making them a worthwhile object of study in their own right.====The reasons for the predictive superiority of our diffusion model are threefold. First, the model is consistent with the empirical Laplace distribution of profit rates, so small deviations around the mean and extreme events occur more often than in models that lead to counterfactual normal distributions. Second, the model is also consistent with the autocorrelation structure of the data that exhibits a particular asymptotic exponential decay, as shown by Mundt et al. (2016). Compared to standard first order autoregressive models, or the Ornstein-Uhlenbeck process as their continuous-time analog, our process implies a distinct adjustment towards the average rate of profit that also improves forecasting performance. Third, Mundt et al. (2017) present evidence that individual firm characteristics are almost negligible for the dynamics of profitability once the entity has survived in the market for an extended period of time, pointing to the existence of a common law of motion governing the profitability of long-lived firms that enables us to exploit the ergodic property in the first place.====We employ the test for superior predictive ability (SPA) by Hansen (2005) and the model confidence set (MCS) by Hansen et al. (2011) to evaluate the forecasting performance of our diffusion process against existing models for predicting corporate profitability such as the Ornstein-Uhlenbeck process as the continuous-time analog of a (stationary) first-order autoregressive model, a structural partial-adjustment model, and non-linear autoregressions assuming an asymmetric adjustment towards the target level of profitability. In order to distinguish the effect of the diffusion’s particular mean-reversion from efficiency gains that originate in the use of cross-sectional data, we start out by comparing the forecasting performance of a set of time-series models with firm-specific parameterizations that do not make use of cross-sectional information. In this setting our diffusion outperforms alternative time-series models such as the Ornstein-Uhlenbeck process, ARIMA models, and the random walk. The finding that our model outperforms these conventional models testifies to the predictability of ROA and the more accurate adjustment mechanism in our model, and it reflects negatively on the so-called persistence of profits literature (see, e.g., Geroski, Jacquemin, 1988, Gschwandtner, 2005, Mueller, 1977, Mueller, 1990, Waring, 1996), which draws heavily on different types of autoregressive models in their studies. In light of the leptokurtic profit rate distribution, the latter are clearly misspecified and accordingly lead to inferior forecasting performance. Moreover, the Ornstein-Uhlenbeck process as their continuous-time analog assumes that the drift towards the average profit rate is stronger the larger the difference from the mean in either direction, as reported by Fama and French (2000) for a mixed sample of firms with different life spans. Our findings indicate that this is not the case for long-lived corporations, testifying to crucial differences in the dynamics of profitability between surviving and shorter-lived corporations.==== Next we show that the dynamics of profitability are remarkably homogeneous across surviving firms. In particular, our diffusion model exhibits the best forecasting performance when parameterized with estimates of average profitability and dispersion that are obtained from the cross-sectional profit rate distribution. We interpret this as an imprint of ergodicity because the dynamics of all surviving firms follow the same stochastic law that is derived from the cross-sectional return distribution. Finally, we show that alternative models that build on cross-sectional information for parameter estimation, like the structural partial adjustment model introduced by Fama and French (2000) or the non-linear autoregressions employed by Fairfield et al. (2009), which do not pay attention to the distribution of profit rates, produce on average larger forecast errors than our methodology, especially for longer forecasting horizons where the effect of the correct adjustment mechanism towards average profitability becomes more pronounced.====Despite the appealing statistical properties of the return on assets, the majority of existing papers (mostly in the accounting and finance literature) deals with the modeling and forecasting of earnings as an alternative measure of firm performance. A plethora of contributions from the early literature in this field explores the time-series properties of earnings and concludes that earnings follow a random walk or martingale process, suggesting that the best prediction is simply the last observation (see, e.g., Ball, Watts, 1972, Lintner, Glauber, 1978, Little, 1962). The forecasting capacity of different flavors of mixed autoregressive (integrated) moving average models on earnings has been investigated by, for example, Callen et al. (1993), Albrecht et al. (1977), Watts and Leftwich (1977), Lookabill (1976), Brown and Rozeff (1979), Foster (1977), Collins and Hopwood (1980) and Griffin (1977) while, more recently, Hou et al. (2012) propose a cross-sectional model to forecast the earnings of individual firms. In the research field of corporate profitability, the majority of extant studies employs structural models. Fairfield et al. (1996) analyze the predictive content of several earnings components on the return on equity, such as operating earnings, non-operating earnings and taxes, and special items, finding that disaggregation improves the forecasting accuracy relative to models that use a higher level of aggregation. In a similar vein, Fairfield and Yohn (2001), Soliman (2008), and Bauman (2014) use the DuPont methodology to decompose ROA into the product of asset turnover and profit margin, arguing that changes in asset turnover and profit margin contain information on the change in ROA. More closely related to our approach, several papers employ cross-sectional profitability forecasting models. Fairfield et al. (2009) predict the return on equity and net operating assets by means of parsimonious first-order autoregressions, while Evans et al. (2017), Fama and French (2000), and Allen and Salim (2005) conduct forecasting analysis on profitability using two stage partial adjustment models. Similar to our approach, these models rely on cross-sectional data to predict changes in ROA. Contrary to our model, however, these approaches build on fundamental measures of expected (and thus unobservable) profitability, while our model merely depends on the (observable) history of realized ROA. Yet the most crucial difference between these models and our methodology is that the former approaches do not take into account the Laplacian nature of profit rates.====Our study also relates to the broader body of work focusing on the identification of robust distributional regularities in key economic variables besides firm profit rates. Popular recent examples in the literature include the distribution of firm size (Axtell, 2001, Stanley, Buldyrev, Havlin, Mantegna, Salinger, Stanley, 1995), firm growth rates (Bottazzi, Secchi, 2003, Bottazzi, Secchi, 2003, Bottazzi, Secchi, 2006, Stanley, Amaral, Buldyrev, Havlin, Leschhorn, Maass, Salinger, Stanley, 1996), income (Reed, 2001, Reed, 2003, Toda, 2012), consumption (Toda, 2017, Toda, Walsh, 2015, Toda, Walsh, 2017), and GDP growth rates (Fagiolo, Alessi, Barigozzi, Capasso, 2010, Fagiolo, Napoletano, Roventini, 2008). Our approach to construct a time series model conditional on a robust distributional regularity might, therefore, also prove useful in other applications in which the notion of ergodicity is a reasonable approximation of the data. As we will show in this paper, these regularities provide a great potential for developing more accurate statistical models, and thus we hope that our contribution will stimulate more work in this promising direction. Once we have fitted the empirical distribution with a parametric model, we can derive a diffusion process whose stationary density coincides with this parametric distribution. To this end, we observe that the functional form of the stationary density prescribes a specific relationship between the drift and diffusion functions that characterize a diffusion process. Imposing additional constraints on these functions, e.g. by assuming homoscedastic shocks or a linear drift, it is possible to derive a unique diffusion process that has the given distribution as its stationary density (see, e.g., Bibby et al., 2005). We then obtain either a closed-form solution to the conditional probability density of this process, or a numerical approximation to it (for the latter case see, e.g., Lux, 2009), which determine the full set of relevant statistics necessary to forecast empirical time-series, including the process’ conditional and unconditional moments. Hence, based on the ergodic property, such a procedure uniquely translates the empirical cross-sectional distributional regularity into a time series model that is consistent with the empirical distributional regularity.====The remainder of this paper is organized as follows. Section 2 describes the data, Section 3 outlines the forecasting design, Section 4 presents the diffusion model and its competitors, Section 5 reports the main results, and Section 6 discusses the results and concludes.",Exploiting ergodicity in forecasts of corporate profitability,https://www.sciencedirect.com/science/article/pii/S0165188919302155,13 December 2019,2019,Research Article,364.0
Wenzelburger Jan,"Fachbereich Wirtschaftswissenschaften, Technische Universität Kaiserslautern, Postfach 3049, D-67653 Kaiserslautern, Germany,University of Liverpool Management School and Institute for Risk and Uncertainty, University of Liverpool, Liverpool L69 7ZH, UK","Received 19 December 2018, Revised 6 December 2019, Accepted 10 December 2019, Available online 13 December 2019, Version of Record 2 January 2020.",https://doi.org/10.1016/j.jedc.2019.103821,Cited by (2), should be replaced by a ,"Ever since its establishment as an equilibrium model by Sharpe (1964), Lintner (1965), and Mossin (1966), the capital asset pricing model (CAPM) has been heavily criticised. According to Levy (2011), the most severe criticism, first articulated in Fama and French (1992), is that beta coefficients of the CAPM have no explanatory power at all. Despite the fact that numerous empirical investigations have questioned its empirical validity, the CAPM is still the most widely employed model in finance. Levy (2011) argues that if the beta coefficients are economically meaningless, the question of why the CAPM is still widely used by academics and practitioners demands a satisfactory answer.====The validity of the CAPM is intimately related to the question of whether the market portfolio is a meaningful concept. The relevance of the market portfolio is debated in the finance literature ever since Roll (1977) and Ross (1977) who states that ”==== CAPM ====”. Fernandez et al. (2016) give a comprehensive account of the current state of the debate on the empirical significance of the market portfolio. Most relevant for the present article is DeMiguel et al. (2009) and Plyakha et al. (2014) who provide empirical evidence that the returns of an equal-weighted portfolio are most of the time higher than those of mean-variance optimal portfolios, which are based on sample means and covariances. These findings confirm an earlier result by Bloomfield et al. (1977) who showed that estimated mean-variance optimal portfolios, typically, do not outperform an equal-weighted portfolio. A second important result for this article is the fact that the market portfolio cannot explain why investors hold short positions. Brennan and Lo (2010) demonstrate that efficient frontiers for which every frontier portfolio has at least one negative weight are difficult to avoid. This finding is inconsistent with the CAPM. According to Fernandez et al. (2016), there are three questions in the centre of the current debate:====Despite this well-known critique, most of the finance literature starts from the assumption that the market portfolio is mean-variance efficient. The classical CAPM has been extended to a multi-period framework in various ways. Fama (1977) starts from an expected risk-return equation that relates the expected return of an asset to the expected return of the market portfolio via the standard security market line. This risk-return equation is static in the sense that all expectations and covariances are not conditioned on available information. Franke (1984) considers a myopic valuation formula based on a multi-period CAPM. In his model, agents have homogeneous beliefs and the time horizon is finite. Merton (1973) and Breeden (1979) extended the classical CAPM to an intertemporal capital asset pricing model (ICAPM) with continuous time which serves as the theoretical foundation for many empirical investigations. The original ICAPM has been refined by many authors, notably by Campbell (1993) and more recently by Campbell et al. (2012) who propose a multi-period setting with discrete time which is akin to our setting. A key feature of their model is a representative agent with an infinite planning horizon and rational expectations. In addition to market returns, the pricing formula established in the refined ICAPM involves a regression on the returns of a so-called hedging portfolio. The model is the basis for a number of empirical investigations including the evaluation of the performance of mutual funds, see Chang et al. (2003) or Maio (2013).====This article is motivated by well-established results for agent-based financial market models in which heterogeneous subjective beliefs of investors take centre stage in the description of how asset prices evolve over time. A comprehensive survey on these models is provided by Hens and Schenk-Hoppé (2009). For models with mean-variance optimising behaviour of myopic investors, Wenzelburger (2004) has introduced a ====, which, unlike the market portfolio, accounts for diverse and erroneous beliefs of investors and thus for heterogeneous investment behaviour. This model may be seen as a dynamic extension of the CAPM with heterogeneous beliefs, introduced in Lintner (1969). Since the reference portfolio coincides with the market portfolio for homogeneous and correct beliefs, it may be seen as a ====. However, the caveat of this concept is that the value of this portfolio may be negative so that its return may not be well-defined.====This article will show that the reference portfolio retains all of its properties when suitably adapted to a more general setting in which, instead of a price process generated by trading activities of agents, asset returns are stipulated by an exogenous stochastic process. The only assumption is that first and second moments of the stochastic process are finite. It will turn out that the central tenets of the CAPM hold in this setting, if the market portfolio is replaced by a mean-variance portfolio that includes the risk-free asset. This mean-variance efficient portfolio is normalised to a reference portfolio which is a tangency portfolio whenever the weights for risky assets sum up to unity. Since the sum of these portfolio weights may well be negative, the reference portfolio, unlike the tangency portfolio, always exists.====The main goal of this article is to show that the answer to all of the above three questions is yes, if the assumption that the market portfolio is mean-variance efficient is discarded. The ==== is by construction ==== for any two consecutive trading periods. Its beta coefficients are time dependent and capture the full cross-sectional variability of a given returns process. With these coefficients, the classical security market line is generalised to a time-dependent ==== that allows for a natural distinction between ==== and ==== risk. The reference portfolio attains the highest possible Sharpe ratio conditional on available information. It is shown, both theoretically and numerically, that the reference portfolio will, in general, not coincide with the market portfolio. Hence, its beta coefficients will not coincide with their counterparts in traditional finance.====The setting in this article imposes no restriction on the preferences and the rationality of investors. With no economic assumption on the formation of asset prices, it shows that the mean-variance analysis and the CAPM may well coexist with irrational behaviour of heterogeneous agents, specifically, as suggested by prospect theory (Kahneman and Tversky, 1979) and cumulative prospect theory (Tversky and Kahneman, 1992). The findings of this article thus support Levy (2011) who emphasises the importance of mean-variance analysis and the CAPM for academics and practitioners alike.====The results of the article confirm as well as contradict findings in the empirical finance literature. On the one hand side, the article provides a theoretical explanation as to why the market portfolio will, in general, not be mean-variance efficient. In contrast to the market portfolio, a reference portfolio naturally allows for short positions. Thus, Brennan and Lo (2010)’s ’impossible frontiers’ become possible as time-dependent efficient frontiers pertaining to a reference portfolio. This article will contradict findings in Bloomfield et al. (1977) by showing that a reference portfolio outperforms any other portfolio in the sense that it provides the best trade-off between risk and return as its conditional Sharpe ratios are always higher. These results re-inforce the view hat the under-performance of estimated mean-variance optimal portfolios is caused by imprecise sample means and sample covariances. From the theoretical viewpoint, therefore, the conclusion is that the market portfolio should be replaced by a reference portfolio.====The article is organised as follows. Section 2 contains all theoretical results of the paper. These findings are illustrated with a simulation exercise of a stochastic volatility model in Section 3. Section 4 discusses the relation to the agent-based CAPM. Conclusions are given in Section 5. All technical proofs are collected in an appendix.",Mean-variance analysis and the Modified Market Portfolio,https://www.sciencedirect.com/science/article/pii/S0165188919302167,13 December 2019,2019,Research Article,365.0
"Francetich Alejandro,Kreps David","School of Business, University of Washington Bothell, Washington, United States,Graduate School of Business, Stanford University, California, United States","Received 29 August 2018, Revised 4 September 2019, Accepted 24 November 2019, Available online 10 December 2019, Version of Record 21 January 2020.",https://doi.org/10.1016/j.jedc.2019.103814,Cited by (6),"We study heuristics for a class of complex multi-armed bandit problems, the period-by-period choice of a set of objects or “toolkit” where the ","When hiring, the manager of a firm may consider hiring multiple candidates at once, at least probationally. If she is uncertain about the candidates’ productivity and she can only observe their performance on the job, if hired, then the hiring problem becomes a multi-armed bandit problem where the arms are the different teams of potential employees. These arms are not independent on two grounds: (a) Learning about the productivity of a candidate might provide valuable information about others with similar qualifications and experience; and (b) even if productivity is independent across candidates, the productivity of two different teams with common members will not be. Thus, the well-known Gittins-index solution for bandits with independent arms (Gittins and Jones, 1974) does not apply.====The general problem of sequentially choosing subsets of some set when their distribution of value is unknown can be formulated as a dynamic-programming problem. However, except in very special cases, such problem is unsolvable—either analytically or numerically—when the set from which we choose is “large” (say, has four or more elements).==== When real economic agents face problems with this structure, we posit that they employ heuristics or rules of thumb. Francetich and Kreps (2019) analyzes simple heuristics that only employ accumulated data. The present paper examines more-sophisticated heuristics that employ the decision maker’s prior assessment of the problem and incorporate Bayesian updating.====Heuristics for multi-armed bandit problems have been extensively analyzed in a literature that spans computer science and operations research (CS-OR) under the rubric of ====. Various categories of bandit problems with non-independent arms are investigated, including linear bandits, Gaussian bandits, and smooth bandits. (For an introduction to this literature focused on Thompson Sampling, see Russo et al., 2018)) The heuristics range from relatively simple to sophisticated and employ both Bayesian and classical inference methods. The paper closest to ours is Sauré and Zeevi (2013), in which the arms of the bandit are subsets of a set of products that a retailer can display to each of a finite set of sequentially-arriving customers.====Our paper departs from the bandit-learning literature in the criterion employed to assess the heuristics. The typical criterion in CS-OR is the minimization of expected undiscounted asymptotic regret. Roughly speaking, the decision-maker’s regret in any period is defined as the difference between what she would receive were she clairvoyant—namely, if she knew the distribution—and what she actually receives by employing a specific heuristic. She seeks to minimize the expected value of the undiscounted sum of her period-by-period regret. This criterion biases her search among heuristics towards those that will learn the truth (or, at least, enough of the truth so that her within-period regret is eventually zero), and only if that is assured does she consider the speed and cost of the learning process.==== Instead, we employ the criterion of maximizing the expected sum of discounted rewards, which is more standard in the economics literature. As is well known for independent-arm bandit problems, as long as future rewards are discounted, there is always positive probability that the decision maker ==== settles for an ==== arm. This means that the optimal strategy under discounted rewards gives infinite asymptotic expected regret.====The optimal strategy is practically unobtainable in our problem, but the fact that it fares poorly (relative to alternatives) under the expected asymptotic regret criterion suggests that, for a decision maker who discounts rewards, prescriptions of the bandit-learning literature must be carefully considered. The direct message of this paper is that this is so: Under discounted rewards, bandit-learning heuristics can perform badly compared to heuristics that are based on the considerations of the classic exploitation–exploration tradeoff.====Specifically, we formulate a stylized model of this type of problem and examine six Bayes-rule based heuristics, two of which—Thompson Sampling and Upper Confidence Bounds—are generally “winners” in the bandit-learning literature. A few theoretical results about the long-run behavior of these heuristics are given, which explain why Thompson Sampling and Upper Confidence Bounds do well for infinitely-patient decision makers. However, when shorter-run costs are taken into account due to discounting, we see in simulations that these long-run-excellent heuristics can fall short in comparison to heuristics that take more seriously the exploitation–exploration trade-off.====As in the companion paper, we do not claim that we have identified the ultimate list of heuristics. We motivate our heuristics either by their simplicity, desirable asymptotic properties, or performance in simulations. This leads us to the broader, less-direct message of this research: While simulations are more problem-specific than theorems, we propose that carefully examined simulation results can provide valuable insights in settings where more-formal analysis—including the identification of payoff bounds, the standard approach in CS-OR—is precluded by complexity.====The rest of the paper is organized as follows. Section 2 presents the formulation of the problem. Section 3 recounts standard results on the optimal solution to the decision-maker’s problem. In Section 4, we describe the six Bayes-rule based heuristics. Section 5 presents asymptotic results, while Section 6 turns to simulations. Section 7 concludes. Proofs are relegated to the appendix. A second, online appendix with supplementary material including R-language and additional data from simulations is available on the website of the first author.","Choosing a good toolkit, II: Bayes-rule based heuristics",https://www.sciencedirect.com/science/article/pii/S0165188918302689,10 December 2019,2019,Research Article,366.0
"Francetich Alejandro,Kreps David","School of Business, University of Washington Bothell, Washington, United States,Graduate School of Business, Stanford University, California, United States","Received 29 August 2018, Revised 4 September 2019, Accepted 24 November 2019, Available online 9 December 2019, Version of Record 19 December 2019.",https://doi.org/10.1016/j.jedc.2019.103813,Cited by (4),"The dynamic problem of choosing subsets of objects or “toolkits” when their value distribution is unknown is a multi-armed bandit problem with non-independent arms. Accordingly, except for very simple specifications, this problem cannot (practically) be solved, either analytically or numerically. ==== facing this problem must resort to decision heuristics, employing past experience and, perhaps, what they know about the problem. This paper focuses on ==== heuristics, the simpler and more naive end of the spectrum of heuristics where the decision maker is guided entirely by past experience. Prior-free heuristics can take a variety of forms, depending on the decision maker’s unit of analysis: tools or toolkits. We examine and compare different prior-free heuristics using both analytical methods and simulations. In a companion paper, Francetich and Kreps (2019), we examine heuristics in which the decision maker engages in ==== updating of her prior beliefs about the environment.","Consider the manager of a professional services firm who must decide on whom to employ to best serve the company’s current and potential new clients. Is Expert ==== going to be worth his salary? By looking at ====’s c.v. and talking to ====’s references, the manager can form a prior assessment on the matter. But imagine that, to know how valuable ==== will be on this particular job, he must be (probationally) hired and evaluated on the job. If there are multiple candidates with ====’s expertise, should she hire candidate ====1 or ====2? If she must hire one or the other, the manager’s problem becomes, essentially, a multi-armed bandit problem. But if she can hire both (at least probationally), the problem is a more difficult multi-armed bandit problem with three arms to the bandit: Hire only ====1, hire only ====2, or hire both. Moreover, suppose there are possible interactions between the professionals she hires; it might be that ====1’s abilities complement those of ====1 and/or ====2, but the only way she can learn this is by having ====1 and ====1, or ====1 and ====2, or ====1 and both ====1 and ====2 on staff. What hiring policy should the manager follow?====In this paper and a companion, we study a simplified and stylized version of this type of decision problem. Each date, a decision maker chooses a subset from a finite set of ====; we call these subsets ====. Depending on the state of nature that prevails and on the toolkit chosen, the decision maker obtains a reward net of the toolkit’s “rental” cost and collects information about the state. The larger the toolkit, the more informative it is. The sequence of states of nature is i.i.d. but their marginal distribution is unknown to the decision maker; she learns about it “as she goes.” Thus, she faces an exploration–exploitation tradeoff: By choosing larger toolkits, her gross reward and information gain will be larger, but her net reward may be smaller. Her objective is to maximize her discounted sum of net rewards.====If we add to the formulation that the decision maker has a prior over the law of states, then the problem of ==== choosing toolkits is a multi-arm bandit problem. Multi-armed bandit problems can be solved in terms of the well-known Gittins Index (Gittins and Jones, 1974) when the arms are statistically independent—that is, if learning about the return distribution of one arm tells the decision maker nothing about other arms. In some applications of the present problem, however, independence may not be an entirely natural assumption: The performance of an employee is likely to be informative about the productivity of candidates with similar qualifications and experience; if the set of tools includes a selection of wrenches, learning about the value of one wrench might provide valuable information about the rest. More fundamentally, even if the rewards from individual tools are independent, the rewards from different overlapping toolkits are ====. Thus, the Gittins Index cannot be employed. In fact, all but the simplest of specifications of the problem are practically unsolvable.====Real economic agents face problems with this basic structure, make decisions, and live with the consequences. Unable to resort to the Gittins Index and in light of the complexity of the problem, how do they proceed? Presumably, they employ heuristics or rules of thumb (or just “go with their gut”).==== The heuristics employed could range from relatively simple-minded to quite sophisticated and complex; indeed, there is a large literature that spans the fields of computer science and operations research (CS-OR), going by the rubric of ====, that investigates heuristics on the sophisticated end of the spectrum.====In this paper, we examine some relatively simple-minded heuristics, ones that do not invoke or involve the decision-maker’s prior assessment over the law of the states. Instead, and in the spirit of ====, the decision maker keeps track of some performance index for each tool or toolkit; at each date, she chooses a toolkit that is the “best” according to the indices, which she then updates based on the outcome in that date. In our companion paper, “Choosing a Good Toolkit, II: Bayes-rule Based Heuristics,” we examine heuristics on the more sophisticated end of the spectrum, with the focus on heuristics that are popular in the CS-OR literature.====In both papers, our interest is on the relative performance of the different heuristics when the decision maker’s discount factor is bounded away from 1. Both papers provide theoretical results on how the heuristics perform as the discount factor approaches 1. In terms of economic analysis, however, we believe that it is most important to understand how the heuristics perform for discount factors close to 1 but, at the same time, not approaching 1. To do this, we resort (in both papers) to simulations of the heuristics on simple specifications of the general problem, drawing conclusions from the results of those simulations that we believe generalize beyond the simulation details.====Our objective is two-fold. First, we believe that the problem of selecting sets of items where the selection process is important both in terms of exploitation and exploration, whether the items are actual tools, human resources, or products to advertise on a webpage, is an important economic problem. Insights into which heuristics work well in which circumstances are valuable. Moreover, we believe that insights can and ==== be obtained using simulations when formal propositions cannot be obtained (and even when they can). At least among micro-theorists, this belief is controversial.==== We argue that a policy of “formal results only” can severely limit the range of questions that can be examined, and we hope that the analysis provided by these two papers will convince micro-theorists that simulations can yield important insights that may otherwise be unavailable (macroeconomists by and large have already learned this lesson).====Now, we do not claim that all of the heuristics we analyze are equally desirable, nor that our list of heuristics is the ultimate, complete list. Our choice of heuristics is motivated by either their simplicity, their asymptotic properties, or their performance in simulations.====Simple models of learning have a long history in cognitive psychology; Gigerenzer and Todd (2000) is an excellent first reference. They have also been studied in various forms in economic contexts. Early references (which discuss the use of heuristics generally) include Simon’s (Simon, 1959, Simon, 1979, Simon, 1982a, Simon, 1982b, Simon, 1997) discussion of bounded rationality, Baumol and Quandt (1964) on ==== and Radner (1975) on ====. More recent work, which emphasizes adaptive learning, includes Roth and Erev (1998), Schlag (1998), Lettau (1999), Rustichini (1999), and Easley, Rustichini, 1999, Easley, Rustichini, 2005.====The basic notion of reinforcement learning can and has been “twisted” in a number of interesting directions. For instance, Schlag (1998) considers the issue of social learning and imitation. If you are learning in the company of others, whom should you imitate? Easley and Rustichini (2005) consider situations where the underlying process is “complex,” not i.i.d. but (say) Markovian. Essentially, their decision maker is trying to understand the transition probabilities so she can make good contingent decisions. Milgrom and Roberts (1991) examine reinforcement co-learning in the play of normal-form games.====Under prior-free heuristics, the decision maker uses the past to compute performance indices for the variety of choices she might make. Her basic “unit of analysis” can be either the tool or the toolkit. The manager may asses employees’ performance individually or within their team. She can ask: ==== Or instead: ==== Since value accrues from the toolkit, it is perhaps obvious that the correct unit of analysis is the toolkit. But with ==== tools, there are 2==== toolkits, so the decision maker might decide that it is simpler to make her unit of analysis the tool rather than the toolkit. How big a price (in terms of her long-run performance) does she pay by doing so? Also, how should the contribution of a tool be measured? Is it the “simple value” of the tool, or the value it adds to a toolkit? Moreover, while prior-free heuristics do not take explicit account of the exploration–exploitation tradeoff, simple and accessible variations on them (such as upper confidence bounds) adapted from the bandit-learning literature are available that do so:==== Do they add much value? Notice that, given the manner in which we’ve formulated the problem, there can be some exploration in exploitation: The decision maker can update the performance indices of all tools in the toolkit she chooses if she focuses on a tool-level analysis, or of all sub-toolkits of the toolkit she chooses if she chooses toolkits as her unit of analysis.====The rest of the paper is organized as follows. Section 2 presents the formulation of the problem. Section 3 introduces the performance indices to be employed by the heuristics, while Section 4 presents the heuristics. Formal asymptotic results are presented in 5 Some asymptotic results, 6 Simulations describes and analyzes our simulations. Finally, Section 7 concludes. Proofs are relegated to the appendix. A second, online appendix with supplementary material such as the detailed proof of one less-significant proposition, the R-language scripts employed in our simulations, and complete data from the simulations, is available on the website of the first author.","Choosing a good toolkit, I: Prior-free heuristics",https://www.sciencedirect.com/science/article/pii/S0165188918302690,9 December 2019,2019,Research Article,367.0
"Fabbri Giorgio,Faggian Silvia,Freni Giuseppe","Grenoble INP, GAEL, University Grenoble Alpes, CNRS, INRA, Grenoble, France,Department of Economics, Ca’ Foscari University of Venice, Italy,Department of Business and Economics, Parthenope University of Naples, Italy","Received 10 January 2019, Revised 10 November 2019, Accepted 2 December 2019, Available online 6 December 2019, Version of Record 24 December 2019.",https://doi.org/10.1016/j.jedc.2019.103818,Cited by (6),"We develop a spatial resource model in continuous time in which two agents strategically exploit a mobile resource in a two-region setup.====To counteract the overexploitation of the resource (the ====) that occurs when players are free to choose where to harvest, the regulator can establish a series of spatially structured policies. We compare the equilibria in the case of a common resource with those that emerge when the regulator either creates a natural reserve, or assigns Territorial User Rights to the players.====We show that, when technological and preference parameters dictate a low harvesting effort, the policies are ineffective in promoting the conservation of the resource and, in addition, they lead to a lower payoff for at least one of the players. Conversely, in a context of higher harvesting effort, the intervention can help to safeguard the resource, preventing extinction while also improving the welfare of both players.","Traditional management tools have often been unsuccessful in preventing the rapid decline of natural resource stocks. As a consequence, over the last few decades the management of renewable and exhaustible resources has been increasingly enforced via property rights, in particular via spatial rights. Practiced over centuries in some parts of the world (Japan, for example), resource management via spatial rights has now spread worldwide (see Quynh et al., 2017, for fisheries), and is usually introduced to address the lack of well-defined property rights of the commons. However, only a few of the resources involved are completely immobile. Fish stocks, to take an obvious example, are spatially distributed and in many cases move across different locations. Similarly, stocks of air or water pollutants are rarely stationary at the emission point, but diffuse in space. Even water reservoirs, and some exhaustible resources such as oil deposits, have spatial dynamics. Hence the theoretical literature has warned from the start that if a ==== resource moves across different locations in a fully connected network, then Territorial User Rights (TURF in the case of fisheries), which assign units of space to single agents, cannot be expected to be very effective at solving the overexploitation problem that tends to arise under common property (e.g, Costello, Quérou, Tomini, 2015, Janmaat, 2005, Kaffine, Costello, 2011, Quérou, Costello, Tomini, White, Costello, 2011). For all that matters, the spatial externality generated by the migration of the stocks allows each agent with access to the network at a single node to actually access the whole resource. So ill-defined property rights persist after the introduction of Territorial User Rights.====In recent years, the fields of growth theory and environmental and resource economics have developed tools to face the challenging task of modeling the economic forces that shape the dynamics of extraction of moving spatially distributed stocks (see e.g., Smith, Sanchirico, Wilen, 2009, Xepapadeas, 2010 and Brock et al., 2014, for surveys). In other works, optimal harvesting is studied for of an immobile spatially distributed resource, see for instance Behringer and Upmann (2014). Dynamic strategic interaction, however, is largely absent from the studies that have introduced spatial-dynamic processes in growth or resource models. In these works, the analysis generally proceeds either on the assumption that rent dissipates instantaneously (e.g., Sanchirico and Wilen, 1999), or on the assumption that the planner either controls the entire environment (e.g, Boucekkine et al., 2013), or takes the spatially distributed stock path as given (e.g., Janmaat, 2005, Santambrogio, Xepapadeas, Yannacopoulos). Clearly, these assumptions are not well suited to the analysis of the spatial externalities that arise when the resource is a moving spatial distributed stock but access is restricted to a small number of extractors. As a consequence, there are still very few studies that contain analytically or numerically tractable dynamic games (Bhat, Huffaker, 2007, Costello, Nkuiya, Quérou, 2019, de Frutos and Martin-Herran, 2019, Herrera, Moeller, Neubert, 2016, Kaffine, Costello, 2011, Quérou, Costello, Tomini).====On the other hand, stationary Markov perfect Nash equilibria in models with a single common-property resource have been studied under different hypotheses in the literature (see e.g., Clemhout, Wan, 1985, Dasgputa, Mitra, Sorger, 2019, Dockner, Sorger, 1996, Levhari, Mirman, 1980, Mitra, Sorger, 2014, Mitra, Sorger, 2015, Negri, 1989, Rowat, Dutta, 2007, Sorger, 1998, Strulik, 2012, Strulik, 2012, Tornell, Lane, 1999, Tornell, Velasco, 1992 and, for a survey of the literature, Long, 2011, Long, 2016). In the typical setting a homogeneous stock, whose growth function is known, is harvested by a finite number or mass of identical agents who reap utility from consuming the resource. Since analysis of the Markov perfect Nash equilibria has turned out to be difficult, straightforward results have been obtained only for special growth and utility functions (see e.g., Dockner et al., 2000, section 12.1 for the case of an exhaustible resource with a isoelastic instantaneous utility function). Although there are a few exceptions, the usual conclusion in this literature is that non-cooperation leads to overexploitation of the resource (the so-called “tragedy of the commons”).====In this work we develop a simplified framework to study “resource wars” with spatially distributed stocks. Our aim is to provide an analytically tractable model that generalizes some of the results obtained in the literature that studied Markov perfect equilibria in differential games with a homogeneous stock, and to highlight how difficult it is to design efficient systems for the management of resources based on spatial property rights, if the spatial externalities stemming from the movements of the stocks are not completely internalized. We compare the behaviors of agents in an initial ====, in which they can decide both where and how much to harvest, with their choices in policy-constrained cases, in which the regulator can establish a natural reserve or assign a harvesting location to each agent. We show that implementing these policies can only be effective when the agents choose a high harvesting effort.====To have an analytically solvable model, some simplifications are made. First, we have chosen to study a two-region, two-player case. Second, as is often assumed in the literature, we suppose that the stock diffuses at a constant rate from the higher density to the lower density location. Third, special growth and utility functions are used since, as it is well known, not even mere existence results for Nash equilibria can be obtained in a general framework. In particular, as we look for linear Markov equilibria, tight restrictions must be imposed on the primitives of the model: we use throughout the paper the family of isoelastic utility functions and linear (re)production functions (see Gaudet and Lohoues, 2008, for the analysis of the conditions that guarantee the existence of a linear Markovian equilibrium in the scalar common pool case).====For the case in which the preferences of the agents and the technology dictate low harvesting effort, the existence of a Markov perfect Nash equilibrium is proved and explicitly characterized in three scenarios: (a) the initial ====, where each agent can decide, at any times, in which regions to harvest in and by how much; (b) the ====, where the regulator forbids agents from harvesting in one of the two regions; (c) the ==== (where TURF stands for Territorial User Rights for Fisheries, as fishery is the straightforward application of the model), where each player can only harvest in an exclusive region. In each situation we characterize the optimal response function of the players, the resource stock evolution (in particular its reproduction rate at equilibrium) and the utility of the players.====It turns out that, in case of low harvesting effort, the mentioned spatial property rights cannot improve the growth rate of the resource and in particular they cannot prevent its depletion in case the implicit rates of growth is positive but small. In addition, their effect on the utilities of the agents is never positive and the policies strictly worsen the utility of at least one of the players.====The analysis of the results allows on the one hand to show (see Section 4.3) that if the elasticity of intertemporal substitution is sufficiently high (higher than 2 with two player, as in our basic model), a ==== (similar to that described by Tornell and Lane, 1999) arises in our spatial context, and on the other hand to identify what kinds of “technological” shocks generate it. As expected, if voracity prevails, then an increase in any of the local intrinsic growth rates of the resource reduces growth. Notably, however, it turns out that a reduction in the spatial mobility of the resource has the same effect.====Things change sharply when a policy induces agents to choose maximal effort (Section 5). Indeed, in the high-intensity harvesting case, the territorial policies we mentioned, and in particular the creation of a reserve, can lead to an effective reduction of the overexploitation and have a consequent positive impact on the rate of reproduction of the resource. The policies can prevent the asymptotic depletion of the resource that would occur under a regime of common property. Moreover, for some sets of parameters, they can also increase the utility of all agents.====Our model is a spatial generalization of the classical Levhari and Mirman (1980) example of a “fish war”, although we use linear growth functions instead of strictly concave functions with a finite carrying capacity.==== On the other hand, our model is closest to those in Herrera et al. (2016) and Kaffine and Costello (2011) (also used in Costello, Nkuiya, Quérou, 2019, Costello, Quérou, Tomini, 2015, Quérou, Costello, Tomini), which use perfect Nash equilibria in the context of mobile spatially distributed stocks. There are still some relevant differences:====Notably, our contribution provides a simple set-up where the policies we consider (TURF, reserve creation) can be directly compared against the pure common case (i.e. the absence of any regulation) so that their impact can be evaluated.====Related papers can be found in other branches of the natural resource economics literature. We mention in particular the bioeconomic model of Bhat and Huffaker (2007) on the dispersion of a small-mammal population over time and the transboundary pollution linear-quadratic differential game proposed by de Frutos and Martin-Herran, 2019. Besides the obvious differences with the model specifications linked to the intrinsic differences between a resource-exploitation model, wildlife control and pollution dynamics, the perfect Nash equilibria, when found, are only partially analytically characterized in these papers.====The paper proceeds as follows: in Section 2 we describe the two-player two-region model, giving in particular the definitions of Markovian Nash equilibrium. In Section 3 we study in full detail the common property scenario, first with relaxed constraints (the relaxed problem) and then with full constraints. In Section 4 we similarly analyze the scenarios in which a marine reserve (Section 4.1) and TURF (Section 4.2) are enforced, and discuss the impact of policies, comparing outcomes and overall growth rates of stock with those of the common property case (Section 4.3). In Section 5 we provide two examples of Markovian equilibria in which agents use their efforts at full capacity. In Section 6 we suggest and briefly discuss several possible extensions of the model. Section 7 contains the conclusions.",Policy effectiveness in spatial resource wars: A two-region model,https://www.sciencedirect.com/science/article/pii/S0165188919302131,6 December 2019,2019,Research Article,368.0
"Ramadiah Amanah,Caccioli Fabio,Fricke Daniel","Department of Computer Science, University College London, United Kingdom,Systemic Risk Centre, London School of Economics and Political Science, United Kingdom,London Mathematical Laboratory, United Kingdom,Deutsche Bundesbank, Directorate General Financial Stability, Germany","Received 15 October 2018, Revised 25 October 2019, Accepted 2 December 2019, Available online 6 December 2019, Version of Record 21 January 2020.",https://doi.org/10.1016/j.jedc.2019.103817,Cited by (21),Financial networks are an important source of ,"The 2007–09 financial crisis has brought the interconnectedness of the financial system to light, and financial networks have been identified as an important source of systemic risk. Accordingly, the regulatory framework has taken a more macroprudential perspective to maintain the stability of the system as a whole. For example, Basel III includes capital surcharges for systemically important financial institutions.====Stress tests are an important tool to assess the vulnerability of a given financial network. To this end, detailed data on (direct or indirect) interactions between individual financial institutions is needed. However, it is difficult to collect such data in full and to make them readily available to researchers (e.g., due to data confidentiality), such that we generally do not have complete information about financial networks. For example, Haldane (2015) suggests that even among the world’s largest banks the collection of interbank exposure data is partial, and even regulators often do not have complete information (Glasserman and Young, 2016). In response, several data collection initiatives have been proposed, but granular interaction-specific data generally remain unavailable (Anand et al., 2017).====Finding accurate reconstruction methods for financial networks from partial information is therefore an important topic. Most of the existing work focuses on the case of interbank credit networks (Anand, Van Lelyveld, Banai, Friedrich, Garratt, Hałaj, Fique, Hansen, Jaramillo, Lee, Molina-Borboa, Nobili, Rajan, Salakhova, Silva, Silvestri, De Souza, 2017, Gandy and Veraart, 2019, Squartini, Almog, Caldarelli, Van Lelyveld, Garlaschelli, Cimini, 2017). Over the last decade, common asset holdings (or overlapping portfolios) have been identified as an important source of systemic risk via price-mediated contagion (Caccioli, Shrestha, Moore, Farmer, 2014, Cont, Wagalath, 2016, Fricke and Fricke, 2020, Greenwood, Landier, Thesmar, 2015, Gualdi, Cimini, Primicerio, Clemente, Challet, 2016, Lillo, Pirino, 2015, Shleifer, Vishny, 2011). The idea is that, when they suffer a decline in their investment portfolios, leveraged investors often have to liquidate (parts of) their investments (Adrian and Shin, 2010). Such liquidations can have systemic effects when asset sales are synchronized among many investors, potentially leading to fire sale contagion dynamics. Empirical evidence suggests that fire sales occur in many different markets (see, e.g., Pulvino, 1998 for real assets, Coval and Stafford, 2007 for equities, and Ellul et al., 2011 for corporate bonds), which can result in contagious dynamics between asset classes (see, e.g., Manconi et al., 2012).==== Hence, understanding the structure and dynamics of common asset holdings is important (Fricke, 2016), but often hampered by data availability.====In this paper, we focus on reconstructing and stress testing bipartite credit networks using detailed micro-data on bank-firm credit interactions in Japan for the period 1980–2010. We explore the performance of several network reconstruction methods at different aggregation levels along two different dimensions. First, we look at their capability to reproduce the topological features of the observed credit networks. This part of the paper is closest to some recent works on unipartite interbank networks (e.g., Anand, Van Lelyveld, Banai, Friedrich, Garratt, Hałaj, Fique, Hansen, Jaramillo, Lee, Molina-Borboa, Nobili, Rajan, Salakhova, Silva, Silvestri, De Souza, 2017, BIS, 2015, Mazzarisi, Lillo, 2017). Different reconstruction methods require different amounts of information as inputs, and we aim to understand how adding such information affects a method’s performance, since one would expect that methods that take more information into account should be able to reproduce the network more accurately. Interestingly, we find that this is not always the case. Overall, there is no single “best” reconstruction method – it depends on the assumed criterion of interest.====We then test each method’s ability to reproduce observed levels of systemic risk. For this purpose, we use the fire-sale stress test of Huang et al. (2013) and apply it to the actual and the reconstructed credit networks. To the best of our knowledge, this is the first paper to conduct a horse race of bipartite network reconstruction methods in terms of their implied levels of systemic risk.==== Our main findings are as follows: first, we identify a significantly negative time trend for the observed systemic risk levels of the Japanese banking system, suggesting that the system has become less vulnerable to systemic asset liquidations over time. Second, in many instances the actual credit networks display the highest levels of systemic risk, at least for the most disaggregated bank-firm interactions. In other words, many reconstruction methods tend to underestimate systemic risk. This is remarkable given that the reconstruction methods under study here can generate completely different network architectures; for example, the MaxEntropy (MinDensity) approach yields a maximally (minimally) connected credit network. Moreover, we find that the network aggregation level can affect the performance of the different reconstruction methods.====Lastly, given that the observed credit networks tend to display relatively high levels of systemic risk compared to most reconstruction methods, we explore different policies (such as merging or breaking-up banks, or leverage caps) in order to improve the robustness of the system. Our main finding is that no single policy can reduce the systemic risk level of the actual network to that of the most stable reconstruction method. Nevertheless, we find that leverage caps and bank mergers could improve the robustness of the network. This finding is driven by the fact that the largest banks in our sample tend to be less leveraged. Therefore, merging those banks results in a very large, but moderately leveraged bank which is less likely to spread shocks through the system.====Overall, this paper contributes to different strands of literature: first, we add to the growing literature on reconstructing financial networks from partial information (Anand, Van Lelyveld, Banai, Friedrich, Garratt, Hałaj, Fique, Hansen, Jaramillo, Lee, Molina-Borboa, Nobili, Rajan, Salakhova, Silva, Silvestri, De Souza, 2017, Gandy and Veraart, 2019, Squartini, Almog, Caldarelli, Van Lelyveld, Garlaschelli, Cimini, 2017, Squartini et al., 2018). For the case of bipartite networks we are only aware of the works of Di Gangi et al. (2018) and Squartini et al. (2017). Given that most reconstruction methods have been designed for the case of unipartite credit networks, we adjust some of these methods to the case of bipartite networks. Second, we contribute to the literature on systemic risk assessment by performing stress tests both for the actual and the reconstructed credit networks in Japan. Lastly, we contribute to the literature that explores the effects of aggregation on stress test results. For example, Hale et al., 2015 study the optimal aggregation level for stress testing models on macroeconomic variables, and they find that the aggregation level matters. We obtain a similar conclusion based on a completely different stress testing approach.====The remainder of this paper is structured as follows: Section 2 defines the credit network at different aggregation levels, and Section 3 briefly describes the dataset. In Section 4, we explore the performance of network reconstruction methods in terms of their ability to match the observed credit network topology. In Section 5, we look at the capability of each methods to reproduce the observed levels of systemic risk. In Section 6, we analyze different policy measures in order to improve the robustness of the system. Section 7 summarizes the main findings and concludes.",Reconstructing and stress testing credit networks,https://www.sciencedirect.com/science/article/pii/S016518891930212X,6 December 2019,2019,Research Article,369.0
"Feng Xu,Lu Lei,Xiao Yajun","College of Management and Economics, Tianjin University, China,Asper School of Business, University of Manitoba, Canada,Michael Smurfit Graduate School of Business, University College Dublin, Ireland","Received 22 April 2019, Revised 2 November 2019, Accepted 28 November 2019, Available online 6 December 2019, Version of Record 23 December 2019.",https://doi.org/10.1016/j.jedc.2019.103816,Cited by (4),Trust companies generate leverage cycle dynamics by intermediating less regulated credit to the financial markets in China. We find that the leverage factor constructed from trust companies can explain the time-series and cross-sectional ,"In the aftermath of the global financial crisis, emerging markets, particularly China and India, experienced a rapid expansion in shadow banking and witnessed a recurrence of a leverage cycle remarkably similar to that of the U.S.==== Following the rapid development of shadow credit, leverage had surged in both countries before China’s equity market crash in June 2015 and before India’s liquidity crunch in September 2018, which saw leverage hit bottom.==== A fair amount of research has been conducted on the regulatory arbitrage, credit intermediation, and leverage-taking present in shadow banking in the emerging markets. However, the scope of such research has been limited to financial stability in the banking sector (e.g., Acharya et al., 2013 and Chen et al., 2018). Little has been done to examine whether leverage risk from shadow banking activities transmits to asset markets or has material impacts on asset prices. We fill this gap by presenting empirical evidence that leverage risk is priced in the largest emerging market – China, where shadow banks intermediate credit to the asset market. We find that in emerging markets, the marginal investors are shadow banks rather than broker–dealers, as in the sophisticated markets (e.g., the U.S.). We also shed light on how to use market data to determine the unobserved investors’ leverage, which is much needed for regulators and policymakers to monitor leverage.====To describe how leverage risk from shadow banks affects asset prices, we modify the intermediary asset pricing model (e.g., Adrian and Shin, 2014) to capture the excessive leveraged bets financed by wealth management products (WMPs) in bank-trust cooperation in China. WMPs are perceived to be as safe as deposits, but with high yields, which caters to households’ risk appetites. The issuance of WMPs has skyrocketed since the early of 2000s. WMPs can be thought of as simple structural products for which banks subscribe to senior tranches and other intermediaries subscribe to junior tranches. Junior tranche holders effectively borrow from households and trust companies are major subscribers to junior tranches in practice (e.g., Hachem and Song, 2016). Banks and trust companies cooperate to remove WMPs from bank balance sheets, as trust companies use little equity to borrow, which increases their leverage. Trust companies can either lend WMP funds to end-users to invest in asset markets or to other intermediaries. These inermediareis earn rent as the middlemen between trust companies and end-users. Using new financial innovations and technologies, they borrow a great deal more from trust companies against even less equity. Such credit intermediation increases the investment leverage, thereby developing the leverage cycle. We model an economy that involves households, intermediaries, and investors. Intermediaries channel WMP funds to investors and take the maximum leverage to earn rent until their value-at-risk constraint becomes binding. We derive a leverage-based CAPM that associates the asset return with trust companies’ leverage and risk management. This motivates us to use the leverage factor constructed from trust companies for our empirical analysis. Our model can also be used to estimate the investment leverage, which can be obtained in the absence of account-level data. In a standard distress test with the loss rate tolerance to 30%, we uncover the investment leverage ratio is 18 in the stock market and 9 in the bond market. Finding such a leverage ratio is useful for the regulators and policymakers to measure and monitor leverage-taking in financial markets when the leverage ratio is unobservable.====We construct the trust leverage factor using the leverage of trust companies and test its explanatory and predictive power.==== We have following findings. First, we regress monthly returns of stocks (or bonds) on monthly trust leverage factor of past 24 months to obtain leverage beta for each stock (or bond). The portfolio analysis shows that the trading strategies by longing high leverage beta and shorting low leverage beta earn positive and statistically significant returns for stocks and bonds, respectively. Second, the trust leverage factor has significant power in explaining both cross-sectional and time-series asset returns. The cross-sectional regressions of the returns of each asset against the trust leverage factor show that the prices of leverage risk are positive and statistically significant. The reason is as follows. Trust companies finance assets through financial innovations. As trust companies’ funding conditions become binding, they will deleverage,==== leading to higher marginal utility of wealth. Therefore, the exposure (or sensitivity) to trust leverage, high covariance between an asset’s return and trust companies’ leverage, should price cross-sectional asset returns. Third, to measure the economic magnitude of the price of leverage risk, we construct a return-based leverage factor by projecting the (nontraded) trust leverage factor onto the leverage factor mimicking portfolio (LMP) returns. We find that the price of risk for the return-based leverage factor is also economically significant. The annualized price of leverage risk is 12% for stocks, 8% for bonds, and 8% for all assets. Finally, the time-series regressions show a significant negative relation between future asset returns and contemporaneous trust leverage. Intuitively, before the economy enters a recession, leveraged investment is massive, giving rise to a positive innovation to leverage. In the recession, the leverage constraint binds, forcing investors to deleverage to satisfy capital requirements. As a result, a lower price needed to clear the market in equilibrium suggests a lower expected future asset return.====Adrian et al. (2014) document that broker–dealers are the marginal investors in developed countries. We show that this does not hold in emerging markets. In China, securities companies, the counterparts of broker–dealer intermediaries, provide margin loans for leveraged investment. Extensive media coverage has strongly maintained that securities companies stimulated investment leverage and boosted the stock market rally in China before its stock market crashed.==== Our results show that the leverage factor constructed from securities companies cannot explain cross-sectional and time-series asset returns. The prices of risk for the security leverage factor are either negative or insignificant. Securities companies are entities licensed to finance leveraged trading in selected stocks via margin trading and short selling. Furthermore, their leverage measures the cost of funding in trading these stocks. However, brokerage lending is often limited and thus the explanatory power of the security leverage factor is restricted. When we examine the selected stocks that are available for margin trading and short selling through the securities companies, the explanatory power of the security leverage factor is insignificant once the leverage factor constructed from trust companies is controlled. Our work complements the empirical evidence of intermediary asset pricing theory in Adrian et al. (2014) and He et al. (2017). This suggests that shadow banks (i.e., trust companies) other than securities companies are marginal investors in the Chinese asset market.====We conduct a number of robustness analyses that support the pricing ability of the trust leverage factor. To alleviate the issue of small samples, we obtain a longer time series of monthly trust leverage factor using two approaches.==== The first approach involves constructing monthly trust leverage factor using the mimicking portfolio. The second one involves collecting the daily online “trust plans” released by trust companies, aggregating the monthly assets under management (AUM), and deriving a monthly trust leverage factor. The results for the monthly trust leverage factor constructed from these two approaches are qualitatively the same as the main results. Secondly, we explore a comprehensive race between the trust leverage factor and other conventional factors (e.g., the Fama-French five factors, the momentum factor, and the Pastor-Stambaugh liquidity factor) and find that the trust leverage factor survives the race.====This paper is related to other papers documenting empirical evidence on intermediary asset pricing theory. Adrian et al. (2014) use broker–dealer book leverage to explain asset returns in the U.S. equity and bond markets. He et al. (2017) use the market leverage of security broker–dealers’ holding companies to explain the returns of multiple assets. Baron and Muir (2018) show international empirical evidence that the balance sheet expansion of intermediaries negatively predicts asset returns in the U.S., the U.K., and Japan from1870 to 2016. Chen et al. (2019) find that the trading quantities of the deep out-of-money options of intermediaries are associated with high risk premia for multiple assets as well. We are the first to report empirical evidence from the emerging markets. In particular, we derive a leverage factor from trust companies other than security broker–dealers and show that trust companies rather than security broker–dealers are marginal investors in China. Further, our method of leverage estimation can be used by regulators and policymakers who do not hold account-level data to monitor leverage.====This paper is related to intermediary asset pricing theory. Early studies in this area include those of Bernanke and Gertler (1989) and Kiyotaki and Moore (1997). A large body of literature about dynamic asset pricing models accounts for the intermediary constraints in the aftermath of financial crisis. Our leverage-based CAPM is related to recently burgeoning theoretical work, including Brunnermeier and Pedersen (2009), Geanakoplos (2010), He and Krishnamurthy (2012), He and Krishnamurthy (2013), Adrian and Shin (2014), and Brunnermeier and Sannikov (2014). It features the intermediary leverage constraint, the leverage-taking through financial innovations in bank-trust cooperation, and the split rule of profit generated from these financial innovations between households, investors, and intermediaries. Our model also embodies the argument by Bian et al. (2018) that Chinese shadow banking is bank-centric, implying that shadow banks perform many traditional banking functions of credit intermediation.====This paper is also related to recent research on shadow banking and financial fragility in emerging markets. Acharya et al. (2013) study the determinants of shadow banking expansion in India. Hachem and Song (2016), Acharya et al. (2017), Allen et al. (2017), and Chen et al. (2018) examine the instruments created in shadow banking activities, such as WMPs and entrusted loans, and their implications for financial fragility. Chen et al. (2017) document how China’s four-trillion-yuan stimulus package in 2009 led to the rapid growth of shadow banking. Feng et al. (2019) study to what extent shadow money (WMPs) benefits economic growth and financial stability. Unlike these studies, we examine the asset pricing implications of credit intermediation and leverage amplification in shadow banking.====The remainder of this paper is structured as follows. We provide an overview of brokerage- and shadow-financed leveraged investment in Section 2 and then build a leveraged-based asset pricing model in Section 3. We outline our data and empirical methods in Section 4 and report the main findings for our leverage factor in Section 5. We conduct robustness checks in Section 6 and conclude the paper in Section 7.","Shadow banks, leverage risks, and asset prices",https://www.sciencedirect.com/science/article/pii/S0165188919302118,6 December 2019,2019,Research Article,370.0
"Cardi Olivier,Restout Romain,Claeys Peter","Lancaster University Management School Bailrigg, Lancaster LA1 4YX, United Kingdom,Université de Lorraine, Université de Strasbourg, CNRS, BETA, Nancy 54000, France,Faculteit Economische en Sociale Wetenschappen, Vrije Universiteit Brussel, Pleinlaan 2, Brussel B-1050, Belgium","Received 21 April 2019, Revised 24 November 2019, Accepted 26 November 2019, Available online 2 December 2019, Version of Record 18 December 2019.",https://doi.org/10.1016/j.jedc.2019.103815,Cited by (4),Our paper investigates the sectoral effects of government spending shocks and highlights the role of ,"As documented recently, the global financial crisis has led to an output decline in OECD countries which varies along the tradedness of industries. Using sectoral data for Spain, Arellano et al. (2018) find that non-traded production has decreased significantly more than traded production between 2007 and 2013. Evidence by De Ferra (2018) reveals that non-exporting firms experienced the largest drop in sales, hours worked and investment in Italy in 2009–2013. Using U.S. data between 2007 and 2009, Mian et al. (2014) find that non-traded employment has been more vulnerable to the recession than employment in traded industries as non-traded firms rely heavily on local demand. To the extent that expansionary fiscal policy targets non-traded industries, a rise in government spending could potentially be an appropriate tool to stabilize output in non-exporting sectors, as emphasized by Schmitt-Grohé et al. (2013). Yet at an empirical and theoretical level, the systematic exploration of how a rise in government spending impacts the non-traded vs. the traded sector is still lacking. In the present paper we address the following question: Do shocks to government consumption affect sectors symmetrically and if not, what are the causes of this asymmetry? We find that shocks to government consumption tend to disproportionately benefit the non-traded sector by producing a labor reallocation toward this sector, and all the more so in countries where workers’ costs of switching sectors are lower.====To guide our quantitative analysis, we document VAR evidence on the sectoral effects of a rise in government consumption for sixteen OECD countries. First, a shock to government consumption has a strong expansionary effect on output in the non-traded sector relative to the traded sector. More specifically, we find empirically that a rise in government spending by 1% of GDP increases non-traded value added by 0.7% of GDP on impact and leads to a decline in traded value added. The expansion in the non-traded sector is associated with a rise in the value added share of non-tradables by 0.35% of GDP. Since the latter result indicates that non-traded value added would increase by 0.35% if GDP remained constant, the reallocation of resources toward the non-traded sector thus contributes to 50% of the rise in non-traded value added. The remaining 0.35% of GDP represents the rise in non-traded value added caused by the aggregate fiscal multiplier split across sectors in accordance with their value added share. A necessary condition for the share of non-tradables to increase is that this sector must receive a disproportionate share of the shock to government spending. Our estimates corroborate this hypothesis as we find empirically that government consumption of non-tradables contributes 90% on average to increases in government spending.====For the increase in the share of non-tradables to materialize, productive resources, in particular labor, must be reallocated toward the non-traded sector. The second set of our empirical findings reveals that non-traded hours worked rise by 0.54% of total hours worked, half of this increase being caused by the reallocation of labor. The shift of labor is subject to labor mobility costs, however, since we detect empirically a significant increase in non-traded relative to traded wages. These findings accord well with the evidence documented by Artuç et al. (2010), Dix-Carneiro (2014), Lee et al. (2006) who find substantial barriers of mobility between sectors and furthermore that wages are not equalized across sectors in the short run nor in the long run.====A first way to gauge the role of labor mobility costs for fiscal transmission is to investigate how impact responses of relative sector size vary over time and whether their movements are positively related to labor reallocation following our identified government spending shock. Our estimates reveal that the responses of sectoral shares are reduced over time by about 40% and that this reduction is concomitant and highly correlated with the decline in the rate of workers shifting from one sector to another. When we turn to international differences, the responses of sectoral value added and hours worked shares display a wide cross-country dispersion. Motivated by the cross-country variations in labor mobility costs documented by Artuç et al. (2015), we estimate the elasticity of labor supply across sectors and empirically detect a positive cross-country relationship between the change in relative sector size following a government spending shock and the degree of labor mobility.====To account for our evidence on fiscal transmission, we put forward an open economy version of the neoclassical model with tradables and non-tradables. In calibrating the model to a representative OECD economy, we assume that the non-traded sector receives a share of the rise in government spending which is larger than its relative size, in line with our evidence, so that the government shock is biased toward non-tradables. Our quantitative results show that the model is successful in replicating the sectoral effects of government spending shocks as long as we allow for imperfect mobility of labor (IML henceforth) and capital adjustment costs.====With these two features, the model produces a rise in the share of non-tradables by 0.38% of GDP, close to our empirical findings. If we remove both or either one of these ingredients, the model fails to account quantitatively for our evidence on fiscal transmission, in particular the responses of sectoral output shares which we estimate empirically. Intuitively, if we do not allow for capital adjustment costs, a government spending shock leads to a dramatic fall in investment which offsets the rise in government consumption. As a result, the excess demand in the non-traded goods market is low or even nil. Due to low incentives to shift resources toward the non-traded sector, the open economy experiences a trade balance surplus resulting in the model substantially understating the rise in the share of non-tradables. Conversely, if we allow for capital adjustment costs, the decline in investment is mitigated, which leads to significant excess demand in the non-traded goods market. However, if we impose perfect mobility of labor across sectors (PML henceforth), high incentives to shift resources toward the non-traded sector result in a large trade balance deficit which leads the model to overstate the rise in the share of non-tradables considerably.====By tilting the demand shock toward non-tradables, financial openness and the tradability of goods are also key dimensions that allow our model with IML to account for the evidence. Shutting down the response of the current account leads the model to understate the rise in the share of non-tradables, the latter increasing by an amount which is twice as small as that estimated empirically. The reason is that when the fiscal stimulus is temporary and the economy has perfect access to world capital markets, households find it optimal to borrow abroad to avoid a large decline in consumption and/or a large increase in labor supply. Since traded goods can be imported and non-traded goods must be produced domestically, access to foreign borrowing further biases the demand shock toward non-tradables.====The final exercise we perform is to investigate whether the model can account for cross-country differences in the responses of sectoral output shares to a fiscal shock. We thus calibrate the model to country-specific data. We find quantitatively that impact responses of sectoral output shares to a government spending shock are sensitive to the degree of labor mobility, as they vary between 0.26% and 0.49% of GDP for non-tradables when we move from the lowest to the highest value of elasticity of labor supply across sectors. In line with the evidence, the cross-country dispersion in the sectoral share responses is the result of international differences in the degree of labor mobility, the rise in the output share of non-tradables being more pronounced in countries with a higher degree of labor mobility.====So far, we have not said much about the sectoral fiscal multiplier which is the result of the change in the sectoral share and the rise in real GDP. Because changes in the sectoral value added and the sectoral share are positively correlated, raising the non-tradable content of the government spending shock or the degree of labor mobility across sectors increases the fiscal multiplier for non-tradables. At an aggregate level, a government spending shock produces a larger fiscal multiplier by targeting the sector that has the highest labor compensation share, i.e., the non-traded sector.==== By contrast, by mitigating the rise in non-traded wages and thus aggregate wage growth, a higher degree of labor mobility reduces the magnitude of the aggregate fiscal multiplier.====. We contribute to the extensive literature investigating fiscal transmission both empirically and theoretically by focusing on the reallocation effect of government spending shocks. Like Ramey et al. (1998), we emphasize the importance of the composition of government spending in understanding the sectoral effects of a fiscal shock. In contrast to the authors who consider three episodes of expansionary defense spending in the United States driven by foreign policy, we identify exogenous increases in government consumption by assuming that discretionary government spending is subject to certain decision and/or implementation lags, as proposed by Blanchard et al. (2002). Putting aside the advantages and disadvantages inherent to the narrative and SVAR approaches, the identification scheme does matter, as the identified government spending shock can be intensive either in tradables or non-tradables. While the Ramey-Shapiro narrative approach suggests that military shocks, which are heavily concentrated in the manufacturing sector, are intensive in traded goods, our study reveals that government spending shocks, identified on the basis of Blanchard-Perotti assumption, lead to a sharp increase in non-traded relative to traded output.====This finding is in line with estimates documented by Monacelli et al. (2008), Benetrix et al. (2010) which show that an increase in government spending disproportionately benefits the non-traded sector. In contrast to the authors who restrict their attention to sectoral output or labor effects and thus do not investigate the reallocation effects, our paper analyzes and rationalizes the labor composition effect caused by shocks to government consumption like Bredemeier et al. (2019). Differently, the authors contrast the effects across occupations rather than between sectors.====One additional key finding with respect to the papers mentioned above is that international differences in workers’ costs of switching sectors can account for the cross-country dispersion in the responses of sectoral shares, as we uncover a positive cross-country relationship between the degree of labor mobility and the changes in relative sector size. In this regard, our study can be viewed as complementary to the work by Ilzetzki et al. (2013), Born et al. (2013), Brinca et al. (2016) who contrast the effects of fiscal policy on output across a number of country characteristics. In contrast to these papers focusing on the aggregate fiscal multiplier, we explore the size of sectoral fiscal multipliers resulting from the reallocation of resources across sectors.====Finally, our paper also relates to a broad literature which studies fiscal transmission by breaking down aggregate government spending into sub-categories. While Baxter et al. (1993) differentiate between government consumption and government investment, we restrict attention to government consumption in accordance with the bulk of the literature investigating fiscal transmission. In contrast to a growing literature exploring the impact on private activity of shocks to government purchases from the private sector and the government sector (the latter essentially consisting of compensation of government employees), respectively, see e.g., Bermperoglou et al. (2017), we focus on the sectoral distribution of an increase in aggregate government spending, the public sector being part of the non-traded sector. Nekarda and Ramey (2011) estimate the effects of a rise in industry-specific government purchases and find that industries with higher concentration and unionization rates experience larger increases in output. Differently, we focus on the asymmetric effects across sectors caused by an increase in government consumption by breaking down sectoral effects into reallocation and aggregate effects.====The remainder of the paper is organized as follows. In Section 2, we investigate empirically the sectoral effects of a government spending shock and highlight the role of labor reallocation. In Section 3, we develop an open economy version of the neoclassical model with IML. In Section 4, we report the results of our numerical simulations and assess the ability of the model to account for the evidence. In Section 5, we summarize our main results and present our conclusions. An Online Appendix contains more empirical results and robustness checks, and solves analytically a restricted version of the model to build up intuition on the implications of labor mobility costs.",Imperfect mobility of labor across sectors and fiscal transmission,https://www.sciencedirect.com/science/article/pii/S0165188919302106,2 December 2019,2019,Research Article,371.0
"Hori Keiichi,Osano Hiroshi","School of Economics, Kwansei Gakuin University, 1-1-155, Uegahara, Nishinomiya, Hyogo 662-8501, Japan,Institute of Economic Research, Kyoto University, Yoshida-honmachi, Sakyo-ku, Kyoto 606-8501, Japan","Received 17 May 2019, Revised 23 September 2019, Accepted 4 November 2019, Available online 30 November 2019, Version of Record 21 December 2019.",https://doi.org/10.1016/j.jedc.2019.103794,Cited by (0),We explore how the timings of compensation payments and contract terminations are jointly determined in a continuous-time principal–agent model under the discretionary termination policy of investors (the principal) when the manager (agent) has loss–averse preferences. Our theoretical findings provide several new empirical implications for backloaded compensation and forced managerial turnover. Our model also shows that mandatory deferral regulation governing incentive pay induces investors to terminate the contract relation earlier and results in the more frequent replacement of managers.,"The compensation and turnover of CEOs and other executives are interrelated because investors can employ performance-based incentives, whereby contracts with incumbent executives are terminated with poor performance and agents are compensated for good performance. The prior literature (see, for example, Huson et al., 2001, and Kaplan and Minton, 2012) indicates that the likelihood of forced turnover increases as business performance deteriorates.====Forced turnover, however, may have only a limited influence on the incentives of executives. This is because the literature identifies only a few percentage points difference in the probability of forced turnover between the top and bottom quartiles of performance. For example, Jensen and Murphy (1990) point out that dismissals do not work as an important source of CEO incentives.====Interestingly, controlling for other factors that affect turnover, Bizjak et al. (2008) report that CEO turnover is more likely in firms with CEOs whose pay is less than the median pay level of their peer group. However, the probability of CEO turnover decreases if CEOs with pay below the peer group median move above the peer group median using large pay adjustments. This suggests that CEO concern about a reference point for income has a significant effect on the dynamic interaction between compensation and contract termination.====Motivated by the above, this paper considers how compensation and contract termination are jointly determined under the discretionary termination policy of investors (as the principals) when the manager (agent) has loss-averse preferences. To this end, we develop a continuous-time dynamic principal–agent model in which the agent can divert a share of the cash flows from the project for personal consumption through taking hidden actions.==== Consequently, our model generalizes the continuous-time agency model à la DeMarzo and Sannikov (2006) by incorporating the investors’ discretionary choice of contract termination and the agent’s loss-averse preferences.====There is a substantial body of work that applies the dynamic contract model to the study of corporate governance in the presence of agency conflicts. What seems to be lacking in the literature, however, is an argument concerning the abilities of investors in the contract termination. Although empirical estimates suggest that firing CEOs involves a very large turnover cost,==== CEOs are likely to be penalized by forced turnover, even when they just miss the latest consensus analyst forecast (see Mergenthaler et al., 2012).==== Furthermore, if an incumbent CEO is likely to lose the flexibility to adapt his skills to reorganize and/or restructure the firm, it is not surprising that investors dismiss him following poor performance. Despite these features of real-world contract termination, the existing continuous-time agency literature assumes that investors do not have technology to replace the agent; that is, investors do not terminate the contract until the agent’s discounted present value of his payoff, referred to as the continuation payoff, hits the reservation payoff (see, for example, DeMarzo and Sannikov, 2006, and He, 2009). However, to capture real-world practice, we need to suppose that investors can discretionarily force the early termination of the contract and fire the agent at any time, conditional on the agent’s continuation payoff.====Nevertheless, investors need to commit to ensure a promised utility for the agent at his replacement, otherwise the contract offered by investors is infeasible at the initial contracting stage if the investors can fire the incumbent agent when his continuation payoff at replacement is larger than the agent’s outside option. Then, the stochastic replacement policy of the agent—that is, either firing the agent and replacing him with a new agent with some probability, or continuing his tenure and offering a new continuation payoff with the remaining probability—plays an important role because it not only incentivizes the agent, but also ensures the feasibility of the optimal contract.====Another important problem is that an agent’s reference-dependent preferences influence his incentives. The agent then derives a gain or a loss in utility by comparing his continuation value with some reference level. However, no existing study has examined the dynamic interaction between compensation and contract termination under the agency problem with reference-dependent preferences, despite the finding of Bizjak et al. (2008) discussed earlier suggesting that the agent’s concern about a reference point for income has a significant effect on the dynamic interaction between compensation and contract termination.====We also examine the effect of a regulation policy mandating the longer deferral of incentive payments. In particular, we discuss how this regulation affects the timing of contract termination and the likelihood of investors firing the agent when the investors discretionarily choose a contract termination policy and fire the agent at any time. Early compensation payment means that the agent has less to lose and this exacerbates any moral hazard (cash-diversion) actions. Many practitioners and researchers have recently argued that current compensation practices in the EU and US are flawed, and have proposed instead the mandatory deferral of incentive pay. The regulatory issues involved in changing the structure of compensation are particularly discussed in the financial industry (see, for example, Hoffmann et al., 2014), whereby some proposals have already become part of the regulatory framework in both the EU (Directive 2013/36/EU) and the US (Dodd–Frank Act of 2010). Even outside the financial industry, short-term incentive pay is subject to ongoing debate (see Bebchuk and Fried, 2010). However, these regulatory frameworks may have an adverse incentive effect on the firm’s contract termination policy of the agent given the interaction between the timings of compensation payments and contract terminations.====Our main theoretical results are as follows. First, consider three benchmark models: the first is the DeMarzo and Sannikov (2006) model, in which investors do not have technology to replace the agent and the agent has no loss-averse preferences; the second is the discretionary termination policy model, in which investors discretionarily choose their contract termination policy ex post and fire the agent at any time when the agent has no loss-averse preferences; and the third is the loss-aversion model, in which investors do not have technology to replace the agent and the agent has loss-averse preferences. Compared with these three benchmark models, our model shows that the discretionary termination policy of investors regarding the contract induces investors to terminate the contract relation at the higher continuation payoff to the agent. We also find that the agent’s loss aversion induces investors to terminate the contract relation at the higher continuation payoff to the agent, in the sense that investors terminate the contract relation, even though reducing one unit of continuation payoff to the agent costs the firm less than the expected marginal termination cost in the absence of loss aversion. In contrast, we also find that if the reference point is sufficiently large, the agent’s loss aversion induces investors to reward the agent at the lower continuation payoff to the agent in the sense that investors commence paying the agent cash, even when the required expected returns of the investors and the agent do not exceed the available expected cash flows.====Second, our comparative static analysis illustrates that an increase in the replacement cost of the agent induces investors to reward the agent at the higher continuation payoff to the agent and to be more likely to fire the agent without offering any new contract with the contract termination, but does not necessarily induce investors to terminate the contract relation at the lower continuation payoff to the agent. Because an increase in the replacement cost of the agent increases the incentives for investors to avoid inefficient contract termination and agent replacement, we could expect that an increase in the replacement cost induces investors to terminate the contract relation at the lower continuation payoff to the agent and to be less likely to fire the incumbent agent.====To understand why our result lies contrary to this prediction, we should note that an increase in the replacement cost raises the marginal cost to investors by reducing one unit of continuation payoff to the agent (that is, the marginal cost to investors of compensating the agent). The reason is that a decrease in the agent’s continuation payoff decreases the firm’s “distance to default”, even though the investors are more willing to retain the agent’s higher conditional payoff to avoid inefficient termination. This implies that an increase in the replacement cost also increases the new continuation payoff received by the agent when he is rehired under a new contract following the contract termination. However, the resulting high continuation payoff received by the rehiring agent in turn raises the firing probability of the incumbent agent, given the promised continuation payoff at the contract termination. Furthermore, investors may terminate the contract at the higher continuation payoff to the agent so as to match the increase in the marginal cost to investors of compensating the agent with a decrease or increase in the expected marginal cost of termination to investors.====Third, our comparative static analysis also demonstrates that if the reference point is sufficiently small (or large), increasing the reference point, or the weight attached to the gain–loss utility (or increasing only the reference point), induces investors to reward the agent at the lower continuation payoff to the agent and to become less likely to fire the agent at the contract termination. However, in the other cases, increasing these same parameters does not necessarily induce investors to reward the agent at the lower continuation payoff to the agent or to become less likely to fire the agent at the contract termination. In addition, increasing the parameters does not necessarily induce investors to terminate the contract relation at the lower continuation payoff to the agent, regardless of the size of the reference point. These results lie contrary to the prediction already discussed.====The reason is that loss aversion requires investors to compensate for the flow stream of the agent’s disutility when he stays in the loss space in the contract period, and for heavy losses when he is fired without being offered any new contract. However, loss aversion has a different effect on the investors’ value function through the flow stream of the agent’s disutility according to whether the agent stays in the loss or gain space. If the reference point is not sufficiently small or large, the agent can stay in both the loss and gain spaces in the contracting period. Hence, regarding the payment timing and the firing probability of the agent at the contract termination, the effect of the loss-aversion parameters in this case is ambiguous. Concerning the termination timing, increases in the loss-aversion parameters may raise the marginal cost to investors of compensating the agent. Hence, the intuition of the result for termination timing with loss-aversion parameters becomes similar to that in the case of the replacement cost.====Our theoretical findings provide several new empirical implications for backloaded compensation and forced managerial turnover, and theoretical explanations for any empirical findings of debt composition under debt heterogeneity.==== Our model can also be used to examine the regulatory implications of the mandatory deferral of incentive pay, the suggestion being to limit the longer deferral of incentive compensation under investors’ discretionary termination policy and a manager’s loss aversion. Our findings show that the mandatory deferral of incentive pay induces investors to terminate the contract relation earlier and results in the more frequent replacement of managers.====Intuitively, mandatory deferral has the effect of increasing the marginal cost to investors of compensating the manager. This increases the continuation payoff received by the manager under a new contract if he is not fired at the contract termination, thereby in turn raising the firing probability of the manager at the contract termination. Furthermore, investors need to terminate the contract at the higher continuation payoff to the agent in response to the increase in the marginal cost to investors of compensating the manager. This is because the regulation does not affect the marginal cost of the contract termination to investors. In equilibrium, the mandatory deferral of incentive pay aggravates any inefficiency regarding the timing of contract termination and the replacement of the manager.====The remainder of the paper is organized as follows. Section 2 reviews the literature. Section 3 describes the baseline model given an exogenous reference point. Section 4 derives the optimal contract and contract termination policy of the baseline model, and compares these with the three benchmark models. The comparative static results are also provided. Section 5 discusses the empirical and regulatory implications of our results. Section 6 concludes. The proofs for all of the lemmas and propositions in the text are in Appendices A–D. Lastly, Appendix E extends the baseline model with the endogenous determination of the reference point, while Appendix F investigates the capital structure implementation of the optimal contract.",Dynamic contract and discretionary termination policy under loss aversion,https://www.sciencedirect.com/science/article/pii/S0165188919301915,30 November 2019,2019,Research Article,372.0
"Eo Yunjong,Kang Kyu Ho","School of Economics, University of Sydney, NSW 2006, Australia,Department of Economics, Korea University, Seoul 02841, South Korea","Received 10 April 2019, Revised 13 November 2019, Accepted 19 November 2019, Available online 27 November 2019, Version of Record 10 December 2019.",https://doi.org/10.1016/j.jedc.2019.103812,Cited by (5),We investigate how conventional and ,"Yield curve forecasts are important for pricing financial assets, optimizing bond portfolios, managing financial risk, and analyzing business cycles. This paper aims to understand how conventional and unconventional monetary policies affect the dynamics of the yield curve by assessing the performance of different yield curve forecasting models in different monetary policy frameworks. The different channels and effects of conventional and unconventional monetary policies on the term structure of interest rates and an increase in the likelihood that further unconventional monetary policy will be necessary in the near future motivate this investigation.====We find that the forecasting performance of various yield curve models (including mixtures thereof) depends on the monetary policy framework. During a period of conventional monetary policy, imposing the no-arbitrage restrictions significantly improves forecasts of the yield curve, though naive forecasts from a random walk model are superior during the period of unconventional monetary policy. We discuss our findings and the effects of the monetary policy frameworks on them in detail below.====We consider three widely used models in this study: (i) the dynamic Nelson–Siegel (DNS) model, (ii) the Arbitrage–free Nelson–Siegel (AFNS) model, and (iii) the random walk (RW) model. Diebold and Li (2006) develop a dynamic version of the Nelson and Siegel (1987) model. The DNS model is a parsimonious factor model in which factors are interpreted as level, slope, and curvature. The affine arbitrage-free model is a bond pricing approach that imposes cross-equation restrictions of no arbitrage opportunities across different maturities and over time.==== This approach provides many economically interpretable outcomes, such as the term premium and term structure of real interest rates. We, in particular, consider the arbitrage-free version of the Nelson–Siegel model, because it can be useful to understand the role of the additional cross-equation restrictions of no-arbitrage in forecasting yield curves when compared to the DNS. The RW is often used as a benchmark in terms of forecasting ability in the literature.====However, it is well known that none of the three individual models uniformly outperform the other models for all maturities and forecasting horizons.==== The mixed results for out-of-sample (OOS) predictions strongly suggest that these yield curve models may be misspecified, which could be driving our results for the individual models. To counter potential misspecifications, we take three different mixture approaches to integrating model uncertainty and parameter uncertainty: (i) equal weights; (ii) constant weights; and (iii) Markov-switching weights. The mixtures consist of either two or three of these models in addition to individual models.==== These forecast combination schemes are in line with the new approaches developed in the econometrics literature, such as linear combinations of predictive densities with constant model weights proposed in Geweke and Amisano (2012) and Markov-switching mixtures of alternative prediction models developed in Waggoner and Zha (2012).==== In the case of constant weights, the weights are estimated as constant parameters. A single model can be regarded as a special case of the mixtures.====We forecast monthly U.S. bond yields with eight different maturities at one-, four-, and 12-month forecasting horizons and evaluate the OOS forecasts for one subsample of conventional monetary policy and one subsample of unconventional monetary policy. We choose the best mixture for each maturity at each forecasting horizon based on the mean squared forecast errors (MSFE). That is, for each subsample, there are 24 cases given by eight different maturities times three different forecasting horizons. For the first OOS forecast period of conventional monetary policy, the AFNS or its mixtures with other models outperform other forecasting models in 21 out of 24 cases. On the other hand, during the second OOS forecast period, which corresponds to the period of unconventional monetary policy, the RW significantly outperforms all possible mixtures and other individual models in 21 out of 24 cases. These results are also confirmed by model confidence sets (MCS) based on Hansen et al.’s (2011) procedure.====A natural question arising from our findings is why the no-arbitrage restriction is relevant during the conventional monetary policy period, but not during the unconventional monetary policy period. We argue that these results are attributable to unconventional monetary policy. The AFNS has a small number of factors and their loadings are tightly constrained by the no-arbitrage restriction so that the model is expected to fit to the bond yields and forecast effectively when the yields are highly correlated. The strong positive correlations between the bond yields are associated with conventional monetary policy because changes in the federal fund rate affect the entire path of expected future short-term interest rates and long-term interest rates through this channel. The usefulness of adding the no-arbitrage condition to yield curve models in improving their forecasts is consistent with the findings in the literature, which are based on OOS forecasts for the sample period of conventional monetary policy (see Ang and Piazzesi (2003), Moench (2008), Carriero and Giacomini (2011), and Christensen et al. (2011) among others). In the period of unconventional monetary policy, however, the Fed directly purchased long-term bonds, while short-term yields were constrained by the zero lower bound. This resulted in low correlations between the short- and long-term bond yields and little variation in the short-term bond yields. Thus, the RW forecasts well in this period. We document empirical evidence for changes to the correlation structure and variation in short-term bond yields across two policy periods.====We also plot cumulative squared forecasting errors over time to examine the relative importance of the competing forecasting models at each point in time. We compare the AFNS with the RW to verify the role of the no-arbitrage restrictions against the RW and find that the AFNS forecasts better during the conventional monetary policy period, whereas the RW started forecasting significantly better around late 2008 when unconventional monetary policy began. The superiority of the RW is more pronounced during the maturity extension program in 2011–2012. We conduct the same procedure to compare the Markov-switching mixture of all three models with the RW and find very similar patterns.====Our findings are closely related to the recent literature on unconventional monetary policy and the term structure of interest rates. Swanson and Williams (2014) estimate the sensitivity of yields to macroeconomic news and find that yields with six months or less to maturity were severely constrained since early 2009, whereas medium- and longer-term yields were unconstrained by the zero bound. Similarly, Inoue and Rossi (2018) calculate the correlations between Romer and Romer’s (2004) monetary policy shocks and yields across different maturities. They document that the correlation was highest for short-term maturities during the conventional monetary policy period, while the correlation was highest for the longest-term maturities during the unconventional monetary policy period.==== In addition, Guidolin and Pedio (2019) show that a regime switching DNS model augmented with variables that capture the state of monetary policy outperforms various alternative yield curve models at one-month forecast horizon. While our work is closely related to these findings, our paper differs from those in the literature: we find that no-arbitrage restrictions are less useful for forecasting the yield curve during the unconventional policy period and show why unconventional monetary policy framework leads to this result.====The remainder of the paper is organized as follows. Section 2 describes yield curve models including their mixtures and estimation methods. Section 3 provides the empirical results for the conventional and unconventional monetary policy periods. Section 4 discusses the implications of the monetary policy framework for forecasting the yield curve. Finally, Section 5 concludes the paper.",The effects of conventional and unconventional monetary policy on forecasting the yield curve,https://www.sciencedirect.com/science/article/pii/S016518891930209X,27 November 2019,2019,Research Article,373.0
"Duarte Victor,Duarte Diogo,Fonseca Julia,Montecinos Alexis","University of Illinois at Urbana-Champaign, USA,Florida International University, USA,University of Illinois at Urbana-Champaign, USA,Sawyer Business School, Suffolk University, USA,Universidad Adolfo Ibáñez, Business School, Chile","Received 19 February 2019, Revised 4 November 2019, Accepted 5 November 2019, Available online 22 November 2019, Version of Record 4 December 2019.",https://doi.org/10.1016/j.jedc.2019.103796,Cited by (0),We investigate the performance of machine-learning software and hardware for ,"In the last 10 years, machine learning has revolutionized many fields of research, from image recognition (Krizhevsky et al. 2012), to machine translation (Wu et al. 2016), to intertemporal optimization (Mnih et al. 2015). These breakthroughs were possible due to major advances in computer hardware, software, and methods.====Importantly, advances on each of these fronts tend to spur advances in the others. For instance, the development of new computer chips reduces the run time of experiments, which allows researchers to explore more ideas, an essential part of developing new methods. New methods and powerful computers make machine learning attractive to more researchers, increasing the pool of software developers who contribute to open-source projects. Better software leads to lower barriers to entry into machine learning, while a larger pool of potential customers provides economic incentives for hardware development. This positive feedback loop has helped create a rich ecosystem of software and hardware that has been largely untapped by researchers in quantitative economics.====In this paper, we argue that research in finance and economics could benefit from this technological spillover. Specifically, we show that the adoption of machine-learning software and hardware can accelerate standard operations in computational economics by up to four orders of magnitude, compared to popular programming languages—such as MATLAB, Python/Numpy, Julia, C++, and R—running on ordinary computers. We use two benchmark models, a strategic sovereign default model (Arellano 2008) and the Least Squares Monte Carlo (LSMC) method of pricing American options (Longstaff and Schwartz 2001). The former is solved using a standard value iteration algorithm, and therefore representative of many dynamic programming problems studied in economics. The key elements of Longstaff and Schwartz 2001, simulation and regression, are also common ingredients in many tasks in economics and are tasks machine-learning frameworks are designed to excel in.====Our paper is closest to Aruoba and Fernández-Villaverde (2015), who compare different programming languages commonly used in economics by benchmarking the solution of a real business cycle model using a value iteration algorithm. The authors show that the choice of tool can make a significant difference in both execution and development time. Specifically, the authors show that compiled languages, such as C++ and Fortran, can run hundreds of times faster than scripting languages, like Python or MATLAB for a typical economic application. At the same time, the authors point out that compiled languages are more complex and therefore harder to master, demanding more development time. We show that machine-learning software can potentially eliminate this trade-off.====This paper is also related to computational economics literature that explores modern hardware to accelerate computation in quantitative economics. Aldrich et al. (2011) show that the use of Graphics Processing Units (GPUs) speed up value iteration by up to 200 times, while Fernández-Villaverde and Valencia (2018) present a guide on parallelization for both GPUs and ordinary Central Processing Units (CPUs). This paper adds to this existing work by benchmarking machine-learning software and also a new type of hardware designed specifically for machine-learning applications, called Tensor Processing Units (TPUs).====The second objective of the paper is to lay out a blueprint for future work on quantitative economics. To the best of our knowledge, this is the first paper to provide an assessment of machine-learning software and hardware for computational economics.==== By providing replication files in our GitHub repository, we hope to encourage the adoption of these tools by researchers in quantitative economics.====The rest of the paper is organized as follows. Section 2 describes the software and hardware, focusing on recent advances and on how these new tools can be used to accelerate compute-intensive tasks in quantitative economics. Section 3 discusses the first benchmarking exercise, the sovereign default model of Arellano (2008). Section 4 details the second benchmarking exercise using the LSMC method of pricing American options. Section 5 concludes.",Benchmarking machine-learning software and hardware for quantitative economics,https://www.sciencedirect.com/science/article/pii/S0165188919301939,22 November 2019,2019,Research Article,374.0
"Elitzur Moshe,Kaplan Scott,Zilberman David","Department of Astronomy, Univ of California, Berkeley, CA 94720, USA,Dept of Physics & Astronomy, Univ of Kentucky, Lexington, KY 40506, USA,Dept of Agricultural & Resource Economics, Univ of California, Berkeley, CA 94720, USA","Received 20 May 2019, Revised 12 November 2019, Accepted 13 November 2019, Available online 20 November 2019, Version of Record 3 December 2019.",https://doi.org/10.1016/j.jedc.2019.103807,Cited by (3),"We develop a formalism to extract the exponential component from a growth process and describe the remainder with the optimal number of parameters. The method is demonstrated analyzing the time variation of Gross Domestic Product (GDP) and population in the US and UK, two nations with continuous data coverage going back more than 200 years. For each of the four datasets we find a successful description, with the deviation of long-term growth from a pure exponential requiring no more than a single free parameter; there is no significant gain from adding more parameters. We find persistent long-term growth patterns, consistent with Jones (1995) and showing directly from the data that population and GDP growth in different countries may follow different trajectories, illuminating their intrinsic differences.","Growth impacts daily activities on many levels. Its prime indicator, the growth rate, plays a key role in decisions that determine quantities as diverse as the stock price of a public company and the prime rate of a national economy. The great interest in growth rates, by both policy makers and the general public, is reflected in the efforts invested in their measurement and dissemination. Every April and October the International Monetary Fund (IMF) releases detailed data and forecasts of economic and population growth by country.==== The magazine ==== updates twice daily economic and financial indicators, foremost among them GDP growth rate, for a large number of countries.====For any quantity ====, such as a company’s revenue, national GDP, etc., the growth rate ==== is derived directly from the data without any modeling and is a predictor of the expected behavior of ==== in the near future. However, this predictive power is diminished over periods sufficiently long that the growth rate itself may vary appreciably. For such predictions, detailed modeling of the actual time variation ====/==== is necessary. In such modeling, key dynamic processes and their mutual interactions and interdependencies are identified and described mathematically. The model then accounts for all the relevant processes and interactions involving ====, and ====/==== is determined from the balance between gains and losses. Such structural models have been developed by both economists (Becker, Murphy, Tamura, 1990, Jones, 2016, Solow, 1956) and demographers (Levin, 1976) to quantify processes that drive the growth of GDP and population. The literature on economic growth aims to decompose the contribution of factors like capital and labor, and identify the impact of processes of learning and buildup of human capital (Acemoglu, 2008, Mankiw, Romer, Weil, 1992). Academic demographers are interested in decomposing population growth dynamics into multiple processes like fertility and life expectancy (O’Neill et al., 2001). Estimation of dynamic systems that aims to identify causal relationships and incorporate multiple relevant factors applies complex autoregressive models (Akaike, 1969) or uses unobserved components models with permanent and stochastic deviations (Stock and Watson, 2007).====Detailed structural modeling is essential for insight into the inner workings of the studied systems. But for more limited purposes it is possible to take a simpler, purely data-driven approach: construct an analytic description of the long-term variation of ==== without attempting to explain the specifics behind its growth. In this phenomenological approach the definition of ==== is inverted to become the growth equation====where ====, too, is in general a function of ====. With some suitable parametrization for ==== this equation can be solved in parametric form and the free parameters estimated from the best fit to the data, resulting in an analytic description for the time-dependent ====(====). Such an approach might be feasible when underneath the volatile fluctuations common to growth rates, their long-term patterns follow reasonably smooth trajectories. While not exploring the reasons behind the growth, such analysis can still provide valuable information since it yields an analytic description with a few key parameters, making it possible to build a methodical, general classification scheme of growth patterns. Given a dataset, the challenge is to make an unbiased selection of the proper mathematical function to describe it analytically and determine the optimal number of parameters. This is what we set out to do here.====Our aim is to describe long term growth—although ==== may fluctuate significantly, and even become negative on occasion (during economic recessions, for example), our interest is only in depicting its underlying smooth variation. The key concepts behind the approach we take can be illustrated with a simple example. Consider a newly discovered desolate island into which apple seeds are introduced. Some seeds will sprout, apple trees will produce new seeds and the tree population will grow. Annual weather variations and an occasional infestation will cause fluctuations in the growth rate, but fundamentally it is determined by the island’s climate, ground fertility, etc., and thus maintains a constant long-term average. Once the tree population has grown to the point that tree crowding becomes a significant factor, the growth rate begins to decline from its initial value. This simple example is illustrative of a general fundamental point: the growth of any quantity ==== occurs within some environment, broadly defined as the collection of all the processes and system components that affect the growth of ==== other than ==== itself. As long as the growing ==== is sufficiently small that its impact on the environment is negligible, the growth rate ==== is determined by intrinsic properties of the environment independent of the magnitude of ==== itself. This rate is maintained until ==== becomes sufficiently large that it significantly impacts the environment, at which point it also affects its own growth rate. In general, this causes the growth rate to decline and we refer to this effect as ====—the growing quantity has become so large as to hinder its own growth. One interpretation of hindering is that there is an initial, “natural” unconstrained rate of growth, but as ==== increases, its rate of growth is constrained and tends to diminish, consistent with the notion of decreasing marginal productivity. This is the phenomenon we wish to describe.====The critical magnitude for the onset of hindering and the specific reasons behind the effect vary from system to system. In the above example, the key parameter is the minimal area for viability of a single apple tree; hindering begins to affect the tree population growth when the average area per tree becomes a significant fraction of this parameter. But even without this knowledge we could discern the onset of hindering and determine the impact of the effect from time records of the island’s apple tree population when the period covered by the data is sufficiently long. Similarly, given records of any quantity with sufficient time coverage one can detect, and describe, the effect of hindering from analysis of the data even when the exact specification of the environment and its interplay with the growing quantity are not understood.====We present here a general framework for fitting the long-term variation of ==== with the minimal number of parameters, without preconditions about the nature of the growth, bounded or unbounded, and without prejudging the specific functional form of the fit to the data. To this end we formulate in Section 2 a general framework to describe hindering, and derive the general solution of the growth equation (Eq. 1) in parametric form. Our solution provides a generic description of growing quantities just as the Fourier series provides a generic description of periodic phenomena. It is based on a straightforward Taylor expansion so that finding the number of free parameters required to describe the time variation of ==== becomes simply a matter of determining the number of terms that should be kept in a Taylor series. We emphasize ==== (Section 2.1), a single-parameter form for the time variation of the growth rate that allows unbounded growth, and derive also the time dependence of higher order terms (Section 2.2). We show that any unbounded growth can be described by a finite sum (Section 2.3) while infinite sums produce bounded growth, of which the logistic function is an example (Section 2.4). Thanks to the simplicity of this new framework we are able to successfully model over 200 years of GDP and population data for both the United States of America (Section 3.1) and the United Kingdom (Section 3.2). We show that no more than a single free parameter is needed to describe the deviations from pure exponential of the long-term variation of the growth of both population and GDP in these two nations; additional parameters make no significant difference. We summarize and discuss our results, including the benefits and limitations of our method, in Section 4.",Hindered growth,https://www.sciencedirect.com/science/article/pii/S0165188919302040,20 November 2019,2019,Research Article,375.0
Barde Sylvain,"School of Economics, Kennedy Building, University of Kent, Park Wood Road, Canterbury CT2 7FS, UK","Received 9 July 2019, Revised 1 November 2019, Accepted 4 November 2019, Available online 6 November 2019, Version of Record 25 November 2019.",https://doi.org/10.1016/j.jedc.2019.103795,Cited by (6),The paper aims to address the issue of comparing agent-based models (ABMs) with more traditional VAR and ,"The last decade has seen a fundamental shift in the ‘technological readiness level’ of agent-based computational economics (ACE), driven mainly by the joint maturation of agent-based models (ABMs) and the validation methodologies required to bring them to the data. As identified by Grazzini and Richiardi (2015, p.150), ACE is gradually transitioning from the purely qualitative replication of stylised facts to a more quantitative replication based on ‘sound econometric techniques’, with a view to inform policy-making.==== This in turn has increased the need to validate the simulations models on empirical data, in order to ensure that they are in fact a suitable description of the phenomena they aim to model. ABM practitioners are acutely aware of this requirement as well as the challenges involved in doing so. However, as pointed out by Marks (2013, p. 41), at the time “validation of any but very simple simulation models has been slow in appearing in the literature”. A practical illustration of this problem comes from the recent and active ABM research into macroprudential regulation: Ashraf et al. (2017), Popoyan et al. (2017), Raberto et al. (2018) and Baptista et al. (2016) all investigate the impact of macroprudential banking regulation, and policymakers may well want to know whether these models agree, which offers the best predictions for a given scenario, or how well they fit the data relative to standard models estimated with traditional techniques.====Validation of an ABM requires overcoming two problems, both of which are complicated by to the fact that ABMs typically do not possess analytical descriptions, and properties of the model instead need to be inferred from the simulated data they produce. The first is the estimation of the ABM’s parameters from available empirical data, and the second is the comparison of or selection amongst various ABM specifications and traditional models. This pressure to validate models has lead to the development of methodologies that can address both these issues. Fagiolo et al. (2019) provide an excellent review of the problems posed by ABM validation as well as the new methods available to address them. Early approaches to estimation typically rely on the simulated methods of moments (SMM) proposed by Gilli and Winker (2003), a more recent version of which can be found in Grazzini and Richiardi (2015). Other recent developments of interest are the simulated maximum likelihood (SML) of Kukacka and Barunik (2017) as well as Grazzini et al. (2017) and Lux (2018), who investigate Bayesian and state-space estimation methods consistent with those used in more traditional macroeconomic DSGE models. A promising contribution which takes a very different approach to the estimation problem is that of Lamperti et al. (2018), who use machine leaning to build a surrogate model of the ABM in order to efficiently explore its parameter space.====Model comparison methods have seen similar developments. Marks (2013) provides a comparison of three distance measures between two vectors of data. Guerini and Moneta (2017) maps the structure of an ABM to a structural VAR in order to help identify the channels through which shocks are transmitted at the aggregate level. Two recent additions are the Generalized Subtracted L-divergence (GSL-div) of Lamperti (2018b) and the Markov Information Criterion (MIC) of Barde (2017), both of which have been used to perform empirical model comparison exercises. Lamperti (2018a) compares 5 distinct versions of the Brock and Hommes (1998) model of asset pricing with heterogeneous beliefs on the EuroSTOXX 50 and CSI 300, while Barde (2016) compares the performance of the Gilli and Winker (2003), Alfarano et al. (2005) and Franke and Westerhoff (2011) models of recruitment on a range of financial indices against standard ARCH/GARCH processes.====In most cases these new estimation and comparison methods are applied to relatively small scale or univariate models in an empirical setting where data is plentiful, typically a financial ABM applied to market index data. This is understandable given the new and often experimental nature of the methodologies involved. However, validation of the large-scale, policy-relevant ABMs discussed above will require demonstrating that these methodologies can be carried over to typical macroeconomic settings characterised by larger sets of observable variables and a smaller number of observations.==== The present paper aims to address this issue by extending the univariate ABM comparison exercise of Barde (2016) to a macroeconomic setting, in order to verify that the MIC can also perform model comparison in this more empirically challenging environment. This first requires developing and validating an extension of the MIC algorithm of Barde (2017) to a multivariate state space. Once this is done the paper provides a proof of concept for multivariate model comparison by carrying out an ABM - DSGE comparison exercise in the spirit of Fagiolo, Roventini, 2012, Fagiolo, Roventini, 2017, using the Caiani et al. (2016) and Smets and Wouters (2007) models.====The remainder of the paper is organised as follows. Section 2 first presents the desirable theoretical properties of the MIC and provides a simple illustration of its effectiveness in univariate settings. Sections 3 and 4 then present the computational strategy used to extend the MIC to multivariate systems and the validation of the strategy, while Section 5 presents the comparison exercise itself.",Macroeconomic simulation comparison with a multivariate extension of the Markov information criterion,https://www.sciencedirect.com/science/article/pii/S0165188919301927,6 November 2019,2019,Research Article,376.0
"Benati Luca,Chan Joshua,Eisenstat Eric,Koop Gary","Department of Economics, University of Bern, Schanzeneckstrasse 1, Bern CH-3001, Switzerland,Department of Economics, Purdue University, 100 Grant St, West Lafayette, IN 47907, USA,School of Economics, University of Queensland, Brisbane St Lucia, QLD 4072, Australia,Department of Economics, University of Strathclyde, 199 Cathedral Street, Glasgow G4 0QU, United Kingdom","Received 14 April 2019, Revised 18 September 2019, Accepted 21 October 2019, Available online 5 November 2019, Version of Record 21 January 2020.",https://doi.org/10.1016/j.jedc.2019.103780,Cited by (3),We study identifying restrictions that allow news and noise shocks to be recovered empirically within a ,"Noise shocks—defined as announcements about future fundamentals which, in fact, never materialize—offer the possibility of generating macroeconomic fluctuations without any variation in the economy’s fundamentals. They are therefore radically different from news shocks, i.e. announcements about future fundamentals which do indeed materialize at some future date.====In this paper we investigate theory-driven restrictions which motivate an identification scheme we use in order to recover the impact of news and noise shocks from the data. The restrictions we propose extend those employed by Barsky and Sims (2011) and Forni et al. (2017) in ways which are both justified from a theoretical standpoint, and effective at recovering the two shocks. In particular, the proposed identification scheme exactly recovers news and noise shocks in population.====Although our identification scheme cannot be imposed within a structural Vector Autoregressive (SVAR) framework, it can indeed be imposed using structural Vector Autoregressive Moving Average (SVARMA) models. We implement it empirically within a Bayesian framework, and we demonstrate, in a Monte Carlo study, its excellent performance. We then use our identification scheme in an empirical application investigating the role of news and noise shocks to Total Factor Productivity (TFP). We compute impulse response functions (IRFs) to news and noise shocks, and forecast error variance decompositions (FEVs). We find that noise shocks play a minor role in macroeconomic fluctuations.====Our identification strategy is based on the general principle that agents’ inability to distinguish news and noise shocks on impact—which is the essence of the entire ‘news ==== noise’ problem—implies that====As time goes by, all else held constant, agents gradually learn whether the announcement was news (i.e. that the fundamentals will indeed change) or simply noise, leaving fundamentals unchanged.====To provide some intuition, consider an announcement that purports to promise a future positive permanent shock to TFP. Is this announcement news or noise? Initially, it is impossible to tell. As we illustrate in Section 3 using the real business cycle (RBC) model of Barsky and Sims (2011) augmented with noise shocks about TFP, such an announcement indeed produces, on impact, identical impulse vectors regardless of whether it is news or noise.====The ==== of our identification strategy is that, although news and noise shocks are observationally equivalent on impact, they are not at all subsequent horizons. The news shock will ultimately cause a permanent increase in TFP, whereas the noise shock will not. This implies that there is no way that the two shocks might be confused once the entire set of their properties is taken into account. Quite simply, within the present example, a permanent shock cannot be observationally equivalent to a transitory one. Working with an RBC model with noise shocks about TFP, we then provide a straightforward illustration of how the IRFs to news and noise shocks, and the fractions of FEV of the variables they explain, can be exactly recovered in population by imposing the model-implied restrictions that (====) news and noise shocks produce identical impulse vectors on impact, and (====) news and non-news shocks are the only disturbances having a permanent impact on TFP.====We employ an econometric methodology based on Bayesian estimation of structural VARMA models, developed in Chan, Eisenstat, Koop, 2016, Chan, Eisenstat, Koop.==== The need for using VARMAs instead of VARs originates from the fact that the ‘news ==== noise’ problem automatically produces a reduced-rank structure for the matrix of the shocks’ impact responses at ==== = 0.==== The details of this and related methodological issues are discussed in Chan et al. (2019). Building on this work, we implement our identification scheme within the SVARMA methodology. In our Monte Carlo study, we find our algorithm to work extremely well.====We consider an empirical application where the ‘news ==== noise’ problem pertains to TFP. We find that noise shocks play a minor role in macroeconomic fluctuations. Our results for the system featuring TFP contrast with those of both Blanchard et al. (2013) and Forni et al. (2017), who found a significant role for noise shocks pertaining to productivity.====The paper is organized as follows. The next section discusses several theoretical implications of the “news ==== noise” problem. In Section 3 we illustrate the main features of the RBC model of Barsky and Sims (2011) augmented with noise shocks about TFP. We show, working in population, that low-order SVARMAs can approximate very well the model’s theoretical IRFs and fractions of FEV explained by individual shocks. We also discuss our identifying restrictions. In Section 4 we then show, working in population, that our identifying restrictions allow to exactly recover the RBC model’s IRFs and FEVs. Section 5 briefly outlines our econometric methodology, and discusses the Monte Carlo evidence about its performance. Section 6 contains our empirical application. Section 7 concludes, and discusses possible directions for future research.",Identifying noise shocks,https://www.sciencedirect.com/science/article/pii/S0165188919301770,5 November 2019,2019,Research Article,377.0
Raei Sepideh,"Department of Economics and Finance, Jon M. Huntsman School of Business, Utah State University, 3565 Old Main Hill, Logan, Utah 84322, United States","Received 12 April 2019, Revised 28 October 2019, Accepted 30 October 2019, Available online 4 November 2019, Version of Record 25 November 2019.",https://doi.org/10.1016/j.jedc.2019.103793,Cited by (5),One of the key practical challenges facing by ,"Multiple features of the current U.S. federal tax system make the study of tax reforms an important object of research. Many proposals for reformulating the tax code suggest eliminating individual and corporate income taxes and implementing a new tax system based on consumption (Zodrow and Mieszkowski, 2008). Several authors show that replacing the current federal tax system with a system that levies taxes on all income with complete deductibility of savings would increase the economy’s long-run output and improve the welfare of people born in the new steady state. This result mainly comes from the fact that taxing consumption does not distort saving desicions at the margin. Ventura (1997) and Altig et al. (2001), among others, show that a newborn agent would prefer to be born into the steady state of the economy with a consumption tax system rather than the one with the progressive income tax and capital income tax system.====However, except for few papers like Altig et al. (2001), Peterman (2013), Fehr and Kindermann (2015),..., the short-run welfare consequences of such a reform have not been addressed and the transitional cohorts who are also affected with the change in the tax regime are usually neglected in the existing literature. Peterman (2013), for example, shows that changing the tax structure to the regime that maximizes the long-run welfare can induce huge welfare costs for generations who are alive at the reform year. The central problem is that although consumption-based tax reform leads to welfare gains in the long run, achieving these gains typically involves welfare losses for generations who are alive at the time of the tax reform. Therefore, these individuals would favor the status quo over the reformed tax regime. It is challenging to implement a consumption tax reform that is simultaneously welfare improving for both current generations and those born in the long run.====This paper addresses the aforementioned challenge. I build an equilibrium life-cycle model with heterogeneous households and endogenous labor supply. Using this model, I show that a move from the current U.S. federal income tax system to a flat consumption tax system can be both feasible and welfare improving for households alive at the time of the policy change. The key to this result is to allow for a voluntary rather than compulsory switch to the new tax regime for generations alive at the time of the policy change. With this method, all households who are alive during the first period of the transition can choose their preferred tax system: the benchmark tax system or the consumption tax system. More specifically, I endogenize households’ move to the new tax system, and quantify the aggregate as well as welfare implications of this kind of reform.====This paper is built on the tradition of analyzing transitional dynamics in overlapping generation economies, in the spirit of Auerbach and Kotlikoff (1987). The specific tax reform exercise that I study in this paper is replacing the current federal income tax system with a flat consumption tax system in a revenue neutral way.====I compute the perfect-foresight transition path, with the initial state parametrized to the U.S economy with the current U.S. federal tax system (hereafter the benchmark tax system). Government revenue is kept constant along the transition and in the new steady state. Thus, the result that I obtain do not depend on an issuance of government debt to finance the new regime. The model features within-cohort heterogeneity, with differences arising from agents’ permanent productivity types, which also evolve as they age. Therefore, I can study effects of the new tax system on different birth cohorts and different income groups.====First, I consider a ==== form of revenue-neutral transition from the steady state of the benchmark economy toward the steady state of the economy with a flat consumption tax. In this version, right after the policy change, all households who are alive at the time of the tax reform, along with those who are born in the new tax system, are taxed using the reformed tax code (a flat consumption tax). Doing a conventional reform in my calibrated model illustrates the tension; future generations born in the long run benefit from the reform while more than 75% of the generations who are alive at the time of the reform experience welfare losses.====Then I introduce a flexible form of revenue-neutral transition, which I will refer to as the ====. In the gradual tax reform, all households who are alive at the time of the policy change have the option to choose between the benchmark tax system and the consumption tax system, with one condition: having chosen the new tax system, they cannot go back to the old one. And for all households who are born after the reform, the new tax code (the flat consumption tax) applies.====In principle, welfare effects for all generations depend on their ages, productivity abilities, and asset holdings. Changing the tax base from income to consumption, changes the distribution of tax burden across generations. In the simple tax reform, during the first transition period, the younger, more productive agents are largely unaffected, or they experience welfare gains. However, the elderly agents face welfare losses independent of their productivity types. That is due to the fact that older generations, who possess a large share of capital stock and do not have a labor income, face a higher tax burden in the consumption tax system. This group are mainly consuming out of their wealth which they saved from their after-tax income. Therefore, consumption taxes are levied on their wealth, placing a higher burden on older agents. Likewise, the low-productivity agents lose given the non-linear features of the current tax system. Altogether, in the first period of transition, 75.6% of agents experience welfare losses and only 24.6% of the population enjoy welfare gains and hence, would favor the tax reform.====By presenting all agents alive at the time of policy change with the choice of their preferred tax system, the gradual tax reform improves the welfare effects compared to the simple tax reform. In the first period of transition, welfare losses are negligible and about 95% of the population experience welfare gains and favor the tax reform. However, these gains are not for free, and the most important difference lies in the speed of the transition to the new steady state. With the gradual tax reform, it takes 1.7 times longer for the economy to reach the new steady state, implying that the beneficial effects of the new tax system materialize more slowly in the gradual tax reform. Another consequence of introducing the tax reform gradually is that adopting the new tax system is a protracted process. Because of the revenue-neutral nature of the reform, the consumption tax rates are higher in the initial periods of the transition. The higher consumption tax rate translates into a higher tax burden on those who are born in the new tax system and affects their welfare.====Various methods have been proposed in the literature to tax consumption (e.g. Hall, Rabushka, 1985, Ventura, 1999, ...). Since consumption is defined as income net of savings and investments, one practical way of collecting consumption taxes would be to levy a flat-rate tax on the reported income with full deductibility of reported savings. Some specific forms of savings are tax deductible in the current tax code and are reported, like contributions to individual retirement accounts or contributions to 401(k). Therefore, one way to implement the tax reform with the proposed gradual method is to have everyone report their income and savings and let them choose their preferred tax system, assuming they have a choice.====The paper is organized as follows: Section 1.1 provides a review of the related literature. Section 2 presents the life-cycle model. Section 3 discusses its parametrization. Sections 4 and 5 contain the main results. Critical discussions of the results including additional exercises and sensitivity analysis are presented in Section 6, and Section 7 concludes.","Gradual tax reforms: If you like it, you can keep it",https://www.sciencedirect.com/science/article/pii/S0165188919301903,4 November 2019,2019,Research Article,378.0
"Linardi Fernando,Diks Cees,van der Leij Marco,Lazier Iuri","University of Amsterdam, The Netherlands,Central Bank of Brazil, Brazil,CeNDEF, University of Amsterdam, UvA Institute of Advanced Study and Tinbergen Institute, The Netherlands,CeNDEF, University of Amsterdam, Tinbergen Institute and Research Department, De Nederlandsche Bank, The Netherlands","Received 15 February 2019, Revised 9 July 2019, Accepted 24 October 2019, Available online 4 November 2019, Version of Record 2 March 2020.",https://doi.org/10.1016/j.jedc.2019.103792,Cited by (10),"Longitudinal network data are increasingly available, allowing researchers to model how networks evolve over time and to make inference on their ","There has been a growing interest in the study of networks in economics over the last years. Specifically in finance the accelerated loss of confidence in financial markets following the failure of Lehman Brothers in 2008 has shown the importance of understanding the network of linkages between financial institutions. The lack of knowledge of counterparty exposures and how the contagion would spread throughout the system during the crisis have led policy makers to adopt a large set of financial reforms to address these and other vulnerabilities, both at the international and domestic levels (Claessens and Kodres, 2014).====Since financial institutions are highly interconnected, network theory provides a useful framework for the analysis of the financial system. Interconnections may be directed due to banks’ claims on each other or may arise indirectly when banks hold similar portfolios or share a common pool of investors or depositors. The role of these interconnections in the propagation of shocks in financial networks has been analyzed by a growing literature. Allen and Gale (2000) and Freixas et al. (2000) have shown how contagion depends on the structure of the interbank network. More recently, Acemoglu et al. (2015) showed that for a small shock hitting the system a more connected financial network enhances financial stability. On the other hand, a less connected financial network is preferable in the case of a large shock since interconnections serve as a mechanism for shock propagation. Many other theoretical or simulation-based works have studied the relevance of the various possible channels of contagion in the propagation of shocks and the implications for financial stability.==== The work in this area usually corroborates the “robust-yet-fragile” tendency of financial systems described by Gai and Kapadia (2010), in which the probability of contagion may be low but when problems occur the effects can be widespread.====Currently central bank authorities have a huge amount of data on common or bilateral exposures of financial institutions or are able to derive them from payment system records. Following Furfine (1999), central bank researchers have used these data to estimate the level of systemic risk in a particular financial system.==== Using the increasing availability of longitudinal data, researchers can track the evolution of the financial network and the implied contagion risk over time. However, in many works the banks’ exposures are considered to be fixed, ignoring the fact that financial networks are inherently dynamic; they change with the environment, in particular when the financial system becomes distressed. For example, Afonso et al. (2011) examine the impact of the financial crisis of 2008 on the U.S. interbank market and show how banks have become more restrictive in their lending operations after the Lehman Brothers’ bankruptcy. Squartini et al. (2013) show the changes in the topology of the Dutch interbank network over the period 1998–2008.====While the simplifying assumption that the financial network is static can be helpful in some cases, in other cases a good understanding of the dynamics of the financial network is required to assess financial stability. For example, if a bank defaults, the bank is removed from the network. Debtors of the defaulting bank are then likely to adapt by substituting their lending relation to the defaulting bank with relationships to non-defaulting banks. Not taking these dynamics into account will lead to biases in the estimates of systemic risk. We therefore need to have a good model that can predict the dynamics of the financial network after an event, whether that event is a bank default, a liquidity shock, new liquidity regulation, a merger, or entry of a new bank. For that, we also need to have a good understanding of the drivers behind the formation of financial linkages.====In this paper we aim to contribute to a better understanding of the dynamics of financial networks by applying the dynamic latent space approach of Sewell and Chen (2015) to model monthly networks of directed interbank linkages. This model is a special type of state space model, in which the observation equation is a logit equation, determining the probability that a link from ==== to ==== is present. The observation equation includes proximity in observable bank characteristics as regressors, such as having similar total assets, credit, and non-performing loans, that capture similarities in scale, health and functions of the bank. Additionally, the observation equation includes a latent regressor that captures unobserved bank characteristics, whose origin might be unknown ex ante. The value of this latent regressor is governed by a state transition equation to track the banks’ states – hence, a dynamic latent space. Given priors for the model parameters and the actual network observations, Bayesian estimation provides a posterior estimate of the latent positions and the dynamics of the network.====The latent space model has been mostly used for modeling social networks but it has not been applied to the study of financial networks so far. Although financial exposures are naturally represented by a weighted network, the choice of using a directed network representation is motivated by our aim to evaluate the ability of the model to characterize the basic structure of the network, which is the presence or absence of a link.====We apply this methodology to analyze two different datasets: the unsecured and the secured interbank lending networks of Brazilian banks. We analyze these two networks separately since banks face distinct risks when lending in these markets. In unsecured lending loans are not collateralized so lenders are directly exposed to losses in the case of borrowers’ default. In secured lending, which in our case is represented by repurchase agreements (repo) collateralized with government bonds, the loss is limited by the collateral value. In the former, counterparty risk plays a key role (see, e.g., Afonso et al., 2011) while in the latter the quality of collateral is an issue in moments of distress (see, e.g., Krishnamurthy et al., 2014).====We estimate the model with the latent space in order to determine factors that drive the formation of linkages in the unsecured and secured interbank lending networks, and compare the processes underlying the two markets. Moreover, an important question is whether the use of a latent space model really matters, that is, whether the inclusion of latent regressor indeed changes the coefficients of the logistic regression, and whether it improves the model’s goodness-of-fit. Hence, we run a model in which the probability of a tie depends only on observed covariates and a second one in which the latent space is included. The former model is simply a logistic regression model in which directed links are the dependent variables. We assess the in-sample and the out-of-sample link predictive ability of the two models. In addition, we evaluate the models’ adequacy by comparing some selected graph statistics calculated for the observed data with their distributions obtained from a large number of networks simulated according to the fitted model (see, e.g., Hunter, Goodreau, Handcock, 2008, Durante, Dunson, Vogelstein, 2017). If the simulated network does not resemble the observed network for a particular statistic, this an indication of the model’s lack of fit. We use this procedure to assess the model’s fit considering a number of graph statistics that are often used in the literature to assess the structure and stability of financial networks, such as average degree, maximum degree, transitivity/clustering, distance to an ideal core-periphery network, and the largest eigenvalue of the adjacency matrix.====Our estimation results show that, both in the unsecured and the secured interbank lending market, there is substantial assortative matching with respect to bank size: banks tend to lend more to similar-sized banks. On the other hand, in both markets, banks tend to lend less to banks with a similar deposit ratio and fraction of securities and derivatives. The unsecured and secured market have one striking difference: whereas in the unsecured market banks tend to lend more to banks with dissimilar non-performing loans (NPL) ratios, in the secured market the opposite is the case. This is likely due to the fact that banks with many non-performing loans are more likely to default, and hence, the credit risk is higher for the banks that lend to them on the unsecured market, as loans are secured with collateral in the repo market.====On top of that, we also learn that many factors that drive the network formation are captured by the latent space. It turns out that the regression results are quite different in the model that does not include a latent space. In particular, the effect of asset size similarity seems much smaller when no latent space is included in the model. Also, the model’s fit in terms of being able to reproduce network statistics is worse without a latent space.====Regarding the relevance of the latent space, a closer look at the results reveals that the latent space seems to capture the core-periphery structure of the financial network, that is, the fact that a few banks act as intermediaries for other banks. Indeed, all big banks are estimated to be located in the center of the latent space. Moreover, the dynamic latent space generates networks that have a core-periphery structure themselves, whereas the model without a latent space does not contain a core-periphery structure. It seems that this core-periphery structure is difficult to capture with observed variables alone.====This paper should be seen as an initial investigation of the dynamics of financial networks. However, modeling the dynamics may significantly improve stress testing methodologies to assess systemic risk. To illustrate the potential of including network dynamics in stress testing, we apply a stress test and contagion variant to the Brazilian financial system, incorporating the network’s predictions. Our aim is to capture some dynamic effects resulting from changing initial conditions in order to evaluate losses due to an exogenous default shock on banks. The results show that contagion losses are higher when we consider the predictions of the model, which may be explained by interactions of the network structure and bank-specific characteristics, such as solvency and funding needs, in the propagation of shocks. Our approach allows policy-makers to estimate the probability of large losses as a result of default cascades and to identify banks that generate the biggest impact on the financial system in a more realistic setting.====We now relate our work to the economic networks literature. The latent space approach has been used in economics before, to account for unobserved homophily that might affect the formation of social networks. Goldsmith-Pinkham and Imbens (2013) add a latent space model of network formation to the linear-in-means peer effects model (Bramoullé et al., 2009) to address the problem of network endogeneity. Like us, they use Bayesian techniques to estimate the model on high school data from AddHealth. They show that taking into account endogenous network formation is important to estimate peer effects on school performance. However, the latent space in Goldsmith-Pinkham and Imbens (2013) is a simple binary space, and agents’ latent variables are considered static. Graham (2017) considers unobserved heterogeneity in the propensity to link, and proposes fixed effects type of estimators to address this heterogeneity. Breza et al. (2018) consider a latent space model that is conditioned on aggregate relational statistics. They argue that generating synthetic network data from this model may well act as a substitute to actual network data in case the actual network is unattainable or too costly to be measured. This literature so far, however, has only considered a static latent space framework. We are the first to apply a dynamic latent space model to (financial-)economic network data.====The methodology for dynamic network analysis is less developed since most of the models are for modeling static networks.==== However, given the importance of the subject, statistical models for the analysis of longitudinal network data are emerging. In addition to the latent space model, examples are the stochastic actor-based model (Snijders and Steglich, 2015), the extension of the Exponential Random Graph Model==== (ERGM; see Snijders, 2002) for modeling dynamic networks (Hanneke et al., 2010) and the mixed membership stochastic block model (Xing et al., 2010). The ERGM and stochastic actor-based model assume a homogeneous representation of the network behavior, while the others allow for nodal heterogeneity in model parameters. Finally, a different approach combines agent-based modeling and the literature on strategic network formation to gain some economic intuition on the network formation process (see e.g. Blasques et al., 2018).====The rest of this paper is organized as follows. Section 2 describes the latent space model for dynamic networks. Section 3 provides an explanation of the Bayesian estimation of model parameters. Section 4 describes the data on unsecured interbank exposures, repo transactions and bank variables. In Section 5 we analyze the two dynamic networks and report the main results and in Section 6 we discuss the importance of the latent space. Finally, Section 7 presents an application of the model and Section 8 concludes.",Dynamic interbank network analysis using latent space models,https://www.sciencedirect.com/science/article/pii/S0165188919301897,4 November 2019,2019,Research Article,379.0
"Majewski Adam A.,Ciliberti Stefano,Bouchaud Jean-Philippe","Capital Fund Management 23 rue de l’Université Paris 75007, France","Received 9 January 2019, Revised 23 October 2019, Accepted 25 October 2019, Available online 1 November 2019, Version of Record 2 March 2020.",https://doi.org/10.1016/j.jedc.2019.103791,Cited by (10),"Trend and Value are pervasive anomalies, common to all financial markets. We address the problem of their co-existence and interaction within the framework of Heterogeneous ","Among the most challenging anomalies for the efficient market hypothesis are the so-called ‘Trend’ and ‘Value’ effects, that pervade all financial markets (see e.g. Asness et al., 2013). Trend means that positive (negative) returns over a short to medium period are likely to be followed by positive (negative) short-term return. Value means that assets with prices lower (higher) than their ‘fundamental value’ tend to have positive (negative) future returns. While the former anomaly essentially implies positive autocorrelations of short to medium-term returns (weeks to months), the latter implies a negative correlation of long term returns (corresponding to price mean-reversion on a multi-year time scale). Both anomalies have been extensively validated statistically in the empirical literature for various types of assets.==== The present paper studies the qualitative and quantitative nature of these two effects and their interaction, within the framework of an agent-based inspired model.====Having acknowledged the existence of a trend effect, it is interesting to investigate the structure of the relation between past and future returns. In two recent papers, Lempérière et al. (2014) and Bouchaud et al. (2017) present empirical evidence that this relation is non-linear and surprisingly non-monotonic. The trend effect saturates for larger trend signals, and even appears to ==== for very large trend signals. The first goal of our study is to introduce a mechanism that leads to such an effect. The intuitive explanation of this phenomenon is that when trend signals are very strong it is very likely that the price is far away from the fundamental value. Fundamentalists (i.e. investors believing in value) then become more active, causing price mean-reversion, overriding the influence of chartists (or trend-followers). It is therefore natural to approach this problem within the framework of heterogeneous agent-based market models.====The cornerstone of classical financial theory is the existence of a representative investor with rational expectations. This paradigm has been questioned and rejected by many scholars who have provided evidence that investors are in fact heterogeneous and at best boundedly rational (see e.g. Daniel, Hirshleifer, Subrahmanyam, 1998, Kahneman, 2011, Landier, Ma, Thesmar, 2017, Shiller, 1987, Thaler, 1993, Thaler, 2005 and many others). In order to meet these empirical findings, the HABM literature assumes the existence of several types of investors with simple investment heuristics, that interact with each other via market prices. One type of traders in agent-based models are trend-followers with extrapolative expectations: their beliefs on future prices are formed by extrapolating recent trends in price. For example, Landier et al. (2017) provide quantitative evidence that investors are more prone to extrapolative expectation. The self-referential behavior of trend-followers creates trends and moves price away from the fundamental value, as in Wyart and Bouchaud (2007). When the market price is far away from the fundamental value, a second group of traders, the fundamentalists, step in and take opposite positions, thus creating the price reversal responsible for the value effect (Chiarella, 1992, Lux, 1998). In Schmitt and Westerhoff (2016) and Dieci et al. (2018) trend and value effects emerge from herding-dependent market entry and exit waves of (homogeneous) investors.====The great advantage of ABMs (Agent Based Models) is that they offer a description of the resulting price dynamics. It can be provided as a solution of a nonlinear dynamical system, where bifurcation and chaos theories may be applied to help understand the resulting model dynamics. This approach is called analytical and in this group of ABMs one can include the seminal models of Zeeman (1974), Beja and Goldman (1980), Chiarella (1992), Brock, Hommes, 1997, Brock, Hommes, 1998, Lux (1998), and later models by Chiarella and He (2001), Chiarella, Dieci, Gardini, 2002, Chiarella, Dieci, Gardini, 2006, Wyart and Bouchaud (2007). It has been shown that ABM are able to replicate and explain a variety of stylized facts of financial markets, like excess volatility, volatility clustering, fat-tails of returns (see for example Challet, Marsili, Zhang, 2013, Giardina, Bouchaud, 2003, Gusev, Kroujiline, Govorkov, Sharov, Ushanov, Zhilyaev, 2015, Lux, 1998, Lux, Marchesi, 1999 and Hommes and LeBaron, 2018). For reviews on HABMs see Hommes (2006), LeBaron (2006), Chiarella et al. (2009) and Dieci and He (2018).====In this paper we assume that market clearing is done by a price impact mechanism. In that case, the accumulated demand functions of investors are crucial for determining the price dynamics. Two elements seem necessary to obtain the non-monotonic shape of the trend effect: ====) a bounded demand of trend-followers that are active for small and medium trend signals, before saturating for large signals; and ====) a demand of fundamentalists that grows with increasing mispricing. The model of Chiarella (1992) encapsulates these features, and is therefore able to reproduce the desired shape of the trend effect. We modify the original model of Chiarella (1992) by allowing the fundamental value to have a long term drift, and by adding a third group of agents, which represent noise traders. This leads to a more realistic price dynamics.====The second goal of our paper is to model and measure the value effect. A detailed analysis reveals the non-linear nature of this effect. It seems natural that profits from value investing grow proportionally to the difference between price and fundamental value. However, a model with such a linear demand of fundamentalists fails to reproduce some aspect of the data. On the other hand a non-linear demand of fundamentalists implies a non-linear shape of the value effect, similar to the one observed empirically.====Finally, one would like to know how often, and by how much, markets are over/under-valued. In his famous paper, Black (1986) suggested that prices are typically ‘a factor 2’ away from value, a conclusion bolstered by the analysis of Bouchaud et al. (2017), who report 50% typical mispricings and a time scale of several years for markets to self-correct. Recently, Schmitt and Westerhoff (2017b) presented evidence that the distribution of price distortion of S&P 500 is actually bimodal, with a local minimum of probability distribution around zero. That is a very surprising discovery suggesting that market is more often overvalued or undervalued than close to fundamental value. We confirm that our model-implied price distortion has a bimodal distribution for certain assets. It is worth stressing that the proposed agent-based model indeed exhibits a phenomenological bifurcation. The qualitative change of mispricing distribution, from unimodal to bimodal, emerges when the destabilizing activity of trend-followers exceeds the trading activity of fundamentalists (see a related discussion in Bouchaud et al. (2018), Chapter 20).====We estimate our HABM on a rich and diverse dataset (stock indices, commodities, FX rates and government bonds) going back to 1800. The main challenge in estimating HABM is that fundamental value is not an observable quantity. HABM models are often estimated by first estimating fundamental value using an additional economic model (for example the model of Gordon, 1962 for stock markets) and then estimating the parameters of the HABM model by standard econometric methods like OLS or maximum likelihood (see Boswijk, Hommes, Manzan, 2007, Chiarella, He, Zwinkels, 2014 and others). Another approach to HABM estimation is via the method of simulated moments, which searches for the parameter setting such that simulated moments and other statistics match the ones observed in the data (Barde, 2016, Franke, Westerhoff, 2011, Franke, Westerhoff, 2016, Ghonghadze, Lux, 2016). An interesting estimation of HABM combining the method of simulated moments with Shiller’s concept of the fundamental value was proposed by Schmitt and Westerhoff (2017a). Contrary to standard method of simulated moments the combined method is able to provide a reasonable estimate of market’s mispricing. Recently, independently from our own research, a new approach has been proposed by Lux (2018) and Bertschinger et al. (2018).==== Both papers treat the unobserved quantities as hidden variables that are estimated using particle filtering. The parameters of the system are determined by a numerical black-box procedure maximizing the likelihood given the filtered-out hidden variables.====Our estimation approach falls into the last category. We treat the fundamental value of the asset as the hidden variable and we filter it out using Bayesian filtering techniques. Contrary to Lux (2018) and Bertschinger et al. (2018), who use computationally expensive particle filtering, we apply a classical Kalman filter to the model with a linear value demand function and an Unscented Kalman Filter to the non-linear version of the model. Moreover, for the linear demand function we apply the Expectation-Maximization (EM) algorithm that provides closed-form formulas for iterative procedure maximizing likelihood of the model. The advantage of Bayesian filtering methods over a method of simulated moments is both higher precision of parameter estimates and the possibility of identification of the hidden variables trajectories that are of crucial conceptual importance in most agent-based models. We also find the Bayesian filtering approach more consistent and more universal than estimating first a fundamental value using an additional economic model and then estimating the HABM: our approach does not need any additional assumption on the fundamental value process and can thus be applied to all asset classes.====The rest of the paper is organized as follows. Section 2 introduces our version of the Chiarella HABM, with a linear demand of fundamentalists. In the same section we provide some basic properties of the model (Section 2.2) and we describe the estimation methodology (Section 2.3). Section 2.4 describes the model-implied trend and value effects and compares them with the ones observed in the data. Section 3 extends the model by considering a non-linear demand function of fundamentalists. In Section 4 we analyze the statistical properties of the stationary distribution of mispricing. Section 5 concludes.",Co-existence of trend and value in financial markets: Estimating an extended Chiarella model,https://www.sciencedirect.com/science/article/pii/S0165188919301885,1 November 2019,2019,Research Article,380.0
"Arifovic Jasmina,Evans George W.,Kostyshyna Olena","Simon Fraser University, Department of Economics, 8888 University Drive, Burnaby, BC V5A 1S6, Canada,University of Oregon, 1285 University of Oregon, Eugene, OR, USA,University of St. Andrews, Scotland, United Kingdom,Bank of Canada, 234 Wellington Street Ottawa, ON K1A 0G9, Canada","Received 29 October 2018, Revised 20 September 2019, Accepted 4 October 2019, Available online 10 October 2019, Version of Record 17 January 2020.",https://doi.org/10.1016/j.jedc.2019.103775,Cited by (5),We conduct experiments with human subjects in a model with a positive production externality in which productivity is a ==== of the average level of employment of other firms. The model has three steady states and a sunspot equilibrium that fluctuates between the high and low steady states. Steady states are payoff ranked: low values give lower profits than higher values. We investigate whether subjects can learn a sunspot equilibrium. We observe coordination on the extrinsic announcements in our experimental economies. Cases of apparent convergence to the low and high steady states are also observed.,"In this paper, we present an experimental study of a model with multiple payoff-rankable equilibria, including a sunspot equilibrium. The objective of this work is to explore whether subjects can coordinate on a sunspot equilibrium and under what circumstances such coordination arises.====The possibility of multiple rational expectations equilibria (REE) is a central issue in macroeconomics, raising the question of coordination and forcefully emphasizing the importance of expectations. In models with multiple REE the equilibria typically include stationary sunspot equilibria (SSE) solutions, in which agents’ actions are conditioned on an extraneous random variable (Cass and Shell, 1983). This phenomenon can be found, for example, in overlapping-generations models of money, real business cycle-type models with non-convexities, New Keynesian models, endogenous growth models, and monetary search models. SSEs provide an interpretation of economic fluctuations as at least in part due to self-fulfilling changes in household and firm expectations.====A question of considerable interest in macroeconomics is whether agents can coordinate on SSEs. For example, Farmer (1999) and Clarida et al. (2000) have argued that SSEs may provide an explanation for business cycle fluctuations. Sunspot-driven fluctuations are more plausible in models in which SSEs are stable under adaptive learning. This possibility has been demonstrated by Woodford (1990), (Evans and Honkapohja, 1994), (Evans et al., 1998), (Honkapohja and Mitra, 2004), (Evans et al., 2007), (Shea, 2013), (Mc Gough et al., 2013), as well as in numerous other papers.====The potential importance of SSEs continues to generate interest. For example, in the context of multiple steady states due to the interest-rate zero lower bound, stressed by Benhabib et al. (2001), the possible relevance to the Great Recession of the existence of associated SSEs has been emphasized by Mertens and Ravn (2014). In a new generation of bubbles models, SSEs have recently been suggested as a source of asset price volatility – see (Gali, 2014), (Farmer, 2015) and Miao et al. (2019).====Given this background, although there is of course important related literature discussed below, there has been relatively little research on SSEs in experimental macro settings. Our aim, therefore, is to determine whether it is possible for subjects in an experimental setting to coordinate on an SSE within a simple and transparent economy. The sunspot equilibria we consider are in the spirit of macroeconomic models that obtain cyclical fluctuations in settings in which multiple equilibria arise through strategic complementarities, externalities or monopolistic competition. Examples include the coordination failure models of Cooper and John (1988), the “animal spirits” model of Howitt and McAfee (1992) based on transactions externalities, and the multiplicities and sunspot equilibria found in non-convex real business cycle and endogenous growth models when positive production externalities or monopolistic competition are present, as in Benhabib and Farmer (1994) or (Evans et al., 1998).====Because in experiments it is crucial to keep the setting simple, we develop a stripped-down, essentially static, framework in which a positive production externality leads to the existence of multiple steady states and sunspot equilibria. We also ensure that the decision the subjects need to make each period is simple and transparent: to forecast aggregate employment. Although the framework is very simple, this makes it possible to study whether agents can coordinate on a sunspot equilibrium in which there are economic fluctuations of the type found in the macroeconomic literature just cited.====The model has three steady states. In the language of the macro learning literature, the low-employment and high-employment steady states are E-stable (and thus stable under adaptive learning rules), while the middle steady state is not E-stable. There also exists an E-stable sunspot equilibrium on which we concentrate in our experiments. This sunspot equilibrium involves fluctuations between values near the two E-stable steady states, i.e between low and high steady states. These two steady states are payoff ranked: the high-employment steady state has higher profits than the low-employment steady state does. This feature presents an additional challenge for coordination on a sunspot equilibrium because it implies switching from high-payoff to low-payoff outcomes.====The payoff rankability of the certainty equilibria motivates two different experimental treatments. In the first, the subjects’ payoff is based on profits, which presents the challenge discussed above. In the second treatment, the subjects’ payoff is based on the forecasting accuracy of their forecasts (forecast squared error), and thus the two certainty equilibria are not payoff-ranked in this case. These two treatments allow us to address question of whether it is easier to coordinate on sunspot equilibria if they yield the same payoffs as certainty equilibria.====Our motivation for using two treatments with different payoffs based on profits and forecast squared error is also based on the observation about accuracy of inflation expectations of firms and consumers relative to the accuracy of inflation expectations of professional forecasters. Although these stylized facts relate to inflation expectations, we believe that the underlying link to the optimization function of these agents is relevant to our understanding of expectations about employment in our experiments. Based on the firm survey in New Zealand, Coibion et al. (2018) document that inflation expectations of firm managers are higher than those of professional forecasters and exhibit more disagreement than those of professional forecasters. Ehrmann et al. (2017) show that household inflation expectations in Michigan Survey of Consumers are above those of professional forecasters. Coibion et al. (2018) argue that inaccuracy in firms forecasts could be attributed to the inattention linked to the characteristics of the firms such as duration until the next price change, slope of the profit function and share of exports in their sales. In other words, firms do not pay attention to the dynamics in aggregate inflation because it is not directly linked to their decision making which determines their payoffs – profits. Similar reasons were suggested to explain bias in consumers inflation expectations – they care about inflation for their own consumption bundle, not aggregate level. Professional forecasters, on the other hand, care directly about the accuracy of their forecasts, and, therefore, their forecasts are more accurate than those of firms or consumers. In our experiments, we implement two treatments with different payoffs to capture different optimization functions – profits for firms and forecast squared error for professional forecasters. Our hypothesis is that forecasts are more accurate in the treatment with payoff based on the forecast squared error than forecasts in the treatment with payoff based on the profits. Our findings are consistent with this hypothesis.====The main result of our experiments is that subjects can coordinate on sunspot announcements in both treatments. We also observe coordination on high- and low-employment steady states where subjects disregarded sunspot announcements. While the experiments show frequent examples of coordination on the sunspot announcements in both treatments, in the treatments with forecasting accuracy, the subjects’ forecasts and outcomes are closer to the equilibria corresponding to the announcement, i.e. the coordination is more accurate (consistent with the hypothesis outlined above). We should note that less accurate forecasts in our profit treatment can be attributed partly to the flatness of the profit function, consistent with the finding in Coibion et al. (2018) that firms with steeper profit function make smaller forecast errors.====The paper is organized as follows. In Section 2 we describe related literature. In Section 3, we describe the model. In Section 4, we present the design of the experiments. Section 5 describes the results of the experiments, followed by Section 6 which presents the discussion of adaptive learning in the experiments. Section 7 concludes the paper.",Are sunspots learnable? An experimental investigation in a simple macroeconomic model,https://www.sciencedirect.com/science/article/pii/S0165188919301721,10 October 2019,2019,Research Article,381.0
"Corgnet Brice,Hernán-González Roberto,Kujal Praveen","Emlyon business school, GATE UMR 5824, F-69130 Ecully, France,University Bourgogne Franche Comté, Burgundy School of Business-CEREN (EA 7477), 29 rue Sambin, 21000 Dijon, France,Department of Economics, Middlesex University Business School, London, UK","Received 11 October 2018, Revised 2 September 2019, Accepted 15 September 2019, Available online 17 September 2019, Version of Record 17 January 2020.",https://doi.org/10.1016/j.jedc.2019.103754,Cited by (4),"We study the effect of ambiguity on the formation of bubbles and crashes in experimental asset markets à la Smith, Suchanek, and Williams (1988) by allowing for ambiguity in the fundamental value of the asset. Although bubbles form in both the ambiguous and the risky environments we find that asset prices tend to be lower when the fundamental value is ambiguous than when it is risky. Bubbles do not crash in the ambiguous case whereas they do so in the risky one. These findings, regarding depressed prices and the absence of crashes in the presence of ambiguity, are in line with recent theoretical work stressing the crucial role of ambiguity to account for surprisingly low equity prices (high returns) as well as herding in asset markets.",None,On booms that never bust: Ambiguity in experimental asset markets with bubbles,https://www.sciencedirect.com/science/article/pii/S0165188919301514,17 September 2019,2019,Research Article,382.0
"Cornand Camille,Hubert Paul","University Lyon, CNRS, GATE L-SE UMR 5824, F-69130 Ecully, France,Sciences Po - OFCE, 10 place de Catalogne, 75014 Paris, France","Received 4 October 2018, Revised 26 August 2019, Accepted 1 September 2019, Available online 5 September 2019, Version of Record 17 January 2020.",https://doi.org/10.1016/j.jedc.2019.103746,Cited by (21),"Establishing the external validity of experimental ====, based on various categories of agents (participants in experiments, households, ","Understanding the formation of economic agents’ inflation expectations is crucial for the conduct of monetary policy. Recently, a growing macro-experimental literature has focused on inflation expectation formation in the laboratory.==== Laboratory experiments—particularly Learning-to-Forecast Experiments (LtFE)==== (see Hommes (2011) for a survey)—have been used to validate expectation hypotheses and learning models. These experiments can also serve as important tools for central bankers, providing a test bed for competing policy actions or monetary policy rules (Cornand and Heinemann, 2014, Cornand and Heinemann, 2019). The external validity of expectations is critical for policy initiatives to be valid outside the laboratory (Duffy, 2008).====The question of how agents form inflation expectations is generally studied using survey data on expectations of future inflation. Survey data present the advantages (over experimental data) of providing “natural” expectations that are, in principle, more representatively sampled and that should be subject to more external validity. However, they present the inconvenience of paying a fixed reward, implying that there is no incentive for economic agents to provide an answer that is as accurate as possible.==== Extracting expectations from financial instruments (such as inflation swaps) provides a response to the lack of incentives. However, there are no direct observations of expectations, and their extrapolation can interfere with biases or other uncontrolled elements (yielding a problem of testing joint hypotheses). While survey measures are not directly linked to financial decisions, market-based measures incorporate liquidity or risk premia. In contrast, laboratory experiments provide data on expectations that respond to incentives. They also offer data, the generating process of which is perfectly known by the experimenter and provides the experimenter with a large number of independent observations. Overall, all inflation expectation data (experimental data, market-based data, survey data and central bank data) have some relative advantages and drawbacks, and they provide an overview of how economic agents form inflation expectations, how successful they are in forming these forecasts, the extent to which they deviate from the benchmark of full information rational expectations, and the variables that enter their determination.====The debate on the external validity of experimental inflation forecasts echoes the issue—noted by Carroll (2003), among others—of the heterogeneity between the inflation expectations of various categories of agents—professional forecasters, industrial forecasters, central bankers, households, financial market participants—and how each of these groups forms expectations. While the literature has provided some comparisons of survey measures of inflation expectations (see, e.g., Thomas (1999) for such a study of US data), a comparison across ==== of measures (survey, market based, central banks, and experimental), based on various categories of agents (including households, industry, professional forecasters, financial market participants, central bankers, and participants in laboratory experiments), has not yet been performed. Our contribution is therefore to document whether different types of measures share common patterns by:====To this end, we analyze for each of our different samples (experimental, survey, financial market and central bank data) whether forecast errors are significant to evaluate whether economic agents (participants in laboratory experiments, households, industry, professional forecasters, financial market participants, central bank staff and policymakers) make systematic mistakes, i.e., form biased inflation expectations. We also measure the magnitude of absolute forecast errors to establish the forecasting quality of our different samples. To do so, we rely on the methodology used by Diebold and Mariano, 1995, Romer and Romer, 2000, and Ang et al. (2007) to test the forecasting performance of different types of data.====To capture these deviations (either due to informational friction or because economic agents use simple rules of thumb), we study for each of our different samples for whether forecast errors are autocorrelated, whether they are correlated with forecast revisions and whether forecast revisions depend on past forecast revisions. Here, we follow the methodology of Andolfatto et al., 2007, Coibion and Gorodnichenko, 2012, Coibion and Gorodnichenko, 2015), and Andrade and Le Bihan (2013), who noted the role of information rigidities.====We study how lagged inflation and output gaps affect inflation forecasts to evaluate whether the usual determinants of inflation are used by the different considered categories of agents. The formation processes of expectations and the variables entering this process have been notably studied by Mankiw and Reis, 2002, Sims, 2003, Lanne et al., 2009, Pfajfar and Santoro, 2010, Fendel et al., 2011, and Dräger et al. (2016). We follow their methodology.====While these analyses have been conducted separately and have excluded experimental data, our paper precisely aims at conducting these analyses in parallel to provide a large set of characteristic comparisons and to enlarge the data sets by including experimental inflation expectations. Using standard measures of forecast characteristics, determinants of forecast errors and expectation formation determination, these comparisons are intended to determine whether the various sets of inflation expectations exhibit heterogeneous or common patterns and thus to examine the external validity of experimental inflation forecasts. While establishing the external validity of experimental inflation expectations represents a key step in the development of macro-experiments, especially those addressing monetary policy issues, to our knowledge, there is no available study relying on a sample of experimental data on inflation expectations, confronting it with field data.==== From this perspective, our aim is also to advise the policymaker about the informativeness of the different types of data. Indeed, all of these sources are used by central banks in their policy decisions.====Despite the considerable heterogeneity among our six different categories of data, we find that the different data sets exhibit various common features: forecast errors are comparably large; autocorrelations of forecast errors are positive and significant; forecast errors and forecast revisions are very often predictable; and the standard lagged inflation determinant of inflation expectations is robust to the data sets. There is nevertheless some heterogeneity among the six different sets. If experimental forecasts are relatively comparable to survey and financial market data, central banks’ forecasts seem superior since they do not exhibit systematic bias, are less autocorrelated and are hardly predictable. Excluding central bank forecasts, we conclude that experimental data are comparable to other data sets in the sense that forecast errors exhibit the same type of bias (except for industry forecasts), and lagged forecast revisions significantly predict forecast revisions.====The paper is structured as follows. Section 2 presents the data. Section 3 describes the empirical results. Section 4 concludes the paper.",On the external validity of experimental inflation forecasts: A comparison with five categories of field expectations,https://www.sciencedirect.com/science/article/pii/S0165188919301459,5 September 2019,2019,Research Article,383.0
"Halim Edward,Riyanto Yohanes E.","Division of Economics, School of Social Sciences, Nanyang Technological University, 48 Nanyang Avenue, Singapore, 639818,Economics Programme, School of Social Sciences, Nanyang Technological University, 48 Nanyang Avenue, HSS, #04-70, Singapore, 639818","Received 3 October 2018, Revised 27 August 2019, Accepted 1 September 2019, Available online 3 September 2019, Version of Record 17 January 2020.",https://doi.org/10.1016/j.jedc.2019.103745,Cited by (3),"We investigate the impact of compulsory insider-trading disclosure and its combination with a mandatory holding rule on price predictability and asset mispricing. We modify the dynamic price-adjustment model to account for insiders’ ====. Our results show that insiders produce weakly-characterized price signals that induce a less than proportional price adjustment to the changes in the dividend value, in both markets with disclosure rule and holding requirement, in comparison with unregulated markets. A shift in insiders’ strategies from information-motivated to liquidity-motivated trading appears to fuel the impairment of price predictability in regulated markets. The exacerbation of asset mispricing in markets with holding restriction is characterized by a growth in the speculative transactions and loss-making trading proposals.","Insider transactions in stock trading are illegal when the trades are carried out based on non-public material information (such as a merger and corporate restructuring plan) that a corporate insider possesses. As long as the non-public material information is not involved, corporate insiders may buy or sell shares in their possession.==== Rampant expropriation and the exploitation of privileged information, however, have led to calls for better protection of imperfectly informed investors and stronger protection of price integrity. The current paper addresses a set of arguments made in connection with the impact of insider trading regulations on market performance. We model our markets on the Smith, Suchanek, and Williams (SSW) model (Smith et al., 1988). Both the presence of insider traders and the insider regulations implemented in our experiments are novel additions to Smith's framework.====We chose compulsory disclosure as one of the two treatments in our experiment, as compulsory disclosure is one of the most commonly implemented and widely discussed insider trading regulatory frameworks. Our post-trade disclosure mechanism is modeled after Section 16(a) of the US Securities Law of 1934, which requires any insider who owns at least 10% of his or her company's stocks to report to the Securities and Exchange Commission (SEC) within the first 10 days of the month following a relevant transaction.====A fair assessment of the empirical evidence indicates mixed outcomes when it comes to the desirability of the disclosure policy. In principle, the protocol is seen as facilitating market learning, despite variations in the speed of price discovery (Huddart et al., 2001). Disclosure fosters traders’ confidence in the market reliability and creates a thicker market through increased market participation (Bhattacharya and Spiegel, 1991, Agarwal et al., 2015). However, it is also inextricably linked to a surge in strategic executions that mislead, obfuscate, and exploit the poor decisions of less sophisticated investors (John and Narayanan, 1997). Even in the absence of an insider's motive to engage in market manipulation, outsiders are still incapable of distinguishing informed trading from individual portfolio adjustment. It follows that, as the signal dissemination process is obfuscated, market quality is impaired (Huang, 2008).====To the extent that insider trading is harmful to uninformed investors, shareholders often voluntarily impose trading restrictions, such as the corporate lockup period or the restricted stock rule in Rule 144 of SEC (Osborne, 1982, Kahl et al., 2003). Provisions such as these are commonly enforced to resolve moral hazards and adverse selection problems by increasing the concentration of managerial holdings in the firm in question, and guard the market against undesirable selling pressures induced by the short-term gain motives of the insiders (DeMarzo and Urošević, 2006; Sircar and Xiong, 2007; Guo et al., 2012). However, while the benefits of reducing agency conflicts are understood, the costs inflicted by these provisions remain predominantly unexplored. On this basis, we consider a combination of the compulsory disclosure rule with a holding rule that captures the spirit of the corporate lockup period and the restricted stock regulation, as our second treatment.====To the best of our knowledge, only Sutter et al. (2012) have assigned permanent identities and made the existence of insiders a matter of common knowledge in order to study the formation of asset bubbles in the SSW framework.==== Insiders receive dividend clues at the start of a period and trade with uninformed traders who are cognizant about the asymmetric information. However, insiders in their experiments are not required to follow any regulations. We build on and extend the literature by not only keeping traders’ role the same (whether they are insiders or normal traders) throughout the experiment, but also making it compulsory for insiders to reveal their transactions and hold acquired assets (when the holding requirement is added to the disclosure requirement) for a minimum of two trading periods before being able to resell them. The transaction revelation is carried out in a real-time fashion and normal traders receive information feeds as soon as insiders execute their trading activities.====Our results demonstrate weak price predictability in markets with disclosure and holding rules. Informed agents deliver price signals that imperfectly co-move with changes in private information. Our analysis suggests that insiders switch from information-driven to liquidity-motivated trading strategies as their transactions are revealed to outsiders, hampering outsiders’ ability to accurately interpret the private information received by insiders. A wider price discrepancy from the fundamentals is observed in markets with holding restriction, characterized by growth in speculative trading and loss-making trade proposal submissions generated in the markets. Our results highlight the need to look deeper into the role of informed agents’ liquidity trading when it comes to asymmetrical information setting with transparent regimes. We also identify the need to incorporate the joint effect of market transparency and liquidity restrictions into the current debate regarding disclosure rules.====This paper is organized as follows. We present the experimental design and procedure in Section 2. We discuss our empirical strategy and results in Sections 3 and 4. Specifically, Section 3 presents a discussion of the dynamic price adjustment model and price predictability and Section 4 presents our experimental results. Finally, Section 5 concludes our findings.",Asset markets with insider trading disclosure rule and reselling constraint: An experimental analysis,https://www.sciencedirect.com/science/article/pii/S0165188919301447,3 September 2019,2019,Research Article,384.0
"Pavan Marina,Barreda-Tarrazona Iván","LEE & Economics Department, Universitat Jaume I, Castellón, Spain,CERME & Management and Economics Departments, Università Ca’Foscari, Venezia, Italy","Received 14 October 2018, Revised 31 May 2019, Accepted 16 August 2019, Available online 21 August 2019, Version of Record 17 January 2020.",https://doi.org/10.1016/j.jedc.2019.103733,Cited by (4),"We study strategic default in the laboratory, i.e., in a controlled experiment. Subjects are initially endowed with a house and a ==== are more likely to default. We observe that subjects default less than optimal, and this decision is significantly affected by social norm concerns in the context of the experiment. Individuals under-consume in the first periods of life: they are “cautious” when indebted. Both introducing a 50% ","The 2007–2008 bust in house prices left millions of homeowners in the U.S. and in many other countries with negative house equity, that is, with an outstanding mortgage debt exceeding the market value of their house. Not surprisingly, mortgage default rates rose during the crisis. Due to the economically harsh times, many of the homeowners that defaulted on their mortgage simply could not afford to make the payments. Some others, though, are likely to have decided that it did not make any financial sense to continue payments, particularly if the institutional framework allowed them to start (almost) afresh. Borrowers who stop servicing their mortgage despite their ability to pay are called “strategic”, and represent a potentially serious problem to the economy, given that they contribute to deepening the decline in house prices and create a negative externality on their neighbors.==== In this paper, we use a laboratory experiment to perfectly identify strategic default behavior, and link it to economic conditions (income, house prices, assets, etc.), moral and social norm concerns and other individual characteristics, under three different institutional arrangements.====Identifying strategic default is very difficult: it requires detailed data on the borrowers’ income and balance sheet, to determine their ability to pay. Some studies use loan-level data and define as strategic defaulters those individuals who did not make any mortgage payments for 180 days or more, while being current on all their other debts (credit cards, car loans, etc.). Using this measure, Experian-Oliver Wyman (2011) reports that in 2008 about 20% of total U.S. defaults were strategic. Guiso et al. (2013) collect questionnaire data and obtain an estimate of strategic default that ranges from 26% in 2009 to 35% in 2010. Gerardi et al. (2018) use data from the Panel Study of Income Dynamics (PSID) surveys between 2009 and 2013 and identify strategic defaulters as those households in default that have the ability to meet their mortgage payments without having to reduce their level of consumption. According to this definition, 38% of defaults in their sample are strategic.==== Recently, Artavanis and Spyridopoulos (2018) obtain a 28% strategic default rate in primary residence mortgages for Greece, exploiting a natural experiment. On the other hand, Gerardi et al. (2018) as well as Bhutta et al. (2017) and White (2010) underline that the majority of (U.S.) homeowners continued to make their mortgage payments despite being deeply underwater. That is, the repayment decision must depend on reasons other than the purely rational motive driven by equity concerns. The sense of guilt or shame of foreclosure, the fear of the perceived consequences of default, a wrong subjective evaluation of house equity, optimistic expectations on future house prices, or the special attachment to one’s own home can play a very important role.====Unfortunately, information on social norms, individual moral concerns, perceived future risks or costs of default is missing in most datasets.==== Moreover, the observed data do not often allow for policy analysis: how would strategic default behavior change under a stricter recourse policy? Or under different mortgage conditions?====We intend to alleviate these difficulties by studying strategic default in the laboratory, that is, in an environment in which we control for default costs, agents’ risk and mortgage conditions. First, we build a dynamic model of individual default decisions, and we parameterize it such that it generates many situations in which default is the utility maximizing choice. In the model, all default is strategic. Second, we compare the model predictions with observed behavior in the laboratory. In the economic experiment, based on our theory, we endow individuals with a house and a mortgage (in a neutral framework), and have them choose at each experimental period for the whole duration of the mortgage life among three options: making the mortgage payment, selling the house and prepaying the mortgage, or walking away from their house and defaulting. In the last two cases, they have to rent for the rest of the decision periods. Subjects face shocks to their employment status and to the market value of houses with a known probability, must decide how much to consume, and have the possibility of accumulating liquid wealth. House price shocks are such that house equity is negative in the first periods of the mortgage life, and individuals’ income is always high enough to allow the mortgage payment and a minimum consumption.====Our main goals are the following:====1) To measure to what extent individuals strategically default in the laboratory in comparison to our theoretical benchmark, and why. Given each combination of initial state variables faced by the experimental subject, we compute the theoretically optimal (i.e. expected utility maximizing) consumption and default choices and compare them to the observed actions. We control for subjects’ risk aversion, cognitive ability, expectations on future house prices and income, moral and social norm concerns and other socio-demographic variables to understand which individual characteristics are linked to default behavior.====2) To test the efficacy of economic incentives aimed at decreasing the incidence of strategic defaults. We do this in two treatments. While in the baseline treatment the defaulter can walk away from the mortgage without having to pay anything back after returning the house, in Treatment 1 (T1) we allow for the possibility of recourse by the lender: with a positive probability, the defaulter must repay the part of the mortgage debt not covered by the collateral. In Treatment 2 (T2), we introduce a cash reward granted to homeowners only once the mortgage has always been serviced up to a certain period, in the spirit of Edmans’ (2010) Responsible Homeowner (RH) Reward. Edmans’ (2010) argues that a contingent incentive would be less costly and much more effective than other solutions (as mortgage principal reductions) to solve the problem of strategic default.==== There is ample evidence in the experimental literature that individuals like and respond more strongly to rewards (Lazear, 2000, among others).====Our theoretical model of optimal default shares some characteristics with the dynamic equilibrium models of Campbell and Cocco (2015); Corbae and Quintin (2015), and Chatterjee and Eyigungor (2015).==== As in those contributions, mortgage default is the rational choice of a utility-maximizing homeowner facing income and house value uncertainty. Differently from these theories that endogenize the mortgage market, however, we build a partial equilibrium model that takes mortgage conditions as given and focuses on the individual decisions of mortgage payment, homeownership and consumption/saving over time. Our parsimonious set-up allows us to conduct an economic experiment on borrowers’ behavior in the lab. Recently, Schelkle (2018) finds that a theoretical model in which mortgage default is triggered by the joint occurrence of negative house equity and a negative shock to income is more consistent with the empirical evidence than a traditional frictionless “option-theoretic” model of default.==== Our experiment is suited to testing this so-called “double trigger” hypothesis.====Economic experiments exploring dynamic consumption/saving decisions under uncertainty similar to the one presented in this paper find that individuals do not generally behave optimally (according to the standard life-cycle model), and conclude that people are “boundedly rational” (see Ballinger, Palumbo, Wilcox, 2003, Carbone, Hey, 2004, Hey, Dardanoni, 1988, among others).==== Despite this gap between observed and optimal behavior, though, the intertemporal optimization model does succeed in predicting behavioral changes as a result of changes in the parameter values, i.e. in the comparative statics. In our experiment, we focus our attention more on the discrete choice of default rather than on consumption/saving behavior, and we analyze the effect of different policies meant to decrease strategic default. It is found that more optimal behavior can be reached with some learning (Ballinger, Palumbo, Wilcox, 2003, Brown, Chua, Camerer, 2009). Following this result, each participant of our experiment goes through five cycles of the decisions in a mortgage life so that they can accumulate some experience in this kind of choices.====Default behavior has also previously been studied in the lab. Brown et al. (2016) use an experimental setting to analyze default behavior and social norms in an economic crisis. To this end, they implement a one shot prisoner’s dilemma in which the ability to cooperate is stochastic, thus creating fundamental as well as strategic default. Trautmann and Vlahu (2013) model the coordination game between two borrowers, who have to decide whether to repay their loan or to strategically default under different expectations on the bank’s strength and on the borrowers’ repayment capacity. Differently from these studies, our experiment focuses on the intertemporality of the individual repayment decision, and on its link to saving/consumption choices, so that our experimental game is more similar to the one faced by actual homeowners. A closer experiment to ours is the one in Rabanal (2014), who tests in the laboratory a frictionless option-theoretic model of the strategic default decision under different asset value volatilities and with or without the existence of social interactions. In contrast to that work, however, we take into consideration the potentially important role played by an unemployment shock in the decision to default, and the fact that the ability and the willingness to pay the mortgage are strictly linked to one’s consumption and saving choices.====Our results show that subjects correctly default in the initial periods of a mortgage life and consume increasingly close to optimal with the game repetition. In particular, the combination of a low house price shock and an unemployment spell is an important trigger of default, coherently with the “double trigger” hypothesis mentioned above. Despite the fact that in our environment there does not exist any social stigma nor any serious consequences for not paying one’s debt, however, experiment participants default much less than optimal: the observed default rate is about half the one predicted by the theory. Individuals’ perception of the social norm in the experiment seems to play a role in explaining this deviation, while more general moral and social concerns do not significantly affect the default decision: an indication, perhaps, that individuals’ answers to survey questions are not always good predictors of their actions. Other individual characteristics, as risk aversion and cognitive ability, do not significantly affect the likelihood of default. Both the treatment introducing a 50% probability of recourse and the treatment offering a Responsible Homeowner Reward at mid-mortgage life are significantly effective in discouraging default, in line with the theory. In fact, the reduction in default observed in the Responsible Homeowner Reward treatment is proportionally higher than what would be predicted by our theory. Both treatments are especially effective for individuals with low income facing deep negative equity, again an indication that income shocks play a potentially important role. Last, in contrast to the intertemporal consumption/saving decisions observed in the laboratory for subjects who do not hold any debt, in our experiment homeowners under-consume in the first periods of life, an indication that they are “cautious” when indebted with the mortgage, even if this debt was exogenously imposed on them.====In the next section we describe our model of the mortgage default decision, and in Section 3 we obtain its theoretical predictions. In Section 4 we describe the experimental design and procedure. Section 5 reports the main results and Section 6 includes a discussion and conclusions.",Should I default on my mortgage even if I can pay? Experimental evidence,https://www.sciencedirect.com/science/article/pii/S0165188919301320,21 August 2019,2019,Research Article,385.0
"Giamattei Marcus,Huber Jürgen,Lambsdorff Johann Graf,Nicklisch Andreas,Palan Stefan","Department of Economics and Business, University of Passau, Germany,Department of Banking and Finance, University of Innsbruck, Austria,Center for Economic Policy Research, HTW Chur, Switzerland,Research group Need-Based Justice and Distribution Procedures (FOR 2104), Germany,Department of Banking and Finance, University of Graz, Austria","Received 10 October 2018, Revised 25 June 2019, Accepted 2 July 2019, Available online 17 July 2019, Version of Record 17 January 2020.",https://doi.org/10.1016/j.jedc.2019.07.004,Cited by (4),"We use a laboratory experiment to study how forecasting contributes to mispricing. In the ====, we assign both the task of forecasting and the task of trading to the same subject. In treatment ====, we separate these tasks and assign them to two different subjects, who share the profits from trade. In treatment ","Investment decisions on asset markets involve at least two tasks: forecasting and trading. Forecasting is the process of making predictions of future prices based on an analysis of past and present data. Trading consists of observing profit opportunities as they arise from deviations between forecasts and market prices, inferring volumes for buying and selling, and adjusting this process in the light of experience with market transactions and the order book. Each of these two tasks contributes to the pricing of assets. If mispricing relative to a fundamental value is found, this may be caused by a number of reasons. If prices are inertial, for example, this may be due to trades that are anchored to historical values. Likewise, bubbles may result from forecasts that extrapolate trends. We investigate how and to what extent forecasting contributes to mispricing.====A laboratory experiment is the only feasible method for studying our research question. It is usually difficult to identify bubbles and mispricing using data from the field because the underlying fundamental value of the asset is unknown. Furthermore, in the field, the two activities – forecasting and trading – cannot always be well separated. Traders use their own forecasts, obtain unobservable insights from team members and would only use publicly available forecasts in some cases. Also, in the field, forecasting and trading tend to interact in different ways. In some cases, forecasters obtain feedback from traders or observe the order book while in other cases they work in isolation as “thinkers” who exploit only publicly available information. This implies that the interaction between forecasting and trading differs across institutional designs.====We approach the research question by isolating the task of forecasting from that of trading. In our ====, we require laboratory subjects to report their forecasts prior to trading across 20 rounds, with the asset’s fundamental value following a random walk. In two additional treatments, ==== and ====, we separate tasks between a forecaster (called “analyst” in our design) and a trader, who work together in groups of two. The forecasters are supplied with some information about the changing dividends, from which they can estimate the fundamental value. Based on this, but potentially also using past prices, they send a price forecast (called a “price estimate”) to their respective traders. The traders buy and sell assets in continuous double auction markets. We implement some novel design choices to ensure that traders cannot infer or guess the fundamental value given that they do not know the current dividend and are not provided with any anchor to infer the fundamental value. They thus need to rely entirely on their respective forecaster’s price estimate. This separation of tasks also implies that forecasts are not influenced by experience from trade. Forecasters rely only on descriptive information regarding fundamental values and past prices. They do not observe the prices of individual transactions or the order book and cannot anchor forecasts to such information. In treatment ====, however, forecasters are connected to traders such that each forecaster is paid the same profit as his or her dedicated trader. In treatment ====, in contrast, forecasters are paid based only on how close their forecast is to the actual market price. They are ranked and paid according to their accuracy. They compete with each other for more accurate forecasts, without having incentives to consider how this may impact the transactions and profits of the traders they are paired with. The incentives given to forecasters in treatment ==== thus correspond to the motivation that forecasting is commonly associated with – the desire to be accurate.====We find only little mispricing in ==== and in ==== and observe large mispricing in ====. Our data reveals no noteworthy differences in the behavior of traders across treatments. Regression analyses show that forecasts are anchored to past prices and exhibit inertia in all treatments. Furthermore, forecasters are strongly affected by trend extrapolation in treatment ====. Our findings are corroborated by results from post-treatment questionnaires. We thus infer that forecasting can be affected by trend extrapolation and can inflate bubbles, in particular if forecasters strive for precision rather than for providing support to profitable trading.",Who inflates the bubble? Forecasters and traders in experimental asset markets,https://www.sciencedirect.com/science/article/pii/S0165188919301113,17 July 2019,2019,Research Article,386.0
"Bao Te,Hennequin Myrna,Hommes Cars,Massaro Domenico","Division of Economics, School of Social Sciences, Nanyang Technological University, 50 Nanyang Avenue, Singapore 639798, Singapore,CeNDEF, Amsterdam School of Economics, University of Amsterdam, PO Box 15867, 1001 NJ Amsterdam, Tthe Netherlands,Department of Economics and Finance, Catholic University of Milan, Largo Gemelli 1, Milan 20123, Italy","Received 8 October 2018, Revised 8 April 2019, Accepted 20 May 2019, Available online 23 May 2019, Version of Record 17 January 2020.",https://doi.org/10.1016/j.jedc.2019.05.009,Cited by (14),"We present a large-group experiment in which participants predict the price of an asset, whose realization depends on the aggregation of individual forecasts. The markets consist of 21 to 32 participants, a group size larger than in most experiments. Multiple large price bubbles occur in six out of seven markets. The bubbles emerge even faster than in smaller markets. Individual forecast errors do not cancel out at the aggregate level, but participants coordinate on a trend-following prediction strategy that gives rise to large bubbles. The observed price patterns can be captured by a behavioral heuristics switching model with heterogeneous expectations.","Expectations are a crucial part of many economic systems. Asset markets are an example of positive feedback systems: when many traders expect the price of an asset to rise, the demand for the asset will increase, leading to a higher market price. This price rise in turn affects expectations. If traders extrapolate a trend in the asset price, the positive feedback can give rise to a bubble, which occurs when the market price becomes significantly higher than the fundamental value of the asset. Such expectations-driven bubbles were observed in several asset pricing experiments (e.g. Hommes, Sonnemans, Tuinstra, Van de Velden, 2005, Hommes, Sonnemans, Tuinstra, Van de Velden, 2008).====The main goal of this paper is to study the effect of group size on the (in)stability of experimental asset markets: will participants coordinate on bubbles in large groups? It is important to know whether the results of small-scale asset market experiments can be generalized to settings with larger groups. Are earlier observed bubbles in small markets perhaps caused by a few “irrational” participants? Surprisingly little work focusing on group size in experiments has been done, maybe because larger experiments are costly. We conduct an asset pricing experiment with larger groups than usual and analyze both individual expectations and aggregate outcomes to empirically test if expectations-driven bubbles also occur in larger markets. Our results show that coordination on bubbles is robust to an increase in group size.====Theory does not provide a definite answer to our research question and offers opposing views. For example, one may argue that the formation of bubbles is more unlikely in large groups, because a single participant has less influence on the market price and is therefore less likely to cause a bubble. Furthermore, coordination on a non-fundamental price is probably harder because it requires more participants to predict the same price in the same period. Instead, individual forecast errors might cancel out at the aggregate level, which would be consistent with the formulation of the rational expectations hypothesis by Muth (1961). If this is correct, a large market could stabilize quickly. On the other hand, participants might see a rising price as a trend and adopt a trend-following strategy, just as in small-scale asset pricing experiments.==== Once a large group coordinates on a bubble, the coordination could be hard to break. This behavior may cause big bubbles in a larger market as well. Only a new experiment with larger group sizes can clarify which of these opposite effects will dominate in a controlled laboratory environment.====Any dynamic model of an asset market strongly depends on the underlying expectations hypothesis. It is therefore essential to develop a theory about how people form expectations and how they adapt their forecasting strategies over time. Laboratory experiments are well-suited to study individual expectation formation. Since the experimenter can control the underlying economic fundamentals, it is possible to obtain explicit observations on expectations, investigate how individual behavior shapes market outcomes and study whether aggregate outcomes deviate from fundamentals. The experimental data can be used to empirically validate different expectations hypotheses, from rational expectations to boundedly rational heuristics. Moreover, the experimental outcomes can provide insight into which forecasting strategies are more likely to be used, so that the “wilderness of bounded rationality” (Sims, 1980) can be disciplined.====Learning-to-forecast experiments (LtFEs) are especially useful to study expectations in dynamic feedback systems. This type of experiment was introduced by Marimon et al. (1993) and Marimon, Sunder, 1993, Marimon, Sunder, 1994. In LtFEs, the participants’ only task is to submit forecasts in a particular economic setting. All other actions are computerized. Because LtFEs separate expectation formation from other choices, such as trading decisions, they provide “clean” data on expectations. Hommes (2011) provides a review of LtFEs in different economic settings.====In the asset pricing LtFE of Hommes, Sonnemans, Tuinstra and Van de Velden (2008) (henceforth HSTV08), participants have to predict the price of a risky asset for 50 periods. Each experimental asset market consists of six participants. In every period, the market price is derived from mean-variance optimization and depends on the average price forecast. Earnings are based on prediction accuracy. Participants only have qualitative information about the asset market, but they know the risk-free interest rate and the mean dividend of the risky asset. This is enough information to calculate the fundamental value of the asset. Nevertheless, large price bubbles occur in five out of six sessions, with prices rising up to fifteen times the fundamental value of 60 (see Fig. 1). Once predictions reach an artificial upper bound of 1000, the trend reverses and the market crashes rapidly. There is no convergence to the fundamental.==== Analysis of the individual expectations reveals that participants within the same market coordinate on a common trend-following prediction strategy. This result is remarkable, since participants do not observe the forecasts of others, so they could only coordinate through the realized market prices. An earlier asset pricing LtFE by Hommes et al. (2005a) is similar, although the design of that experiment inhibits the formation of large bubbles because of the presence of fundamental robot traders and an upper bound on price forecasts of 100.====In macroeconomic and financial market experiments, group size is an important issue. The scale of many macroeconomic situations can often not be replicated in experiments because of the limited lab size and financial constraints. Yet, the use of macroeconomic models with microfoundations makes it possible to test assumptions on individual behavior in lab experiments on a smaller scale. Many examples of macroeconomic experiments are discussed by Duffy, 2008, Duffy, 2016 and Ricciuti (2008), who argue that lab experiments are useful to complement the theoretical and empirical research in macroeconomics. The vast majority of experiments uses small groups, often consisting of less than ten participants. Experiments with large groups are rare.==== Hence, the question remains whether the results of small-scale experiments are robust to increases in group size. The answer to this question is critical if the results from the experiment are compared to real macroeconomic situations with many interacting agents.====In this paper, we address the issue of group size by analyzing an asset pricing experiment with the same design as HSTV08, but with larger markets. The experiment consists of seven sessions with 21 to 32 participants per market. This is a group size that fits in most labs for economic experiments. Although the groups in this experiment are still small compared to the number of traders in a real asset market, the increase in market size compared to HSTV08 can shed light on the differences that group size could make in both individual expectations and market outcomes. Our experiment provides an empirical answer to the question whether bubbles will arise in large groups as well.====A large-scale classroom experiment based on the classical design of Smith et al. (1988) has shown that bubbles and crashes also occur in markets with 244, 304 and 310 participants (Williams, 2008, Williams, Walker, 1993). In their experiment, participants buy and sell a risky asset in a double auction market. The fundamental value of the finitely lived asset is monotonically decreasing over time. Despite the fact that the fundamental value is explicitly given to all participants in each trading round, a large price bubble forms in all markets, followed by a crash. The main difference with our experiment is that Williams and Walker (1993) only consider trading decisions, while our LtFE specifically focusses on expectation formation. Futhermore, the fundamental value of the risky asset in our experiment is constant instead of declining, and not explicitly given. Lastly, the experiment of Williams and Walker (1993) was an extra-credit exercise for a microeconomics course, lasting for eight weeks. We use a standard laboratory setting with monetary incentives.====The results of our LtFE show that bubbles also occur in large groups: six out of seven markets exhibit large price bubbles. The typical price pattern shows multiple large bubbles with decreasing amplitude. Due to the high instability of the market, the forecasting performance of most participants is poor, resulting in very low earnings. Participants are able to coordinate on similar prediction strategies, but we also observe heterogeneity in expectations and strategy switching. Estimation of individual prediction strategies shows that the behavior of many participants can be captured by simple linear forecasting rules that resemble benchmark heuristics, such as trend-following rules and anchoring and adjustment. It is remarkable that the bubbles in our large groups occur even faster and more frequently than in the smaller markets of HSTV08. Nevertheless, we do not find any structural differences between expectations and prices in large and small groups. The results of related small-group experiments thus seem robust to an increase in group size.====The individual prediction rules are further investigated with a behavioral heuristics switching model (HSM). This model takes account of heterogeneity in expectations and evolutionary selection among different forecasting heuristics. We use the same benchmark HSM as in Anufriev and Hommes (2012b), which is an extension of the model of Brock and Hommes (1997). The results of one-period-ahead simulations show that the bubbles are amplified by the use of a strong trend-following prediction rule. The use of an anchoring and adjustment heuristic can explain the persistent price oscillations after the first crash. The flexibility of the HSM substantially improves the model fit compared to six homogeneous prediction rules.====Our experiment relates to previous asset pricing LtFEs (e.g. Hommes, Sonnemans, Tuinstra, Van de Velden, 2005, Hommes, Sonnemans, Tuinstra, Van de Velden, 2008), but also connects to two other strands of experimental literature concerning the role of expectations in mispricing. The first line of literature originates from the asset trading experiment of Smith et al. (1988), a setting in which bubbles frequently occur. Several studies show that traders’ expectations are adaptive and primarily depend on trends in the markets (Haruvy, Lahav, Noussair, 2007, Holt, Porzio, Song, 2017, Smith, Suchanek, Williams, 1988). Carlé et al. (2019) find that heterogeneity in expectations plays a key role in the market dynamics, and Cheung et al. (2014) and Akiyama et al. (2017) report that reducing strategic uncertainty and facilitating the formation of common expectations mitigates mispricing. The second line of literature on information mirages shows that bubbles and mispricing may emerge as a result of traders reacting to uninformative trades by others when there possibly exists inside information. The incorrect expectations of traders then lead to a failure of the asset market as a mechanism for information aggregation (Anderson, Holt, 1997, Barner, Feri, Plott, 2005, Bossaerts, Frydman, Ledyard, 2013, Camerer, Weigelt, 1991, Copeland, Friedman, 1992, Drehmann, Oechssler, Roider, 2005, Noussair, Xu, 2015).====The remainder of this paper is organized as follows. Section 2 explains the experimental design in detail. The results of the experiment are discussed in Section 3. In Section 4, we estimate forecasting rules for each participant. The specification of the HSM is described in Section 5 and simulations with the model are presented in Section 6. Section 7 summarizes and concludes.",Coordination on bubbles in large-group asset pricing experiments,https://www.sciencedirect.com/science/article/pii/S0165188919300880,23 May 2019,2019,Research Article,387.0
"Farvaque Etienne,Malan Franck,Stanek Piotr","LEM (CNRS - UMR 9221), Université de Lille, Faculté des Sciences Economiques et Sociales, Bâtiment SH2, 59655 Villeneuve d’Ascq Cedex, France and CIRANO, Québec, Canada,LEM (CNRS - UMR 9221), Université de Lille, UFR Mathématique Infomatique Managment and Economie (MIME), Domaine de Pont de Bois, Bâtiment F, 59650 Villeneuve d’Ascq Cedex, France and EDEHN, Université du Havre, France,Cracow University of Economics, Faculty of Economics and International Relations, ul. Rakowicka 27, Krakow 31–510, Poland","Received 15 October 2018, Revised 1 April 2019, Accepted 3 May 2019, Available online 15 May 2019, Version of Record 17 January 2020.",https://doi.org/10.1016/j.jedc.2019.05.004,Cited by (4),.,"What could influence individual central bankers decision making? What influences the development of their preferences? Are the preferences innate, directly inherited, or acquired by some more oblique transmission channels? If laypersons’ preferences are shaped by the individual internalization of cultural norms and values, through the intra-family transmission, and influenced by one’s experiences through early life, is this also the case for central bankers?====For laypersons, it has been shown that personal experiences matter, especially in early life. Dohmen et al. (2011), for instance, show a considerable influence of parental attitudes towards risk-taking on children’s behavior. As Emmenegger et al. (2017) explain that early-life experiences can “scar” people, and young-age unemployment spells can have a lasting impact on future political interest. Closer to the point we make in this article, Malmendier and Nagel (2011) demonstrate that individuals who have experienced low stock market returns throughout their lives report lower willingness to take a financial risk, and are more pessimistic about future stock returns. These “Depression babies”, as Malmendier and Nagel (2011) have called them, have different risk-taking attitudes. Such a behavioral pattern is confirmed by Giuliano and Spilimbergo (2013), who find that those who experienced a recession when young believe that success in life depends more on luck rather than effort, support more government redistribution, and tend to vote for left-wing parties.====But what happens when Depression babies grow-up as central bankers? Do they also suffer from such scars, and do they grow more risk-averse, and more recession averse? Anecdotal evidence, such as the one revealed by Pixley (2004),==== tends towards an affirmative answer to this question. However, so far, the literature has not dealt in detail with this question.==== Therefore, in this article, we focus on central bankers and aim to analyze if the “Depression baby effect”, enlarged to a “recession childhood effect” is also present when agents are at the helm of a central bank. In other words, we aim to verify if central bankers who have been through the recession(s) in their early life develop a greater recession-averse behavior than their counterparts.====The common sense intuition that leadership matters is empirically backed increasingly. For instance, Besley et al. (2011) or Hayo and Neumeier (2012) confirm that leaders’ background matters in macroeconomic developments. This line of thought applies to central bankers as well, and it has been shown that the votes of the Federal Reserve’s Federal Open Market Committee (FOMC) members are significantly affected by their educational and professional achievements (Chappell, McGregor, Vermilyea, 2005, Eichler, Lahner, 2013). Results from larger samples indicate that central bankers’ occupational background, as well as their education, can be an important determinant to consider (Farvaque, Hammadou, Stanek, 2011, Farvaque, Stanek, Vigeant, 2014, Gohlmann, Vaubel, 2007, Lebaron, Dogan, 2016).====However, the literature thus far has not considered the issue raised by Malmendier and Nagel (2011). If the “recession childhood effect” implies a lower degree of risk-taking, it may induce a reluctance towards “hawkish” policies and a bias towards the reduction of policy rates, to avoid recessions at any price. Hence, we test the “recession childhood effect”, using a discrete-choice modeling, which is more adapted to capture the effect, if it is found present. Discrete-choice models allow us to take into consideration the nature of the policy rate changes rather than the exact stance of this policy.====The study by Eichengreen et al. (1985) is among the first to use discrete-choice modeling in a monetary policy-making context, studying the Bank of England’ s discount rate policy in the interwar gold standard period. For more recent periods, Dueker (1999) considers the FOMC decisions, and measures the policy rate inertia, while Hamilton and Jorda (2002) focus on the size of rate changes for the same central bank. Hayo and Neuenkirch (2010) use this methodology to analyze the communication of the Fed, while Hu and Phillips (2004) also study the FOMC and, after controlling for non-stationarity, show that reactions to economic shocks by the FOMC are delayed (i.e., the Fed does not react immediately to a shock, but with a one-period lag). Among the studies that have used discrete choice modeling of monetary policy decisions for other (individual) central banks, Gerlach (2007) focused on the European Central Bank, Smales (2013) on the Reserve Bank of Australia, Torres and Shepherd (2013) on the Bank of Mexico, Hayo and Neuenkirch (2011) on the Bank of Canada, whereas Kim et al. (2015) analyzed South Korea. In a multi-country setting, Dolado et al. (2005) examined four central banks’ decisions, whereas Nojkovic and Petrovic (2015) investigated six Central and Eastern European monetary authorities. Hayo and Neuenkirch (2010) and Hayo and Neuenkirch (2011) also use a discrete choice model to predict interest rate changes in the US and Canada, respectively.====An analysis of interest rate setting in a discrete choice framework not only better reflects the reality of monetary decisions, but also allows us to assign probabilities of moves of the policy rates in different directions. This, in turn, means that explicit behavioral asymmetries that can be important if central bankers are “recession-averse” (Cukierman and Muscatelli, 2008), as they could be if the “recession baby effect” is present, are considered.====Thus, we adopt here a multinomial logit modeling of determinants of policy changes, with an emphasis on the leadership effects. Our main contribution is the study of the impact of the recession(s) experienced in the first 25 years of the chairpersons’==== lifetime on the policy behavior of central banks. Controlling for an analogical experience of the members of the committees they preside and other diverse characteristics of the policymakers, we hypothesize that they have significant consequences for individual and collective preferences.====Such approach aims at accounting for the role that recession aversion played in monetary policy decisions, before and during the financial crisis of 2008. Generally, strong leadership has to be built in non-recession times to allow the enforcement of otherwise hard-to-take decisions when necessary.====It has to be signaled, that, even though our period covers years that have been characterized by a Zero Interest Rate Policy (ZIRP), we, nevertheless, focus on interest rate changes for three related reasons. First, less than half of the central banks in our sample have approached the Zero Lower Bound (ZLB), and they obviously were not doing so before the crisis. This means that interest rate changes are a central policy tool for a large part of the decision-makers we observe, for most of the period under review. Second, even in a world where some central bankers have had to rely on quantitative measures, these were implemented as complementary instruments, and with an understanding that interest rate changes are the main tool of monetary policy, even though temporarily ineffective. Moreover, central bankers implementing quantitative easing have had to give some guidance about the (future) values of policy rates. Third, our modeling strategy is precisely aimed at taking into account the fact that most of central bankers’ decisions are, in fact, no-change decisions. This is even truer when policy rates hit the zero lower bound (ZLB), giving additional justification to our empirical approach.====In order to justify our behavioral hypotheses on theoretical grounds, we first derive a decision-making model of a committee headed by a potentially recession-averse chairperson. Results from this theoretical section emphasize the importance of such asymmetrical preferences in deciding over the policy rate, while the multinomial logit estimates reveal that, if the standard determinants (inflation, an inflation targeting framework, or the output gap) have an influence, leadership effects and central bankers’ backgrounds also have an impact on interest rate changes. There is a recession baby effect for central bankers, and we demonstrate that its size is significant, and policy-relevant. In other words, growing up in a recession influences central bankers similarly to other agents. Moreover, the results resist to several alternative or data-reduction hypotheses.====The remainder of the article is organized as follows: Section 2 introduces the theoretical model. Section 3 describes the data and methodology used, while Section 4 presents the results. Section 5 contains the conclusions and suggestions for further research.",Misplaced childhood: When recession children grow up as central bankers,https://www.sciencedirect.com/science/article/pii/S0165188919300752,15 May 2019,2019,Research Article,388.0
"Corgnet Brice,DeSantis Mark,Porter David","EMLYON Business School, GATE L-SE 69130 Ecully, France,Argyros School of Business and Economics, Economic Science Institute, Chapman University, Orange, CA, 92866, United States","Received 10 October 2018, Revised 31 January 2019, Accepted 5 February 2019, Available online 15 February 2019, Version of Record 17 January 2020.",https://doi.org/10.1016/j.jedc.2019.02.006,Cited by (13),"Apparently contradictory evidence has accumulated regarding the extent to which financial markets are informationally efficient. Shedding new light on this old debate, we show that differences in the distribution of ","Markets are essential to well-functioning economies. Yet, the debate regarding the efficiency of markets is still rampant. To shed light on this conundrum, we study whether differences in the structure of information between markets can account for the differences in their informational efficiency. In particular, we posit that the concentration of private information across market participants is a key determinant of the informational efficiency of a market.====Because it is impossible to clearly assess the informational efficiency of markets with archival data (Fama, 1991), we make the methodological choice to rely upon experimental markets. A growing number of studies have demonstrated the unique benefits of the experimental method in allowing researchers to control and thus clearly identify the extent to which private information is captured by market prices (e.g., Bossaerts, 2009, Frydman et al., 2014, Noussair and Tucker, 2014).====We extend the seminal work by Plott and Sunder (1988) where they find that with a complete set of Arrow Debreu spanning securities markets are efficient information gatherers. The structure of their environment provides a particular insight into how information is quickly and efficiently transmitted in the market. With spanning markets, one-half of the traders are ==== informed in all but one of the markets. Thus, these markets are populated by competing insiders who will reveal the states that cannot occur thus making the true asset value transparent to all traders.====We build on this work to assess whether the presence of insiders is indeed the driving force leading markets to aggregate private information. We consider market experiments in which the asset can take one of three possible values. In our markets, traders can either be ==== informed or ==== informed. The ==== trader only knows the prior distribution of the three possible asset values. The ==== informed trader is given one private ==== regarding the value (out of the three possible ones) the asset will not take. The ==== informed trader knows with certainty the value the asset will assume at the end of the market. If markets satisfy strong-form efficiency, then prices should reflect all the available private information regardless of the distribution of private information (Fama, 1970). It follows that the presence or absence of ==== informed traders will not affect prices as long as the aggregate information of all traders in the market is complete.====To derive conjectures regarding the informational efficiency of our experimental markets, we rely on a learning model following the work of Friedman (1991) and Copeland and Friedman, 1987, Copeland and Friedman, 1991). We extend their approach to the case in which a proportion of the traders may be boundedly rational. Following the work of Corgnet et al., 2015a, Corgnet et al., 2018 on the cognitive underpinnings of trading in experimental markets, we assume traders to be either ====, in which case they properly apply Bayes’ rule to infer the true asset value from market orders,==== or ==== in which case they exclusively rely on their private information to value the asset. Corgnet et al. (2015a) show that the distinction between ==== and ==== traders is crucial for understanding the informational efficiency of markets. This distinction echoes previous research in Finance regarding people's limited capacity to learn others’ private information from prices (e.g., see Eyster et al., 2019).====Using model-based simulations, we establish several conjectures regarding the informational efficiency of markets and learning dynamics. Keeping the number of private ==== constant across markets, the simulations indicate that informational efficiency should be higher in markets populated by both ==== informed and ==== traders than in markets solely populated by ==== informed traders. Our simulations show that ==== or ==== informed traders learn the true asset value substantially faster when ==== informed traders are present in the market. Intuitively, orders involving ==== informed traders carry substantially more information than orders involving ==== informed traders thus facilitating Bayesian inference and the transmission of private information into prices. ==== informed traders provide a clear signal of the asset's price relative to its true value as they follow a riskless strategy of buying (selling) when the price is below (above) the true value.==== Our simulations also predict that the efficiency of markets will increase and trading volumes will decrease as the number of ==== informed traders rises. Finally, our simulations also deliver a point prediction according to which a market populated with at least three ==== informed traders should reach a higher level of informational efficiency than a market populated entirely by ==== informed traders.====Our experimental findings provide clear-cut support for our conjectures including our point prediction. Our experimental results also support our model by showing that markets populated by a higher number of ==== traders (assessed using the cognitive reflection test, Frederick, 2005) reach higher levels of informational efficiency.",The distribution of information and the price efficiency of markets,https://www.sciencedirect.com/science/article/pii/S0165188919300314,15 February 2019,2019,Research Article,392.0
"Dupor Bill,Faria-e-Castro Miguel","Federal Reserve Bank of St. Louis, United States","Available online 26 February 2020, Version of Record 13 June 2020.",https://doi.org/10.1016/j.jedc.2020.103870,Cited by (0),None,"The increasing availability of micro data has allowed researchers to study and uncover new facts regarding important economic phenomena, as well as sharpen our understanding of long-standing questions. In particular, there has been growing interest in how the distribution of specific characteristics of individual economics agents — preferences, wealth, expectations, etc. — affect aggregate macroeconomic outcomes. Expanding computational power and the development of new computational techniques have also allowed researchers to leverage this increasing availability of micro data and study it in the context of models that can then be used to conduct policy analysis.====In October 2019, a group of leading macroeconomists gathered at the Study Center Gerzensee in Switzerland over two days to discuss and present their work, on this ongoing agenda of integrating microeconomic data in macroeconomics. These papers and discussions were discussed have been compiled in this special issue. The aim of the conference was to discuss the integration of micro data in macroeconomic models in a very broad way. For this reason, the conference featured a wide variety of papers, ranging from empirical to theoretical, but with heavy emphasis on quantitative and computational contributions.====This special issue of the Journal of Economic Dynamics and Control consists of formal papers that were prepared in advance and revised after the conference, as well as written discussions of each of these papers.====In “Trade Integration in Colombia: A Dynamic General Equilibrium Study with New Exporter Dynamics,” George Alessandria and Oscar Avila examine the experience of Columbia in the decades following that country’s 1989-1991 trade reforms. Through the lens of a general equilibrium model where firms enter into export markets and, afterwards, leave the market or attempt to improve their export technology, they examine how much of the observed growth in exports can be explained by various factors, such as declining tariffs and improved exporting technology. The model calibration and model evaluation are informed, in part, by firm-level data on exports both at the intensive and extensive margins. The authors also consider welfare implications of changing tariff policy and exporting technologies, as well as perform counterfactuals.====In recent years, qualitative survey data has become more useful in macroeconomics as an empirical tool. In “What Drives Aggregate Investment? Evidence from German Survey Data,” Rüdiger Bachmann and Peter Zorn study firm-level data from the German manufacturing industry. In that survey, firm managers are asked to categorize the primary determinants of their own investment decisions. Some of the factors include return expectations, technology, macroeconomic policy and sales. From the survey responses, which have both a time series and cross-sectional dimension, the authors construct index measures to quantify the importance of each explanatory determinant of investment. Moving beyond simply describing these indices, they use time-series methods to find the contribution of these shocks to explaining manufacturing investment growth. They show that in the shorter run, investment is explained by sales factors, whereas in the longer run investments are explained by technological factors. The authors then move their attention to regionally disaggregate data and find similar results.====In the introduction to his paper “Regional Data in Macroeconomics: Some Advice for Practitioners,” Gabriel Chodorow-Reich first observes that analysis of regional data has been become an important tool for macroeconomists. He then lays out several issues and questions that have arisen in this burgeoning literature. Through a variety of techniques, including Monte Carlo simulations and analysis of data from the 2009 Recovery Act, the author provides practical guidance on how to navigate these issues. As a first example, he looks at the role of weighted versus unweighted regressions in the presence of possible heterogeneous treatment effects. As a second example, he analyzes how violation of the Stable Unit Treatment Assumption across regions affects how one should interpret cross regional coefficient estimates.====In “Rising Bank Concentration,” Dean Corbae and Pablo D’Erasmo point out that concentration in the U.S. commercial banking industry has increased substantially in the last four decades. They point out that much of that rise in concentration followed the 1994 Riegle-Neal Act, which removed interstate bank branching restrictions. They use these motivating facts to build an industry dynamics model of the U.S. banking industry, where heterogeneous banks have some degree of market power in the lending market and differ primarily in the type of stochastic process that governs their deposit base. Banks can invest to grow in size and access a better deposit base (that grants larger and more stable quantities of funding). Interpreting the Riegle-Neal Act as a reduction in the cost of this investment, they find that a transition to such a new steady state results in rising concentration, as observed in the data.====Understanding the monetary transmission mechanism has been one question for which disaggregate data has become increasingly important. One approach has used analytical solutions for consumers’ problems by applying a set of simplifying assumptions. Following this approach, Jiri Slacalek, Oreste Tristani and Giovanni Violante, in “Household Balance Sheet Channels of Monetary Policy: A Back of the Envelope Calculation for the Euro Area,” apply these simplifying assumptions to compute equilibrium consumption functions for three types of households: poor hand to mouth (HtM), wealthy HtM, and non-HtM. Next, using data from four European countries, they use micro data to group households into the three types. For each country, they next estimate structural VAR responses to a monetary policy shock and then trace out each group’s responses of variables (e.g., consumption) to the shock. The authors then use the analytical results to decompose these responses into various channels, such as net interest rate exposure, the labor income channel and wealth channel. They also explore how differences across the relative importance of these channels can explain differences in responses across the four countries.====In “Estimating Linearized Heterogeneous Agent Models Using Panel Data,” Tamas Papp and Michael Reiter make two computational contributions to the literature on estimating heterogeneous agents models (HAM). First, they show how to incorporate micro data, in either repeated cross sections or panels, to estimate model parameters, such as those that govern the persistence and volatility of idiosyncratic shocks. They build on the previous work of Reiter, who shows that HAM can be solved in an aggregate linearized representation that allows for multiple aggregate shocks. This literature has expanded and developed techniques that allow for the estimation of aggregate shock parameters using linear state space techniques. The authors contribute to this literature by showing how to use micro data to estimate idiosyncratic shock parameters. Second, they recognize that micro data is typically available at a lower frequency than aggregate data (which is typically quarterly) and offer solutions for how to address this issue.====In “Capital Income Taxation with Housing”, Makoto Nakajima argues that any discussion of optimal capital income taxation must take into account housing, since it is a significant component of total household wealth and subject to special tax treatment in most countries. He extends a workhorse overlapping-generations model with uninsurable idiosyncratic shocks to labor productivity to account for housing as an asset that is distinct from physical capital and subject to a realistic tax treatment, which includes property taxes and mortgage interest payment deductions. Since physical capital and housing are substitutable savings vehicles, the taxation regime of one asset affects the equilibrium effects of taxation of the other. Nakajima proceeds to calibrate the model to the U.S. and run a series of policy experiments, where we compares social welfare measures for alternative tax systems. Taking the current U.S. housing tax policy as a given, he finds the the optimal capital income tax rate should be close to zero due to the preferential treatment of housing that causes overinvestment in this sector.====In “Labor Market Trends and the Changing Value of Time,” Job Boerma and Loukas Karabarbounis study diverging trends in wages, hours worked, and expenditures across households in the last two decades. To make sense of the data, they develop an incomplete markets model where households are heterogeneous with respect to their preferences for market- and home-produced goods, as well as with respect to their individual productivities of market and home goods. Boerma and Karabarbounis develop a series of results that allows them to use the equilibrium of the calibrated model as a powerful measurement device. Micro data on consumption and time use for the U.S. allows them to use the calibrated model to back out trends in unobservable components of heterogeneity, such as individual productivity and preferences. Their key finding is that the productivity of leisure time inputs has increased substantially over time; this has led to an increase in mean consumption that has resulted in substantial welfare gains and a reduction in dispersion of consumption across households.====We close with some acknowledgements. First, we thank our fellow members of the conference organizing committee, all of the conference participants and the sponsoring organizations. Dirk Niepelt was instrumental in helping us organize the conference and in ensuring flawless management of conference logistics. Elizabeth Wileman at Elsevier did an excellent job seeing the conference proceedings to publication. Finally, we thank the Study Center Gerzensee and its staff for providing an ideal location and hospitable environment for the meeting.====We sincerely hope that readers appreciate the contributions and respective discussions and consider them insightful in understanding this ongoing research agenda.",Introduction for the special issue on the Gerzensee conference,https://www.sciencedirect.com/science/article/pii/S0165188920300397,26 February 2020,2020,Research Article,400.0
"Dawid Herbert,Hanaki Nobuyuki,Tuinstra Jan","Chair for Economic Theory and Computational Economics, Bielefeld University, Germany,Institute of Social and Economic Research, Osaka University, Japan,CeNDEF, Amsterdam School of Economics, University of Amsterdam, the Netherlands","Available online 20 November 2019, Version of Record 17 January 2020.",https://doi.org/10.1016/j.jedc.2019.103806,Cited by (0),None,None,Introduction for the special issue on “Experimental and behavioral analyses in macroeconomics and finance”,https://www.sciencedirect.com/science/article/pii/S0165188919302039,20 November 2019,2019,Research Article,403.0
"Barletta Andrea,Santucci de Magistris Paolo,Sloth David","Department of Economics and Business Economics, Aarhus University, Fuglesangs Alle 4, 8210, Aarhus, Denmark,Department of Economics and Finance, LUISS “Guido Carli” University, Viale Romania 32, 00197 Roma, Italy,CREATES, Aarhus University, Fuglesangs Alle 4, 8210, Aarhus, Denmark,Danske Bank A/S, Holmens Kanal 2-12, 1092, Copenhagen, Denmark","Received 30 May 2018, Revised 13 September 2018, Accepted 20 November 2018, Available online 11 January 2019, Version of Record 1 February 2019.",https://doi.org/10.1016/j.jedc.2018.11.008,Cited by (1),"We propose a novel non-structural method for hedging European options, relying on two model-independent results: First, under suitable regularity conditions, an option price can be disentangled into a ==== of risk-neutral moments. Second, there exists an explicit approximate functional form linking the risk-neutral moments to the futures price of the underlying asset and the related variance swap contracts. We show that S&P 500 call prices are mainly explained by two factors that are related to level and volatility of the underlying index. We empirically compare the performance of two strategies where the vega exposure is adjusted either by a direct position in a variance swap contract or, indirectly, through an at-the-money call. While both strategies ensure effective immunization in periods of market turmoil, taking direct exposure on variance swaps is not optimal during extended periods of subdued volatility.","Option contracts are known to facilitate the efficient transmission of risk in the market, enabling traders to run balanced portfolios in terms of profit and loss uncertainty. The uncertainty of an option payoff can be hedged by taking offsetting positions in one or several securities deemed to be driving its risk. In practice, traders compute hedge ratios in terms of the so-called ====, with the ==== and ==== being the most prominent examples. The delta and the vega are the first-order sensitivities of an option price to changes in the underlying asset price and its volatility, respectively. Unfortunately, the Greeks are not observable quantities and the method chosen for their calculation has a crucial impact on the effectiveness of hedging.====To compute the Greeks, traders tend to resort to relatively simple models with a “layer of heuristics” on top. For example, a standard practice to determine the delta of a given vanilla option is to compute the Black–Scholes delta with the implied volatility of the option as volatility parameter. This ad-hoc usage of the Black–Scholes pricing formula is commonly referred to as the “practitioners’ Black–Scholes” (PBS) model, see Christoffersen and Jacobs (2004) and Hull and White (2017). A somewhat similar generalization of the Black–Scholes valuation model is the local volatility model of Dupire (1994), where volatility is a deterministic function of the underlying level and it is uniquely determined by the implied volatility smile. El Karoui et al. (1998) show that the performance of delta hedging through a local volatility model depends on how precisely the “true volatility” is tracked by the model. There is no clear consensus on which specification, between PBS and local volatility, provides the best delta hedging performance, see e.g. Dumas et al. (1998) and Crépey (2004). However, both approaches inherently treat the implied volatility of each option as an individual source of risk. Thus, to adjust the vega exposure of a bucket of options with a fixed maturity, one has to construct a hedging portfolio that is immune to changes in any point of the volatility smile. This, coupled with the fact that the implied volatility itself is not traded, makes vega hedging all but an easy task.====In view of these limitations, the hedging of volatility risk is typically accomplished by amending the standard PBS delta position, for instance by minimizing the variance of the hedging error, see among others Badescu et al. (2014) and Hull and White (2017). This approach neatly improves the performance of the PBS model in terms of hedging volatility risk, although no direct position on volatility is actually taken. Nowadays, however, traders are offered the opportunity to achieve vega hedging more directly, since volatility has become a fully-fledged asset class thanks to the availability of traded instruments such as variance swaps and VIX derivatives. This is endorsed by several studies suggesting that delta-vega hedging is possible by resorting to only two or three risk factors. Adhering to this principle, stochastic volatility models assume that all the randomness driving the implied volatility surface is embedded in the underlying asset and its instantaneous variance. Many authors have dealt with hedging under stochastic volatility, see for instance Bakshi et al. (1997), Gondzio et al. (2003) and Kaeck (2012). Although the class of stochastic volatility models is widely adopted in the option pricing and hedging literature, it should be noted that any fully parametric pricing model carries a certain amount of ==== (Cont, 2006), which may lead to biased computation of hedge ratios, see e.g. Bakshi et al. (2000) and Alexander et al. (2009). Among all sources of model risk, of particular importance is the risk arising from possible model misspecification. Branger et al. (2012) and Kaeck (2012) note that, although the specific functional form of the variance in a stochastic volatility model has little effect on pure delta hedging, the risk of model misspecification may markedly affect the performance of more complex strategies, such as delta-vega hedging.====In the present paper, we propose a new method for computing hedge ratios of European options by only assuming the existence of a risk-neutral density fulfilling mild regularity conditions and avoiding strong assumptions on the dynamics of the underlying process. For this reason, and following the classification of Jondeau et al. (2007, Chapter 11), our methodology belongs to the class of ==== methods. By very definition, a non-structural approach limits the aforementioned risk of model misspecification by a large extent. Over the years, many non-structural hedging techniques have been proposed in the literature. Hutchinson et al. (1994) propose a non-parametric method to estimate delta hedge ratios based on learning networks. Alcock and Gray (2005) derive non-parametric delta hedge formulas based on the canonical valuation approach of Stutzer (1996). Tebaldi (2005) proposes a non-parametric approach to derivative hedging based on least squares regression. Bates (2005) and Alexander and Nogueira (2007) derive model-independent formulas for the delta and gamma of a European option, which can be evaluated through the option sensitivities to the strike price. Finally, Branger and Mahayni (2006) develop robust hedging strategies that do not depend on the exact specification of the stochastic volatility process. A recent comprehensive review of non-structural approaches to hedging is in Davis (2016).====This paper contributes to the existing literature by developing a new non-structural hedging methodology along the following lines: First, we reinterpret a classic non-structural pricing technique as a novel general method to compute hedge ratios of European-style derivatives. Relying on the classic theory of orthogonal polynomials, an option price can be disentangled into a linear combination of risk-neutral moments, see Jarrow and Rudd (1982), Coutant et al. (2001) and Jondeau and Rockinger (2001) for classic references, and Xiu (2014) and Schneider (2015) for recent applications. Second, we derive a model-independent relation between the risk-neutral moments, the underlying futures price, and the associated variance swap. This allows for explicit computation of the risk-neutral moments sensitivities to changes in the risk factors and, therefore, of delta and (variance swap) vega hedge ratios. In our empirical analysis we consider a panel of S&P 500 options and we show that two components associated with futures and variance swaps are the relevant factors to be hedged. Moreover, we empirically compare the performance of two different non-structural hedging strategies, where the vega exposure is adjusted either by taking a direct position on a variance swap contract or, indirectly, through an at-the-money (ATM) call option. We find that both the direct and indirect strategies provide effective immunization during market turmoil, whereas ATM options prove more reliable hedging factors than variance swaps in persistent state of low volatility. Finally, we show that the non-structural delta and vega ratios provide better hedging than the corresponding Greeks based on the PBS model.====This manuscript is organized as follows. Section 2 lays down the general non-structural hedging methodology constituting the backbone of the paper. Section 3 provides numerical illustrations supporting the validity of the proposed methodology. Section 4 discusses the empirical validity of the assumptions underlying the methodology. Section 5 discusses the performance of two non-structural hedging strategies based on real market data on real market data. Section 6 concludes the paper. The appendix contains further details on the implementation of the methodology and additional empirical results.",It only takes a few moments to hedge options,https://www.sciencedirect.com/science/article/pii/S0165188919300077,March 2019,2019,Research Article,404.0
"Mazzarisi Piero,Lillo Fabrizio,Marmi Stefano","Scuola Normale Superiore, Pisa, Italy,Department of Mathematics, University of Bologna, Italy","Received 3 May 2018, Revised 13 December 2018, Accepted 30 December 2018, Available online 11 January 2019, Version of Record 23 January 2019.",https://doi.org/10.1016/j.jedc.2018.12.009,Cited by (2)," of risk, we show that the evolution of the system is described by a slow-fast ","Borrowing is an essential aspect of the business of financial institutions operating in financial markets because of the possibility of leveraging the investment returns by buying on margin. The recent empirical literature (Adrian and Shin, 2010) captured perfectly this aspect characterizing the leverage management of the largest investors operating in the market. But financial leverage is directly related to risk. As several recent papers suggest (Aymanns, Farmer, 2015, Brunnermeier, Pedersen, 2009, Corsi, Marmi, Lillo, 2016, Geanakoplos, 2010), financial leverage is probably the most important engine in driving endogenously the booms and the busts of asset prices in the market.====Regulators try to limit the exacerbated use of leverage by imposing some constraints to financial institutions in order to make the financial system more robust and resilient to shocks. Value-at-Risk (VaR) constraint is probably the most popular one, but other more sophisticated ones have been proposed in recent years by the successive Basel regulations. All the regulators’ constraints require an estimation of the riskiness of the investments as well as of the dependencies between extreme events of financial returns. Therefore both the capital requirement constraint and the risk/dependency expectations play a crucial role in determining the systemic stability of financial markets.====On one hand, as documented by many papers (see for example Adrian and Shin, 2010), VaR capital requirements, as other risk constraints, can induce a perverse demand function: in order to target leverage, a financial institution will sell more assets if their price drops and viceversa when their price rises. Thus, a marked-to-market and VaR constrained financial institution will have a positive feedback effect on the prices of the assets in its portfolio. This fact is known as ==== of the Value-at-Risk constraint which results in amplifications of market movements, especially during the falling period of a financial crisis. Recently, solutions have been suggested to overcome the problem of procyclicality of the VaR constraint, ==== Aymanns et al. (2016) proposed countercyclical policies to control the dynamics of the leverage. Furthermore, in a market where many financial institutions are forced to follow similar risk management rules, the coordinated rebalancing of portfolios composed by illiquid assets creates a feedback effect which depends on the degree of leverage and diversification. Thus, while diversification of investment should reduce portfolio risk, a significant overlap (==== similarity) of portfolios of many financial institutions can instead destabilize the market and increase its susceptibility to price shocks because of fire sale spillovers, ==== when asset sales depress prices, in which case one institution’s sales impact other institutions with common exposures. Several models of fire sale spillovers have been proposed to evaluate the vulnerability of institutions (Greenwood et al., 2015) as well as to construct indexes of systemic risk in financial markets (Di Gangi, Lillo, Pirino, 2018, Duarte, Eisenbach).====On the other hand, the implementation of any capital requirement depends on the expectations financial institutions have on the risk of the assets in the portfolio and on their statistical dependence. For this reason there is a vast literature on the estimation of risk and dependencies (Bao, Duffy, Hommes, 2013, Heemeijer, Hommes, Sonnemans, Tuinstra, 2009, Hommes, Sonnemans, Tuinstra, Van De Velden, 2007, Tsay, 2005), many of them based on the recent history of prices in a time window of the recent past. The choice of the length of the estimation window is critical, since there is a tradeoff between choosing a long estimation window to improve statistical significance and preferring a short window in order to capture a more timely measure of risk. In period of financial turbulence, when non stationary effects are more likely, investors might prefer to use short estimation windows. Since trading decisions drive endogenously the market in the presence of illiquid assets and depend on expectations, the length of the risk estimation window can impact the dynamical properties of prices. This creates a second feedback effect in addition to the one, described above, due to target leveraging.====In this paper we present an analytical model of the financial system where both feedbacks mechanisms are present. Building on Corsi et al. (2016), a set of financial institutions (banks) investing in a portfolio of risky and illiquid assets follow a target leveraging strategy to satisfy Value-at-Risk capital requirements. The estimations of risk of the investment assets, and as a consequence the leverage, are periodically updated and banks use a backward-looking expectation scheme which considers price returns in a past time window to build estimates. The two feedback mechanisms are coupled by the price dynamics, which on one side is used to mark-to-market the portfolio and to estimate risk and correlations, and on the other one is endogenously affected by the trading activity of financial institutions.====Interestingly, the two feedback mechanisms described above act on different time scales. In our model the time scale of leverage targeting is shorter than the time scale over which financial institutions update their risk expectations. This separation of time scales is crucial in our modeling. Since the slow variables, associated with updates of risk expectations, evolve in time as a function of averages over the fast variables, associated with leverage targeting, our model can be casted as a discrete time slow-fast dynamical system.==== The ratio between the two time scales is the key parameter determining the type of mathematical modeling. We show that when this ratio tends to infinity, ==== financial institutions are continuously marked-to-market, the dynamics is described by a deterministic map. The window used to form expectations of risk plays a central role in determining systemic stability and leverage cycles appear when investors become more myopic relative to past history of asset prices, ==== the memory becomes smaller than a given threshold. Our model predicts that the deterministic dynamics of the financial system becomes chaotic when the memory decreases further and goes below a second smaller threshold. When the ratio between the two time scales is finite a random slow-fast dynamical system describes the system. Even if mathematically this is harder to study, because of the joint chaotic and stochastic dynamics, we show by analytical arguments and numerical simulations that the main dynamical characteristics remain unchanged.====We are therefore able to characterize the possible dynamical outcomes for the considered financial system as a function of the memory window used to form expectations, the tail parameter of the Value-at-Risk, the number of asset classes available for investment, the ratio between the two time scales (related to the presence of market frictions), and a parameter determining the level of financial innovation. We show how the breaking of the fixed point equilibrium for the financial system occurs via a period-doubling bifurcation when any of these parameters are varied and how the dynamics of the financial system may be intrinsically chaotic in certain parameter regions. Each of these parameters can at least in part be controlled by regulators, thus our model is able to provide policy recommendation for enhancing financial stability, as discussed at the end of the paper.==== Our paper aims to combine several streams of literature: (i) the analysis of portfolio rebalancing induced by the mark-to-market accounting rules and VaR constraint (Adrian, Shin, 2010, Adrian, Shin, 2013); (ii) the investigations on the impact of the imposition of capital requirements on the behavior of financial institutions and their possible procyclical effects (Adrian, Shin, 2013, Corsi, Marmi, Lillo, 2016, Danielsson, Shin, Zigrand, 2004, Danielsson, Shin, Zigrand, 2012, Tasca, Battiston, 2016); in particular, we generalize the model of Corsi et al. by studying how the procyclical effects on asset prices and risk expectations influence the portfolio decisions about leverage and diversification; (iii) the literature on distressed selling and its impact on the balance sheets of the financial institutions because of the overlapping among portfolios (Caccioli, Shrestha, Moore, Farmer, 2014, Cont, Wagalath, 2013); (iv) the research on the role of expectation feedbacks in financial systems (Farmer, Gallegati, Hommes, Kirman, Ormerod, Cincotti, Sanchez, Helbing, 2012, Hommes, 2000, Hommes, 2013, Hommes, 1994); in particular, our paper focuses on the role of risk expectations which are formed by using statistical models of past observations of investment prices; (v) finally our paper contributes to literature on the application of dynamical systems theory to the problem of systemic risk in financial markets (Castellacci, Choi, 2015, Choi, Douady, 2012) and, specifically, to the study of the dynamics of leverage cycles and its relation with the financial regulation (Aymanns, Caccioli, Farmer, Tan, 2016, Aymanns, Farmer, 2015, Brunnermeier, Pedersen, 2009, Geanakoplos, 2010, Halling, Yu, Zechner, 2016, Poledna, Thurner, Farmer, Geanakoplos, 2014).==== The remainder of this paper is organized as follows. In Section 2 we briefly review the model of Corsi et al. (2016) which represents the background of our study. Then, we introduce the model with backward-looking risk expectations. In Section 3 we briefly summarize the results of Corsi et al. (2016) which represents the limit case of our model without backward-looking expectations. In Section 4 we analyze the dynamical properties of the model and its policy implications for financial markets in the limit in which the model is fully analytical. This limit corresponds to study the deterministic skeleton of the slow-fast random dynamical system. In Section 5, we present an analytical argument to extend the obtained results also in the random framework, we give more insights into the model via numerical simulations, and we discuss the role of taxation for financial systemic risk. Section 6 contains some conclusions.",When panic makes you blind: A chaotic route to systemic risk,https://www.sciencedirect.com/science/article/pii/S0165188919300053,March 2019,2019,Research Article,405.0
"Umezuki Yosuke,Yokoo Masanori","Graduate School of Humanities and Social Sciences, Okayama University, Tsushimanaka 3-1-1, Kita-ku, Okayama 700-8530, Japan,Graduate School of Humanities and Social Sciences and Faculty of Economics, Okayama University, Tsushimanaka 3-1-1, Kita-ku, Okayama 700-8530, Japan","Received 13 February 2018, Revised 1 November 2018, Accepted 19 November 2018, Available online 11 January 2019, Version of Record 23 January 2019.",https://doi.org/10.1016/j.jedc.2018.11.006,Cited by (6),"In this study, we develop a simple, computable overlapping generations model that exhibits endogenous fluctuations. The key assumption is that a firm can choose from multiple technologies of production. Since the model reduces to a piecewise linear map on the unit interval, it allows us to conduct an in-depth analysis of its dynamic properties. Particularly, this piecewise linearization reveals the ability of the model to exhibit periodic attracting cycles of an arbitrarily large period as well as non-periodic attractors. Furthermore, it is demonstrated that the occurrence of periodic patterns is completely characterized by the rotation number or the “devil’s staircase.”","Apparently, the overlapping generations (OLG, hereafter) model is one of the most popular dynamic economic models in the literature. It has been widely used in many fields of economics. Especially, the OLG setting has been used as a building block for ==== models, as is used in this study. The endogenous growth cycle theory argues that the internal economic excitement, rather than the exogenous shocks, is responsible for the perpetual fluctuations of the economy. A few prominent examples of chaotic (i.e., random-looking but deterministic) economic dynamics in an OLG model, in the early literature, can be found in the studies by Benhabib and Day (1982) and Grandmont (1985).====An OLG model used for demonstrating endogenous fluctuations often contains additional assumptions that make its functional form less tractable. For example, the existence of indeterminacy was evident in the aforementioned model studied by Grandmont (1985). Michel and de la (2000) and Chen et al. (2008) demonstrated that an OLG model with myopic expectations can exhibit chaotic behavior when the utility function and/or production function has a constant elasticity of substitution (CES) form.====The study aims to develop an explicit OLG model that is tractable and has rich dynamics that would allow the endogenous cycles to emerge in a deterministic way. In fact, the model provided in this study comprises only of the Cobb-Douglas utility ==== Cobb-Douglas production technologies. Furthermore, unlike Michel and de la (2000) and Chen et al. (2008), we do not assume imperfect foresight, such as set myopic expectations, or employ complicated learning mechanisms to achieve rich dynamic results for the model.====Instead, we assume that, unlike in ordinary textbook-type OLG models with production, a representative firm (or its owner) can choose from multiple production technologies.==== In other words, the firm faces a discrete choice problem==== before it commences the production of a good. Through this simple case, the study reveals that the firm faces a binary choice problem that results in the emergence of endogenous cycles induced by strong nonlinearity. Our model reduces into a simple, first-order piecewise linear difference equation with one endogenous discontinuity, which allows us to provide an elaborate characterization of the periodic and non-periodic dynamics of the model.====However, it must be noted that the relationship between discrete choice and complex dynamics has been discussed in different contexts in economics. Ishida and Yokoo (2004) developed a macroeconomic model wherein newly entered firms choose whether to invest in a time-consuming project, giving rise to a piecewise linear dynamic model that can generate periodic cycles of any arbitrarily large period. Matsuyama (2007) presented an OLG-type growth model with an endogenous technology switch that is induced by financial imperfections; he showed that the model has the ability to exhibit several dynamic growth patterns including perpetual fluctuations. Asano et al. (2012) focused on a special case of the growth model of Matsuyama (2007) and showed that the piecewise-linearized model can exhibit periodic and non-periodic fluctuations. The model presented in this study is constructed along the lines of the study by Matsuyama (2007) in terms of the discrete choice. However, our model is simpler than Matsuyama’s model in terms of both the story behind the modeling and the functional form of the model. Prior to this study by Matsuyama (2007), Iwaisako (2002) study presented an OLG model with a technology choice; by using a graphical argument, Iwaisako suggested the possible occurrence of several growth patterns including cycles. Iwaisako (2002) investigated a situation in which investors choose one of the two technologies: constant returns to scale and increasing returns to scale. On the other hand, the present study assumes that all the available technologies are in the form of the Cobb–Douglas production function. In this sense, the model presented in this study is simpler than Iwaisako’s model.====Despite the simplicity of our model, results derived from the analysis of the model seem to be sharper than the earlier related works in terms of the characterization of dynamic patterns. A piecewise-linearization, similar to Asano et al. (2012), helps us to directly apply some useful results borrowed from the studies on the mathematical neuron models to our model;==== the employment of this method helps us to conduct a detailed characterization of the cyclical regime-switching patterns arising in our model.====Particularly, we show that the occurrence of a periodic attractor can be completely characterized by the ==== or its visualization, referred to as the devil’s staircase, which is the graph of a function that is monotonic but flat almost everywhere. Furthermore, we also show that the set of parameter values for which the non-periodic attractors appear is very small. In other words, the non-periodic motions in our model cannot be virtually observed.====The organization of the paper is as follows. Section 2 derives the piecewise linear model by incorporating the technology choice into the double Cobb-Douglas OLG model. Section 3 shows that the model can exhibit periodic as well as non-periodic fluctuations. Section 4 investigates the relationship between the parameters and the periodic patterns by introducing the rotation number. Section 5 concludes the study. Some mathematical proofs are delegated to appendices.",A simple model of growth cycles with technology choice,https://www.sciencedirect.com/science/article/pii/S016518891930003X,March 2019,2019,Research Article,406.0
"Bielagk Jana,Horst Ulrich,Moreno-Bromberg Santiago","Department of Mathematics, Humboldt-University Berlin, Unter den Linden 6, Berlin 10099, Germany,Center for Finance and Insurance, University of Zurich, Andreasstrasse 15, Zurich 8050, Switzerland","Received 17 November 2017, Revised 12 September 2018, Accepted 16 September 2018, Available online 9 January 2019, Version of Record 21 January 2019.",https://doi.org/10.1016/j.jedc.2018.09.009,Cited by (4),"We use a model with agency frictions to analyze the structure of a ==== that faces competition from a ====. Traders are privately informed about their ==== (e.g. their portfolios), which is something the dealer must take into account when engaging his counterparties. Instead of participating in the dealer market, the traders may take their business to a crossing network. The dealer must take into consideration that traders have this alternative when choosing a pricing schedule. We show that the presence of a crossing network may benefit traders even if they do not trade in it. Furthermore, it results in more traders being serviced by the dealer and the book’s ==== shrinking (under certain conditions). We allow for the pricing on the dealer market to determine the structure of the crossing network, which itself influences the structure of the dealer market. This results in a feedback look that, under the same conditions that lead to a reduction of the spread, yields an equilibrium book/crossing network pair.","In traditional dealer markets (DM for short), liquidating large portfolios may lead to an unfavorable price impact. In response to this problem, alternative trading venues such as crossing networks (CNs for short), in which no price generation takes place but trading is opaque, have been established.==== These two types of venues compete for liquidity, which leads to the question of how the prices and traded volumes in DMs are affected by the emergence of CNs.==== Moreover, prices in a CN are a function of the prices in the competing DM, which in turn depend on the volume traded in the CN. In this paper we analyze the interplay between DMs and CNs within a principal-agent framework with private information. The principal represents a monopolistic DM run by a profit-maximizing dealer. The agents correspond to traders who choose between engaging the dealer, trading the CN or abstain from trading all together. They trade for liquidity reasons, are privately informed about their inventories and have (possibly idiosyncratic) beliefs about the probabilities of trade execution in the CN. The price in the CN depends in a pre-specified manner on the price schedule offered by the dealer and, simultaneously, it determines the traders’ outside options. Within our model the CN leads to a positive externality on agents trading in the DM: independently of the distribution of types, a trader may benefit from the CN even if he takes his business to the DM. For the benchmark case of uniformly distributed types, we find that if the CN benefits only agents with larger inventories, then, in equilibrium, its introduction shrinks the spread in the DM. These effects can already be inferred from the linear adverse selection model of Mussa and Rosen (1978), where both the spread and the lowest type serviced are linearly increasing functions of the highest agent type (the one with the highest inventory in our setting). When high-type traders take their business to the CN, the highest type transacting with the dealer decreases. As a result, the spread shrinks and the dealer increases the number of low-type traders he services. In particular, more types earn positive rents. On the other hand, we show via a simple example that if the CN benefits “small traders” then the spread may widen in equilibrium.==== This effect may be avoided if access to the CN is costly.====There are basically two branches of the literature that deal with issues related to optimal simultaneous trading in DMs and CNs by means of almost orthogonal approaches. On the one hand, starting with the contributions of Almgren and Chriss (2001) and Obizhaeva and Wang (2013), the mathematical-finance literature has extensively analyzed models of optimal trading under market impact in recent years. In this literature, it is typically assumed that trading is liquidity-driven and that it takes place under an exogenous, non-linear pricing schedule in the DM. Horst and Naujokat (2014) and Kratz and Schöneborn (2015) were the first to allow orders to be simultaneously submitted to a DM and a CN. In their models, arbitrary amounts can be submitted to CNs; order executions in CNs are exogenous. The assumption of exogenous order execution in a CN seems reasonable when trading is liquidity-driven. However, it is undesirable that there is no feedback from CN trading to the price setting in the DM. We extend the mathematical-finance literature on simultaneous trading in DMs and CNs by allowing for an impact of off-exchange trading on the prices in an associated monopolistic DM and vice versa. Since our focus is the nonlinear pricing (i.e. market impact) in the DM and not the matching mechanism in the competing CN, we do not model the latter explicitly but assume instead that the traders act on their (possibly private) beliefs about the probability of order execution.====On the other hand, the financial-economics literature is rife with equilibrium models analyzing the impact of alternative trading venues on DMs and trading behavior. For example, Parlour and Seppi (2003), study competition for order flow between heterogeneous exchanges and establish that different trading architectures, such as pure limit-order and hybrid specialist/limit-order markets, can be supported in equilibrium. Glosten (1994) studies electronic limit-order books and the characteristics that other trading venues should have to successfully compete with an electronic exchange. Degryse et al. (2009) build a dynamic model of a dark pool and analyze how various transparency requirements for dark-pool orders affect traders’ behavior and welfare. Daniëls et al. (2013) investigate the allocation of order flow between a DM and a CN and show how differences in traders’ liquidity preferences generate a unique equilibrium in which patient traders use the CN while impatient traders submit orders directly to the DM. Buti et al. (2019) model the competition between an open limit order book and a dark pool, and focus on the interaction between dark-pool trading and characteristics of the limit order book. To simplify the analysis of market impact, this literature typically assumes that the market participants trade only a single unit of the stock. This is the main point of divergence with our model: whereas we allow for differently-sized trades but do not endogenize matching probabilities, most if not all of this literature targets the equilibrium workings of the off-exchange venues at the cost of only considering unitary trades. In particular, pricing rules are linear. For instance, in their seminal paper, Hendershott and Mendelson (2000) use a setup where multiple dealers play a Bertrand game against each other, but are passive in equilibrium. Information asymmetry corresponds to private information about the common value of the asset, which may be short or long-lived (in the sense that it may be used sequentially) and not all traders have. All traders may first submit unitary orders to the CN and, if unexecuted, may then move on to the DMs. The timing of the actions of uninformed traders depend on an exogenously-given impatience factor. This, together with the duration of private information, determines the number of trading counterparties in the CN; thus, the probability of execution and, ultimately, the equilibrium spread. The authors identify two counteracting effects of larger trading volumes in the CN. On the one hand, a ====: a higher trading volume in the CN increases liquidity, which benefits all traders. On the other hand, a ====: low- and high-liquidity preference trades may compete against each other on the same side of the market. We do not find the aforementioned probability endogenously, as we do not model the matching mechanism in the CN explicitly. Instead, we assume each trader computes her expected utility of trading in each venue and then chooses which one to use. This takes into account the possibility of finding a match in the CN, as well as the price at which trades are executed there. The said expected utilities contemplate trading multiple units of the stock according to possibly non-linear pricing rules.====The remainder of this paper is organized as follows: First, in the spirit of Biais et al. (2000), we formulate the dealer’s optimization problem for given execution prices in the CN. We assume each trader computes her expected utility of trading in the different venues and then chooses which one to use. This takes into account the possibility of finding a match in the CN, as well as the price at which unitary units are traded there. The dealer’s objective is then to devise a pricing schedule so as to maximize his expected profits from trading, roughly defined as the gains from trading certain positions net of the associated costs. We show that this problem has a unique solution on the set of traders participating in the DM.====Second, we study the qualitative influence of a CN on an existing market, as introducing a CN adds constraints to the dealer’s optimization problem. We gauge the impact of these constraints using the Lagrangian techniques introduced in Jullien (2003). This allows us to identify traders who defect the DM and those who previously did not trade in it but, by virtue of more appealing conditions, now engage the dealer. As a consequence, we fully describe the DM via a non-linear pricing rule and determine the volume of trading in each venue. We prove that, under certain conditions, the presence of the CN results in a positive externality on agents trading in the DM: more traders earn positive rents and the spread in the DM shrinks. We illustrate these results by means of several examples with and without a CN.====Finally, having understood the dealer’s problem with exogenous CN prices, we proceed to show the existence of equilibrium prices. The first step is to specify a ==== via which prices in the DM determine those in the CN. Given an exogenous sell/buy price pair ==== in the CN, the dealer optimally chooses his pricing schedule, which will most likely not induce ==== through the price generation mechanism. We identify conditions under which this iterative procedure converges to a fixed point: an equilibrium price schedule is such that the optimal reaction of the dealer to the prices ==== in the CN induces again ====.==== Our study of such a feedback loop is novel and it is a crucial component in our analysis of the interactions between DMs and CNs, which is typically not unidirectional. As an application we consider a problem of optimal portfolio liquidation where traders can chose between a DM and a CN (in this particular case a dark pool). We show the presence of the DP leads to a shrinkage of the spread and prove the existence of an equilibrium price.====Summarizing, our main results are: (====) identifying sufficient conditions for the spread in the DM to shrink in the presence of a CN, and (====) identifying sufficient conditions for the existence of equilibrium prices. We conclude this introduction with a statement from Section 7.3 in Gomber et al. (2017), which coincides with our findings:",Trading under market impact: Crossing networks interacting with dealer markets,https://www.sciencedirect.com/science/article/pii/S0165188919300016,March 2019,2019,Research Article,407.0
"Gallegati Marco,Giri Federico,Palestrini Antonio","Università Politecnica delle Marche, Piazzale Martelli, n. 8, Ancona, Italy","Received 3 October 2017, Revised 4 October 2018, Accepted 15 October 2018, Available online 7 January 2019, Version of Record 22 January 2019.",https://doi.org/10.1016/j.jedc.2018.10.004,Cited by (9),We estimate a ,"DSGE models are currently the most widely used framework for quantitative business cycle analysis both in academia and policy institutions. Among the many open questions in DSGE models, the limited improvement in the estimation performance of financial friction frameworks over the frictionless benchmark in modeling business cycle dynamics is one of the most puzzling (e.g. Brzoza-Brzezina and Kolasa, 2013 and Christiano et al., 2018). As an attempt to expand the analysis on the importance of frictions originating in financial markets, this paper suggests a different approach to the development of alternative more sophisticated frameworks. Following the recent contributions estimating DSGE model over different frequency bands==== in this paper we propose the estimation of a standard DSGE model with nominal and financial rigidities a la Gerali et al. (2010) over different frequencies. In particular, in the light of the difficulties facing alternative approaches to fully explain all the observed regularities in the data it may be instructive splitting business cycle frequencies into different subsets of frequencies corresponding to higher (1–4 years) and lower (4–8 years) business cycle frequency bands.====The motivation for separating business cycle fluctuations into subsets of frequency bands is based on a twofold evidence. The first stems from the observed properties of business cycles and, in particular, the robust increasing evidence of a lengthening in the business cycle. Indeed, NBER chronology in the post-WWII period provides clear evidence of a striking shift in the properties of US business cycle fluctuations. The average duration of US business cycles has changed from about 50 months (about 4 years) in the pre-1960s period to about 76 months (slightly more than 6 years) in the post-1960s period. And this tendency is still going on, with the last three cycles in the 1980s, 1990s and early 2000s lasting approximately 10 years each. Similar evidence about the average length of the business cycle is also reported in Proietti et al. (2007) for the euro area and in Bartoletto et al. (2017) for Italy. Hence, with business cycle duration progressively increasing over time, the relevant periodicities for the empirical analysis of the modern business cycle are likely to concentrate on the low end of the traditional business cycle frequency.====The second motivation is based on the findings recently reported in Gallegati and Ramsey (2014) and Gallegati et al. (2014) which document for the US a significant negative relationship at business cycle frequencies, especially at frequencies longer than 4 years, between corporate bond spread and investment, and between credit spread and real economic activity, respectively. These findings provide interesting insights on the interaction between financial conditions and economic activity, and indicate that attempts to quantify the role of financial frictions in business cycle fluctuations may require disentangling the traditional business cycle span into subsets of frequency bands.====The contribution of our paper to the existing literature is twofold. First, we extend Sala’s contribution by providing a deeper understanding on parameter variations and on the role of frictions in goods, labor and credit markets at the business cycle. In particular, by separately reporting the structural parameter estimates over subsets of business cycle frequencies, our results show that several parameter estimates change considerably across the frequency bands over which the model is estimated, with the most notable differences pertaining to the estimated coefficients related to financial adjustment costs. In addition, the interest rates adjustment costs seems to have much more impact at longer-run business cycle frequencies while the adjustment cost related to the Basel II capital requirement is more effective at shorter-run business cycle frequencies. These results are also confirmed by the impulse response analysis that shows an amplification of the response of the model to financial shocks, while monetary policy shock are less frequency-dependent. Moreover, variance decomposition analysis confirms the major role played by financial factors at lower business cycle frequencies.====Second, in terms of goodness of fit, the Bayesian comparison of financial friction and frictionless frameworks shows that the performance of the financial friction model is largely superior to the frictionless benchmark at lower business cycle frequencies, a result consistent with the hypothesis of a more intense role of financial frictions and macro-financial linkages at longer time frames. Our results are in line with Brzoza-Brzezina and Kolasa (2013) findings at all business cycle frequencies, in the sense that adding financial factors do not improve the fit of the model with respect to the baseline New Keynesian model. Otherwise, once we get rid of the 1–4 years frequency band, the model including financial variables significantly improves over the frictionless framework.====The paper is divided in 5 sections. Section 2 presents the model with financial frictions. Section 3 shows the estimation results over different frequency bands, along with the impulse response function and variance decomposition analysis. The Bayesian comparison of the financial friction model against its frictionless framework is presented in Section 4. Section 5 concludes.",DSGE model with financial frictions over subsets of business cycle frequencies,https://www.sciencedirect.com/science/article/pii/S0165188918304032,March 2019,2019,Research Article,408.0
Madison Florian,"University of California, Irvine, USA,University of Basel, Switzerland","Received 15 February 2018, Revised 22 September 2018, Accepted 24 September 2018, Available online 30 December 2018, Version of Record 14 January 2019.",https://doi.org/10.1016/j.jedc.2018.09.008,Cited by (4),", exchanging information-sensitive assets for risk-free bonds, as conducted by the Federal Reserve in response to the global financial crisis.","This paper studies asset reallocation in over-the-counter (OTC) markets subject to search, bargaining, and information frictions, and discusses their implications for optimal monetary policy. Under a lack of commitment, agents bilaterally trade liquid (fiat money) for illiquid one-period lived assets, where the future return of the latter is private information to the holder. Gains from reallocation emerge due to idiosyncratic consumption opportunities resulting in different individual valuations for a particular asset. Monetary policy is conducted in two ways: a change in the growth rate of the money supply, or open market operations, exchanging information-sensitive assets for fiat money or risk-free bonds, where distinction is made between early and late interventions.====The importance of information frictions in modern economies was once more highlighted by the events of the last decade. As documented by Gorton (2008), at the onset of the great recession, asset classes formerly perceived as information-insensitive all of the sudden switched to being information-sensitive.==== The resulted opacity created new challenges for trade. While markets for information-insensitive assets with centralized trading continued to function smoothly, bilateral exchange in OTC markets - allowing to trade a variety of asset classes - declined significantly and had to be restored through policy interventions. Apart from conventional measures, central banks reacted with direct purchases of distressed assets through open market operations, reemphasizing the close relation between the functionality of these markets, asset prices, and monetary policy.====Motivated by these events, the aim of this paper is to understand the consequences of information frictions for asset reallocation in OTC markets, and to provide further guidance for the optimal design of conventional and unconventional policy under adverse selection. The baseline environment corresponds to Geromichalos and Herrenbrueck (2016), extended to incorporate information-sensitive one-period lived assets along the lines of Rocheteau (2011). Three markets open sequentially: a centralized primary market, an OTC secondary market, and a decentralized goods market. Under uncertainty regarding a future consumption opportunity, in the primary market, agents choose a portfolio consisting of fiat money and information-sensitive assets, where only fiat money is accepted as a medium of exchange in the upcoming goods market. Following an idiosyncratic signal regarding an upcoming consumption opportunity, agents reallocate their portfolios in the secondary market conditional on individual liquidity needs. The bilateral nature of the meetings allows for explicit game-theoretic foundations, whereas a signaling game refined by the Cho and Kreps (1987) Intuitive Criterion characterizes the terms of trade.====The general results show that information frictions reduce asset liquidity, increase the demand for fiat money, and lower trading volume and consumption. More precisely, agents in the OTC market use traded quantities in bilateral meetings as a signaling device to reveal private information, where high-quality asset-holders distinguish themselves from low-quality asset-holders through ====, resulting in reduced trading volume. The signalized willingness to hold on to parts of the assets improves terms of trade, but reduces consumption in the goods market relative to trade without private information. Consequentially, in order to circumvent these signaling costs, agents have a higher demand for fiat money in the primary market. A strictly positive rate-of-return differential between fiat money and information-sensitive assets is the consequence.====The provided insights allow for a welfare analysis, whereas the results show that in a constrained equilibrium, for some degrees of search frictions in the secondary market, information frictions can in fact be welfare-increasing. This result stems from the fact that the presence of secondary markets, i.e., the option to reallocate portfolios in response to an idiosyncratic preference shock, has a pecuniary externality reducing the demand for fiat money, and thus its price in the primary market, depressing consumption of buyers that were unable to reallocate their portfolio. Information frictions can correct for this externality by making asset reallocation more costly. This effect can be so strong that the consequential appreciation in the value of fiat money becomes welfare-increasing, since it benefits buyers that remained unmatched in the secondary market.====The inefficiencies arising from trade under private information motivate policy interventions. To begin with, optimality occurs under the Friedman rule, since it eliminates the opportunity cost of holding money and hence the necessity for portfolio reallocation. A shutdown of the secondary market is the consequence. However, considering the fact that a central bank’s inflation target may be nonequivalent to the Friedman rule, or non-distortionary lump-sum taxation is unavailable, further policy implications need to be discussed. A distinction is made between open market operations before (primary market) and after (secondary market) the private information has materialized, since timing matters. Following Mankiw (1986), functionality can be restored if the government manages to reduce the prevailing adverse selection problem, capturing the motive for the large-scale asset-purchases recently conducted by the Federal Reserve. An open market operation in the primary market, exchanging information-sensitive assets for risk-free bonds, achieves this goal by eliminating information frictions in the OTC market, restoring trading volume, and thus consumption. A government intervention of that kind in the secondary market, bilaterally exchanging information-sensitive assets for fiat money, however, has no such effect, since at this stage, information frictions characterize the terms of trade. Nonetheless, by providing a centralized outside option for reallocation, potential inefficiencies arising from decentralization - namely search frictions in the secondary market - can be reduced.",Frictional asset reallocation under adverse selection,https://www.sciencedirect.com/science/article/pii/S0165188918304044,March 2019,2019,Research Article,409.0
Michau Jean-Baptiste,"Ecole Polytechnique, France","Received 31 July 2018, Revised 27 November 2018, Accepted 11 December 2018, Available online 27 December 2018, Version of Record 3 January 2019.",https://doi.org/10.1016/j.jedc.2018.12.006,Cited by (2), persistence to characterize the optimal monetary and fiscal policy in a ,"A major constraint on the conduct of monetary policy is that the nominal interest rate set by the central bank cannot be negative.==== When this constraint is binding, the economy is in a “liquidity trap” situation. This results in an excessively high real interest rate and a depressed level of aggregate demand. As the experience of Japan over the last two decades has shown, the economy can remain stuck into a liquidity trap for an extended period of time. The government therefore needs to rely on monetary or fiscal policy to stimulate economic activity.====To analyze monetary and fiscal policy in a liquidity trap, the literature has extensively relied on the baseline new Keynesian model. However, this model is purely forward looking, which implies that inflation does not have any persistence. My contribution in this paper is to characterize the optimal monetary and fiscal policy in a liquidity trap under inflation persistence. As we shall see, inflation persistence significantly modifies the main features of the optimal monetary or fiscal policy and it enhances their ability to lift the economy out of the trap.====There are empirical controversies about the structural degree of inflation persistence.==== On the one hand, reduced form estimates show that inflation persistence has declined since the onset of the Great Moderation. On the other hand, the structural degree of inflation persistence can hardly be identified over an episode of history characterized by low and stable inflation, where monetary policy ensured that inflation rarely departed from target. Moreover, while endogenizing inflation persistence is beyond the scope of my analysis, persistence is likely to be higher under liquidity trap circumstances. Indeed, the inability of the central bank to prevent inflation from falling below target can de-anchor inflation expectations, which could increase the structural degree of inflation persistence.==== Not surprisingly, past inflation currently appears to be an important determinant of Japanese inflation expectations (Hausman and Wieland, 2015).====To allow for inflation persistence, without departing from rational expectations, indexation of non re-optimized prices to the last observed rate of inflation is routinely added to the new Keynesian model (see, for instance, Smets, Wouters, 2003, Woodford, 2003, or Christiano et al., 2005). For most of my analysis, I therefore rely on this standard new Keynesian model with inflation persistence.====I first investigate monetary policy alone. Krugman (1998), Eggertsson and Woodford (2003), Jung et al. (2005) and Werning (2012) have shown that, in the absence of inflation persistence, the central bank can only get some grip on economic activity by promising to create an output boom after the crisis is over. This requires a strong degree of commitment as this policy is not time consistent. By contrast, I show that, if inflation persistence is sufficiently strong, commitment beyond the end of the crisis is not necessary to stabilize the economy. The central bank can raise inflation by committing to implement a path of positive nominal interest rates ==== the crisis.====To understand this result note that, without inflation persistence, a rise in future nominal rates reduces inflation expectations and, hence, through the forward looking behavior of agents, the current rate of inflation. However, with inflation persistence, the lower current rate of inflation further reduces future rates of inflation. This effect can be so strong as to generate a never-ending feedback loop between a fall in current inflation and in future inflation. In that case, the only rational expectation equilibrium is that a rise in future nominal rates ==== inflation expectations, which increases the current rate of inflation. This reduces the current real interest rate, which stimulates the demand for consumption.====This result shows that the interplay between the forward and backward looking components of the Phillips curve can make the monetary policy implemented ==== the liquidity trap effective. This enhances the scope of monetary policy as it is admittedly much easier for a central bank to credibly commit to a path of nominal interest rates during the crisis than to commit beyond the end of the crisis.====I then turn to fiscal policy. Without inflation persistence, if the government cannot commit beyond the end of the crisis, then fiscal policy alone is responsible for avoiding a depression. In that case, the optimal fiscal policy is characterized by a mostly back-loaded profile of government expenditures. This is due to the purely forward looking nature of the economy: expenditures realized towards the end of the crisis stimulate the economy when they occur, but also beforehand through their effect on expectations. This optimal policy stands in contrast with the common practice of governments, which is avoid back-loaded stimulus packages.==== In fact, a common concern with the inclusion of infrastructure spending in such packages is that they are too slow to implement.====Inflation persistence provides a countervailing force. I show that, with inflation persistence, the government can always spend sufficiently in the first period of the crisis to raise inflation by a sufficient amount to guarantee that the zero lower bound will never be binding in the future. Of course, this policy of “pump priming” the economy requires a huge amount of government expenditures in the first period and is therefore unlikely to be optimal. However, this example shows that inflation persistence makes the front loading of government expenditures desirable. Simulations show that, for a strong degree of inflation persistence, the optimal fiscal stimulus is mostly front-loaded. For an intermediate degree of inflation persistence, and in the absence of support from monetary policy,==== it is double-peaked: government expenditures are concentrated towards the very beginning and the very end of the crisis.====Simulations also show that inflation persistence substantially reduces the magnitude of the fiscal stimulus that is necessary to stabilize the economy. The interaction between the forward and the backward looking components of the Phillips curve enhances the effectiveness of the fiscal stimulus. The backward looking component allows inflation to get started, while the forward looking component ensures that the expectation of future inflation raises current inflation.====While most of the paper investigates inflation persistence within a rational expectation framework, in a final section, I consider the possibility that inflation persistence results from backward looking expectations. If agents form adaptive expectations, then the Phillips curve is purely backward looking. In that case, monetary policy is useless while fiscal policy is less effective than before. Indeed, in the absence of a forward looking component to the Phillips curve, future government spending cannot raise inflation expectations. Thus, a much larger stimulus package is needed to stabilize the economy. In that case, the optimal fiscal stimulus is heavily front-loaded.==== Keynes (1936) and Hicks (1937) were both aware of the possibility of the economy falling into a liquidity trap.==== However, this phenomenon was seen as a purely theoretical scenario until the Japanese nominal interest rate hit the zero lower bound in the mid-1990s. This event led to a renewal of interest for the topic.====Starting with Krugman (1998), the modern liquidity trap literature has emphasized the extent to which the forward looking behavior of agents can make monetary policy potent (see also Eggertsson, Woodford, 2003, Jung, Teranishi, Watanabe, 2005, Mankiw, Weinzierl, 2011, Svensson, 2001, and Werning, 2012). However, this requires the central bank to “promise to be irresponsible” by allowing inflation and the output gap to rise above target after the crisis is over. If the central bank cannot overcome the time consistency problem, then the government needs to rely on fiscal policy to stimulate aggregate demand.====Adam and Billi (2007) have shown that an occasionally binding zero lower bound markedly raises the value of monetary commitment. While inflation persistence strengthens their conclusion, they did not consider the possibility that the monetary authority can only commit for the duration of the crisis, but not beyond. I show that, under inflation persistence, the value of commitment beyond the end of the crisis is much reduced.====While the fiscal multiplier is small under normal circumstances, Christiano et al. (2011), Woodford (2011) and Farhi and Werning (2016) have shown that its is much larger in a liquidity trap.==== Indeed, under normal circumstances, a fiscal stimulus raises inflation, which induces the central bank to increase the nominal and, hence, the real interest rate. Government expenditures therefore crowd out private investment and consumption. By contrast, in a liquidity trap, the nominal rate is stuck against the zero lower bound while the inflationary effect of government spending reduces the real rate, which crowds in investment and consumption. My simulation results show that fiscal policy is even more effective with inflation persistence.====Werning (2012) and Schmidt (2013) characterized, within the new Keynesian model, the optimal time path of government expenditures during a liquidity trap episode. Interestingly, Werning (2012) distinguished “opportunistic” from “stimulus” spending. The former is the mechanical response to a fall in the opportunity cost of public expenditures, and is therefore always countercyclical; while the latter corresponds to the spending realized for purely stimulative purposes. Werning (2012) showed that, under full commitment, monetary policy does most of the job of stabilizing the economy and, hence, the stimulus component can be equal to zero. By contrast, under discretionary monetary policy, a fiscal authority that can commit should implement a positive stimulus component of government expenditures. Moreover, the stimulus component should be back-loaded, which, as discussed above, is driven by the forward looking nature of the environment. Schmidt (2013) found similar results and, in addition, established the usefulness of fiscal policy in the absence of monetary and fiscal commitment.====In the same vein, Nakata (2016) showed that economic uncertainty raises the optimal level government spending at the zero lower bound, especially under discretion. Also, Roulleau-Pasdeloup (2018) showed that the government spending multiplier is much larger when the monetary authority follows a Taylor rule than when it is committed to an optimal policy. This confirms that fiscal policy is most valuable in the absence of monetary commitment.====Regarding the policy consequences of inflation persistence, Steinsson (2003) characterized the optimal monetary policy within a new Keynesian model with a backward-looking component. However, he focused on a “cost push” supply shock and did not consider the zero lower bound. Hasui et al. (2016) is a closely related contribution, written at the same time as this paper. They also characterize optimal monetary policy at the zero lower bound in the new Keynesian model with inflation persistence. However, they assume that the natural real interest rate follows an AR(1) process. Relying on numerical simulations, they show that persistence results in early monetary tightening. My paper complements their findings by explaining the precise mechanism allowing for this result and by considering the possibility of limited commitment.====Finally, my monetary policy result that higher nominal rates in the future can raise inflation is reminiscent of the neo-Fisherian literature (Cochrane, 2017a, Schmitt-Grohé, Uribe, 2017). However, the underlying mechanism is completely different. My result is driven by the interaction between the forward and the backward looking components of the Phillips curve, while the neo-Fisherian result is due to the selection of long-run inflation expectations consistent with the existence of a non-degenerate equilibrium over an infinite horizon (Kocherlakota, 2016).====Section 2 is dedicated to monetary policy. Section 3 incorporates fiscal policy into the analysis. Section 4 deals with adaptive expectations. The paper ends with a conclusion.",Monetary and fiscal policy in a liquidity trap with inflation persistence,https://www.sciencedirect.com/science/article/pii/S0165188918301921,March 2019,2019,Research Article,410.0
"Karahan Fatih,Rhee Serena","Federal Reserve Bank of New York, 33 Liberty Street, New York, NY 10045, USA,Korea Development Institute, 263 Namsejong-ro, Sejong City 30149, Republic of Korea","Received 30 August 2018, Revised 23 November 2018, Accepted 11 December 2018, Available online 27 December 2018, Version of Record 9 January 2019.",https://doi.org/10.1016/j.jedc.2018.12.007,Cited by (10),We provide a spatial general equilibrium model of migration and job searching to study the role the housing bust played in mobility rates and labor market outcomes. We estimate the parameters governing the migration decisions of households using data on state-level migration and labor market patterns from 1991 to 2007 and use the estimated model to characterize the effects of the housing market collapse on labor market outcomes. Our analysis suggests that the housing bust is responsible for 12% of the increase in unemployment as it reduces workers’ reallocation and forces more homeowners to look for jobs in areas with lower job-finding rates.,"The unemployment rate in the United States increased from 5% in January 2007 to 10.1% in October 2009, as the economy experienced its deepest downturn in the postwar era. During the same period, following a sharp decline in house prices, the net migration rate declined by 50% to an all-time low.====It has been suggested that the decline in house prices might be responsible for the sluggish performance of the labor market during the Great Recession (e.g., see Kocherlakota, 2010). One explanation is that some households that would have normally moved out of low-productivity regions may stay (or may be “locked in”) and continue searching for jobs within a distressed labor market to avoid selling their current houses at a low price. If more of the population resided in under-performing local labor markets, the overall unemployment rates would have risen.====However, previous empirical studies suggest that the decline in migration does not appear to be the main cause of the poor labor market performance during this period. Household data show weak linkages between the size of home equity and the migration propensity of homeowners (Schulhofer-Wohl, 2012). Similarly, Demyanyk et al. (2017) evaluate this house-lock hypothesis using the geographic variation in default policies on mortgages, and conclude that, during the Great Recession, the benefit of moving away from the distressed local economy would outweigh the costs of migration.====In this paper, we revisit the house-lock hypothesis and argue that the underlying distortion caused by the housing market collapse might be more substantial than suggested by the previous empirical literature. A decline in house prices can affect homeowners’ job search behavior beyond the scope of the job search area. When homeowners in a distressed housing market have restricted options to search for jobs outside their locality, they may adjust their job search behavior within local labor markets. In particular, with a binding liquidity constraint and low home equity, unemployed homeowners may apply for jobs with lower wages than they otherwise would to find a job faster. If this endogenous wage response is significant, then the housing bust could affect the labor market beyond the degree implied by the observed decline in migration and rise in unemployment. Indeed, more recent empirical research examines the labor market outcomes of homeowners in collapsed housing markets and confirms that they tend to apply for entry-level, low-paying jobs with lower wage growth (Brown and Matsa, 2016 and Yang, 2019).====To understand the effects of the housing bust on the U.S. labor market by taking into account this endogenous response in the job search stage, we build an equilibrium model with multiple locations and an endogenous labor supply decision in frictional labor markets. Each location has a local labor market that is subject to regional productivity shocks. Workers reside in different locations and can look for jobs in multiple locations. They choose to move for a combination of idiosyncratic reasons: house prices and conditions in the local and aggregate labor market. Once in a location, workers may decide to purchase a home or remain as renters. Households can save using a risk-free asset. To finance housing purchases, workers can take on a mortgage at a fixed exogenous interest rate after making a down payment from their savings.====In general, solving an equilibrium model with heterogeneous households out of its steady state would require us to solve a functional equation with an endogenous distribution of household characteristics across locations as a state variable. Our structural model preserves its computational tractability even though it includes essential ingredients of household heterogeneity. This is because the directed search environment in labor markets allows a block-recursive equilibrium where the optimization problems of firms and workers do not depend on the endogenous distribution (Menzio, Shi, 2010b, Menzio, Shi, 2011).====Estimating the model by the method of simulated moments, we find that the model explains the key features of labor and population trends over the business cycle well. Using the estimated model, we simulate counter-factual recessions and examine the effects of the housing bust on the patterns of job search, employment, and migration. Consistent with findings in the previous literature, the housing bust induces a moderate increase in the unemployment rate through the decline in migration, accounting for 12% of the total change in unemployment.",Geographic reallocation and unemployment during the Great Recession: The role of the housing bust,https://www.sciencedirect.com/science/article/pii/S0165188918302227,March 2019,2019,Research Article,411.0
Fehrle Daniel,"University of Augsburg, Department of Economics, Universitätsstraße 16, Augsburg 86159, Germany","Received 8 May 2018, Revised 15 November 2018, Accepted 11 December 2018, Available online 19 December 2018, Version of Record 28 December 2018.",https://doi.org/10.1016/j.jedc.2018.12.004,Cited by (3),I present a multisectoral ,"Aggregate and sectoral co-movements are central features of business cycles. With respect to the housing sector, Davis and Heathcote (2005) point out three stylized facts: (i) gross domestic product (GDP), private consumption expenditure(PCE), business and residential investment, as well as house prices are positively correlated, (ii) residential investment is more than twice as volatile as business investment, and (iii) business investment lags GDP whereas residential investment leads GDP. Davis and Nieuwerburgh (2015); Iacoviello and Neri (2010); Kydland et al. (2016) and Iacoviello (2010) corroborate these findings. Fig. 1 displays the cyclical components of GDP, house prices, as well as residential and business investment and verifies that the facts (i)–(iii) still characterize the cyclical properties. The co-movements, the volatility ratio between residential and business investment, the lead-lag pattern, and the housing boom-bust cycle are identifiable.====This paper presents a multisector DSGE model and examines the extent to which investment adjustment cost and variable capital utilization in the different sectors can account for the observed stylized facts. My starting point is the model of DH. It has had a lasting impact on the housing literature over the last decade. The model is able to explain the positive co-movements of aggregate and sectoral quantities and the volatility ratio of the investment types. It fails to predict the positive correlation between residential investment and house prices as well as the lead-lag pattern of residential and business investment.====The DH model distinguishes three sectors of production. In each sector, labor augmenting technical progress grows at a sector-specific rate and is subject to sector-specific stationary shocks generated by a three-dimensional, first-order vector-autoregressive process (VAR(1)). As I will demonstrate below, the correlation between the sectoral shocks is responsible for the model’s ability to replicate some of the stylized facts. In particular, with sectoral uncorrelated shocks the co-movement between the sectoral outputs and all co-movements concerning residential investment weaken. Most notable, the model predicts a highly negative correlation between house prices and residential investment and fails to predict the positive co-movement between business and residential investment.====The multisector DSGE model presented here is an extended version of the DH model with sectoral uncorrelated shocks that accounts for the stylized fact (i), in the sense of stronger positive correlations concerning residential investment, as well as for (ii). It also partly replicates the lead of residential over business investment. Furthermore, business investment lags GDP. As the DH model, however, it fails to mimic the high volatility of house prices. Furthermore, the co-movements between the sectoral outputs remain weak.====While it is possible that productivity shocks are correlated across sectors as a result of common technological innovation or spillover, there are at least three arguments in favor of uncorrelated sectoral shocks: parsimony, explanatory content, and accordance with econometric practice. The DH model with VAR(1)-process has nine more parameters than a model with sector-specific but uncorrelated AR(1)-processes. The VAR(1)-assumptions transfer part of the explanation of the stylized facts outside the model whereas the AR(1)-assumption has to rely on the endogenous propagation mechanism. Therefore, the latter opens the way to a deeper understanding of economic mechanisms than the former. Finally, considering the practice of structural vector autoregressive models, researchers seem to favor uncorrelated structural shocks.====The extensions vis-à-vis the DH model are in detail: variable capacity utilization as in Jaimovich and Rebelo (2009), adjustment costs to business investment as in Christiano et al. (2005) (CEE adjustment costs hereafter), sectoral frictions in the allocation of capital as in Boldrin et al. (2001), and, finally, higher costs of accumulating homes due to an increased share of land in the production of new houses. Bayesian inference, using the Solow residuals found by DH as observables, provides the estimates of the parameters of the three independent AR(1)-processes that drive the model as well as the two unobservable parameters that are crucial for the propagation of these shocks.====To understand the contribution of each of these ingredients, consider a shock to labor augmenting technical progress in one of the three sectors. The supply of goods produced in this sector will increase as well as the demand of labor and capital services employed in this sector. At the final stage of production, the price of that good decreases that has a higher cost share of the respective input. For instance, since manufacturing goods have approximately the same cost share in the production of consumption goods (see Table 1) and housing investment, the price effect of a shock in the manufacturing sector is limited. On the other hand, a shock to the construction sector will decrease the price of housing investment whereas a shock to the service sector will decrease the price of consumption goods. Adjustment costs to housing and business investment limit the effects of price changes on the respective demand so that the income effects of the shock prevail and consumption, residential and business investment will move together. Variable capacity utilization enhances the income effects and, thus, strengthens the co-movements between the major macroeconomic aggregates. It also acts as a substitute for limited sectoral capital mobility, which, therefore, contributes little to the model’s ability to mimic the stylized facts. CEE adjustment costs penalize rapid changes between current and past investment so that business investment lags GDP.====The paper relates to the growing literature on housing and the business cycle, comprehensively reviewed by Davis and Nieuwerburgh (2015), as well as to the literature on multisectoral real business cycle models.====Jaimovich and Rebelo (2009) designate the ability of a model to reproduce co-movements between sectoral and aggregate economic quantities as a litmus test. Nevertheless, most models fail this test. Early attempts by Benhabib et al. (1991); Greenwood and Hercowitz (1991) and Fisher (1997) examine the co-movement problem in models with market and home production. They find that investment in market and home capital correlate negatively. Gomme et al. (2001) assume that market capital requires more time to be built than home capital and partly succeed in explaining that investment in market capital lags investment in home capital. Fisher (2007) chooses another approach. He solves the lead-lag puzzle by modeling home capital as a complement of market production. Khan and Rouillard (2016) show that his approach requires implausibly high capital tax rates. My approach differs in the propagation channel: housing and productive capital are not complements, but adjustment costs disable the substitution. This approach is in line with Chang (2000) and, in a broader sense, with limited capital mobility as in Boldrin et al. (2001). Furthermore, as shown by Lucca (2007), CEE adjustment costs are, to a first-order linear approximation, equivalent to a more sophisticated time-to-build model of capital accumulation.====Kydland et al. (2016) investigate the stylized facts (i)–(iii) in a one-sector model with nominal and real frictions. The latter take the form of a concave production possibilities frontier and a convex increasing transformation rate of output to new houses. They can be seen as a reduced form of the multisectoral supply-side of DH to account for (ii). Kydland et al. (2016) successfully replicate that residential investment leads the business cycle albeit the nominal interest rate, which drives this result, is exogenously linked with a lead to the business cycle.====Khan and Rouillard (2018) explore the role of residential investment as a collateral to finance consumption. They calibrate the model for the time period following the U.S. financial reforms in the early 1980s and show that residential investment leads the business cycle to finance future consumption. Fig. 1, however, indicates that residential investment also leads the business cycle before the financial reforms.====To account for the large house price volatility, Dorofeenko et al. (2014) introduce CEE adjustment costs in business and residential investment as well as default risk in the DH framework. Furthermore, they adopt the exogenous process with correlated shocks. The model tends to the opposite direction of the lead-lag pattern as empirically observed and it is not clear which parts are driven endogenously.====As in Dorofeenko et al. (2014), many papers have tried to explain the large volatility of house prices and the boom-bust cycle in the first decade of the 21st century (see, e.g. (Favilukis et al., 2017) and the review paper of Davis and Nieuwerburgh (2015)). These models consider, amongst other factors, heterogeneous agents, risk shocks, and financial frictions. All of these ingredients are beyond the scope of the present model that does not aim to contribute to this debate.====The remainder of the paper is organized as follows. Section 2 introduces the model and stresses the differences in the investment and capital types. Section 3 explains the calibration and estimation strategy, presents the results in form of second moments and discusses the robustness of these findings. Section 4 concludes. An Appendix covers additional material: the system of equations that determines the model’s dynamics, further results, descriptions of the data and the Monte-Carlo-algorithms.",Housing and the business cycle revisited,https://www.sciencedirect.com/science/article/pii/S0165188918303932,February 2019,2019,Research Article,412.0
"Galanis Giorgos,Veneziani Roberto,Yoshihara Naoki","Institute of Management Studies, Goldsmiths, University of London, New Cross, London SE14 6NW, UK,School of Economics and Finance, Queen Mary University of London, Mile End Road, London E1 4NS, UK,Department of Economics, University of Massachusetts Amherst, Crotty Hall, Amherst, MA, 01002, USA,The Institute of Economic Research, Hitotsubashi University, Naka 2-1, Kunitachi, Tokyo 186-0004, Japan,School of Management, Kochi University of Technology, Tosayamada, Kami-city, Kochi, 782-8502, Japan","Received 1 December 2017, Revised 5 December 2018, Accepted 13 December 2018, Available online 18 December 2018, Version of Record 5 January 2019.",https://doi.org/10.1016/j.jedc.2018.12.005,Cited by (4),"Introducing a concept of fairness of economic allocations, namely ","Recently, a vast literature has analyzed the persistent, and widening, inequalities in income and wealth observed in the vast majority of nations.==== Less attention has been devoted to a specific form of inequality related to the systematic ==== of labor in relation to their contribution to production, which is known as ==== and has normative implications. Two questions immediately arise. First, what is the relation between inequalities in income and/or wealth, and the existence and persistence of exploitation? Second, what are the mechanisms that drive the existence, and persistence, of exploitation in growing economies?====The answers to these questions are not obvious, and depend on the concept of exploitation adopted.==== This paper adopts a specific concept of exploitation, namely ==== (henceforth, UE exploitation), and analyzes the relation between inequalities in income and/or wealth, and the existence and persistence of UE exploitation by focusing on linear economies.==== The basic intuition of UE exploitation can be traced back to Aristotle and Karl Marx,==== but its modern, rigorous formulation is due to Roemer, 1980, Roemer, 1982a, Roemer, 1982b: an agent is UE exploited (resp. exploiting) if the amount of labor she contributes to the economy (e.g., the labor she expends in productive activities) is higher (resp. lower) than the amount of labor she ‘receives’ via her income (e.g., the labor contained in the consumption bundle she can purchase). As (Roemer, 1982b, p. 168) emphasized, this kind of disproportional distribution of labor and national income is indeed worth calling ==== under the differential ownership of productive assets.====To see this, consider a simple perfectly competitive economy with only two agents, ==== with the same preferences and the same skills. If agent ==== is wealthy and therefore can optimize without working, while agent ==== has very little capital and needs to work for ====, then “[p]roducer ==== is exploiting ====. This comes about because ...==== is wealthier than ====, and is able to use his wealth as leverage through the exchange mechanism to force ==== to work “for” him. ...That this phenomenon deserves to be called [UE] exploitation can be seen by the following. Suppose ...==== expropriated ====’s endowment and killed him. Producer ==== would now be wealthier than before; yet, ... for the new economy in which only he is a member, he will have to work ...longer than when ==== was there. Thus, exploitation is an explicitly social phenomenon: ==== can get away with working less ...only because there is someone else working more ..., to “support” him. Producer ==== appears to be gaining at the expense of ====, ...even though all producers gain from trade” (Roemer, 1982b, p.168).====This last argument also makes the concept of UE exploitation distinct from other forms of inequalities. Indeed, if these agents engaged in their autarkic economic activities and developed no division of labor between them, income and wealth inequalities would still be observed due to the differential ownership of productive assets, but there would be no UE exploitation in this case.====The concept of UE exploitation also directs our attention to the joint distribution of income and labor/leisure in the economy, which is also relevant information for the modern theory of distributive justice. Indeed, as Fleurbaey (2014) has argued, UE exploitation can be linked with the issue of inequalities in the distribution of material well-being purchasable via income (ex. goods, services, etc.) and free hours (leisure time) discussed in the theory of distributive justice. For instance, they are relevant for ==== (Rawls, 1971; Sen, 1999), because material well-being and free hours are key determinants of individual well-being freedom.====Given these distinctive features of the concept of UE exploitation, the first question raised at the beginning of this section is important in order to understand whether analyses of income and wealth inequalities provide exhaustive information about the unfairness of competitive economies. If this was not the case, then policies that alleviate inequalities would not necessarily reduce, and might even increase, other forms of unfairness, such as UE exploitation.====The relation between growth and UE exploitation is also unclear, and equally relevant from both a theoretical and a policy perspective. Does growth tend to reduce, or worsen, exploitative relations? What are the mechanisms that guarantee the persistence of UE exploitation?====In this paper, we address these issues and analyze the relation between wealth inequalities, profits, accumulation and UE exploitation in a dynamic general equilibrium model with heterogeneous optimizing agents which generalizes Roemer, 1980, Roemer, 1982a, Roemer, 1982b economies. To be specific, we assume that there exist two types of agents in the economy, capitalists and workers. Both types of agents have the same preferences over consumption bundles and solve an intertemporal optimization program. They also have the same skills and labor endowments. However, we allow for inequalities in their endowments of productive assets, which have some major behavioral implications.====Capitalists own wealth which allows them to hire workers and activate production processes by using a linear technology. They do not work and allocate their income optimally between consumption and savings. In contrast, workers own no wealth and their only source of income is their labor. Although, like capitalists, they choose consumption bundles to maximize utility, we assume that they cannot accumulate. This assumption is consistent with two other key features of our model, namely the existence of wealth constraints and the incompleteness of credit markets,==== whose importance has long been emphasized in the literature.==== It is also in line with standard models in the tradition of Classical economics (see Roemer, 1980, p.509), and with more recent analyses of the effect of wealth inequality and credit constraints on macroeconomic dynamics (see, most notably, Kaplan and Violante, 2014 and Krueger et al., 2016).====In this economy, we rigorously define the concept of UE exploitation (Definition 3): agents suffer from an unequal exchange of labor if the amount of labor they contribute to the economy is higher than the amount of labor necessary to produce a consumption bundle purchased by their income. We show that in equilibrium, the profitability of capitalist production is synonymous with the existence of UE exploitation and a monotonic relation exists between the profit rate.==== and the exploitation rate (Theorem 1). This result is important because the rate of profit is one of the key determinants of investment decisions, and of the long-run dynamics of capitalist economies. Thus, Theorem 1 can be interpreted as providing a link between inequality, exploitation, and growth: in an economy where only capitalists invest, more exploitation implies a higher profit rate and thus more funds for accumulation. But the result is important also because it proves that, given private ownership of productive assets, profits are a counterpart of the transfer of social surplus and social labor from asset-poor agents to the wealthy. This provides a link between UE exploitation and the functional income distribution. This result confirms the relevance of the concept of UE exploitation in examining the dynamics of capitalist economies, and it provides the foundations for the rest of the analysis.====We then derive a number of results concerning the relation between inequality, UE exploitation, and growth. First, in the basic economy with constant labor supply and a given technology, there exists no equilibrium path with persistent growth and persistent UE exploitation even in the presence of significant asset inequalities (Proposition 1): absent any countervailing measures, accumulation leads capital to become abundant, and profits and exploitation to disappear. Second, and perhaps more surprising, asset inequalities and competitive markets alone do not guarantee the persistence of profits and UE exploitation, even in equilibrium paths ==== in which capital does not become abundant. Neither at stable growth paths converging to a long-run steady state, nor at stationary states without accumulation, is UE exploitation a persistent feature of the economy, unless agents discount the future (Theorems 3 and 4). Finally, Theorem 7 proves that in the long-run labor-saving technical progress may yield sustained growth with persistent exploitation and inequalities by reducing the demand for labor, thus creating the conditions for an excess supply of labor, which restrains wages from rising.====These results correspond to the question of the mechanisms driving the persistent existence of UE exploitation in growing economies. First, some of them shed light on the role of time preference in this question. If the discount factor of agents is unity, then inequalities, UE exploitation, and profits may well disappear after a finite number of periods, ==== (Theorems 3 and 4). Instead, if agents discount the future, inequalities and UE exploitation can be persistent even in paths with capital accumulation (Theorem 4). Furthermore, the steady state value of the profit rate (and thus, by Theorem 1, the rate of UE exploitation) is a positive function of the discount factor whenever it is less than unity. In contrast, overaccumulation leading to the disappearance of UE exploitation cannot be ruled out in equilibrium unless the discount factor is less than unity (Proposition 2). These results also suggest that the analysis of UE exploitation cannot be reduced to the analysis of wealth inequalities, as discussed in more detail in Sections 5 and 6 below.====Given these results, one may argue that, along equilibrium paths where capital remains scarce, a positive rate of time preference by capitalists is all that is needed, in addition to differential ownership of (scarce) productive assets and competitive markets, to guarantee the persistence of factor income inequalities and UE exploitation. Although this is certainly a legitimate interpretation of Theorems 3 and 4, we do not think that it is the most insightful or normatively robust. On the one hand, in our view, one of the distinguishing features of exploitation theory is an emphasis on the structural, objective characteristics of capitalist economies, rather than on subjective and empirically contingent factors such as time preference.====On the other hand, our results point to an alternative, more structural explanation of the persistence of inequalities and UE exploitation. In particular, Theorem 7 suggests that the central role of asset inequalities is better understood in conjunction with labor market conditions and institutions, and the mechanisms that ensure the scarcity of capital. It provides an alternative explanation of the persistence of UE exploitation focusing on the interaction between technical change ==== labor market conditions in shaping distributive outcomes and the equilibrium growth paths of capitalist economies.==== By raising labor productivity, labor-saving technical progress ensures the persistent abundance of labor, which in turn keeps wages low and guarantees the profitability conditions necessary for growth. Further, as the wage rate and employment remain relatively stagnant while the economy grows, the interaction between labor market conditions and technical progress leads the wage share in national income to fall steadily over time, as has been observed in most advanced countries in the last few decades (Karabarbounis and Neiman, 2014; Piketty and Zucman, 2014).====Compared with the recent macroeconomic literature exploring the emergence and persistence of inequality, our approach with a linear production technology is distinctive, in that it allows us to examine some relatively unexplored mechanisms through which inequality affects growth, and vice versa. In economies with credit constraints and standard production functions with diminishing marginal productivity, for example, inequality tends to reduce growth because poorer agents cannot access production loans and therefore cannot invest in projects yielding higher returns (see Bénabou, 1996; Matsuyama, 2004; and the literature therein). Conversely, one of the main channels through which growth affects inequality is via the effect of accumulation on the dynamics of the marginal productivity of capital and labor. In our model, we analyze the relation between inequality and growth abstracting from any consideration concerning marginal productivity, while focusing on the role of labor market conditions and technical change.====Further, while technical change and capital accumulation are also central in Acemoglu, 2002a, Acemoglu, 2002b, Acemoglu, 2003, they play a different role in their interaction with labor market conditions. Innovations, in our model, do not allow for persistent growth by affecting total factor productivity, or via their ==== effect on output per worker, and they do not influence the functional distribution of income by altering marginal productivity. Rather, labor-saving technical change allows UE exploitation and inequalities to persist by constantly creating an excess supply of labor that weakens the workers’ position in the labor market and drives wages down.",The dynamics of inequalities and unequal exchange of labor in intertemporal linear economies,https://www.sciencedirect.com/science/article/pii/S0165188918304020,March 2019,2019,Research Article,413.0
"Epstein Brendan,Finkelstein Shapiro Alan","Department of Economics, University of Massachusetts, Lowell. Falmouth Hall, Room 302; One University Avenue, Lowell, MA 01854, USA,Department of Economics, Tufts University, Braker Hall, 8 Upper Campus Road, Medford, MA 02155, USA","Received 9 March 2018, Revised 3 November 2018, Accepted 5 November 2018, Available online 15 December 2018, Version of Record 26 December 2018.",https://doi.org/10.1016/j.jedc.2018.11.004,Cited by (6),"We document a negative and significant relationship between domestic financial development and unemployment volatility in developing and emerging economies (DEMEs) and the absence of such relationship in advanced economies (AEs). A business cycle labor search model with firm heterogeneity, collateral constraints, and interfirm input credit capital can quantitatively rationalize these facts. Greater financial development is associated with lower usage of input credit capital, greater bank credit, and greater capital accumulation, all of which make firms more resilient in the presence of financial shocks. Firms’ increased shock ==== stabilizes employment decisions, ultimately leading to smoother unemployment fluctuations. Then, by establishing explicit linkages between firms, interfirm input credit acts as an important mechanism that fosters lower volatility across firms under greater financial development. As such, increases in financial development have a smaller impact in stabilizing unemployment in economies with higher average bank credit-GDP ratios and less input credit usage, which is the case of AEs, compared to economies with lower average bank-credit GDP ratios and more input credit usage, which is the case of DEMEs.","Economies differ widely in their levels of domestic financial development, and higher average bank credit-GDP ratios are often used as a defining characteristic of greater financial development. In particular, developing and emerging economies (DEMEs) tend to have lower average bank credit-GDP ratios compared to advanced economies (AEs). Economies also exhibit differences in the domestic financing structure of firms, as smaller firms tend to face relatively high barriers to bank financing compared to larger firms.==== As a result, smaller firms’ reliance on informal external financing, which often takes the form of interfirm input credit obtained from financially-stronger firms, is important and is particularly prominent in DEMEs. Moreover, across countries and particularly in DEMEs, smaller firms account for an important share of both total employment and employment creation (Ayyagari et al., 2014). As such, financial development may have non-negligible consequences for short- and long-run labor market outcomes. These facts motivate the following question: Is there a connection between financial development and unemployment volatility? The answer to this question is important for DEMEs since several of these economies are striving to expand and improve domestic financial access, especially to smaller firms (IDB, 2013, IFC, 2010).====Using a large sample of AEs and DEMEs with available time series on unemployment, we document a negative and statistically significant negative relationship between financial development and unemployment volatility in DEMEs, but no significant (albeit negative) relationship between these variables in AEs. These results are robust to a host of econometric specifications.====To shed light on these new facts, we develop a business cycle model with equilibrium unemployment and heterogeneity in both firms’ financial frictions and financing structure. Our model and analysis are guided by the fact that, compared to AEs, DEMEs have both lower bank credit-GDP ratios and lower shares of bank credit devoted to smaller firms, as well as greater interfirm input credit usage by small firms. Our framework features two firm categories, labeled “large” and “small” for simplicity. Both firm categories use labor (hired via frictional markets) and internally-accumulated capital to produce and pledge a portion of their internally-accumulated capital as collateral for bank borrowing. However, small-firm production also relies on a third input, namely capital supplied by large firms (we refer to this input as input-credit capital, i.e., physical inputs supplied by large firms). This modeling structure tractably captures the relative reliance of small firms on interfirm input-credit capital vis-à-vis large firms.====Given our focus on financial development, our notion of small firms is not related to size or age per se, but instead to the differential external financing structure between firm categories. Furthermore, given our research question, we do not model the specific microeconomic rationales for large firms extending input credit to small firms.==== Instead, we focus on the volatility implications of small firms’ relative reliance on interfirm input credit. More broadly, two distinguishing features of DEMEs relative to AEs in our model are the comparative usage of resources obtained via interfirm input credit among small firms, and the lower levels of average bank credit-GDP ratios in DEMEs. These two characteristics are jointly important for quantitatively capturing the change in unemployment volatility as financial development (reflected in greater steady-state bank credit-GDP ratios) takes place.====We calibrate our baseline model to capture an average DEME in terms of financial development and unemployment volatility per our data. Using our baseline DEME model, we then consider an alternative economy that differs from our baseline model only in its average (or steady-state) bank credit-GDP ratio, which we obtain by changing firms’ steady-state borrowing capacity. This change delivers an alternative economy that is an average AE financial development-wise per our data (referred to as the AE model). Changing the relevance of input-credit capital for small firms in our DEME and AE models so as to generate ranges of steady-state bank credit-GDP ratios that are empirically in line with the two country groups shows that unemployment volatility is decreasing in financial development. Critically, this relationship is much stronger from a quantitative standpoint in the DEME model than in the AE model. This outcome is in line with our empirical findings. In fact, model-based results of this relationship quantitatively match the empirical relationship between financial development and unemployment volatility in both DEMEs and AEs very well.====The intuition behind these main findings is as follows. A reduction in the relevance of input-credit capital (and hence of input credit) among small firms boosts these firms’ internal accumulation of (pledgeable) capital to obtain bank credit, which facilitates access to bank credit by relaxing firms’ collateral constraints and increases the economy’s steady-state bank credit-GDP ratio. Given the presence of collateral constraints, having a larger amount of pledgeable internally-accumulated capital also makes firms more resilient to credit disruptions. This increased resiliency stabilizes small firms’ decisions over employment for a given set of shocks. Critically, this greater employment stability also reduces the cyclical variability of demand for input-credit capital given shocks, which in turn reduces the volatility of large firms’ employment decisions. Thus, both large-firm and small-firm employment, and ultimately aggregate unemployment, become less volatile as greater financial development takes hold.====Importantly, starting from higher average bank credit-GDP ratios, increases in financial development rooted in small firms’ decreased reliance on input credit capital (and increased reliance on bank financing) have a smaller impact on the cyclical volatility of employment compared to cases in which average bank credit-GDP ratios are lower. Since average bank credit-GDP ratios are higher in AEs compared to DEMEs, then greater financial development in AEs reduces unemployment volatility by less than in DEMEs, whereas the greater prevalence of input-credit capital in DEMEs compared to AEs leads to a larger reduction in unemployment volatility under greater financial development.====All told, our findings can quantitatively rationalize the empirical link between financial development and unemployment volatility in DEMEs, as well as the relative weakness of such link in AEs. More broadly, our results suggest that the differential financial development-unemployment volatility link between DEMEs and AEs depends in a non-negligible way on firms’ domestic external financing structure. This relevant characteristic has generally been absent in studies focusing on labor market outcomes and the aggregate implications of financial development.====: The business cycle literature emphasizes the role of interest rate (or, more broadly, financial) shocks for DEME business cycles (Boz, Durdu, Li, 2015, Chang, Fernández, 2013, Neumeyer, Perri, 2005). More recently, models of financial frictions for AEs (e.g., Buera, Fattal-Jaef, Shin, 2014, Jermann, Quadrini, 2012, Kiyotaki, Moore, 1997; Iacoviello, 2015) have been expanded to incorporate frictional labor markets in order to analyze the financial-labor market link in DEMEs (Fernández, Andrés, Meza, 2015, Finkelstein Shapiro and González Gómez, 2017). In turn, a number of empirical studies have explored the connection between financial development, growth, and aggregate volatility (Aghion, Philippe, Bacchetta, Rancière, Rogoff, 2009, Beck, Demirgüç-Kunt, 2006, Beck, Demirgüç-Kunt, Laeven, Levine, 2004, Dabla-Norris, Era, Townsend, Unsal, 2015, Manganelli, Popov, 2012, Wang, Wen, 2013).====To the best of our knowledge, our paper is the first to present and explain a stylized fact explicitly linking financial development to cyclical labor market dynamics, and also to highlight the relevance of firms’ external financing structure for a deeper understanding of the link between financial development and labor market fluctuations.====While the importance of sectoral heterogeneity has received some attention, few studies, if any, have considered the implications of such heterogeneity for understanding labor market dynamics in DEMEs. Our modeling framework is most related to Epstein and Finkelstein Shapiro (2017) with regards to the reliance on input credit by a category of firms, and to Epstein and Finkelstein Shapiro (2017) with regards to financial frictions and collateral constraints in an environment with production heterogeneity. A distinct feature of our framework that is central to our focus on financial development in DEMEs is the combination of a tractable notion of interfirm input credit with standard collateral constraints.====The remainder of this paper is organized as follows. Section 2 summarizes empirical evidence on the prevalence of interfirm input credit as a source of firms’ external financing in AEs and DEMEs, and presents new evidence on the link between domestic financial development and unemployment volatility across country groups. Section 3 develops the model. Section 4 describes the model’s operationalization and main results. Section 5 concludes.","Financial development, unemployment volatility, and sectoral dynamics",https://www.sciencedirect.com/science/article/pii/S0165188918303919,February 2019,2019,Research Article,414.0
"d’Albis Hippolyte,Boubtane Ekrame,Coulibaly Dramane","Paris School of Economics, CNRS, 75014 Paris, France,University Clermont Auvergne, CNRS, IRD, CERDI, Clermont-Ferrand F-63000, France,EconomiX-CNRS, University Paris Nanterre, 92001 Nanterre, France","Received 30 July 2018, Revised 12 November 2018, Accepted 11 December 2018, Available online 13 December 2018, Version of Record 5 January 2019.",https://doi.org/10.1016/j.jedc.2018.12.003,Cited by (17)," on a panel of 19 ==== countries over the period 1980–2015 reveals that a migration shock increases GDP per capita through a positive effect on both the ratio of working-age to total population and the employment rate. International migration also improves the fiscal balance by reducing the per capita transfers paid by the government and per capita old-age public spending. To rationalize these findings, an original theoretical framework is developed. This framework highlights the roles of both the demographic structure and intergenerational public transfers and shows that migration is beneficial to host economies characterized by aging populations and large public sectors.","According to United Nations (2017), OECD countries host more than 40% of all immigrants worldwide. Moreover, the share of immigrants in the population of those countries has increased from 7% in 1990 to 13% in 2017. Because global population trends include a predicted concentration of young people in Africa, immigration figures are likely to rise.==== This article analyzes the effects of international migration on the macro-economic and fiscal situation of host countries. This point is important because most OECD countries structurally run public deficits. Moreover, opinion polls show that whatever the position of natives toward immigrants, the cost for public finances appears the main economic concern associated with international migration. For instance, according to European Social Survey (2014), 52% of European natives say they agree to allow many or some immigrants from poorer countries outside Europe to come and live in their home country. Among them, 30% believe that, on balance, immigrants take more (in terms of health and welfare services used) than they add (in terms of taxes payed), whereas 18% believe that immigrants generally take jobs away from native workers. Among those who say they want few or no immigrants, these proportions are 61% and 45%, respectively.====An analysis of the fiscal effects of immigration faces two main challenges. The first is reverse causality, because public spending, for example, can not only rise with flows of migrants but also increase those flows. The second is the focus on macroeconomic and fiscal variables, whose data coverage is considerably lower than that on microeconomics ones. To date, the literature on the effect of migration on public finance is quite inconclusive with respect to the sign of the effects, which are in all cases expected to be quantitatively low (Rowthorn, 2008, Liebig, Mo, 2013).====In this paper, we propose empirical evidence and an original theoretical framework that suggest that the macroeconomic and fiscal consequences of international migration are positive for OECD countries. Most notably, we explore the role of changes in the age structure of the population induced by net flows of migrants. As noted by Bloom et al. (2003) and Lee and Mason (2007), an increase in the share of the working-age population can produce a ==== with respect to economic growth and affect virtuous cycles of wealth creation. Because many migrants are of working age,==== we have focused our analysis on the demographic aspect of migration and are able to present robust evidence of the existence of a ==== in OECD countries.====We estimate a structural vector autoregression (VAR) on a panel of 19 OECD countries with annual data for 1980–2015. Since the initial article by Blanchard and Perotti (2002), the VAR approach has been widely used for fiscal studies (Alesina, Ardagna, Perotti, Schiantarelli, 2002, Beetsma, Giuliodori, Klaassen, 2006, Beetsma, Giuliodori, Klaassen, 2008, Monacelli, Perotti, Trigari, 2010, Beetsma, Giuliodori, 2011, Brückner, Pappa, 2012). This method’s multivariate dynamic structure appears to us to be appropriate for assessing the fiscal effects of international migration. Most notably, it proposes an identification strategy for assessing the dynamic effect of an exogenous migration shock on the economic and public finances of the host country. This strategy is suitable for macroeconomic studies, which complement micro-economic ones that address the endogeneity issue with instrumental variables.====We find that international migration is beneficial to OECD countries. In response to an exogenous shock that increases the net flow of migrants by one incoming individual per thousand inhabitants, GDP per capita rises significantly by 0.25 percent the year of the shock and peaks at 0.31 percent after one year; additionally, fiscal balance improves by 0.12 percentage points of GDP at its peak, which occurs the year of the shock and one year after the shock. The estimates confirm those found in many studies using different methodologies and data (Clemens, 2011, Ager, Brückner, 2013, Ortega, Peri, 2014).====To highlight the crucial importance of the demographic dividend of international migration, we initially use an accounting decomposition of the GDP per capita and show that following a migration shock, both the ratio of working-age to total population and the employment rate increase. Then, we propose a decomposition of public spending and analyze the effect of a migration shock on some of its components. Specifically, we show that the net flow of migrants reduces old-age spending but increases family spending, which suggests that the demographic dividend of international migration goes through public finances. Moreover, spending on active labor market programs increase following a migration shock while spending associated with unemployment benefits decrease. This point can be explained by the fact that international migration significantly reduces the unemployment rate in the host countries.====To understand the main mechanisms involved, we ultimately developed an analytical framework. This framework relies on an original overlapping-generation model in which migrants arrive as working age adults. To highlight in our model the demographic aspect of migration, we make no further assumptions that might lead to positive effects of international migration, such as complementarity between migrants and native-born, even when these effects are identified in the literature (Blau and Mackie, 2016). We lay out the simple conditions under which the net flow of migrants is positive for per capita income, savings and net taxes. In particular, we show that the effect is positive if the population growth rate is low and the share of public expenditure dedicated to young and old populations is high. Because OECD economies typically have aging populations and often have large intergenerational transfers, we deduce that the model helps in understanding the positive effect of international migration found in our empirical analysis. These results reinforce previous studies’ findings that highlighted the role of the age structure of the population on macroeconomic performances (Boucekkine, de la Croix, Licandro, 2002, Beaudry, Collard, 2003, d’Albis, 2007, Lee, Mason, Members of the NTA Network, 2014). Our model also provides a theoretical framework for the analysis of the demographic dividend of international migration.====The article is structured as follows. Section 2 presents the related literature. Section 3 describes the data and the econometric methodology. Section 4 presents the empirical results. Section 5 introduces an original overlapping-generation model that explains the mechanisms underlying our main results. Section 6 concludes.",Immigration and public finances in OECD countries,https://www.sciencedirect.com/science/article/pii/S0165188918303920,February 2019,2019,Research Article,415.0
"Hué Sullivan,Lucotte Yannick,Tokpavi Sessi","Université d’Orléans, CNRS, LEO, FRE 2014, F-45067 Orléans, France,PSB Paris School of Business, Departement of Economics, 59 rue Nationale, 75013 Paris, France","Received 9 August 2018, Revised 12 November 2018, Accepted 2 December 2018, Available online 12 December 2018, Version of Record 11 January 2019.",https://doi.org/10.1016/j.jedc.2018.12.001,Cited by (26)," and consists of measuring how far the proportion of statistically significant connections in the system breaks down when a given ==== is excluded. We analyse the performance of our measure of ==== by considering a sample of the largest banks worldwide over the 2003–2018 period. We obtain three important results. First, we show that our measure is able to identify a large number of banks classified as global systemically important banks (G-SIBs) by the Financial Stability Board (FSB). Second, we find that our measure is a robust and statistically significant early-warning indicator of downside returns during the last financial crisis. Finally, we investigate the potential determinants of our measure of systemic risk and find similar results to the existing literature. In particular, our empirical results suggest that the size and the business model of banks are significant drivers of systemic risk.","The US financial market turmoil that began in August 2007 and was amplified by the collapse of Lehman Brothers spread to the global financial market and had a severe impact on the real economy around the world. The size of the negative impact and the related social costs in most countries made new macro-prudential devices more necessary for systemic risk in the financial sector to be stabilised more efficiently. The Financial Stability Board (FSB) and the Basel Committee on Banking Supervision (BCBS) as regulatory authorities responded to these challenges with a number of reforms, where the main element was the identification of global systemically important financial institutions (G-SIFIs). This allocates G-SIFIs into buckets according to the level of additional loss absorbency they require. A set of principles was also established at country level to allow national authorities to identify domestic systemically important financial institutions (D-SIFIs).====Methodologically, a deep knowledge of the nature of systemic risk is needed for SIFIs to be identified, and suitable tools need to be developed to measure it. The academic literature in this area has evolved over recent years, offering different models or methodologies for evaluating the level of systemic risk for financial institutions. The profusion of different methodologies springs from the range of different sources or facets that systemic events can have, including size, contagion or interconnectedness, lack of substitute financial products, global cross-jurisdictional activity, and the complexity of business models. Indeed Bisias et al. (2012), in their survey of systemic risk analytics, identify 31 quantitative measures of systemic risk in the economics and finance literature, which can be classified in six homogeneous groups as macroeconomic measures like credit-gap indicators; cross-sectional measures, including the delta conditional value-at-risk (CoVaR) of Adrian and Brunnermeier (2016) and the systemic expected shortfall (SES) of Acharya et al. (2017); forward-looking risk measures like contingent claims analysis; measures of illiquidity and insolvency; stress tests scenarios; and network measures.====In this article, our interest is in the last of these, network measures that are based on the interconnections between financial institutions. Although this group of measures represents only a small part of the literature, it has always been, and still is, the subject of major studies. From the theoretical side, the literature on network measures of systemic risk is related to financial contagion with major contributions from Allen and Gale (2000), Freixas et al. (2000), Dasgupta (2004), Acemoglu et al. (2015), Glasserman and Young (2015) among others. The core of these papers is their analysis of the role played by the linkages between financial institutions in amplifying exogenous shocks that hit the system. The analyses generally consider various angles, looking at either the shape of the network and whether it is complete or incomplete, the level of uncertainty prevailing in the financial markets, or the complexity and concentration of the network (for a recent review, see Chinazzi and Fagiolo, 2015). Empirical papers on network measures of systemic risk contributions are also broad and can be categorised in two main groups by whether the input data sets are private or public. The first category of measures uses private data on contractual obligations to measure counterparty connections and establish a counterparty network graph. An illustrative example of such a graph is described in IMF (2009), where the systemic importance of a financial institution is approximated by the degree of connectivity of its associated node in the graph. The second category is based on publicly available data such as asset returns or credit default swaps. The differences between the many contributions arise from the econometric or statistical methodologies used to establish the network, which range from variance decomposition (Demirer, Diebold, Liu, Yilmaz, 2018, Diebold, Yılmaz, 2014) to tail risk dependencies (Betz, Hautsch, Peltonen, Schienle, 2016, Hautsch, Schaumburg, Schienle, 2015), to a combination of both approaches (Härdle et al., 2016), and to Granger-causality inference (Basu, Das, Michailidis, Purnanandam, 2017, Billio, Getmansky, Lo, Pelizzon, 2012, Etesami, Habibnia, Kiyavash, 2017).====This article is specifically devoted to the last group of contributions, which try to evaluate contributions to systemic risk using a Granger-causality network. The seminal paper is Billio et al. (2012), who propose that the systemic risk contribution of a given financial institution can be evaluated by its importance in a network built from pair-wise Granger-causality tests. More precisely, they define a statistic that is equal to the frequency of the statistically significant pair-wise Granger-causality relations, regardless of the direction of causality, in which an institution is involved. Thus higher values for this statistic correspond to more systemic financial institutions and lower values to less systemic ones. This statistic can be further disentangled by focusing on the direction of causality. With empirical applications using monthly returns data for hedge funds, broker/dealers, banks and insurers, the authors show that these statistics help in identifying periods of financial crisis, and have good out-of-sample predictive powers.====Nevertheless, some recent papers give a critical assessment of Granger-causality networks as used by Billio et al. (2012) for measuring systemic risk contributions (Basu, Das, Michailidis, Purnanandam, 2017, Etesami, Habibnia, Kiyavash, 2017). The focal point of the criticism is the pair-wise Granger-causality inference that underlies this approach, which can lead to spurious causalities that arise because of indirect contagion effects. Indeed, as underlined by Basu et al. (2017), the pair-wise approach evaluates the statistical association between any two institutions A and B, focusing on the direct connectivity between them and also on the indirect connectivities through all the other nodes, or institutions, in the network. Therefore, Granger-causality networks that are based on the direct and indirect effects do not reveal which institutions are the most systemic. Indeed, systemic institutions are central in spreading shocks through the whole system. The consequence on the empirical side is that the pair-wise approach generally leads to networks that are highly dense due to spurious causalities, with a potentially misleading ranking of the systemic importance of financial institutions. This stylised fact is well known in the statistical literature about Granger-causality inference and is usually tackled using conditional Granger-causality (Geweke, 1984) in a vector autoregressive (VAR) model. More precisely, the autoregressive equations of the bivariate Granger-causality tests, are extended using controlling variables that correspond to the lagged values of the returns on the other ==== institutions, where ==== is the total number of institutions in the system. However, with realistic large values of ====, a large dimensional VAR model is subject to overfitting with traditional estimation methods such as the least squares approach. Penalised least squares methods can be used to overcome the curse of dimensionality as proposed by Basu et al. (2017), but there is then a no less important challenge in the choice of the penalty parameter. Indeed there are many methods available for calibrating the penalty parameter, such as information criteria or cross-validation, and it is known from the statistical literature that the estimation results can be sensitive to the retained choice.====Our main goal in this article is to rehabilitate the pair-wise Granger-causality approach for the evaluation of systemic risk contributions by addressing these two shortcomings of indirect causalities and the curse of dimensionality. We show that when combined with the leave-one-out (LOO) concept, this approach is still valuable in providing consistent measures of contributions to systemic risk. Formally, for a given financial institution A, we introduce a new measure of systemic risk importance, which evaluates how far the total number of significant Granger-causalities breaks down when this institution is excluded from the system. We control for causalities between the remaining ==== institutions that arise from the indirect effect of financial institution A being excluded, using a conditional Granger-causality test. It may be noted that using the conditional version for each of the ==== pair-wise Granger-causality tests in the system, that excludes the financial institution A, allows us to clean all spurious causalities between the remaining ==== institutions that arise from the indirect effect of the institution A. Moreover, and importantly, this conditional version is free of the curse of dimensionality, as it only involves lagged values of the returns for financial institution A.====Empirical applications are conducted using daily market returns for a sample of 90 large banks from around the world. The data run from 12 September 2003 to 19 February 2018, and include the global financial crisis of 2007–2008. The dataset includes almost every global systemically important bank (G-SIBs) identified by the Financial Stability Board (FSB). The results show that our measure gives a meaningful ranking of the systemic importance of financial institutions that is found to be consistent with the ranking of G-SIBs provided by the FSB. Moreover, the new measure of systemic importance from the viewpoint of interconnectedness is shown to be a robust and significant early-warning indicator of large losses from a systemic event. The predictive power is larger than that associated with the measures in Billio et al. (2012). These results demonstrate that the pair-wise approach is more valuable when the effects of indirect causalities are cleaned out in a meaningful way.====Lastly, we search for the economic contents of our measure by estimating panel regression models with balance-sheet variables as predictors. The results show that our measure of systemic risk importance is strongly related to the size, the business model and the profitability of banks. In line with the existing literature on systemic risk, we find a positive and significant relationship between size, as measured by the logarithm of total assets, and our measure of systemic risk contribution. We also assess whether the business model of banks drives our measure of systemic risk. Like those of Brunnermeier et al. (2012) and Laeven et al. (2016), our results suggest that banks specialising in market-based activities tend to have a higher level of systemic risk than do banks specialising in traditional intermediation activities. Furthermore, we investigate the link between the profitability of banks, proxied by the return on equity, and their contribution to systemic risk. We find a positive and statistically significant relationship between these two variables.====It is worth noting that using the LOO approach is not new in the literature on systemic risk measures. Indeed, Zedda and Cannas (2017) employ this methodology to analyse systemic risk and the determinants of contagion in a banking system. Formally, they base their approach on a simulated distribution of the losses of the entire system, and of each subsystem in which one bank was removed. Recently, Li et al. (2017) also use the LOO concept applied to the ====-score, as measured by return on assets (ROA) plus the equity-to-assets ratio divided by the standard deviation of ROA. They define an aggregate ====-score for the whole system, and the “Minus one bank ====-score”, which is the ====-score of the system when one bank is removed. The difference between these two measures is the contribution of the removed bank to the systemic risk of the system. Note that as underlined by Zedda and Cannas (2017), the LOO methodology has some similarities to the Shapley (1953) value, which is used by many authors to measure systemic risk contributions (Drehmann, Tarashev, 2013, Tarashev, Borio, Tsatsaronis). Nonetheless, to the best of our knowledge, our paper is the first to mobilise the LOO concept for measuring systemic risk contributions using Granger-causality networks.====The remainder of the article is structured as follows. Section 2 is devoted to a review of the literature on network measures of systemic risk, covering both theoretical and empirical issues. Section 3 provides, in the line of Basu et al. (2017), a critical assessment of measures of systemic risk contributions based on pair-wise Granger-causality tests. In Section 4, we present the new measure based on the LOO approach, and we assess its reliability using real datasets in Section 5. Section 6 searches for the micro-economic determinants of the LOO measure using balance-sheet data, and the last section concludes the article.",Measuring network systemic risk contributions: A leave-one-out approach,https://www.sciencedirect.com/science/article/pii/S0165188918302100,March 2019,2019,Research Article,416.0
Iskrev Nikolay,"Banco de Portugal; Research in Economics and Management (REM); Research Unit on Complexity and Economics (UECE), Portugal","Received 20 July 2018, Revised 28 November 2018, Accepted 7 December 2018, Available online 10 December 2018, Version of Record 27 December 2018.",https://doi.org/10.1016/j.jedc.2018.12.002,Cited by (4),I propose two measures of the impact of calibration on the estimation of ,"It is a common practice in the empirical macroeconomic literature to mix estimation of some model parameters with calibration of others. The rationale behind this approach is either that some parameters are difficult to identify from available data, or that their values have been well-established elsewhere in the literature. While these may be reasonable arguments in some cases, the list of calibrated parameters often includes some for which the empirical evidence is far from settled, and whose values are simply taken from previous studies, often based on very different models and data patterns. Convenience and ease of estimation may be a more plausible explanation of the common practice of fixing some parameters than the possession of true knowledge of their values. It is therefore important to understand the impact, if any, parameter calibration has on model estimation.====The practice of mixing calibration and estimation can have two potentially important consequences. First, the values of the calibrated parameters may affect the point estimates of the free parameters.==== Thus, mis-calibration could result in biased estimates of some estimated parameters. Second, from the point of view of estimation, calibration of some parameters is equivalent to assuming that their values are known. This may introduce information about parameters that are estimated. Put differently, by eliminating all uncertainty with respect to calibrated parameters, one may also remove some of the uncertainty about freely estimated parameters.====Clearly, not all free parameters are affected equally by calibration. In general, the size of the impact will depend on the interactions between free and calibrated parameters in the context of a given model. Except in very simple cases with a small number of parameters, it is generally difficult to identify, by intuition or heuristic reasoning alone, which estimated parameters will be affected, in what way and by how much, as a result of calibrating one or more model parameters.====One possible way of quantifying the amount of information introduced by calibration is to re-estimate the model in the absence of calibration, and compare the resulting uncertainty with that of the restricted model. Similarly, the effect of changing the calibration values can be assessed be re-estimating the model multiple times conditional on different values of the fixed parameters. Whether or not these are reasonable ways to proceed depends on how feasible it is to estimate the larger unrestricted model, or to estimate multiple times the restricted model, and also how strongly one feels about the reasons for calibration in the first place. Note that estimating the unrestricted model is almost certain to result in point estimates of the previously fixed parameters that are different from the calibration values. This might be undesirable if one has strong views about what those values should be. Furthermore, the point estimates of at least some freely estimated parameters are likely to be different in the unrestricted model. This will complicate the comparison of the estimation uncertainty in the restricted and unrestricted cases.====The purpose of this paper is to present an alternative approach, which does not require estimating models more than once, and only uses the estimation results under the original calibration. The method is based on the asymptotic posterior distribution of the parameters in the unrestricted case, which is used to construct two different measures. The first is a measure of the amount of information gained with respect to each free parameter as a result of knowing the value of one or more calibrated parameters. It shows the reduction of asymptotic uncertainty as a percent of the uncertainty in the unrestricted case. The second is a measure of the sensitivity of parameter estimates to perturbations in the values of different calibrated parameters. In particular, it shows the sign and the magnitude of the response of different estimated parameters to changes in the values of the calibrated ones.====The intuition behind the proposed approach is simple: the effect of calibration will depend on how different parameters interact in a given model. From the point of view of estimation, these interactions are captured by the parameters’ impact on the model log-likelihood function. Closely-related parameters are difficult to distinguish on the basis of their effect on the log-likelihood. Fixing one or more of them provides a lot of information about the other related parameters, which are also very responsive to changes in the calibration values. The opposite holds true for unrelated parameters whose effects on the likelihood function are orthogonal to each other. For instance, consider a standard business cycle model. In such models there are typically a few parameters that determine the steady state of the economy. Calibrating some of them will naturally have a stronger impact on the other steady state-related parameters, both in terms of location and spread of their posterior distribution. On the other hand, more weakly-related parameters, such as variance coefficients of shocks, are likely to be unaffected.====The measures I propose formalize this intuition. Specifically, I use the asymptotic Gaussianity of the posterior distribution of the model parameters, and study the effect of calibration by comparing the mean and variance of the distribution in the unrestricted case to the same moments in the restricted case, i.e. conditional on some parameters being known and fixed. Simple closed-form expressions show that the impact of calibration depends on the model-implied interdependence between free and calibrated parameters, which is captured by the correlation structure of the asymptotic posterior distribution.====From a Bayesian perspective, calibration of some model parameters could be interpreted as having very strong prior beliefs about the values of those parameters. In this sense, my paper is similar to Müller (2012), who proposed measures of prior sensitivity and prior informativeness in Bayesian models. As Müller (2012) observes, “likelihood information about different parameters can be far from independent, so that the marginal posterior distributions crucially depend on the interaction of the likelihood with the whole prior.” The same argument implies that calibrating some parameters can have a significant impact on the posterior distributions of freely-estimated parameters. Unlike the sensitivity and informativeness measures in this paper, the measures of Müller (2012) cannot be applied to parameters that are held fixed during estimation since computing them requires sampling from the posterior distribution of the full parameter vector. As noted earlier, combining estimation, both frequentist and Bayesian, with calibration is a rather common practice in the DSGE literature, which makes my contribution complementary to that of Müller (2012).====In terms of methodology, my paper is most closely related to Andrews et al. (2017), who introduced a measure of sensitivity of parameter estimates to the empirical moments they are based on. The purpose of their analysis is to identify the most influential moments, which, if misspecified, could result in a large estimation bias. Even though my measure of sensitivity is with respect to calibrated parameters and not moments, its derivation is based on the same idea: I use the joint asymptotic distribution of free and calibrated parameters, whereas Andrews et al. (2017) use the joint asymptotic distribution of free parameters and empirical moments. In both cases sensitivity is measured locally and can be used as an indicator of how robust the estimation results are to small perturbations in either the calibration values or the moment conditions. My paper also shares Andrews et al. (2017) larger goal, namely, to help increase the transparency of estimated structural models by providing easy-to-use tools for assessing the importance of different estimation assumptions. In the context of DSGE models, I believe it is important for researchers to discuss not only the reasons for and methods of calibration, but also the likely impact of calibration on the estimation results. The measures derived in this paper serve precisely that purpose and can be easily incorporated into the standard estimation output usually reported in empirical DSGE research.====The remainder of the paper is organized as follows. Section 2 defines and motivates my measures of information gains and sensitivity. Section 3 illustrates the use of the proposed measures using two different DSGE models. The models are a new Keynesian model estimated in Smets and Wouters (2007), and a real business cycle model with news shocks estimated in Schmitt-Grohé and Uribe (2012). In each case I show how calibration used by the authors affects their estimation results. Section 4 offers some concluding remarks.",What to expect when you're calibrating: Measuring the effect of calibration on the estimation of macroeconomic models,https://www.sciencedirect.com/science/article/pii/S0165188918303907,February 2019,2019,Research Article,417.0
Mitra Shalini,"Management School, University of Liverpool, Chatham Street, Liverpool L69 7ZH, United Kingdom","Received 11 May 2018, Revised 20 September 2018, Accepted 24 September 2018, Available online 6 December 2018, Version of Record 10 January 2019.",https://doi.org/10.1016/j.jedc.2018.09.007,Cited by (2)," (IC) I show that a rise in the income share of IC in the production function, in line with data can account for a significant share of the increase in ==== volatility (both absolute and relative to income) and labor input volatility (relative to income) observed in the U.S. since the mid 1980s even as volatility of output declined. Intangible capital accumulates stochastically and similar to final goods requires physical capital, ==== and labor to produce. Under these conditions an increase in the share of IC in production increases the propagation of the IC-specific shock which raises (absolute and relative) wage and labor input volatility. The higher propagation of the IC shock also accounts for the large decline in the pro-cyclicality of labor productivity (relative to both output and labor) observed during this period.","Recent literature documented substantial changes in the dynamics of key labor market aggregates that accompanied the large drop in output volatility in the post-1984 period in the U.S. These changes are:====I argue in this paper that a rise in the importance of intangible capital (IC) in production in recent decades, can jointly account for the observed shifts in labor market dynamics along with the decline in aggregate output volatility that characterized the so called Great Moderation of this period.====Intangible capital, as defined in (McGrattan and Prescott, 2012) is “accumulated know-how from investing in research and development, brands, and organizations, which is for the most part expensed by companies rather than capitalized”. Hall, Hall, 2001 attributes the majority of the increase in the valuation of corporations in the 1990s to what he calls e-capital. Nakamura (2001) reports, using different estimates of intangible investments that the rate of such investments, and its economic value, accelerated significantly beginning around 1980. U.S. private gross investment in intangibles was at least $1 trillion by the end of 1999, same as business investment in traditional, tangible capital. This finding is matched by Corrado et al. (2005) while Corrado et al. (2009) find that IC’s share in income increased from 9.4% in the period 1973–1995 to 14.6% in 1995–2003. The ratio of intangible to tangible assets increased from 20% in the pre-1980 period to around 60% in 2010 (Corrado, Hulten, Sichel, 2009, Corrado, Hulten, 2010, Dottling, Perotti, Falato, Kadyrzhanova, Sim).====As expected, the share of employment in occupations that are predominantly associated with production of intangibles, as a fraction of total employment, also increased substantially during this period. Using occupation data from the Department of Labor’s March Current Population Survey (CPS), I split workers into two groups, (a) workers engaged in the creation of innovative property like engineers, architects, scientists, artists, entertainers and IT workers, and (b) organizational workers namely managers, marketers and human resource specialists all of whom are associated with developing economic competencies====. I term the total employment of the two groups together as IC employment. This is similar to the classification of IC related employment in Nakamura (2001). McGrattan and Prescott (2012) also use occupation data to show the shift in employment to IT sectors, that occurred in the 1990s. Here, I focus more broadly on the intangible capital revolution that began in the years leading up to the Great Moderation, and which includes but is not limited to the IT sector====.====Fig. 1 plots the evolution of employment of the two groups of IC workers in recent decades. Between 1970 and 2010, employment of group (a) doubled while group (b)’s employment increased by 65%. Total IC employment rose by a marked 80% during this period.====Two key features of IC investment that distinguishes it from investment in physical capital are, the former is associated with additional risks and, it requires labor, physical capital and intangible capital to produce (see for example Brynjolfsson, 2002; Eisfeldt and Papanikolaou, 2013; McGrattan and Prescott, 2012). In addition, IC itself is non-rival, in the sense that the same IC can be used for final good production as well as to produce more IC. I incorporate these features into an otherwise standard Real Business Cycle (RBC) framework by introducing an additional IC investment producing sector with an IC sector specific productivity shock (interchangeably called the IC shock throughout this paper).====The main result of this paper is driven by two observations. Firstly, a rise in the importance of IC in production increases the propagation of IC shocks in the model relative to neutral technology shocks. In other words, as IC’s share in production increases, the same change in IC has a larger impact on output. Secondly, output of the final goods sector falls upon impact of a positive IC shock. This is because more productive IC causes an increase in IC generation which requires labor to produce. Labor is reallocated from the final goods to the IC sector leading to a fall in final goods output upon impact of the IC shock. Thus a permanent increase in IC productivity causes an initial decline in final output, followed by a recovery leading to a higher level of output than before the shock.====The results of the paper follow from these two elements. As the share of IC rises, business cycles are progressively more driven by IC-specific shocks. This reduces the volatility of output compared to labor input (as output initially falls in response to the IC shock which increases labor input) and weakens the positive correlation between productivity and labor input, even making the correlation negative. Finally, although the cyclicality of productivity falls, it does not imply a decline in the volatility of real wage. In fact real wage volatility rises since the marginal product of labor, and therefore the wage, internalizes the effect of building up the IC stock for future production.==== – the literature exclusively attributes the shift in labor market dynamics to a rise in US labor market flexibility around this time. Gali and Rens (GVR from now on) (2014), show that these changes can be caused by a reduction in hiring costs arising from an increase in labor market turnover. Champagne and Kurmann (2013) and Nucci and Riggi (2011) both argue that a shift towards performance-pay contracts played an important role in the changing U.S. labor market dynamics. The former also use micro-data to establish the empirical evidence and show that changes in workforce composition did not play a role in the rising wage volatility of this period. Comin et al. (2009) associate the higher wage volatility with a general increase in firm level (profit-to-sales ratio or the growth rate of sales, employment or sales per worker) volatility. They too rule out any role played by compositional changes of the workforce and observe that the relationship between sales and wage volatility at the firm level is stronger since the 1980s and for services rather than manufacturing firms. To my knowledge, the current paper is the first to focus on the link between the rising importance of intangible capital and changes in labor market dynamics.====The mechanism in this paper can be likened to the “productivity slowdown” literature of the 1990s. The latter contends that exogenous technological progress initially reduces measured productivity through an increase in mismeasurement of one or more of, learning and quality (Hornstein and Krusell, 1996), investment for setting up and learning new technologies (Greenwood and Yorukoglu, 1997) or the aggregate capital stock (Mukoyama, 2005). My paper is most closely related to Greenwood and Yorukoglu, (1997) who argue that new technology requires investment in costlier skilled labor and particularly in learning which is expensed rather than capitalized, similar to IC investments in the current paper. More specifically, an exogenous technological progress in their paper, causes learning to increase which is unmeasured while benefits of the new technology are not completely realized leading to a slowdown in productivity. In the current environment, an exogenous increase in the importance of IC in production, leads to an increase in its investment which is unmeasured, like investment in learning in the previous paper. A key difference, however, is that benefits of learning dissipate with time in Greenwood and Yorukoglu, (1997) as knowledge of the new technology becomes widespread, whereas the benefits of IC investment, that is IC output, accumulates over time. As the importance of IC rises in my model, the higher accumulated stock of IC ensures an increase in output growth and productivity. Arguably however, productivity growth would be higher if IC investments were capitalized and not expensed. Finally, the mismeasurement in investment in the previous framework corrects itself over time since the new technology benefits become better measured and the cost of investing in learning (skill premium) falls as more people become familiar with the technology. In the current framework the mismeasurement in IC investment is permanent.====This paper is also related to the recent literature that includes IC in standard RBC models to shed light on otherwise puzzling business cycle phenomena. (McGrattan and Prescott, 2010), in such a framework, generate the observed boom of the 1990s. Without IC their model predicts a depressed economy in the 90s. McGrattan and Prescott (2012), using the same model, reassess the Great Recession of 2008–2009 and the slow recovery period from 2009 to 2011 and show that the inclusion of IC and non-neutral technology change in the production of final goods and services can account for the fact that labor productivity rose during the Great Recession even as GDP crashed. Thus these authors are the first to my knowledge to attribute the fall in the pro-cyclicality of labor productivity to a rise in the productivity of intangible capital. However, they do not consider the role of an IC shock and they focus on the productivity boom of the 1990s whereas my focus is on the period generally associated with the Great Moderation beginning in the mid 1980s. I choose this break date following common practice in the literature (Barnichon, 2010, Champagne, Kurmann, 2013, Gali, Gambetti, 2009) and GVR (2014) of dating the changes in labor market dynamics, including the vanishing pro-cyclicality of productivity, from the start of the Great Moderation, regarding the timing of which there is some consensus (McConnell, Perez-Quiros, 2000, Stock, Watson, 2003) . Finally, Gourio and Rudanko (2014) are able to account for the counter-cyclical and highly volatile labor wedge (ratio of the marginal rate of substitution of households and the marginal product of labor of firms) when they incorporate complementary IC production into a simple RBC framework.====The rest of the paper is organized as follows, Section 2 provides a summary of the changes in labor market dynamics documented in the literature for the pre and post-84 periods, Section 3 presents the model with IC and highlights the key channels through which labor market and other aggregates are affected, Section 4 analyzes quantitatively the impact of a rising share of IC in the model economy and Section 5 concludes.",Intangible capital and the rise in wage and hours volatility,https://www.sciencedirect.com/science/article/pii/S0165188918303890,March 2019,2019,Research Article,418.0
"Brand Thomas,Isoré Marlène,Tripier Fabien","CEPREMAP, 48 boulevard Jourdan, 75014, Paris, France,University of Helsinki, P.O Box 17, FI-00014 Helsinki, Finland & Bank of Finland, Finland,EPEE, Univ Evry, Université Paris-Saclay & CEPII. Bd François Mitterrand, 91025 Evry Cedex, France","Received 17 April 2018, Revised 28 September 2018, Accepted 16 November 2018, Available online 29 November 2018, Version of Record 14 December 2018.",https://doi.org/10.1016/j.jedc.2018.11.003,Cited by (11),"We develop a business cycle model where endogenous firm creation stems from two credit market frictions. First, entrepreneurs search for a lending relationship with a bank. Second, an optimal debt contract with monitoring is implemented. We analyze the interplay between both frictions, and embed it into an otherwise standard business cycle model which we estimate with ","A striking feature of the Great Recession in the US is the sharp drop in firm creation in 2008–2010 and its following slow recovery (Fig. 1). It is well known that the US economy has not recovered from the Great Recession as strongly as expected (e.g., Taylor, 2014), and the lack of firm creation has likely contributed to it (e.g., Gourio et al., 2016; Clementi and Palazzo, 2016). Meanwhile, the contemporaneous rise in credit spread has been extensively calling for models with macro-financial linkages. Uncertainty shocks are particularly interesting in these models as they are often found to contribute to both macroeconomic and financial dynamics. This includes Christiano et al.’s (2014) (henceforth, CMR) so-called “risk shocks”, defined as changes in the volatility of firms’ idiosyncratic productivity.==== Yet, this literature mostly ignores the effects of uncertainty shocks on the extensive margins of activity, i.e. firm creation, so far.====In this paper, we thus ask whether uncertainty shocks can explain the drop in firm creation observed in the data, along with the increase in credit spread, during the Great Recession. We develop a general equilibrium model where the credit market is characterized by an interplay between two frictions. First, a ==== friction between entrepreneurs and financial intermediaries (or “banks”, for short). It allows us to endogenize an entry decision which depends on expected costs and gains of long-term lending relationships. Second, a match coincides with the implementation of a loan contract under a ==== (henceforth, CSV) problem ã la Townsend (1979). In short, banks do not observe the idiosyncratic productivity of entrepreneurs and therefore have to monitor them in case of default. Although this latter mechanism has become standard in macroeconomic models (e.g., Carlstrom and Fuerst, 1997; Bernanke et al., 1999 – henceforth, BGG), it is usually restricted to ==== firms in the economy only, whereas we also consider its effects on firm ==== here. By combining these two financial frictions, we analyze how search frictions affect the optimal terms of the CSV contract, but still nest the searchless economy as a limit case. We then embed these credit market features, namely search and monitoring, into an otherwise standard Dynamic Stochastic General Equilibrium (DSGE) model, which we estimate with Bayesian techniques for the US economy over the period 1980Q1–2016Q4.====The intuition on the effects of a risk shock is as follows. By definition, a risk shock increases the cross-sectional dispersion of entrepreneurs’ productivity. In the CSV setup, this implies a higher loan default rate. As a consequence, the credit spread goes up to ensure the participation of bankers. Therefore, the demand for credit falls, leading to a macroeconomic downturn, characterized by drops in investment and output. In terms of firm dynamics, the entrepreneurial activity becomes less profitable, both because of the macroeconomic contraction and because bankers’ share increases. For new potential entrepreneurs, searching for a lending relationship is thus less attractive, such that firm creation slows down. Simultaneously, defaults on loans become more frequent and are associated with an increase in firm destruction. The combination of lower firm creation and higher firm destruction overall contributes to a persistent decline in the number of productive firms in the economy. Overall, uncertainty shocks thus generate appealing responses of both macro-financial aggregates and firm dynamics.====From a quantitative point of view, our main results are as follows. First, uncertainty shocks turn out to be a major business cycle contributor of both macro-financial aggregates and firm dynamics. Indeed, the variance decomposition reveals that they are the first contributor to business cycles fluctuations, not only for credit spread and credit growth, in line with the literature, but also for firm creation. Second, during Great Recession episode in particular, uncertainty shocks explain most of the initial drop in firm creation and output, together with the rise in credit spread. However, they rapidly vanish in the aftermath of the crisis, while firm creation remains low due to other reasons, such as productivity and investment efficiency shocks in particular. Third, as for the importance of the credit search friction, we find that an average entrepreneur in our sample pays two third of a quarterly income over the search for its lending relationship. Finally, we show that the credit search friction tends to dampen the financial acceleration effect of uncertainty shocks on macro-financial variables as compared to a model with CSV only. Indeed, entrepreneurs chose to default less when default is associated with a risk of losing their lending relationship and having to search for a new one.====The paper continues as follows. Section 2 reviews the related literature. Section 3 presents the core of our model, which consists of the optimal loan contracting problem between entrepreneurs and banks in the presence of search frictions. The rest of the general equilibrium environment is standard and relegated to Appendix. Section 4 provides a Bayesian estimation of the model and simulations of an uncertainty shock in particular. Section 5 presents counterfactual exercises in order to explicit the mechanism at play in the model, as well as external sectoral-level data evidence on the role of uncertainty shocks on firm dynamics. Finally, Section 6 concludes.",Uncertainty shocks and firm creation: Search and monitoring in the credit market,https://www.sciencedirect.com/science/article/pii/S0165188918303798,February 2019,2019,Research Article,419.0
"Kraft Holger,Weiss Farina","Goethe University, Faculty of Economics and Business Administration, Theodor-W.-Adorno-Platz 3, 60323 Frankfurt am Main, Germany","Received 31 March 2018, Revised 17 September 2018, Accepted 18 September 2018, Available online 22 November 2018, Version of Record 28 November 2018.",https://doi.org/10.1016/j.jedc.2018.09.006,Cited by (9),This paper studies a consumption-portfolio problem where money enters the agent’s utility function. We solve the corresponding Hamilton–Jacobi–Bellman equation and provide closed-form solutions for the ,"Following the classical work by Merton, 1969, Merton, 1971, most papers that study consumption-portfolio decisions disregard money. However, as for instance pointed out by Obstfeld and Rogoff (1996, p. 513), “many of the most intriguing and important questions in international finance involve money”. Now, there are several ways to take money into account. One approach assumes that money enters the utility function. This idea of modeling preferences for liquidity is well established. Starting with Sidrauski (1967) and Brock (1974), a strand of macroeconomic literature uses “money in the utility function” to address economic problems involving monetary issues.==== Recent papers in financial economics applying this approach involve Balvers and Huang (2009) and Gu and Huang (2013), among others. Intuitively, this approach could be justified by the implicit assumption that the agent has utility from both consumption and leisure. Real money balance thus enters the utility function indirectly since agents save time in conducting their transactions when holding cash.====Despite its widespread use in the theory of monetary policy, a rigorous treatment of consumption-portfolio decisions with preferences for cash is missing. Our paper fills this gap and studies a canonical consumption-portfolio problem of a household with these preferences. We provide closed-form solutions in both the finite- and infinite-horizon case and also prove formal verification theorems showing that these solutions are indeed optimal. For the infinite-horizon case, the optimal stock demand is characterized by a polynomial. More precisely, it is one particular root of this polynomial. For the finite-horizon case, we show that the optimal stock demand is given by the inverse of the solution to an ordinary differential equation. The solution to the differential equation can be calculated explicitly. Since individual optimality of household decisions is part of every general equilibrium analysis, our analysis also contributes to this field.====From an economic point of view, we find that in the finite-horizon setting preferences for cash lead to time-dependent risky portfolio shares that are decreasing in age for realistic calibrations. Such a pattern is qualitatively in line with rules of thumb that the risky share should decrease in age. It is also supported by recent empirical evidence showing that households who participate in the stock market have high and fairly constant risky shares during young ages, while investors reduce their risky share at a steady pace from about age 45 until they reach retirement (see Fagereng et al., 2017). Finally, we also study the decision problem of an agent who has recursive utility and preferences for cash. In this setting, we provide evidence that our findings are robust to varying the elasticity of intertemporal substitution.====Our paper is related to Dixit and Goldman (1970) who also assume that money enters the agent’s utility. However, they consider a discrete-time setup and analyze first-order conditions only. In contrast to our paper, they cannot solve the model further and do not provide closed-form solutions or a verification theorem. Other related papers that assume that money provides utility are Fama and Farber (1979) and LeRoy, 1984a, LeRoy, 1984b. Fama and Farber (1979) study an economy where the government provides money supply in a partial equilibrium model while LeRoy, 1984a, LeRoy, 1984b consider a similar general equilibrium setup. An alternative way to derive a demand for money is the cash-in-advance approach. In this approach, money is necessary to make transactions and does not directly affects the utility (see, e.g., Lucas, 1982 and Svensson, 1985).====The remainder of the paper is structured as follows: Section 2 describes our framework. Section 3 provides and solves the Hamilton–Jacobi–Bellman equation of the agent’s consumption-portfolio problem. It also studies some properties of the first-order conditions. Sections 4 and 5 analyze the infinite- and finite-horizon case in detail and derive the candidates for the optimal controls. Section 6 shows that these candidates are indeed optimal and proves the corresponding verification results. Section 7 studies the decision problem of an agent with recursive utility. Section 8 provides numerical examples both for time-additive and recursive utility. Section 9 concludes. All proofs can be found in the Appendix.",Consumption-portfolio choice with preferences for cash,https://www.sciencedirect.com/science/article/pii/S0165188918303804,January 2019,2019,Research Article,420.0
"Huber Samuel,Kim Jaehong","Department of Economic Theory, University of Basel, Switzerland,Wang Yanan Institute for Studies in Economics and the School of Economics, Xiamen University, China","Received 7 February 2018, Revised 12 July 2018, Accepted 28 August 2018, Available online 16 November 2018, Version of Record 5 December 2018.",https://doi.org/10.1016/j.jedc.2018.08.012,Cited by (3),"We develop a dynamic general equilibrium model to analyze the role of trading frictions in over-the-counter markets. In our model, agents face idiosyncratic preference shocks and a financial market allows them to rebalance their portfolio composed of money and bonds in response to these shocks. We disentangle the effects of search and bargaining on welfare and study each one of them. We show that bargaining is generally welfare-improving, while search frictions are not. This is because search frictions do not only affect the demand for the respective assets, but also their allocation.","In models of money, the inflation tax drives a wedge between the private and social benefit of carrying money; hence, people will tend to hold too little. The existence of a secondary financial market, where people can sell assets for money when they need it, makes it easier to avoid storing wealth as money, and thereby to avoid the inflation tax. However, in equilibrium, ==== has to hold money, hence, the overall impact of the inflation tax can be magnified through secondary financial markets; thus, their existence can be good or bad for welfare. The question for this paper is whether certain frictions in the secondary financial market, which make it harder to trade for some or for all agents, are likely to be good or bad.====In our model, agents are hit with idiosyncratic preference shocks: some become producers of a special good, others become consumers of this good. Producers and consumers meet bilaterally, and due to anonymity and limited commitment, they need a medium of exchange (“money”) to conduct their trade. With inflation above the Friedman rule, money is costly to hold, hence agents tend to hold too little of it. After it is revealed that an agent is a consumer, she may wish to sell other assets (“bonds”) in a financial market in order to obtain more money for consumption (referred as “liquidity demanders”, hereafter).==== That market can only be accessed with a certain probability, and the bond price may be determined competitively or with Kalai bargaining.====We show that for any access probability, going from competitive trading to bargaining is good for welfare, and reducing the bargaining power of liquidity demanders is generally good for welfare, too. Whether or not search frictions are welfare-improving depends on the bargaining power of liquidity demanders. If their bargaining power is not too high, search frictions are not welfare-improving since the negative distributional effect of having fewer trades dominates the positive general equilibrium effect on the demand for the respective assets.====The fact that some trading frictions can be welfare-improving is not new. However, all the papers that have studied this issue so far simply speak about “search and bargaining” taken together, and ignore the specific effects of each one of them. Indeed, a model that contains search gives rise to bilateral meetings, and when one has bilateral meetings, bargaining theory is the most obvious tool that one can use in order to study how the terms of trade within the pair are determined. But search does not necessarily imply bargaining, and even if it did, one still needs to understand which results in our theories stem from search (i.e., the difficulty to find trading partners), and which ones stem from bargaining (i.e., the fact that prices are not determined in a competitive way). To our knowledge, we are the first to disentangle the effects of search and bargaining on welfare. We find that their impact is quite different.====Bargaining determines how agents split the trade surplus in the financial market. The lower the bargaining power of liquidity demanders, the lower the demand for bonds in the primary market, which depresses their issuance price. Hence, bargaining increases the demand for money, which marginally increases its value and thereby improves self-insurance against idiosyncratic preference shocks of all market participants. This effect is so strong that bargaining is welfare-improving for a wide range of parameter values.====Search frictions determine how easy it is to find a trading partner in the financial market. Higher search frictions imply that there is a lower probability of agents to trade; therefore, bonds become less attractive. Consequently, higher search frictions also increase the demand for money and so its value (as in the case of bargaining). However, we show that search frictions are generally not welfare-improving.==== This is because the negative distributional effect of having fewer trades in response to an increase in search frictions dominates the positive effect on the demand for money.",The role of trading frictions in financial markets,https://www.sciencedirect.com/science/article/pii/S0165188918303786,February 2019,2019,Research Article,421.0
"Arouri Mohamed,M’saddek Oussama,Pukthuanthong Kuntara","GRM, Université Côte d'Azur, Nice, France,CRCGM, Université Clermont Auvergne, Clermont-Ferrand, France,IPAG Business School, 184 Boulevard Saint-Germain, Paris 75006, France,University of Missouri, Columbia, United States","Received 8 February 2018, Revised 12 September 2018, Accepted 10 November 2018, Available online 15 November 2018, Version of Record 24 November 2018.",https://doi.org/10.1016/j.jedc.2018.11.002,Cited by (11),"This paper examines the patterns of intraday cojumps between international equity markets as well as their impact on international asset holdings and portfolio diversification benefits. Using intraday index-based data for exchange-traded funds as proxies for international equity markets, we document evidence of significant cojumps, with the intensity increasing during the global financial crisis of 2008–2009. The application of the Hawkes process also shows that jumps propagate from the US and other developed markets to emerging markets. Correlated jumps are found to reduce diversification benefits and foreign asset holdings in minimum risk portfolios, whereas idiosyncratic jumps increase the diversification benefits of international equity portfolios. In contrast, the impact of higher-order moments induced by idiosyncratic and systematic jumps on the optimal composition of international portfolios is not significant.","It is now well established in the finance literature that price discontinuities or jumps should be taken into account when studying asset price dynamics and allocating funds across assets (Bekaert, Erb, Harvey, Viskanta, 1998, Branger, Muck, Seifried, Weisheit, 2017, Das, Uppal, 2004, Guidolin, Ossola, 2009). In this regard, the recent development of non-parametric jump identification tests has enabled jump detection in financial asset prices. The seminal works in this area include (Barndorff-Nielsen, Shephard, 2004, Barndorff-Nielsen, Shephard, 2006a) who test for the presence of jumps at the daily level using measures of bipower variation. The same family of intraday jump identification procedures includes the tests developed by, among others, Jiang and Oomen (2008), Andersen et al. (2012), Corsi et al. (2010), Podolskij and Ziggel (2010), and Christensen et al. (2014). Andersen et al. (2007) and Lee and Mykland (2008) have developed techniques to identify intraday jumps using high frequency data. All of these jump detection techniques provide empirical evidence in favor of the presence of asset price discontinuities or jumps.====More recently, researchers have been interested in studying cojumps between assets (Aït-Sahalia, Xiu, 2016, Dungey, Hvozdyk, 2012, Dungey, McKenzie, Smith, 2009, Lahaye, Laurent, Neely, 2010, Pukthuanthong, Roll, 2015). For instance, Gilder et al. (2012) examine the frequency of cojumps between individual stocks and the market portfolio. They find a tendency for a relatively large number of stocks to be involved in systematic cojumps, which are defined as cojumps between a stock and market portfolio. Lahaye et al. (2010) show that asset cojumps are partially associated with macroeconomic news announcements. Ait-Sahalia et al. (2015) develop a multivariate Hawkes jump-diffusion model to capture jumps propagation over time and across markets. They provide strong evidence for jumps to arrive in clusters within the same market and to propagate to other international markets. Bormetti et al. (2015) find that Hawkes one-factor model is more suitable to capture the high synchronization of jumps across assets than the multivariate Hawkes model.====Our study furthers the above-mentioned literature in two ways. First, we empirically investigate intraday cojumps between international equity markets. Second, we show their impact on international asset allocation and portfolio diversification benefits. To the best of our knowledge, we are the first study that examines the impact of intraday cojumps on portfolio allocation decisions in an international setting. Past studies focus more on the impact of return correlation without separating between continuous and jump parts. Modern portfolio theory suggests that international diversification is an effective way to minimize portfolio risks given that international assets are often less correlated and driven by different economic factors. However, one might expect that cojumps can lead to an increase in the correlation between these international assets and thus reduce the benefit from international diversification. Inversely, if price jumps of different assets do not occur simultaneously, they are categorized as idiosyncratic jumps and will not affect portfolio allocation decisions in an international setting. Choi et al. (2017) show, in contrast to traditional asset pricing theory and in support of information advantage theory, concentrated investment strategies in international markets are associated with higher risk-adjusted returns. Our study complements their study by showing investors prefer concentrated portfolios tilted toward home market because cojumps between home and foreign stock markets significantly reduce diversification benefits.====Accordingly, a risk-averse investor who holds an international portfolio is exposed to two types of jump risks: cojump or systematic jump risk (jumps common to all markets) and idiosyncratic jump risk (jumps specific to one market). If an investor’s portfolio is well diversified, the idiosyncratic jump risk will be reduced or even eliminated. On the other hand, the cojump risk cannot be eliminated through diversification, thus making its identification central to asset pricing, asset allocation and portfolio risk hedging. Identifying cojumps is also important for policymakers attempting to propose the policies that stabilize financial markets.====Our empirical tests rely on the use of intraday returns for three dedicated international exchange-traded funds (ETFs) – SPY, EFA, and EEM – which respectively aim to replicate the performance of three international equity market indices: S&P 500, MSCI EAFE (Europe, Australasia and Far East), and MSCI Emerging Markets.==== We use the technique proposed by Andersen et al. (2007) and Lee and Mykland (2008) to empirically identify all intraday jumps and cojumps of the three funds from January 2008 to October 2013. Lee and Mykland (2008) show that the power of their non-parametric jump identification test increases with the sampling frequency and that spurious detection of jumps is negligible when high frequency data are used. Unlike Ait-Sahalia et al. (2015) who use low frequency data to study the dynamics of jumps, we employ a bivariate Hawkes model to reproduce the time clustering features of intraday jumps and the dynamics of their propagation across markets. The application of the Hawkes process allows us to capture the dependence between the occurrences of jumps which cannot be reproduced by, for example, the standard Poisson process, owing to the hypothesis of independence of the increments (i.e., the numbers of jumps on disjoint time intervals should be independent). Under this analysis, we find jumps from the US propagate to other developed markets and emerging markets. However, the evidence of jump spillover from emerging markets to developed markets is weak.====Finally, we assess the impact of cojumps on international portfolio allocation by considering a domestic risk-averse investor who selects the portfolio composition based on one domestic asset and two foreign assets in a way to maximize his expected utility.====As investors are concerned about negative movements of asset returns, we take the risk of extreme events into account using the Conditional Value-at-Risk or CVaR (Rockafellar and Uryasev, 2000) as a risk measure in our portfolio allocation problem. Unlike the standard mean-variance approach, which typically underestimates the risk of large movements of asset returns, the mean-CVaR approach allows us to provide a fairly accurate estimate of the downside risk induced by negative cojumps of asset returns. As to cojumps, we apply two approaches to assess how assets jumps are linked to each other. The first one is cojump intensity measure obtained from the co-exceedance rule (Bae et al., 2003) and univariate jump identification tests proposed by Andersen et al. (2007) and Lee and Mykland (2008). The second one is based on the realized jump correlation measure proposed by Jacod and Todorov (2009). Contrary to the first approach that only measures the frequency of simultaneous jumps, the second approach captures both the intensity and size effects of cojumps. It has also the advantage to be robust to jump identification tests.====Once the optimal portfolio composition is determined, we analyze how jumps and cojumps affect investor demand for domestic and foreign assets. Our results show evidence of a negative and significant link between the demand for foreign assets and the jump correlation between the domestic and foreign markets. We also find a negative effect of cross-market cojumps on diversification benefits. In contrast, we find that idiosyncratic jumps have a positive effect on foreign asset holding and diversification benefits.====We also examine how higher-order moments induced by idiosyncratic and systematic jumps affect the optimal portfolio composition. For this purpose, we consider an investor who recognizes idiosyncratic and systematic jump risks and assumes that asset returns are given by a multivariate jump-diffusion process as well as another investor who ignores jumps and assumes a pure-diffusion process for asset returns. Both investors have a CRRA utility function and select their respective portfolios composition in a way to maximize their respective expected utilities. Our results show that both investors have almost the same portfolio composition, which typically indicates that the impact of jump higher-order moments on optimal portfolio composition is not significant.====The remainder of the paper is organized as follows. Section 2 introduces the jump and cojump identification techniques used in our study. Section 3 presents the portfolio allocation problem. Section 4 describes the data. Section 5 discusses our main empirical findings. Section 6 concludes the paper.",Cojumps and asset allocation in international equity markets,https://www.sciencedirect.com/science/article/pii/S0165188918303774,January 2019,2019,Research Article,422.0
"Berthod Mathias,Benchekroun Hassan","CEE-M, Univ. Montpellier, CNRS, INRA, SupAgro Montpellier, 2 place Pierre Viala, Montpellier, 34060, France,Department of Economics and CIREQ, Mc Gill University, 855 Sherbrook Street West, Montreal, Quebec, H3A 2T7, Canada","Received 19 May 2018, Revised 18 October 2018, Accepted 1 November 2018, Available online 13 November 2018, Version of Record 27 November 2018.",https://doi.org/10.1016/j.jedc.2018.11.001,Cited by (2),"We consider a nonrenewable resource ==== with economic exhaustion. We characterize the set of Pareto efficient equilibria. We show that when firms are sufficiently patient, there exists no Pareto efficient agreement that yields short-run gains with respect to the noncooperative equilibrium. Given a pair of stocks, there exists a unique interior Pareto efficient agreement. We characterize the set of stocks where a Pareto efficient agreement results in larger discounted sum of profits for both players. We show that social welfare under the interior Pareto efficient agreement is smaller than under non-cooperation, despite the gains from a more cost effective extraction of the resources under an agreement.","Nonrenewable resources markets have key ingredients for the emergence of explicit cooperative agreements between oligopolists in order to control the overall quantity supplied in the market. In the case of nonrenewable resources, entry is limited by the resource endowment and very few countries can own large shares of resource. Furthermore, agreements between sovereign countries are typically not subject to national antitrust laws. The Organization of Petroleum Exporting Countries (OPEC) which controls around 40 percent of the current crude oil production is perhaps the most famous resource cartel. However, the oil market is not the exception: the markets of mercury, uranium, diamond, bauxite or copper have each experienced at least a period of successful cartelization====.====In this paper we characterize the set of potential agreements, i.e, Pareto efficient equilibria, between cartel members in a nonrenewable resource duopoly.====In the literature, a cartel is often modeled as a strong and monolithic coalition. For instance, the “cartel versus fringe models”, following the seminal papers of Salant (1976) and Ulph and Folie (1980), consider a coherent cartel, as one player, facing a large number of firms acting in pure competition, see also Kagan et al. (2015) for a more recent contribution. Another branch of literature, following Gilbert (1978), Newbery (1981) and more recently Groot, Withagen, De Zeeuw, 2000, Groot, Withagen, De Zeeuw, 2003, considers the case of a first-mover cartel in a Stackelberg equilibrium. However, in these papers as well, the leader or the cartel is assumed to be a coherent structure and the agreement between cartel members is not examined.====To the best of our knowledge, only two references consider a cartel as the result of a bargaining process. Hnyilicza and Pindyck (1976) model two firms with asymmetric discount rates. They find a “bang-bang” solution consisting for the firms to produce in alternation. This solution may not be realistic “since the temptation to cheat would be considerable” (Hnyilicza and Pindyck, 1976, p. 147). Okullo and Reynés (2016) consider a framework that captures important features that are relevant in the case of the oil market (e.g., the cartel faces a fringe with a capacity constraint, endogenous reserve development) in order to specifically study the OPEC cartel. They introduce a coefficient of cooperation and select the level of the coefficient using the Nash Bargaining solution. The equilibrium under non-cooperation is an open-loop equilibrium where each player chooses a production path at time zero. In our paper, the noncooperative equilibrium we consider is a vector of Markovian stationary strategies and therefore it is subgame perfect. Moreover in contrast with both papers where the results are established using numerical simulations, our results are shown analytically.====We use a cooperative differential game framework, see e.g., Hämäläinen et al. (1985) for a seminal application in the case of the fisheries, Leitmann (1974) and more recently Reddy and Engwerda (2013) for influential methodological contributions. We consider cooperation between two firms, each owning a private stock of a nonrenewable resource. We examine the case of economic exhaustion or scarcity rather than physical exhaustion of the resource: the amount of the resource used by each firm is endogenously determined and is dictated by market conditions as well as how the extraction cost is affected by the level of the stock of the resource available. We make use of a necessary condition to Pareto optimality in cooperative differential games recently found in Reddy and Engwerda (2013) to fully characterize the set of all Pareto optimal payoffs and study group and individual rationality of the potential agreements. The noncooperative scenario has been already described by Salo and Tahvonen (2001).====In a cooperative differential game, players negotiate to establish a cooperative agreement on how to produce through time and how to share the total payoff. The cooperative agreement must satisfy group rationality and individual rationality (Yeung and Petrosyan, 2006 or Zaccour, 2008). To achieve group rationality, the outcome has to satisfy Pareto optimality. Indeed, the objective of cooperation is to reach a Pareto efficient allocation, which is, in general, unattainable in a noncooperative market structure. To achieve individual rationality, both players’ payoffs must be at least equal to what they can get in the noncooperative equilibrium. Otherwise, the cooperation is not self-enforced. Inspired by the context of international markets of nonrenewable resources, we rule out direct monetary transfers between players.====Focusing first on group rationality, we find that for each ratio of resource stocks, there exists a unique interior Pareto solution. All the remaining solutions are corner ones, involving that one of the firms does not produce for a period of time. This agreement has several properties which have been corroborated in empirical studies of resource cartels. First, each firm should get a share of total production exactly equal to its share of the total nonrenewable resource stock of the cartel. This result is often met as an assumption in the literature (Mason and Polasky, 2005 or Wiggins and Libecap, 1987) and is highly suspected to be the policy currently run by OPEC. Second, as an immediate consequence, the cooperative agreement presents a bias in favor of the small firm which also finds empirical evidence (Griffin and Xiong, 1997).====Checking the cooperative agreements that satisfy individual rationality, we find that the interior cooperative agreement may be non-acceptable for the firm with the largest resource stock. Specifically, if firms are too asymmetric, the player with the larger stock will have a discounted sum of profits lower than in the noncooperative equilibrium. Motivated again by the context of the countries involved in nonrenewable resource markets, we further examine the impact of an agreement on short-run profits. In many instances countries face budgetary constraints and the income derived from the natural resource sector represents a substantial share of their gross domestic product. It is therefore highly unlikely that a country would accept an agreement that involves a short-run loss of income with respect to the noncooperative equilibrium even if this loss will be more than compensated by future gains. We call this condition the immediate individual rationality, i.e., at the initial time of the agreement instantaneous profits are required to be larger under cooperation than under non-cooperation. We show that when the discount rate is small enough, there exists no Pareto efficient agreement such that the condition of short-run individual rationality is satisfied. This is a rather surprising result that runs against the general wisdom from the Folk Theorem in game theory whereby more patience is viewed as more conducive to cooperation. In our case players are not using punishment strategies with delay of detection. In our game, when players are sufficiently patient they have an incentive to postpone the extraction of the resource since a larger stock in the ground allows to extract at a lower cost. This incentive to conserve the resource can be strong enough to imply a high initial reduction of extraction with respect to the noncooperative equilibrium which ultimately results in smaller short-run profits.====Several aspects of these contributions may find empirical applications. The literature that tests the degree to which OPEC can control oil prices is very controversial. Starting with the seminal papers by Griffin (1985) and Loderer (1985), no consensus about OPEC’s behavior really emerged until now, see for instance Golombek et al. (2018), Loutia et al. (2016) and Kisswani (2016) for recent works about this topic. In that respect, the interior cooperative agreement we characterize offers a new opportunity to test wether OPEC acts as a cartel. The second application concerns the literature that estimate OPEC’s gains from cartelization. In particular, Pindyck (1978) finds gains of 50%, Griffin and Xiong (1997) find 24% and Berg et al. (1997) find 18%. However, these authors compare the net present values for a monolithic cartel against pure competition. Our results suggest that the gain is also sensitive to the asymmetry of a cartel.====The remainder of the paper is the following. In Section 2, we expose the model and characterize a unique interior Pareto solution for any ratio of initial resource stocks. In Section 3, we focus on the individual rationality of the feasible cooperative agreements. In Section 4, we conduct a welfare analysis. In Section 5, we make some concluding remarks.",On agreements in a nonrenewable resource market: A cooperative differential game approach,https://www.sciencedirect.com/science/article/pii/S016518891830366X,January 2019,2019,Research Article,423.0
"Da-Rocha José-María,Restuccia Diego,Tavares Marina Mendes","ITAM, Av. Camino Santa Teresa, 930 C.P., Mexico D.F. 10700, Mexico,ECOBAS, Facultad CC Economicas y Emp., Universidade de Vigo, Campus Universitario Lagoas-Marcosende, Vigo 36310, Spain,Department of Economics, University of Toronto, 150 St. George Street, Toronto, ON M5S 3G7, Canada,NBER, Cambridge, MA, United States,International Monetary Fund, 700 19th Street, N.W., Washington, D.C. 20431, United States","Received 21 June 2018, Revised 18 August 2018, Accepted 14 September 2018, Available online 13 November 2018, Version of Record 29 November 2018.",https://doi.org/10.1016/j.jedc.2018.09.005,Cited by (15),"We study the impact of firing costs on aggregate total factor productivity (TFP) in a dynamic general-equilibrium framework where the evolution of establishment-level productivity is not invariant to the policy. Firing costs not only generate static factor misallocation, but also distort the selection of establishment’s growth by size, contributing to larger aggregate TFP losses. Numerical experiments indicate that firing costs equivalent to 5 year’s wages imply a reduction in TFP of more than 20%. Factor misallocation accounts for 20% of the productivity loss, whereas the remaining 80% arises from distorted selection in the productivity process.","A fundamental issue in economic growth and development is identifying the policies and institutions that account for the large differences in total factor productivity (TFP) and output per capita across countries. A recent literature has emphasized factor misallocation across heterogeneous production units for aggregate TFP differences (Banerjee, Duflo, 2005, Hsieh, Klenow, 2009, Restuccia, Rogerson, 2008). While the empirical evidence of factor misallocation across countries is overwhelming, the connection with the specific policies and institutions that create the bulk of misallocation remains elusive.==== In this paper, we study impact of a specific policy—firing costs—on aggregate TFP in a framework where the evolution of establishment-level productivity is not invariant to the policy. We focus on firing costs because unlike other specific policies, firing costs are easily measurable in the data, show substantial variation across countries, and have been studied extensively in the misallocation literature. While there is some uncertainty about the exact quantitative magnitude of the channel we emphasize, we show through a series of numerical experiments that empirically-plausible measures of firing costs can generate large aggregate TFP loses arising from distorted selection in the evolution of establishment’s productivity, in contrast to the relatively small impact of firing costs on aggregate productivity found in earlier studies where the evolution of establishment-level productivity is invariant to the policy.====We consider an otherwise standard model of producer heterogeneity building on the seminal works of Hopenhayn (1992) and Hopenhayn and Rogerson (1993). For analytical tractability, the model is set up in continuous time. Establishments are heterogeneous in their TFP that follows a stochastic process over time. Crucially, and differently from the related previous literature, policies that distort the size of establishments such as firing costs, have an effect on the evolution of productivity for individual establishments and, hence, on the stationary distribution of productivity and aggregate TFP. A well-known property of firing costs in the context of dynamic models of producer heterogeneity is that these policies generate an inaction zone in employment decisions whereby some small but productive establishments remain small and some large but relatively less productive establishments remain large. In other words, the inaction zone is a range of productivities for which even establishments with the same productivity have different employment levels. This type of inaction in employment decisions to changes in productivity generates factor misallocation, as the policy weakens the relationship between the allocation of employment and establishment productivity; but in particular, inaction creates rank reversals—the situation where some relatively more productive establishments are smaller than some relatively less productive establishments and vice versa—that is shown to be important in generating large output losses from misallocation (Hopenhayn, 2014b). In addition, in our framework, we emphasize that firing costs also alter the selection of establishment’s productivity growth contributing to a substantial reduction in aggregate productivity that is beyond static factor misallocation.====Our model also differs from the literature on firing costs in that rather than a continuous choice of employment, we assume that establishments choose among a finite discrete set of employment levels. We make this assumption for tractability in order to be able to characterize analytically the distribution of establishment-level productivity in an environment where the employment demand is a correspondence. An implication of this assumption is that discrete employment levels together with production heterogeneity generate dispersion in the value of marginal products even in an undistorted economy with no firing costs. This dispersion can be interpreted as arising from “real” adjustment costs and is not treated as misallocation in our analysis since we emphasize the impact of firing costs on output and productivity relative to a benchmark economy with no firing costs. Importantly, firing costs create misallocation by inducing inaction in employment decisions but when interacted with discrete employment levels, they economize on adjustment costs which are present in the undistorted economy. As a result, to the extent that in real economies there are frictions to the adjustment of establishment employment that are not due to policies, our model with discrete employment levels can potentially provide a more conservative estimate of firing-cost policies than in a setting with continuous employment choices.====To study the importance of firing-cost policies, we conduct a series of numerical experiments in the context of a benchmark economy with no firing costs. In particular, we calibrate a benchmark economy with no firing costs to micro and macro data for the United States and consider quantitative experiments that increase the size of firing costs—the cost for an individual establishment to reduce employment—with a range from 6 months to 5 year’s wages. Relative to the benchmark economy with no firing costs, aggregate TFP in the economy with a firing cost of 6 months’ wages is 0.97 and in the economy with 5 year’s wages is 0.79. These are large TFP losses when compared to the previous literature (Hopenhayn, Rogerson, 1993, Hopenhayn, 2014, Moscoso-Boedo, Mukoyama, 2012). Interestingly, when we decompose the total effect of firing costs on aggregate TFP between static misallocation and dynamic misallocation (as changes in the productivity distribution of establishments), we find that dynamic misallocation accounts for around 80% of the total effect. This implies that the quantitative effect of static misallocation is of similar magnitude or even smaller than that in the existing literature. For example, in Hopenhayn (2014a) where there is no entry/exit of establishments as in our framework but instead features continuous employment choices, the TFP loss due to factor misallocation of a firing cost policy equivalent to 5 year’s wages is 7.5% compared to 4.8% in our framework. But this policy translates into a 21.3% TFP loss in our framework when accounting for dynamic misallocation.====A desirable property of our framework is that we are able to provide direct analytical results on the main variables of interest. Following the seminal work of Dixit (1989), in the benchmark economy with no firing costs, there is only one threshold productivity for which large establishments whose productivity decline become small and small establishments whose productivity increase become large. In economies with firing costs there is an inaction zone. Additionally, the inaction zone becomes larger with higher firing costs and general equilibrium effects shift the inaction zone towards lower levels of productivity. These properties of establishment decisions entail a rich pattern of misallocation whereby employment is misallocated within establishments of the same productivity and across establishments of different productivity, by shifting employment from more to less productive establishments and generating rank reversals. Furthermore, we fully characterize the productivity distribution of establishments and show how changes in firing costs impact this distribution by making it flatter and reducing its average productivity. We solve the model using Laplace transforms techniques, which allows us to fully characterize how changes in firing costs impact the rate at which establishments adjust their employment size. In the model, higher firing costs reduce average productivity in the economy.====As discussed earlier, our paper relates closely to the literature assessing the aggregate productivity losses of firing costs. An important distinction of our framework is that firing costs affect the evolution of establishment productivity which can greatly contribute to amplify the negative impact of firing costs on aggregate productivity. In this respect, our paper is closer to the seminal work of Lagos (2006) that, in the context of a model of production heterogeneity and search frictions, studies the negative impact of firing costs on measured aggregate TFP, emphasizing the effect of this labor-market policy on the set of active producers in addition to resource misallocation. Also related to our work is Mukoyama and Osotimehin (2016) studying the effects of firing costs in a model of endogenous growth and Bentolila and Bertola (1990) emphasizing the effect of firing costs on establishment’s labor demand in European countries in a partial equilibrium setting. More broadly, our paper shares with a growing literature emphasizing the dynamic effects of distortionary policies such as Hsieh and Klenow (2014), Da-Rocha et al. (2017), Guner et al. (2018), and Bento and Restuccia (2017). We differ from this literature in quantifying the effect of a specific measurable policy. A key element of the distortionary effects of firing costs is the property of an inaction zone in employment decisions that generate misallocation. While firing costs share with many other policies and institutions that also produce inaction zones in economic decisions, the effects of firing costs are different because the inaction is with respect to the dynamic response of employment decisions, generating the rich patterns of misallocation discussed earlier. For instance, size-dependent policies such as those studied in Guner et al. (2008) imply inaction in employment decisions whereby inputs are constant for a range of establishment productivity. However, these policies do not generate rank reversals.==== We also emphasize that our analysis of the productivity effects of firing costs has broader implications as we show an equivalence between the effects of firing costs policies and the effects of hiring costs policies. Overall, our analysis reveals the importance of considering the dynamic productivity effects of the policy for a more accurate assessment of the aggregate impacts of these policies.====The remainder of the paper is organized as follows. In the next section, we describe the economic environment in detail and in Section 3, we characterize its main properties. Section 4 calibrates a benchmark economy with no firing costs to data for the United States and perform a series of numerical experiments to study the aggregate implications of firing costs. We conclude in Section 5. Appendix A contains formal proofs of all the lemmas in the paper.","Firing costs, misallocation, and aggregate productivity",https://www.sciencedirect.com/science/article/pii/S0165188918303762,January 2019,2019,Research Article,424.0
"Delalibera Bruno Ricardo,Ferreira Pedro Cavalcanti","FGV/EPGE Brazilian School of Economics and Finance, Brazil,FGV/EPGE Brazilian School of Economics and Finance, FGV Crescimento & Desenvolvimento, Brazil","Received 11 May 2018, Revised 16 October 2018, Accepted 29 October 2018, Available online 2 November 2018, Version of Record 7 December 2018.",https://doi.org/10.1016/j.jedc.2018.10.002,Cited by (23),"We study the effects of early childhood ==== on productivity and schooling. We add early childhood human capital to a standard continuous time life cycle economy and assume complementarity between educational stages. Agents are homogenous and choose the intensity of preschool education, how long to stay in formal school, labor effort and consumption. The model is calibrated to the US from 1961 to 2008 and matches the data very well and closely reproduces the paths of schooling, hours worked, relative prices and GDP. We find that early childhood education can explain a large part of the observed increase of years of schooling in the U.S. since 1961, and it was as important as formal education for the increase of labor productivity in the period. Furthermore, we show that small reallocations of public expenditures from formal education to early childhood education would have sizable impacts on income per capita and productivity.","Several recent studies on educational investments in early childhood have shown that expenditures in this stage of life increase the cognitive and non-cognitive development of children and increase the investment return in later stages of their lives. For instance, Rolnick and Grunewald (2003) and Schweinhart (2004) show that the private return on investments in preschool is high as it increases the marginal productivity of individuals. However, they show that there is also a sizable external effect due to improvement in the socioeconomic conditions of these persons in the form of, for instance, less crime, less time in jail and less use of social services.====Heckman and Carneiro (2003) assess the Perry Preschool Program==== and find that, when measured through age 27, the program returns $5.7 for every dollar spent. When returns are projected for the remainder of the lives of program participants, the return on the dollar rises to $8.7. Also analyzing the Perry Preschool Program, (Heckman et al., 2010) document that the social return of the program is between 7 and 10%, which is considered high compared to other investments. The education, skills and competences acquired at this stage of life facilitate learning throughout the rest of the student’s life. Other studies emphasize that the early years are crucial for the formation of brain connections that capture the different impulses of the environment in which the child is located, affecting their intellectual development, personality and social behavior (Irwin, Siddiqi, Hertzman, 2007, Knudsen, Heckman, Cameron, Shonkoff, 2006, Myers, de San Jorge, Young, Mundial, 1996). According to Heckman (2011), the economic and intellectual inequality among individuals begins in early childhood because different investments in this stage lead to inequality in cognitive and non-cognitive skills in adulthood.====This article studies the impact of early childhood skill formation on the evolution of labor productivity and human capital accumulation in the US between 1961 and 2008, a period in which pre-primary education went from practically nonexistent to enrollment rates close to 75%. We add early childhood human capital to a standard continuous time life-cycle economy and assume complementarity between the two types of educational stages, in accordance with the evidence from the empirical literature (Cunha, Heckman, Schennach, 2010, Heckman, Cunha, 2007). In the model, there are three sectors: goods, early childhood and formal education. Agents are homogenous and choose the intensity of preschool education,==== how long to remain in formal schooling,==== labor effort and consumption. Individuals do not work when they are in school. Retirement and the early childhood timespan are exogenous.====The model is calibrated to reproduce the American economy in 2008 and the trajectory of key variables since 1961. Some variables, such as distortions of the prices of early childhood education, formal education, labor and capital, are calibrated to match targets of US data using the simulated method of moments. The model fits the data very well and closely reproduces the paths of schooling, hours worked, relative prices and GDP. We use the model to estimate endogenously early childhood education from 1961 to 2008, circumventing the lack of data, especially in the initial years.====Early childhood education plays a central role in our results. Although the estimated value of its weight in the human capital function is very low, the complementarity in human capital formation between both stages of education ultimately amplifies the impact of early childhood education through its effect on formal schooling. That is, a rise of early childhood education and other forms of skill formation at this age positively affects the return to formal education, leading to an increase in schooling. In our simulations, we find that the expansion of early childhood education in the 1960–2008 period could explain approximately 60% of the observed increase in schooling in the US. Moreover, it is as important as formal education in explaining GDP per capita and productivity.====Our formulation of early childhood human capital is quite general. It can be considered as a reduced form specification that encompasses different factors that affect the cognitive and noncognitive skill development of younger children. The literature emphasizes that parenting time, money expenditures, preschool education and different forms of early intervention are all important sources of learning and skill formation, and key to future economic and social success (e.g., Heckman (2000) and Heckman and Cunha (2007)). The Perry Program, the Abecedarian Program in North Carolina and the Chicago Child-Parenting Centers, for instance, had classroom curricula that were based on key factors of cognitive development related to planning, expression, understanding, etc. However, they also included visits to children’s home, whose purpose was to involve mothers in the educational process of their children.====Carneiro and Rodrigues (2009) and Agostinelli and Sorrenti (2018) using cross-section time surveys, find that parental time is a very important factor for child development. These articles, and many more, find that maternal labor supply has a negative impact on the skill formation of their children. Using longitudinal survey data from the UK Millennium Cohort Study, Bono et al. (2016) also estimates that maternal time is a quantitatively important determinant of skill formation and finds evidence of long-term effects of early maternal time inputs on cognitive skill development. Of course, not all time spent with the children has the same effect, watching TV together is not as good as reading and playing (Samuelsson, Carlsson, 2008, Swing, Gentile, Anderson, Walsh, 2010), which means that the quality time parents spend with their offspring is more important for skill development.====It is not clear, however, how these factors have evolved over time. For instance, it is well documented that the labor force participation of woman increased significantly in the second half of the last century. However, we do not know if this has had any important effect on the time mothers spend with their sons and daughters. In a recent article, Cardia and Gomme (2018) using data from the American Time Use Survey shows that between 1965 and the average of the years 2003–2015 married woman in their prime childcare years (aged 24–29) decreased their primary childcare time (e.g., direct and exclusive activities) in the period, but the opposite happened for older woman. It is shown that along their entire lifetime the time mothers spend with their children increased in this period, but not by much.====Our formulation is a simple way to circumvent this problem and allows us to work in a longer time span. The early childhood human capital variable used in the model represents the intensity of preschool education and of any other factor affecting skill formation at this age. There is no direct correspondence to any single variable such as time spent in school, money expenses or parenting (quality) time. It is measured endogenously from the model and for simplicity we call it early childhood education.====We extend the model in later sections by introducing school quality and a stylized form of parenting time in the human capital production function. Our main results are robust to these modifications. We also added, in another version of the model, government expenditures in both education. The model was recalibrated, and the main result here is that a small reallocation of educational resources for early childhood education (an addition of 0.3% of public expenditures) would have increased income per capita by 0.36%. This finding reinforces our previous results and calls for a reassessment of educational policies in favor of more attention and expenditures in the early stages of human capital formation.====This article extends and improves the previous literature in several directions. First, we relate to the literature that studies human capital accumulation in a dynamic macroeconomic framework (Castro, Coen-Pirani, 2016, Kong, Ravikumar, Vandenbroucke, 2018, Lee, Wolpin, 2010, Rangazas, 2000, Rangazas, 2002, Restuccia, Vandenbroucke, 2013, Restuccia, Vandenbroucke, 2013, You, 2014). None of these articles studies early childhood education. Kong et al. (2018) use a model of on the job human capital accumulation embedded in a college choice model to study the flattening of life-cycle earnings profiles. You (2014) use the Ben-Porath (1967) model of human capital formation and finds that one-fifth of US labor quality growth between 1967 and 2000 was due to the rise in educational spending. The main factor in explaining educational attainment is the skill price. In Lee and Wolpin (2010) and Castro and Coen-Pirani (2016), skill price is also an important force in explaining educational attainment.==== Therefore, these papers address distinct but complementary sets of mechanisms to explain schooling attainment.====The second body of literature that relates to our study investigates the links among government incentives, preschool education and economic development. For instance, similarly to this paper, Abington and Blankenau (2013) and Blankenau and Youderian (2015) embody the results of Cunha et al. (2010) and Heckman and Cunha (2007) in their human capital accumulation function and study the reallocation of government resources between educational stages and how it could affect aggregate income. The first article is a theoretical study, and neither paper is concerned with the evolution of labor productivity across time, as we are.====Our paper is also related to the body of literature that studies cross-country income differences and human capital (Córdoba, Ripoll, 2013, Erosa, Koreshkova, Restuccia, 2010, Klenow, Bils, 2000, Manuelli, Seshadri, 2014, Restuccia, Vandenbroucke, 2014, Schoellman, 2012, Schoellman, 2016). Although Manuelli and Seshadri (2014) and Córdoba and Ripoll (2013) include preschool education in their models, they do not consider the complementarity between early childhood and later education stages, as documented by Cunha et al. (2010) and Heckman and Cunha (2007). Moreover, their objective is different from ours. Manuelli and Seshadri (2014) investigate the role of human capital in explaining cross-country differences of output per worker, while Córdoba and Ripoll (2013) provide a theory that explains the cross-country distribution of average years of schooling. In contrast, we study the effect of early childhood skill formation on subsequent stages of human capital investment and on productivity, focusing on a single country (USA) over time.====Schoellman (2016) documents, using data of refugees living in the US, that adult outcomes are independent of age at arrival in the US, up to age 6. He interprets this finding, using a simple model of human capital accumulation, as evidence that the differences across countries in early childhood education are not important in explaining development differences. Del Boca et al. (2014) use a standard life-cycle model to study the child quality investment trade-off: Parents work more to have more money to invest in their children, but in turn, less time is allocated to child development. They find that for early investments, parental input is more important than monetary investments. Although it is clear that parental input is also important, we do not model explicit this dimension because in our model fertility is exogenous. However, as said before, our modeling strategy is a reduced form formulation that aim to measure all factors affecting early childhood skill formation.====This paper proceeds as follows. In Section 2, we present some stylized facts about evolution of early childhood human capital and preschool education. In Section 3, we present the model. In Sections 4 and 5, we describe the calibration strategy and report how well the model fits US data, respectively. In Section 6, we discuss results regarding the effect of both educational stages in the evolution of human capital, GDP and US development. In Sections 7 and 8, we modify the model, introducing public expenditures in education and quality of formal education, respectively. In Section 9, we analyze the robustness of our results with respect to parameter values. Section 10 concludes the paper.",Early childhood education and economic growth,https://www.sciencedirect.com/science/article/pii/S0165188918303646,January 2019,2019,Research Article,425.0
