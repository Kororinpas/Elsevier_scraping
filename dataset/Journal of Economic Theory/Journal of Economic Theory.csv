name,institution,publish_date,doi,cite,abstract,introduction,Title,Url,Time,Year,Type,Unnamed: 0
Suzuki Toru,"University of Technology Sydney, PO Box 123, Broadway NSW 2007, Australia","Received 9 December 2022, Revised 31 May 2023, Accepted 4 June 2023, Available online 8 June 2023.",https://doi.org/10.1016/j.jet.2023.105686,Cited by (0),"This paper studies a sender-receiver game in which both players want the receiver to choose the state-optimal action. Before observing the state, the sender observes a “contextual signal,” a payoff-irrelevant signal that correlates with states and is imperfectly shared with the receiver. Once the sender observes the state, the sender sends a message to the receiver, incurring a small messaging cost. It is shown that there is no miscommunication in any efficient equilibrium if the messaging cost is uniform or contextual information is poorly shared between players. However, if the messaging costs are different between some messages, and contextual information can affect the probability ranking of states and is shared reasonably well, any efficient equilibrium that favors the sender exhibits miscommunication. Furthermore, the messages that cause miscommunication can be coarse or ambiguous, depending on how well players share contextual information.","Many economic activities are cooperative, and communication is essential for efficient operations. At first glance, when agents share an objective and have no incentive to lie, communication seems to be a trivial task. However, in reality, we occasionally fail to understand each other regardless of our intention to communicate. The purpose of this paper is to provide a model of equilibrium miscommunication to apprehend the subtlety of communication between players who share an objective.====Miscommunication can be caused in various ways. To illustrate the type of miscommunication this paper studies, consider a situation where a teacher wants to describe a complicated idea to a student. Suppose that even though it is taxing for the teacher to describe the idea precisely, she still prefers to make the student understand the idea by describing it precisely rather than leaving him uninformed. However, if she believes that the student has been listening to the earlier part of her lecture, she would simplify her description, expecting that the student would interpret it correctly with the help of the context the earlier part of her lecture would give. Then, miscommunication occurs when the student has missed the earlier part of her lecture and fails to interpret her imprecise description correctly. This paper provides a formal model to analyze this type of miscommunication. It is shown that there is no miscommunication in any efficient equilibrium if the messaging cost is uniform or the players share contextual information too poorly. However, if the messaging costs differ between some messages, and the contextual information can affect the probability ranking of states and is shared reasonably well, any efficient equilibrium that favors the sender exhibits miscommunication. Moreover, the messages that cause miscommunication can be either coarse or ambiguous, depending on the distribution of states and how well players share contextual information.====Section 2 introduces the model. There is a sender (she) and a receiver (he). At each state, there is a unique state-optimal action: both players want the receiver to choose the optimal action at each state. First, the sender observes a noisy signal about the state, which is called contextual information and is imperfectly shared with the receiver. Specifically, the sender does not know how much the receiver knows about the contextual information. The sender then observes the state and sends a message to the receiver, who does not observe the state. Sending a message is costly, and the cost can vary across messages, reflecting that speaking or writing is a taxing activity whose cost depends on the length of speech or text. The receiver then chooses an action given the message and his knowledge of the contextual information. Both players get rewarded only if the receiver chooses the state-optimal action. If the message cost is higher than the value of the optimal action, imperfect communication is inevitable. Thus, we consider the setting where the message cost is smaller than the reward from the state-optimal action. This paper then analyzes the perfect Bayesian equilibria of the game.====In Section 3, we analyze the model. To begin with, we introduce basic concepts. First, a message is coarse if it is used at more than one state given a contextual signal. Thus, even if the receiver observes a contextual signal, a coarse message does not reveal the state. Second, a message is ambiguous if it is used at a different state across some contextual signals. Thus, unless the receiver observes a contextual signal, he cannot identify the state an ambiguous message refers to. Third, a message is precise if it is used at only one state across contextual signals, and the receiver can pin down the state even without contextual information. Finally, we say an equilibrium exhibits miscommunication if the receiver can fail to choose the state-optimal action with a positive probability in the equilibrium.====Before stating the main results, this paper provides some preliminary analysis. The first lemma states that any miscommunication in this model is caused by either a coarse message or an ambiguous message. Thus, we can focus on the equilibrium use of those messages to study miscommunication. In the current model, there is always an equilibrium with miscommunication as well as one without miscommunication, suggesting that the model does not preclude perfect communication by design. Thus, our focus is on when and how miscommunication occurs in Pareto-efficient equilibria that favor the sender, i.e., “sender-optimal equilibria.” The second lemma shows that if a sender-optimal equilibrium does not exhibit miscommunication, the equilibrium strategy can take a simple form. However, finding a sender-optimal equilibrium strategy becomes much harder when we do not know whether a sender-optimal equilibrium exhibits miscommunication. For example, the most economical strategy that fully separates states seems to be a good candidate for a sender optimal equilibrium strategy. Nevertheless, such a strategy can fail to be an equilibrium. That is, a sender-optimal equilibrium strategy needs to solve the trade-off between the informativeness and economy of communication under equilibrium constraints.====The first result of this paper provides some necessary conditions for miscommunication in a sender-optimal equilibrium. The first condition is that the messaging cost must not be constant. The second condition is that contextual information must be shared reasonably well. Specifically, we provide an upper bound for the probability that the receiver misses a contextual signal under which a sender-optimal equilibrium can exhibit miscommunication. Thus, any sender-optimal equilibrium only uses precise messages and exhibits no miscommunication if the messaging cost is uniform or/and contextual information is shared too poorly. It is also shown that whenever a coarse message causes miscommunication in a sender-optimal equilibrium, players do not share contextual information very well. Specifically, this paper provides a lower bound for the probability that the receiver observes a contextual signal under which a sender-optimal equilibrium can use a coarse message. The result suggests that any miscommunication in a sender-optimal equilibrium is caused by an ambiguous message if players share contextual information sufficiently well. It is also shown that whenever miscommunication in a sender-optimal equilibrium is caused by an ambiguous message, the probability ranking of states can be changed by contextual information. That is, a sender-optimal equilibrium never uses an ambiguous message if the distribution of states is too “stable” regardless of contextual information.====The next result provides a sufficient condition for miscommunication in a sender-optimal equilibrium. It is shown that a sender-optimal equilibrium exhibits miscommunication if the messaging costs vary across some messages, and the contextual information can affect the probability ranking of states and is shared reasonably well. The result also gives the formula that quantifies how well contextual information needs to be shared in order to have miscommunication in a sender-optimal equilibrium. The proof of the result is by construction. First, we construct a communication strategy with ambiguous messages by modifying the strategy in the best equilibrium without miscommunication. While the ambiguous messages in the modified strategy cause miscommunication, it allows the sender to save her communication cost when the messaging costs vary across some messages, and the contextual information can affect the probability ranking of states. Then, it can be shown that the strategy with ambiguous messages can be supported in equilibrium, and the expected gain from the ambiguous communication exceeds the expected loss from the ambiguity when the probability that the receiver observes the contextual signal satisfies the provided condition. That is, the condition guarantees the existence of an equilibrium with miscommunication that gives the sender a higher expected payoff than the best equilibrium without miscommunication. It is also demonstrated that the degree of ambiguity in a sender-optimal equilibrium can be substantial; an ambiguous message can be used in a sender-optimal equilibrium even if the receiver can miss contextual information with a probability close to 0.5, and the ambiguous message can mislead the receiver with a probability close to 0.5 conditional on the state and the contextual signal.====In order to obtain insight into the efficient use of coarse messages, this paper also gives another sufficient condition for miscommunication in a sender-optimal equilibrium. Suppose contextual signals can be categorized into either “usual” or “unusual,” where one state is more likely than another conditional on a usual contextual signal, whereas it is reversed conditional on an unusual contextual signal. This paper then provides a sufficient condition for miscommunication in a sender-optimal equilibrium on the probability that the receiver observes the contextual signal. The condition guarantees that a strategy with a coarse message is supported in equilibrium, and the sender strictly prefers the equilibrium to the best equilibrium without miscommunication. The coarse message in the equilibrium strategy exhibits the defining property of vagueness; there is a borderline state that is referred to by one meaning but not by another meaning of the coarse message. It is also shown that a sender-optimal equilibrium can use such a vague message rather than an ambiguous message if contextual information is reasonably but rather poorly shared.====Section 4 provides discussions. First, if the value of the correct decision is much higher for the receiver than the sender, miscommunication in a sender-optimal equilibrium can be very costly for the receiver. In such a case, the receiver might pre-announce how he responds to each message, i.e., “an interpretation rule,” before communication. This paper then provides the property of interpretation rules that maximize the receiver's expected payoff when the sender responds optimally. It is shown that such a rule possesses a property commonly observed in professional languages and organizational codes. Second, it is illustrated how the basic insights of this paper can be preserved in a more general setting. Third, it is argued that the current model suggests “context” does not exist independently by itself but is merged together with the equilibrium use of a message. Finally, we discuss how different types of equilibrium messages can be interpreted with various linguistic concepts.====The paper is concluded in Section 5.====: There is a vast literature on imperfect communication in economics; particularly, the role of a conflict of interests in imperfect communication has been studied extensively since Crawford and Sobel (1982). The current paper contributes to a growing literature that studies imperfect communication that is caused by the presence of communication friction rather than a conflict of interests. For example, Cremer et al. (2007) and Jäger et al. (2011) consider the model where the set of messages is smaller than the set of states and analyze the optimal use of coarse messages. Blume and Board (2013) study the model where the set of available messages, i.e., “vocabulary,” is private information. In their model, since the sender with a rich and a poor vocabulary use the same message differently, the message cannot reveal the exact state in the efficient equilibrium. Blume (2018) analyzes the role of higher-order uncertainty about language availability in imperfect communication.====The current paper provides another framework in this literature. There are several differences between the existing papers and the current paper. First, unlike in the existing literature, the communication friction of this paper is small enough to have an equilibrium that perfectly reveals the state. That is, this paper considers the model where the sender has a rich set of messages with small messaging costs. Our question is then when and how an efficient equilibrium can exhibit miscommunication. Second, in this paper, imperfect communication is not caused by single friction but by a combination of frictions. In fact, no efficient equilibrium exhibits miscommunication if there is no communication cost or imperfectly shared contextual information. Third, unlike in the existing models where larger communication friction can make the equilibrium communication noisier, larger friction can make an efficient equilibrium more informative in this paper. Specifically, even though the probability of miscommunication in the sender-optimal equilibrium can increase when contextual information is shared less accurately, the equilibrium miscommunication can disappear when contextual information is shared too poorly. Finally, in the existing literature, imperfect communication is caused by coarseness, i.e., the message that refers to a set of states. By contrast, imperfect communication in this paper is caused not only by coarseness but also ambiguity, i.e., the message refers to a specific but different state across contextual signals. This paper shows when contextual information is shared poorly between players, imperfect communication can be caused by coarseness. However, ambiguity plays the dominant role in imperfect communication when contextual information is shared sufficiently well.",Endogenous ambiguity and rational miscommunication,https://www.sciencedirect.com/science/article/pii/S0022053123000820,Available online 8 June 2023,2023,Research Article,0.0
"Matysková Ludmila,Montes Alfonso","University of Alicante,University of Chile","Received 22 April 2021, Revised 18 May 2023, Accepted 27 May 2023, Available online 8 June 2023.",https://doi.org/10.1016/j.jet.2023.105678,Cited by (0),"We consider a Bayesian persuasion model, in which the receiver can gather independent information about the state at a uniformly posterior-separable cost. We show that the sender provides information that prevents the receiver from gathering independent information in equilibrium. When the receiver faces a lower cost of information, her ‘threat’ of gathering independent information increases, thus decreasing the sender's power to persuade. Lower cost of information can also hurt the receiver, because the sender may provide strictly less information in equilibrium. Furthermore, we propose a solution method that can be used to solve our model in specific applications.","Models of Bayesian persuasion study situations in which a sender provides information to a receiver to persuade her to take a particular action. For instance, tobacco and pharmaceutical companies fund research to influence politicians, while software producers offer free trials of their products to encourage purchases. However, decision-makers often have access to independent information at a cost. Politicians may conduct their own research on tobacco and drugs, and customers can spend their time reading independent software reviews. In this paper, we analyse how access to independent information affects the information provided by the sender and social welfare in such settings.====We consider a standard Bayesian persuasion setup (Kamenica and Gentzkow, 2011) in which the receiver can gather independent information at a cost. We consider a sender who influences a receiver's decision by providing information about an unknown payoff-relevant state of nature. After observing the sender's information, the receiver updates her prior belief about the state to an interim belief. She then decides what independent information to gather about the state at a uniformly posterior separable (UPS) cost. We scale the cost function by a parameter. When the parameter decreases (increases), we say that the receiver's cost of information decreases (increases).====The contribution of the paper is twofold. First, we analyse how the players' well-being changes as the receiver's cost of information decreases. The sender's power to persuade always weakens. On the other hand, the effect on the receiver's well-being is ambiguous. When there are only two states and two actions, a decrease in the receiver's cost of information (generically) induces the sender to provide more information, thus making the receiver better off. However, contrary to naive expectations, in settings with many actions a decrease in the receiver's cost of information may harm the receiver. The sender may provide less information and the receiver ends up with less total information in equilibrium. We present an application—====—in which the receiver's expected payoff is non-monotone in her cost of information.====Second, we provide an algorithm—====— that analysts can use to solve our model in specific applications. The key observation is that the sender provides information that prevents the receiver from gathering independent information in equilibrium. Using this observation, we reduce the problem of finding a sender's optimal information strategy to a small subset of all possible strategies, which is finite under certain assumptions.==== We look at the receiver's simplex of interim beliefs and identify beliefs about the state at which the receiver is confident enough to take an action without gathering independent information. We call the set of such beliefs a non-learning region of that action. We show in Proposition 1 that there exists a ==== in which the receiver does not gather independent information. Therefore, we can focus on the sender's information strategies that are supported entirely on non-learning regions. Then, we show in Proposition 2 that we can further constrain ourselves to the sender's information strategies that are supported at finitely many extreme points of the non-learning regions.====In general, the non-learning regions could have infinitely many extreme points. For cost based on ====, a leading example of the UPS cost functions, we characterize the extreme points by explicit linear inequalities in Lemma 2. These linear inequalities imply finitely many extreme points, and consequently finitely many candidate sender's information strategies for an optimum.====In settings with two states, our solution method provides a straightforward way to analyse comparative statics. The simplex of the receiver's interim beliefs is a line and the sender's optimal strategy is supported by at most two extreme points. As the receiver's cost of information changes, we determine whether those extreme points are moving away from (or towards) each other, which implies that the sender provides more (or less) information in equilibrium.==== Our application provides insights into strategic situations where a decrease in the receiver's cost of information has detrimental effects on the players' well-being. We consider a setup where the players' payoffs are given by the (quadratic) distance between the receiver's action and the state. As the players become more informed about the state, they agree on the optimal type of action (negative vs positive), but disagree on the optimal magnitude of action (extreme vs conservative).====First, we consider a ==== and an extreme sender. The sender can never persuade the receiver to consider an action of extreme magnitude, not even by providing full information. Thus, the sender can only influence what type of action of conservative magnitude the receiver takes. In that case, the sender optimally provides full information to avoid mistakes in the type of action of conservative magnitude. This effect is independent of the receiver's cost of information, and hence, a ====.====Second, we consider an ==== and a conservative sender. The sender faces a trade-off between preventing actions of extreme magnitude that the receiver would consider if he provides too much information, and avoiding mistakes in the type of action of conservative magnitude if he provides too little information. Thus, the sender optimally provides as much information as possible while ensuring that the receiver never considers actions of extreme magnitude. As the receiver's cost of information decreases, this tension becomes greater, and the sender provides less information than before. In equilibrium, the receiver does not gather independent information, and thus ====.==== This paper contributes to the literature that explores the constraints on information provision in sender-receiver games. We allow the receiver to gather independent information at a cost, which constrains the sender's power to persuade. To be precise, we generalize the standard Bayesian persuasion model of Kamenica and Gentzkow (2011) to allow for an endogenously privately informed receiver.====Our model is closest in spirit to Bizzotto et al. (2020). The authors consider the standard Bayesian persuasion model with two actions and two states, in which a receiver can buy a binary signal of fixed precision at a fixed fee.==== The receiver cannot adjust the amount of information that she buys. The sender takes advantage of this rigidity, which leads to a non-monotone dependence of the receiver's well-being on her fee. In contrast, we show that with flexible information technology such a non-monotonicity arises only when considering more than two actions and it arises from a different mechanism. In addition, we provide a solution to a general model with a finite number of states and actions and an arbitrary payoff structure.====We assume that the receiver faces a uniformly posterior separable (UPS) cost. Other contemporaneous papers on models of information provision with costly information processing consider a similar cost function. The most relevant paper is Wei (2021), in which a receiver processes information about an unknown payoff relevant state at the UPS cost. The processed information must be within an upper bound disclosed by a sender who has different material preferences from the receiver. When the receiver's cost of processing information is low, the sender provides very little useful information for the receiver, because the receiver processes almost any information disclosed by the sender regardless of its information value. Similarly to our result, this mechanism leads to non-monotonicity of the receiver's well-being in her cost parameter. In our model, the sender sometimes provides less information when the cost parameter decreases, because such behaviour prevents the receiver from gathering independent information. However, contrary to Wei (2021), for very low values of the cost parameter, the receiver's well-being is highest as she can gather almost full information at a negligible cost. Other papers that consider a receiver who processes information at the UPS cost function are Lipnowski et al. (2020), Lipnowski et al. (2022), and Bloedel and Segal (2018).====To solve the model, we exploit similarities between the solution techniques outlined in Bayesian persuasion and in single agent models of costly information acquisition with UPS costs (models of rational inattention).==== These similarities allow us to use a posterior-based approach, working directly with Bayes-plausible distributions of the receiver's updated beliefs instead of joint distributions between signals and states. The UPS cost not only makes our model tractable, but also arises naturally from processes of sequential learning, as shown in Hébert and Woodford (2019) and Morris and Strack (2017). We build on known results in Caplin and Dean (2013), Caplin et al. (2019), Caplin et al. (2022), and Matějka and McKay (2015) to characterize the receiver's optimal behaviour, and the extreme points of non-learning regions with Shannon cost.====Our Extreme-point solution method, derived by bridging Bayesian persuasion and rational inattention, complements the results of Lipnowski and Mathevet (2017) and Lipnowski and Mathevet (2018). These papers consider the standard Bayesian persuasion model, and a persuasion model with psychological preferences and aligned interests, respectively. Lipnowski and Mathevet (2017) define a ‘posterior cover’ to be a partition of the simplex of the receiver's posterior beliefs into sets on which the sender's value function is (weakly) convex. They show that there exists a sender's optimal strategy that is supported at the extreme-points of these sets. Note that in our setting, as the scaling cost parameter goes to infinity, the receiver will never gather information, and thus the simplex of the receiver's interim beliefs is partitioned into non-learning regions. This partition coincides with the posterior cover characterized in Theorem 1 of Lipnowski and Mathevet (2017).==== The rest of the paper is organized as follows. In Section 2, we set the model up. In Section 3, we state the main simplification steps. In Section 4, we describe the solution algorithm (Extreme-point solution method) for Shannon cost. In Section 5, we analyse comparative statics and present the application. In Section 6, we conclude. In Appendix A, we present preliminaries to the proofs of the results based on a graphical representation of the model. In Appendix B, we provide proofs of the results. In Appendix C, we provide examples with alternative cost assumptions.",Bayesian Persuasion With Costly Information Acquisition,https://www.sciencedirect.com/science/article/pii/S0022053123000741,Available online 8 June 2023,2023,Research Article,1.0
Kim Yonggyun,"Florida State University, United States of America","Received 18 June 2021, Revised 3 April 2023, Accepted 28 May 2023, Available online 5 June 2023.",https://doi.org/10.1016/j.jet.2023.105679,Cited by (0),"I study the value of information in monotone decision problems with potentially multidimensional action spaces. As a criterion for comparing information structures, I develop a condition called ====, which involves adding ==== to an existing information structure. Specifically, this noise is more likely to return a higher signal in a lower state and a lower signal in a higher state. I show that monotone quasi-garbling is a necessary and sufficient condition for decision makers to obtain a higher ex-ante expected payoff. This new criterion refines the garbling condition by ====, ==== and is equivalent to the accuracy condition by ==== under the monotone likelihood ratio property. To illustrate, I apply the result to problems in nonlinear monopoly pricing and optimal insurance.","Consider a pair of information structures that provide signals about uncertain states. When can we say that one is superior to the other? This fundamental question has numerous economic applications including investment, monopoly pricing, and auctions. A common feature in such settings is that the decision maker would like to take a higher action when a higher signal is realized, that is, the decision problem is often ====.====The classical way of comparing information structures is to use the ==== developed by Blackwell, 1951, Blackwell, 1953. By this criterion an information structure (====) is worse than another (====) if ==== can be obtained from ==== by adding some noise—in other words, ==== is a garbling of ====. Intuitively, the added noise reduces the value of information. Indeed, Blackwell's condition implies that for ==== preferences that satisfy the von Neumann-Morgenstern axioms, the expected payoff under ==== is higher than under ====. This is a powerful result because once a pair of information structures are ranked by the garbling order, the rank is preserved in every decision problem. Although the garbling order is an important criterion for comparing two information structures, its usefulness is restricted because it is difficult to meet its requirements.====If we restrict attention to monotone decision problems, it is sometimes possible to compare information structures that are unrankable by Blackwell's condition. One well-known criterion for doing so is the ==== by Lehmann (1988). Lehmann considers a specific class of monotone decision problems and shows that his criterion refines Blackwell's garbling condition. Although the accuracy condition has been applied widely in economic settings, its precise meaning remains underexplored. Specifically, the accuracy criterion does not deliver as clear an economic interpretation as Blackwell's garbling condition does. This leads to the first main question of this paper: can we understand Lehmann's condition by using the garbling notion?====To answer this question, I introduce a novel concept called ==== by modifying Blackwell's garbling order. Specifically, I relax the assumption of the garbling condition that the added noise is independent of the state. I define the monotone quasi-garbling order by (i) allowing noise to be state-dependent; (ii) but restricting noise to be ==== in the sense that it is more likely to return a higher signal in a lower state and a lower signal in a higher state. In other words, ==== is a monotone quasi-garbling of ==== if ==== can be obtained from ==== by adding reversely monotone noise. I show that if the monotone likelihood ratio property (MLRP) holds, then Lehmann's order and the monotone quasi-garbling order are equivalent (Theorem 1). This result provides a fresh view for interpreting Lehmann's condition by using the noise notion: under the MLRP, ==== is more Lehmann accurate than ==== if and only if ==== is generated by adding reversely monotone noise to ====.====The second question of this paper is whether we can extend Lehmann's analysis to general classes of monotone decision problems. In the seminal paper on monotone comparative statics, Milgrom and Shannon (1994) posit a partial order on a multidimensional action set to study the direction of change of the optimal actions in response to exogenous changes in parameter values. Nevertheless, in most extant work studying information in monotone decision problems, the common assumption is that the action set is unidimensional and the order of actions is simply inherited from the real line (Quah and Strulovici, 2009; Chi, 2015; Athey and Levin, 2017; Li and Zhou, 2020). Because of this restriction, we cannot directly apply these earlier results to problems involving multidimensional actions such as an investment decision with Arrow-Debreu securities or a nonlinear monopoly pricing mechanism involving a menu of tariffs and quantities. I show that the introduction of the monotone quasi-garbling order helps extend information comparisons to general monotone decision problems exhibiting multidimensional actions in applications such as these.====As a first step towards this characterization, I introduce the notion of general monotone decision problems, where the action space can be multidimensional. Since there is no natural order in a multidimensional set, the action set needs to have a partial order that determines which actions are higher or lower. To ensure the meaningfulness of this partial order, I impose a condition called the ==== (DDDR). Specifically, this condition requires that for any state-contingent decision rule that decreases in states with respect to the partial order, there exists an action dominating the decision rule independently of states. In addition, given an information structure, to say that a decision problem is monotone, an optimal action under a higher signal realization needs to be higher in the partial order, namely the ==== (MCS) condition. Given a decision maker's payoff function and an information structure, if there exists a partial order on the action set satisfying the DDDR and MCS conditions, I say that the decision problem is ====.====One of the key results of this paper is that if ==== is a monotone quasi-garbling of ====, for any general monotone decision problem, the decision maker obtains a higher ex-ante expected payoff under ==== than under ==== (Theorem 2). That is, the monotone quasi-garbling order is a sufficient condition for informativeness on general monotone decision problems. Intuitively, under monotone decision problems, the information structure ==== is degraded by adding reversely monotone noise to ====, thus, the expected payoff under ==== will be less than under ====. In addition, when a class of decision problems includes every simple hypothesis testing problem, I also show that monotone quasi-garbling can serve as a necessary condition for informativeness on the class of decision problems (Theorem 3).====I apply this result to analyze the value of information in monopoly pricing with second-degree price discrimination as in Maskin and Riley (1984). At the beginning of the game, the seller chooses between two sources of information about the buyer's type. Unlike a standard monopoly pricing problem where the seller makes a unidimensional choice of price or quantity, in a nonlinear pricing problem, the seller needs to provide a menu of tariffs and quantities–which is multidimensional. Although this problem is usually modeled as a two-player game, it can be cast as a multidimensional decision problem for a seller constrained by incentive compatibility and individual rationality by regarding the buyer's type as a state. Moreover, with fairly mild assumptions, I show that this application satisfies the DDDR condition and the MCS condition, thus the main result of this paper is also applicable in this setting. In Appendix C, I also provide another application on optimal insurance with Arrow-Debreu securities.====The rest of the paper is organized as follows. I review the literature in Section 2. Section 3 sets up the preliminary notions. In Section 4, I introduce the monotone quasi-garbling order and compare it to other orderings such as Blackwell's garbling condition and Lehmann's accuracy condition. In Section 5, I define general monotone decision problems and show that the monotone quasi-garbling order is a necessary and sufficient condition for informativeness. I apply this result to a nonlinear pricing problem in Section 6. Proofs and examples omitted from the text are in the Appendix.",Comparing information in general monotone decision problems,https://www.sciencedirect.com/science/article/pii/S0022053123000753,Available online 5 June 2023,2023,Research Article,2.0
"Lanzara Gianandrea,Santacesaria Matteo","IGIER, Bocconi University, Italy,MaLGa Center, Department of Mathematics, University of Genova, Italy","Received 29 October 2021, Revised 21 February 2023, Accepted 24 May 2023, Available online 29 May 2023, Version of Record 15 June 2023.",https://doi.org/10.1016/j.jet.2023.105675,Cited by (0),"This paper proposes a spatial model with a realistic geography where a continuous distribution of agents (e.g., farmers) engages in economic interactions with one location from a finite set (e.g., cities). The spatial structure of the equilibrium consists of a tessellation, i.e., a partition of space into a collection of mutually exclusive market areas. After proving the existence of a unique equilibrium, we characterize how the location of borders and, in the case with mobile labor, the set of inhabited cities change in response to economic shocks. To deal with a two-dimensional space, we draw on tools from computational geometry and from the theory of shape optimization. Finally, we provide an empirical application to illustrate the usefulness of the framework for applied work.","This paper proposes a spatial model with a realistic geography where a continuous distribution of agents engages in economic interactions with at most one location from a finite set. The spatial structure of the equilibrium consists of a tessellation, i.e. a partition of space into a collection of mutually exclusive market areas. This equilibrium structure exhibits some novel properties that depart from the extant literature. First, the model comprises a notion of borders that may be compared with borders observed in the real world. Second, when labor is mobile, some of the locations in the finite set may fail to attract workers, and therefore remain vacant. Thus the model also provides a framework for thinking about the emergence and the location of discrete economic entities, such as firms within a neighborhood, business districts in a metropolitan area, or cities within a larger economy. Both the geography of borders and the set of inhabited locations are equilibrium outcomes that depend on the parameters of the model.====These research topics have been the focus of a large number of theoretical studies in the fields of new economic geography (Krugman, 1993; Fujita and Krugman, 1995; Fujita et al., 2002), urban economics (Fujita and Ogawa, 1982; Henderson, 1974; Henderson and Wang, 2005), and political economy Alesina and Spolaore (1997); Alesina et al. (2000). These studies posit a continuous space interacting with a discrete set of locations, but with the spatial dimension confined to stylized geographies. While simple geographies are often appropriate to generate valuable theoretical insights, they also offer a weaker connection with the data and do not lend themselves easily to quantitative analysis.====On the other hand, a more recent literature has developed economic models with realistic geographies to assess the importance of spatial frictions for market outcomes and welfare (Allen and Arkolakis, 2014; Redding, 2016; Redding and Rossi-Hansberg, 2017). Location and trading choices in these models are typically based on a random utility approach,==== whose equilibrium outcome is that, in general, each location interacts with all other locations, unless exogenous factors prevent it. While this feature renders the framework flexible and tractable enough to be confronted with the data in a wide variety of empirical contexts, endogenous borders and vacant locations struggle to materialize in this class of models.====In this paper, we combine endogenous market areas and realistic geographies within a tractable theoretical framework. To do so, we draw on a set of tools from the literature on Voronoi diagrams. A standard Voronoi diagram is a simple assignment rule such that each point in a set ==== is assigned to the nearest point in a finite set of locations ====. An additively weighted Voronoi diagram is a generalization such that each location ==== is associated with a weight ====, ==== that determines its relative attractiveness over and above geographic distance. Hence a point ==== is assigned to a point ==== if and only if==== This construction has appeared in previous work in economics and geography to describe the size and shape of market areas on the Euclidean plane.==== In these studies, the Voronoi weights are usually set equal to the market prices ====, ==== in a partial equilibrium setting. If, for instance, ==== represents a set of sellers and ==== represents a set of markets, then this simple model works as illustrated in Fig. 1 for ====. In Fig. 1a, all markets offer the same price. In this case, sellers care only about Euclidean distance and the tessellation reduces to a standard Voronoi diagram. Fig. 1b depicts an additively weighted Voronoi tessellation with ====, going anticlockwise. The boundaries shift in an intuitive manner. Market ==== offers the highest price and therefore attracts sellers from longer distances, expanding its market area at the expense of the other market locations. Market ==== loses territory to market ==== but expands its market area in the direction of market ====, which offers the lowest price. Finally, in Fig. 1c, the vector of prices (or weights) is ==== and market ==== fails to attract any sellers.====The main idea of this paper is to treat the Voronoi weights as endogenous objects that are determined in general equilibrium. As a result, the problem of characterizing the equilibrium borders and the set of inhabited locations reduces to studying the properties of the equilibrium weights. Furthermore, the weights will not necessarily coincide with the market prices; they will be more complex functions derived from the economic primitives and will depend on the parameters of the model.====We frame our discussion in terms of an urban model where the elements of ==== are cities and the Voronoi regions in ==== are rural areas or hinterlands. These sets are endowed with different technologies for the production of either an urban or a rural good, respectively, which enter with a constant elasticity of substitution (CES) into the consumer's utility function. Space matters, in that carrying rural goods from the countryside to an urban market incurs the payment of a shipping cost that increases with distance.====We consider both a scenario with a fixed population distribution and a scenario with a mobile population across locations and sectors. In both cases, we find that a unique equilibrium exists independently of the underlying geography and of the size of shipping costs, and we characterize its comparative statics with respect to some of the model parameters. The comparative statics of the Voronoi weights provide insights into which market areas will expand, shrink, or vanish following a parameter change.====In the setting without labor mobility, we first prove that the equilibrium conditions can be expressed as first-order conditions of a given cost function ====. With this result in hand, we then show that ==== indeed attains a unique critical point. We also show that the model delivers intuitive comparative statics as long as urban and rural goods are subsitutes, and shipping costs increase sufficiently fast with distance. Specifically, if urban population increases in, say, city ====, then its market area expands at the expense of the surrounding market areas. This result relies on a graph-theoretic interpretation of Voronoi diagrams, which allows us to circumvent the fact that the gross substitution property Mas-Colell et al. (1995) does not hold in our setting. As a matter of fact, the ==== effect of city ===='s population change is nil in cities that do not share a border with ====. However, because the Voronoi diagram defines a connected graph, the impact of a shock is eventually transmitted to all cities, and this weaker condition turns out to suffice to sign the comparative statics.====In a setting with labor mobility, the welfare equalization condition for urban workers yields a closed-form expression for the Voronoi weights. The main challenge here is to prove that there is a one-to-one relationship between the welfare scalar and total population, so as to ensure the uniqueness of the equilibrium in a “closed economy” scenario where total population is fixed. The reason is that the shape of the tessellation varies with the level of welfare. We thus have to account for these endogenous border changes and their impact on the equilibrium total population. To overcome this challenge, we make use of a classic result in the theory of shape optimization which provides a general formula for the derivative of a function over a variable domain (a generalization of the Leibniz rule) Henrot and Pierre (2006). To the best of our knowledge, this is the first application of the shape derivative to an economic problem. The key observation, then, is that a parameter change affecting all Voronoi weights at the same time will exert on each border segment between neighboring market areas two opposing forces, one from each side of the border: decomposed segment-wise in this fashion, the overall effect can be handled and signed.====Once these technical hurdles are surpassed, the model becomes highly tractable and allows for a rich set of comparative statics. Thanks to the closed-form solution for the Voronoi weights, we are able to characterize the effect of changes in shipping costs and total population on the size of market areas. The sign is unambiguous for all cities that are either more productive or less productive than all their neighbors. In particular, reductions in shipping costs increase the market area of more-productive urban centers. In contrast, the effect of changes in the size of the total population depends on the elasticity of substitution between urban and rural goods: with elastic demand, an increase in total population will favor urban centers that are more productive than their neighbors, similar to a reduction in shipping costs.====As mentioned above, the spatial equilibrium of our framework features two novel aspects: the emergence of a well-defined notion of borders, and a set of vacant locations. Regarding this latter aspect, we use the model to derive sharp conditions for an urban site to be inhabited or vacant in equilibrium. For instance, when shipping costs are prohibitively high, all urban sites will be inhabited, and as shipping costs decrease, urban sites are “sequentially” abandoned until only one of them (the most productive one) remains inhabited. The conditions depend explicitly on the underlying geography: urban sites that happen to be located near more-productive ones will be abandoned earlier. While the set of ==== urban sites is a primitive of the model, we remark that no other restriction is placed on it except that it is finite.====We conclude the paper by applying the model to the case of Switzerland. This case study illustrates the workings of the version of the model without labor mobility, as well as the usefulness of a notion of hard borders for applied work. The empirical counterpart of the set of cities ==== is the set of cantonal capitals: the capital cities of the federated state (cantons) forming the Swiss confederation. We compute alternative theoretical tessellations, which we evaluate against the tessellation of cantonal borders. We find that the equilibrium tessellation computed via our model is a better approximation of Swiss internal borders than alternative tessellations that neglect the roles of geography and market forces.====  Endogenous market areas have appeared in Nagy (2022) and Nagy (2023) to investigate, respectively, the link between trade and urbanization in Hungary after the First World War and the link between city formation and growth during the U.S. westward expansion in the 19th century. In Nagy (2022), location-specific varieties of a tradable good can be exchanged at a finite set of trading locations. In the model presented below, we also take the set of urban locations as given, whereas we follow Nagy (2023) (where, instead, the location technology is not predetermined) in assuming a distinction between a rural good, produced in hinterlands, and an urban good, produced in cities.====Our contribution to this line of research is twofold: first, though we forego some important features (such as, for instance, trade costs for urban goods), we obtain a more-complete analytic characterization of the equilibrium of the model; second, we leverage a set of technical tools, such as Voronoi diagrams and the shape derivative, that make the problem more tractable and will be useful, we hope, in spurring further work in this area.====Our characterization of the set of inhabited cities, in the version of the model with mobile labor, contributes to the economic theories of city formation, such as the system-of-cities theories (see Henderson, 1974 and subsequent work) and the new economic geography approach (see, for instance, Fujita et al., 2002). However, our analysis differs in two main respects. First, we work with a realistic (rather than a stylized) geographic setting. Compared to Henderson (1974), for instance, urban sites are arranged in space, which allows us to derive implications not only on the ==== of inhabited sites but also on their ====. Second, the economic mechanism is different. In those traditions, economies of scale at the urban level are a key ingredient for generating the agglomeration of economic activity in a subset of the available locations, even on a featureless line or plane.====In contrast, in our model, a necessary condition is that urban sites have heterogenous characteristics, even in the absence of urban spillovers. However, it would be incorrect to conclude that agglomeration forces are entirely missing from the model: as we explain below, while rising agricultural prices encourage urban workers to disperse, they may also lead farmers to serve a limited subset of urban markets. In sum, heterogeneity must be combined with endogenous trading choices among spatially ordered locations for some urban sites to remain vacant.====Our work is also related to a small number of papers that borrow tools from computational geometry to solve economic problems. For instance, Rossi-Hansberg et al. (2020) focus on the optimal plant-location decision of a firm given a continuous distribution of consumers in space. Because consumers patronize one plant only, the model delivers a spatial tessellation that corresponds to a weighted Voronoi diagram. In a recent working paper, Allen (2022) leverages centroidal Voronoi tessellation to study the evolution of national borders in a quantitative framework. Finally, Voronoi diagrams appear in Merlo and de Paula (2016) in the context of a spatial theory of voting and in Lambert (2022) in the context of mechanism design.====  In Section 2, we introduce some basic concepts and definitions of additively weighted Voronoi diagrams. In Section 3, we lay out the economic framework. Sections 4 and 5, respectively, characterize the properties of the equilibrium with immobile labor and mobile labor. Section 5 also presents our results on the endogeneous set of inhabited cities and discusses them in light of extant theories of city formation. In Section 6, we apply the model empirically to Switzerland. Section 7 concludes.",Market Areas in General Equilibrium,https://www.sciencedirect.com/science/article/pii/S0022053123000716,Available online 29 May 2023,2023,Research Article,3.0
"Hiller Victor,Wu Jiabin,Zhang Hanzhe","Université Paris-Panthéon-Assas, LEMMA, France,Department of Economics, University of Oregon, United States of America,Department of Economics, Michigan State University, United States of America","Received 27 December 2022, Revised 26 March 2023, Accepted 8 May 2023, Available online 12 May 2023, Version of Record 22 May 2023.",https://doi.org/10.1016/j.jet.2023.105671,Cited by (0),"Building on previous literature that examines the influence of intergenerational transmission in cultural evolution, we highlight the importance of the marriage market in the determination of cultural homogeneity (“melting pot”) versus heterogeneity (“diversity”). To do so, we characterize cultural evolutionary processes under different distributions of marital preferences and stable matching schemes. In our setting, cultural substitutability (====) is neither sufficient nor necessary for cultural heterogeneity. We introduce a new concept, elasticity of cultural substitution, to capture the degree of increase in vertical socialization efforts in response to minority population decline. With perfect or inelastic vertical transmission in homogamous families, cultural heterogeneity is sustained only if all proposers are homophilic or all members of a cultural group are homophilic. With imperfect vertical transmission in homogamous families, the presence of heterophilic agents may destabilize cultural heterogeneity, and the proportion of heterophilic agents and elasticity of cultural substitution determine whether cultural heterogeneity can be sustained. We discuss the model's implications for the long-lasting impact of temporary gender imbalance on cultural evolution as well as the cultural assimilation and preservation of minorities and immigrants under distinct governmental and religious attitudes toward intermarriage.","Culture plays a crucial role in many economic choices and outcomes at the individual and national level (Landes, 1998; Guiso et al., 2006; Fernández, 2008, Fernández, 2011; Alesina and Giuliano, 2015). Which cultures or cultural traits survive and thrive and what familial and societal factors influence their success are important questions. A growing literature seeks to disentangle the mechanisms that drive the evolution of culture across generations; see Bisin and Verdier, 2011, Bisin and Verdier, 2023 for surveys.====Children are primarily cultivated in families. Familial transmission of preferences in cultural evolution has attracted biologists and anthropologists since Cavalli-Sforza and Feldman (1981) and Boyd and Richerson (1985) and economists since Bisin and Verdier, 2000, Bisin and Verdier, 2001. In this paper, we demonstrate that family formation—what marital preferences individuals possess and how parents are matched—is also of prime importance for our understanding of cultural evolution. Empirical evidence and historical examples point to the important roles of the marriage market and marital preferences in cultural evolution. For instance, cultural norms differ in patriarchal and matriarchal societies (Andersen et al., 2008; Gneezy et al., 2009; Andersen et al., 2013; Giuliano, 2017; Lowes, 2020; Brulé and Gaikwad, 2021; Tène, 2021); factors that affect the respective positions of men and women in the marriage market, such as a temporary gender imbalance, could have a long-run impact on cultural evolution (Grosjean and Khattar, 2019; Gay, 2019; Teso, 2019; Alix-Garcia et al., 2020; Baranov et al., 2021); and government policies and religious practices with respect to intermarriage lead to different cultural assimilation patterns of ethnic minorities and immigrants (Silcock, 1963; Bisin and Verdier, 2000; Skinner, 2008; Dien and Knapp, 2020).====A central question regarding cultural evolution is under what conditions cultural integration and preservation occur. Our paper contributes to this line of inquiry by studying cultural evolution when agents have heterogeneous marital preferences and family formation is determined by stable matching (Gale and Shapley, 1962). Whereas most models in the literature sidestep the marriage market or assume homogeneous marital preferences and exogenous matching, we consider heterogeneous marital preferences and endogenous matching.====The primitives of the Gale-Shapley matching model are sets of men and women and preference orderings of individuals over agents of the opposite sex. These elements are determined in our model as follows. At the beginning of each discrete period, a mass of women and a mass of men become adults. Each adult wants to match with an adult of the opposite sex to form a family. Each participant in the matching market has a cultural trait, acquired during childhood, and marital preferences over the cultural trait of their partner. In addition to the usually considered ==== marital preferences—individuals prefer partners with the same trait the most—we incorporate the possibility that some men and women are ====: Individuals prefer partners with a different cultural trait. Stable matching depends on the distribution of cultural traits and the distribution of marital preferences in both populations such that no positive mass of individuals of opposite sexes would both rather have each other than their current mates, in the case of multiplicity, the set of stable matchings forms a lattice, and among them there is one ==== (MOSM) and one ==== (WOSM), with MOSM (resp., WOSM) being the stable matching the most preferred by men (resp., women) and the least preferred by women (resp., men), achieved by men (resp., women) acting as the proposers through the so-called deferred acceptance (DA) algorithm, which Baïou and Balinski (2002) generalize to the setting with a continuum of agents. We assume that, either MOSM or WOSM is chosen in every period. One can imagine that marriages in a society are arranged through the DA algorithm, and the society either has a convention that only men propose or one that only women propose.====Once matched, each couple has two children, one son and one daughter. Children acquire the cultural trait they will retain when they become adults. Through the cultural transmission process, stable matching in one period will determine the joint distribution of cultural traits among populations of both men and women in the next period. Since ====—marriages in which spouses have the same cultural trait—have a well-defined cultural trait to transmit, they have a more efficient socialization technology than other families. We consider two transmission technologies of homogamies. We first consider ====: Homogamies transmit this trait to their children with probability one. Then, we consider ==== that is ==== to societal transmission: Homogamies transmit their culture with a probability that is strictly less than one and strictly decreasing in the proportion of that trait in the population. If they fail to transmit their cultural trait, their children are socialized by the society at large (====): They adopt the trait of a randomly chosen adult role model. Children of ====—marriages in which two spouses have different cultural traits—do not have a well-defined familial model to follow, and thus we assume they are socialized by the society at large.====Broadly speaking, we analyze how cultural evolution is influenced by the determinants of stable matching—namely, the distribution of marital preferences and the side of the market favored by the matching procedure—under common transmission technologies. In this regard, we provide a unified and generalizable model to investigate the effects of different forms of matching and intergenerational transmission on cultural evolution. More specifically, we characterize the conditions under which cultural heterogeneity is sustained in the long run, which refines and advances prior results (Cavalli-Sforza and Feldman, 1981; Boyd and Richerson, 1985; Bisin and Verdier, 2000, Bisin and Verdier, 2001; Della Lena and Panebianco, 2021). In particular, we identify two key factors that affect the extent of cultural diversity in the stable steady state. First, the fraction of homophilic proposers. Since this fraction influences the distribution of homogamies in the population and homogamies possess a better transmission technology, it crucially shapes cultural evolution and the long-run distribution of traits. Second, our new concept, ====, which captures the strength of substitutability between oblique and vertical transmission. It describes the percentage increase in the chance of vertical transmission for the minority group—the crucial source of cultural preservation for minorities—as the minority population decreases. Our main results, listed below, illustrate the interactions between these two factors.====We start by demonstrating that if all proposers are homophilic, the stable steady-state distribution of traits is culturally heterogeneous. This holds true regardless of the cultural transmission technology (with or without cultural substitutability) and regardless of the distribution of receivers' marital preferences. Indeed, if all proposers are homophilic, as many homogamies as possible are formed. Then, in each cultural trait, the proportion of homogamies is determined by the short side of the marriage market. Since homogamies have a better transmission technology, this creates a tendency toward the equal distribution of traits between the populations of men and women, and the society ends up in a situation in which all marriages are homogamous so that each cultural group can maintain its legacies. We reach exactly the same conclusion when one cultural group is strongly opposed to intermarriage, such that all members of this group (both men and women) are homophilic.====With heterophilic proposers, the long-run distribution of traits crucially depends on the transmission technology under consideration. Under either perfect or inelastic vertical transmission (i.e., in the absence of cultural substitutability), an arbitrarily small fraction of heterophilic proposers is sufficient for cultural diversity to disappear so that cultural homogeneity is the generic long-run outcome. Moreover, the surviving culture is the initially dominant one in population size. This is because although heterophilic preferences facilitate the formation of heterogamies, the majority group manages to preserve a larger fraction of homogamies. Since homogamous couples are more effective in transmitting their traits, this creates an evolutionary advantage for the majority group, which ultimately drives out the minority group.====These results illustrate the crucial role played by the distribution of proposers' marital preferences. For instance, under either perfect or inelastic vertical transmission, diversity is preserved if all proposers are homophilic, but this is not the case if some proposers are heterophilic. This implies that when one side of the matching market is fully homophilic and the other side is not, the matching institution, which determines the side of the market that will play the role of proposers, might have dramatic consequences for the long-run cultural composition of the society. We demonstrate that because of this feature, even though MOSM is the outcome men prefer from a static point of view, this is not necessarily true from a dynamic point of view: Cultural evolution under MOSM (resp., WOSM) might lead to a path considered to be suboptimal by men (resp., women). This result also implies that factors that influence the selection of one particular stable matching (MOSM or WOSM) might have crucial consequences for the long-run distribution of traits. The choice between MOSM and WOSM might also be related to gender imbalance. The distribution of marital preferences of agents on the shorter side of the marriage market, even if they are not proposers, determines the matching outcome. Hence, a slightly skewed gender ratio can result in a dramatic change in the matching outcome and, consequently, the cultural evolution. We introduce gender imbalance in our model and show that a short-run change in gender ratio leads to a long-run change in the cultural distribution; this is in line with empirical findings of long-lasting impacts of temporary gender imbalance (Grosjean and Khattar, 2019; Gay, 2019; Teso, 2019; Alix-Garcia et al., 2020; Baranov et al., 2021).====Under imperfect vertical transmission, cultural substitutability helps maintain cultural diversity except in cases in which either all proposers and/or receivers are heterophilic or an entire cultural group is heterophilic with the other group being nonhomophilic. In other words, as long as there is a positive fraction of homophilic proposers cultural diversity is sustainable in the long run. The extent of cultural diversity (i.e., the size of the minority group) in the stable steady state increases with the fraction of homophilic proposers and the elasticity of cultural substitution. We apply our results to connect intermarriages with cultural assimilation and preservation, and provide examples of how a more open attitude toward intermarriages due to government or religion leads to cultural homogeneity and a more restricted attitude leads to cultural heterogeneity (Silcock, 1963; Bisin and Verdier, 2000; Skinner, 2008; Dien and Knapp, 2020).====Most of the cultural evolution literature (Cavalli-Sforza and Feldman, 1981; Boyd and Richerson, 1985; Bisin and Verdier, 2001; Cheung and Wu, 2018) considers asexual reproduction models in which cultural transmission is the result of vertical parental socialization and oblique society socialization. Since a child is socialized by one parent, couple formation does not play a role in cultural evolution. We depart from these foundational models by considering a two-sex cultural transmission model in which marital preferences are heterogeneous and the matching between spouses is endogenous. Whereas the literature highlights the fact that cultural substitutability is key for the preservation of cultural diversity, we show the central role played by the interactions between cultural substitutability and preference for homophily. In particular, cultural heterogeneity may arise even in the absence of cultural substitutability, and cultural homogeneity might be the long-run outcome even in the presence of cultural substitutability. For some matching structures, cultural substitutability is neither necessary nor sufficient for cultural heterogeneity.====Some papers consider socialization by two parents. Bisin and Verdier (2000) propose a cultural transmission model with a marriage market. Individuals might be one of two types and prefer that their children have their trait. Agents must enter a frictional marriage market to marry and reproduce. The marriage market consists of two restricted matching pools exclusive to the two types, respectively, and a common matching pool. Entering a restricted matching pool is costly. The authors assume that homogamous parents enjoy more efficient socialization for their shared type than heterogamous parents. As a result, individuals prefer to and do marry their own type (homophily). They also assume that daughters and sons are socialized in the same way, such that the cultural distribution is the same across gender. In contrast, we propose a two-sex cultural transmission model and allow for heterophilic preferences. These features separate the consideration of marital preferences and the socialization of cultural traits, and allow consideration of the joint cultural evolution of men and women.====Recently, some two-sex cultural evolution models have been developed. Hiller and Baudin (2016) and Baudin and Hiller (2019) propose models in which parents may socialize their sons and daughters differently. However, their analysis considers random matching; the effects of stable matching on the evolution of preferences are not considered. Wu and Zhang (2021) allow for random or assortative matching of spouses, but implicitly assume homophily (because heterophilic individuals are not distinct from homophilic ones when stable matching is not considered). In contrast, we consider stable matching with heterophilic individuals so that there could exist multiple stable matches; as a result, multiple cultural equilibria may arise.====The rest of the paper is organized as follows. Section 2 presents the general setup of the model. Section 3 shows that uniformly homophilic proposers or an entirely homophilic cultural group leads to culturally heterogeneous states. Section 4 shows that under perfect or inelastic vertical transmission in homogamies, even an arbitrarily small fraction of heterophilic proposers is sufficient for cultural homogeneity. Section 4 also presents the implications of having gender-differential distributions of marital preferences. Section 5 presents the results for imperfect vertical transmission with cultural substitutability in homogamies, and Section 6 concludes. Appendices collect omitted proofs and details.",Marital preferences and stable matching in cultural evolution,https://www.sciencedirect.com/science/article/pii/S0022053123000674,12 May 2023,2023,Research Article,4.0
"Keister Todd,Mitkov Yuliyan","Department of Economics, Rutgers University, USA,Department of Economics, University of Bonn, Germany","Received 2 March 2022, Revised 8 May 2023, Accepted 9 May 2023, Available online 12 May 2023, Version of Record 29 May 2023.",https://doi.org/10.1016/j.jet.2023.105672,Cited by (0),"We study the interaction between the government's ==== policy and a bank's willingness to impose losses on (or “bail in”) investors based on its ====. In the absence of regulation, bail-ins in the early stages of a crisis are too small, while bailouts are too large and too frequent. Moreover, the bank may face a run by informed investors, creating further distortions and leading to a larger ====. We show how a regulator with limited information can raise welfare and, in some cases, improve financial stability. The optimal policy involves partial delegation: the regulator sets bounds on the size of the bank's bail-in, but allows the bank to choose within these bounds.","The losses suffered by banks and other financial institutions during a crisis translate into losses for their own investors and creditors and, possibly, losses for the public sector in the form of a bailout. How a bank's losses are divided between private agents and the public sector affects incentives and behavior in normal times, as well as the allocation of resources in society. After the global financial crisis of 2008 and the subsequent European debt crisis, a consensus emerged that too many of the losses in these events fell on the public sector, that is, bailouts were too frequent and too large. This perception led policy makers to draft rules requiring financial institutions to impose more losses on (or “bail in”) their investors/creditors in future crises. How effective these rules will be in practice remains to be seen. Even at a conceptual level, however, it is not well understood how losses ==== be allocated in a crisis, nor what types of bail-in policies are likely to be most effective.====We study the interaction between bail-ins and bailouts, focusing on the early stages of a crisis. Our model builds on the classic framework of Diamond and Dybvig (1983), where investors face idiosyncratic liquidity risk and pool their resources in a bank.==== Bank assets are risky in our model, and the size of the bank's loss during a crisis is initially not known to policy makers. Some of the bank's creditors have private information about this loss and can withdraw before the information is revealed. The bank has the ability to bail in these creditors, meaning it could pay them less than they receive in normal times. In practice, this bail-in represents any action that preserves resources within the bank, including lowering dividend payments, restricting withdrawals and/or imposing withdrawal fees. In our model, the initial bail-in is simply a haircut imposed on funds withdrawn from the bank before its loss becomes public information. We study a bank's incentives in making this bail-in decision and ask if regulating its choice can improve welfare.====Our model provides a framework for evaluating policies like the reforms to money market mutual funds adopted in the U.S. in 2014.==== Under these rules, some funds are permitted to limit redemptions and impose withdrawal fees – a type of bail-in – during periods of financial stress. A fund is directed to take these actions if doing so is in the best interests of its investors. This policy raises interesting questions: What are the best interests of an institution's investors in such a situation? Are these rules likely to achieve desirable outcomes? Another example is the debate over whether regulators should restrict dividend payments by banks during periods of stress. At the onset of the Covid-19 pandemic, for example, the European Central Bank recommended that banks “refrain from making dividend distributions and performing share buy-backs aimed at remunerating shareholders.”==== In the U.S., the Federal Reserve moved on June 25, 2020, to prohibit share repurchases and to cap dividend payments by large banks. When is it desirable to impose broad restrictions on the payments banks make to their investors? What types of restrictions are most effective? We develop a model to address these questions.====The efficient allocation of a bank's loss in our model depends on the public sector's cost of funds. If this cost is high enough, a benevolent planner will provide no bailouts and will cover the entire loss by decreasing the consumption of the bank's investors, that is, by bailing them in. When this cost is lower, however, the planner will provide a bailout if the bank's loss is sufficiently large. In other words, the planner wants the public sector to absorb some of the “tail risk” in the economy, which implies a combination of bail-ins and bailouts is efficient. In both cases, the planner applies the same haircut to all investors, regardless of whether they withdraw early or remain invested in the bank.====In the decentralized economy, the bank's incentive to bail in early-withdrawing investors depends on what bailout policy it expects. We assume the government cannot commit; it will choose the bailout as a best response to the situation at hand when the bank's loss is revealed. This situation will depend on whether the investors who have already withdrawn at that point were paid in full or bailed in. Paying these investors in full leaves the bank in worse condition, which leads to a larger bailout. In this way, bailouts distort the bank's incentive when choosing the initial bail-in. We show that, in states where the bank anticipates being bailed out, its initial bail-in is always smaller than the planner would choose and the subsequent bailout is always larger.====When the bank applies a smaller bail-in to investors who withdraw early, it increases the incentive for all investors to withdraw. We show that, in some cases, the smaller initial bail-in leads to a fundamentals-based run on the bank. This run is partial and only involves investors who are able to withdraw before the government intervenes. The bank knows this partial run will occur and could prevent it by imposing an initial bail-in large enough to discourage early withdrawals. But doing so is costly because a bail-in reduces the bank's bailout dollar-for-dollar. In this way, our model identifies a new channel through which bailouts can increase financial fragility: by raising the bank's cost of using bail-ins to discourage early withdrawals.====Given these problems, we ask whether regulation can improve outcomes. The regulator has the ability to impose a haircut on funds withdrawn from the bank. However, because the regulator only observes the bank's loss after some withdrawals have been made, it does not know the appropriate initial level for this bail-in. We formulate the regulator's decision as a ==== in the spirit of Holmström, 1977, Holmström, 1984. The regulator decides to what extent it will require the bank to bail in investors who withdraw early and to what extent it will delegate that decision to the bank. The aim of the policy is to increase the bail-in in those states where the bank will later be bailed out while minimizing the distortion created in states where no bailout occurs. We show that, as long as the bank is bailed out in some states, regulation can improve welfare.====We characterize the optimal policy and show it takes one of two forms. If parameter values are such that a run will never occur, regardless of the bank's choice, the optimal policy is to impose a minimum bail-in. This minimum will bind in states where the bank's loss is large enough for a bailout and also in states where the bank's loss is small or zero. In between these states, the bank chooses a bail-in above the required minimum, which demonstrates the value of delegation. For other parameter values, the optimal policy uses the threat of a run by investors to discipline the bank's choice. In these cases, the optimal policy allows the bank to choose from a set of small initial bail-ins or a set of large ones, but not values in between. The policy is calibrated so the bank would suffer a run if it chose a small bail-in following a large loss. The desire to avoid a run leads the bank to self-select into a large bail-in in these states. In cases where a run occurs in the absence of regulation, this approach improves financial stability as well as welfare.====Overall, our results demonstrate the value of regulating bail-ins early in a crisis rather than waiting for more precise information about banks' losses. Much of the existing policy discussion has focused on tying bail-ins to information that is observed by regulators, either publicly or privately. For example, contingent-convertible bonds (CoCos) can be structured to convert from debt to equity when the book value of a bank's equity falls below some pre-specified level.==== In our model, the regulator can solve this ==== problem by imposing the efficient bail-in on those investors who remain when it observes the bank's loss. There is, however, an earlier period when the regulator knows a problem may exist but does not yet know how badly the bank is affected. Our results show the value of acting promptly to bail in the withdrawals made during this period.====  Wallace, 1988, Wallace, 1990 provided an early analysis of bank bail-ins in a version of the Diamond-Dybvig model with aggregate risk. He showed that when a bank observes withdrawal demand gradually, through a process of sequential service, the efficient allocation has a feature that he called “partial suspension of convertibility” but which in current terminology would be called a bail-in. Subsequent work derived the efficient pattern of bail-ins within an individual bank for different specifications of the environment; see, for example, Green and Lin (2003), Peck and Shell (2003), Ennis and Keister (2009b), and Sultanum (2014). This literature emphasizes that investors ==== their bank to use bail-ins to efficiently allocate risk; there is no need for regulation or supervisory bail-ins in these models. From a policy perspective, this literature broadly supports the type of reforms adopted for money market mutual funds in the U.S. in 2014. In particular, it suggests that if intermediaries are allowed to take bail-in actions such as limiting withdrawals and imposing withdrawal fees, they will do so in times of stress and these actions will promote financial stability. In our setting, in contrast, the anticipation of being bailed out undermines the incentive to use bail-ins, rendering such reforms ineffective and creating a rationale for requiring bail-ins even before policy makers have precise information about a bank's loss.====Our work also relates to the recent literature that studies the incentive effects of bail-ins and the resulting policy tradeoffs. Bernard et al. (2022) study a game in which a regulator and banks negotiate over the allocation of losses, focusing on how the structure of the interbank network affects the credibility of a no-bailout plan. Walther and White (2020) study how a bail-in improves a bank manager's incentive to exert effort by increasing her stake but risks provoking a run if it leads creditors to infer the bank is in bad shape. Colliard and Gromb (2018) study the negotiation between a bank's shareholders and its creditors over how the losses will be distributed, while Bolton and Oehmke (2019) study the problem of coordinating bail-ins in multinational banks. Overall, this literature focuses on how a regulator should react to the information it receives about a bank's situation. We focus instead on an earlier stage, when the regulator has limited bank-level data and bank insiders have private information. We show that the regulator should act promptly rather than wait for bank-specific information to arrive.","Allocating losses: Bail-ins, bailouts and bank regulation",https://www.sciencedirect.com/science/article/pii/S0022053123000686,12 May 2023,2023,Research Article,5.0
"Cai Zhifeng,Dong Feng","Department of Economics, Rutgers University, 75 Hamilton St, New Brunswick, NJ 08901, USA,School of Economics and Management, Tsinghua University, China","Received 5 July 2020, Revised 1 May 2023, Accepted 4 May 2023, Available online 11 May 2023, Version of Record 25 May 2023.",https://doi.org/10.1016/j.jet.2023.105670,Cited by (0),"This paper studies public information disclosure in a model of dynamic financial markets with endogenous information acquisition. Due to an information complementarity, multiple equilibria may emerge, complicating comparative statics analysis. By adding noise to agents' information costs, we establish equilibrium uniqueness using global-game techniques. We show that while public information always crowds out ","Public information disclosure plays a critical role in contemporary financial market policy. Since the 2008 global financial crisis, financial transparency has become increasingly important, leading to the implementation of regulatory measures such as the Dodd-Frank Act of 2010, intended to enhance disclosure in financial markets.====Although information disclosure in financial markets has been demonstrated to offer benefits such as improved market liquidity and reduced firm financing costs, its overall desirability remains a topic of debate.==== In particular, there are concerns that public information disclosure could suppress private information production (Colombo et al., 2014), leading to less informative asset prices overall or even negative welfare consequences (Morris and Shin, 2002; Amador and Weill, 2010; Goldstein and Yang, 2019).==== However, these results are often derived from settings where information held by various parties acts as substitutes, meaning that information provided by one party (e.g., a public authority) diminishes the incentive for others to acquire information. In contrast, recent research has indicated that information is not always a substitute but can also become complementary through various channels. These findings challenge the assumption that public disclosure always has a crowding-out effect on private information production and opens up new perspectives on the dynamics of information disclosure in financial markets.====This paper fills this gap by analyzing the implications of public information disclosure where private information acquisition exhibits strategic complementarity. A challenge arises as information complementarity leads to multiple equilibria, complicating comparative statics analysis. This multiplicity, however, hinges on the strong assumption that all agents possess perfect knowledge about others' expected returns to information acquisition and their corresponding strategic actions in equilibrium. We depart from this paradigm by incorporating strategic uncertainty into agents' information choices, in line with the global-game literature (Morris and Song Shin, 2003).====Our analysis reveals that this refinement not only ensures the existence of a unique equilibrium, as expected, but also surprisingly alters the model's implications concerning public information disclosure. We find that while the disclosure of information always crowds ==== private information in all equilibria under common knowledge, it can instead encourage the acquisition of private information under the unique global-game equilibrium. This is because in certain common-knowledge equilibria, agents have perfect knowledge of how much information other agents have acquired. With information disclosure, the value of information function shifts to the left, resulting in fewer agents acquiring information, thereby leading to the crowding out of private information in all common-knowledge equilibria.====However, under the global-game equilibrium, agents are not able to directly observe the state of the economy and are only provided with a single piece of private information about it. This creates strategic uncertainty, meaning that agents are unsure about how many informed investors there are and must form an expectation about it. In this case, agents care about the changes in the ==== value of information rather than the value of information at a particular point, as in the common-knowledge equilibria. In the presence of information complementarity, the public disclosure of information can increase the ==== value of information, leading to the crowding in of private information acquisition.====In Section 2, we formalize this idea in a general strategic game framework, without specifying the source of strategic substitutability or complementarity. We demonstrate how the global-game refinement can be applied by introducing noise into agents' individual states (i.e., information costs). We then highlight the key difference between common-knowledge equilibria and the global-game equilibrium: under common-knowledge equilibria, agents are fully aware of where the equilibrium is located and focus on the ==== impact of information disclosure, whereas under the global-game equilibrium, agents consider the ==== shift of the payoff function due to strategic uncertainty.====Section 3 presents a model with dynamic information complementarity as in Avdis (2016), extending the Grossman and Stiglitz (1980) model by adding an additional round of trade conducted by new investors. Information complementarity arises through a discount-rate channel, where more informed trading results in less stock price loading on noisy supply variations. This translates to reduced unpredictable noise in future resale stock prices, leading to less discounting in asset prices and increased information value. The value of information becomes hump-shaped and initially increases then decreases with the share of informed investors, causing equilibrium multiplicity. We show that public information disclosure always shifts the hump-shaped information value to the left, causing all interior equilibria to shift accordingly. We conclude that public information always crowds out private information acquisition, even with dynamic information complementarity.====In Section 4, we apply global-game refinement to the model based on Section 2's insights. Our first main result demonstrates that in the unique global-game equilibrium, public information disclosure can crowd in private information acquisition. This crowding-in effect relies on a public-private information complementarity: more public information increases the value of information for private investors since it is observable to future investors, who then trade more aggressively, reducing noise in future ==== prices. This reduces current investors' unlearnable risk, who trade more aggressively, raising the value of information. When this effect is sufficiently strong, the expected value of information increases, crowding in more information. The public-private complementarity is akin to the noise-reduction effect in Bond and Goldstein (2015) and Goldstein and Yang (2019) but with an important distinction: the complementarity effect's strength is endogenous and potentially varies with the model's primitives, as it depends on the overall shape of the information value determined in equilibrium.====Our second main result reveals the “state-dependent” nature of the crowding-in effect. We demonstrate that public-private complementarity is more pronounced when fundamental uncertainty is high. This occurs because under high uncertainty, risk-averse investors nearly cease trading entirely, rendering private information valueless. In this situation, releasing even a small amount of public information has a particularly potent marginal impact on private information acquisition, as it incentivizes investors to resume trading, thereby increasing the value of information. Due to this state-dependence, public information disclosure has a nonmonotonic effect on private information acquisition: as the public signal becomes more precise, it initially crowds in and later crowds out private information acquisition. The analysis thus highlights an advantage of disclosing more information during periods of high market volatility. Essentially, high market volatility/uncertainty diminishes agents' incentive to trade and acquire information. Public disclosure effectively stimulates private trading and boosts the value of information during periods of high uncertainty. This channel surpasses the conventional crowding-out effect, resulting in an improved information environment overall.==== The paper is related to three different strands of literature. First, it draws on financial market models with endogenous information acquisition, initiated by Grossman and Stiglitz (1980). Subsequent works illustrate that complementarity in information acquisition can arise for various reasons.==== Information complementarity can arise because of increasing returns in the information sector (Veldkamp, 2006), private information on endowments (Ganguli and Yang, 2009), relative wealth concerns (García and Strobl, 2011), differential investment opportunities (Goldstein et al., 2014), nonnormal distributions (Breon-Drish, 2015), multiple sources of information (Goldstein and Yang, 2015), and Knightian uncertainty (Mele and Sangiorgi, 2015). This paper is built on dynamic trading models in which information complementarity arises (Froot et al., 1992; Chamley, 2007; Avdis, 2016; Cai, 2019; Glasserman et al., 2021). Benhabib and Wang (2015) extends Grossman and Stiglitz (1980) such that noise traders are replaced with sunspots. Benhabib et al. (2016) considers the idea of complementarity between public information and private information acquisition similar to ours but in a setting where the firm discloses its signal to traders. The contribution of the present paper is to propose a tractable global-game technique that refines multiplicity in information equilibria and thus enhances unique predictions. Another closely related paper is by Chamley (2007), who also applies global-game techniques to study complementarity in information markets.==== Different from this paper, Chamley (2007) does not explore regime switches in response to fundamental changes (e.g., public disclosure), which is the focus of this paper.====Second, this work is related to the literature on disclosure in financial markets. Bond and Goldstein (2015) and Goldstein and Yang (2019) study multiple dimensions of information in essentially static financial markets. This paper instead studies single dimensions of information in a dynamic financial market. The dynamic channel of public information studied in this paper is similar to the mechanisms studied in Hirshleifer (1978), Dye (1990), Gao (2010), and Dutta and Nezlobin (2017). The key difference is that we study the implications for private information production. In a more recent work, Banerjee et al. (2018) illustrates that the crowding-out effect could be strong enough that disclosure makes asset prices less informative. Han et al. (2016) studies public disclosure in the presence of endogenous noise trading and finds that more precise information attracts more noise trading and that this crowds in private information production. Kurlat and Veldkamp (2015) investigate the social value of public information and shows that information can improve investors' welfare only when issuers strategically manipulate the supply of assets to obfuscate information or the information encourages firms to take on riskier investments. Gaballo and Ordoñez (2021) investigate the role of public information in market insurance and finds that more public information can be socially undesirable.====Finally, this paper relates to the literature on global games. The key insight that departing from common knowledge may restore uniqueness in coordination games stems from the seminal works by Carlsson and van Damme (1993) and Morris and Shin (1998). We consider a novel application to the information acquisition game in financial markets. This application also helps to address a critique of the global-game literature that comparative statics of global-game selections can be the same as the comparative statics of the equilibria of the unperturbed underlying game. This paper provides an example where the two yield strikingly different predictions.==== Szkup and Trevino (2015) introduce endogenous information acquisition into a generic global game of regime switching. Ordoñez (2013) uses global-game techniques to study equilibrium fragility in a credit market with reputation concerns.",Public disclosure and private information acquisition: A global game approach,https://www.sciencedirect.com/science/article/pii/S0022053123000662,11 May 2023,2023,Research Article,6.0
Pei Harry,"Department of Economics, Northwestern University, United States of America","Received 28 September 2022, Revised 1 March 2023, Accepted 27 April 2023, Available online 5 May 2023, Version of Record 15 May 2023.",https://doi.org/10.1016/j.jet.2023.105668,Cited by (0),I study a repeated communication game between a patient sender and a sequence of myopic receivers. The sender has persistent ,"The commitment power of informed experts affects communication outcomes. In Crawford and Sobel (1982)'s seminal model, an expert's temptation to mislead his audience undermines his credibility. When an expert can commit to a disclosure policy, as in the model of Kamenica and Gentzkow (2011), his messages are more credible and can have more influence over others' decisions.====In practice, experts often face credibility issues when committing to disclosure policies. For example, real estate agents receive high commissions when their clients purchase overpriced properties, and pharmaceutical lobbyists have incentives to recommend policies that benefit pharmaceutical companies but hurt social welfare. These experts' optimal disclosure policies require them to reveal unfavorable information, such as that the property they are selling is overpriced or the policy they are lobbying for is socially harmful. However, if they are aware of such unfavorable information, honoring their commitment to reveal it is against their own interests.====A plausible foundation for an expert's commitment is that he communicates with ====, one in each period. Each receiver observes the optimal actions of previous periods and is able to compare them to the expert's recommendations. However, when the expert's optimal disclosure policy is stochastic, which is the case in the leading example of Kamenica and Gentzkow (2011), the result in Fudenberg et al. (1990) implies that the expert can never attain his optimal commitment payoff in the repeated communication game, no matter how patient he is.====Several insightful papers bypass the above problem by introducing alternative forms of commitment, under which a patient expert can attain his optimal commitment payoff in the repeated game. For example, Best and Quigley (2022) assume that there is a third-party who can commit to a public record system that determines which information the receivers can observe about the past history. Mathevet et al. (2022) assume that with positive probability, the expert is a commitment type who uses his optimal disclosure policy in every period.====I examine the extent to which patient experts can restore their commitment power when it is common knowledge that they ==== honor any promise against their own interests. I focus on a repeated version of the leading example in Kamenica and Gentzkow (2011). Each period, a patient sender privately observes an i.i.d. state, which is either ==== or ====, and recommends a ==== or a ====. He may also send a cheap talk message that has no intrinsic meaning together with his action recommendation. A myopic receiver chooses an action after observing the sender's recommendation and message, together with the history of states, recommendations, and messages.====The receiver prefers to match her action with the state. The sender prefers the good action regardless of the state. My modeling innovation is that the sender incurs a psychological cost of lying whenever his action recommendation does not match the realized state.==== I assume that this lying cost is fixed over time and is the sender's private information, which I call his ====.====I start from a setting where the lying costs of all types are lower than the sender's benefit from the good action, i.e., every type prefers to mislead the receiver in the bad state when his recommendation is taken at face value. Theorem 1 characterizes the ==== for each sender type in the limit where the sender is arbitrarily patient. This payoff depends only on his own lying cost and ==== in the support of the receivers' prior belief. Intuitively, when a sender type decides which of the other types to imitate, he prefers to imitate types that occur with higher probabilities, since it takes fewer periods to build reputations for behaving like those types; he also prefers to imitate types that have higher lying costs, since those types are more trustworthy and reputations for behaving like them are more valuable. When the sender is more patient, he puts more weight on his long-term payoff after he has established a reputation and puts less weight on his stage-game payoff in periods where he builds his reputation. As a result, only the sender's true lying cost as well as the lying cost of the highest-cost type affect his highest equilibrium payoff.====My result implies that senders of all types except for the highest-cost one can benefit from their persistent private information, in the sense that their highest equilibrium payoff in the repeated incomplete information game is ==== greater than their highest equilibrium payoff in a repeated game where their lying cost is common knowledge. This is because the highest-cost type is the most trustworthy type, so he has no good candidate to imitate in the incomplete information game.====My result also implies that the highest equilibrium payoff for every type converges to his optimal commitment payoff as the highest lying cost converges to the sender's benefit from the good action. This can be viewed as a justification for the commitment assumption in Bayesian persuasion models: Even though it is common knowledge that no type can commit and every type prefers to mislead the receivers in the stage game, all types of the sender can approximately attain their optimal commitment payoffs in the repeated communication game as long as the set of types is rich enough.====Next, I allow for ==== whose lying costs exceed their benefits from the good action. Theorem 2 shows that there exist equilibria where all ethical types attain their optimal commitment payoffs and all non-ethical types attain the payoffs described in Theorem 1. This is because in the first period, the sender can credibly reveal whether he is ethical via a cheap talk message. After he claimed to be ethical, he always recommends the receiver-optimal action and the receivers follow his recommendation. After he claimed to be non-ethical, players coordinate on the sender-optimal equilibrium in the game without ethical types. Theorem 2 implies that all types can approximately attain their optimal commitment payoffs as the set of types becomes rich enough.====In the case where the sender can only recommend actions but ==== send cheap talk messages, Theorem 3 shows that all the non-ethical types can attain their optimal commitment payoffs ==== the lying cost of the highest type is below some cutoff. This suggests that the presence of types with high lying costs can lower the non-ethical types' equilibrium payoffs. This observation is driven by a novel ====. Suppose there is a type who has an infinite lying cost. In equilibrium, this type will never lie, so the other ethical types can secure their full disclosure payoffs by imitating his behavior. As a result, the non-ethical types cannot lie while pooling with the ethical types and hence cannot obtain their optimal commitment payoffs. More generally, each ethical type's lowest equilibrium payoff increases with the highest lying cost in the support of the receivers' prior belief, and the non-ethical types cannot attain their optimal commitment payoffs when every ethical type's payoff from imitating the highest-cost type exceeds some threshold.====  My paper is related to the literature on repeated communication games pioneered by Aumann and Maschler (1965), Sobel (1985), and Benabou and Laroque (1992).==== Best and Quigley (2022) and Mathevet et al. (2022) use this framework to provide justifications for the sender's commitment power in Bayesian persuasion models.====Best and Quigley (2022) show that the sender can attain his optimal commitment payoff if a third party can commit to a particular public record system that determines the receiver's information about the game's history. Mathevet et al. (2022) show that a patient sender can attain his optimal commitment payoff whenever there is a positive probability that he is a ==== who uses his optimal disclosure policy in every period. In contrast to those papers, I provide a justification for the commitment assumption using a repeated communication game where it is ==== that the sender cannot honor any promise against his own interest.====One limitation of my approach is that it restricts attention to a special class of games. Although my upper bound on the sender's payoff can be extended to more general settings, to establish that these bounds are attainable, one needs to construct equilibria in repeated communication games with persistent private information, which is an intractable problem.==== Hence, I view my main result as a proof of concept rather than a general lesson. It demonstrates that it is possible for a patient sender to obtain his optimal commitment payoff even when it is common knowledge that (i) he ==== commit to disclosure policies and that (ii) he has a strict incentive to deceive the receivers.====I also characterize the extent to which the sender can partially overcome his lack-of-commitment problem in environments where he cannot attain his optimal commitment payoff. This is related to Lipnowski et al. (2022), who characterize the sender's highest equilibrium payoff in a one-shot game when he can commit to a disclosure policy with positive probability. In Guo and Shmaya (2021) and Nguyen and Tan (2021), the sender faces a lying cost, chooses an information structure, and receives information about the state according to that information structure. By contrast, the sender in my model cannot commit to receiving coarser information about the state.====This paper is also related to the literature on repeated games with incomplete information, pioneered by Aumann and Maschler (1965). Hart (1985), Shalev (1994), and Pȩski (2014) characterize the set of equilibrium payoffs when all players are patient. When the uninformed player is impatient, Cripps and Thomas (2003) show that every equilibrium payoff must satisfy the conditions in Shalev (1994), although some of the payoffs that satisfy this necessary condition cannot be attained in any equilibrium. By contrast, I characterize a patient informed sender's highest equilibrium payoff when his opponents' discount factor is either zero or close to zero. This paper is also related to Pei (2021) who characterizes a patient seller's highest equilibrium payoff when he has private information about his production cost. I elaborate on the differences between the two papers in Section 6.",Repeated communication with private lying costs,https://www.sciencedirect.com/science/article/pii/S0022053123000649,5 May 2023,2023,Research Article,7.0
"Solan Eilon,Zhao Chang","School of Mathematical Sciences, Tel-Aviv University, Israel,School of International Trade and Economics, University of International Business and Economics, China","Received 25 October 2021, Revised 18 December 2022, Accepted 26 April 2023, Available online 3 May 2023, Version of Record 19 May 2023.",https://doi.org/10.1016/j.jet.2023.105667,Cited by (0),"We consider a dynamic inspection problem between a principal and several agents. The principal observes the full inspection history, whereas each agent only observes inspections imposed on himself. When inspection resources are limited, the inspection intensities for agents are negatively correlated, and hence each agent cares not only about his own inspection history, but also about the inspection histories of the other agents. In such cases, should the principal publicly reveal past inspection history, or should she conceal this information? We show that the principal benefits from concealing inspection history. Nevertheless, this benefit becomes less significant as the number of agents increases, and disappears in the limit case with a continuum of agents.","A utility-maximizing agent (he) can often increase his utility by breaking some rules: given the opportunity, firms will violate regulations that increase their costs, workers will shirk on their tasks, and taxpayers will under-report their income. To reduce the rate of violations, it is customary for the principal (she) to use inspections as a means to discipline agents. The principal's inspection scheme determines which agents are to be inspected in each period, as well as the information on past inspections that will be revealed to the agents.====There are two channels to affect agents' beliefs on past inspections. The first operates by concealing an agent's inspection result from himself: that is, an agent who is inspected is not informed of his own inspection outcome. This channel has been well studied in the literature,==== and Fuchs (2007) shows that the principal is best off keeping the agent uninformed of the intermediate outcome realizations until the last period, since this allows for the reusability of punishments. Nevertheless, concealing an agent's own inspection result from himself is possible ==== when the performance measure is subjective. Scenarios like environmental inspection or tax auditing do not fit into this category. In this paper, we focus on contexts where the performance evaluation is objective, and each agent knows his own inspection history. In these cases, the principal has another channel to affect agents' beliefs on past inspections, namely, by strategically revealing the inspection history of the other agents.====At a first glance, if each agent's payoff depends only on the actions he takes and the inspection intensity he faces, the inspection outcomes for other agents are irrelevant to the agent. This is not true when inspection resources are limited and full compliance is not implementable. Indeed, in such cases, the inspection intensities for agents are negatively correlated — from the perspective of agent ====, a lower inspection intensity for other agents implies that more inspection resources are left for agent ====, and hence he is less willing to violate. When the principal uses an inspection scheme that is history dependent, the strategic revelation of past inspection histories constitutes an important policy instrument.====In reality, more often than not, no information about inspection outcomes is publicly available. Yet the past few decades have witnessed a considerable increase in the amount of public information provided by inspection authorities. Back in the 1980s, the US started adopting the “public disclosure programs” that mandate public reporting of firms' compliance with environmental regulations. Since then, similar programs have been developed in many countries.====Despite the growing popularity of such public disclosure programs, their benefits are open to debate, and no universal and firm conclusions can be drawn from the empirical ground. Some empirical works support the view that disclosure has a positive impact, but there are also some doubts about this conclusion, due to lack of data prior to the introduction of the disclosure program,==== the implementation of other programs concurrently with the disclosure program,==== and the non-universality of the positive results==== (Folmer and Tietenberg, 2007).====From a theoretical perspective, there is a large literature devoted to the effect of publicizing inspection results. Yet, most of these works have abstracted away from the direct interaction between inspectors and agents. This literature shows that with public disclosure, firms face additional pressure for compliance from third parties, e.g., neighboring communities, consumers, investors, or stockholders. This helps to discipline the behavior of firms and yields a positive result (see, e.g., Hamilton, 1995, Konar and Cohen, 1997, Lanoie et al., 1997, Tietenberg and Wheeler, 2001, Foulon et al., 2002, Blackman et al., 2004, Stephan, 2002).====We contribute to this literature by focusing on the interaction between inspectors and agents, and showing that contrary to previous positive results, when the number of agents is small, public disclosure of inspection histories may hurt the principal. This being said, the advantage of concealing inspection histories becomes less significant as the number of agents increases. In the limiting game with a continuum of agents, the advantage of concealing information disappears. We mainly deal with the model where the principal has a commitment power. The no-commitment case is discussed in Section 3.2.====Formally, we compare two realistic monitoring structures: public monitoring and private monitoring.==== Under ====, the principal announces her observations after each period (that is, the identity of the inspected agents and their actions). Under ====, the principal conceals her observations (that is, each agent only observes his own inspection history).====To illustrate the superiority of private monitoring, we start with an important feature of the optimal inspection scheme under public monitoring. Under public monitoring, it is optimal for the principal to adopt an inspection scheme that favors agents who successfully passed inspections (i.e., were found adhering) in early periods. Indeed, to improve upon the static scenario, a successfully passed inspection has to be accompanied by a future bonus to the agent: that is, the promise of the principal turning a blind eye with positive probability in the future, so that the agent can violate without penalty. The feature that greater compliance typically leads to less enforcement is well recognized in both theory and practice.==== When the principal plans her activities according to a pre-allocated budget, less enforcement on certain agents implies greater enforcement on other agents. This implies that agents who are inspected ==== often in early periods are subject to ==== enforcement later on, and they violate less in the long run.====Under private monitoring, using a similar inspection scheme, but adding a small probability to no inspection in early periods, the principal manipulates agents' beliefs to her advantage: this way, with a positive probability each agent assigns a high probability to the event that the other agents were inspected in early periods and hence he himself will face high inspection intensities in the future. Since all agents follow the same reasoning, in such eventuality all agents violate less in the long run.====From a technical perspective, the advantage of private monitoring over public monitoring is that it allows the principal to transfer inspection resources across histories, and attain a more efficient use of it. We illustrate this important point by an example. Suppose that agent ==== adheres if and only if the probability he is inspected is at least ====, and suppose that there are two histories ==== and ====, which differ only in the actions of some other agent ====. Suppose further that at ==== (resp., ====) agent ==== is inspected with probability 1 (resp., 0). Under public monitoring, agent ==== adheres at ==== and violates at ====. Under private monitoring, agent ==== cannot distinguish between ==== and ====. If these two histories constitute an information set for agent ====, and if, given that the information set is reached, agent ==== assigns probability at least ==== to ====, then at this information set agent ==== adheres. In particular, he adheres at ====.====Thus, the potential superiority of hiding information (private monitoring) stems from pooling some histories together, so that an agent's incentive constraint has to be satisfied only in expectation rather than state by state. This pooling is advantageous only if at some histories the inspection resource is inevitably superfluous (hence agents' incentive constrains are slack), and the pooling allows a more efficient use of this redundant resource as it can average out some other histories where the inspection resource is scarce.====The above effect is more significant when the number of agents is small. As the number of agents increases, under public monitoring, the ability to finely divide agents into groups (with agents in the same group being treated similarly) allows the principal to attain a more efficient use of the inspection resource at any given history, and the gain from pooling histories decreases. In the limit case with a continuum of agents, the perfect divisibility of agents allows public monitoring to attain the most efficient use of the inspection resource, so that agents' incentive constraints are binding at any history, and there is no gain from pooling histories.====The key point of our paper is that sometimes the principal wants to pool information to keep agents in the dark, so as to attain a better use of her resources.==== This manipulation works under various conditions, which include, among others, a negative correlation between the resources available for inspecting each agent,==== a commitment power of the principal,==== and the inability of agents to communicate and figure out the past actions of the principal.==== This result has important policy implications. As mentioned before, a large literature argues that public monitoring has an important advantage over private monitoring, as it makes firms face additional pressure for compliance from third parties. We show that the call for publicly disclosing more information may miss out some of the tradeoffs that arise under limited inspection resource. Especially when having a few agents, the flexibility in manipulating agents' beliefs offered by private monitoring may outweigh the said advantage of public monitoring. Yet as the number of agents increases, the advantage of private monitoring disappears, and public monitoring becomes unequivocally preferable.====Our paper is related to information design problems where the designer can send private signals to agents. In ====, Bergemann and Morris (2016) and Taneva (2019) relate the optimal information disclosure to the best Bayes correlated equilibrium from the sender's perspective, and propose mechanisms to compute the optimal utility of the sender. In particular, Bergemann and Morris (2016) show that more information reduces the set of outcomes by imposing more incentive constraints. Arieli and Babichenko (2019) study the private information disclosure in a specific Bayesian persuasion model of product adoption, and analyze how information should be revealed optimally by the firm in order to maximize its revenue as a function of its utility and the utilities of the consumer.====Even though in the static model there is a clear pattern that more information reduces the set of outcomes and hence hurts the designer, this is not necessarily the case in repeated games. In dynamic settings, Matsushima (1991) and Bhaskar and Van Damme (2002) provide examples of repeated games where public monitoring is superior to private monitoring. In these examples, public disclosure is beneficial because it implies a larger set of strategies for the players, since they can condition their future actions on more information.====In contrast, Phelan and Skrzypacz (2012) provide dynamic examples in which less information benefits incentives. In these examples, players do not observe each other's action directly. Instead, they observe a noisy signal of it. In such cases, a less accurate signal serves to make the threat of punishment credible: if a player knows too well the action of the opponent, then even if he observes unexpected signals, the incentive to punish the opponent can be insufficient. More noise on the signal helps in generating more uncertainty on the opponent's past actions, which makes it easier to satisfy incentive constraints.====In our model, each agent interacts mainly with the principal, and his incentive to punish a deviation of another agent is less relevant. The change on the monitoring structure thus affects agents' behavior in a way different from Phelan and Skrzypacz (2012): in the two-period game, under private monitoring, the equilibrium play in the second period can be a correlated equilibrium of the stage game (with private histories as the correlation device), while under public monitoring, the equilibrium play in the second period is necessarily a Nash equilibrium of the stage game.====The fact that private histories can constitute an appropriate correlation device for the second-period play to support the efficient correlated equilibrium has long been observed in the private strategy literature (see, e.g., Lehrer, 1991, and Mailath et al., 2002). Lehrer (1991) studies two-player infinite undiscounted repeated game with a specific public monitoring structure; Mailath et al. (2002) provide an example with two agents and two periods. We study finitely-repeated inspection problems that involve multiple agents. In addition to the above driving force, there exist others that favor public monitoring (see Remark 2 and Theorem 5). We identify conditions under which private (or public) monitoring dominates.====Andrews and Barron (2016) study a dynamic relational contract problem between a principal and several agents, and show that concealing information may be beneficial to the principal. In their paper, monetary transfers between the principal and each agent are allowed. If the principal can commit to a strategy, then the first-best outcome can be implemented with a simple stationary contract. Andrews and Barron (2016) therefore focus on no-commitment cases (the outputs produced by agents are non-contractable) and construct a dynamic allocation rule — the Favored Supplier Allocation — that attains the first-best outcome whenever any allocation rule does. In such cases, information does not play a crucial role, and the principal does not benefit from disclosing/concealing information on past play. They show that in the two-agent case, when the first-best outcome cannot be attained, full disclosure may not be optimal and concealing some information may yield a superior outcome to the principal.====Similarly to ours, this last result is related to the literature on correlated equilibrium: The private histories can constitute an appropriate correlation device for future play. A key difference between our paper and Andrews and Barron (2016) is that we do not allow monetary transfers between players. Therefore, even in the commitment case, concealing information can be superior. Also, we study cases that involve more than two agents. In particular, in the no-commitment case, we show that when the number of agents is large, in sharp contrast to the two-agent problem, ==== monitoring can be strictly superior to private monitoring.====Kandori and Obara (2006) study the difference between public and private strategies, and show that the latter may be superior. They consider a repeated prisoner's dilemma game with public signals on the stage outcomes. In their model, the signal is rather insensitive to a deviation at the cooperative point, but it is quite sensitive to one's deviation when the opponent is playing ====. In such cases, private strategies (which depend not only on public signals, but also on players' own actions in the past) can achieve a more efficient outcome than public strategies (which depend solely on the history of publicly observable signals). This is because private strategies allow players to use additional information (i.e., their own past actions) to improve the efficiency of punishments. In contrast, in our model private monitoring implies that an agent has ==== information than in the model with public monitoring, since he can observe the actions of others only in the latter case.====Our paper is also related to repeated moral hazard problems that involve private evaluation. In those problems, there is an agent who takes an action, which generates a (noisy) signal to the principal. The principal does not observe the action of the agent, and the agent does not observe the signal to the principal. Levin (2003) and MacLeod (2003) were the first to analyze this environment, yet their analysis is essentially static. Fuchs (2007) extends the analysis to a dynamic environment, and analyzes the optimal timing to reveal the principal's private signal. A single-agent problem is studied and it is shown that the principal is best off keeping the agent uninformed of the intermediate outcome realizations until the last period.====In our paper, similarly to Fuchs (2007), concealing inspection histories makes the use of the reward more efficient. Nevertheless, there is a substantial difference: Unlike Fuchs (2007), we study multiple-agent problems, and the uncertainty no longer stems from the agent's own inspection history, but rather from ==== agents' inspection histories. Moreover, unlike Fuchs (2007), where monetary transfer is allowed, in our paper the reward can only take the form of violations and is bounded by 1 in each period. These two differences in the setup imply the following differences in the equilibrium analysis. First, in our model with multiple agents and no monetary transfer, it is no longer clear that concealing information is always better (we elaborate on this point in Remark 2 and Theorem 5). Second, for concealing information to be beneficial, a negative correlation between agents is required, and hence the inspection cost has to be convex in the number of inspections. Third, the effect of information now depends crucially on the number of agents.====The paper is organized as follows. Section 2 studies the benchmark model with one inspector and two agents, and Section 3 generalizes the benchmark result. Discussion appears in Section 4. Proofs are relegated to the Appendix.",When (not) to publicize inspection results,https://www.sciencedirect.com/science/article/pii/S0022053123000637,3 May 2023,2023,Research Article,8.0
"Nezafat Mahdi,Schroder Mark","Ross School of Business, University of Michigan, Ann Arbor, MI 48109, United States of America,Broad College of Business, Michigan State University, East Lansing, MI 48824, United States of America","Received 16 July 2020, Revised 11 March 2023, Accepted 18 April 2023, Available online 26 April 2023, Version of Record 9 May 2023.",https://doi.org/10.1016/j.jet.2023.105664,Cited by (0),"In the standard perfectly competitive rational expectations model of information acquisition, the marginal benefit of ","Over the last few years, retail traders have received considerable attention by the media as it is widely believed that their behavior has led to high levels of price volatility for stocks with small market capitalization.==== Traditionally, such traders have been modeled as noise traders, and, typically, they have been treated as an afterthought. This is because in traditional rational expectation models, noise traders have little influence on asset prices in equilibrium (see, e.g., the model in De Long et al. 1990 for an opposing view). In this paper, we investigate how the bullishness/bearishness of noise traders and their trading volume affects the incentives of rational traders to obtain private information in an imperfect competition model.====In the standard perfect-competition model of asset prices, the bullishness/bearishness of noise traders has no effect on rational traders' incentives to obtain private information, and an increase in noise-trading volume generally increases the incentives to obtain private information. However, evidence suggests that retail investors tend to have a meaningful impact on returns of stocks with small market capitalization (see Kumar and MC Lee 2006), and it is well known that the market for such stocks is imperfectly competitive. We show that in contrast to the perfectly competitive models, in an imperfect-market setting, noise trading of sufficient volume and strong bullishness or bearishness can completely eliminate rational traders' incentives to obtain private information. This can lead to excess volatility and prices that are decoupled from fundamentals.====Our model is static, with one risky asset being traded in the financial market. The market is populated by rational traders, who can obtain private information, and noise traders. The model has two stages: first an ====, and second a ====. In the information-acquisition stage, each rational trader chooses a costless private-signal precision while anticipating both the price impact at the trading stage and the impact of his precision choice on the trading strategies of all other investors. In the trading stage of the model, rational investors choose their demand schedules and trade after observing their private signals.====We provide necessary and sufficient conditions for the existence of a Nash equilibrium in which all rational traders obtain a zero-precision signal (i.e., no private information acquisition). That is, we provide conditions under which no individual investor optimally chooses a positive-precision signal in response to zero-precision signals received by all the other investors, despite the availability of costless information.====Unlike the case of perfect competition, traders' demand functions in an imperfect market are interlinked through both price informativeness and ====, which represents the anticipated dollar change in the equilibrium price resulting from a unit increase in shares purchased. We will focus our discussion on market impact because it is the main driver of zero-information equilibrium. Each investor's demand function specifies the optimal number of shares as a function of the private signal and the market price; this function is linear because of the Gaussian information setting. Each trader's price impact is declining in the price sensitivity of demand (i.e., the coefficient of price in the demand function) of each of the other traders. This follows because if a particular trader wants to buy 100 shares of stock, and the other traders have highly price-sensitive demand functions, then a very small price increase will be sufficient to induce the other traders to collectively sell 100 shares. In fact, Kyle (1989) shows that each trader's price impact equals the inverse of the sum of the price sensitivities of all the other traders.====Now consider how information acquisition affects market impacts and demand functions. The intuition is clearest if we assume, for the sake of argument, that investors ignore the information in the price. Suppose an individual rational trader, who we refer to as the ====, chooses a positive-precision signal while the other rational traders, who we refer to as the ====, receive a zero-precision signal. The reduced payoff uncertainty resulting from a more precise private signal increases the price sensitivity (as well as the signal sensitivity) of the deviating investor's demand, and this increased price sensitivity reduces the price impacts of the conforming traders. Moreover, the decline in the conforming traders' price impacts increases their demand-function price sensitivities, which further reduces price impacts. Lower price impacts (i.e., more liquid markets) generated by the deviating investor's improved signal cause ==== rational investors to trade more aggressively (i.e., increase the absolute size of their trades), which reduces the equilibrium absolute risk premium of the stock. A zero-precision equilibrium arises when the utility cost of the lower risk premium exceeds the utility benefit from more precise private information. When we reintroduce learning from the price, the price-impact mechanism explained above holds only if the price informativeness is sufficiently weak: that is, noise-trading volume is sufficiently high.====To summarize, an increase in the precision of private information increases price sensitivities (if the price is not too informative), and increased price sensitivities of trading increases liquidity. An increase in liquidity induces all the traders to take absolutely larger positions, which shrinks the absolute risk premium. The trade-off faced by the investor acquiring information, or provided free information, is the payoff-forecasting benefit of the information versus the possible deflating effects of a more liquid market on the absolute risk premium.====Noise trading gives rise to zero-information equilibria through two channels: 1) If noise-trading volume is sufficiently high, then price informativeness is muted. Low price informativeness makes traders' demands more price sensitive, and causes price impacts to decline with the signal precision chosen by the deviating trader. 2) Strong noise-trader bullishness or bearishness increases the equilibrium absolute risk premium, making it more sensitive to the increased competitiveness of trading resulting from endogenously declining price impacts. The stronger the bullishness/bearishness of noise traders, the more sensitive the absolute risk premium is to an increase in the deviating trader's signal precision.====We also examine a ==== in which each trader chooses her precision and demand schedule while holding fixed the demand schedules of the other traders (see, e.g., Vives 2014 for a similar setting). In contrast to our two-stage model, we show that the marginal value of private information to the informed trader is always strictly positive because traders in the single-stage model do not anticipate the effects of their precision choices on the optimal trading strategies of the other investors. A zero-information Nash equilibrium, therefore, cannot exist in a single-stage model with costless information.====An extensive body of literature has explored the roles of private information in perfectly competitive models to understand a range of empirical observations that would otherwise seem puzzling (see, e.g., Van Nieuwerburgh and Veldkamp 2009 and Veldkamp 2006, among others). However, the literature on the roles of information in imperfectly competitive settings is scarce because of the complexity of the equilibrium. Subrahmanyam (1991) studies the endogenous ==== of informed traders through a numerical example in an extension of the imperfectly competitive model presented in Kyle (1985), and argues that market liquidity may not be monotonic in the variance of liquidity trades. The seminal paper Kyle (1989) also studies the number of informed traders willing to pay a fixed cost to acquire a given amount of information. However, an equilibrium in which no trader chooses to pay a fee to become informed is much different from a zero-precision equilibrium caused by a negative marginal value of information. Any market can have an equilibrium with no buyers for a fixed discrete quantity of a good when the cost is prohibitive.====Our paper is related to the seminal work of Grossman and Stiglitz (1980), who argue that in the absence of noise, prices perfectly reveal the information available to traders, thereby undermining the incentive to acquire costly information. A zero-information equilibrium cannot exist either because with no informed traders in the market, the trading advantage of information would exceed its cost for any individual trader.==== This result is known as the Grossman-Stiglitz paradox. Their “Conjecture 5” posits that the proportion of informed traders in equilibrium will increase in the magnitude of noise. We show that this conjecture fails to hold with ==== competition: when noise exceeds some threshold, the marginal benefit of information can be negative even when all traders are uninformed, resulting in a zero-information Nash equilibrium, even if information is free.====Our paper is also related to Hirshleifer (1971) and Morris and Shin (2002). Hirshleifer (1971) shows that in the standard competitive market setting with complete markets, public information can hurt all market participants, as it reduces the opportunities for risk-sharing; however, private information always has a positive value because the investor can act on the information to favorably reallocate consumption. Morris and Shin (2002) argue that public information plays a dual role of conveying fundamental information and serving as a focal point of beliefs. They show that in a model with public and private information, private information that is more precise always increases social welfare, but when private information is very precise, an increase in public information can reduce social welfare because public information facilitates coordination, which may lead to distorted action. However, the marginal utility of private information to each trader is always positive in their model: With their assumed continuum of traders, an improved private signal of one trader has no effect on the strategies of the others.====On a policy note, our paper raises several issues that are of interest to regulators. Many assets, including corporate bonds, shares of small-cap companies, and shares of privately held firms, are traded in imperfectly competitive markets, and introducing regulations to improve price efficiency and liquidity in these markets is a priority to regulators.==== Our results demonstrate the challenges faced by regulators to improve price efficiency and liquidity. For example, an increase in trading volume by noise traders encourages information acquisition in competitive markets, but can eliminate information acquisition altogether in imperfect markets.",The negative value of private information in illiquid markets,https://www.sciencedirect.com/science/article/pii/S0022053123000601,26 April 2023,2023,Research Article,9.0
Borie Dino,"CY Cergy Paris Université, THEMA, Cergy, France","Received 24 June 2021, Revised 17 April 2023, Accepted 18 April 2023, Available online 24 April 2023, Version of Record 28 April 2023.",https://doi.org/10.1016/j.jet.2023.105665,Cited by (0),"We axiomatize the maxmin expected utility model in Savage's original framework, which does not require a rich set of consequences nor objective probabilities. The key conditions in our result reformulate Gilboa–Schmeidler's (1989) Uncertainty Aversion and Certainty Independence for Savage's acts. Also, we propose a definition for comparative ambiguity aversion in this context.","The work of Ellsberg (1961) ignited a large literature aimed at developing decision models under ambiguity. In a seminal contribution, Gilboa and Schmeidler (1989) suggested the theory of maxmin expected utility (MEU), according to which beliefs are given by a set of probabilities, and decisions are made in order to maximize the minimal expected utility of the act chosen.====The result was obtained in Anscombe and Aumann's framework, where acts are maps from states to objective lotteries over consequences. Namely, an environment that combines two independent sources of uncertainty: a subjective state space with an exogenous randomization device capable of generating arbitrary objective probabilities. While elegant and simple, this approach ultimately relies upon objective uncertainty and its independence from subjective uncertainty.====The differences between Savage framework's and Anscombe and Aumann framework's are not only philosophical. For instance, Eichberger and Kelsey (1996) show that for MEU preferences, in Savage's framework, uncertainty aversion does not imply a strict preference for randomization; and Sarin and Wakker (1992) show that the two formulations yield different conclusions about the preference rankings of acts for Choquet expected utility.====More recently, several contributions, that avoid the use of objective probabilities, have proposed different axiomatizations of the MEU criterion in a Savage-style setting. Casadesus-Masanell et al. (2000a), Casadesus-Masanell et al. (2000b), Ghirardato et al. (2003), and Alon and Schmeidler (2014) established purely subjective versions of Gilboa and Schmeidler's representation result by imposing (or deriving in the case of Ghirardato et al. (2003)) topological or structural restrictions upon the consequence space. However, a rich set of consequences excludes decision problems where there are just a few deterministic consequences like in medical decisions.====In this paper, we provide necessary and sufficient conditions for a preference relation over Savage acts to admit a MEU representation. Our axiomatic does not rely on objective probabilities and is applicable to a finite set of consequences. This characterization can help us see more clearly what the exact implication of the rule is in many applications.====In arriving at this result, there are two key steps. The first is to introduce a preference-based notion of perceived ambiguity that can distinguish between ambiguous and unambiguous events. For MEU preferences, the set of unambiguous events is in fact equal to the collection of events over which the set function representing the decision maker's willingness to bet is complement-additive, as shown by Ghirardato and Marinacci (2002). Differentiating between ambiguous and unambiguous events allows us to reformulate ==== which is the first key axiom of Gilboa and Schmeidler (1989). The second is to define a notion of subjective “midpoint” of acts from the decision maker's preference relation. This definition is an important step in the characterization of subjective mixtures in Savage's original framework. We can then reformulate ====, the second key axiom of Gilboa and Schmeidler (1989).====Recent decision-theoretic models relax the certainty independence and uncertainty aversion axioms in specific ways. The present paper provides a useful starting point for developing these models in the Savage framework. For instance, variational preferences (Maccheroni et al. (2006)) weaken certainty independence; the model studied by Cerreia-Vioglio et al. (2011) drops certainty independence entirely, but retains uncertainty aversion. In contrast, Siniscalchi (2009) weakens certainty independence and drops uncertainty aversion entirely; Chandrasekher et al. (2022) drop only uncertainty aversion and characterize the class of invariant biseparable preferences (Ghirardato et al. (2004)).====More generally, with the definition of subjective midpoints introduced in this paper, all decision-theoretic models that use the convex structure of Anscombe and Aumann's framework can be studied in Savage's framework. For instance, Denti and Pomatto (2022) characterize the class of identifiable smooth ambiguity preferences by a joint weakening of Savage's sure-thing principle and Anscombe and Aumann's mixture independence.====We also analyze comparative attitudes toward ambiguity. We show that more ambiguity averse MEU preferences are characterized, up to a normalization, by larger sets of priors.====Our paper complements the original axiomatization of MEU developed by Gilboa and Schmeidler (1989) and other studies in a Savage-style setting, as cited above. Since Anscombe and Aumann's approach assumes that the consequence space is a convex subspace of a linear space; all the above mentioned works impose richness restrictions upon the consequence space. The difference between our work and those cited above is that we impose richness restrictions upon the state space. Our construction forces the state space to be infinite but has at least two significant conceptual advantages. First, it enables us to work with a finite set of consequences. For instance, in the case of two consequences, uncertainty perception may be identified with betting behavior; that is, a decision maker deems event ==== more desirable than event ==== if she would rather bet on ==== than on ====. Our representation theorem is then equivalent to a representation by a coherent lower probability.==== Second, in comparison with Anscombe and Aumann's framework, the distinction between an ambiguous event and unambiguous event is endogenous and their independence, if any, is not exogenously assumed.====As to the papers that discuss ambiguity aversion, we were inspired by Ghirardato and Marinacci (2002). We show how their ideas can be transposed to our setting. Therefore, we refer to that paper for detailed discussion of the relation of what we do with other works that address the characterization of ambiguity attitude.====The paper is organized as follows. Next we introduce notations and definitions. In Section 3, we axiomatize MEU. In Section 4, we explore comparative ambiguity aversion. We discuss alternative definitions of the set of unambiguous events in section 5. Finally, proofs are given in the Appendix.",Maxmin expected utility in Savage's framework,https://www.sciencedirect.com/science/article/pii/S0022053123000613,24 April 2023,2023,Research Article,10.0
"Lichtig Avi,Weksler Ran","University of Bonn, Germany,University of Haifa, Israel","Received 5 January 2022, Revised 16 March 2023, Accepted 6 April 2023, Available online 14 April 2023, Version of Record 20 April 2023.",https://doi.org/10.1016/j.jet.2023.105653,Cited by (0),"Does a better-informed sender transmit more accurate information in equilibrium? We show that, in a general class of voluntary disclosure games, unlike other strategic communication environments, the answer is positive. If the sender's evidence is more Blackwell informative, then the receiver's equilibrium utility increases. We apply our main result to show that an uninformed sender who chooses a test from a Blackwell-ordered set does so efficiently.","Voluntary disclosure plays a central role in many markets with information asymmetry. Even when agents' interests are not aligned, if an informed agent holds hard evidence, she can disclose pieces of it to promote her interests. The ubiquitousness of voluntary disclosure in communication environments, e.g., of annual reports by public companies, verifiable curricula vitae by job candidates, etc., has inspired a vast body of literature in economics, finance, and accounting.====A question that has not been answered yet by this literature is whether a sender with better information transmits more of it in equilibrium. In cheap-talk models, for example, the answer is negative.==== Recent literature addressing other communication models, such as costly disclosure and signaling,==== also points to a non-monotone link between the sender's access to information and equilibrium communication. Nevertheless, we show that in voluntary disclosure environments, provided that they have the same opportunities for disclosure, better-informed agents transmit more accurate information in equilibrium.====We study a general model of voluntary disclosure: in each state of the world, nature performs ==== conditionally independent lotteries. The first ==== lotteries determine the realizations of a given set of ==== signals (Blackwell experiments), and the ==== lottery determines which subset of those signals is in the sender's possession.==== The sender decides which of the signals in her possession she should disclose, and the receiver chooses an action (a real number). The two players' interests regarding this action are not aligned. Whereas the receiver's goal is to coordinate his action with the state, the sender aims to maximize the receiver's action. Our main result states that, in a truth-leaning equilibrium (Hart et al., 2017), whenever the signals are more Blackwell informative the receiver's expected utility increases.====When analyzing the equilibrium effect of a change in the sender's informativeness, the first obstacle one faces is the characterization of the equilibrium. In general voluntary disclosure games, the equilibrium can be quite involved and usually includes mixing. Therefore, we apply a recent result by Hart et al. (2017) to show that the equilibrium question can be reduced to a mechanism design question. In this way, we can prove that the receiver's utility is increasing in the signals' informativeness, without providing a characterization of the equilibrium.====Hart et al. (2017) show that commitment power does not help the receiver obtain higher utility. That is, the receiver's utility under the optimal (deterministic) mechanism is the same as his utility in (a truth-leaning) equilibrium. To prove our general claim, we construct a direct (potentially) random mechanism that generates the exact same joint distribution of the state of the world and the receiver's action as the optimal mechanism of any given less informative evidence structure. Subsequently, we construct a deterministic incentive-compatible mechanism that gives the receiver a higher utility than the mimicking mechanism. Therefore, we deduce that the mimicking mechanism gives the receiver a lower utility than the optimal deterministic mechanism, and the reduction implies that the receiver's ==== utility is increasing in the sender's informativeness. In Section 2, we exemplify this proof method in a simple evidence structure, namely, the Dye (1985) model.====We apply our main result to DeMarzo et al.'s (2019) model of an endogenous Dye (1985) evidence structure, where an uninformed sender chooses a test (Blackwell experiment) in private and then decides whether she should disclose its result. They show that, in equilibrium, the sender's choice minimizes the disclosure threshold. Therefore, our main result implies that when the sender faces a Blackwell-ordered set her choice is efficient. Since the disclosure threshold is decreasing in the test's informativeness (Jung and Kwon, 1988), the sender chooses the most informative test which, according to our result, maximizes the receiver's expected utility.====To provide the reader with some intuition for our main result, we discuss a simple example in Section 6.2. Intuitively speaking, making the sender's information more accurate has two effects. First, when presented to the receiver in full, the sender's evidence is more informative about the state of the world, which is clearly beneficial to the receiver. Second, the informational shift also has a strategic effect. In Example 2, by presenting this shift in a model that keeps the set of types fixed, we show that this shift can be interpreted as a reduction in the sender's set of “lies.” In addition, we employ Example 2 to explain why changing the probability that the sender obtains evidence in a way that seemingly makes her more informed can decrease equilibrium communication.====  This paper contributes to the literature on strategic disclosure. Starting with Grossman (1981) and Milgrom (1981), the economic literature discusses environments in which communication is verifiable. Though the early models indicate information unraveling, Dye (1985) and Jung and Kwon (1988) establish an equilibrium with partial disclosure by allowing for uncertainty regarding the sender's informativeness. Verrecchia (1983) shows that such an equilibrium can be obtained if disclosure is costly.====Our model generalizes other voluntary disclosure models previously discussed in the literature. In Section 6.1, we briefly discuss a few other structural models, such as Shin's (1994) evidence model and Hart et al.'s (2016) partition model, in both of which the evidence formation process is defined explicitly, and we show that they can be accommodated in our model. Moreover, we show that even reduced-form models, such as Hart et al. (2017) and Ben-Porath et al. (2019), in which the evidence formation process is not defined, can be accommodated in our model. Even though our framework is structural, it is rich and flexible, and we can specify the primitives in a way consistent with those reduced-form models. Finally, note that, with regards to Hart et al.'s (2017) model, the converse is also true. That is, for every instance of our model, we can define the primitives of Hart et al.'s (2017) model in a way that will induce the same game.==== Thus, for our main result, we can use Hart et al.'s (2017) equivalence result.====Our proof applies findings in the disclosure literature on the value of commitment power. Glazer and Rubinstein (2004, 2006) study verifiable communication models in which the receiver's action is binary, and they show that the receiver does not gain from commitment power. This result is extended to multi-action environments by Sher (2011) and is further generalized by Hart et al. (2017). Ben-Porath et al. (2019) prove that the “no value for commitment” result can be extended to a multi-sender environment, in which some senders wish to maximize the receiver's action and some wish to minimize it. A similar result is shown to hold in a special case by Bhattacharya and Mukherjee (2013). To the best of our knowledge, our paper is the first to show that these equivalency results can constitute a tool for answering natural questions in the disclosure literature.====The closest paper to ours in the disclosure literature is Rappoport (2020) who shows that a more informed sender encounters more pessimism on the receiver's part. Unlike our model, Rappoport's (2020) does not distinguish between the distribution of the underlying state and the sender's ability to provide evidence. He studies a reduced-form evidence model in the spirit of Hart et al. (2017) where the sender's type is defined as a pair: the first dimension corresponds to the receiver's utility, and the second dimension specifies the set of types the sender can mimic. In this model, Rappoport (2020) defines the evidence structure as more informative if, for every two types of the sender such that one type can mimic the other, the relative probability of the mimicking type is higher. He shows that if the evidence structure is more informative according to this definition then the receiver's equilibrium action is lower for each report of the sender.====Our model differs from the reduced-form model in a way that allows us to compare the receiver's utility across different evidence structures. In principle, one can interpret the reduced-form model as if the sender's type corresponds to an actual state of the world or a distribution over a state space. However, under such interpretation, Rappoport's (2020) definition of a more informed sender would typically imply a change in the prior distribution of the state and thus does not allow for a comparison of the receiver's utility in the two evidence structures. Moreover, in Hart et al.'s (2017) model, an alternative definition of an increase in the informativeness of an evidence structure based on the Blackwell order (similar to the one in our model), is also not applicable for such a comparison. In Section 6 (Example 2), we show why such a definition does not capture a change in the informativeness of the evidence structure in a clean way. Since the sender's type also determines which types she can mimic, a garbling of the types' distribution changes not only the quality of the information but also the strategic environment. As a result, a garbling of the types in this example can ==== the receiver's utility. Therefore, to study the effect of an informativeness change in the evidence structure on the receiver's utility, we define an explicit model of evidence that disentangles the quality of the sender's information from her available strategies.====Our paper also connects to the literature that studies the relation between the sender's informativeness and equilibrium performance. Harbaugh and Rasmusen (2018) study a model of voluntary certification with a disclosure cost in the spirit of Verrecchia (1983), and they show that grade-coarsening is optimal. Due to higher participation, the receiver may observe more information in equilibrium if the quality of the sender's information worsens. Similarly, Bertomeu et al. (2021) show, in a simple evidence structure à la Dye (1985), that if sending a message is costly, the receiver can be better off when the sender's information is obscured. They also show that if there is no cost, a better-informed sender transmits more information in equilibrium. Our main result extends the latter result to a general voluntary disclosure environment. Ball (2020) studies a communication model with costly distortion, in which an intermediary observes the sender's message and aggregates it into a score. He shows that a partly informative score can be optimal for the receiver. In a related framework, Whitmeyer (2019) shows how a receiver can profit from garbling the sender's message.====The rest of the paper is organized as follows. In Section 2 we present an example. In Section 3 we discuss our model. In Section 4 we study the effect of a change in the informativeness of the evidence structure. In Section 5 we present an application. In Section 6 we discuss our results and conclude.",Information transmission in voluntary disclosure games,https://www.sciencedirect.com/science/article/pii/S0022053123000492,14 April 2023,2023,Research Article,11.0
"Barberà Salvador,Bossert Walter","MOVE, Universitat Autònoma de Barcelona and Barcelona School of Economics, Facultat d'Economia i Empresa, 08193 Bellaterra (Barcelona), Spain,Centre Interuniversitaire de Recherche en Economie Quantitative (CIREQ), University of Montreal, P.O. Box 6128, Station Downtown, Montreal, QC H3C 3J7, Canada","Received 14 October 2021, Revised 4 April 2023, Accepted 7 April 2023, Available online 12 April 2023, Version of Record 20 April 2023.",https://doi.org/10.1016/j.jet.2023.105654,Cited by (0),"In a world admitting a fixed finite set of alternatives, an opinion is an ordered pair of alternatives. Such a pair expresses the idea that one alternative is superior to another in some sense, and an opinion aggregator assigns an aggregate relation on the set of alternatives to every possible state of opinion. Our primary motivation is to extend the standard model of ==== theory to a more general one in which no specific reference to agents generating or holding opinions is needed. Although our analysis has some bearing on those cases where opinions reflect the goodness relations of agents in a society, it is not limited to them. In addition to that interpretation, opinions can also be used to represent other forms of comparative assessments emerging from different sources. The main results of the paper provide characterizations of two essential aggregation methods that remain well-defined in our larger context. These are the Borda rule and Condorcet's majority rule formulated in terms of opinion aggregation. Moreover, we show that these two opinion aggregators agree with the plurality rule and with the approval-voting rule when opinions are generated by ballots that are specific to these two well-established voting rules.","In a world admitting a fixed finite set of alternatives, an opinion is an ordered pair of alternatives. Such a pair expresses the idea that one alternative is superior to another in some sense, and an opinion aggregator assigns an aggregate relation on the set of alternatives to every possible state of opinion. Our primary motivation is to extend the standard model of social choice theory to a more general one in which no specific reference to agents generating or holding opinions is needed. Many aggregators have been defined where opinions reflect the goodness relations of agents. These are often assumed to satisfy additional properties, such as completeness or transitivity, although such assumptions can be relaxed to some extent with no harm. But not always. Consider, for example, the classical de Borda (1781) rule. This method assigns scores to the alternatives that are then used to establish an aggregate ranking, and the way to reach these scores admits several equivalent descriptions when opinions are derived from complete, transitive, and antisymmetric individual relations. The score of an alternative can be derived by adding the weights assigned to the positions it occupies in the goodness relations of the agents, or the number of times that this alternative defeats another, or the size of the difference between the number of its pairwise wins and losses. We shall argue that only the third formulation is adequate to use in our general framework, and we will provide a characterization of it. In that case, it is fortunate that the spirit of a classical rule can be fully maintained in the larger framework: Borda's method is well-defined even if the origin of opinions is not attached to specific individuals; moreover, it still satisfies properties that make it attractive and that we shall spell out when characterizing it. Other rules are not so robust and cannot be properly extended unless individuals are well identified; this is the case, for example, for the remaining scoring rules (Young, 1975) that assign different weights to alternatives according to their positions in the individual rankings.====Clearly, Borda's is not the only rule that can be defined without reference to specific individuals. In particular, the simple majority rule advocated by de Condorcet (1785) can also be formulated in our context, along with others. And these two competing forms of aggregating individual goodness relations also share another important characteristic that we highlight. This is the observation that they both are based on the difference between the number of wins and the number of losses that each alternative experiences when confronted with others in pairwise contests. The notion that this difference is what really matters may, on the surface, appear to be debatable, but it is natural and unifying—and it precludes the difficulties alluded to in the previous paragraph that emerge when only wins are taken into consideration in defining the Borda rule. We prove that all difference-based rules share a cancellation property that is, conceptually, familiar from earlier contributions such as that of Young (1974). Our main results provide characterizations of the methods advocated by Borda and by Condorcet, and they necessarily differ from the existing ones because one can no longer use axioms that rely on the identification of those individuals who hold the opinions to be aggregated. While the axiomatic treatment of majority is quite close to that provided by May (1952) and reestablished by Sen (1970, Theorem 5*1), the characterization of Borda's rule is based on a significant departure from previous approaches that are valid on smaller domains.====It is worth remarking that, although usually expressed in the context where individual relations are complete and transitive, known characterizations, like those by Young (1974) or Nitzan and Rubinstein (1981), could be naturally extended to avoid the use of these regularity conditions. Indeed, such properties are not needed when applying our rules to the larger framework, and several interesting conclusions that we think have not been previously emphasized are derived from this fact. Specifically, we show that several important rules that only require the expression of partial information rather than of full profiles on the part of the agents can be rephrased in our language that involves opinions. Moreover, we prove that the plurality rule is equivalent to the use of Borda's rule or the Condorcet method of majority decision when translated into the opinion-aggregation setting—and the same observation applies to approval voting and some of its variants.====In summary, we propose a new and larger framework to investigate aggregation rules, we offer a characterization of the important class of difference-based rules, we observe that the two most classical aggregation methods, which clearly belong to this large class, can be naturally extended to and characterized in the framework, and we show that both are equivalent to the plurality rule and to approval voting in the case where the individuals who express opinions are identified but submit ballots that contain less information than full profiles of individual goodness relations.====Although our analysis has some bearing on those cases where opinions reflect the goodness relations of agents in a society, it is not limited to them. In addition to this interpretation, opinions can also be used to represent other forms of comparative assessments emerging from different sources. For example, the opinion ==== may indicate that a book ==== is better than another book ====, an exam ==== handed in by one student deserves a better grade than another exam ==== submitted by another student, a sports team ==== defeats another team ====, a statement ==== is more likely to be true than another statement ====. Another interesting application emerges when it comes to observed choices. If, in a repeated series of observations, an object is chosen over another, it may very well be the case that several of these acts of choice are performed by one and the same agent—and this is more difficult to model if only a single goodness relation is to be assigned to each agent; note that it is perfectly possible that the same agent chooses ==== over ==== in one instance and ==== over ==== in another. Furthermore, there seems to be evidence that some cognitive processes operate by aggregating different (and potentially conflicting) impulses; see, for instance, Jackson and Yariv (2015, p. 151) for a discussion. Therefore, the opinion-based framework may serve as an adequate model of decision-making that can be observed in the human brain. Clearly, this list is by no means intended to be complete but serves as an indication of the broad applicability of the notion of opinions.====A state of opinion is a function that identifies the number of times each opinion appears in the state under consideration. The special case of the empty state of opinion—a function that assigns the number zero to each possible opinion—is included as a possibility. We do not assume anything about who expresses the opinions but note that we can accommodate cases in which a single agent may express any finite number of opinions or none at all. For instance, the standard case familiar from social choice theory where each agent can express one and only one goodness relation is covered, and so is the possibility of each agent merely passing judgment on a single pair of alternatives. But, as we already hinted at, the notion of an opinion is more general because several opinions regarding the same pair of alternatives can be expressed by the same agent. A more detailed discussion of the significance of states of opinion and their relationships to more conventional modeling devices is provided once the requisite definitions have been introduced formally.====An opinion aggregator is a function that assigns a relation on the set of alternatives to any possible state of opinion, with the convention that the universal equal-goodness relation is associated with the empty state of opinion. The latter requirement is very natural: if no opinion is expressed whatsoever, there are no grounds for favoring any alternative over any other. The distinctive feature of our approach is that, unlike in the standard theories of social choice or judgment aggregation, we do not impose initial assumptions about linkages between the opinions that constitute a state. Nor do we require them, or the relations into which they are aggregated, to possess a priori any form of coherence. In particular, this implies that properties such as completeness or acyclicity are not required. See also Fishburn (1984) and Chebotarev (1994), for instance, who dispense with the completeness assumption but retain the basic framework phrased in terms of individual agents.====In the spirit of Borda and Condorcet, we characterize the Borda opinion aggregator and the majority opinion aggregator as illustrations of how our framework of opinion aggregation can be put into practice. The ideas underlying Borda's method diverge considerably from the majoritarian ideas expressed by Condorcet. Both had supporters as well as detractors among their contemporaries. For instance, Morales (1797) was a committed advocate of Borda, whereas Daunou (1803) favored Condorcet's ideas and was rather critical of Borda's method. Daunou also formulated his own proposal of a voting method, which is analyzed and characterized by Barberà et al. (2021). Herrero and Villar (2021) examine a rule that represents a combination of the proposals by Borda and Condorcet.====As elaborated on earlier, one motivation for the use of the Borda method and the majority rule is that they are widely applicable and can be defined in our general context without the need for any further assumptions regarding the structure of the states of opinion. A second important reason for our focus is that, in natural contexts where different well-known rules can be defined, the two aggregators coincide with these rules. Consider, for example, the case of plurality voting, and the kind of special states of opinion that the balloting under this rule can generate. By declaring their top alternative, agents express their opinions between that alternative and any other. We show that if attention is restricted to such states of opinion, the aggregate ranking according to the plurality rule coincides with the aggregate ranking according to the Borda opinion aggregator and with that according to the majority opinion aggregator. As another example, consider approval voting where each agent submits a set of alternatives—namely, the set of approved-of alternatives. Approval voting ranks the alternatives on the basis of their approval scores—that is, the number of times an alternative is approved of by an agent. A natural interpretation is that such a ballot is represented by a state of opinion that consists of all opinions ====, where ==== is approved of and ==== is not. Again, the ranking of the alternatives according to the approval-voting rule coincides with the ranking according to the Borda opinion aggregator and with that according to the majority opinion aggregator.====A third reason that motivates us is of a historical character. Both de Borda's (1781) own description of his proposed voting method and its defense by Morales are very inspiring. In fact, the notion of an opinion appears in Morales's (1797) Memoir on the Calculus of Opinion—a contribution that was well-respected and appreciated not only by Borda himself but several other contemporaries. The term opinion is used by Morales in the same way as in our model but, in addition, Morales strongly defends this notion as a natural unit of measurement, and he advocates the view that all opinions should count equally, no matter who holds them. Borda's presentation also refers to what he calls merit as a magnitude admitting a definite and fixed value, unique up to affine scaling. In a similar manner, he insists on the fact that the difference of merit between two successive alternatives in a voter's relation remains the same independently of the position they occupy. Of course, we do not maintain this significance of differences as a starting point, but our axiomatic characterization can be interpreted as a foundation for its ex-post defense.====We hope that the observations presented here will further underline the importance and significance of the methods advocated by Borda and by Condorcet. As alluded to earlier, these two methods can be defined within our framework, whereas many other rules only make sense in the more limited context of classical social choice theory based on profiles of individual orderings. That the Borda method does not require stringent assumptions on individual relations is already pointed out by Young (1974, p. 51) who states that his characterization of the classical Borda social choice function can be rephrased in a way that applies to general profiles of individual relations that do not have to be orderings. Likewise, Nitzan and Rubinstein (1981) characterize the Borda rule without assuming individual goodness relations to be transitive. However, our model of collective choice in terms of opinion aggregation has, to the best of our knowledge, not been examined in the earlier literature. That Condorcet's method of majority decision does not rely on any properties of the individual inputs such as completeness or acyclicity is well-known and follows immediately from inspecting the requisite criterion to rank the alternatives. Each of these well-established methods of collective choice allows for a characterization in our model of opinion aggregation. Both axiomatizations differ from analogous results in the traditional setting in that no reference to individual agents is made here. There is, however, a conceptual difference between our two characterizations. The majoritarian criterion only uses information on ==== and ==== to rank any two alternatives ==== and ====, to the exclusion of all other alternatives. This feature is associated with a rather forceful neutrality property than cannot but be present in some guise or other. As a consequence, our characterization of the majority opinion aggregator shares the central features of May's (1952) result—other than being able to avoid all references to specific individuals. In contrast, our axiomatization of the Borda opinion aggregator is, as we believe, considerably more novel and utilizes the rich framework of opinion aggregation to its full advantage. See, for example, Young (1974), Hansson and Sahlquist (1976), Nitzan and Rubinstein (1981), and Mihara (2017) for characterizations of the Borda rule in the traditional setting.====We conclude this introduction with an informal description of the properties that we employ in this paper. Our first formal result identifies all opinion aggregators that are based on net wins—that is, on the difference between the number of wins and the number of losses experienced by an alternative in a state of opinion. This is achieved by imposing a pairwise cancellation axiom that is familiar from the earlier literature; for instance, Young (1974) employs such a property in his characterization of the Borda social choice function. Intuitively, it requires that opposing opinions cancel each other out when determining the aggregate relation.====Our first step in identifying a system of axioms that characterizes the Borda opinion aggregator consists of strengthening the pairwise opinion cancellation condition alluded to in the previous paragraph. In analogy to pairwise opinion cancellation, the property deals with how a favorable opinion for an alternative ==== over a second alternative ==== is to be traded off against a favorable opinion for ==== over a third alternative ====. If ==== and ==== are one and the same alternative, the pairwise variant of the property results. If these two alternatives differ, however, the situation is more subtle: although a win is compensated by a loss as far as alternative ==== is concerned, it must be taken into consideration that removing a favorable opinion for ==== over ==== leaves ==== with one win less, and an analogous observation applies to the removal of a loss of ==== against ====. Thus, these changes must be accounted for in the version of the axiom that also applies to distinct alternatives ==== and ====. We stress that one of the possible motivations of the axiom goes back to Morales's (1797) view that all opinions should count equally.====The second property we employ in axiomatizing the Borda opinion aggregator is an opinion-monotonicity requirement. Loosely speaking, the axiom requires that if the situation of some undominated alternatives improves by adding a favorable opinion for each of them, then they improve in the aggregate ranking while remaining equally good among each other. A dual requirement is imposed on the response of the opinion aggregator to the deterioration of alternatives that are not better than any of the others. This is an intuitively appealing property that ensures a positive response to a change in a state of opinion that unambiguously favors some of the alternatives. We note that the two axioms just described exploit the structure of the entities being studied here—the states of opinion.====Turning to our characterization of the majority opinion aggregator, we note first that this method always generates complete aggregate relations. Because this property is not implied by our remaining axioms, it needs to be required explicitly for all states of opinion other than the empty state. The other two axioms employed parallel those of May (1952). The first is opinion neutrality, the opinion-aggregation variant of May's (1952) well-known strengthening of Arrow's (1951) independence of irrelevant alternatives, and the second is an adaptation of May's (1952) positive responsiveness to our setting.",Opinion aggregation: Borda and Condorcet revisited,https://www.sciencedirect.com/science/article/pii/S0022053123000509,12 April 2023,2023,Research Article,12.0
"Rocheteau Guillaume,Wang Lu","UC Irvine, United States of America","Received 27 November 2022, Revised 27 March 2023, Accepted 2 April 2023, Available online 7 April 2023, Version of Record 14 April 2023.",https://doi.org/10.1016/j.jet.2023.105652,Cited by (0),"Is asset liquidity a source of price volatility? We answer this question within a continuous-time, New Monetarist economy under extrinsic uncertainty where the role of an asset to ==== expenditure shocks depends on its pledgeability. If assets are intrinsically valuable and pledgeability is exogenous, then their prices are invariant to extrinsic uncertainty – unlike environments with discrete-time pricing that can feature sunspot equilibria. We derive conditions under which extrinsic uncertainty matters when pledgeability is endogenous and varies with the asset price or its rate of return. In the latter case, we link sunspot equilibria to deterministic cycles.","Does the liquidity of an asset – the ease with which it can be traded to finance expenditures – make its price excessively volatile relative to fundamentals?==== The recent literature in monetary theory answers positively based on the following logic. The price of an asset is composed of the value of its dividend stream and the value of its liquidity services. To the extent that liquidity is based on self-referential beliefs, the liquidity value of an asset can fluctuate as beliefs change over time. This view has been formalized in the context of discrete-time New Monetarist models by Lagos and Wright (2003) and Rocheteau and Wright (2013), among several others reviewed in Section 1.1, by showing the existence of equilibria where asset prices vary with changes in ==== states unrelated to economic fundamentals.====In this paper, we revisit the liquidity-volatility relationship in the continuous-time, New Monetarist economy of Choi and Rocheteau (2021) where centralized and decentralized (over-the-counter) markets open concurrently and continuously over time. This environment is characterized by idiosyncratic expenditure shocks and lack of commitment so that liquid assets have a role as means of payment or collateral. We add two ingredients to the model: (i) a pledgeability constraint, as in Kiyotaki and Moore (2005) or Venkateswaran and Wright (2013), according to which the size of the payment made by the asset holder in the decentralized market cannot be greater than a fraction (the pledgeability coefficient) of her asset holdings; (ii) extrinsic uncertainty in the form of a sunspot obeying a continuous-time Markov chain. In the first part of the paper, we follow most of the existing literature and take asset pledgeability as exogenous. In the second part, we endogenize pledgeability. In both parts, we characterize and compare the equilibrium set of economies with intrinsically valuable Lucas trees and economies with intrinsically useless fiat monies.====Our first result contradicts the common wisdom described above and articulated in, e.g., Lagos and Wright (2003). It shows that extrinsic uncertainty does not matter for the price of a Lucas tree yielding a constant, positive, dividend flow irrespective of its pledgeability and irrespective of the discount rate. There is a unique equilibrium and it corresponds to the steady state. Even if the asset has zero intrinsic value, there does not exist a proper stationary sunspot equilibrium where the value of money is always positive. This result suggests that the liquidity role of an asset can affect the level of its price through a liquidity premium but it does not exacerbate its volatility.====Our result admits one caveat: the focus on positive recurrent sunspot equilibria is restrictive in the case of fiat money. Indeed, we establish the existence of sunspot equilibria that are transient, i.e., one sunspot state is absorbing. In those equilibria, there is a state where money loses its value. In other words, liquidity might not explain the volatility of stocks or housing prices but it could explain the volatility of crypto-currencies. According to our model, a crypto-currency is volatile when it is anticipated that there is a state where it will ultimately fail. The main results discussed so far are summarized in Table 1.====In order to understand why the equilibrium sets differ in discrete- and continuous-time models, we write a version of our environment where the centralized asset market opens infrequently, at fixed time intervals. The equilibrium is characterized by a difference equation similar to the one in, e.g., Lagos and Wright (2003). As the length of the time interval between two consecutive centralized markets vanishes, the difference equation morphs into the differential equation of our model. We prove that if centralized markets open frequently enough, then there is no sunspot equilibrium. Intuitively, the presence of a liquidity premium creates a negative expectational feedback that gets amplified when prices are scattered over time, potentially leading to endogenous fluctuations. In contrast, the expectational feedback vanishes when the market opens at high frequency so that the equilibrium is unique provided the asset has a positive fundamental value.====In the second part of the paper, we show that proper sunspot equilibria can exist if the pledgeability of assets varies with their price. Our investigation proceeds in two steps. First, we assume that pledgeability can be written as a general function of the asset price, or its rate of return, and we study the equilibrium set taking as given the pledgeability function. Second, we provide explicit microfoundations for the relationship between pledgeability and asset prices. We obtain the following insights.====If pledgeability can be written as a function of the level of the asset price, then sunspot equilibria exist if and only if there are multiple steady states. The multiplicity arises when there is a strong feedback loop between prices and pledgeability. In that case, there exists a steady-state equilibrium where the asset price and the pledgeability coefficient are high and there is another steady state where the opposite is true. By a standard logic, sunspots allow agents to coordinate on one outcome or another. When the steady-state equilibrium is unique because the feedback loop is weak, it is locally dynamically unstable, so that there are no other deterministic equilibria or sunspot equilibria.====If pledgeability can be written as a function of the rate of return of the asset, then the existence of multiple steady states is no longer a necessary condition for the existence of sunspot equilibria. In particular, if the steady state is unique, sunspot equilibria exist if and only if deterministic periodic equilibria exist. The logic goes as follows. When pledgeability depends on the rate of return of the asset, there can exist an interval of asset prices over which the same price is consistent with either a positive or a negative appreciation. Intuitively, if agents believe the asset price will appreciate, then the expected rate of return is high. If pledgeability increases with the rate of return, then the pledgeable wealth is more abundant, which can lead to a reduction in the liquidity premium of the asset. As a result, the asset price must appreciate over time in order to compensate for the low liquidity premium, in accordance with the initial belief. Conversely, if agents anticipate a negative appreciation, the rate of return is low, pledgeability is low, and the scarcity of the pledgeable wealth raises the liquidity premium of the asset, thereby rationalizing the low rate of return of the asset. The result that asset prices within an interval can either increase or decrease depending on self-fulfilling beliefs allows us to construct deterministic cycles and sunspot equilibria. The results are summarized in the following table where ==== denotes the pledgeability coefficient. (See Table 2.)====We consider two types of microfoundations for the pledgeability function. We first provide microfoundations based on a costly enforcement problem. Such foundations could explain the pledgeability of assets that are easy to hide or divert away, e.g., financial securities. We show pledgeability increases with market capitalization, which can generate multiple steady states and sunspot equilibria by the logic described above. The second microfoundations are based on a moral hazard problem along the lines of Li et al. (2012). Buyers can either hold genuine assets or they can produce fake ones at a cost. These microfoundations could apply to all assets that are prone to fraud, e.g., currencies, asset-backed securities, and even real assets (such as houses). We show that pledgeability is a linear and increasing function of the rate of return of the asset, which allows us to obtain a variety of deterministic and stochastic equilibria along which the asset price and pledgeability fluctuate.",Endogenous liquidity and volatility,https://www.sciencedirect.com/science/article/pii/S0022053123000480,7 April 2023,2023,Research Article,13.0
"Li Kai,Liu Jun","Institute of Financial Studies, Southwestern University of Finance and Economics, China,Macquarie Business School, Macquarie University, NSW 2109, Australia,Rady School of Management, University of California San Diego, La Jolla, CA 92093, United States of America","Received 1 July 2022, Revised 29 March 2023, Accepted 2 April 2023, Available online 6 April 2023, Version of Record 17 April 2023.",https://doi.org/10.1016/j.jet.2023.105651,Cited by (0), and fundamental extrapolation helps resolve puzzles.,"Asset prices are jointly determined by agents' subjective beliefs about cash flows and returns. An increasing literature of asset pricing models explores the formations of these beliefs to explain asset price behaviors. On the one hand, Guo and Wachter (2019) and Nagel and Xu (2022a) show that subjective dividend expectations alone are enough to fit the data. On the other hand, Barberis et al. (2015) and Jin and Sui (2022) explain asset price behaviors with return extrapolation in the sense that investors' expectations about future returns depend positively on past returns. However, in these return extrapolation models, either consumption good market does not clear or return extrapolation is effectively changed to dividend extrapolation. It is natural to wonder the net effects of return extrapolation on asset prices.====In this paper, we study return extrapolation in an otherwise standard asset pricing model. We look at a setting with an endogenous short rate and pure return extrapolation, which is not tied to dividend extrapolation. This setting allows us to isolate the effects of return extrapolation. We solve for equilibrium in closed form. We find that return extrapolation may not be very effective in explaining features related to the equity premium and return volatility; however, its effects are more likely to be found in the short rate.====Although return extrapolation exogenously specifies subjective expected return as a whole, it separately determines the short rate and the risk premium (i.e., the subjective expected return in excess of the short rate) under consumption-based models. We find that the time variation in the agent's subjective expected return mainly leads to time variation in the short rate. Furthermore, the short rate depends almost linearly on the subjective expected return, and return extrapolation in our model effectively leads to a Vasicek (1977) term structure model.====These results are surprising since the effect of an assumption about the risky asset actually manifests in the riskless asset. To understand these results, we consider two extreme cases. When the dividend shocks of the asset that the agent extrapolates are independent of the shocks to the pricing kernel, the asset has a risk premium of zero. In this case, the short rate is identical to the subjective expected return in equilibrium. At the other end, when the dividend shocks and the pricing kernel shocks are perfectly correlated, the return expectation bias leads to the same bias in dividend expectations and pricing kernel expectations, and the latter directly cause the short rate to depend on the subjective expected return. In fact, the strong effect of the subjective expected return on the short rate can be also understood from the basic consumption-based pricing equation (Cochrane, 2005; Chapter 1) that implies a strong comovement between the (subjective) expected stock return and the short rate.====Return extrapolation causes asset return volatility to be lower than the rational counterpart, suggesting that return extrapolation amplifies the excess volatility puzzle. Due to the discount rate effect, the price-dividend ratio is a decreasing function of the subjective expected return. As a result, movement in dividends is partially offset by movement in the price-dividend ratio, leading to deficient return volatility (lower volatility than the rational counterpart). On the other hand, extrapolation on dividend growth (instead of returns) increases return volatility as a result of the income effect. These results are supported by recent empirical studies. By decomposing the variance of price-dividend ratio and price-earnings ratio, De la O and Myers (2021) find that subjective cash-flow growth expectations account for the vast majority of variations in price ratios, while subjective return expectations actually “negate” the variations.====Return extrapolation specifies the beliefs about the mean of stock returns but does not change the perceived risks for the stock. As a result, return volatility has a weak relationship with the subjective expected return. We show that return volatility is almost a constant for plausible levels of the subjective expected return and varies within a narrow band with two boundaries that correspond to the two limiting cases when the agent bases her return expectation only on current return and equally on all past returns, respectively.====The weak relationship between volatility and the subjective expected return causes return extrapolation to have a weak effect on the risk premium. The above results are consistent with the empirical evidence about the two components of subjective expected returns that the short rate is procyclical and the subjective expected excess return (risk premium) is acyclical (e.g., Nagel and Xu, 2022a; 2022b). These results are starkly different from rational expectations asset pricing models, where the risk premium rather than the short rate accounts for most of the time variation in expected returns.====In a consumption-based asset pricing model, return extrapolation has direct implications for the agent's subjective expected consumption growth rate. Indeed, return extrapolation exogenously specifies the subjective expected return, which is an endogenous variable in traditional asset pricing models; thus, in equilibrium it implies ==== on beliefs about exogenous variables, such as aggregate consumption.==== We show that the agent's subjective expected consumption growth rate depends positively on her subjective expected return. In particular, if the asset's dividends are independent of consumption, the agent's subjective expected return and subjective expected consumption growth rate completely determine one another. The direct implication for consumption expectations (other than consumption shocks) provides a further understanding of the little effect of return extrapolation on the risk premium. Moreover, it causes the short rate to be insensitive to the elasticity of intertemporal substitution (EIS) but sensitive to risk aversion, in contrast to both rational expectations models and subjective dividend expectations models in which the short rate depends on the EIS, suggesting that return extrapolation restricts the effect of the EIS (e.g., in recursive utility) on the short rate in asset pricing models.====Given an asset, return extrapolation is equivalent to explicitly specifying the asset price====; however, it does not directly specify the equity premium (the expected excess return under ====). Because return expectation formation, such as return extrapolation, is a deviation from rational expectations, the subjective measure of the agent differs from the physical measure. However, this difference caused by return extrapolation does not lead to deviation of the equity premium from the risk premium (the expected excess return under ====) since return extrapolation does not affect the perceived return shocks that are actually driven by the agent's subjective dividend expectations.====In contrast, because return shocks include dividend shocks, dividend expectation bias is directly reflected in the equity premium. This implies that the equity premium has two components. The first component is the risk premium as the compensation for risk. The second component is sentiment premium, which is not risk compensation but accounts for the difference between the subjective expected dividend growth and the physical expected dividend growth. It disappears when the agent has a correct dividend expectation (even with return extrapolation). This result directly implies that time-varying dividend expectation bias (rather than return extrapolation bias) helps produce a volatile equity premium as observed in the data (e.g., Cochrane, 2011; 2017).====The sentiment premium represents a significant departure from the rational expectations models in which the risk and equity premia are the same. It causes the equity premium to be less sensitive to risk aversion than in rational expectations models. Furthermore, the sentiment premium depends negatively on the agent's subjective expected dividend growth, which is consistent with the “sentiment effect” that high sentiment (high subjective expected dividend growth in our model) leads to low average future return, as widely documented in the literature.==== However, we show that this sentiment effect is unlikely due to return extrapolation that actually predicts that high subjective return expectation leads to high (instead of low) average future return, opposite to the results in the investor sentiment literature. Thus, interpreting return extrapolation as “sentiment” (e.g., in Barberis et al., 2015) may lead to confusion.====Our analytical results help differentiate the effects of subjective return expectation and subjective dividend expectation. Return expectation places significant restrictions on the agent's consumption expectation due to the consistency conditions; however, it does not directly put restrictions on dividend expectation.==== While subjective return expectation largely affects the short rate and has little or no effect on the equity premium, subjective dividend expectation largely affects the equity premium (through return volatility and the sentiment premium) rather than the short rate. With biased dividend expectations, the dividend shocks that are orthogonal to the pricing kernel shocks can also affect the equity premium (due to the sentiment premium), which affects empirical estimations of the pricing kernel (e.g., Hansen and Jagannathan, 1991). Importantly, we show that return extrapolation tends to have the opposite effects to dividend (fundamental) extrapolation. While return extrapolation exacerbates asset pricing puzzles, dividend extrapolation helps resolve puzzles.====Return extrapolation generates a “negative feedback” in the sense that prices decrease with the subjective expected return,==== due to its discount rate effect. With an infinite horizon, sufficiently strong negative feedback can lead to instability in the sense of a more restrictive transversality condition, which demonstrates a destabilizing feature of return extrapolation, another unappealing feature of subjective return expectations. However, for finite horizons, equilibrium always exists even in the presence of the feedback effect.====The negative feedback is closely related to return momentum. Under the subjective measure, the agent believes that asset return has momentum (in the sense of positive return autocorrelations). Because the sentiment premium does not depend on return extrapolation, the features of subjective expected return implied by return extrapolation, such as momentum, are also preserved under the physical measure. In the literature where extrapolation is defined over price changes (rather than returns), reversal is commonly found (e.g., Hong and Stein, 1999; Barberis et al., 2015).====Although our main results on return extrapolation rely on a representative-agent model with CRRA utility, we show that many of these results continue to hold in more general settings, such as recursive utility, heterogeneous agents, and a general form of subjective return expectations, which, however, do not allow for analytical solutions. In these settings, return extrapolation always produces a time-varying short rate, and its effects are more likely to be found in fixed-income securities====; however, it may not be very effective in explaining features related to the equity premium. Although we focus on a Lucas economy, many of our intuitions and conclusions can be extended to more general settings (as generally discussed in Section 2).====Jin and Sui (2022) develop a model of return extrapolation and find that their model can quantitatively fit the moments of stock index and survey expectation data. In their model, changes in the agent's subjective expected return lead to larger changes in her expected dividend growth, which dominate the discount rate effect of the return expectations.==== Our paper finds that the consistency conditions have almost no implication for dividend expectations, and hence subjective dividend expectations can be explored to fit the data, especially to fit the observed features related to the equity premium. Moreover, by isolating the effects of return extrapolation and dividend growth extrapolation in a tractable model, our paper shows that the price features in their model are largely due to a mechanism of dividend extrapolation, similar to theories that directly examine subjective dividend expectations.====Barberis et al. (2015) study extrapolation of price changes in a Lucas-style setup where heterogeneous agents have constant absolute risk aversion (CARA) utility functions and aggregate consumption follows a Brownian motion with drift. They show that their model can explain many features of asset returns. The short rate in Barberis et al. (2015) is exogenously fixed (hence not volatile). This assumption implies that the consumption good market does not clear, and as a consequence the pricing kernel does not equal the marginal utility evaluated by either agent's optimal consumption.==== The non-clearing market leads to a discrepancy between the optimal consumption and aggregate consumption as shown in Loewenstein and Willard (2006). This discrepancy can be unbounded in some states and largely absorbs the effect of extrapolation on prices. In our model where all markets clear, the effect of extrapolation is mainly on the short rate rather than the equity premium. Furthermore, by imposing market-clearing conditions to the economy of Barberis et al. (2015), we find that price extrapolation does have a first-order effect on the short rate (see Appendix D), consistent with our model conclusion.====Another strand of literature models return/price extrapolation with bounded rationality (Brock and Hommes, 1998; Hong and Stein, 1999).==== These models can produce a number of asset price features, such as excess return volatility, bubbles and crashes, momentum, and reversal. Our model does not contradict these models. While these models have larger departures from rational expectations models because the agents also have limited knowledge of the pricing function, return extrapolation in our model is the only deviation from rational expectations models in the form of a subjective return expectation. Our results help isolate the effects of return extrapolation and contribute to the literature of deviations from rational expectations.====In addition to return extrapolation, the literature shows that subjective expectations about fundamentals, such as dividends, help explain many features of asset returns, e.g., Fuster et al. (2011), Hirshleifer et al. (2015), Choi and Mertens (2019), Guo and Wachter (2019), and Nagel and Xu (2022a). The high uncertainty about dividend growth perceived by the agent generates excess return volatility and a high equity premium. Because dividends are exogenous variables, these models do not involve consistency conditions. Return extrapolation models in which the agent's subjective expected dividend growth is a function of the return extrapolation state variable imply that the agent also simultaneously extrapolates dividend growth. Our setup that separates subjective return expectations from subjective dividend expectations helps distinguish return extrapolation models from fundamental extrapolation models.====Return by definition is a function of prices and dividends. Thus, knowing the beliefs of two variables in this triplet implies the beliefs about the other. The subjective dividend expectations literature as discussed above shows that extrapolative dividend expectations lead to extrapolation of price growth, which helps explain excess volatility. Another strand of literature keeps dividend expectations largely rational and considers instead extrapolation of price growth, e.g., Adam et al. (2016) and Adam et al. (2017). This approach has also been shown to produce excess volatility. Unlike these two types of extrapolations, we find that return extrapolation instead leads to deficient volatility. These studies together provide some theoretical guidance for understanding which survey data empirical researchers should look at when seeking to assess whether or not subjective beliefs contribute to return volatility: they should either look at dividend or price expectations, but not at return expectations.====In addition to subjective return or dividend expectations, Lochstoer and Muir (2022) show that subjective volatility expectations can also help explain asset pricing puzzles. Unlike return extrapolation that does not significantly affect risk premium, volatility expectations specify perceived risks and directly affect risk premium. Atmaz (2022) claims that the interaction of biased volatility expectations and return extrapolation helps explain the dependence of derivative prices on past returns and investor sentiment.====The remainder of the paper is organized as follows. Section 2 discusses some general results about subjective return expectations. Under return extrapolation, Section 3 presents our model setup and solves for equilibrium, Section 4 studies the asset pricing implications, and Section 5 provides further discussions. Section 6 concludes the paper. Appendix provides the proofs.",Extrapolative asset pricing,https://www.sciencedirect.com/science/article/pii/S0022053123000479,6 April 2023,2023,Research Article,14.0
"Azevedo Eduardo M.,Mao David,Montiel Olea José Luis,Velez Amilcar","The Wharton School, University of Pennsylvania, United States of America,Department of Economics, University of Pennsylvania, United States of America,Department of Economics, Cornell University, United States of America,Department of Economics, Northwestern University, United States of America","Received 1 June 2021, Revised 24 March 2023, Accepted 25 March 2023, Available online 3 April 2023, Version of Record 18 April 2023.",https://doi.org/10.1016/j.jet.2023.105646,Cited by (0),"A risk-neutral firm can perform a randomized experiment (A/B test) to learn about the effects of implementing an idea of unknown quality. The firm's goal is to decide the experiment's sample size and whether or not the idea should be implemented after observing the experiment's outcome. We show that when the distribution for idea quality is Gaussian and there are linear costs of experimentation, there are exact formulae for the firm's optimal implementation decisions, the value of obtaining more data, and optimal experiment sizes. Our formulae—which assume that companies use randomized experiments to help them maximize expected profits—provide a simple alternative to i) the standard rules-of-thumb of power calculations for determining the sample size of an experiment, and also to ii) ad hoc thresholds based on statistical significance to interpret the outcome of an experiment.","There has been a revolution in the use of randomized experiments in the last twenty years across a number of fields. One prominent example is that of large internet companies, which routinely use experiments with tens of millions of users to test almost all of their product innovations. Technology companies like Google, Facebook, and Microsoft call these experiments “A/B Tests”. A/B tests have revolutionized how these and other companies screen product innovations.====We propose practical tools and formulae for determining the sample size of experiments that are used to screen innovations, and offer concrete recommendations to decide which product innovations are worthy of being adopted based on the outcome of a randomized experiment. Our formulae—which assume that companies use randomized experiments to help them maximize expected profits—provide a simple alternative to i) the standard rules-of-thumb of power calculations for determining the sample size of an experiment, and also to ii) ad hoc thresholds based on statistical significance to interpret the outcome of an experiment.====We build upon the “A/B testing problem” proposed in Azevedo et al. (2020). In their model, a firm has a set of potential ideas of unknown quality, and can perform experiments to learn about each idea, subject to some limitations. The goal is to maximize the expected sum of quality of implemented ideas.==== Relative to this previous work, our key contribution is to solve the A/B testing problem for the specific case in which the distribution of idea quality is Gaussian. For clarity, we also focus exclusively on the case of a single idea and experimental cost linear in the size of the experiment. This allows us to provide exact results about the optimal implementation and experimentation strategies.====The A/B testing problem with Gaussian priors considered in this paper has already been studied in a prescient early literature in statistical decision theory. Although the terminology and the context are different, Raiffa and Schlaifer (1961) and other researchers at the time were interested in the optimal use of scarce experimental resources long before this became a commonplace problem in the internet industry. The book by Raiffa and Schlaifer (1961), for example, provides a comprehensive treatment of the “mathematical analysis of decision making when the state of the world is uncertain but further information about it can be obtained by experimentation”. Here we generalize some of the known results in this classical statistical decision theory literature and explain their significance in the empirically relevant context of experimentation in technology companies.====We present four main results. First, there is a simple closed-form solution to the firm's optimal ====—that is, the firm's decision of whether to adopt a product innovation after observing the outcome of a randomized experiment (Proposition 1). According to our result, the firm should calculate the usual ====-statistic based on the estimated quality obtained from the experiment, and implement the idea only if the ====-statistic is above a threshold. The threshold depends on the parameters of the Gaussian prior and the experimental noise, and can be positive or negative.==== A practical takeaway is that a profit maximizing firm might find it optimal to implement some ideas for which there is no evidence of a statistically significant positive effect on expected profits.====Second, there is a closed-form solution to the value obtained from an experiment (Proposition 2). The closed-form solution can be used to show that the value obtained from an experiment as a function of its sample size (assuming the firm uses the optimal implementation strategy) is nonnegative, bounded from above, strictly increasing, and, if the prior mean is different from zero, the value is first convex and then concave. The latter property has two qualitative implications for the determination of an experiment's sample size. First, when the prior mean is different from zero, small randomized experiments have a limited scope (as the marginal value obtained from a small experiment will be close to zero).==== Second, the marginal value of large experiments is eventually decreasing and close to zero, which means that very large randomized experiments are unlikely to be optimal. The formula for the value obtained from an experiment was known in the case where the firm uses an optimal implementation strategy in Raiffa and Schlaifer (1961). We generalize the result to the case of an arbitrary threshold implementation strategy and we document the differences vis-a-vis the optimal production function.====Third, there is a simple characterization of the experiment's optimal sample size (Proposition 3). A firm that maximizes expected profits can find the optimal sample size for an experiment by equating its marginal value to its marginal cost. Because in the A/B testing problem the value obtained from an experiment is first convex and then concave (as a function of the sample size), there will be typically two solutions satisfying the first-order conditions for optimality. The optimal sample size corresponds to the larger of these solutions. It is straightforward to write a simple computer algorithm that solves the first-order conditions and then selects that largest solution. Thus, our results give practical and easy-to-implement alternatives to power calculations for sample size determination.====Fourth, we derive comparative statics of the firm's optimal expected profits and the optimal sample size (Proposition 4). One qualitatively interesting result in our comparative statics is that the relation between the size of an experiment and the variance of the prior is not monotone. We show that, when the prior mean is (in absolute value) smaller than the prior standard deviation, a higher prior variance can lead to smaller or larger experiments.====By construction, the formulae for the implementation strategy and the experimentation strategy are optimal if and only if the distribution of idea quality is indeed Gaussian. As a robustness check, we derive similar results for the case of ==== minimization.==== Our results provide similar practical alternatives to using power calculations for determining an experiment's sample size and to basing implementation decisions on statistical significance, although there are some important qualitative differences relative to model with a Gaussian prior. For instance, it is optimal to implement a product innovation whenever its ====-statistic is positive. Also, in this adversarial setting it is never optimal not to experiment. Our results generalize results from the statistical decision theory literature in Bross (1950) and Somerville (1954). One advantage of the regret minimization approach is that it can be applied to a setting with little data from prior experiments, where estimating a reasonable prior is difficult.====From the firm's perspective, the principle of determining an experiment's sample size by maximizing profits seems more appealing than the standard and prevalent practice of using power calculations for sample size determination.==== This point echoes the critiques of Meltzer (2001) and Manski and Tetenov, 2016, Manski and Tetenov, 2019. The implementation of the optimal sample size is especially appealing in data-rich environments, such as experimentation in online firms, where information from past experiments is readily available and can be used to choose the prior following an empirical Bayes approach (see the discussion in Azevedo et al. (2019) and the references therein).====The rest of the paper is organized as follows. Section 2 presents the model, section 3 the results, section 4 the minimax regret case, and section 5 a numerical example. Section 6 concludes. Proofs are in the appendix.",The A/B testing problem with Gaussian priors,https://www.sciencedirect.com/science/article/pii/S002205312300042X,3 April 2023,2023,Research Article,15.0
"Roy Souvik,Sadhukhan Soumyarup","Applied Statistic Unit, Indian Statistical Institute, Kolkata, India,Department of Mathematics and Statistics, Indian Institute of Technology Kanpur, India","Received 29 November 2020, Revised 25 March 2023, Accepted 26 March 2023, Available online 30 March 2023, Version of Record 5 April 2023.",https://doi.org/10.1016/j.jet.2023.105648,Cited by (0),"We consider the problem of choosing a committee from a set of available candidates through a randomized ==== when there are restrictions on the committee to be formed and agents have separable preferences over the committees. We show that when the set of feasible committees is non-vacuously restricted, that is, cannot be seen as the set of ==== committees with a subset of members, a random ==== is onto and strategy-proof if and only if it is random dictatorial.","We consider the problem of forming a committee consisting of members from a fixed set of candidates through a randomized rule when not every committee is feasible. Such situations occur, for instance, when all feasible committees have a size greater (lower) than some exogenous threshold, or when certain combinations of members are not compatible. See Barberà et al. (2005) for a detailed discussion on the importance of a committee formation under constraints.====Individuals have separable preferences over the committees. Separability intuitively means that individuals have a (marginal) preference for each candidate regarding whether she should be included in a committee or not, and they always prefer to include (exclude) their favored (not favored) candidate in a committee no matter who else is already there in the committee. A ==== (RSCF) decides a probability distribution over the feasible committees for every collection of (separable) preferences of the individuals. It is ==== if each feasible committee is selected with probability 1 at some preference profile. It is ==== if no individual can improve the outcome (with respect to first order stochastic dominance comparison) by misreporting her preference. In other words, strategy-proofness incentivizes individuals to reveal their preferences truthfully by making it a weakly dominant strategy.====As mentioned in the beginning, we consider situations where there are restrictions on the committees to be formed. If the restriction is such that some members are present in all the feasible committees or are not present in any of the feasible committees, then it boils down to the situation where those members are not considered in the decision problem to begin with. In other words, such a restricted set of feasible committees can be seen as the set of all committees that can be formed with a strict subset of members. We call such restrictions vacuous. Clearly, all the results known for the committee formation problem without any feasibility constraints apply to situations where the set of committees is vacuously restricted.====We show in this paper that when the set of feasible committees is non-vacuously restricted, a random rule is onto and strategy-proof if and only if it is random dictatorial on the set of feasible committees. A dictatorial rule on the set of feasible committees always selects the best committee of a particular agent (called the dictator) in the feasible set, and a random dictatorial rule is a probability distribution over dictatorial rules.",Committee formation under constraints through randomized voting rules on separable domains,https://www.sciencedirect.com/science/article/pii/S0022053123000443,30 March 2023,2023,Research Article,16.0
"Yang Erya,Kopylov Igor","Business School, Sun Yat-sen University, 518107 Shenzhen, China,Department of Economics, University of California, Irvine, United States of America","Received 8 April 2022, Revised 20 March 2023, Accepted 27 March 2023, Available online 30 March 2023, Version of Record 5 April 2023.",https://doi.org/10.1016/j.jet.2023.105650,Cited by (0),"We propose a ==== (RQUM) where quasi-linear utility functions are drawn randomly via some probability distribution ====, and utility ties are broken by a convenient lexicographic rule. We characterize RQUM and identify ==== uniquely in terms of stochastic choice data. McFadden's (1973) additive random utility model is obtained as a special case where utility ties have a zero probability in all menus. Another distinct case of RQUM captures finite populations and derives ==== with a finite support. Our main axioms are testable. They prohibit context and reference dependence, and also modify the non-negativity of Block-Marschack polynomials for monetary cost variations. We also characterize RQUM through a stronger version of McFadden and Richter's (1990) axiom of revealed stochastic preferences (ARSP). This approach extends to incomplete datasets.","Empirical observations of consumers' aggregate choices are stochastic in transportation (McFadden (2001)), recreational fishing (Train (1998)), selection of appliance efficiency levels (Revelt and Train (1998)), and many other settings. A single agent's choices can also be random due to intertemporal planning (Rust (1987)) or spontaneous variations in their tastes (e.g., Agranov and Ortoleva, 2017).==== (RUM) represent stochastic choices by maximization of utility functions that are randomly drawn via some probability distribution ====. Such ==== is interpreted in terms of heterogeneous preferences. More formally, ==== is defined over some set Θ of complete and transitive preferences on some consumption space ====. Then any alternative ==== in any finite menu ==== should be chosen with probability====In the ==== RUM of Block and Marschak (1959) (henceforth BM), the domain ==== is finite, and Θ is the set of all ==== (i.e., complete, transitive, antisymmetric preferences over ====). Falmagne (1979) characterizes the classic RUM via non-negativity of BM polynomials. McFadden and Richter (1990) provide another characterization via the axiom of revealed stochastic preference (ARSP).====In many applications, it is convenient to associate the set Θ with a particular class of utility functions on ====. Most importantly, McFadden's (1973) additive RUM adopts representation (1) where the domain==== consists of pairs of consumption goods ==== and monetary costs ====, and the set Θ consists of all ==== preferences. By definition, such preferences can be represented by quasi-linear utility functions that are standard in discrete choice theory and estimation methods (e.g. Nocke and Schutz, 2017). Quasi-linearity is also very common in mechanism design, auction theory, bargaining theory, public economics, etc. The quasi-linear structure implies that Θ can be parametrized by the Euclidean space ==== and hence, ==== can be modeled as a ==== over ====.====To make the additive RUM well-defined, it is necessary that “the probability of ties is zero” (McFadden, 1980, p. S15). Therefore, ==== cannot have atoms==== and hence, cannot have a finite support either. Thus, finite populations are inconsistent with the additive RUM, which can be problematic for welfare analysis and other applications.====Williams (1977) and Daly and Zachery (1979) (henceforth, WDZ) characterize the additive RUM in terms of derivatives of choice probability functions. In particular, the WDZ Theorem assumes a symmetry condition==== where ==== and ==== denote the probabilities of choosing goods ==== and ====, respectively when ==== is the cost vector. Such differential equations cannot be refuted by empirical data because partial derivatives like ==== are unobservable. Moreover, the WDZ theorem does not accommodate some familiar continuous distributions (e.g., uniform or exponential) for which partial derivatives ==== do not exist at some cost vectors ====.====Our random quasi-linear utility model (RQUM) extends the additive RUM characterization and achieves several objectives.====To formulate RQUM, associate each vector ==== with the quasi-linear preference ==== that is represented over ==== by the function ====. Here ==== reflect ==== for consumption goods ==== respectively, and ==== by convention.====Our main representation for stochastic choices is==== where ==== is a Borel probability measure on ====, and the ==== of any pair ==== is defined as ====. Thus all utility ties are assumed to be broken in favor of alternatives with lower grades.====Our main result (Theorem 1) characterizes RQUM via axioms that do not assume or imply differentiability for the functions ====. The first two axioms (No Complementarity and Cross-Price Neutrality) prohibit behaviors like context dependence and reference dependence. The third axiom (Joint Monotonicity) is more complicated, but still testable. It asserts roughly that the revealed probability of any bounded rectangle in the type space ==== should be non-negative. This condition converges to the non-negativity of some BM polynomial when the rectangle expands to an unbounded orthant. We close the model with continuity assumptions and use them to deliver refinements where (i) “the probability of ties is zero” as in McFadden's additive RUM, or (ii) the support of ==== is finite, which accommodates finite populations.====Next, we illustrate how ==== can be uniquely and explicitly derived from the observed stochastic choice rule ====. The key observation is that the ==== of ==== for all ==== must satisfy==== where the menu==== provides all goods ==== at costs ====, respectively. Here it follows from (3) that for any vector ====, the comparisons ==== should hold for all ==== if and only if the preference ==== is maximized by the alternative ==== in the menu ====. Formula (4) implies the uniqueness of ====, which is not guaranteed by the classic RUM. Turansick (2021) shows that such uniqueness can be only obtained under stringent single-crossing conditions on the support of ====. Apesteguia et al. (2017) use a strong version of single-crossing to derive ==== uniquely in terms of choices in binary menus. Identification (4) is substantially simpler than the counterpart in the classic RUM, where the construction of ==== employs a multi-step procedure based on BM polynomials. The identification (4) is also the cornerstone of our proofs, but the full argument invokes results from probability theory (e.g., Billingsley, 1995, Theorem 12.5) rather than differentiability techniques. Corollary 2 applies our Theorem 1 to the reduced domain that appears in the WDZ theorem.====Our next result (Theorem 3) characterizes RQUM via McFadden and Richter's (1990) linear programming approach. This approach extends to finite datasets in Theorem 4 where the identification of ==== is based on the Farkas Lemma rather than the formula (4). We argue that there are no observable distinctions between grading procedures if all ties are broken by any permutation of the set ====.====Finally, we derive the WDZ Theorem as a corollary and discuss several other examples that satisfy or reject RQUM.",Random quasi-linear utility,https://www.sciencedirect.com/science/article/pii/S0022053123000467,30 March 2023,2023,Research Article,17.0
Ma Zizhen,"Economics and Management School, Wuhan University, China","Received 26 April 2021, Revised 9 December 2022, Accepted 27 March 2023, Available online 30 March 2023, Version of Record 9 May 2023.",https://doi.org/10.1016/j.jet.2023.105649,Cited by (0),"This paper studies majoritarian reputational bargaining. Three agents bargain over the division of one dollar under majority rule, and proposers are randomly chosen. Each agent has ==== about whether she is a rational type that maximizes her expected share of the dollar or an obstinate type that commits to claiming a certain share of the dollar. Efficiency and surplus distribution in majoritarian reputational bargaining may differ from their counterparts in bilateral reputational bargaining. In a particular equilibrium of our majoritarian game, efficiency loss vanishes asymptotically as the agents become patient, and bargaining ends immediately if all agents are rational. Moreover, the agent who has the lowest positive ex ante probability of being obstinate achieves the highest ex ante payoff, when such probabilities for all agents are sufficiently low.","Legislative bargaining usually involves multiple agents and proceeds by majority rule, and a legislator may have private information about why she insists on a certain claim: In particular, a claim may be the result of rational equilibrium calculation, or it may be the non-rational behavior of an obstinate type. However, little is known about how such information asymmetry regarding ==== affects efficiency and surplus distribution in ==== bargaining. Inspired by Abreu and Gul (2000) and Baron and Ferejohn (1989), we study the question by introducing obstinate types and imposing the Baron-Ferejohn protocol.====Specifically, three agents with a common discount factor bargain over the division of a dollar. Bargaining continues until a proposal wins majority support, and it is potentially infinitely repeated, if no agreement is reached. A proposer is randomly chosen in each period, and each agent has private information about whether she is a rational type that maximizes her expected share of the dollar or an obstinate type that commits to claiming a certain share of the dollar. Agents' obstinate claims are identical and exceed one half of the dollar.====We construct an equilibrium which satisfies “stationarity” and “minimal screening”: History affects behavior only through public belief about agents' types; and each rational type accepts proposals in which her share exceeds her minimal continuation value of rejection.==== In this equilibrium, even with ==== ex ante probabilities of agents' obstinacy, efficiency loss vanishes asymptotically as the agents become patient, and bargaining ends immediately if all agents are rational. Regarding surplus distribution, the agent who has the lowest positive ex ante probability of being obstinate==== achieves the highest ex ante payoff, when such probabilities for all agents are sufficiently low.====The idea behind asymptotic efficiency is that agents can use threat of exclusion from winning coalitions to deter rational types from imitating obstinate types. To illustrate, suppose agents 2 and 3 observe obstinate behavior by agent 1 and believe that agent 1 is obstinate. Based on this belief, agents 2 and 3 will ally with each other and exclude agent 1 from future winning coalitions, because it is too costly to ally with agent 1. Facing this exclusion, the rational type of agent 1 would rather not imitate the obstinate type, and thus obstinate behavior by agent 1 reveals her obstinacy. As such threat of ==== incentivizes full information revelation, the bargaining process is asymptotically efficient.====Our belief-based exclusion is reminiscent of the opt-out strategies in the bilateral game of Compte and Jehiel (2002): An agent exercises her outside option if she believes that her opponent is obstinate and the outside option, which is ==== fixed, is “preferable” to conceding to the obstinate opponent; such preferable outside options give rise to asymptotic efficiency. In our previous illustration, agent 3 is like an “outside option” for agent 2 (and vice versa), when they believe that agent 1 is obstinate. However, why agent 2 first places agent 3 “outside” and later finds allying with agent 3 “preferable” can be understood only through an analysis of ==== formation of winning coalitions.====Whereas our asymptotic efficiency result relies on exclusion from winning coalitions, our surplus distribution result hinges on inclusion in winning coalitions. The agent who has the lowest positive ex ante probability of being obstinate is ex ante the best off, as she can guarantee inclusion: Each rational proposer prefers to ally with (only the rational type of) the agent who has the lower positive ex ante probability of being obstinate.====To see why, consider the following situation: Agent 1 is rational and acts as proposer; agent 2 has a positive ex ante probability of being obstinate. When all agents' ex ante probabilities of being obstinate are sufficiently small, agent 1 can ally with agent 2 (or 3) with a probability close to one. When agent 3 is known to be rational and thus impervious to the threat (of belief-based exclusion), agent 2's demand is distinctively lower than agent 3's; allying with agent 2 is preferable as agent 1 can benefit from the threat. When agent 3 has a higher probability of being obstinate than agent 2 and is also subject to the threat, agent 2's demand is no longer distinctively lower. Now agent 1 has to take account of the subtle difference between the other agents' demands, which is closely related to the benefit from the threat, and the continuation value after her proposal is rejected. We show that the benefit from the threat exceeds the continuation value of rejection. Thus, allying with agent 2 is preferable as it places a higher probability on the benefit from the threat and a lower probability on the continuation value of rejection.====Efficiency and surplus distribution in our majoritarian game differ from their counterparts in bilateral environments.==== In bilateral reputational bargaining, Abreu and Gul (2000) and Kambe (1999) show that if each agent has only one obstinate type, and if both agents' obstinate claims are identical and exceed one half of the dollar, then efficiency loss remains significant as the agents become patient.==== Moreover, the agent with higher ex ante probability of being obstinate is ex ante better off.====We have discussed the seminal works on bilateral reputational bargaining, namely, Abreu and Gul (2000), Compte and Jehiel (2002) and Kambe (1999). Threat of exclusion has received attention in other environments: Eraslan and Merlo (2002) study stochastic dollar value; Ali (2006) studies disagreement on recognition probabilities. Özyurt (2015) precludes threat of exclusion and studies two consecutive wars of attrition.====Studies of legislative bargaining with complete information have been fruitful since Baron and Ferejohn (1989) formulated the Baron-Ferejohn protocol. Banks and Duggan (2000, 2006) establish existence of stationary equilibria and core convergence for general sets of preferences and alternatives. Eraslan (2002) and Eraslan and Mclennan (2013) show the uniqueness of stationary equilibrium payoffs for divide-the-dollar games.====Legislative bargaining with asymmetric information has received less attention: Tsai and Yang (2010), Chen and Eraslan (2014), Chen (2021) and Meirowitz and Shotts (2009) make contributions to the literature by studying models with two or three periods. Our paper attempts to address fundamental questions of efficiency and surplus distribution that have remained open.====The remainder of the paper is organized as follows. Section 2 sets up the model. Section 3 presents the main results on existence, efficiency and surplus distribution. Section 4 presents the equilibrium construction. Section 5 discusses multiplicity and extensions. Section 6 concludes.",Efficiency and surplus distribution in majoritarian reputational bargaining,https://www.sciencedirect.com/science/article/pii/S0022053123000455,30 March 2023,2023,Research Article,18.0
"Chen Zengjing,Epstein Larry G.,Zhang Guodong","Zhongtai Securities Inst. for Financial Studies, Shandong U., China,Dept. of Economics, McGill U., Canada,School of Mathematics, Shandong U., China","Received 10 June 2021, Revised 20 March 2023, Accepted 22 March 2023, Available online 27 March 2023, Version of Record 4 April 2023.",https://doi.org/10.1016/j.jet.2023.105645,Cited by (0),"This paper studies a multi-armed bandit problem where the decision-maker is loss averse, in particular she is risk averse in the domain of gains and risk loving in the domain of losses. The focus is on large horizons. Consequences of loss aversion for asymptotic (large horizon) properties are derived in a number of analytical results. The analysis is based on a new central limit theorem for a set of measures under which conditional variances can vary in a largely unstructured history-dependent way subject only to the restriction that they lie in a fixed interval.","We study the following (multi-armed bandit) sequential choice problem.==== There are finitely many arms (or actions), each yielding a random payoff. Probability distributions have a common mean but differ otherwise and may not be known to the decision-maker (DM). At each stage ====, DM chooses one arm, knowing the realized outcomes from previous choices. Ex ante she chooses a strategy to maximize expected utility, where the utility index is a function of the (suitably weighted) average payoff. Because we are interested in varying horizons, it is convenient to define a strategy for an infinite horizon, and then to use its truncation for any given finite horizon. Refer to a strategy as ==== if the expected utility it implies in the limit as horizon ==== is at least as large as that implied by any other strategy. We study large-horizon approximations to the value (indirect utility) of the bandit problem and corresponding asymptotically optimal strategies.====A second novelty in our model is the assumption that DM is loss averse (global risk aversion is a limiting special case). Loss aversion was introduced via cumulative prospect theory by Tversky and Kahneman (1992), and has since been well-established empirically and widely applied in economics and finance (see for example, Kahneman and Tversky, 2000; Kobberling and Wakker, 2005; Barberis, 2013, and the references therein). Its essential elements are (i) a reference point; (ii) utility depends only on gains and losses relative to that reference point rather than on the total payoff (or total wealth); (iii) risk aversion (concavity) for gains and risk loving (convexity) for losses; and (iv) greater sensitivity to losses than to gains. Our interest in this paper is the effect of loss aversion in the sequential context defined by a bandit problem. To our knowledge, this is the first study of loss aversion in bandit problems.====We have two related reasons for studying asymptotics. First, it promotes tractability and the derivation of analytical results. Though the literature on bandit problems is enormous, theoretical analysis of Bayesian models is, to the best of our knowledge, restricted to the special case of risk neutrality (see section 2.1.3 for elaboration and a qualification).==== Besides its obvious limitations, risk neutrality also imposes the invariance of risk attitude as some outcomes are realized, and this invariance is key to well-known sequential properties of optimal strategies derived in the literature.==== In contrast, endogenously varying risk attitude is inherent in loss aversion. Moreover, in our setting where means are known and common to all arms, risk neutrality would trivialize the problem.====Our second reason for studying asymptotics is that tractability is plausibly a concern not only for the modeler but also for the decision-maker within the model. We view her as struggling to comprehend an extremely complicated finite-horizon optimization problem, and adopting instead the simplifying assumption of an infinite horizon. She does so with the recognition that an asymptotically optimal strategy is approximately optimal if her horizon is sufficiently long.====Two settings that fit our model well are:====(i) A gambler chooses sequentially which of several given slot machines to play.====(ii) Each visitor to a news website decides whether to click depending on the news header presented to her. The website (DM) chooses the header (arm) with clicks being the payoffs. Visitors are drawn independently from a fixed (possibly unknown) distribution.====In both cases, outcomes are realized very soon after an arm is chosen and plausibly a large number of trials occur in a short period of time (arguing against discounting).====Here is an informal outline of some of our analytical results, which obtain as stated in the infinite-horizon limit and approximately for sufficiently large finite horizons.====Finally, we turn to the proofs of these and other results about bandits and loss aversion. It is not surprising that asymptotic results may be approached via limit theorems. However, classic limit results do not apply, and the key to our proofs is a new central limit theorem (CLT). The martingale version of the central limit theorem considers a sequence ==== of random variables having zero conditional mean and constant conditional variance ====, and shows that (under suitable additional conditions) the distribution of ==== converges to the normal ==== as ====. (The classic result for identically and independently distributed random variables is an immediate special case.) This paper establishes a CLT under the relaxed assumption on variance according to which conditional variances can vary in a largely unstructured history-dependent way subject only to the restriction that they lie in a fixed interval ====, in which case limits take a novel and tractable form. This CLT is the main technical contribution of the paper. One well-known motivation for generalizing from a single probability distribution (hence single variance) to a set of probability distributions (hence set of variances) is robustness to model uncertainty or ambiguity. However, model uncertainty plays no role in our bandit problem - DM is a Bayesian agent, perfectly confident in her understanding of the environment - thus highlighting the usefulness of sets of measures even for Bayesian models.====We proceed as follows. The bandit model and the results outlined above are described in detail in the next section. Proofs for these results must await the CLT which is presented next in section 3.2. Proofs of the CLT and most related results are presented in Appendix A and proofs for the bandit application are in Appendix B.","A central limit theorem, loss aversion and multi-armed bandits",https://www.sciencedirect.com/science/article/pii/S0022053123000418,27 March 2023,2023,Research Article,19.0
"Jermann Urban,Xiang Haotian","Wharton School of the University of Pennsylvania and NBER, 3620 Locust Walk, Philadelphia, PA 19104, United States of America,Guanghua School of Management, Peking University, No. 5 Yiheyuan Road, Haidian District, Beijing 100871, China","Received 30 September 2022, Revised 16 March 2023, Accepted 17 March 2023, Available online 27 March 2023, Version of Record 5 April 2023.",https://doi.org/10.1016/j.jet.2023.105644,Cited by (0),"The majority of bank liabilities are deposits typically not withdrawn for extended periods. We propose a dynamic model of banks in which depositors forecast banks' leverage and default decisions, and withdraw optimally by trading off current against future liquidity needs. Endogenous deposit maturity creates a time-varying dilution problem that has major effects on bank dynamics. ==== cuts produce delayed increases in bank risk which are stronger in low rate regimes. Deposit insurance can exacerbate the deposit dilution and amplify the increase in bank risk.","According to the FDIC, deposits account for 83% of U.S. banks' balance sheets at the end of 2021. Time deposits with limited maturities account for only 6% of all deposits. The remaining deposits are non-maturing: they have no explicit maturity date and are typically not withdrawn for extended periods. Because these deposits can be withdrawn at any time, their maturity varies with bank risk and market conditions.====In this paper, we bring to center stage this feature of bank liabilities and develop a dynamic model of banks with endogenous deposit maturity. Banks create liquidity through issuing deposits and are exposed to the risk of default. Depositors withdraw from their accounts optimally by trading off between current and future risk-adjusted liquidity benefits. We highlight the first-order effects the endogenous deposit maturity has on bank dynamics.====In typical macro-finance banking models, deposits are modeled as one-period debt. As such, when banks issue new deposits, there is no outstanding deposits that can be diluted. With long-term debt, when new debt is issued, borrowers do not internalize the reduction in value of the outstanding debt. This gives rise to debt dilution. Recent studies have shown that dilution affects debt dynamics in major ways (Gomes et al., 2016; Admati et al., 2018).====In our model, there are frictional costs that prevent deposits from being withdrawn and repriced every period. Depositors face liquidity shocks and they withdraw only if the liquidity value net of withdrawal costs exceeds the expected future liquidity benefit of the deposit. This converts redeemable deposits into long-term debt with endogenous maturity, and thus exposes deposits to time-varying dilution. Banks cannot credibly commit to not dilute depositors in the future. We show that bank dynamics are affected by time-varying dilution in significant ways through the lens of banks' responses to interest rate shocks. Overall, our analysis suggests that accounting for endogenous deposit withdrawals is important for modeling banks and studying monetary policy transmission.====First, we find that an interest rate cut generates an initial reduction in bank default risk but is followed by an extended period of significantly higher default risk and higher leverage. Endogenous maturity is key for such boom-bust dynamics. Following a rate reduction, future liquidity benefits of deposits become more valuable. Deposit withdrawals decline and lengthen the maturity, which gives rise to stronger debt dilution incentives. Banks issue new deposits and drive up the subsequent default risk.====A second finding is that such a delayed bust is more pronounced in a low interest environment than in a high interest environment. In a low interest environment, future liquidity benefits of deposits are large and deposit withdrawals are less sensitive to marginal variations in banks' default risk. As a result, when absorbing new deposits and driving up the default risk, banks are less worried about existing depositors pulling out their money. This means deposit dilution is less disciplined in a low interest environment. Banks in this case would like to dilute more aggressively the additional unwithdrawn deposits created by an interest rate cut, relative to banks in a high interest environment.====It is important to notice that the key mechanism in the model—deposit dilution—does not vanish with the presence of deposit insurance. The FDIC estimates 53.3% of US bank deposits to be insured at the end of 2021. We extend our model to include both uninsured and insured deposits. Issuing insured deposits does not dilute the value of legacy insured depositors who are protected by the deposit insurance against default. However, issuing insured deposits can dilute the value of uninsured depositors because banks' default decisions are also affected by the amount of insured deposits. We show that giving banks the choice to substitute into insured deposits can exacerbate the dilution of uninsured deposits and in turn amplify the increase in bank risk following an interest rate cut.====—Our paper contributes to the literature on dynamically modeling banks. In particular, we study bank deposits with endogenous maturity and when dilution is possible. Our work is related to several research areas.====Since the pioneering work by Farmer (1984); Williamson (1987), and Bernanke and Gertler (1989), a large literature has emerged to study the macroeconomic implications of financial markets and institutions, i.e. banks.==== While many papers in the literature follow a tradition of modeling lending-side frictions, our focus is on the liability side. Van den Heuvel (2008) and Begenau (2020) model the liquidity value of bank liabilities for households through a deposit-in-utility setup. In more micro-founded environments, Williamson (2012) highlights how bank liabilities facilitate transactions, and Quadrini (2017) highlights how they provide insurance. Begenau and Landvoigt (2020) propose a model where bank and shadow-bank liabilities provide different liquidity values. Egan et al. (2017) estimate a model where banks compete for insured and uninsured deposits. Different from these papers, we model explicitly deposit withdrawals and our analysis highlights long-term debt subject to dilution.====There are several studies that have emphasized the rich dynamics of long-term corporate debt with dilution, for instance, Gomes et al. (2016); Crouzet (2017); Admati et al. (2018); DeMarzo and He (2021); Malenko and Tsoy (2021), and Jungherr and Schott (2022). Different from these, our model features endogenous maturity. Xiang (2023) shows how covenant violations and subsequent debt restructuring generate endogenous debt maturity. In our paper, maturity is driven by deposit withdrawals, and how depositors respond to interest rate shocks is key to our results.====Dilution has been studied in the sovereign debt literature, for example Hatchondo and Martinez (2009); Arellano and Ramanarayanan (2012); Chatterjee and Eyigungor (2012); Aguiar et al. (2019); Bocola and Dovis (2019). The problem of a sovereign differs in a number of ways from the problem of a bank. One key difference is that a bank in our model can issue equity, but a sovereign does not have this option. Another difference is that bank debt in our model generates direct service flows, for sovereigns debt serves only as an instrument for intertemporal substitution.====The liability-side theory where banks engage in maturity transformation by offering the option to withdraw deposits early is central to a large finance literature building on Diamond and Dybvig (1983). In this approach, a sequential service constraint can lead to bank runs. Instead, our model features debt dilution. Models have mostly abstracted from time-varying maturity. Exceptions are several papers interested in the valuation of non-maturing deposits from a bank's perspective where maturity is exogenously specified, including Hutchison and Pennacchi (1996); Jarrow and van Deventer (1998); Nyström (2008) and Wolff (2000). Some recent macro-finance studies also emphasize the long-term nature of bank deposits. For instance, Bolton et al. (2020) present a model where banks have limited control over deposit inflows, and Drechsler et al. (2021) present empirical evidence suggesting that bank franchise value confers long duration to bank deposits. Different from these, we model the dilution problem associated with long-term debt.====The paper proceeds as follows. Section 2 presents our model for non-maturing deposits and studies banks' responses to interest rate shocks. Section 3 considers extensions with insured deposits and when bank asset profitability comoves with interest rates. Section 4 discusses our model ingredients and empirical implications. Section 5 concludes.",Dynamic banking with non-maturing deposits,https://www.sciencedirect.com/science/article/pii/S0022053123000406,27 March 2023,2023,Research Article,20.0
"Segura Anatoli,Villacorta Alonso","Bank of Italy, Italy,CEPR, United Kingdom of Great Britain and Northern Ireland,University of California, Santa Cruz, United States of America","Received 29 September 2021, Revised 7 March 2023, Accepted 12 March 2023, Available online 24 March 2023, Version of Record 7 April 2023.",https://doi.org/10.1016/j.jet.2023.105640,Cited by (0),"Safe asset demand increases loan risk. This arises in a competitive model in which ==== vehicles create safe assets by pooling loan payoffs purchased from loan originators. Equity investors allocate their ==== between originators, who need skin-in-the-game due to moral hazard, and vehicles, who need loss-absorption capacity against aggregate risk. An increase in demand for safety fosters safe asset creation through a ==== boom: originators sell more of their loan payoffs to vehicles, equity is reallocated from originators to vehicles, and the two effects contribute to an increase in loan risk. The model is consistent with a broad set of facts in the run-up to the Global Financial Crisis.","A surge in safe asset demand is considered an important driver of the securitization boom in the run-up to the Global Financial Crisis (GFC) (Caballero and Krishnamurthy, 2009; Bernanke et al., 2011). Recent evidence shows that the senior tranches of securitized loans, including subprime loans, issued ahead of the GFC were effectively safe (Ospina and Uhlig, 2018). At the same time, a substantial body of research has also found evidence of a weakening of lending standards and excessive risk-taking by loan originators during that period (Loutskina and Strahan, 2011; Ashcraft et al., 2019). A question arises: How can an increase in ==== asset creation be met with the origination of ==== loans?====We develop a novel competitive equilibrium model of securitization and the capital structure of the modern intermediation chain to answer this question. The economy has two dates and two types of investors: savers and experts. Savers are infinitely risk-averse and only invest in safe securities. Their overall endowment determines the demand for safety in the economy. Experts are risk-neutral and their overall endowment is normalized to one. Experts can set-up and invest their wealth in the equity of financial firms that originate loans under a constant return to scale technology, called ====. Each originator-issued loan is exposed to aggregate and institution-specific idiosyncratic risks, and the loan default risk can be reduced by increasing the monitoring intensity of the originator's expert, which entails convex disutility costs. The presence of idiosyncratic loan risk at the originator level creates a role for loan payoff diversification across originators to back the manufacturing of safe assets, which allow funds to flow from savers towards loan origination.====We first consider, as a benchmark, a frictionless economy in which each expert can set-up many originators and loan risk (or monitoring) is observable. Each expert can diversify loan risk across its multiple originators, pledge their loans as safe collateral, and directly issue safe securities. By originating safer loans (by monitoring more intensively), an expert increases the safe collateral value of the diversified pool and is able to expand borrowing through safe asset issuance placed to savers. An increase in demand for safety pushes the equilibrium safe rate down, providing experts more incentives to reduce loan risk to expand cheap borrowing from savers. The frictionless economy thus responds to a surge in demand for safety by creating ==== assets backed by the origination of ==== loans.====We then consider an economy that realistically captures key features of the modern intermediation chain in which the securitization process involves different intermediaries and is subject to agency frictions.==== We assume that each expert can set up only one originator, exposed to idiosyncratic risk, and loan risk (or monitoring) is unobservable, which leads to moral hazard when risky loans are placed to outsiders. Safe asset creation through diversification cannot be performed under one roof, but it requires that originators sell loan payoffs to a different intermediary, a ====. Vehicles then pool those loan payoffs from multiple originators, diversify their idiosyncratic risks away, and manufacture safe assets that can be placed to savers.====A central role in this economy is played by the allocation of experts' endowment as equity in the two financial firms. First, as originators raise external funds by selling loan payoffs to vehicles, a moral hazard problem in the unobservable loan monitoring arises as in Holmstrom and Tirole (1997). Experts' equity investments in originators improve monitoring incentives and reduce moral hazard (====). Second, the diversified pools of loan payoffs from many originators purchased by vehicles are exposed to losses under negative aggregate shocks. Experts' equity investments in vehicles provide the loss-absorption capacity against aggregate risk needed to manufacture safe assets (====).====The capital structure of the financial firms in the intermediation chain, the risk of the originated loans, the returns of safe and risky securities and of financial firms' equity, aggregate loan issuance and the size of the securitization sector are all determined in equilibrium. In particular, the allocation of experts' endowment between the equity of originators and vehicles induces their equity returns to be equal in equilibrium. The equity allocation thus trades off the gains from skin-in-the-game at origination and those from credit enhancement in securitization vehicles.====The presence of moral hazard and the need of a different entity to buy and pool loans from different issuers to diversify their idiosyncratic risks invert the response of loan risk to an increase in demand for safety relative to that in the frictionless economy: more safe asset creation is backed by the pooling of ==== loans. As demand for safety increases, the safe rate falls, allowing securitization vehicles to offer cheaper funding to originators. As a result, originators increase leverage through additional sales of risky loan payoffs. In addition, equity to support the issuance of safe assets in the vehicle sector becomes relatively scarcer, and experts in search of high returns on their wealth reallocate their investments from the equity of originators to that of vehicles. The off-loading of risks and reallocation of equity from originators to vehicles exacerbate originators' moral hazard, inducing more loan risk. Thus, as demand for safety increases, a securitization boom fuels the aggregate lending expansion, the intermediation chain becomes “longer” as a larger fraction of aggregate lending is channeled through vehicles, and loan risk increases. The manufacturing of more safe assets is thus paradoxically tied to the origination of riskier loans.====The increase in loan risk reduces the expected payoff net of monitoring costs of the loans originated and output in the economy. However, there is no scope for policy interventions that make all agents better off as the existence of competitive markets for safe and risky securities and financial firms' equity ensure that the equilibrium of the economy is constrained efficient.====The model predictions regarding the financial sector response to a credit expansion driven by demand for safety are consistent with a broad set of empirical evidence in the run-up to the GFC. In particular, the model implies that the spread between the expected return of risky loans and safe assets may fall while that between equity and safe assets increases. To the best of our knowledge, our model is the first in explaining this “puzzling” differential movement in the risk premia of fixed-income securities versus equity (Bernanke et al., 2011; Caballero et al., 2017). This is generated by our model because, as safety demand increases, the securitization expansion allows an increase in originators' leverage that more than offsets the reduction on the loan spread.====  Our model shows how demand for safety leads paradoxically to an increase in the risk of the loans that back the manufacture of safe assets. Such effect of demand for safety on credit risk is new in the literature, which has focused on how demand for safety may lead to an increase in refinancing risk (e.g., Caballero and Krishnamurthy, 2009; Stein, 2012), or has explained the securitization expansion in the run-up to the GFC by relying on investors' misperception of the actual risk of “safe” securities (Gennaioli et al., 2013). Some other papers focus instead on demand for low-risk quasi safe assets and highlight how this may give rise to boom-bust episodes in risky asset markets and economic activity (Gorton and Ordoñez, 2014, Gorton and Ordoñez, 2022; Moreira and Savov, 2017). The increase in loan risk in our model leads to a reduction in expected output but not to an increase in output volatility. The mechanisms underpinning our safe asset creation paradox are related to, but different from, those in the literature on credit booms and financial fragility (Dell Ariccia et al., 2014; Martinez-Miera and Repullo, 2017 and Bolton et al., 2021). In fact, we show that similar agency frictions do not lead to more risk-taking in a credit expansion driven by safety demand when diversification can be undertaken under one roof, as the safety demand from investors prevents the off-loading of risk from loan originators to them.====The analysis of how the financial sector satisfies demand for safety has been the focus of a strand of the banking literature. We share with Gennaioli et al. (2013) and Diamond (2020) the focus on the manufacturing of safe assets through diversification. These papers take the risk and payoffs of the productive assets of the economy as given, while we analyze the interplay between safe asset creation through securitization and endogenous loan risk at origination.====We develop a framework in which vehicles are financial intermediaries with a diversified portfolio and a capital structure designed to satisfy investors' different risk-appetites. A related paper is DeMarzo (2004), in which a long intermediation chain with several rounds of pooling and tranching emerges as the solution to a security design problem in presence of asymmetric information. The paper exhibits endogenous risk retention along the chain but the risk of the originated loans is exogenous. The diversification role of financial intermediaries has also been emphasized in the delegated monitoring literature (the seminal paper of Diamond (1984), and more recent contributions in Krasa and Villamil (1992), Diamond (2004), Villacorta (2021)). In these papers, the capital structure of intermediaries is designed to minimize the costs of monitoring the monitor.====Our paper is also related to other strands of the theoretical securitization literature. Moral hazard and adverse selection problems in the originate-to-distribute intermediation chain and how to address them with endogenous risk retentions have been extensively studied in the literature (Parlour and Plantin, 2008; Daley et al., 2020; Vanasco, 2017; Caramp, 2017; Neuhann, 2019). These papers, though, abstract from safe asset creation and the diversification benefits associated with securitization, which is the focus of our paper. The interplay between origination incentives and diversification benefits when there is no aggregate risk is studied in Malherbe (2012), who focuses on the trade-off between ex ante and ex interim risk-sharing contracts that are issued under symmetric and asymmetric information, respectively. We only consider ex ante contracts in an environment in which aggregate risk implies the need of equity tranches to support the issuance of safe tranches in securitization.====Some recent papers analyze the endogenous capital structure of non-financial firms and banks (Allen et al., 2015; Gornall and Strebulaev, 2018; Diamond, 2020; Gale and Gottardi, 2020). We share with these papers the interest on how market forces shape the equity allocation in the economy.====The paper is organized as follows. Section 2 presents the model environment. Section 3 describes the relationship between demand for safety and loan risk in some frictionless diversification benchmark economies. Section 4 focuses on the paradox of safe asset creation. Section 5 discusses the safety demand narrative of the run-up to the GFC through the lens of our model. Section 6 concludes. All the proofs of formal results in the paper are in the Appendix.",The paradox of safe asset creation,https://www.sciencedirect.com/science/article/pii/S0022053123000364,24 March 2023,2023,Research Article,21.0
"Hashimoto Ken-ichi,Ono Yoshiyasu,Schlegl Matthias","Graduate School of Economics, Kobe University, Japan,Institute of Social and Economic Research, Osaka University, Japan,Department of Economics, Sophia University, Tokyo, Japan","Received 5 July 2022, Revised 12 February 2023, Accepted 12 March 2023, Available online 23 March 2023, Version of Record 11 April 2023.",https://doi.org/10.1016/j.jet.2023.105641,Cited by (0),"In this paper, we show that underemployment and not necessarily high unemployment becomes the main measure of economic slack in the labor market under secular stagnation. Specifically, involuntary underemployment in the form of a persistent shortfall of working hours occurs in the search and matching model, provided that households derive utility from holding ====, and quickly dominates the total employment gap under stagnation. Conventional policy measures aimed at reducing unemployment may increase the labor market gap through their effects on underemployment and should be used with caution. In contrast, increases in aggregate demand improve unemployment and working hours, while increases in labor productivity worsen underemployment without improving unemployment (“paradox of toil”). Our analysis provides new insights into empirical puzzles such as Japan's seemingly decent employment record during its lost decades.","Macroeconomists conventionally rely on the unemployment rate as the main measure of slack in the labor market and its implications for wage and price dynamics. However, such an analysis is incomplete at best and potentially misleading once an economy is constrained by the effective lower bound on the nominal interest rate and suffers from persistent stagnation. Under such circumstances, it can be underemployment, a state in which employed workers want to increase working hours at the same wage but are unable to do so, rather than unemployment that provides a more accurate measure of labor market slack.====Japan is the prime example of an economy stuck in a stagnation equilibrium without any natural recovery. However, Japan's economy has performed surprisingly well in terms of its employment record during the lost decades as the unemployment rate has remained low by international comparison peaking at slightly more than 5% in 2002 as illustrated in panel (a) of Fig. 1. Unemployment in 2018 had reached its lowest level since the early 1990s. Yet, this decline has not resulted in significant wage or price pressures.==== The absence of widespread unemployment is often attributed to the specific features of the Japanese labor market, most notably the traditional practices of long-term employment, which make unemployment in Japan less responsive to fluctuations in spending.==== Then how, if not in unemployment, did the lack of demand manifest itself in the labor market?====During the stagnation period the structure of Japan's labor market has changed profoundly with a substantial rise in underemployment in the form of part-time and non-regular employment and declining working hours. Panel (b) of Fig. 1 shows the share of part-time employment in total employment in contrast to the OECD average. Having been less than 12% in the 1980s, the share of part-time employees has sharply increased throughout the lost decades, primarily reflecting the lack of alternative employment opportunities.==== By 2018, it had doubled compared to the 1980s level whereas it only increased by 3 percentage points over all OECD countries. A similar trend can be observed for other forms of non-regular employment (see Japan Institute for Labour Policy and Training, 2015). Finally, the steady decline in average hours worked in the OECD data is also indicative of the rise of underemployment in Japan since 1990.====Rising underemployment and the absence of wage and price pressures despite low unemployment are no longer exclusively observed in Japan, but have become well-documented phenomena in several European countries, and to a lesser extent in the United States, after the Great Recession as well.==== Surveying the empirical evidence, Blanchflower (2019, p.144) argues that “underemployment has replaced unemployment as the main measure of labor market slack”.====In this paper, we propose a model of secular stagnation that distinguishes between unemployment, its natural rate (i.e. structural unemployment) and underemployment. Using this model, we show that secular stagnation leads to underemployment and a persistent wedge between unemployment and its natural rate in the labor market but not necessarily to high unemployment, thereby providing a rationalization for the puzzling case of Japan and the theoretical counterpart to the empirical analysis described above. Specifically, we incorporate a preference for wealth into the standard search and matching model of the labor market. In a setting with infinitely-lived households, such a preference replaces the bequest motive within dynasties that is frequently incorporated in overlapping generations models. The preference for wealth creates a strong motive to save in addition to the standard consumption smoothing motive, which can explain why empirically the saving rate of households is increasing in their wealth (see Benhabib and Bisin, 2018; Fagereng et al., 2019).==== Importantly, the preference for wealth allows for the possibility of a secular stagnation equilibrium as is well-known from contributions such as Ono (1994), Michau (2018) or Schlegl (2018). High desired savings can drive the natural real interest rate into negative territory. Yet, the effective lower bound, which we model as a rate of zero for simplicity, prevents the nominal interest rate from falling sufficiently. This causes excess savings which depress aggregate demand. Combined with downward nominal wage rigidities in the spirit of Schmitt-Grohé and Uribe, 2016, Schmitt-Grohé and Uribe, 2017, the model economy operates in a secular stagnation equilibrium characterized by deflation and a persistent lack of demand.====In the absence of demand shortage, our model behaves similar to the standard search and matching model except that the preference for wealth creates a new transmission channel for shocks via induced changes in the real interest rate. Unemployment occurs in equilibrium as vacancies and job-seekers have to match in a costly process before a position can be filled, but it is purely structural and conditional on being employed, households realize their potential working hours and there is no underemployment. The tightness of the labor market affects both the intertemporal consumption decision of the household via changes in the wealth premium and market entry and exit of firms via changes in returns on firm ownership. In equilibrium, labor market tightness is determined such that optimal household and firm behavior are compatible with each other. The nominal interest rate then adjusts endogenously in the money market.====In the stagnation equilibrium, in contrast, unemployment persistently exceeds its natural rate and underemployment occurs. Both mutually affect each other. When desired savings are sufficiently strong, the zero lower bound on the nominal interest rate becomes a binding constraint and the real interest rate is determined by the rate of deflation. Faced with a lack of demand and depressed sales, firms respond by cutting working hours resulting in underemployment, lower output and less consumption. The lack of demand, however, also affects the incentives for job creation. Firm ownership becomes less attractive due to lower profits (“job creation channel”), which reduces job creation incentives for firms and results in an increase in unemployment above its natural rate. The total employment gap then exceeds the natural rate of unemployment via both an unemployment gap and underemployment due to the shortfall of working hours. Quantitatively, worsening stagnation is primarily reflected in rising underemployment, while the response of the unemployment rate is weak, unless the elasticity of the job finding rate is sufficiently high and the employment gap is sufficiently severe.====In addition, we examine the effects of labor market policies and demand and supply variations. Conventional policies aimed at reducing unemployment, specifically a reduction in unemployment benefits, the introduction of a search cost subsidy and increased wage flexibility, might increase the total employment gap under stagnation via their effects on underemployment. The effects of demand and supply shocks are reversed under demand shortage. Higher demand, in the form of government spending, increases working hours and lowers unemployment thereby raising consumption and output. In contrast, a positive productivity shock worsens underemployment without affecting unemployment or total output. This is equivalent to the paradox of toil that is common to models of stagnation. These results are consistent with those observed in Japan during the 1990s and the financial crisis.====We then calibrate this model to match characteristics of Japan's labor market and quantify the relative importance of unemployment and underemployment under stagnation as well as the effects of various parameter variations. For all of our specifications, underemployment quickly dominates the total employment gap as stagnation becomes more severe. This finding is confirmed in several robustness checks. It is only in the case of an increasing job destruction rate under stagnation that unemployment rises substantially. While empirical evidence points towards a relatively stable job destruction rate in Japan, this job destruction channel is likely of relevance in explaining labor market patterns in other economies.====Our analysis concludes that secular stagnation causes underemployment and a persistent wedge between unemployment and its natural rate, but not necessarily high unemployment. As long as the economy is stuck in the stagnation equilibrium, wage growth and inflationary pressures do not emerge even when unemployment is low as they are subdued by the prevalence of underemployment. It is then primarily underemployment that responds to economic disturbances and labor market policies under stagnation. These findings highlight the need for further policy intervention in support of aggregate demand despite a seemingly decent employment situation in terms of the unemployment rate.====  Our paper analyzes the interactions of structural unemployment based on search frictions and underemployment under secular stagnation, thereby contributing to two fields of macroeconomics that have so far been treated fairly independently.====The canonical search and matching model of the labor market (see Diamond, 1982; Mortensen, 1982; Pissarides, 1985, Pissarides, 2000, among others) has initiated a rich literature on structural unemployment as a consequence of frictions in the labor market.==== Variations in working hours are introduced into this framework by Fang and Rogerson (2009) as households optimally choose their labor supply based on the consumption versus leisure trade-off. Workers are always on their labor supply curves and part-time and non-regular employment is purely voluntarily. Their rise in Japan is, for instance, explained by productivity shocks in combination with labor immobility due to differences in training costs among workers (see Ariga and Okazawa, 2011) or by an exogenous increase in part-time labor supply inducing households and firms to optimally re-allocate between full-time and part-time jobs (see Kang et al., 2020). These voluntary shifts in the composition of the labor force do not constitute underemployment as defined in this paper.==== In addition, underemployment in our model does not rely on firm or worker heterogeneity.====Other contributions, such as Manning (2003) and Ashenfelter et al. (2010), provide a microeconomic view on underemployment based on market concentration and monopsony power of firms. From the firm's perspective, variations in working hours are a cost-efficient way to avoid hiring and firing costs. In contrast, firms are perfectly competitive with free market entry and exit in our model. Underemployment is a macroeconomic phenomenon. Workers want to work additional hours at the same wage, but are constrained by a lack of demand.====Demand-driven variations in working hours are modelled in a related framework by Kudoh et al. (2019) who analyze changes in the composition of firms' labor demand over the business cycle. While labor market participation is chosen by each individual, working hours conditional on employment are adjusting in response to demand-driven fluctuations in production, which is a feature similar to our model. However, these fluctuations occur over the business cycle and are hence purely temporarily in nature, which is in stark contrast to our model which allows for the possibility of demand-driven underemployment as a steady state phenomenon due to the binding effective lower bound on the nominal interest rate.====The secular stagnation hypothesis, first proposed by Hansen (1939), argues that an oversupply of savings at full employment permanently depresses aggregate demand as the zero lower bound on the nominal rate prevents the real interest rate from falling sufficiently to stimulate spending. In the presence of downward nominal wage rigidity, this results in an equilibrium with a persistent output gap. The oversupply of savings has been modelled among others as a consequence of demographics (Eggertsson et al., 2019), a shortage of safe assets (Caballero and Farhi, 2018) or strong liquidity preferences (Ono, 2001; Illing et al., 2018).====In this paper, we rely on a preference for wealth in line with the contributions of Ono (1994, Chapter 11) and Michau (2018), which, however, do not consider structural unemployment and abstract from labor market frictions.==== The labor market gap is either abstracted from or it consists of a demand-driven shortfall of working hours only. We extend these models by allowing for potential interactions of unemployment and underemployment in an integrated framework.====Several approaches have been proposed in the literature to model involuntary unemployment in the search and matching model, which are related to our concept of underemployment. Michaillat (2012) shows how wage rigidities and decreasing marginal returns to labor can result in rationing unemployment in addition to frictional unemployment. The former occurs only during recessions following negative technology shocks which reduce the marginal product of labor below the wage to which firms optimally respond by reducing employment. While Michaillat (2012) focuses only on the supply side and business cycle variations, we complement this analysis by modelling the demand side allowing for the possibility of persistent demand shortage. Interestingly, a similar mechanism creates spillovers from underemployment to unemployment in our model. Underemployment occurs when the downward price rigidity becomes binding. Then, firm profits and the marginal product of labor fall due to lower working hours resulting in higher structural unemployment.====Similarly to our model, Michaillat and Saez (2022) also rely on a preference for wealth allowing for an equilibrium with a permanent liquidity trap. Keynesian unemployment occurs in addition to frictional unemployment when the real interest rate, which is implicitly set by the central bank, is excessively high and depresses demand. Higher unemployment reduces desired savings of households thereby restoring equilibrium. The same channel is at work in our model as the real interest rate is excessively high under stagnation.==== However, our model also explicitly considers firm behavior under free market entry. Then working hours and unemployment are jointly determined to reconcile optimal household and optimal firm behavior for a given real interest rate. Adjustment then occurs via a combination of fewer working hours and higher unemployment. Moreover, we show that the first effect is quantitatively more important.====This paper is organized as follows. Section 2 presents a general equilibrium model of underemployment. Section 3 analyzes the steady state equilibria. In section 4, we theoretically derive the responses of the main model variables to parameter variations as well as the condition under which the response of underemployment quantitatively exceeds the unemployment response. In section 5, we calibrate the model to Japanese labor market data and then numerically simulate the effects of various parameter variations on the employment gap under stagnation. The final section concludes. All proofs are in the mathematical appendix.","Structural unemployment, underemployment, and secular stagnation",https://www.sciencedirect.com/science/article/pii/S0022053123000376,23 March 2023,2023,Research Article,22.0
"Chen Yi,Jungbauer Thomas,Wang Zhe","Samuel Curtis Johnson Graduate School of Management at Cornell University, 114 Feeney Way, Ithaca, NY-14853, United States of America,Smeal College of Business at Pennsylvania State University, 343 Business Building, University Park, PA-16802, United States of America","Received 25 January 2021, Revised 6 March 2023, Accepted 7 March 2023, Available online 15 March 2023, Version of Record 20 March 2023.",https://doi.org/10.1016/j.jet.2023.105639,Cited by (0),"We propose a model of strategic delegation in professional labor markets in which big and fringe firms compete for heterogeneous workers. In this context, big firms decide whether to exercise their market power to suppress wages (====) or to delegate hiring to divisions, thereby committing to bidding more fiercely for more skilled workers (====). This reduces the incentive for other firms to go toe-to-toe with the decentralizing firm. In equilibrium, a big firm commanding labor market power opts for either of these two strategies. We find that the presence of a big firm in the labor market is detrimental to social welfare only if its size exceeds the tipping point beyond which it ceases to decentralize. Moreover, our model is compatible with recent studies that negatively link labor market concentration and labor share. Finally, the introduction of a wage floor encourages the big firm to decentralize, increasing match quality, such that social welfare may increase despite a drop in employment.","As a new age of big firms continues to take shape, controversies have emerged over the outsized economic influence these behemoths wield. Changes in market structure—due to factors such as higher fixed costs, technological advancements, a decline in antitrust enforcement (de Loecker et al., 2020), and greater product market competition through globalization and growth of platform competition (Autor et al., 2020)—now allow these dominant firms to command substantial market power. Beyond the perils of imperfect competition in downstream markets, concern has lately grown about a corresponding increase in labor market concentration.==== Azar et al. (2020) find that labor market concentration in the US has significantly risen, estimating that, according to Department of Justice guidelines, nearly three quarters of labor markets in the US are moderately or highly concentrated, accounting for about a third of the overall workforce.====Big firms, however, do not always exercise their market power, often delegating recruitment to independent divisions or subsidiaries. According to the World Management Survey, 31% of senior managers in the manufacturing sector report having full authority to hire workers for their division without permission from headquarters, while another 32% report that they have extensive authority over hiring despite still needing their superiors to sign off.==== These figures indicate that decentralized recruiting is indeed a common phenomenon.====When a company delegates recruiting, its divisions often end up competing for the same crop of workers. Recruiting platforms and business magazines even provide recommendations both to job-seekers on how to successfully apply for several jobs at the same company (Lindzon, 2020), and to firms on how to handle inter-division competition for workers (McNeal, 2001).====Conventional wisdom suggests that firms decentralize recruiting to leverage dispersed knowledge. Large corporations are impersonal and slow to act, and often struggle to exploit their employees' expertise (Steakley, 2013). Decentralized recruiting accelerates the pace of decision-making and adds a personal touch and local expertise (Gregory, 2019).==== Consequently, we would expect large firms to delegate recruiting more often than smaller ones. However, there is strong evidence that points to the contrary. World Management Survey data indicates that firms with larger market power actually decentralize less, which is to say, there is a statistically significant negative correlation between a firm's market power and its degree of decentralization. In further support of this finding, a 2017 Mercer survey of firms from 26 industries in 43 countries uncovers a negative relationship between firm size and decentralization over a significant range of firm sizes (Mercer, 2017).====In this paper, we argue that—beyond leveraging dispersed knowledge—firms may ==== delegate recruiting. When a firm encourages competition among its divisions, wages go up, which increases the skill of its incoming workforce while deterring rival firms from bidding for more skilled workers. Strategic delegation thus allows firms to hire better talent at only moderately increased wages. This finding is consistent with another dimension of the World Management Survey, namely, that firms who place a comparatively high value on recruiting top talent are more likely to decentralize recruiting. Conversely, a centralized recruiting approach allows the firm to exploit its market power to suppress wages. It is the firm's navigation of this trade-off that determines the extent of its effectively leveraged market power.====Formally, we introduce a general three-stage model of recruiting. Big firms compete against a continuum of fringe firms for heterogeneous workers.==== In stage one, big firms choose whether or not to decentralize recruiting, and, if they opt to do so, the number of agents (divisions). In stage two, each agent posts a schedule of wages, one per job, while simultaneously each fringe firm posts a single wage for its only job. Potential fringe firms may also enter the market at a cost and post a wage. Varying this cost allows us to analyze the spectrum from complete closure to free entry. Agents, even if put in action by the same big firm, act in their own best interest. Ultimately, in stage three, more able workers are assigned to jobs paying higher wages.==== We invoke backwards induction to identify equilibrium delegation and bidding patterns.====The key tension in our model pits a big firm's market power versus the strategic effect of delegation. On the one hand, by not delegating, the big firm enjoys great labor market power due to its size, allowing it to profit from severely suppressed wages. We refer to this approach as ====. On the other hand, by delegating to multiple agents, a big firm commits to offering higher wages in the bidding stage through self-induced competition among its agents. This commitment alters the incentives of the other firms, as they foresee fierce competition in the high-wage range and thus lower their bids. This strategic effect, in turn, eases pressure on the agents and ultimately benefits the big firm that decentralizes. In other words, the big firm acts like a Stackelberg leader to crowd out competition for top workers. We refer to this approach as ====.====In our model definition, we disregard one benefit of decentralization, that is, to leverage decentralized knowledge, which arguably should increase with firm size. Instead, we focus on the strategic delegation effect of decentralization, that, to best of our knowledge, has not been described in the literature. Not only does our model introduce a novel motivation for the decentralization of recruiting, it also explains why big firms' tendency to decentralize may in fact sometimes decrease with firm size. Intuitively, when a big firm commands a significant fraction of vacancies, any gains from commitment through delegation are outweighed by its losing the ability to suppress wages. On the contrary, a moderately sized big firm does not enjoy significant market power, and hence considers delegation more attractive. This contrast lays out an empirical strategy to detect ==== in recruiting. If comparatively larger firms in an industry decentralize recruiting less often, or if a firm switches to a centralized (decentralized) recruiting approach when it increases (decreases) in size and/or market power, strategic considerations of decentralization are likely at play.====We analyze two specific scenarios of the general model described above. The first is a ====, in which the demand side is characterized by two big firms. Analyzing this scenario allows us to understand delegation patterns of big firms competing against each other. On the other hand, we consider a ====, in which one big firm commanding market power is pitted against a multitude of small competitors, a so-called competitive fringe. The tractability of this scenario not only allows for a comprehensive welfare analysis, but also to characterize the effects of entry and the introduction of a binding wage floor on the big firm's decision to decentralize. Finally, we compare the partial commitment from delegation with a hypothetical full commitment Stackelberg equilibrium.====When two big firms of similar size compete for heterogeneous workers in a duopsony, we find that, in equilibrium, one opts for ==== while the other chooses ====. That is to say, one big firm delegates hiring to multiple agents, inducing competition among them, which ultimately allows them to hire top talent. The other big firm, discouraged from competing for the most skilled workers by its competitor's aggressive approach, does not delegate; it relies on its market power to dampen wages, and ultimately hires the least skilled workers in the market. The more aggressively a big firm delegates, the less attractive a talent acquisition approach becomes for its competitor, and vice versa.====Thereafter, we turn to the analysis of a ====, the tractability of which allows us to conduct a comprehensive strategic and welfare analysis. We find the big firm's delegation pattern to be non-monotone in its relative productivity compared to fringe firms. At one extreme, the big firm never delegates if it is less productive than fringe firms. In this scenario, its productivity does not warrant an aggressive wage schedule; the big firm opts for ==== and hires the least able workers in the market at low wages. At the other extreme, the big firm opts against delegation if it is highly productive. In this case, the big firm acquires almost all of the top workers even without delegation, and as such, there is no point in sacrificing market power. Only when the big firm is moderately productive does there exist an interior threshold size below which it will decentralize. Moreover, whenever delegation occurs, the optimal number of agents decreases in the big firm's productivity. This is because the big firm wants to create competition only to the point of deterring fringe firms, but no more. When productivity is higher, fewer agents are needed.====The unique equilibrium of the quasi-monopsony has implications for welfare and its distribution among firms and workers. When the big firm is less productive than fringe firms, it hires the least able workers and does not harm efficiency. When the big firm is more productive than the fringe, overall welfare depends on the endogenous decision of decentralization. A moderately productive and modestly sized big firm opts to delegate in equilibrium; the ensuing competition raises their wages. More skilled workers are thus attracted to the big firm, improving matching quality. As a result, a delegating big firm poses little threat to efficiency. However, if its size exceeds the threshold beyond which it ceases to decentralize, the big firm retains its market power to suppress wages. As a consequence, matching quality deteriorates. Nevertheless, even in this case, potential mismatching is limited due to the large fraction of workers hired by the big firm. Entry costs for fringe firms affect the equilibrium in so far as the big firm's gains from suppressing wages are disciplined by potential entry. The welfare effect is most salient when the entry barrier is lowered to the point where the big firm starts to decentralize, boosting both efficiency and labor share. Finally, our model is compatible with the findings of recent studies exploring how an increased labor market concentration, whether due to superstar firms (Autor et al., 2020), mergers (Arnold, 2021; Prager and Schmitt, 2021) or increased fixed cost (de Loecker et al., 2020), decreases the labor share.====In addition, we study the effects of a wage floor, such as a union wage, in a professional labor market when firms command market power and workers are heterogeneous. In line with standard economics textbook intuition, we find that—when binding—a minimum wage unambiguously increases unemployment.==== More importantly, we introduce a potential additional effect of a wage floor that emerges in response to an endogenous delegation choice. A minimum wage hurts the big firm only if it hires the least able workers. In this case, delegation to multiple agents—resulting in higher wages in exchange for more able workers—becomes relatively more attractive. This pushes the big firm towards delegation, thereby improving the matching between firms and workers. As a result, aggregate output may increase despite the smaller number of matches—which is to say, despite higher unemployment—in the market. We then analyze the Stackelburg equilibrium of a hypothetical game in which the big firm can fully commit to a wage schedule before fringe firms post their wages. In terms of profits, this equilibrium provides an upper bound to what is achievable by strategic delegation.====Finally, we support the validity of our findings by presenting three extensions of our model. First, we show how the incentive to decentralize is affected by matching frictions and delegation costs. Second, we investigate the case in which each agent offers a single wage for all its jobs, motivated by the non-discrimination policy observed in many real-world entry-level job markets. We show that such a restriction does not alter the results presented in this paper. Third, we generalize the production function and show that the main intuition established throughout the paper does not rely on linearity.====With this paper, our primary goal is to introduce the effect of strategic delegation in the labor market to the literature. We argue that big firms face a strategic incentive to decentralize recruiting if the gain in workforce talent at moderately increased wages outweighs the ensuing loss of market power. In order for this incentive to be substantial, however, posting higher wages must lead to the acquisition of superior talent. That is, a real-world labor market has to sufficiently satisfy the following conditions for our model to apply: (a) There is variation in the industry-relevant skill of workers; (b) the marginal product of labor is sensitive to worker skill; and (c) the cost of delegation is limited. Notably, on the other hand, our model does not apply when firms compete for homogeneous (or sufficiently similar) workers, when skill does not affect productivity once it clears a certain threshold, or when workers' skill is not industry-specific.====In order to streamline the exposition, we simplify by drawing on impersonal wages only. There is no indication, however, that this simplification is crucial to the qualitative message of this paper. When a big firm with market power delegates hiring to independent agents who act in their best interest, self-induced competition for more desirable workers deters other firms from competing for these workers. As a result, we argue that the qualitative insights of our model pertain to labor markets in which some employers command market power and conditions (a) to (c) above are satisfied. While evidence suggests that strategic decentralization is a common phenomenon, specific examples that match our setup closely are specialized labor markets dominated by a few competing firms, e.g., the market for biomedical engineers (Johnson & Johnson, Roche, Pfizer, Novartis), business consultants (“MBB & The Big 4”), medical residents (highly concentrated subfields), nurses (local healthcare systems), teachers (public school districts) or criminal lawyers (the Department of Justice).====The remainder of Section 1 contextualizes our paper within the relevant literature. Section 2 introduces the model. Section 3 establishes general preliminary results. Section 4 presents our duopsony analysis pitting two big firms against each other. In Section 5 we focus on quasi-monopsony and build on our analysis to characterize the welfare effects of strategic decentralization as well as the introduction of a wage floor. We conclude the section by contrasting our equilibrium outcome with a hypothetical Stackelberg equilibrium. Section 6 extends the general model in three aspects. Finally, Section 7 concludes.==== This paper contributes to several streams of literature:====Market power is a widespread phenomenon in labor markets (Bhaskar et al., 2002; Ashenfelter et al., 2010). Specifically, monopsony power has generated a variety of theoretical contributions and surveys (Boal and Ransom, 1997; Bhaskar and To, 1999; Bhaskar et al., 2002; Manning, 2006; Ashenfelter et al., 2010), as well as studies of specific examples such as the markets for teachers (Landon and Baird, 1971; Ransom and Sims, 2010), nurses (Buerhaus and Staiger, 1996; Staiger et al., 2010), academics (Ransom, 1993) and even professional athletes (Kahn, 2000). This literature has mainly focused on social welfare measured via unemployment and the effects of labor market policies (Manning, 2006; Bhaskar and To, 1999). In the theoretical model presented by Galenianos et al. (2011), firms are interested in hiring a single worker, and market power is ensured by the finiteness of the market. In their model, high productivity firms face greater incentives to reduce wages, causing misallocation. We deviate from this literature by introducing heterogeneous workers, allowing us to study a trade-off between acquired worker skill and wages mediated by strategic delegation.====As introduced by the seminal work of Schelling (1960), a principal may prefer to have an agent acting in her stead, not despite but rather because the agent's objectives do not coincide with her own. The classic paper of Fershtman and Judd (1987), for example, provides an analysis of how to incentivize managers under competition. Models of strategic delegation to principals cover an array of different scenarios.==== Our model is the first to apply this idea to labor markets and to describe the delegation pattern, demonstrating that negative interdivisional spillover arises if one division competes for skilled workers against the others.====The concept of divisionalization in output markets is also closely related to our study. Salant et al. (1983) observe that when setting quantities, mergers (which are mirror image of divisionalization) lead to losses. Schwartz and Thompson (1986) and Veendorp (1991) show that splitting a firm into competing divisions may prevent competitors from entering the market. Polasky (1992) connects divisionalization to a firm's desire to be a Stackelberg leader. Finally, Baye et al. (1996a) and Baye et al. (1996b) analyze optimal splitting patterns in these games and show that corporate gains can exceed the sum of divisional losses from increased competition. A common denominator of these models is that, in quantity-setting games, merging firms (insiders) always lose while other firms (outsiders) gain. In addition, as in standard price setting games, outsiders always gain more than insiders from merging (Creane and Davidson, 2004). In other words, in these games firms would prefer to split indefinitely if the cost of doing so was infinitesimally small. This is not the case in our model. Due to the tension between market power and the strategic effect of delegation, a big firm taking on a competitive fringe optimally wants to delegate to a finite number of agents.",The strategic decentralization of recruiting,https://www.sciencedirect.com/science/article/pii/S0022053123000352,15 March 2023,2023,Research Article,23.0
"del Mercato Elena L.,Nguyen Van-Quy","Paris School of Economics – Université Paris 1 Panthéon-Sorbonne, Centre d'Economie de la Sorbonne, 106-112 Boulevard de l'Hôpital, 75647 Paris Cedex 13, France,Centre d'Economie de la Sorbonne, Université Paris 1 Panthéon-Sorbonne, 106-112 Boulevard de l'Hôpital, 75647 Paris Cedex 13, France","Received 9 July 2021, Revised 27 February 2023, Accepted 1 March 2023, Available online 8 March 2023, Version of Record 15 March 2023.",https://doi.org/10.1016/j.jet.2023.105637,Cited by (0),We consider a pure ,"The Second Welfare Theorem is one of the key results in economics. It is the basis of the normative approach to welfare studies in competitive markets. This theorem sets general conditions under which Pareto optima can be decentralized through competitive prices and transfers. Further, under differentiable assumptions, it is used to determine the market prices that implement Pareto optima as competitive equilibria.====In addition to standard technical assumptions, the Second Welfare Theorem relies on another crucial assumption, namely ‘self-interest’, that is individual preferences depend on own consumption only. In the absence of the latter, the Second Welfare Theorem may fail. However, externalities in preferences are important and they have been largely recognized in the literature.==== In the past few years, the normative approach to welfare analysis of competitive equilibrium in economies with externalities has attracted renewed attention, see Geanakoplos and Polemarchakis (2008), Dufwenberg et al. (2011), and Bourlès et al. (2017).==== Restoring the Second Welfare Theorem in the presence of externalities is an important issue and finds its place in this recent research stream.====We consider an exchange economy with consumption externalities in preferences. We use the notion of competitive equilibrium that combines Arrow–Debreu with Nash. That is, all individuals maximize their preferences by taking as given both the prices and the consumptions of the others and, at equilibrium, markets clear. This definition generalizes the classical notion without externalities. The decentralization of Pareto optima in economies with externalities can be obtained using other equilibrium notions à la Lindahl and personalized prices.==== As is well known, the decentralization through Lindahl equilibria is more problematic as it requires the opening of personalized markets for externalities. A simple assumption restores the Second Welfare Theorem, without calling for personalized prices and Lindahl equilibria. Such assumption demands ====, the set of Pareto optima, to be a subset of ====, the set of ==== Pareto optima. ==== is the set of feasible allocations that are Pareto optimal whenever individuals take the consumptions of the others as given, see Winter (1969) and Parks (1991). In this paper, we provide new assumptions that make the set inclusion ==== hold and new results for decentralizing Pareto optima via a competitive price.====The first assumption is Social Redistribution. The idea is that ==== Pareto improvements can be translated through redistributions into Pareto improvements. This assumption generalizes a condition proposed in Osana (1972), that we call Strong Redistribution.==== Theorem 1 is one of our key result. It states that, under classical assumptions on utilities, Social Redistribution is equivalent to the set inclusion ====. Importantly, Social Redistribution is weaker than all the other assumptions used in the literature for decentralization (i.e., Strong Redistribution, Non-malevolence, and Social Monotonicity).====We then extend our analysis to differentiable economies, because the differentiable approach is a powerful tool to characterize supporting prices through marginal utilities, see for instance Mas-Colell (1985), and Balasko (1988). We translate Social and Strong Redistribution into assumptions on first order effects of externalities, namely Directional Social Redistribution and Directional Strong Redistribution.==== The idea of these assumptions is that any direction for achieving an ==== Pareto improvement can be redistributed so that everybody is strictly better off. In differentiable economies, it is easier to check whether Directional Social (resp., Strong) Redistribution is satisfied or not, rather than checking for Social (resp., Strong) Redistribution. To the best of our knowledge, only the concept of Non-malevolence has been translated into Local Non-malevolence in a differential setting, see Parks (1991). Hence, the differentiable characterizations of Social and Strong Redistribution make a step forward in this line of research. Further, Directional Social (Strong) Redistribution is weaker than Local Non-malevolence. It is worth noticing that all our assumptions do not require preferences to be separable.====In differentiable economies, our key results study properties of equilibrium prices for decentralization. Under Social Redistribution, at a Pareto optimal allocation, all the gradients with respect to own consumptions are positively proportional. Therefore, the competitive supporting price is determined as in the case without externalities (see Theorem 2). This price reflects the individual marginal utilities, while the “social” price reflects a weighted average of the individual marginal utilities.==== These two prices have the same dimension, but they do not need to be equal (i.e., positively proportional). Theorem 3 states that, under Directional Strong Redistribution, the competitive supporting price is positively proportional to the social price. However, this property fails in a simple economy where the externality is generated by the presence of a public good (or “bad”).==== In such economies, Social Redistribution creates an interesting comparison between the social price and the competitive supporting price. We show that the (relative) social price of the public good (or “bad”) is the sum between the (relative) competitive supporting price and the (relative) marginal utilities (see Theorem 4). Theorem 5 shows that Local Non-malevolence is equivalent to have that the competitive supporting price is positively proportional to personalized prices for externalities. Furthermore, we illustrate several economic settings where the decentralization through a competitive price can be done using our assumptions and results, while it could not be done under the stronger assumptions adopted in literature, see Examples (a), (d), (e).(i) and (e).(ii).====Finally, we focus on the class of preferences of the Bergson-Samuelson form. This class, firstly introduced in Bergson (1938) and Samuelson (1947), has been largely studied in economic theory, especially in welfare economics. For these preferences, positive externalities entail the set inclusion ====. Example (e) shows that this is not the case for more general preferences. We further characterize Directional Strong Redistribution in terms of properties of the Jacobian matrix of Bergson-Samuelson utilities (see Theorem 6). This is relevant because the entries of this matrix deliver significant information on the nature of externalities. For instance, Directional Strong Redistribution is ensured if all individuals positively care about one individual or if the negative external effects are not too strong. These conditions can be easily applied to a larger class of preferences, that we call ====.====The paper is organized as follows. Section 2 presents the economy with consumption externalities, and the notions of competitive equilibria and Pareto optima. In Section 3, we provide the simple version of the Second Welfare Theorem, we introduce Social Redistribution, and we compare it with previous assumptions. Section 4 is devoted to a differentiable approach. We provide Directional Social (Strong) Redistribution assumptions, and we study the relationships with Social (Strong) Redistribution and Local Non-malevolence. In Section 5, we present all our results on the decentralized implementation of Pareto optimal allocations. Section 6 discusses the class of Bergson-Samuelson utility functions. All the results are proved in Section 8.",Sufficient conditions for a “simple” decentralization with consumption externalities,https://www.sciencedirect.com/science/article/pii/S0022053123000339,8 March 2023,2023,Research Article,24.0
"Bonnisseau Jean-Marc,del Mercato Elena L.,Siconolfi Paolo","Paris School of Economics – Université Paris 1 Panthéon-Sorbonne, Centre d'Economie de la Sorbonne, 106-112 Boulevard de l'Hôpital, 75647 Paris Cedex 13, France,Graduate School of Business, 665 W 130th St, New York, NY 10027, United States of America","Received 12 January 2022, Revised 27 February 2023, Accepted 1 March 2023, Available online 8 March 2023, Version of Record 16 March 2023.",https://doi.org/10.1016/j.jet.2023.105638,Cited by (0),We study the existence of quasi-equilibria and equilibria for pure ,"In the presence of consumption externalities, restoring the two Fundamental Theorems of Welfare Economics requires setting up markets where individuals face personalized Lindahl prices and choose allocations, that is, profiles of consumption bundles one for each of the individuals in the economy. This was observed a long time ago by Samuelson (1954), Arrow (1969), and Laffont (1976).====Surprisingly, the literature on the existence of competitive equilibria in economies with markets for externalities (hereafter Arrow-Lindahl equilibria) is very limited. Foley (1970) proves the existence of Arrow-Lindahl equilibria in economies with private and public goods, without consumption externalities and with monotonic preferences. Bergstrom (1976b) proves the existence of an equilibrium for so-called communal commodities and shows that it encompasses the case of markets for external effects through a technical construction.==== All existence results in the literature are based on specific fixed-points like arguments.====We study economies with consumption externalities in preferences, and find general conditions for the existence of quasi-equilibria and equilibria. This is not a trivial task. Even when preferences are locally non-satiated and individual endowments are strictly positive, quasi-equilibria and equilibria may fail to exist. We provide two examples that make this point. The existence failures of markets for externalities were hidden in the literature by technical assumptions playing the role of the survival and irreducibility assumptions.====We do not develop a direct argument, but rather follow the intuition of Arrow that in externality economies, “individual ===='s consumption is regarded as the production of joint outputs, one for each individual whose utility is affected by individual ===='s consumption” (Arrow, 1969, see pages 9-10). We show that a pure exchange economy with consumption externalities where trade takes place in Arrowian markets for externalities yields the same equilibrium allocations of an appropriately defined production economy with constant returns to scale and no externalities. Once this equivalence is established, existence of equilibrium, quasi-equilibrium, irreducibility and survival for economies with consumption externalities follow from well known and understood results for standard production economies.====The heavy lifting in our argument is done by the construction of the production economy and in particular of its technology. The technology transforms inputs of physical commodities into a collection of as many identical allocations as individuals in the economy. The total amount of physical commodities in an allocation cannot exceed the total amount of inputs used in the production process. The technology displays constant returns to scale, thereby generating zero equilibrium profits and making it irrelevant to specify individual property rights over production activities. Further, the chosen form of the technology restricts equilibrium prices to satisfy the classical Lindahl compatibility conditions. The latter establishes equivalence in equilibrium prices, and thus equilibrium allocations of the two economies.====We exploit the equivalence of feasible allocations to map classical irreducibility and survival conditions of the production economy into equivalent such conditions of the economy with externalities. In doing this, we understand the reasons for the failure of existence of quasi-equilibria and equilibria of the externality economy.====Without free disposal in the externality economy, the equivalent production economy does not satisfy the standard weak survival assumption, explaining the lack of quasi-equilibria in our first example. In the second example there is free disposal, and thus quasi-equilibria exist, but equilibria do not because the equivalent production economy is not irreducible. Indeed, irreducibility for externality economies is more demanding. In economies without externalities, irreducibility means that at any feasible allocation, any group of individuals can be made better off (in the Pareto sense) by adding to their consumption bundles something of the privately owned resources of the complementary group. For economies with externalities, any group must choose an allocation that specifies not only their consumption bundles, but also those of the individuals in the complementary group. This makes it harder to find an allocation (Pareto) dominating the target feasible allocation for all the members of the chosen group.====The paper is organized as follows. Section 2 describes the fundamentals, the basic assumptions and the structure of the markets. Section 3 provides examples of nonexistence of quasi-equilibrium and equilibrium in economies satisfying the assumptions of Section 2. Section 4 is the key part of the paper describing the equivalent production economy without externalities. Section 5 shows arguments for the existence of a quasi-equilibrium and equilibrium and it defines notions of irreducibility and survival for the economies with externalities.",Existence of an equilibrium in arrowian markets for consumption externalities,https://www.sciencedirect.com/science/article/pii/S0022053123000340,8 March 2023,2023,Research Article,25.0
"Chen Yi-Chun,Kunimoto Takashi,Sun Yifei","Department of Economics and Risk Management Institute, National University of Singapore, Singapore,School of Economics, Singapore Management University, Singapore,School of International Trade and Economics, University of International Business and Economics, China","Received 10 October 2019, Revised 26 November 2022, Accepted 12 February 2023, Available online 5 March 2023, Version of Record 13 March 2023.",https://doi.org/10.1016/j.jet.2023.105624,Cited by (0),"The literature on robust mechanism design assumes players' knowledge about a fixed payoff environment and investigates global robustness of optimal mechanisms to large changes in the information structure. Acknowledging global robustness as a demanding requirement, we propose continuous implementation as a notion of local robustness. Keeping the assumption of payoff knowledge, we say that an SCF is continuously implementable if there exists a mechanism which yields the outcome close to the desired one for all types close to the planner's initial model. We show that when a generic correlation condition is imposed on the class of interdependent-value environments, any (interim) incentive compatible SCF is continuously implementable with arbitrarily small transfers imposed on and off the equilibrium. This stands in stark contrast to ====, who show that their global robustness amounts to ex post incentive compatibility, as well as to ====, who show that continuous implementation without payoff knowledge generates a substantial restriction on the SCF, tightly connected to full implementation in rationalizable strategies.","While Bayesian mechanism design has been successful in generating many applications, it is rightly criticized for its sensitivity to the precise information that the agents and the planner have about the environment. To properly describe an incomplete information environment, an agent's private information is summarized by the notion of ====. For an agent, a type specifies (i) his private information about his own preferences (====); (ii) his belief about the payoff types of others (====); (iii) his belief about others' first-order beliefs (====), and so on, leading to an infinite hierarchy of beliefs. The set of all coherent belief hierarchies described above is called the ====.==== Bayesian mechanism design theory typically works with a type space which is smaller than the universal type space and incorporates certain common knowledge among the agents, e.g., their beliefs are derived from an independent common prior. While the common knowledge assumptions often enhance the tractability of the model, they are at best an idealization of the reality.====The literature of ==== examines the restrictiveness of this kind of common knowledge assumption by fixing a payoff environment. The payoff environment specifies a set of outcomes, a set of payoff types, as well as utility functions for each agent, but makes no assumptions about all possible type spaces (including both belief types and payoff types) constructed from the fixed payoff environment. We call this approach ====, which is pursued by Bergemann and Morris (2005). Bergemann and Morris (2005) define the planner's objective as a ==== (henceforth, SCC) that maps payoff type profiles into a nonempty subset of outcomes, and say that an SCC is ==== on a type space if there exist a mechanism and ==== Bayes Nash equilibrium of that mechanism which yields the outcome in the set specified by the SCC for every payoff type profile.==== Thus, this paper treats the problem of mechanism design and that of implementation interchangeably. In what they call ====, Bergemann and Morris (2005) show that an SCC is interim implementable over all type spaces if and only if it is ==== implementable; by the revelation principle, the existence of an ex post implementable SCC is equivalent to the existence of an ex post incentive compatible SCF contained in the SCC.==== However, this equivalence result carries negative news for robust mechanism design because Jehiel et al. (2006) have shown that only ==== SCFs are ex post incentive compatible when payoff types are multidimensional and interdependent value functions are generic.====To seek positive results, we propose a notion of ==== robustness which weakens the notion of global robustness. Formally, we fix a benchmark type space associated with a given payoff environment and consider an SCF that maps payoff and belief type profiles into outcomes. Our notion of locally robust implementation adapts the notion of ==== implementation of Oury and Tercieux (2012) (henceforth, OT) to the setup of Bergemann and Morris (2005) in which the players always know their own payoff type. We say that an SCF is continuously implementable by a mechanism if there exists a (possibly mixed-strategy) equilibrium of the mechanism which yields the outcome close to the desired one for all types close to the planner's benchmark model. Following OT, we consider the closeness of types in terms of the product topology of weak convergence of infinite belief hierarchies in the universal type space. We also verify that any ex post incentive compatible SCF is indeed continuously implementable; hence, locally robust implementation in our sense is weaker than globally robust implementation in the sense of Bergemann and Morris (2005).====To establish our main result, we further assume that the agents' utility functions are quasilinear with respect to monetary transfers. We say that an SCF is continuously implementable ==== if it is continuously implementable by a mechanism in which arbitrarily small transfers are added both on and off the equilibrium. Our main result (Theorem 1) shows that when a generic correlation condition, which we call Assumption 1, is imposed on the class of interdependent values environments, an SCF is continuously implementable with small transfers if and only if it is (interim) incentive compatible on the benchmark type space. Since interim incentive compatibility is a necessary condition for interim implementation, our continuous implementation result is as permissive as it can be.====To achieve continuous implementation, we establish instead full implementation of any incentive compatible SCF under a permissive solution concept denoted as ====, which is the set of message profiles surviving the iterative elimination of weakly dominated messages followed by the iterative elimination of interim strictly dominated messages.====We next expand on the implication of Assumption 1. Assumption 1 is stronger than the BDP property proposed by Neeman (2004). A type space satisfies the BDP property if different types of any player have different beliefs. In contrast, Assumption 1 requires that different types of any player have different beliefs over ==== payoff types in the sense of Proposition 2 in Bergemann and Morris (2009b). In private-value environments where a player's different types have different preferences, Assumption 1 is equivalent to imposing the BDP property on the benchmark model. Since we are aiming for a permissive result that would allow for the locally robust implementation of ==== interim incentive compatible SCF, it shall come as no surprise that our results leverage conditions on the benchmark type space such as Assumption 1 or the BDP property. This differentiates our exercise from the global robust implementation exercise due to Bergemann and Morris (2005), which aims to relax all common knowledge assumptions, including the BDP property.====The rest of the paper is organized as follows. Section 2 positions our contribution in the broader context of the literature and relegates to Section 4.4 detailed comparisons with related papers. In Section 3, we introduce (i) the general setup for the paper, (ii) the notion of continuous implementation with small transfers, (iii) the notions of strategic distinguishability and the maximally revealing mechanism, and (iv) the generic correlation condition used in this paper (Assumption 1). In Section 4.1, we state the main result of this paper (Theorem 1) and discuss two special cases: the case with a complete-information benchmark model and the case with private-value environments. Section 4.2 describes how our main result is proved in a heuristic manner. In Sections 4.3, 4.3.1, and 4.3.2, we prepare all the machinery needed for the proof of our main result. Section 6 concludes the paper. In the Appendix, we provide all the proofs omitted from the main body of the paper.",Continuous implementation with payoff knowledge,https://www.sciencedirect.com/science/article/pii/S0022053123000200,5 March 2023,2023,Research Article,26.0
"Tang Rui,Zhang Mu","Department of Economics, Hong Kong University of Science and Technology, Hong Kong SAR,Department of Economics, University of Michigan, United States of America","Received 1 May 2021, Revised 7 January 2023, Accepted 15 February 2023, Available online 23 February 2023, Version of Record 2 March 2023.",https://doi.org/10.1016/j.jet.2023.105636,Cited by (0),"We study a decision maker's ex-ante choices over menus. The decision maker has in mind a set of possible future preferences that can be justified, for instance, by her past behavior, and she naively evaluates each menu according to the best option in the menu among those that can be rationalized by her future preferences. We provide a characterization for this menu preference, discuss the uniqueness of its representation, and propose a comparative measure of the decision maker's naivete. We apply our model to study two behavioral biases: naivete about present bias and the disjunction effect.","Understanding dynamic choices of a decision maker (DM) plays an important role in the analysis of various economic problems. Evidence from a large literature suggests that DMs are typically naive in predicting their future choices (Larwood and Whittaker, 1977). This leads to contingent plans that are usually overturned. For instance, health-club members often choose to sign monthly or annual contracts with the gym instead of paying for per-visit passes, which turns out to cost them more than $600 on average during their membership (DellaVigna and Malmendier, 2006); subjects may overestimate their effort input initially and tend to work less than planned in real effort tasks (Augenblick et al., 2015; Augenblick and Rabin, 2019; Fedyk, 2018).====Although DMs may have naive beliefs about their future choices, which are typically biased towards their current preferences, they do not simply believe what they want to believe. For instance, individuals who never exercise may not anticipate their future selves to work out frequently although they want their future selves to do so; drunkards may not believe that they will quit drinking immediately. Indeed, even for a naive DM, she may not justify some future choices if she finds no clue for making these choices based on her past behavior. To capture this idea, we consider a DM who has ====. The DM is naive in the sense that she evaluates each menu according to the best choice in the menu that can be rationalized by her future preferences. Such naivete is based on motivated reasoning since the set of future preferences the DM deems possible is constrained by and inferred from her past behavior.====To illustrate our model, consider a food hunter who is choosing between two restaurants at which she needs to make a reservation for tomorrow's dinner. Restaurant 1 offers fried chicken and ramen noodles, and Restaurant 2 offers sandwiches and salads. The food hunter wants to eat healthily and has a normative ranking over the dishes: Salads are ranked the first, ramen noodles the second, sandwiches the third, and fried chicken the last. However, she is aware that her preference tomorrow may not be aligned with her normative ranking, and infers her future preferences based on her past choices: She sometimes chose fried chicken and sometimes chose ramen noodles in Restaurant 1, depending on her mood or which friend was joining for dinner, but she always chose sandwiches in Restaurant 2. She naively believes that she will choose the healthiest food in each restaurant among those she has chosen in the past, i.e., ramen noodles in Restaurant 1 and sandwiches in Restaurant 2. Thus she decides to make a reservation at Restaurant 1.====The above example highlights several key features of our model. First, the DM is naive as her actual future choice may differ from her anticipated one. The food hunter chooses Restaurant 1 over 2 since she believes her future self to order ramen noodles. However, she may end up choosing fried chicken as what she did in the past, which turns out to be less healthy than sandwiches. Second, the DM adopts motivated reasoning to justify her beliefs based on her past choices. In this example, the food hunter does not consider salads as a possible future choice since she never ordered salads in Restaurant 2 previously. Such belief can also be revealed from the choice of the DM: Since each dish offered by Restaurant 1 is worse than salads, a necessary condition for the DM to choose Restaurant 1 over 2 is that the choice of salads is not justified and is irrelevant for the evaluation of Restaurant 2. Finally, our DM has a preference for commitment. To see this, consider two smaller restaurants with one only offering salads and the other only offering sandwiches. The DM can be better off by making her reservation at the restaurant that only offers salads.====We formally present our model in Section 2. Let ==== be a finite outcome space. A preference of the DM is represented by a von Neumann-Morgenstern expected utility function over the set of lotteries on ====. A menu is a non-empty and compact subset of lotteries. Following Kreps (1979), the primitive of our model is a preference ≿ over menus.==== A DM has a current (or normative) preference ==== and has in mind a compact set of possible future preferences ==== that governs her future choices. We interpret the future preferences as those inferred by the DM based on her past choices. For a given menu ====, the set of rationalizable future choices is given by==== and the DM evaluates the menu ==== according to the best option in ====, which she naively anticipates her future self to choose. Thus the DM prefers menu ==== to menu ==== if and only if ====. The induced menu preference, represented by the tuple ====, is called a ==== (====).====Theorem 1 characterizes MPMNs with seven axioms, among which four are the same as or natural weakening of standard axioms in the literature on menu preferences: (1) The menu preference is non-trivial, complete and transitive. (2) The DM's preference over two menus is not reversed if they are mixed with the same lottery using the same mixture ratio. (3) Allowing randomization over choices in any given menu does not change the attractiveness of that menu. (4) The menu preference satisfies certain continuity conditions.====The remaining three axioms are closely related to the DM's motivated naivete. First, consider an option in a given menu that is strictly better than the menu. The option cannot be rationalized by any possible future preference, since otherwise, the naive DM should consider this option to be a potential future choice and evaluates the menu to be at least as good as it. Hence, options that are strictly preferred to a given menu are irrelevant, and deleting them will not change the attractiveness of the menu. This is the axiom of ====.====Second, consider menus ====, ====, and ====. Suppose that the DM naively believes her future self to choose option ==== from menu ==== and thus evaluates ==== according to ====. Since ==== is deemed a possible future choice in ====, it is also deemed a possible future choice in either ==== or ====. The naive DM then considers either ==== or ==== to be weakly better than ====. That is, the DM cannot strictly prefer ==== to both ==== and ====. This is the axiom of ====.====Finally, our axiom of ==== is stated exactly the same as the axiom of ==== introduced by Ergin and Sarver (2010) (henceforth ES10). The axiom connects the DM's naivete with her preference for an early resolution of uncertainty by stating that the mixture of any two menus cannot be strictly better than both of them. Together, the seven axioms are sufficient and necessary for a menu preference to be an MPMN.====We present the uniqueness result in Theorem 2 and comparative statics of our model in Theorem 3. We adopt the notion “====-alignment” following Dekel and Lipman (2012) (henceforth DL12) and Ahn et al. (2019). For three preferences ====, ====, and ====, the preference ==== is ==== than ==== if for any menu ==== and any option ==== that is optimal in ==== under ====, there exists an option ==== that is optimal in ==== under ==== such that ====. Hence, for a naive DM whose current preference is ==== and whose future preference set contains both ==== and ====, she behaves as if she ignores ==== since ==== justifies better choices than ==== does. The notion of ====-alignment generates a pre-order ==== over the set of preferences. Our uniqueness result says that if ==== and ==== represent the same MPMN, then ==== and ==== denote the same preference, and the sets of ====-undominated preferences in ==== and ==== agree. For the comparative statics, we consider two DMs whose preferences over menus are MPMNs. DM1 is said to be ==== than DM2 if whenever DM2 prefers a menu to a singleton menu, so does DM1. We show that DM1 is more naive than DM2 if and only if they share the same current preference ====, and the set of future preferences of DM1 is more aligned with ==== than that of DM2.====We apply our model to investigate two behavioral biases in Section 6. First, we demonstrate how our model accommodates the naive quasi-hyperbolic discounting model introduced by O'Donoghue and Rabin, 1999, O'Donoghue and Rabin, 2001. The model features a DM who exhibits present bias: She discounts future utility by ==== and has an additional present bias parameter ==== capturing her taste for immediate gratification. The DM exhibits naivete when she underestimates her actual future present bias. To see how this fits our model, consider a three-period model with a DM uncertain about her future preferences. According to her period-1 preference, the discount rate between periods 2 and 3 is ====. She also knows that her present bias parameter ==== in period 2 lies in the interval ====, where each ==== corresponds to a discount rate of ==== between periods 2 and 3. Our model predicts that the DM behaves as if she anticipates her period-2 present bias parameter to be ==== since ==== is the closest discount rate to ==== and corresponds to the most aligned period-2 preference with her period-1 preference. Therefore, when the DM's realized period-2 present bias parameter is ====, she has naive quasi-hyperbolic discounting. We also show that our model allows choice patterns inconsistent with predictions of the naive quasi-hyperbolic discounting model.====Second, we show that our model generates the disjunction effect in choices over menus. In Tversky and Shafir (1992), the disjunction effect is viewed as a violation of Savage's Sure-Thing Principle, where the DM prefers ==== to ==== conditional on knowing that event ==== occurs or that event ==== does not occur, but reverses her preference if she does not know whether ==== occurs or not. Croson (1999) and Hristova and Grinberg (2008) show that the disjunction effect exists in Prisoner's Dilemma games. In Section 6.2, we use a simple example to illustrate that a DM whose menu preference is an MPMN may also exhibit such choice patterns over menus.==== Our paper contributes to the literature on modeling naivete in dynamic choices (O'Donoghue and Rabin, 1999, O'Donoghue and Rabin, 2001; Ahn et al., 2020). A recent paper that closely relates to ours is Ahn et al. (2019). They provide a behavioral characterization of naivete that features a DM who prefers a menu to her ex-post choices in that menu. The naivete of our DM is in line with this characterization: She prefers the ex-ante menu to any rationalizable future choice in the menu.====Our paper adds to the literature on choices over menus with self-conflicting preferences, among which the most related papers are Strotz (1955) and DL12. Strotz (1955) considers a DM who anticipates her future self to have a single preference. DL12 and our paper extend Strotz (1955) towards different directions by considering multiple future preferences: DL12 study the random Strotz model and demonstrate how it relates to the costly self-control model introduced by Gul and Pesendorfer (2001) (henceforth GP01); our model takes a non-probabilistic approach and captures the naivete of the DM. As we will show, our model intersects the model of DL12 at exactly the Strotz model.====Another closely related stream of literature studies DMs who can affect their future choices in menus they choose today, including Chandrasekher (2018) (henceforth C18), Koida (2018) (henceforth K18), Mihm and Ozbek (2018) (henceforth MO18), and Kopylov and Yang (2021) (henceforth KY21). Although the DM in our model cannot affect her future choices, our DM has similar choice behavior over menus as DMs in their models.====C18 considers a finite outcome space and studies the planner-doer model introduced by Thaler and Shefrin (1981), where the planner (current self) can restrict the feasible set of the doer (future self) in each menu using informal commitments. Our model of menu preferences coincides with that of C18 when the space of alternatives is finite (as formally shown in Appendix B).====K18 proposes a model of anticipated stochastic choice (ASC) in which the DM can exert cognitive control over her mental states to affect her future choices. Each mental state corresponds to a choice function that specifies a choice made by her future self in each menu. Unlike in our paper, K18 considers an extended choice domain where the DM's preference is defined over ====. For each random menu, the DM chooses from a set of distributions over her mental states to maximize her ex-ante expected payoff, where the set of feasible distributions is determined by the maximal cardinality of the menus in the support of the random menu. We note that our model is a special case of the ASC model when restricting the DM's preference to non-random menus with finite choices: An MPMN with current preference ==== admits an ASC representation in which the DM has current preference ====, and all her mental states correspond to the same choice function that selects the best rationalizable future choice in each menu.====MO18 consider a DM who has a normative preference but suffers from internal conflicts, as her actual future choices are mood-driven. Each mood is represented by a distribution over future preferences, and the DM can costly regulate her future mood. Such a DM ranks menus as if she exerts an optimal level of self-regulation before making a choice from each menu. When each mood of the DM is a degenerate distribution over future preferences and the cost of regulation for each mood is either zero or positive infinity, the model of MO18 reduces to ours.====In a concurrent paper, KY21 study a model wherein a planner can select or persuade doers to make choices from menus. The planner evaluates each menu via the best delegable alternative that can be chosen in the menu by some doer. Although their representation is the same as ours, the two models differ in motivations and interpretations. KY21's model can be best applied, for instance, to settings where the planner can affect the doer's actions via persuasions, while our model aims to capture motivated naivete in dynamic decisions. Also, KY21 work with finite menus and a different set of axioms, which facilitate their adaptation and extension of Aizerman and Malishevski (1981)'s results on multi-utility representations.====We highlight several additional contributions of our paper compared with the above four papers. First, compared with C18, our model concerns menus of lotteries rather than menus of discrete choices. This enables us to apply the model to study how uncertainty affects DMs' choice behavior (e.g., our application in Section 6.2). The richer choice domain we consider also allows clearer identification results and comparative statics. Second, our paper provides a new interpretation of the DM's ex-ante choices over menus based on motivated naivete, which leads to new applications. In particular, our interpretation can accommodate inconsistent behavior of the DM, since the normatively best possible future choice that drives her to choose a menu may not be actually chosen by her future self. We elaborate this point with more details in Section 6.1. Third, the DM's ex-post choice behavior in our model is different from that in those papers, since our DM cannot affect her future preference. We provide a more detailed discussion in Section 5. Fourth, our axiomatization exercise portrays a coherent picture in understanding the connection between the general model of MO18 and our special one. In Section 7.2, we show that the two behavioral axioms, Independence of Irrelevant Choices and Positive Set Betweenness, are sufficient and necessary for an important subclass of MO18's model to reduce to ours through a direct proof (Proposition 2). Last but not least, in the proof of our main theorem, we provide a direct approach to identify the DM's largest set of future preferences. Our approach differs from the ones adopted in the four papers mentioned above.====Our paper also relates to the literature on choices with multiple rationales (Kalai et al., 2002). For instance, Cherepanov et al. (2013) consider a DM who has a preference over alternatives and multiple rationales. The DM chooses from each menu the alternative that is the best under her preference among those that can be rationalized by some rationale. Ridout (2021) further studies a special case of the model of Cherepanov et al. (2013) with stronger identification properties and extends it to a random version. Similarly, our DM justifies her choice of a menu using her possible future preferences. Our model differs from theirs in two aspects. First, they study the DM's choices ==== each menu, while we focus on the DM's ex-ante choices ==== menus. Second, our model has different applications from theirs. Their models can be applied to accommodate choice biases in a given menu (e.g., violations of the Weak Axiom of Revealed Preference), while our model is applied to study choices over menus.====Motivated naivete in our model is reminiscent of the literature on optimism (Guarino and Ziegler, 2022) and wishful thinking (Yildiz, 2007) in games. For example, Guarino and Ziegler (2022) show that Point Rationalizability of Bernheim (1984) is equivalent to a new solution concept called Optimistic Rationalizability in simultaneous move games, where optimism is defined relative to the strategic uncertainty concerning the co-player's actions. One game-theoretical interpretation of our model is a leader-follower delegation game where the leader does not know the preferences of the follower. The strategic analysis of such games is simple as it follows the backward induction principle. Our main focus in this paper concerns how to model the leader's belief about the follower's preferences.====The remaining part of the paper is organized as follows. We introduce our model in Section 2 and characterize it in Section 3. In Section 4, we study the uniqueness of our model and comparative statics. We discuss the DM's ex-post choices in Section 5 and our applications in Section 6. In Section 7, we discuss the connection between our model and other existing models of menu preferences. All omitted proofs are in Appendix A, and the comparison between our model and that of C18 is presented in Appendix B.",Motivated naivete,https://www.sciencedirect.com/science/article/pii/S0022053123000327,23 February 2023,2023,Research Article,27.0
"Artige Lionel,Cavenaile Laurent","University of Liège, Belgium,University of Toronto, Canada","Received 27 January 2021, Revised 2 February 2023, Accepted 5 February 2023, Available online 13 February 2023, Version of Record 22 February 2023.",https://doi.org/10.1016/j.jet.2023.105622,Cited by (1)," with ====. Finally, the existence of such a trade-off depends on how public education spending is financed.","Public education is one of the largest items of public spending worldwide. Governments in high-income countries spent an average of 4.9% of GDP on all levels of education in 2017 (UNESCO UIS database).==== In primary and secondary education, student enrollment rate in public institutions in 2013 exceeded 80% on average across OECD countries (92% in the U.S.). In tertiary education, on average, 69% of students (72% in the U.S.) were enrolled in public institutions in 2013 (OECD (2015)).====In theory, there are good economic reasons for governments to invest in public education. Public education can be an instrument to support economic growth and correct market inefficiencies arising from human capital externalities (Lucas (1988), Azariadis and Drazen (1990) and Romer (1990)) or credit market imperfections (Galor and Zeira (1993)). It may also reduce inequality (Glomm and Ravikumar (1992), Saint-Paul and Verdier (1993), Eckstein and Zilcha (1994) and Zhang (1996)).====These predictions, however, lack strong empirical support. Empirical work on the link between public education spending and economic growth is rather scarce and inconclusive (see for instance Levine and Renelt (1992), Easterly and Rebelo (1993), Sylwester (2000) and Blankenau et al. (2007)). Empirical results on the relationship between public education spending and income inequality are no more conclusive. For instance, Keller (2010) finds a negative association between the level of public education expenditures and income inequality while Sylwester (2002) finds no statistically significant correlations and Braun (1988) and Barro (2000) even observe a positive relationship. These results suggest that, if there is a connection between public education expenditures, growth, and inequality, it is more nuanced than assumed in existing theoretical frameworks. In particular, this relationship should be modulated by other economic conditions.====In this paper, we take the stance that a relevant modulating factor between public education expenditures, economic growth and inequality is the human capital of agents who choose to become teachers. In particular, we argue that the effects of public education expenditures depend on the relative average human capital of teachers, which is the result of occupational decisions along the human capital distribution. We show that this affects the relationship between public education expenditures, growth and inequality in several ways that are relevant to the design of education policies. The importance of teacher quality for student achievement and individual returns to education has been largely documented in the literature (see, among many others, Rockoff (2004), Rivkin et al. (2005) and Hanushek et al. (2019) for evidence of the central role of teacher quality on student performance). In an extensive review of the literature, Glewwe et al. (2011) find that teacher education and knowledge positively contribute to student learning. Hanushek (2011) finds that better teacher quality is associated with higher student earnings. Card and Krueger (1992) show that the return to education is positively associated with education quality (proxied by teacher salary) and teacher education level while DeCicca and Krashinsky (2020) find that the return (in terms of earnings) to education policies crucially depends on teacher quality measured by teacher relative salary. Figlio (1997), Loeb and Page (2000), Figlio and Kenny (2007), Dolton and Marcenaro-Gutierrez (2011), Hendricks (2014) and Britton and Propper (2016) provide further evidence for a positive relationship between teacher pay, education quality and student performance.====We build an overlapping-generations general equilibrium growth model which crucially departs from the existing literature by featuring occupational choice and endogenous education quality. In the model, agents can choose between three occupations: worker, teacher and manager. Managers' span of control determines their demand for workers and leads to a wage function that is convex in human capital for managers. An increase in the relative wage of teachers, financed by higher public education expenditures, raises the number of teachers and education quality. This, in turn, accelerates human capital accumulation and economic growth. We provide several suggestive empirical facts that relate to these predictions of our model. In particular, we show that higher public education expenditures are associated with higher teacher salaries, a larger teacher employment share and a higher average level of teacher education.====Our model produces new predictions regarding public education expenditures and growth: economies with fatter right tails of their human capital distribution attract better teachers for a given level of public education spending and have a higher elasticity of growth to public education expenditures. In other words, the effectiveness of public education policies at raising income growth depends critically on the distribution of human capital in the economy and might be higher in economies with higher inequality ====. From a theoretical point of view, our model also allows for new growth decompositions i.e. decomposing worker income growth into human capital and wage rate per unit of human capital growth as well as decomposition aggregate growth into a measure of aggregate manager human capital and worker human capital growth.====Our model also sheds light on the link between public education expenditures and income inequality. In particular, it shows that these two variables are linked via two main channels that relate to occupational choice and the shape of the human capital distribution, as a change in public education affects both the bottom and top of the income distribution differently. First, raising public education expenditures alters relative wages across occupations, the human capital distribution and ultimately occupational decision. At the bottom of the distribution, higher public education investment affects labor supply. As the supply of human capital by workers decreases, their wage rate goes up. As this increases the cost of labor for managers, this reduces profits and wages at the top of the income distribution. These two forces tend to decrease income inequality. Second, higher public education expenditures also affect the top of the income distribution more directly. Managers benefit the most from an increase in their human capital as their wage function is convex. Profits and manager wages become more concentrated at the very top of the income distribution, which tends to increase income inequality. Overall, whether public education decreases income inequality depends on which of these two forces dominates. We show that, in theory, both effects can dominate depending on parameter values. These results have policy implications. In particular, they show that economies could face a trade-off between income inequality and economic growth through public education.====We calibrate our model and find that a significant share of US states faces such a trade-off. In addition, we show that whether an increase in public education spending raises income inequality crucially depends on the way increased spending is financed. An increase in public education spending financed by an increase in tax progressivity, rather than by an overall increase in the tax level, is more likely to lead to an increase in (before-tax) income inequality. Our results also suggest that states with higher public education expenditures, higher teacher employment share and relative wage and higher intergenerational mobility are overall more likely to face a tradeoff between growth and income inequality through public education ====. Finally, our calibrations reveal that raising public education spending through an overall increase in the level of the tax achieves better results than through increased tax progressivity in terms of both increased growth and smaller increase (or larger decrease) in income inequality.==== Robust evidence shows that investment in education has positive effects on individual earnings.==== Empirical evidence also tends to demonstrate that education attainment is positively correlated with aggregate income growth.====Our theoretical model is related to the literature on public education, economic growth and inequality. Glomm and Ravikumar (1992), Saint-Paul and Verdier (1993), Eckstein and Zilcha (1994), Zhang (1996), Glomm and Ravikumar (2003) and Blankenau et al. (2007) among others show that public education should raise long-run economic growth and lower income inequality.==== Our model departs from this literature by allowing for a non-degenerate distribution of human capital in a balanced growth path and by modeling occupational choice. Agents in our model decide whether to become teachers, which endogenously determines the quality of education. This implies that both public education expenditures and the distribution of human capital in the economy matter for the quality of education. In turn, the quality of education affects the shape of the human capital distribution, growth and inequality. These two features of our model (occupational choice and endogenous teacher quality) imply that the direction of relationship between public education and inequality in the long run is ==== ambiguous.====The notion that education quality matters for economic outcomes, in particular economic growth, has a long tradition. Hanushek and Kimko (2000), Hanushek and Woessmann (2012) and Hanushek and Woessmann (2015) show that differences in the quality of education can explain variations in economic growth rates across countries. This suggests that factors affecting the quality of education (and not only years of schooling) should be taken into consideration when analyzing the role of education on economic growth. Manuelli and Seshadri (2014) show that a large share of TFP differences across countries can actually be explained by differences in human capital when agents can choose both the number of years of schooling but also the amount of human capital acquired per year of schooling. Schoellman (2012) also concludes that education quality differences can explain a significant share of cross-country income differences using data on foreign-educated immigrants. We add to this literature by considering the supply side of education in a general equilibrium model of growth and occupational choice. To the best of our knowledge, this is the first paper that endogenizes the supply side of education in a model of endogenous growth.==== In particular, we show that the outcome of public education policies crucially depends on the occupational choice response of agents (in particular teachers).====The results of our paper also have policy implications. First, they highlight that the return in terms of economic growth from education expenditures depends on the human capital distribution. Second, the increase in income inequality since the late 1970s has recently attracted a lot of attention both in academic and policy circles.==== The role of several public policies in shaping the evolution of income inequality has been investigated in the literature.==== Our results complement this literature by showing that public education expenditures do not necessarily reduce inequality. Our joint results regarding growth and inequality have important implications for the optimal design of public policies and, in particular, regarding the factors that are relevant to the success of such policies.====We also relate to the empirical literature on public education, growth and inequality. Empirical work on the link between public education spending and economic growth is rather scarce and inconclusive (see for instance Levine and Renelt (1992), Easterly and Rebelo (1993), Sylwester (2000) and Blankenau et al. (2007)). The empirical literature on public education and income inequality also finds mixed results. For instance, Keller (2010) finds a negative association between the level of public education expenditures and income inequality, Sylwester (2002) finds no statistically significant correlation and Braun (1988) and Barro (2000) even observe a positive relationship.==== We contribute to this literature by showing that the relationship between public education expenditures, growth and income inequality can be non-linear and potentially ambiguous.====The remainder of the paper is organized as follows. Section 2 introduces a static version of the model with occupational choice between three occupations (worker, teacher and manager). Section 3 embeds this static occupational choice in a dynamic general equilibrium model of endogenous growth. It also derives the theoretical properties of our model in a balanced growth path and shows how both economic growth and income inequality are affected by changes in public education expenditures. Section 4 presents some suggestive empirical evidence about the relationship between public education expenditures, teacher employment share, wage and quality, which are in line with the predictions of our model. Section 5 presents calibrations of our model to US states. Section 6 concludes.","Public education expenditures, growth and income inequality",https://www.sciencedirect.com/science/article/pii/S0022053123000182,13 February 2023,2023,Research Article,28.0
"Garrett Daniel F.,Georgiadis George,Smolin Alex,Szentes Balázs","University of Essex, Wivenhoe Park, Colchester CO4 3SQ, UK,Kellogg School of Management, Northwestern University, Evanston, IL 60208, USA,Toulouse School of Economics, University of Toulouse Capitole, 31080 Toulouse Cedex 06, France,Department of Economics, London School of Economics, London, WC2A 2AE, UK","Received 11 May 2022, Revised 30 January 2023, Accepted 4 February 2023, Available online 10 February 2023, Version of Record 22 February 2023.",https://doi.org/10.1016/j.jet.2023.105621,Cited by (1),"This paper considers a moral hazard model with agent limited liability. Prior to interacting with the principal, the agent designs the production technology, which is a specification of his cost of generating each output distribution. After observing the production technology, the principal offers a payment scheme and then the agent chooses a distribution over outputs. We show that there is an optimal design involving only binary distributions (i.e., the cost of any other distribution is prohibitively high), and we characterize the equilibrium technology defined on the binary distributions. Notably, the equilibrium payoff of both players is ====.","A central result in contract theory is that agency rents are a key source of economic welfare. When analyzing environments with asymmetric information, most microeconomic models take the determinants of these agency frictions as given. In hidden-information models, for example, the distribution of types, which determines information rents, is typically treated as exogenous. Similarly, in principal-agent problems with hidden actions, the production technology available to the agent, which governs the principal's cost of implementing various actions, is usually part of the model description. However, if an agent's payoff depends on agency frictions, then he is likely to pursue generating these frictions in a way that enhances his payoff. The goal of this paper is to reconsider the standard limited-liability moral hazard problem and understand how an agent might maximize rents by optimally designing the production technology.====For a potential application where such a problem may arise, consider an entrepreneur who is starting a business, and will eventually need venture capital backing to grow it. Prior to contracting with venture capitalists, he must make a host of choices pertaining to the product, the business model, the product market strategy, and so on.==== If the venture capitalist has strong bargaining power, the entrepreneur benefits from making choices that exacerbate the moral hazard problem to increase agency rents. Even if there were more profitable alternatives, they may not be considered in the contractual negotiations if the venture capitalist is unaware of them.====Identifying the agent-optimal technology turns out to be useful even in those environments where the agent has no meaningful way of influencing the production technology and, from his viewpoint, it is given exogenously. Indeed, we are able to express predictions regarding surplus sharing in principal-agent models with limited liability that are robust to the set of available technologies. Specifically, we characterize the entire set of payoff combinations in such models which can arise for ==== production technology.==== The agent-optimal technology corresponds to an extreme point in this set at which the agent's payoff is maximal. This exercise is similar in spirit to that of Bergemann et al. (2015), who characterize the set of consumer and seller payoffs in a model of third-degree price discrimination for some information of the seller.====In the baseline setup, we consider a risk-neutral agent who can choose a production technology (or “project”) before interacting with a principal.==== A production technology specifies the agent's cost of each output distribution with support contained in ====. That is, the only restriction on the available projects is that output is uniformly bounded. Such a bound may represent a physical constraint and is normalized to one. After observing the agent's project, the principal offers a wage contract, which is a mapping from output realizations to monetary compensation. We assume that the agent has limited liability and hence the payment must be non-negative. Finally, the agent chooses an output distribution at a cost determined by his first-stage choice.====In order to focus on the incentives to generate agency rents we consider a particularly stylized model which abstracts from a number of forces that may have first-order importance in applications. Perhaps most importantly, the agent in our model does not incur any costs in choosing a production technology. In practice, developing a project is likely to require a substantial amount of irreversible investment. In fact, the necessity of such investments may prevent the technology from being renegotiated at the contracting stage. The reason is that, even if both parties are aware of production technologies that are more profitable than the one put forward by the agent, they would not be implemented if modifying the agent's project is too expensive.==== Since the agent chooses the production technology before the contracting stage, our model is a hold-up problem. The expense of modifications could also prevent the agent from secretly adjusting the technology after contracting to make the chosen output distribution less costly, helping to make the agent's commitment to the technology credible.====We also recognize that, in practice, the agent's ability to shape the production technology may be severely limited. For example, some output distributions may be impossible to generate. The constraints on the set of available technologies are likely to be important determinants of the optimal design. However, even when the space of projects is restricted, our main result holds as long as this space includes the agent-optimal technology. Moreover, in the Discussion Section, we demonstrate that our analysis can accommodate certain moment constraints on the domain of production technologies.====Our first main result is that the optimal project involves only binary distributions on ====.==== In other words, the cost of all other distributions can be assumed to be so high that the principal never wants to implement them, and the agent would never choose them irrespective of the payment scheme. This means that the equilibrium project can be thought of as a task which yields a positive payoff only if completed. The production technology specifies the cost of each probability of completion. The principal's wage contract can be viewed as a bonus paid for project completion, with payments set to zero otherwise.====Let us explain the optimality of binary projects. Just like in standard moral hazard problems, output plays a dual role in our model. On the one hand, it is the principal's revenue, and on the other hand, it is an informative signal about the output distribution chosen by the agent, which is used by the principal to incentivize the agent. By the Informativeness Principle, if this signal is made less informative, incentivizing the agent becomes more expensive. The key observation is that each binary distribution with support ==== can be viewed as a garbling of a distribution with the same mean. Consider now a transformation of each project so that, if the agent incurs a cost of a distribution, output is distributed according to the binary distribution with the same mean. This means that the agent's cost of inducing a given level of expected output remains the same in the transformed project but the principal's cost of implementing it goes up. In this sense, such a transformation exacerbates the moral hazard problem. We show how this observation can be used to replace any project with a binary one for which the agent's payoff is at least as high.====Our second main result is a full characterization of the optimal binary project. In this project, the cost of completing the task with probability ==== is zero. That is, even if the agent incurs no cost, project completion can be achieved with probability ====. In equilibrium, the principal offers a bonus which induces the agent to complete the project with probability one. Furthermore, the principal is indifferent between offering this bonus and anything less than that. This indifference condition pins down the cost to the agent of any probability of success between ==== and one. Since the marginal cost of the success probability is less than one and the maximal output is produced surely, the equilibrium is ex-post efficient. That is, given the equilibrium production technology, the allocation is efficient. It turns out that the optimal project yields an equal split of surplus: both the principal and the agent earn payoff ====.====The first-best social surplus in our model is one since projects that can generate output one at no cost are feasible. Of course, the agent does not choose such a project because then the principal could achieve the maximal output without making any payment. To earn rents, the agent designs the technology so that generating high expected output is artificially costly. In fact, since the equilibrium output is one with probability one, the only source of distortion induced by the optimal design relates to this cost. This might be considered a form of “cost padding”, different from others identified in the literature.====As mentioned above, an identifying feature of the optimal project is that the principal is indifferent between implementing a large range of completion probabilities. Let us explain the economic reasoning behind this feature. Note that, since the agent receives the bonus offered by the principal with the probability of completion, her marginal benefit at each completion probability is the bonus. Hence, at the agent's optimal completion probability, the marginal cost equals the bonus. This means that the principal's expected payment to implement a given completion probability is increasing in the marginal cost at that probability, so lowering this marginal cost makes it more attractive to the principal to implement the completion probability. However, if the principal strictly prefers to implement the equilibrium probability of completion to implementing some smaller probabilities, then the marginal costs at these smaller probabilities can be lowered without affecting the principal's equilibrium choice. Since the cost of a completion probability is the integral of the marginal costs of smaller probabilities, such a modification of the project decreases the agent's total cost and thus increases his overall payoff (while the principal is still willing to offer the same bonus that implements the equilibrium completion probability). The agent can improve any project in this way unless the principal is indifferent between implementing any completion probability which has a positive marginal cost.====We demonstrate that our main results remain valid even if the agent is risk averse.==== In particular, the search for an optimal project can still be restricted to the set of binary projects. Moreover, the optimal binary project is still ex-post efficient; that is, the principal implements completion probability one. The optimal binary project is still characterized by the requirement that the principal must be indifferent between offering the equilibrium bonus and anything less than that. Of course, the equilibrium payoffs of the principal and the agent are no longer ==== and they depend on the agent's concave utility function.====Despite the fact that our model is stylized, we believe that there are a number of lessons emerging from our analysis which may be useful in applications. First, a general message is that the agent in a technology design problem will often commit to a project that is not cost-efficient. By padding costs, he may be able to earn additional rents. Second, there is reason to expect that, where possible, an agent's optimal technology will be binary. That is, the project essentially specifies a task and the agent either completes it or fails to solve it. More generally, the agent is likely to favor designs where the information content of output regarding his effort is limited. The reason is that this makes it more expensive for the principal to induce the target level of effort. A third observation that we expect to generalize is the importance of principal incentive constraints in the design of the agent's project. The agent's design must dissuade the principal from choosing ungenerous incentive schemes that give the agent little rent. Where such incentive constraints are slack, however, it may be possible for the agent to redesign the project to lower his equilibrium cost.====Finally, we characterize the payoff combinations that can arise in limited-liability moral hazard problems for some exogenously given technology where output is restricted to the interval ====. This follows by first identifying the largest payoff the agent can achieve as a function of a given profit of the principal. The domain of this function is the interval ==== because the principal can always guarantee a nonnegative profit by offering zero wage and she cannot get more than the maximal output. This function is shown to be strictly concave and zero at the boundaries of its domain. We argue that a payoff profile can be generated by some production technology if and only if it lies weakly below this curve.====The rest of the paper is as follows. Next we discuss related literature. Section 2 introduces the model, Section 3 provides our characterization of the agent's optimal project, and Section 4 provides discussion and extensions, including the characterization of the set of possible player payoffs. Appendix A provides proofs not given in the main text and Appendix B discusses the uniqueness of the agent's optimal project. The Online Appendix solves the agent's project design problem when the agent is risk averse.==== The limited liability model of moral hazard for a risk-neutral agent is a staple of introductory courses on contract theory, where a restriction to binary output (which emerges endogenously in our setting) is often made for tractability. A classic reference for limited-liability moral hazard is Innes (1990), who demonstrates the optimality of simple debt contracts in a model with a continuum of outputs. More recent treatments of moral hazard with limited liability include Poblete and Spulber (2012) for a model with a continuum of outputs and Ollier and Thomas (2013) for a model with binary output, but complicated by the presence of adverse selection.====While the models discussed above feature a risk-neutral agent with limited liability, the alternative friction commonly explored is risk aversion. Seminal work for the moral hazard model with a risk-averse agent includes Mirrlees (1976), Holmstrom (1979), Grossman and Hart (1983), Rogerson (1985) and MacLeod (2003); see Bolton and Dewatripont (2005) and Holmstrom (2017) for comprehensive treatments, and Georgiadis (2022) for a review. More recently, a strand of this literature has focused on models where the agent can shape the entire distribution of output under different assumptions about the cost of distributions (Hebert, 2018; Bonham and Riggs-Cragun, 2021, Georgiadis et al., 2022, and Mattsson and Weibull, 2022). The focus of the moral hazard literature, then, has been on contract design taking the agent's technology as given. Our paper departs from this approach by viewing the production technology as a choice of the agent, raising the problem of technology design. We are unaware of this kind of problem being posed elsewhere in the moral hazard literature.====The question of project design is also related to work on how the primitive contractual environment affects payoffs in moral hazard problems. A relevant example in our context is the development of the Informativeness Principle by Holmstrom (1979), which was later refined for instance by Chaigneau et al. (2019). These papers clarify how additional information about the agent's action can reduce agency costs for the principal.====Another related paper is Condorelli and Szentes (2020) who study the problem of optimally generating information rents in the context of a bilateral trade model. Before interacting with the seller, the buyer can choose the distribution of her valuation for the seller's good. This choice is observed by the seller before she makes a take-it-or-leave-it offer. It turns out that the equilibrium distribution generates a unit-elastic demand, that is, it makes the seller indifferent between setting any price on its support. This is reminiscent of our optimal binary project which makes the principal indifferent across a range of bonuses.==== This similarity might explain why, when the buyer is restricted to choose distributions with support in the interval ====, the equilibrium payoffs of the buyer and the seller are also ====. We further elaborate on the relationship between these two results in Section 4.",Optimal technology design,https://www.sciencedirect.com/science/article/pii/S0022053123000170,10 February 2023,2023,Research Article,29.0
Tan Teck Yong,"College of Business, University of Nebraska-Lincoln, United States of America","Received 8 February 2022, Revised 23 January 2023, Accepted 1 February 2023, Available online 8 February 2023, Version of Record 14 February 2023.",https://doi.org/10.1016/j.jet.2023.105620,Cited by (0),"This paper studies information design in an inspection game. A principal first privately decides whether to acquire a costly monitoring capability that is needed to detect shirking by an agent; subsequently, the agent decides whether to work or shirk. I show that overall efficiency can be improved by providing the agent with a partially — instead of fully — informative signal about the principal's decision; however, the resulting equilibrium involves the agent shirking with a positive probability and all the efficiency gained is captured by the agent. Overall efficiency can be further improved by providing the principal with ==== about the signal generating process; this additional feature also allows the principal to capture the efficiency gained. The analysis sheds light on the extent to which information control can be used to improve monitoring efficiency in such inspection games.","Efficient monitoring is an important aspect of many principal-agent relationships. In many instances, to detect “shirking” by the agent, the principal must first acquire a costly monitoring capability. For example, in an employment relationship, to detect and record evidence of the employee's shirking behavior so that the shirking employee can be fired for “good cause,” the employer must have an employee monitoring system — such as workplace surveillance cameras or software to monitor the employees' online activities — already in place before the employee begins work. In a procurement relationship — particularly for perishable or time-sensitive goods — to ascertain the quality provided by the supplier, the procurer must already possess the necessary quality verification expertise at the time of the goods' delivery. This feature creates an “====” tension, wherein the agent wants to “work” only if the principal has the monitoring capability to detect shirking, but the principal has no incentive to acquire the costly monitoring capability if the agent is expected to work.====Inspection games have been well studied in the literature====; their applications to principal-agent relationships highlight the tension between inducing effort from the agent (by monitoring him) and incentivizing the principal to exert effort on monitoring. This paper studies how controlling the players' information can affect the equilibrium outcomes in such inspection games. More specifically, I consider whether creating some endogenous uncertainty about the principal's monitoring capability can help improve monitoring efficiency.====To motivate this idea, consider the suggestion by eighteenth-century philosopher, Jeremy Bentham, who proposed that one can effectively create the perception of a monitor's omnipresence by keeping the “watched” constantly guessing about his monitoring situation. This idea is reflected in Bentham's design of the ====, which is a “perfect prison” that requires only one guard. The Panopticon's design comprises a circular observation tower surrounded by the cells housing the prisoners. A guard is stationed at the top of the tower, and the guard station is equipped with blind windows that prevent the prisoners from looking in. Thus, at any given moment, a prisoner is unsure whether he is being specifically watched by the guard, and he must consequently behave as though he is under constant observation. ====This feature — wherein a party attempts to convince another party of the existence of a monitoring or deterrence capability while maintaining its actual existence ambiguous — is also present in modern-day security. For example, until quite recently, 77% of the surveillance cameras on board the trains of the Bay Area Rapid Transit (BART) were cheap, nonfunctioning “dummy” cameras designed to resemble recording surveillance cameras and meant to fool people into thinking that they are being monitored. It was only when a man was shot on the train in January 2016 — and BART could not produce the video from the onboard camera — that BART was forced under pressure to divulge this information. After the incident, BART announced that it was preparing to spend $1.4 million to install 470 real cameras because divulging their use of dummy cameras “makes [this] tactic of deterrents ineffective” and caused BART to “have to evolve,” indicating that the fake cameras were intentionally installed in the first place.====The two examples above suggest that there could be improvements in monitoring efficiency by creating some uncertainty about the monitoring situation. Against this backdrop, I study the extent to which information control can be used to improve monitoring efficiency in a stylized inspection game. There is a principal (she), who first privately decides whether to invest in acquiring a monitoring capability; subsequently, an agent (he) decides whether to work or shirk. The principal must pay the agent a price for his output. However, if (and only if) the agent shirked ==== the principal invested, the principal will have verifiable evidence that the agent had shirked, and the principal will then receive a damage compensation from the agent. In this model, the principal's investment is inefficient because it is costly but does not directly create any surplus. However, the principal's investment is necessary to deter the agent from shirking, thereby creating an inspection game tension.==== I augment this setup with an information structure that generates private signals to the players, and I study the set of equilibrium payoffs attainable across all information structures.====The benchmark case is ====, wherein the principal's action (i.e., to invest or not) is perfectly revealed to the agent. Under FT, a noninvestment will always be discovered by the agent, who then responds by shirking. Thus, to induce effort from the agent, the principal always invests, which induces the agent to always work on the equilibrium path. Therefore, under the FT equilibrium outcome, there are both maximum “====” (because the principal always invests) and maximum “====” (because the agent always works).====The effects of any information structure are determined by how it controls the principal's incentive to invest and how it controls the information to the agent to “persuade” him to work. To highlight the differential effects of providing information to the two players, I separate the analysis into two classes of information structures, with each illustrating the extent of how controlling each player's information can affect the equilibrium outcome. Let me describe some of the key findings.====In Section 4, I first study “====,” which are information structures that provide the agent with a signal that is correlated with the principal's (privately chosen) action. With a partially informative signal structure, it is possible to induce the principal to not invest with some probability in equilibrium, thereby decreasing investment inefficiency. However, such gains must come at the expense of incurring some quality inefficiency, with the agent now shirking with a positive probability in equilibrium. I show that it is possible for the former gain to outweigh the latter loss, thereby yielding a net gain in overall efficiency relative to the FT outcome. However, all of the net efficiency gained is always entirely captured by the agent. Thus, although information about the principal's action allows the agent to make his (ex post) work-or-shirk decision more accurately, the agent sometimes benefits (ex ante) from having less information about it, whereas the principal is always weakly worse off from the agent having only partial information about her action.====Next, in Section 5, I consider controlling the principal's information about how her action affects the signal that the agent observes later. To do so, I analyze a class of information structures called “====,” which first provide the principal with a private signal before she decides whether to invest; subsequently, the agent receives a signal that is correlated with ==== the principal's action and her private signal. A warning device expands the set of attainable outcomes and payoffs relative to signal structures; this is because a warning device can separate the task of maintaining the principal's incentive to invest from the task of hiding information from the agent.====For example, a warning device can induce an equilibrium in which the principal does not invest with some probability but the agent still always works, and the principal captures all the resulting efficiency gained. To illustrate this intuitively, consider having a third-party inspector (it) to verify the principal's investment at some publicly known probability. The inspector is instructed to expose the principal to the agent if it finds no investment but must maintain silence otherwise; the inspector's response can be viewed as the agent's “signal.” Additionally, the inspector must privately inform (or warn) the principal before coming to conduct the inspection; this information is the principal's private “signal.” Under this arrangement, the principal will invest if and only if the inspector is coming. Thus, on the equilibrium path, the inspector will never catch a noninvestment and always maintains silence, meaning that the agent essentially receives no information. However, the agent understands that the principal will invest whenever there is going to be an inspection. Thus, the probability of the principal having invested is the probability of an inspection, and if this (publicly known) probability is not too low, the agent will always be deterred against shirking. In turn, the principal's equilibrium payoff is higher than her FT payoff because the agent always works, whereas the principal saves on the investment cost with some probability (i.e., whenever there is no inspection).====The rest of the paper is organized as follows. The next section discusses the related literature. Then, Section 3 presents the model. Sections 4 and 5 provide the analysis on signal structures and warning devices, respectively. Finally, Section 6 concludes. Unless stated otherwise, all omitted proofs are found in Appendix A.1.",Optimal transparency of monitoring capability,https://www.sciencedirect.com/science/article/pii/S0022053123000169,8 February 2023,2023,Research Article,30.0
"Ishiguro Shingo,Yasuda Yosuke","Graduate School of Economics, Osaka University, Japan","Received 14 July 2021, Revised 5 January 2023, Accepted 1 February 2023, Available online 8 February 2023, Version of Record 15 February 2023.",https://doi.org/10.1016/j.jet.2023.105619,Cited by (0),"This study investigates optimal contracts to solve the moral hazard problem with subjective evaluation in a static environment with multiple agents wherein the principal observes agents' performances privately. Despite the limitations of feasible contracts that the principal can credibly offer, we show the irrelevance theorem that even under subjective evaluation, the principal can be as well off as if agents' performances are objective and verifiable, provided there are at least two risk-neutral agents whose actions satisfy a certain condition on their output distributions. Our irrelevance result encompasses a large class of moral hazard problems, including statistically correlated and technologically interdependent performance signals. We also extend the model to include multidimensional actions, limited liability constraints, and collusion among agents.",None,Moral hazard and subjective evaluation,https://www.sciencedirect.com/science/article/pii/S0022053123000157,8 February 2023,2023,Research Article,31.0
"Basteck Christian,Ehlers Lars","WZB Berlin Social Science Center, Berlin, Germany,Département de Sciences Économiques and CIREQ, Université de Montréal, Montréal, QC H3C 3J7, Canada","Received 22 December 2020, Revised 21 December 2021, Accepted 28 January 2023, Available online 6 February 2023, Version of Record 14 February 2023.",https://doi.org/10.1016/j.jet.2023.105618,Cited by (0),"We study the random assignment of indivisible objects among a set of agents with strict preferences. We show that there exists no mechanism which is unanimous, strategy-proof and envy-free. Weakening the first requirement to ====-unanimity – i.e., when every agent ranks a different object at the top, then each agent shall receive his most-preferred object with probability of at least ==== – we show that a mechanism satisfying strategy-proofness, envy-freeness and ex-post weak non-wastefulness can be ====-unanimous only for ==== (where ==== is the number of agents). To demonstrate that this bound is tight, we introduce a new mechanism, Random-Dictatorship-cum-Equal-Division (RDcED), and show that it achieves this maximal bound when all objects are acceptable. In addition, for three agents, RDcED is characterized by the first three properties and ex-post weak efficiency. If objects may be unacceptable, strategy-proofness and envy-freeness are jointly incompatible even with ex-post weak non-wastefulness.","Consider the problem of assigning indivisible objects among a set of agents – each agent is to receive at most one and we assume they have strict preferences over the set of objects. Further, while objects' characteristics may include a fixed monetary payment, there are no additional transfers. Problems like this arise in many real-life applications such as on-campus housing (where rents are fixed), organ allocation, school choice with ties in applicants' priorities, etc. Whenever several agents would like to consume the same object, the indivisibility of objects, together with the absence of any compensating transfers, will render any deterministic assignment unfair. This is the main reason for implementing random assignments in such contexts.====Since agents' preferences are private information, the design of random assignments has to provide incentives for agents to reveal their preferences.==== Moreover, in many applications, the resulting assignments should be based on agents' (ordinal) rankings of objects, rather than on preferences over all possible lotteries as the elicitation of the latter is difficult in practice – for example, school choice programs will typically ask applicants to provide a list of schools, ranked from most- to least-preferred.====Strategy-proofness makes truthful reporting a dominant strategy and thus should ensure that agents truthfully reveal their ordinal preferences over objects for any underlying utility representation of preferences. Unfortunately, the literature on random assignment mechanisms contains several impossibility results as soon as strategy-proofness and equal-treatment-of-equals, as a minimal fairness requirement, are married with different ex-ante/ex-post notions of efficiency.==== In some sense, these efficiency notions are hence “too strong”. In addition, equal-treatment-of-equals may be considered “too weak” a notion of fairness as it only constrains random assignments in rare cases where agents' preferences are identical, which seems contrived given that fairness concerns were the principal reason to consider random assignments in the first place. Our paper keeps the strategy-proofness requirement, strengthens equal-treatment-of-equals to envy-freeness and explores the efficiency frontier given these two constraints. From a practical point of view, this allows to answer the question whether losses in efficiency are mild enough to allow to insist on strategy-proofness and envy-freeness. For example, in the related problem of school choice with priorities, the well known Deferred Acceptance mechanism is strategy-proof and justified-envy-free,==== and is hence widely used despite being inefficient.====First, we marry strategy-proofness and envy-freeness with arguably one of the weakest well known efficiency requirements, namely unanimity. In our setting it requires that if all agents rank different objects first, then each agent shall receive his most-preferred object with probability one – in other words, whenever there exists a unique efficient assignment, then this assignment is chosen for sure. Unfortunately, we find this requirement to yield another impossibility (together with strategy-proofness and envy-freeness). Given this, we introduce a quantitative measure of how much unanimity is respected: ====-unanimity means that in any such situation every agent receives his most-preferred object with probability of at least ====. Of course, 0-unanimity is satisfied by any mechanism and by lowering ==== from one to zero we obtain a possibility together with strategy-proofness and envy-freeness. The important question is to determine the exact bound. We show that for three or more agents this bound is equal to ====, where ==== denotes the number of agents – no mechanism may be ====-unanimous for any ==== larger than ====. We also introduce object-unanimity whereby a certain object shall be assigned to a specific agent who ranks this object first while all other agents rank it last. We show that the impossibility pertains when ====-unanimity is replaced with ====-object-unanimity (whereby the specific agent shall receive his most preferred object with probability of at least ====).====To demonstrate that this bound can be achieved, we introduce a new mechanism, called Random-Dictatorship-cum-Equal-Division mechanism (RDcED). In this mechanism, any agent is chosen with equal probability to choose as dictator and receive his most-preferred object, while all other objects are assigned uniformly (at random) among the remaining agents. For three agents we show that RDcED is the unique mechanism which is ex-post weakly non-wasteful, ex-post weakly efficient, strategy-proof and envy-free on the domain where all objects are acceptable. Hence, RDcED is characterized by a natural set of properties for three agents. RDcED satisfies ====-unanimity for three agents (as any agent is chosen with probability ==== to be the dictator and when another agent is the dictator, the agent receives his most-preferred object with probability ====); for an arbitrary number ==== of agents, RDcED satisfies ====-unanimity and hence achieves the maximal bound for ====-unanimity among all mechanisms that are ex-post weakly non-wasteful, strategy-proof and envy-free. The same is true for ====-object-unanimity.====When agents may consider objects unacceptable, our impossibility result becomes more severe. Strategy-proofness and envy-freeness are then jointly incompatible even with ex-post weak non-wastefulness – that is, we have to accept situations where, ex-post, an agent remains unassigned even though there exists an unassigned object that is acceptable to that agent. Only after further weakening ex-post weak non-wastefulness by restricting it to hold for profiles where there exists a unique non-wasteful assignment of objects do we arrive at a possibility result.====Finally, we show that by allowing waste on the domain of acceptable objects, one can increase the bound for ====-unanimity beyond ==== for three agents. More precisely, we construct a mechanism for three agents which (i) assigns any agent no object with probability ==== and (ii) satisfies strategy-proofness, envy-freeness and ====-unanimity. However, gains in efficiency (at least among agents that are assigned an object) by allowing for waste are limited – we show that the maximal level of ====-unanimity achievable by allowing for waste is monotonically decreasing in the number of agents and that for three agents both ====-unanimity and ====-object-unanimity can be satisfied with at most ==== in the class of strategy-proof and envy-free mechanisms.====Below we discuss the related literature in detail. The main starting point is the impossibility of strategy-proofness, envy-freeness and ex-ante efficiency. Bogomolnaia and Moulin (2001) show that this remains unchanged when envy-freeness is weakened to equal-treatment-of-equals. Furthermore, they introduce the probabilistic serial (PS) mechanism and show that it is envy-free and ex-ante efficient (hence necessarily violates strategy-proofness).==== Nesterov (2017) shows that the impossibility persists when ex-ante efficiency is weakened to ex-post efficiency.==== In closely related ongoing work, overlapping with our first result, Shende and Purohit (2020) show independently that strategy-proofness and envy-freeness are incompatible with unanimity (which they refer to as contention-free efficiency). After observing this impossibility, the authors move on by restricting attention to so-called (Balanced) Pairwise Exchange Mechanisms. They show in this limited class of mechanisms the equivalence of strategy-proofness and envy-freeness, and analyze them in further detail. In contrast, we keep the set of mechanisms unrestricted throughout and determine the possibility frontier (in terms of unanimity).==== Recently, Mennle and Seuken (2021) decomposed strategy-proofness into three properties which they call swap-monotonicity, upper invariance and lower invariance.====In terms of possibility results, it is known that the random serial dictatorship (RSD, also known as random priority) mechanism satisfies strategy-proofness, equal-treatment-of-equals and ex-post efficiency – i.e., weakening ==== envy-freeness and ex-ante efficiency results in a possibility. Our contribution is to keep envy-freeness (since fairness understood as equity is the principal reason for implementing a random assignment) and strategy-proofness and to explore by how much exactly we have to weaken ex-post efficiency to arrive at a possibility result.====Most of our results focus on the preference domain where all objects are acceptable. If agents may rank objects as unacceptable and possibly receive no object, notions of efficiency have to take into account the set of (un)assigned objects====: a deterministic assignment is (weakly) non-wasteful if no (unassigned) agent prefers an unassigned object to his assignment. As a stronger requirement, ex-ante non-wastefulness demands that if an agent finds an object acceptable but receives no object with positive probability, then the acceptable object must be assigned with probability one. Moreover, if he prefers an object over another and is assigned the less-preferred with positive probability, then the preferred object must be assigned with probability one. Martini (2016) shows that there is no mechanism satisfying strategy-proofness, equal-treatment-of-equals and ex-ante non-wastefulness, i.e., another principal impossibility result on the full domain. In comparison, our impossibility result invokes a stronger notion of fairness and a weaker notion of non-wastefulness. Erdil (2014) studies the ex-ante waste of strategy-proof mechanisms, and shows that RSD is dominated by a strategy-proof mechanism which is less wasteful. It is an open problem to establish the minimal waste in the class of mechanisms satisfying strategy-proofness and equal-treatment-of-equals.====The paper is organized as follows. Section 2 introduces random assignments, their properties and several popular mechanisms. Section 3 contains the impossibility pertaining to unanimity and establishes the upper bound of ==== for ====-unanimity among ==== agents. Section 4 introduces RDcED to show that the bound is tight and characterizes it for three agents. Section 5 presents a novel impossibility result for non-wasteful assignments on the full preference domain and shows how non-wastefulness or unanimity can be weakened to allow for a possibility result. Finally, Section 6 analyses how the upper bound on ====-unanimity may be increased by allowing for waste and Section 7 concludes.",Strategy-proof and envy-free random assignment,https://www.sciencedirect.com/science/article/pii/S0022053123000145,6 February 2023,2023,Research Article,32.0
"Heller Yuval,Nehama Ilan","Department of Economics, Bar-Ilan University, Israel,Department of Exact Sciences, Haifa University, Israel","Received 30 October 2021, Revised 7 November 2022, Accepted 23 January 2023, Available online 1 February 2023, Version of Record 9 February 2023.",https://doi.org/10.1016/j.jet.2023.105617,Cited by (0),"We examine the evolutionary basis for risk aversion with respect to aggregate risk. We study populations in which agents face choices between alternatives with different levels of aggregate risk. We show that the choices that maximize the long-run growth rate are induced by a heterogeneous population in which the least and most risk-averse agents are indifferent between facing an aggregate risk and obtaining its linear and ==== for sure, respectively. Moreover, approximately optimal behavior can be induced by a simple distribution according to which all agents have constant relative risk aversion, and the coefficient of relative risk aversion is uniformly distributed between zero and two.","Our understanding of risk attitudes can be sharpened by considering their evolutionary basis in situations in which agents face choices that affect their number of offspring (see Robson and Samuelson, 2011, for a survey). Lewontin and Cohen (1969) shows that idiosyncratic risk (independent across individuals) induces a higher expected long-run growth rate (henceforth, abbreviated as ====) than aggregate risk (correlated across individuals) with the same marginal distribution (i.e., for each individual the distribution of offspring induced is the same). Specifically, the growth rate induced by an idiosyncratic-risk lottery is equal to its arithmetic mean, while the growth rate induced by an aggregate-risk lottery is equal to its geometric mean.====In his seminal paper, Robson (1996) shows that when a population faces a choice between several aggregate-risk lotteries, the optimal growth rate can be achieved by choosing the mixture of these lotteries that maximizes the expected logarithm of the number of offspring.==== For example, consider a choice between an alternative that yields 2 offspring to each agent for sure (the ==== alternative) or a ==== alternative yielding either 1 offspring or 5 offspring to all agents who choose it, with equal probabilities. One can show that the expected logarithm of the number of offspring is maximized (and the optimal growth rate is achieved) by having ==== of the agents choosing the safe alternative, and the remaining ==== of the agents choosing the risky alternative. This mixture yields an average number of offspring in the population that is either 4 or ==== in each generation. This heterogeneity in the choices of the agents can be interpreted as ==== the expected number of offspring in the population (Cohen, 1966; Cooper and Kaplan, 1982; Bergstrom, 1997). The existing literature typically does not specify how the optimal bet-hedging mixtures are implemented.====Note that each agent faces a non-convex choice between alternatives, and thus she cannot hedge her own personal risk (for example, there is no choice that yields her either 4 or ==== offspring in the above example). One way to implement the optimal level of bet-hedging is to induce each agent to randomly choose her alternative according to the probability that maximizes the population's growth rate. Observe that this behavior is complicated in the sense that it is not induced by a von Neumann–Morgenstern (vNM) utility. For example, in the above scenario she strictly prefers choosing a lottery to either of the two alternatives. Hence, when facing a choice between two alternatives, she needs to evaluate her utility also from various lotteries over them.====In this paper, we introduce a new mechanism for implementing the optimal growth rate. We show that the optimal growth rate is induced by a heterogeneous population of utility maximizers in which agents have different levels of risk aversion with respect to the aggregate risk (and all agents are risk neutral with respect to idiosyncratic risk). In this population, the most risk-averse agent is indifferent between obtaining a risky lottery ==== and obtaining the harmonic mean of ==== for sure, while the least risk-averse agent is risk neutral, that is, indifferent between obtaining the risky lottery ==== and obtaining the arithmetic mean of ==== for sure. Moreover, we show that a nearly-optimal growth rate can be achieved by a simple distribution of vNM utilities, according to which all agents have constant relative risk aversion, and the risk coefficient is uniformly distributed between zero and two.==== We consider a continuum population with asexual reproduction. Each agent lives for a single generation, during which she faces a choice between two lotteries over the number of offspring: a ==== (degenerate) ==== that gives ==== offspring for sure, and a ==== ==== with aggregate risk.==== Nature induces a distribution of risk preferences with regard to aggregate risk, according to which each agent in the population (deterministically) chooses between the risky alternative and the safe alternative.==== If nature were limited to endowing all agents with the same preference, then it would be optimal for all agents to evaluate any risky alternative ==== as having a certainty equivalent of its geometric mean. However, heterogeneous populations can induce a substantially higher growth rate, because heterogeneity in risk aversion allows the population to hedge the aggregate risk by enabling scenarios in which only a portion of the population (the less risk-averse agents) chooses the risky alternative.====Theorem 1 characterizes the optimal share of agents that choose the risky alternative, that is, the share that maximizes the long-run growth rate. In particular, it shows that all agents should choose the safe option iff ====, and all agents should choose the risky option iff the harmonic mean of ====, ====, satisfies ====. Moreover, we characterize the optimal preference distribution (Theorem 2) as follows: (1) the least risk-averse agent in the population is risk neutral, (2) the most risk-averse agent in the population has harmonic utility, i.e., she evaluates any risky alternative ==== as having a certainty equivalent of its harmonic mean, and (3) all agents in the population should be risk averse, but less risk averse than the harmonic utility.====The optimal distribution of preferences is quite complicated. By contrast, our numerical analysis shows that a nearly-optimal growth rate can be achieved by a simple distribution of vNM utilities with constant relative risk aversion (====), where the relative risk coefficient is uniformly distributed between zero (risk neutrality) and two (harmonic utility). The predictions of our model fit reasonably well with the empirical works on the distribution of risk attitudes in the population, as discussed in Section 5.==== The paper is structured as follows. In Section 2 we describe the model. Section 3 presents the analytic results, which are supplemented by a numerical analysis in Section 4. We conclude with a discussion in Section 5.",Evolutionary foundation for heterogeneity in risk aversion,https://www.sciencedirect.com/science/article/pii/S0022053123000133,1 February 2023,2023,Research Article,33.0
"Babus Ana,Hachem Kinda","Washington University in St. Louis & CEPR, One Brookings Drive, St. Louis, MO 63130, United States of America,University of Virginia & NBER, 100 Darden Blvd, Charlottesville, VA 22906, United States of America","Received 16 September 2020, Revised 20 January 2023, Accepted 23 January 2023, Available online 28 January 2023, Version of Record 9 February 2023.",https://doi.org/10.1016/j.jet.2023.105615,Cited by (0),"We develop a theory of financial innovation in which both market structure and the payoffs of the claims being traded are determined endogenously. Intermediaries use the cash flows of an underlying asset to design securities for investors. Demand for securities arises as investors choose markets then trade using strategies represented by quantity-price schedules. We show that intermediaries create increasingly riskier asset-backed securities when facing deeper markets in which investors trade more competitively. In turn, investors elicit less risky securities when they choose thinner markets, revealing a novel role for market fragmentation in the creation of safer securities.","It has long been acknowledged that non-financial firms adjust product design in response to market structure.==== Financial firms that mediate security issuances have the opportunity to do the same, and anecdotally they do. However, little is known about the theoretical forces that shape the relationship between the design of a financial claim and the market in which it trades. This is especially so when it comes to standardized securities whose payoffs are not commissioned by any one investor. What are financial intermediaries' incentives to adjust product design in response to market structure when creating securities to connect investors with markets? What are investors' incentives to participate in markets for certain securities?====We build a tractable model in which both security design and market structure are endogenously determined to study these questions. We then use the model to speak to the intricate relationship between the safety and liquidity of a security, which has recently received renewed interest in the literature. Our environment is one where financial intermediaries use the cash flows of an underlying asset to design standardized securities for investors to trade. A security specifies a payoff for every realization of the underlying asset. As in Ross (1976) and Allen and Gale (1994), we consider that financial innovation arises in response to investors' demand. However, key to our model is that the demand for securities is itself endogenous. This demand is modeled in two steps. First, investors choose a market in which to trade, which determines the market structure. Second, once markets open and investors can trade, their trading strategies are represented by quantity-price schedules, with each investor understanding the impact of her trade on the price of the security. Markets can be thinner and more fragmented with investors trading more strategically, or deeper and less fragmented with investors trading more competitively.====Financial intermediaries design securities knowing the depth of the markets they face. They do so strategically, taking into account how the payoffs of the security they design will affect the price at which investors ultimately trade the security in the market. Each investor chooses a market, and thus commits to a financial intermediary, before intermediaries design securities.==== Investors are rational and internalize how their choices will affect the market structure faced by financial intermediaries and thus the design of the securities that will be traded.====There are two implicit frictions in the environment that are worth making explicit here. First, investors cannot directly invest in the same assets as financial intermediaries. This is realistic as financial intermediaries frequently create asset-backed securities that give investors exposure to markets that they could not otherwise invest in. Mortgage-backed securities are one such example.==== Second, intermediaries design securities bounded by limited liability. That is, a security's payoff cannot exceed the payoff of the asset that backs it in any given state of the world. In practice, most securities are implicitly designed to respect this constraint. In our set-up, limited liability is equivalent to the spanning constraint in the financial innovation literature (Duffie and Rahi, 1995) which requires that the securities a financial intermediary issues span the payoff of the asset that backs them.====We obtain two major sets of results. The first set of results characterizes the security that an intermediary finds optimal to offer taking as given the market structure. We show that this security depends monotonically on the depth of the intermediary's market. In particular, we show (i) that the optimal security belongs to the family of debt contracts, paying the lesser of a flat payoff and the full value of the underlying asset in every state of the world, and (ii) that the state in which the security starts paying the flat payoff is higher in markets with more investors. In other words, financial intermediaries design progressively riskier asset-backed securities when facing investors that trade more competitively. In the limit, the security approaches the payoff of the underlying asset in all states, which we refer to simply as selling “equity” to the investor.====The intuition for this first set of results is as follows. When choosing how to design a security, the intermediary's main incentive is to obtain a high price for it. Investors naturally like high expected payoffs but dislike risk, captured most simply as having mean-variance preferences. The equilibrium price at which a security is traded is therefore increasing in its mean payoff and decreasing in the variance of its payoffs across states. The intermediary thus faces a trade-off between the mean and the variance of the security he designs, making a debt contract optimal as debt has the least variance among all limited liability securities with the same expected value. Importantly, though, the equilibrium price decreases less with the variance of the security in deeper markets where investors have a lower price impact. Thus, the strength of the mean-variance trade-off faced by the intermediary (and hence where on the spectrum of debt contracts the security lies) depends on the depth of the market. The deeper the market, the less pronounced the trade-off and the more equity-like the intermediary makes his security.====The second set of results focuses on the equilibrium market structure. This is crucial to ensure that the securities intermediaries design in a given market structure can indeed be supported in equilibrium. If no investor benefits from trading in a particular market structure, then we should not expect the corresponding securities to arise in equilibrium.====When choosing which market to trade in, an investor weighs the gains from trade with other investors against the ability to influence the security that the intermediary designs. An investor who trades in a thinner market will have a larger price impact. On one hand, this amplifies the mean-variance trade-off in the intermediary's security design problem and delivers a less risky security. On the other hand, it also amplifies the extent to which the investor will move the price of the security against herself when trading with other investors.====Investors benefit from trading with each other because their valuations of any given security are subject to idiosyncratic preference shocks as markets open. When investors expect to be relatively homogeneous in their valuations of the same security, they anticipate limited benefits from trading with each other and are therefore willing to accept a larger price impact in order to elicit a less variable security from the intermediary. In contrast, when investors expect to be relatively heterogeneous, they understand that they may want to engage in large trades with each other so they seek to limit their price impact by choosing to trade in a large market, albeit with a riskier security.====At the core of our theoretical model is the market power that investors have relative to the financial intermediary that designs the security they trade, where market power is captured by price impact. When investors trade in thin markets, they have more market power relative to the intermediary. In this case, investors use their market power to obtain a security that is most favorable to them. In contrast, when investors trade in a deep market, the intermediary uses its relatively superior market power to design a security that is more favorable to it. These patterns are supported by emerging empirical evidence that links market power to certain aspects of bond design (e.g., Adelino et al. (2017) and Brancaccio and Kang (2022)). Our model thus elucidates a theoretical mechanism through which market power affects security design when market structure is endogenous.====By shedding light on the interaction between security design and market structure, our paper can contribute to an important agenda that studies the relationship between the safety and liquidity of a security. It is often assumed that safer securities are at least as liquid as riskier ones, but in practice the relationship between safety and liquidity is complex (e.g., Friewald et al. (2017) and Geromichalos et al. (2022)). Our baseline model predicts that less variable asset-backed securities are traded in thinner, more fragmented markets while more variable asset-backed securities are traded in deeper, less fragmented markets when securities are backed by the same underlying asset and every investor can freely choose a market in which to trade. Introducing constraints on the market choice of investors generates asymmetric equilibria where securities with different payoff profiles co-exist in markets of different sizes. If, in addition, intermediaries differ in the underlying assets they use to design asset-backed securities, then less variable securities may trade in deeper markets while more variable securities trade in thinner markets. These results highlight that multiple factors contribute to the relationship between the safety and the liquidity of a security.====A planner who chooses across market structures but is constrained to satisfy the equilibrium security design of intermediaries and the trading equilibrium among investors (when the latter can freely choose markets) would choose deeper markets, even though the security that emerges in these markets has more variable payoffs. Intermediaries are always better off designing a security for a large market than for a small market. Investors thus benefit at the expense of intermediaries in any equilibrium where debt is traded. In aggregate, however, the benefits to investors in an equilibrium where debt is traded are outweighed by the losses to intermediaries, such that total welfare is higher when markets are deeper. If the planner could decouple the security design choice from the market structure choice, then he could achieve the highest welfare by designing a debt security for risk averse investors and having them all trade this security in one large market in order to maximize the gains from trade. The problem is that security design cannot be decoupled from market structure in equilibrium. This result is at the heart of our paper. Intermediaries respond to market-based incentives when designing a security for investors to trade. These incentives come from the price of the security, which is endogenously less sensitive to investors' risk aversion in a large market because the price impact of an individual investor is decreasing in market size.====  This paper relates to several strands of literature. The most relevant studies are those on security design and endogenous market structure.====The literature on security design has been very prolific over recent decades. The classic problem explored in these papers is that of a firm needing to raise funds from an investor to finance an investment project. In exchange, the firm proposes a security to the investor. A common result in this literature is that debt is the optimal security in the presence of asymmetric information or moral hazard (e.g., Gale and Hellwig (1985), Gorton and Pennacchi (1990), Nachman and Noe (1994), DeMarzo and Duffie (1999), Biais and Mariotti (2005), Yang (2020), Hébert (2018), Asriyan and Vanasco (2018)).==== We explore a variant of the typical set-up. In particular, financial intermediaries issue securities which allow investors to have exposure to assets in which they cannot directly invest. The family of debt contracts is optimal even absent informational asymmetries, and, more importantly, financial intermediaries offer low-variance debt only when investors trade in a thin market. As the market gets deeper, the optimal security becomes equity. Our paper thus isolates investor market power as a force that disciplines the incentives of intermediaries in security design. Isolating this force is important as investor market power can have unique policy implications; see Babus and Hachem (2021) for an application of our model to regulatory debates about centralized trading, specifically the introduction of a centralized exchange to improve market access and increase liquidity.====Parallel to the literature on security design, there is a body of work on financial innovation that studies the role of security issuances in completing markets. From the seminal paper of Allen and Gale (1991) to the more recent contribution of Carvajal et al. (2012), the main focus of this line of research is to analyze whether competition among asset-holders affects their incentives to introduce new securities. Complementarily to this literature, we study a model in which a financial intermediary's decision to issue securities is affected by the strategic competition between investors when trading the securities they are offered.====There is a young but growing literature on endogenous market structure. Babus and Parlatore (2022), Cespa and Vives (2022), Dugast et al. (2022), Lee and Wang (2018), and Yoon (2018) provide models that seek to explain why trade takes places in a variety of venues, centralized or decentralized. However, in these papers, the asset traded is taken to be exogenous. We endogenize both the security design and the market participation decision, which allows us to study the relationship between the type of security and the market structure in which it is traded.====A small number of papers study the effect of market structure on security design. In a set-up which assumes that investors are better informed about the prospects of the issuer than the issuer himself, Axelson (2007) shows that debt is optimal if the degree of competition among investors is low. Rostek and Yoon (2022) analyze the role of market structure for introducing non-redundant derivatives. In both of these papers, however, the market structure is taken to be exogenous. In our paper, the market structure is endogenously determined. This is important, as it ensures that the securities traded in a given market structure can indeed be supported in equilibrium.====The rest of the paper proceeds as follows: Section 2 introduces the model environment; Section 3 defines and characterizes the equilibrium; Section 4 connects some predictions of the model to real world markets; Section 5 discusses aggregate welfare; and Section 6 concludes. All proofs are collected in Appendix A.",Markets for financial innovation,https://www.sciencedirect.com/science/article/pii/S002205312300011X,28 January 2023,2023,Research Article,34.0
"Andrei Daniel,Carlin Bruce I.","McGill University, Desautels Faculty of Management, 1001 Sherbrooke Street West, Office 549, Montréal, Québec H3A 1G5, Canada,Jones School of Business, Rice University, 1900 Rice Blvd., Houston, TX 77005, United States of America","Received 25 February 2021, Revised 17 January 2023, Accepted 20 January 2023, Available online 27 January 2023, Version of Record 3 February 2023.",https://doi.org/10.1016/j.jet.2023.105613,Cited by (0),"We model a rent-seeking game where agents experiment with a new technology and compete for claims to a consumption stream. We characterize how creative destruction affects risk, ","We characterize the effect of Schumpeterian competition on the aggregate consumption stream in a standard Lucas endowment economy. When paradigm shifts occur, agents fight for market share, which is a claim to both current and future cash flows, and includes real options such as the option to expand, flexibility, and synergies. We investigate the consequences that this type of competition has for the structure of the consumption stream, risk in the economy, and asset pricing dynamics.====Surely, aggregate consumption changes when innovation arises and agents compete for monopoly rents. The relationship between creative destruction and rent-seeking has been studied in a variety of contexts (e.g., Reinganum, 1983; Aghion and Howitt, 1992; Baye and Hoppe, 2003).==== Though, it still remains unclear whether creative destruction is socially beneficial (e.g., Witt, 1996; Aghion et al., 2015; Komlos, 2014), and whether it worsens income inequality (e.g., Jones and Kim, 2018; Gabaix et al., 2016).====Beyond the potential gains that innovation promises, Schumpeter also stressed that creative destruction “takes considerable time in revealing its true features and ultimate effects” (Schumpeter, 1942). Some of this risk is due to the eventual fate of the innovation (====). But, creative destruction may also involve spillovers and irreversible changes in systematic risk (Pastor and Veronesi, 2009) because the fate of existing assets becomes uncertain: some assets face ====, while others enjoy new growth options and complementarities. So, a paradigm shift may indeed change both the expected value and variance of future consumption.====To fix ideas, consider the recent widespread adoption of Zoom Video Communications. This not only displaced technologies like Skype and Face Time, it impacted many other dimensions of the economy. Remote meetings have made firms more efficient in connecting global employees and they have improved the ability of researchers from different academic institutions to conduct research together. At the same time, the future of corporate real estate has become more uncertain as businesses are re-assessing their needs for office space. Residential real estate has become more risky as well: people are migrating away from city centers because they are able to work remotely. The evolution of this paradigm shift has also likely affected travel, hotel usage, the census of state populations (which have political implications), and labor markets themselves. So, while the direct impact of this creative destruction is on obvious substitutes, spillovers make many other existing assets more uncertain as well.====We study a Tullock contest (Tullock, 1980) in which agents make simultaneous choices about how much to experiment with a new technology. Agents compete for market share, which is a proportion of an aggregate consumption stream that endogenously results from innovation (Hirshleifer, 1989). In our model, however, creative destruction by the agents has two effects on the aggregate consumption stream: it changes the growth rate of consumption—hopefully for the better—and it amplifies the magnitude of its diffusion. The former characterizes technological uncertainty and the latter captures disruption risk.====We solve for a first-best benchmark. The socially optimal amount of experimentation results from the classic tradeoff between maximizing the expected value of future consumption and minimizing its future volatility. Naturally, experimentation is higher with less technological uncertainty, a higher expected benefit, lower risk aversion, and less spillover to existing assets through disruption risk.====Rent-seeking competition fundamentally alters this tradeoff and catalyzes inefficient investments. When agents fight for a slice of the pie, aggregate experimentation is higher than the social optimum and grows monotonically with competition. Moreover, if the experimental good offers some degree of technological diversification (Koren and Tenreyro, 2013), then the agents have higher incentives to over-invest, which again increases experimentation. But, the agents do not internalize their effect on risk. As such, even though excessive experimentation allows for faster learning about the new technology, it amplifies the impact of technological uncertainty and causes more disruption of existing assets.====Disruption risk permanently increases both the equilibrium stock market volatility and the risk premium, and competition amplifies this. However, technological uncertainty has a negative and dynamic effect on volatility and the risk premium. This effect arises in general equilibrium because of a hedging demand from agents, who insulate themselves against the adverse effect of technological uncertainty.====From the model, we learn that experimentation affects systematic risk by changing the term structure of wealth. The ====, which measures the weighted average maturity of the price of a claim to aggregate consumption, describes the risks that agents bear based on the timing of consumption from the dividend stream. Experimentation tends to increase wealth duration, and this amplifies the effect of technological uncertainty and increases agents' hedging motives. This complementarity may even cause the risk premium to be negative. But this effect is transient: as learning progresses and technological uncertainty drops, both systematic risk and the risk premium rise, which is a result of persistent disruption risk.====The price-dividend ratio increases with experimentation but decreases as uncertainty about the new technology is resolved through learning. Higher rent-seeking and increased competition make the occurrence of boom-bust episodes more likely, which does not occur in the social optimum. To test this empirically, we discretize our continuous-time setup, run simulations of the model, and conduct predictive regressions. Our results show that high competition causes the price-dividend ratio to negatively predict future returns. This not only lowers predicted expected excess returns, but it may also result in negative expected excess returns when competition is particularly intense. This pattern aligns with the empirical characterization of booms and busts in Hoberg and Phillips (2010) and the theoretical implications of DeMarzo et al. (2007).====We investigate extensions of the model, several of which amplify our baseline results. For instance, when the new technology is subject to obsolescence (Aghion and Howitt, 1992), Schumpeterian competition leads to a higher degree of over-experimentation than in our baseline setup. Additionally, we expand the representative agent model to include the option for agents to change their experimentation over time. This allows agents to abandon a technology if it is determined to be non-value enhancing. This option for abandonment increases experimentation, which in turn increases risk as agents may experiment more knowing that they have the option to back off.====To determine the effect of Schumpeterian competition on residual claimants in the economy, we incorporate passive agents into our model. These agents do not participate in competition for market share, but instead are residual claimants of the consumption stream. Our findings indicate that the welfare of passive agents decreases as competition increases. Not only do they receive a smaller portion of the consumption stream due to over-experimentation, but the aggregate value of the consumption stream also decreases. Thus, our model suggests that creative destruction may exacerbate income inequality, particularly with higher levels of competition (e.g., Jones and Kim, 2018). Finally, our analysis of a setting with recursive preferences reveals that learning by doing endogenously creates long-run risk (Bansal and Yaron, 2004), aligning with a growing literature that provides equilibrium foundations of long-run risk through innovation and R&D (Kung and Schmid, 2015; Kung, 2015).==== Our paper is closest to Pastor and Veronesi (2009). In their model, learning about the new technology takes place with an infinitesimally small consumption stream, so the new technology does not perturb the rest of the economy. Once the new technology becomes sufficiently promising, there is an instantaneous transition in which the new consumption stream displaces the status quo. Prices rise and then fall as risk changes from idiosyncratic to systematic. Our work complements theirs. First, we consider that learning by doing (Arrow, 1962) does disrupt the status quo and there is an observer effect. Second, because of our game-theoretic framework, we can consider the effect of competition on risk, wealth, and prices. Third, both technological uncertainty and systematic risk evolve continuously, so that we can evaluate their effects over the long term. Last, because we model game-theoretic claims to a consumption stream, we can show how the term structure of those claims changes and affects volatility and risk premia.====In a related paper, DeMarzo et al. (2007) develop a model of relative wealth concerns where risky technologies attract excessive investment that can be largely unprofitable, which they argue is consistent with a real investment bubble. In our setting with Schumpeterian competition, excess investment arises in a rent-seeking contest. So, we extend the analysis in DeMarzo et al. (2007) on two dimensions. First, we establish a novel link between excessive real investment and asset price bubbles. As over-investment magnifies technological uncertainty, it leads to price paths consistent with boom-bust patterns and transient episodes of negative expected excess returns. Second, our model shows how over-investment may worsen wealth inequality and expose agents who do not actively invest in the new technology to excessive risks.====Our study builds on the seminal contribution of Tullock (1980) and a number of papers that have explored various aspects of rent-seeking competitions.==== We contribute to this literature by considering a game in which the aggregate prize changes non-linearly with effort. This is a key aspect of our analysis, whose outcome is that agents over-experiment despite the fact that the aggregate prize decreases, because their incentives are mainly focused on their own share of the prize. Furthermore, because we place the Tullock contest into a general equilibrium asset pricing setting with heterogeneous agents, we can consider the effect of competition on systematic risk, social welfare, and wealth inequality.====Our model relates to stochastic variants of Schumpeterian growth models (Barlevy, 2007; Kung, 2015; Kung and Schmid, 2015), which provide microfoundations for the way innovation impacts economic growth and its fluctuations. Our framework builds on these microfoundations and analyzes the effect of competition and technological uncertainty on risk and asset prices. The long term effects of the model provide a new perspective on how competition may drive secular trends related to innovation and risk premia. Recent studies have attributed slumping investment and innovation (Gutiérrez and Philippon, 2018) and rising risk premia in equity markets (Corhay et al., 2020) to declining competition, an observation that aligns well with the predictions of our model. Finally, through its effect on duration and risk, experimentation changes the term structure of risk and implies lower returns for long-duration assets, consistent with a recent literature that studies the risk premia of equity claims with different maturities (Lettau and Wachter, 2007, Lettau and Wachter, 2011; Weber, 2018; Van Binsbergen, 2020; Gonçalves, 2021).====The rest of the paper proceeds as follows. Section 2 poses the model, characterizes learning and socially optimal experimentation, and contrasts the equilibrium behavior of competitive agents to the social optimum. Section 3 characterizes the risks of Schumpeterian competition and its implications for asset prices and return predictability. Section 4 discusses various extensions of the model. Section 5 concludes. All proofs and additional calculations are in the Appendix.",Schumpeterian competition in a Lucas economy,https://www.sciencedirect.com/science/article/pii/S0022053123000091,27 January 2023,2023,Research Article,35.0
"Mookherjee Dilip,Tsumagari Masatoshi","Department of Economics, Boston University, 270 Bay State Road Boston MA 02215, United States of America,Department of Economics, Keio University, 2-15-45 Mita Minato-ku Tokyo 108-8345, Japan","Received 21 August 2021, Revised 13 January 2023, Accepted 22 January 2023, Available online 27 January 2023, Version of Record 7 February 2023.",https://doi.org/10.1016/j.jet.2023.105614,Cited by (0),"We study regulatory mechanism design with collusion between a privately informed agent and a less well-informed supervisor, incorporating ‘extortion’ which permits redistribution of rents within the coalition. We show the Collusion Proof Principle holds, and that the allocation of bargaining power between the supervisor and agent matters. Specifically, the Principal does not benefit from hiring the supervisor if the latter has less bargaining power vis-a-vis the agent. We provide an example where hiring the supervisor is valuable if she has greater bargaining power. These results indicate the importance of anti-collusion strategies that augment bargaining power of supervisors vis-a-vis agents.","The design of mechanisms to limit the harmful effects of collusion between supervisors and agents in adverse selection settings has been studied by many authors following Tirole (1986) and Laffont and Tirole (1993), with many applications to design of procurement, regulation and internal organization of firms. Subsequent literature has explored the implications of enlarging the severity of the collusion problem, such as soft information which provides wider scope for manipulation of reports (Faure-Grimaud et al. (2003), Celik (2009)), and collusion over both reporting and participation decisions (Mookherjee et al. (2020)).====All these papers however assume that collusion is ‘weak’ in the sense that the supervisor and agent play non-cooperatively if either vetoes the offered side-contract. This ensures collusion occurs only to realize joint gains for the colluding parties. The allocation of bargaining power within the coalition then does not matter for the Principal's capacity to control corruption. This implication of weak collusion follows from the Collusion Proof Principle (CPP) which applies quite generally in this class of models (Tirole (1992)).==== It is important to note that the irrelevance of bargaining power (among parties engaging in weak collusion) applies more generally than conditions usually needed for the Coase Theorem to apply: for instance, bargaining takes place with asymmetric information between the supervisor and agent. These models are therefore incapable of explaining the observed success of anti-corruption policies which lowered bargaining power of the agent vis-a-vis the supervisor by preventing agents from selecting their own auditor in India (Duflo et al. (2013)) and Italy (Vanutelli (2020)).====In this paper we argue that a strengthening of the nature of collusion to allow for ‘extortion’ can explain why relative bargaining power of the supervisor could matter. Extortion permits the colluding partner with greater bargaining power to extract rents from the other party, by threatening to send reports to the Principal which would hurt the latter if (s)he did not agree to the offered side-contract. Colluding parties can commit to such threats at the time of bargaining. This alters the sub-game following refusal of an offered side-contract by one party, from a simultaneous move noncooperative game, to one where the report of the accepting party is stipulated by the side-contract (with the other party choosing a best response). Earlier work by Dequiedt (2007) and Che and Kim (2009) has studied an analogous notion in an auction design setting, using the term ‘strong collusion’. As some other authors (e.g., Quesada (2003)) have used the term ‘strong collusion’ to mean something different, we refer to the combination of extortion and collusion as ‘extortionary collusion’.====While we do not endogenize the source of commitment power, we have in mind settings where S and A interact with one another in many other transactions with many principals, either at the same time or at later dates, and reputational concerns provide the required enforcement.==== It should be added that this is consistent with the rest of the mechanism design literature which examines the consequence of alternative assumptions regarding commitment among players, rather than providing an explicit microfoundation for these assumptions. The existing literature which has studied weak collusion is based on a specific assumption of lack of commitment among colluding players. Here we are interested in understanding the consequence of the opposite assumption where they can commit to reporting threats when they fail to agree on a side-contract.====In a law review article, Ayres (1997) refers to ‘bribery’ and ‘extortion’ as the ‘twin faces of corruption’. In the language of mechanism design theory, bribery corresponds to weak collusion, while the combination of bribery and extortion corresponds to extortionary collusion. The importance of extortion has been stressed by numerous authors in descriptive accounts of corruption, from medieval England (Cam (1930)), to more contemporary accounts of corruption in Burma (Furnivall (1956)) or other developing countries (Klitgaard (1988)). Ayres (1997) and Andrianova and Melissas (2008) discuss extortion from a legal standpoint. Some papers studying tax evasion, regulations or intra-firm organization in a moral hazard setting (Mookherjee (1997), Hindricks et al. (1999), Khalil et al. (2010)) have shown that anti-corruption policy design is significantly altered by the presence of extortion. By contrast our focus is on an adverse selection context, where the modeling issues as well as results are quite different. Section 2 provides a more detailed discussion of the relation to existing literature.====Section 3 describes the model of extortionary collusion in which S obtains a noisy signal of A's cost, and A also observes the realization of this signal. A and S enter into a collusive side contract at the ex ante stage, when neither have received their respective signals. Moreover, the allocation of bargaining power between the supervisor (S) and agent (A) is given and known by the Principal (P). The primary question studied in this paper is how relative bargaining power matters in the presence of extortion. In some contexts P may have no control over the allocation of bargaining power between S and A. Our results will show that the consequences of extortion on P's welfare and the value of hiring S vary with bargaining power allocation. In other contexts P may be able to influence bargaining power: in these settings our results imply that raising relative bargaining power of S can be an important policy instrument.====For instance, P may be able to influence the process by which S and A are appointed, which affects their relative bargaining power. Consider a setting where A is allowed to appoint her own supervisor, from a set of potential supervisors. This would give A the opportunity to exercise monopsony power over S in the choice of side contract, resulting in a higher welfare weight on A's payoff. An alternative institutional rule (as in the policy reforms in India and Italy studied by Duflo et al. (2013) and Vanutelli (2020)) is one where the supervisor is selected by P instead, and assigned to a given agent. This would alter the side contract negotiation to a bilateral monopoly where bargaining power is more equally divided between S and A. In some settings (e.g., in construction, or procurement of a particular service) there could be many potential agents available to carry out the project for P, where P could appoint a supervisor S and delegate the choice of A to S. This would confer monopoly power to S over A.====Given a certain allocation of bargaining power within the coalition, Section 4 verifies that the Collusion Proof Principle continues to apply in a extortionary collusion setting, once message spaces are augmented to include some non-type messages. This result is of some independent interest, as it contrasts with the moral hazard setting studied by Khalil et al. (2010). Other differences in our results from the moral hazard setting are described in Section 2. The result is used to characterize the class of feasible allocations in terms of a set of coalition incentive compatibility constraints.====This characterization is used in Section 5 to provide results for alternative ranges of bargaining power allocation. First, if A has a greater bargaining power than S,==== appointing a supervisor is worthless for P (Proposition 2). To explain the underlying intuition, consider the case where A has all the bargaining power, and the side-contract is designed to maximize A's ex ante payoff subject to S's participation constraint. Since A knows the realization of S's signal as well as the latter's outside option, there is no asymmetric information within the coalition. The resulting absence of bargaining frictions allows A to extort all of S's rents without creating any distortion, implying that the coalition behaves in a manner that maximizes their joint payoff. In effect, A becomes the residual claimant to the coalition's joint surplus and the presence of S becomes redundant (by having no choice other than to ‘rubber-stamp’ whatever A wants to report). This logic extends to the case where A has greater (but not all) the bargaining power.====Turning next to contexts where S has greater bargaining power, we provide an example to illustrate that hiring S can be valuable to P. Intuitively, the main difference from contexts where A has higher bargaining power is that the ‘direction’ of extortion is reversed: the side-contract is now designed to transfer rents from A to S and is subject to asymmetric information constraints (owing to A's private information regarding the realization of production cost). The associated bargaining frictions resemble ‘sand in the wheels of corruption’ which benefits P. The example features procurement of an indivisible good by P, where S's signal has two possible realizations and satisfies a monotone likelihood property. We construct a specific mechanism where hiring S enables P to achieve a higher payoff. An earlier working paper version (Mookherjee and Tsumagari (2022)) explores the corresponding optimal contracting problem with extortionary collusion, and shows that (conditional on S being valuable) the value of hiring S is globally decreasing in S's bargaining power over the range where S has greater bargaining power.====The concluding Section discusses consequences of extending the model in different directions, while the Appendix provides details of proofs omitted from the text.",Regulatory mechanism design with extortionary collusion,https://www.sciencedirect.com/science/article/pii/S0022053123000108,27 January 2023,2023,Research Article,36.0
Roy Nilanjan,"Department of Economics and Finance, College of Business, City University of Hong Kong, 83 Tat Chee Avenue, Kowloon Tong, Hong Kong","Received 3 July 2020, Revised 9 December 2022, Accepted 16 January 2023, Available online 20 January 2023, Version of Record 30 January 2023.",https://doi.org/10.1016/j.jet.2023.105611,Cited by (0), is supported.,"In various strategic interactions, actions chosen by the players are usually the result of a long preparation process. Static models fail to capture the underlying and often complex dynamics that are generally present in the deliberation process before players finally settle for an action choice. In this paper, we analyze a dynamic model of quantity competition, where firms can continuously adjust their production targets during a revision phase that precedes the play of the game.====Revision of quantity choices happens in the production industry where firms compete in quantity.==== Although production is made at the end, several factors might influence the final quantity of output produced by the firms. Demand and cost uncertainty, information about competitors' planned production, capacity constraints, and other reasons might make the final choice of production different from the initial plans. We use a laboratory experiment to investigate the impact of introducing action revision on market competitiveness in a Cournot duopoly. We study two types of games that differ in the technology of implementation of revised actions:====A real-time revision game implements intended actions without any imperfection. On the other hand, a stochastic revision game provides a stylized representation of situations where communication and implementation occur at different times, possibly due to delays. It models the inefficiencies in implementing the desired choices. These games can be used to model several economic and social relations. For example, in international negotiations, parties prepare proposals and counter-proposals but only the final draft of the agreement signed at the end matters. Another example is the preopening period in some stock markets, where traders can submit orders that can be withdrawn and changed until the opening time. Only the orders recorded in the system at the opening time are binding and payoff relevant.====Stochastic revision games are theoretically studied in Kamada and Kandori (2020), where players prepare actions at the beginning and obtain revision opportunities according to a Poisson process. When a revision opportunity arises, players simultaneously revise their actions. The authors show that adding the revision phase can widen the set of achievable payoffs compared to the static game without any such pre-play mechanism. They characterize a class of symmetric equilibria that can sustain partial cooperation, which, when applied to our Cournot duopoly game, can attain more than 97% of collusive profits.====The primary objective of our study is to explore the efficacy of this novel pre-play institution in fostering cooperation in a situation in which it would otherwise be impossible to observe such cooperation and to understand how this mechanism works in practice. We simulate the experimental environment as closely as possible to the theoretical setup of Kamada and Kandori (2020), including synchronous stochastic revisions, so that the arguments for the equilibria apply to our experiments.====Consistent with the previous literature, we find that markets with no pre-play communication quickly converge to the Cournot-Nash equilibrium. However, when participants can revise their actions in real-time with perfect observability of rival's revisions, quantity choices are more competitive than the Cournot-Nash equilibrium output. Markets with real-time revision are characterized by sharp upward quantity adjustments as the revision phase nears the end, and best response behavior accounts for this observation.====On the other hand, stochastic revision games can implement individual quantities significantly lower than those selected in the other markets, with modal individual quantity equaling the symmetric joint profit-maximizing output. In the later half of the experiment, close to 95% of the collusive profits are realized in these markets, and almost a third of the markets attain perfect collusion. Thus, our findings demonstrate that this novel pre-play mechanism can sustain partial collusion in an environment where it is otherwise impossible to support quantity choices lower than the static Cournot-Nash output.====Although the positive effect of stochastic revision of actions on collusion is in line with the theoretical prediction of Kamada and Kandori (2020), group-level dynamics show that instances of one or both players in a group choosing action revision paths that are consistent with the equilibria described in their model are quite low when the entire revision phase is taken into account. However, such instances go up when the analysis is restricted to the second half or the final quarter of the stochastic revision phase.====The average dynamics of quantity adjustment reveal why lower quantities are implemented with a stochastic revision phase. Prepared quantities start at high levels before starting to decline as the revision phase progresses. As the revision period nears an end, quantities are adjusted upward and approach the Cournot-Nash output. Despite substantial heterogeneity at the group level, the stochastic revision game allows individuals to prepare quantities in the collusive zone, that is, between the symmetric joint profit-maximizing output and the Cournot-Nash quantity. Even though individuals revise their quantities upward toward the end of the revision phase, the stochastic nature of the revision mechanism results in the implementation of quantities from the collusive zone, thereby supporting partial collusion.====If one is willing to broadly interpret the theoretical model of Kamada and Kandori (2020), we find evidence supporting the central intuition of the theory of revision games: players spend a significant amount of time in the collusive zone with the introduction of stochastic revision, and these prepared quantities could be implemented with positive (exogenous) probabilities. Such a theoretical possibility does not arise in any other mechanism.====The Cournot duopoly game can be used to study cooperation between two individuals, similar to the prisoner's dilemma game.==== There is a tension between what is efficient for the industry and what is individually best, given that the other firm chooses the cooperative action. This allows us to interpret our main finding more generally as follows: in the absence of explicit communication, a pre-play mechanism based on a sequence of random revision opportunities can promote cooperation where the static Nash equilibrium predicts no cooperation.====Our study is among the early ones to implement games with random revision opportunities in the laboratory.==== Avoyan and Ramos (2021) confirm the prediction of Calcagno et al. (2014) that a stochastic revision phase can narrow down the set of achievable payoffs in games of coordination and demonstrate that the efficient outcome is uniquely selected in the extended minimum-effort game. On the other hand, our paper investigates the efficacy of a stochastic revision phase in promoting collusion in a game of quantity competition.====The remainder of the paper is organized as follows. Section 2 presents the model, treatments, and hypotheses. Section 3 lays out the laboratory methods and procedures. In section 4, we present the results, while section 5 provides a discussion of the results and our contribution to the existing literature. The last section concludes.",Fostering collusion through action revision in duopolies,https://www.sciencedirect.com/science/article/pii/S0022053123000078,20 January 2023,2023,Research Article,37.0
"Hébert Benjamin,Woodford Michael","Stanford University, United States of America,Columbia University, United States of America","Received 25 March 2020, Revised 13 January 2023, Accepted 16 January 2023, Available online 20 January 2023, Version of Record 6 February 2023.",https://doi.org/10.1016/j.jet.2023.105612,Cited by (1),"Decisions take time, and the time taken to reach a decision is likely to be informative about the cost of more precise judgments. We formalize this insight using a dynamic model of optimal evidence accumulation. We provide conditions under which the resulting belief dynamics resemble either diffusion processes or processes with large jumps. We then consider the in which discounting effects are small relative to the opportunity cost of time, and show that the state-contingent choice probabilities predicted by our model are identical to those predicted by a static rational inattention model, providing a micro-foundation for such models. In the diffusion case, our model provides a normative foundation for a variant of the drift-diffusion model from mathematical psychology.","It is common in economic modeling to assume that, when presented with a choice set, a decision maker (DM) will choose the option that is ranked highest according to a coherent preference ordering. However, observed choices in experimental settings often appear to be random, and while this could reflect random variation in preferences, it is often more sensible to view choice as imprecise. Models of rational inattention (such as Matêjka et al. (2015)) formalize this idea by assuming that the DM chooses her action based on a signal that provides only an imperfect indication of the true state. The information structure that generates this signal is optimal, in the sense of allowing the best possible joint distribution of states and actions, net of a cost of information. In the terminology of Caplin and Dean (2015), models of rational inattention make predictions about patterns of state-dependent stochastic choice. These predictions will depend in part on the nature of the information cost, and several recent papers have attempted to recover information costs from observed behavior in laboratory experiments (Caplin and Dean (2015); Dean and Neligh (2019)).====However, in both laboratory experiments and real-world economic settings, decisions take time, and the time required to make a decision is likely to be informative about the nature of information costs.==== In this paper, we develop a framework to study rational inattention problems in which decisions take time, providing a means of connecting decision times to information costs and state-dependent stochastic choice. Our analysis focuses on the question of whether beliefs will evolve continuously, in a single jump, or as a sequence of smaller jumps. We first show that the answer to this question depends on the nature of the assumed information costs, and then show that the distribution of decision times will in turn depend on how the belief process evolves.====There is an extensive literature in mathematical psychology that focuses on these issues. Variants of the drift-diffusion model (DDM, Ratcliff (1985), Ratcliff and Rouder (1998), Wagenmakers et al. (2007)) also make predictions about stopping times and state-dependent stochastic choice.==== In particular, these models are designed to match the empirical observation that hasty decisions are likely to be of lower quality.==== However, these models are not based on optimizing behavior, and this raises a question as to the extent to which they can be regarded as structural; it is unclear how the parameters of the DDM model should be expected to change when incentives or the costs of delay change, and this limits the use of the model for making counter-factual predictions. The framework we develop includes as a special case variants of the DDM model, while at the same time making predictions about state-dependent stochastic choice that, in some cases, match those of a static rational inattention (RI) model. Consequently, our framework is able to both speak to the relationship between stopping times and state-dependent stochastic choice (unlike standard RI models) and make counter-factual predictions (unlike standard DDM models).====We propose a class of rational inattention models in which the DM's imprecise perception of the decision problem evolves over time, and an optimization problem determines a joint probability distribution over stopping times and choices. We introduce two properties of information cost functions, which we call a “preference for gradual learning” and a “preference for discrete learning.” We then show that a (sufficiently strong) preference for gradual learning leads to dynamics for the belief state prior to stopping that resemble a pure diffusion (as assumed in the DDM), and that a preference for discrete learning can lead to belief dynamics involving a single large jump. The exact diffusion result holds only in a limiting case, in which discounting effects are negligible relative to the opportunity cost of time. Away from this limit, beliefs will follow a pure jump process, as shown by Zhong (2022). In this more general case, with a sufficiently strong preference for gradual learning, beliefs evolve as a series of jumps, approximating a diffusion in the aforementioned limit. In contrast, with a preference for discrete learning, there are optimal policies in which the DM acts immediately after the first jump.====Our model is particularly tractable under the limiting assumption that discounting effects are negligible relative to the opportunity cost of time. In this case, beliefs follow a Markov process and move in a space whose dimensionality is one less than the number of actions (e.g., a line in the case of a binary decision problem, as assumed in the DDM). Our results therefore contribute to the literature on DDM-style models by presenting a model with many features of the DDM that also — because it is developed as an optimizing model — makes predictions about how decision boundaries and choice probabilities should change in response to changes in incentives.====We also characterize the boundaries of the stopping regions and the predicted ex ante probabilities of different actions in this limiting case, as functions of model parameters including the opportunity cost of time. The key to this characterization is a demonstration that the resulting state-dependent stochastic choice probabilities of our continuous-time model are equivalent to those of a static RI model. Thus in addition to providing foundations for interest in DDM-like models of the decision process, our paper provides novel foundations for interest in static RI problems. For example, we provide conditions under which the predictions of our model will be equivalent to those of a static RI model with the mutual-information cost function proposed by Sims (2010) — and thus equivalent to the model of stochastic choice analyzed by Matêjka et al. (2015) — but the foundations that we provide for this model do not rely on an analogy with rate-distortion theory in communications engineering (the original motivation for the proposal of Sims).====More generally, as noted above, we show that any cost function for a static RI model in the uniformly posterior-separable family studied by Caplin et al. (2022) can be justified by the process of sequential evidence accumulation that we describe. This includes the neighborhood-based cost functions discussed in Hébert and Woodford (2021a), that lead to predictions that differ from those of the mutual-information cost function in ways that arguably better resemble the behavior observed in experiments such as those of Dean and Neligh (2019). Our result provides both a justification for using such cost functions in static RI problems, and an answer (not given by static RI theory alone) to the question of how the cost function should change as the opportunity cost of time changes.====The connection that we establish between the choice probabilities implied by a dynamic model of optimal evidence accumulation and those implied by an equivalent static RI model holds both in the case that the belief dynamics in the dynamic model are described by a pure diffusion process and in the case that they are described by a jump process; thus we also show that with regard to these particular predictions, these two types of dynamic models are equivalent. However, the predictions of the two types of model differ with regard to the distribution of decision times, so that it is possible in principle to use empirical evidence to determine which better describes actual decision making.====The key to our analysis is a continuous-time model of optimal evidence accumulation, in which beliefs are martingales (as implied by Bayes' rule). The evolution of beliefs in our model is limited only by a constraint on the rate of information arrival, specified in terms of a posterior-separable cost function. This flexibility is consistent with the spirit of the literature on rational inattention, but with some noteworthy differences. Much of the previous literature considers a static problem, in which a decision is made after a single noisy signal is obtained by the DM. This allows the set of possible signals to be identified with the set of possible decisions, which is no longer true in our dynamic setting.====Steiner et al. (2017) also discuss a dynamic model of rational inattention. In their model, because of the assumed information cost, it is never optimal to acquire information other than that required for the current action. As a result, in each period of their discrete-time model, the set of possible signals can again be identified with the possible actions at that time. We instead consider situations in which evidence is accumulated over time before any action is taken, as in the DDM; this requires us to model the stochastic evolution of a belief state that is not simply an element of the set of possible actions.==== Our central concerns are to study the conditions under which the resulting continuous-time model of optimal information sampling gives rise to belief dynamics and stochastic choices similar to those implied by a DDM-like model, and to study how variations in the opportunity cost of time or the payoffs of actions should affect stochastic choice.====A number of prior papers have endogenized aspects of a DDM-like process. Moscarini and Smith (2001) consider both the optimal intensity of information sampling per unit of time and the optimal stopping problem, when the only possible kind of information is given by the sample path of a Brownian motion with a drift that depends on the unknown state, as assumed in the DDM.==== Fudenberg et al. (2018) consider a variant of this problem with a continuum of possible states, and an exogenously fixed sampling intensity.==== Woodford (2014) takes as given the kind of stopping rule posited by the DDM, but allows a very flexible choice of the information sampling process, as in theories of rational inattention. Our approach differs from these earlier efforts in seeking to endogenize ==== the nature of the information that is sampled at each stage of the evidence accumulation process and the stopping rule that determines how much evidence is collected before a decision is made.====Section 2 introduces our continuous-time evidence-accumulation problem, and presents some preliminary results. In section 3, we define two special conditions that information costs may satisfy: a “preference for gradual learning” or a “preference for discrete learning.” These properties represent the conditions under which we can show that the optimal belief dynamics will evolve either as a sequence of bounded jumps (a diffusion in the limit case) or a single jump. In section 4 we demonstrate that the state-dependent choice probabilities predicted by our model in the limit case are equivalent to those predicted by a static rational inattention model with a uniformly posterior-separable cost function. In section 5 we discuss how the diffusion and jump cases can nonetheless be distinguished using data on response times. Section 6 concludes.",Rational inattention when decisions take time,https://www.sciencedirect.com/science/article/pii/S002205312300008X,20 January 2023,2023,Research Article,38.0
"Cetemen Doruk,Feng Felix Zhiyu,Urgun Can","City, University of London, United Kingdom,University of Washington, United States of America,Princeton University, United States of America","Received 31 January 2021, Revised 5 September 2022, Accepted 1 January 2023, Available online 11 January 2023, Version of Record 20 January 2023.",https://doi.org/10.1016/j.jet.2023.105606,Cited by (1),"This paper studies a continuous-time, finite-horizon contracting problem with renegotiation and dynamic inconsistency arising from non-exponential discounting. The problem is formulated as a dynamic game played among the agent, the principal and their respective future “selves”, each with their own discount function. We identify the principal optimal renegotiation-proof contract as a Markov perfect equilibrium (MPE) of the game, prove that such an MPE exists, and characterize the optimal contract via an extended Hamilton-Jacobi-Bellman system. We solve the optimal contract in closed-form when discounting is a function of the time-difference only and demonstrate the applicability of the results in several different settings.","Dynamic contracting explores how two parties can use inter-temporal incentives to mitigate agency frictions. Most existing contracting models assume that inter-temporal preferences take the special form of exponential discounting because of its technical conveniences, such as dynamic consistency and stationarity. However, there is extensive evidence – both anecdotal and empirical – that “time preferences can be non-exponential” (Laibson, 1997). Laboratory and field studies find that discount rates are much greater in the short-run than in the long-run (Harris and Laibson, 2012). When a group of decision makers (such as a board of directors, team, or committee of experts) follow unanimous decisions, they can collectively exhibit non-exponential, present-biased behaviors even when each individual follows exponential discounting (Jackson and Yariv, 2015). Therefore, it is natural to ask: how would dynamic contracting be different with non-exponential discounting?====This paper aims to provide a general framework to answer this question. We begin with a standard continuous-time dynamic moral hazard model in which a principal hires an agent to manage a project over a finite horizon. The project's outcome is noisy, and the agent controls its drift with private actions. We include three critical elements in this general framework: first, we allow the principal and the agent to have different, generic, time-varying, and non-exponential discount functions. Second, the principal and the agent are fully rational and sophisticated: they make decisions knowing exactly how their future preferences will change due to non-exponential discounting. Third, renegotiation is allowed as long as both parties can agree on the proposed change of terms. An example of this setting is the contracting between a CEO and a board of directors. At the onset of their relationship, the two parties consent to a long-term contract that provides the CEO with incentives to exert the effort that is beneficial to the firm. At any point in time, the board can renegotiate by offering a new contract that replaces the old one if the CEO agrees. If the CEO disagrees, the old contract stays in place, and the same protocol applies to any future possible renegotiation.====The importance of considering sophisticated contracting parties and renegotiation stems from the fact that non-exponential discounting generates ====: a contract that appears “optimal” to the board or the manager given their current preferences may appear sub-optimal in the future, not because of any inefficient punishments or constraints, but simply due to changes in their discounting. If either the manager or the board is naive, or if renegotiation is not allowed, then one party may regret having previously agreed to the contracting terms at some point, calling into question whether the previously agreed-on contract was actually “optimal”. Instead, we assume that both contracting parties are sophisticated and the contract can be renegotiated, and we provide an intuitive definition for a ==== under such a setting. In particular, the dynamic contracting relationship can be formulated as an inter- and intra-personal game, in which each player is a ==== of the principal or the agent and the optimal, renegotiation-proof contract corresponds to an appropriately defined equilibrium of the game. By exploiting the link between dynamic contracting and static non-atomic games, we formally characterize the optimal, renegotiation-proof contract and prove its existence under fairly general conditions.====Our first finding is that under our framework, the dynamic inconsistency on the agent's side turns out to have much less impact than a time-inconsistent principal does. Despite the fact that the agent is also playing an intra-personal game with his future selves, his incentive compatibility condition is characterized by a local pay-performance sensitivity similar to that in time-consistent benchmarks (e.g., Sannikov, 2008). The principal only needs to provide incentives for the current agent and not his future selves. This provides a much-needed simplification, allowing us to take the promised equilibrium value to the agent as a state variable and focus on the effect of dynamic inconsistency on the principal's side.====With the agent's problem solved, we present our main theorem characterizing the incentive-compatible, renegotiation-proof, optimal long-term contract with as broad a generality as possible. The theorem includes two innovations. First, we demonstrate how to characterize the optimal contract using recursive techniques when the usual tool of Bellman equation is no longer applicable. We argue that given the agent's local incentive compatibility and our notion of renegotiation-proofness, the optimal contract is equivalent to the equilibrium of a simpler auxiliary game in which each ====-self of the principal can only influence the agent's action, consumption, and the evolution of continuation utility at time ====. The recursive characterization yields an ==== with additional terms compared to the usual HJB equation for time-consistent benchmarks. These additional terms capture the equilibrium incentives of the principal, who takes into account the impact of the contract policies on the payoff for all her future selves.====The second innovation of our main theorem is the proof that such an optimal contract exists. This is known to be a thorny problem because the well-posedness of the extended HJB system is not well understood even in the mathematical literature.==== We prove that such a system must have a solution by exploiting the unique connection between partial differential equations and static non-atomic games of incomplete information. This novel connection enables us to utilize known existence results in non-atomic games to establish the existence of a solution to our extended HJB system. We also show that any equilibrium of the principal's auxiliary game must also be a contract that solves the extended HJB system. That is, our characterization of the optimal contract is complete. The generality of our framework opens the door for a broad range of applications.====We demonstrate the applicability of our general framework by solving a more specific class of contracting problems. For simplicity, we assume the agent has constant-absolute-risk-aversion (CARA) utility and exponential discounting, and the set of discount functions of the principal belongs to a ====: i.e., the discount factor between any two dates ==== is only a function of their difference ====. This applies to some of the most commonly-studied examples of non-exponential discounting in economics, such as hyperbolic discounting and anticipatory utility. Importantly, we are able to obtain closed-form solutions for this class of simplified problems and highlight the specific effects brought by non-exponential discounting (on the principal's side). In particular, we illustrate a “deadline” effect and the resulting non-monotonic paths of equilibrium actions and incentives under ==== discounting (as in Harris and Laibson, 2012).==== We also demonstrate how these theoretical predictions can help reconcile the observed managerial compensation practices, such as the incentive power of the CEOs as they approach retirement.==== This paper is broadly related to several strands of research. First, it belongs to the thriving literature of continuous-time dynamic contracting. Benchmark models with time-consistent preferences such as DeMarzo and Sannikov (2006), Biais et al. (2007), and Sannikov (2008) demonstrate the analytical convenience of a continuous-time formulation, which allows the derivation of economic insights from problems that are otherwise challenging to solve in discrete time.==== Our paper differs in that we explicitly model dynamic inconsistency from non-exponential discounting for both the principal and the agent. Moreover, dynamic inconsistency necessitates a formal discussion of contract renegotiation, which is often ruled out or assumed away in benchmark models that assume full commitment power for the principal.====Our definition and analysis of renegotiation are related to both contract renegotiation in dynamic moral hazard problems as well as equilibrium renegotiation in dynamic games. In particular, we adopt the same procedure of renegotiation as that considered in Watson et al. (2019): once a renegotiation is proposed, if that proposal is rejected, the previous contract remains in place. The setting of Watson et al. (2019) is a more general principal-agent setup that incorporates an explicit renegotiation phase in which both parties have different bargaining power. In comparison, we make the simplifying assumption that the principal has full bargaining power. In that regard, our model also resembles Fudenberg and Tirole (1990), who explore contract renegotiation in a one-time setting with the principal having full bargaining power. In addition, our renegotiation-proof contract is characterized as a Markov perfect equilibrium, which is closely related to the notion of internal consistency in Bernheim and Ray (1989) for repeated games as well as Ray (1994) and Strulovici (2020) for dynamic games.====Our study is also related to contracting problems with behavioral preferences.==== In particular, we assume the contracting parties are sophisticated regarding their time-inconsistency. This resembles Galperti (2015), which focuses on the optimal provision of commitment devices with sophisticated agents. In contrast, a substantial number of studies on behavioral preferences assume the contracting parties are naive or partially naive. For example, Gottlieb (2008) studies the optimal design of non-exclusive contracts and identifies different implications of immediate-cost goods and immediate-reward goods for dynamically inconsistent but naive consumers. Gottlieb and Zhang (2021) study repeated contracting between a risk neutral firm and dynamically inconsistent but partially naive consumers, and find that at-will terminations may improve welfare if the level of dynamic inconsistency is sufficiently high. These studies focus on adverse selection problems (reporting), while we focus on a moral hazard (hidden effort) problem. Moreover, DellaVigna and Malmendier (2004) analyze the optimal two-part tariff of a firm facing a partially naive consumer with present-biased preferences. Heidhues and Kőszegi (2010) study a similar setting with naive agents and show that simple restrictions on contract forms may significantly improve welfare.====In general, the idea that preferences may be dynamically inconsistent as the result of non-exponential discounting can be traced to early work of Strotz (1955) and Pollak (1968). Since then, many studies have explored dynamically inconsistent preferences in various settings: including consumption-saving problems (Laibson, 1997; Krusell and Smith, 2003; Bernheim et al., 2015; Ray et al., 2017; Bond and Sigurdsson, 2018; Cao and Werning, 2018); investment and asset allocation (Caplin and Leahy, 2001; Grenadier and Wang, 2007; Brunnermeier et al., 2016); monetary policy (Kydland and Prescott, 1977); fiscal policy (Halac and Yared, 2014); procrastination (O'Donoghue and Rabin, 1999), public finance (Bisin et al., 2015; Harstad, 2016), etc. These studies are typically limited to a single party being dynamically inconsistent. In particular, Li et al. (2016), Liu et al. (2017), Wang et al. (2020) and Rivera (2022) expand standard dynamic moral hazard settings (DeMarzo and Sannikov, 2006; DeMarzo et al., 2012; and Sannikov, 2008, respectively) to explore the differences in the optimal contract if the agent has quasi-hyperbolic discounting (while the principal still has normal exponential discounting). Consistent with what we demonstrate in our paper, time-inconsistency does not significantly affect the solution to the agent's problem: the incentive compatibility conditions in these studies are identical to their respective benchmarks. In contrast, we focus on the case in which the principal is time-inconsistent. This makes the contracting problem significantly different, and consequently much more involved. One exception in this line of research is Chade et al. (2008), who provide a recursive characterization of a repeated game in which all players share the same quasi-hyperbolic discount function. In comparison, we allow generic, non-exponential, and potentially different discounting for all players, including quasi-hyperbolic discounting as a special case.====Our paper also synthesizes and utilizes some of the latest advancements in mathematical finance research. We use the extended HJB system developed in Björk et al. (2017) to characterize the equilibrium strategies in time-inconsistent problems. However, Björk et al. (2017) studies a single-agent problem without moral hazard or contracting, and leaves the existence of the extended HJB system as an open question.==== Our solution technique partly follows Yan and Yong (2019), which offers two different but related approaches: the ==== strategy which stems from the stochastic maximum principle, and the ==== strategy which discretizes time into a mesh and defines the equilibrium as the individual mesh size goes to zero. The former approach yields solutions with known properties such as existence but are difficult to interpret or directly utilize. The latter approach is more natural in most economics settings and can lead to HJB equations with Markov controls. However, Yan and Yong (2019) notes that when the diffusion terms are also part of the controls, as in the case of dynamic contracting problems, the limit of such HJB equations as the mesh size goes to zero is not necessarily well posed. Thus, the existence of the solution is not always guaranteed.====Our paper is the first that integrates these individual results into a principal-agent model and solves a general renegotiation-proof optimal contract with non-exponential discounting. We also expand this literature with novel analytical results that are potentially applicable to future studies of related topics. First, we demonstrate how the extended HJB system in Björk et al. (2017) and the solution techniques of Yan and Yong (2019) can be applied when the diffusion terms of the state variables (from the solutions to the agent's problem) are also part of the (principal's) controls.==== Second, we prove the existence of the solution to the extended HJB system by exploiting the connection between differential equations and non-atomic games of incomplete information. In fact, the novelty of this proof of existence is independent of the specific contractual setting used in the paper and can thus be applied to a broad class of problems involving dynamic inconsistency and intra-personal games. Finally, we demonstrate the applicability of our methodology in economics by explicitly solving a special class of non-exponential discounting functions commonly used in economic research. In particular, our examples include quasi-hyperbolic discounting, which has been used as the pre-requisite of many of the aforementioned studies. Our paper thus provides both convenient results for researchers wishing to adopt quasi-hyperbolic discounting as one of their model elements and a general framework for those interested in exploring the impact of non-exponential discounting beyond quasi-hyperbolic discounting.",Renegotiation and dynamic inconsistency: Contracting with non-exponential discounting,https://www.sciencedirect.com/science/article/pii/S0022053123000029,11 January 2023,2023,Research Article,39.0
Won Dong Chul,"School of Business, Ajou University, 206 World cup-ro, Yeongtong-gu, Suwon 16499, South Korea","Received 6 January 2021, Revised 24 December 2022, Accepted 4 January 2023, Available online 9 January 2023, Version of Record 20 January 2023.",https://doi.org/10.1016/j.jet.2023.105607,Cited by (0),"This paper investigates the uniqueness of equilibrium in the economy where agents have preferences with possibly distinct levels of relative risk aversion. Equilibrium prices exist in a price range determined by micro and ==== information. Macroeconomic information is inferred from two representative-agent economies, which provide lower and upper bounds on aggregate demand, respectively. Analysis of the first and second-order price effects shows that individual demand has a unique inflection point at which the income effect maximally dominates the substitution effect. In contrast, aggregate demand may have multiple inflection points, which creates multiple equilibria. Sufficient conditions for the uniqueness of equilibrium are characterized by the local behavior of individual demand at the inflection point. They require that aggregate demand must have no more than two inflection points in the equilibrium price range. The result of this paper sheds light on the Sonnenschein-Mantel-Debreu theory that has negative implications for the uniqueness of equilibrium.","The Edgeworth box provides a simple test ground for checking the multiplicity of equilibria. It is easy to build a two-person or two-country model with at least three equilibria in the Edgeworth box. One can also easily build an economy with a single equilibrium. Thus, identifying sensible conditions under which the economy has a unique equilibrium is challenging. The market equilibrium is uniquely determined in a general equilibrium model if the law of demand holds at the aggregate level. Such an ideal situation is generally infeasible by the Sonnenschein-Mantel-Debreu (SMD) theorem. The SMD theorem states that any continuous function that is homogeneous of degree one and satisfies Walras' law locally behaves as the aggregate excess demand function for some economy with sufficiently many agents. Aggregate excess demand fails to preserve the restrictions on individual demand imposed by the utility maximization. Mas-Colell et al. (1995) mention that “even with homothetic preferences, the weak axiom of revealed preferences can easily be violated when endowments are not proportional.”==== According to the SMD theorem, restrictions on individual preferences may not carry over to aggregate demand. For instance, the Mitiushin-Polterovich (MP) criterion is a well-known condition that is sufficient for individual demand to be monotone with respect to normalized prices. As discussed in Mas-Colell (1991), however, the MP criterion fails to carry over to aggregate demand without additional restrictions on the endowment profiles.====Individual demand functions do not obey the law of demand if they exhibit the Giffen phenomenon over certain price ranges. As shown later (Lemma 2), the income effect dominates the substitution effect beyond a certain price if the constant relative risk aversion (CRRA) coefficient is greater than 1. The Giffen behavior of individual demand prevents aggregate demand from taking a downward slope across the entire price range. However, Giffenity may not affect the determination of equilibrium prices if it begins to occur at high prices that significantly reduce aggregate demand below the total supply of the underlying good. Based on the global analysis of demand functions, this paper investigates the uniqueness of equilibrium in the economy where agents have preferences with possibly distinct levels of relative risk aversion (Theorem 3, Theorem 4, Theorem 3′, Theorem 4′). Equilibrium prices exist in a price range determined by micro and macroeconomic information (Theorem 1, Theorem 2).==== Macroeconomic information about the equilibrium price range (EPR) is deduced from two representative-agent (RA) economies, which provide lower and upper bounds on aggregate demand, respectively. Individuals' marginal rates of substitution at the initial endowments provide microeconomic information about the EPR. Each piece of information is incorporated into the lower and upper bounds of the price range that includes all equilibrium prices. Analysis of the first and second-order price effects shows that individual demand has a unique inflection point at which the income effect maximally dominates the substitution effect (Lemma 2). In contrast, aggregate demand can have multiple inflection points, which creates multiple equilibria (Example 1). Sufficient conditions for the uniqueness of equilibrium are characterized by the local behavior of individual demand at the inflection point. They require that aggregate demand have no more than two inflection points in the EPR (Theorem 4, Theorem 4′). Specifically, equilibrium uniquely exists if each individual demand is locally convex at the upper bound of the EPR.====We focus on a two-good economy where agents may have heterogenous CRRA preferences, and then shift our attention to an economy with more than two goods. The EPR in the two-good economy is characterized by two types of artificial economies. First, we consider the individual-agent (IA) economy in which each agent lives in autarky. There are as many IA economies as the number of agents within the economy. The equilibrium price of each IA economy equals the marginal rate of substitution (MRS) at the initial endowment. Equilibrium prices of the economy belong to the price range defined by the maximum and minimum equilibrium prices in the IA economies. If an equilibrium price were greater than the maximum (minimum) MRS, every agent would want more of the first good (or second good) in exchange for the second good (or first good, respectively). Thus, equilibrium prices fall into the price range determined by the IA economies.====By restricting the heterogeneity of CRRA preferences, we can infer macroeconomic information about equilibrium prices from the fundamentals of the economy. To this end, we build the second type of artificial economies that are closely linked to the aggregation of CRRA preferences. If all agents have common CRRA and time preferences, the multi-agent economy is described by a representative agent (Gorman aggregation). In this knife-edge case, the representative-agent (RA) economy has the same unique equilibrium price as the multi-agent economy. This result is no longer valid if all agents have common CRRA but different time preferences. Differences in time preferences make Gorman aggregation impossible. In this case, the distribution of individual time preferences can provide useful information on equilibrium prices in the multi-agent economy. Specifically, we construct two RA economies in which the representative agent's time preferences match the lowest and highest of the individual time preferences, respectively. The two RA economies provide lower and upper bounds on aggregate demand and yield equilibrium prices that constitute lower and upper bounds on the EPR. The RA-based price information, combined with the IA-based price information, leads to a tighter EPR (Theorem 2).====The approach to the two-good economy is extended to a complete-market economy with homogeneous CRRA preferences. If agents have distinct time preferences and common utility weights for future consumptions, the multi-good economy is transformed into a two-good economy in which agents have endowments of composite goods. In this case, the results in the two-good economy are carried over to the multi-good economy (Section 6).====A fundamental obstacle to examining the uniqueness issue is the failure of the law of demand. Thus, it is important to understand the behavior of aggregate demand in the EPR. An in-depth analysis of the price effects reveals that the global behavior of the individual demand function is characterized by two threshold prices marking a change in income effect relative to substitution effect. The threshold prices are computed uniquely as the turning point of the first and second-order price effects, respectively. The first-order threshold (FOT) price is the critical point of the demand function at which the income effect exactly offsets the substitution effect. Individual demand reaches its minimum at the FOT price and then increases in response to price increase (Lemma 1). Thus, the underlying good becomes Giffen as the price rises above the FOT price. Beyond the FOT price, the income effect dominates the substitution effect; this dominance culminates at the second-order threshold (SOT) price, which corresponds to the inflection point of the demand curve. Consequently, the individual demand function is strictly convex at prices below the SOT price and strictly concave at prices above the SOT price. Therefore, it is strictly quasiconvex over the entire price domain (Lemma 2). The inflection point makes the individual demand function globally non-convex. The aggregate demand function is usually not quasiconvex because quasiconvexity is not maintained under aggregation. This property may cause aggregate demand to have multiple inflection points, which allows the economy to have multiple equilibria (Example 1). The multiplicity of inflection points makes the aggregate demand fundamentally different from individual demand. Thus, understanding how the inflection points for all agents affect the behavior of aggregate demand is key to analyzing the uniqueness of equilibrium in the two-good economy.====The FOT and SOT prices provide the key information for finding the two sufficient conditions for the uniqueness of equilibrium. The first condition concerns the law of demand (Theorem 3, Theorem 3′) and the second concerns the convexity of aggregate demand over the EPR (Theorem 4, Theorem 4′). Both conditions are based on the consequences of Lemma 2, which shows that the price effect decreases for agents whose SOT prices are placed to the left of the price range, and increases for agents whose SOT prices are placed to the right of the price range. The first sufficient condition requires that i) every agent has the SOT price either to the left or right of the EPR and ii) the sum of all agents' maximum price effects over the EPR must be negative. This condition makes the aggregate price effect negative over the EPR. The second sufficient condition requires that the SOT prices of all agents be placed at the right of the price range. In this case, the aggregate demand function is strictly convex in the EPR, which guarantees the uniqueness of equilibrium.====As in Geanakoplos and Walsh (2018), the two-good economy can be viewed in an intertemporal context where the two goods represent current and future (possibly composite) consumption, respectively. In this case, the utility weight is interpreted as a time discount for future consumption. The two-good economy provides a parsimonious framework for studying bank runs (Diamond and Dybvig (1983)), Giffen behavior for risk-free assets (Kubler et al. (2013)), and international transfer problems (Samuelson (1952), Bhagwati et al. (1983)).====This study is closely related to Geanakoplos and Walsh (2018) and Giménez (2022), who show the uniqueness of equilibrium in two-good economies. Geanakoplos and Walsh (2018) assume that agents have the same endowments, the same declining absolute risk aversion (DARA) utility functions, and different time preferences. CRRA preferences are a subclass of DARA preferences. With CRRA preferences, they assume that the endowment ratios are aligned with the degrees of patience. This assumption makes the market income effect negative in the equilibrium, which leads to the uniqueness of equilibrium. Giménez (2022) establishes the uniqueness of equilibrium based on individual demand behavior in a two-good, two-agent economy with strictly quasi-concave utility functions. He provides an insightful analysis of individual offer curves under general conditions on risk aversion and the distribution of endowments. However, the offer curve approach does not apply to multi-agent economies due to the aggregation problem with the offer curves in the Edgeworth box.==== Toda and Walsh (2020) prove the uniqueness of equilibrium in a two-period economy where agents have heterogeneous risk aversion and collinear endowments. Based on this result, they show that increasing wealth concentration leads to a decline in the equity premium.====Chipman (1974) and Chipman and Moore (1979) study conditions under which individuals' homothetic preferences are aggregated into the preferences of a representative agent. Hildenbrand (1983) proves the uniqueness and stability of equilibrium in a distribution economy and applies the result to the exchange economy, where agents have collinear initial endowments.==== Chipman (2010) shows the stability and uniqueness of equilibrium in the two-agent, two-good economy with CES preferences, when agents' preferences and endowments are mirror images of each other. He also discusses conditions for the economy to have multiple equilibria. Mas-Colell (1991) and Kehoe (1998) provide a survey of the literature on the uniqueness of equilibrium. Mas-Colell (1991) explains the role of the Mitiushin-Polterovich theorem combined with collinear endowments. Kehoe (1998) notes that restrictive conditions for the uniqueness of equilibrium are generally difficult to translate into economic intuition and that applied general equilibrium models often do not incorporate these uniqueness results. Gauthier et al. (2022) develop algorithms to compute all equilibria under certain conditions and apply them to construct multi-equilibrium economies that display highly unequal endowment ratios and large differences between individual utility weights. Hens and Pilgrim (2002) provide a comprehensive overview of the literature on uniqueness of equilibrium and preference aggregation with both complete and incomplete markets. Figure 5.4 of Hens and Pilgrim (2002) depicts a state-of-the-art summary of the uniqueness results that are built on gross substitution and the Mitiushin-Polterovich restrictions. It is well-known that a complete-market economy has a unique equilibrium if the excess demand function satisfies the gross substitution property. The gross substitution property often imposes a severe restriction on the degree of risk aversion. For instance, Hens and Loeffler (1995) show that market demand satisfies strict gross substitution when the relative risk aversion is less than or equal to 1.====The current study differs from the existing literature in two major aspects. First, the paper provides the EPRs based on information that is inferred from the equilibrium conditions in both the IA and RA economies. Giménez (2022) takes a similar approach to the two-agent economy but does not consider price information from the RA economies when determining the EPR. Second, the behavior of aggregate demand in the EPR is explained based on the new finding that each individual demand function is strictly convex at prices less than the unique SOT price. If the inflection points occur to the right of the EPR for all agents, then aggregate demand is locally convex in the EPR, which ensures the uniqueness of the equilibrium price.",A new approach to the uniqueness of equilibrium with CRRA preferences,https://www.sciencedirect.com/science/article/pii/S0022053123000030,9 January 2023,2023,Research Article,40.0
Jurado Kyle,"Department of Economics, Duke University, 419 Chapel Drive, Durham, NC 27708, United States of America","Received 30 December 2021, Revised 23 December 2022, Accepted 30 December 2022, Available online 9 January 2023, Version of Record 20 January 2023.",https://doi.org/10.1016/j.jet.2022.105604,Cited by (0),"This paper solves a dynamic rational inattention problem by formulating it in the frequency domain. The main result is a rational inattention version of the classical Wiener-Kolmogorov filter. This filter permits an infinite-dimensional state vector, provides a new line of attack for obtaining closed-form solutions, and can be implemented numerically using a simple iterative algorithm. The frequency-domain approach also sheds new light on why rational inattention produces forward-looking behavior: inattentive agents are willing to accept more uncertainty about the timing of disturbances in exchange for less uncertainty about fluctuations at the most important frequencies.","Rational inattention is a theory about how purposeful agents make choices when faced with more information than they are able to process. Since its proposal by Sims, 1998, Sims, 2003, it has been increasingly adopted across many different areas of economics. Its main appeal is that it can provide an explanation for a number of “behavioral” phenomena, but in the context of a formal optimizing model in which counterfactual experiments can be conducted.==== One of the bottlenecks delaying even more widespread adoption of this theory is that it can be difficult to solve even some of the simplest dynamic rational inattention problems.====This paper analyzes a simple multivariate tracking problem with rational inattention. The problem involves choosing an action process (or equivalently, an information structure) which provides the minimum mean-squared error estimate of an exogenous target process, subject to an information processing constraint. It is exactly analogous to the classical linear least-squares filtering problem of estimating the value of one process on the basis of observations of another, except that under rational inattention, the dynamics of the observation process are determined endogenously. While simple, this problem is important because, by the certainty equivalence principle, the more general task of solving a linear-quadratic-Gaussian control problem with rational inattention can be reduced to solving a tracking problem of this type.====The first contribution of the paper is to formulate and solve this problem in the frequency domain. This complements existing work in the literature, which operates in the time domain.==== The frequency-domain approach is based on the spectral representation theorem, which says that every stationary process can be decomposed into an integral of frequency-specific periodic components with uncorrelated random coefficients.==== This theorem provides a formal way of thinking about fluctuations in a process at different frequencies, which economists invoke, for example, when referring to fluctuations at “business-cycle frequencies.” Decomposing fluctuations by frequency is especially helpful in information problems because the rate of information flow between two Gaussian processes can be conveniently expressed as the average of the information between their random coefficients at each frequency.====The main benefit of solving the problem in the frequency domain is that this approach does not require the target process to have a finite-dimensional state-space representation. This is valuable from an economic perspective because such a representation fails to exist when the target process is endogenous, as in most dynamic equilibrium models.==== However, there are also benefits to this approach even when such a representation does exist. The first is that numerical implementation of the solution is less vulnerable to the “curse of dimensionality” as the dimension of the state vector increases, because the solution does not depend on the size of the state vector. The second is that this approach provides a new procedure for obtaining closed-form solutions, which are helpful both for gaining intuition and evaluating the accuracy of numerical algorithms.====The second contribution of the paper is to provide new insight into why rational inattention produces forward-looking behavior, in the sense that it is optimal for rationally inattentive agents to process information in advance. This is done by analyzing the conceptually important limit case when information can be processed arbitrarily far in advance. What this case shows is that when they are sufficiently constrained, rationally inattentive agents would ideally like to pay attention only to fluctuations at the most important frequencies, meaning the frequencies where the spectral density places the largest weight, and ignore the rest. However, allocating attention in this way is like running the target through an ideal band-pass filter before processing information, which requires having some information about the process at all points in time, including the future.====Relative to existing work on linear-quadratic rational inattention problems, there are two main costs associated with the analysis in this paper. The first is that it requires the target process to be stationary, which rules out stochastic trends. The second is that the loss function is the mean-squared tracking error, which rules out intertemporal discounting and transitional dynamics.==== Both costs can be circumvented by working in the time domain, provided the target has a finite-dimensional state-space representation. Furthermore, if the target does have such a representation, then numerical algorithms that exploit this additional structure are typically faster than the one from this paper, which does not.====The structure of the paper is as follows. Section 2 formulates the problem, discusses its key assumptions, and proves the preliminary result that it always has a Gaussian solution. Section 3 uses this preliminary result to articulate the problem in the frequency domain, and characterizes the solution to the problem in Theorem 1, which is the main result of the paper. Section 4 then analyzes the implications of Theorem 1 regarding the incentives that inattentive agents have to respond to information about disturbances in advance, and Section 5 concludes. The Appendix contains proofs of Theorem 1 and the two propositions from Section 4. The Online Appendix contains the proof of the preliminary result from Section 2 along with a number of other supplemental results, which are mentioned throughout the paper.",Rational inattention in the frequency domain,https://www.sciencedirect.com/science/article/pii/S0022053122001946,9 January 2023,2023,Research Article,41.0
"Albrecht James,Cai Xiaoming,Gautier Pieter,Vroman Susan","Georgetown University and IZA, United States of America,Peking University HSBC Business School, China,Vrije Universiteit Amsterdam and Tinbergen Institute, the Netherlands","Received 2 August 2022, Revised 29 December 2022, Accepted 1 January 2023, Available online 5 January 2023, Version of Record 16 January 2023.",https://doi.org/10.1016/j.jet.2023.105605,Cited by (0),"The literature offers two interpretations of competitive search equilibrium, one based on a Nash approach and the other on a market-maker approach. When each buyer visits only one seller, the two approaches are equivalent. However, when each buyer visits multiple sellers, this equivalence can break down. We present a model in which every buyer visits 2 sellers. A buyer who trades with one seller receives a value of ====, while a buyer who trades with 2 sellers receives value 1. Letting ==== vary from 0 (perfect complements) to 1 (perfect substitutes) we characterize the competitive search equilibrium under the two interpretations. We show that for low values of ====, the Nash and market-maker competitive search equilibria coincide, but the common equilibrium is inefficient. For intermediate values of ====, the two equilibria again coincide and are efficient. Finally, for high values of ====, the Nash and market-maker equilibria differ, and only the latter is efficient.","In competitive search models, capacity-constrained agents on one side of the market post and commit to terms of trade. Agents on the other side of the market, after observing all posted terms of trade, decide where to direct their search. Consider, for example, a product market application of competitive search with ==== sellers, each with one unit of a homogeneous good to sell, and ==== buyers, each wanting to purchase one unit of the good.==== Each seller posts a price, and buyers, after observing all posted prices, direct their search. That is, each buyer chooses a seller from whom he or she tries to purchase the good. When setting its price, each seller faces a tradeoff. The higher the price, the greater is the payoff the seller receives if the good is sold but the lower is the probability of sale. The seller chooses a price to maximize its expected payoff taking this tradeoff into account through a market utility constraint; i.e., the seller realizes that any buyers its posted price might attract must expect a payoff no lower than is available elsewhere in the market. In competitive search equilibrium, each seller takes the buyer expected payoff that is available elsewhere in the market, i.e., the “market utility,” as given. This, of course, requires ==== and ==== sufficiently large, i.e., competitive search equilibrium is a large-market concept.====The literature offers two common interpretations of competitive search equilibrium. The first views competitive search equilibrium as the limit of a sequence of Nash equilibria as the numbers of players on the two sides of a market get arbitrarily large. Consider again the above product market example. For fixed ==== and ====, one can compute the symmetric Nash equilibrium of this game. Letting ==== while holding ==== fixed, we can compute the sequence of Nash equilibria and take the limit. This gives the competitive search equilibrium. This first interpretation is the one presented in Peters, 1991, Peters, 2000, Burdett et al. (2001), and Galenianos and Kircher (2012) among others.====A second interpretation of competitive search equilibrium uses the concept of “market maker.” Again imagine an arbitrarily large market, i.e., ==== with ==== fixed. Suppose all sellers post the same price, ====, so that buyers randomize their search and each seller expects ==== buyers. Then ==== is a symmetric competitive search equilibrium if there is no profitable possibility for a market maker to set up a “submarket” in which sellers are promised an arrival rate ==== of buyers in return for posting a price of ====. This is the interpretation of competitive search equilibrium presented in Moen (1997) and Mortensen and Wright (2002) among others.====When each buyer can contact only one seller, these two interpretations lead to the same equilibrium allocation. This can be shown by comparing the analyses of, e.g., Burdett et al. (2001) and Mortensen and Wright (2002). However, in settings in which buyers make multiple visits, the equivalence can break down. A deviating market maker can attract multiple sellers giving buyers the option to pay more than one visit to the deviant submarket, but under the Nash interpretation, only one seller deviates so each buyer can pay at most one visit to a deviant seller. This difference can change the equilibrium outcome.====The purpose of our paper is to examine the Nash and market-maker equilibrium concepts in a competitive search environment in which buyers direct their search to multiple sellers or in which workers apply to more than one job. We are interested in determining which market characteristics lead to equivalence, i.e., when do the Nash and market-maker approaches yield the same equilibrium outcomes, and when do they differ? Under what circumstances do the two approaches generate equilibrium outcomes that are constrained efficient? Of course, if it is optimal for buyers to pay exactly one visit to the deviant submarket in the market-maker approach, then the two approaches yield the same outcome on and off the equilibrium path. However, if buyers find it optimal to pay more than one visit to the deviant submarket, then the two equilibria can differ.====We consider a simple static environment. Buyers are homogeneous, and so too are sellers, each of which has one unit of a homogeneous good for sale. Every buyer visits two sellers, and we assume that buyer value is (weakly) increasing in the number of units purchased. Specifically, we assume that the buyer value of one unit is ==== while the buyer value of two units is 1. We allow the value of purchasing one unit to vary from ==== (perfect complements) to ==== (perfect substitutes). The case of perfect substitutes is clearly relevant for both the product market and the labor market (buyers who need only one unit of a good, firms that open a vacancy to hire a single worker). Regarding perfect complements, a simple example is one in which firms require two workers to complete a task; absent the second worker, nothing gets done.==== In any case, most products (and most workers) lie somewhere between perfect complements and perfect substitutes. The competitive search literature has so far only considered the case of ====, that is, the case in which each buyer wants to buy only one unit of the good or each firm wants to hire only one worker. By allowing ==== to vary, we generalize the model to deal with the case in which buyers may want to buy more than one unit of the good (or firms want to hire more than one worker). In these cases, the value that the seller offers the buyer may depend on what other sellers offer so that the buyers face a portfolio problem. This suggests that even though we consider a particular trade arrangement in this paper, the underlying logic for our results likely carries over to other settings where buyers face similar portfolio problems.====To attract buyer visits, every seller posts and commits to a maximum price for its unit. A seller may then be visited by more than one buyer. In this case, since buyers are ex ante identical, the seller selects one of its prospective buyers at random and attempts to transact with that buyer. Once a seller has selected its buyer, we assume the seller can observe whether its chosen buyer has also been selected by the other seller its buyer visited. Given this information, the seller may then choose to reduce its offered price to try to consummate the transaction. Finally, we also assume no recall; that is, if a seller fails to transact with its chosen buyer, we do not allow the seller to move on to a different buyer (if present). We discuss these assumptions below.====The relationship between the Nash and market-maker equilibria depends on the value of ====, i.e., on the extent to which the two units are complements/substitutes. When ====, i.e., when the units are perfect complements, the two approaches yield the same equilibrium allocation, and that common allocation is not constrained efficient. The same is true when ==== is sufficiently close to zero, i.e., when the goods are “strong complements.” It is noteworthy that the market-maker approach fails to yield the constrained efficient outcome when the units are strong complements. Suppose a deviant market maker could enforce the following ====: buyers must either pay both visits to the deviant submarket or none. In this case, we could think of the deviant market maker as purchasing a pair of buyer visits. Then, following the logic of competitive search models with single visits, the equilibrium would be constrained efficient. However, in the absence of such an exclusive participation rule, the number of visits per buyer to the deviant submarket is chosen optimally by the buyers, and when the units are strong complements, any buyer who visits a deviant submarket chooses to pay only one visit there. The reason, as we show below, is that buyer visits are strategic substitutes when the units are strong complements.====When the units are perfect substitutes (====), the Nash and market-maker equilibria are no longer the same. Only the market-maker allocation is constrained efficient. The same is true when ==== is sufficiently close to one, i.e., when the units are “strong substitutes.” In the Nash equilibrium, sellers understand that buyers want to receive two offers since a buyer with two offers can anticipate that his or her sellers will lower their prices via Bertrand competition. This leads sellers to post a price that is inefficiently high. In contrast, in the market-maker equilibrium, when the units are strong substitutes, buyer visits are strategic complements; i.e., a buyer who visits the deviant submarket chooses to pay both of his or her visits there. Since buyers who visit a deviant submarket pay both of their visits there, the market maker can solve the sellers' coordination problem.====The last case occurs when the two units are neither strong complements nor strong substitutes. Over this intermediate range of ====, the Nash and market-maker equilibria coincide, and the common allocation is constrained efficient. The intuition is easily seen for the case of ====. When ====, a buyer's marginal value for the first unit is the same as his or her marginal value for the second unit. It is as if there were 2==== buyers, each of whom pays a single visit to one of the ==== sellers. In this case, the standard competitive search logic applies, sellers post a price between the competitive (====) and monopoly (====) levels, and the equilibrium is constrained efficient, exactly as, for example, in Burdett et al. (2001). In this case, the posted price is a ====; i.e., a seller has no incentive to lower its price whether or not its selected buyer has another offer. A similar logic – sellers post fixed prices, the Nash and market-maker equilibria coincide, and the common equilibrium allocation is constrained efficient – applies for ==== sufficiently close to 1/2.====Finally, we have studied the relation between the two concepts of competitive search equilibrium using a particular selling mechanism. In an online appendix (Appendix B.1), we consider a different selling mechanism in which sellers collect an application fee and then offer the good for free to a randomly selected buyer. Although fee posting is very rare, we consider it nevertheless because it helps us understand what forces are at work and how they interact with the type of good, i.e., substitute versus complement. We find that with fees, both the Nash equilibrium and the market-maker equilibrium are constrained efficient. The difference between the two approaches arises only off equilibrium in the case of substitutes.",On the foundations of competitive search equilibrium with and without market makers,https://www.sciencedirect.com/science/article/pii/S0022053123000017,5 January 2023,2023,Research Article,42.0
Faro José Heleno,"Insper, Rua Quatá 300, Vila Olímpia, 04546-042 São Paulo, Brazil","Received 4 February 2020, Revised 21 December 2022, Accepted 23 December 2022, Available online 3 January 2023, Version of Record 17 January 2023.",https://doi.org/10.1016/j.jet.2022.105596,Cited by (0),"We propose the notion of replicas in the context of discrete choices and introduce axioms that characterize the ====. This model does not fall prey to the well-known “red bus-blue bus” example originally proposed by ==== and later made famous by McFadden. Our model satisfies regularity, strong stochastic transitivity, and is a random utility model. Due to its simplicity, this model is more parsimonious than others aimed at overcoming the “duplicates problem”.","Inconsistencies observed in individual behavior have led to theoretical developments that treat choice as a probabilistic process (====, Thusrtone, 1927; Luce, 1959; Marschak, 1960). Luce's (1959) model is perhaps the most well-known stochastic choice formulation, and it is also ubiquitous in applications to discrete choice problems. In this model, each alternative ==== has a positive Luce value ==== and the probability of choosing ==== from a set ==== is==== where ==== is unique up to a strictly positive multiplicative constant.====The behavioral condition introduced by Luce is the famous property called independence of irrelevant alternatives (IIA): for all alternatives ==== and ====, the ratio ==== is constant across all available choice sets ==== that contains ==== and ====. This axiom simplifies experimental routines of choice data collection by allowing multinomial choice probabilities to be deduced from binomial choice experiments. Under full support, Luce's model is equivalent to IIA. This model is also widely referred to as the multinomial logit model in econometric studies (McFadden, 1974).====Although extremely important in empirical studies, as illustrated by the applications in the widely used econometric analysis of discrete choice, some systematic violations of the IIA are documented in the empirical literature on random choice.====The best-known violation anticipated by Debreu (1960) illustrates the ==== (a terminology introduced by Tversky 1972a, 1972b), ====, the fact that choice probabilities may depend not only on the values of the alternatives but also on their similarities. Debreu (1960, p. 188) proposes an example with two alternative recordings of the eighth symphony of Beethoven (by the ==== orchestra conducted by ==== and by the ==== orchestra conducted by ====) and the Debussy quartet (by the ==== quartet), which inspired the famous “red bus-blue bus” example. The set of alternatives is ==== and the stochastic choice function satisfies:====Debreu discusses the possible values for ==== and observes that IIA implies that it should be given by 3/7. However, his reasoning is consistent with ==== and ==== (the subject would rather have Debussy in any context including it), while being indifferent between ==== and ====. Becker et al. (1963) provided empirical evidence for Debreu's hypothesis.====This type of violation of IIA is often referred to as the ==== in the discrete choice estimation literature. The two most known approaches that deal with this are Tversky's (1972a, 1972b) elimination by aspects (EBA) model and the nested logit models widely applied in the discrete choice estimation literature (see, e.g., Train, 2009).====Our main goal is to formulate an adequate choice theory as a probabilistic process less restrictive than Luce's model and more parsimonious than the available models that may deal with the “duplicates problem”. Therefore, we do not intend to cover all possible patterns of similarity effects in this contribution, but just the knife-edge cases of the similarity effect proposed by Debreu (1960).====Our first step is to propose the notion of ====. The random choice rule must reveal whether two alternatives are replicas. Physical characteristics may have nothing to do with this because they are a property of the choice rule and not the alternatives. For a given random choice function, we say that two alternatives ==== and ==== are replicas if, for any set of alternatives ==== that contains ==== and ==== the following two properties hold: (====) the odds of ==== and ==== under ==== are the same, and ==== for any third option ==== in ====, the probability of choosing ==== from ==== is the same as the probability of choosing ==== from ====. We write ==== if ==== and ==== are replicas. It is worth noticing that for ==== random choice function ====, the induced binary relation ==== is an equivalence relation, which induces a partition of choice set ==== into endogenous nests of replicas.====In Debreu's example, the two alternatives recording of the eight symphonies of Beethoven are replicas, and the choice set is partitioned into ====. An important pattern of stochastic choice that may come up in the context of revealed replicas is illustrated in this example. The probability of chosen ==== from the menus ==== and ==== is 3/5. Our first axiom, called ==== (NR), states that the probability of choosing an object ==== from a set ==== is not affected by the introduction of replicas of some other object ==== in ==== unless ==== and ==== are replicas. In general, this axiom is not consistent with full IIA.====We relax IIA in the following way: Given two objects ==== and ====, no matter if they are replicas or not, the ratio ==== equals to ==== for all menus ==== containing ==== and ==== that does not include any other replicas of ==== or ====. This is our second axiom, called ==== (IINR). The meaning of IINR is the same as the intuition of the ==== property as discussed by Train (2009, p. 84). In Debreu's example, IINR can be applied only to the two alternative recordings of the eight symphonies of Beethoven (i.e., we can take only ====). If we enrich the set of alternatives in Debreu's example by incorporating the Nocturne of Chopin (by the A orchestra and denoted by ====), then, for instance====Our main result shows that a random choice function ==== (with full support) satisfies NR and IINR if and only if there are a partition ==== of ==== and Luce values ==== such that for all ==== and ==== where ==== is the set of replicas to which ==== belongs. We call this specification the Luce model with replicas (LR), which is summarized by ====. In the original Debreu's example we have ==== with ==== and ====. Moreover, we can choose ==== and ====.====We show that every LR model is a particular case of the EBA model, being more parsimonious than other random choice models that deal with the “duplicates problem”. Moreover, LR satisfies regularity, strong stochastic transitivity, and is a random utility model.====The Luce model with replicas naturally has the econometric analysis of qualitative multinomial response models as its source of potential applications. In those models, endogenous variables may result from economic behavior that is intrinsically categorical, such as entry to a product market, choice of occupation, or the prescription of a brand-name drug versus generic drugs. In conclusion, we discuss a potential application.",The Luce model with replicas,https://www.sciencedirect.com/science/article/pii/S0022053122001867,3 January 2023,2023,Research Article,43.0
"Gan Tan,Hu Ju,Weng Xi","Department of Economics, Yale University, United States of America,China Center for Economic Research, National School of Development, Peking University, China,Guanghua School of Management, Peking University, China","Received 15 June 2021, Revised 5 October 2022, Accepted 24 December 2022, Available online 30 December 2022, Version of Record 6 January 2023.",https://doi.org/10.1016/j.jet.2022.105597,Cited by (0),"This paper investigates a two-agent mechanism design problem without transfers, where the principal must decide one action for each agent. In our framework, agents only care about their own adaptation, and any deterministic dominant incentive compatible decision rule is equivalent to contingent delegation: the delegation set offered to one agent depends on the other's report. By contrast, the principal cares about both adaptation and coordination. We provide sufficient conditions under which contingent interval delegation is optimal and solve the optimal contingent interval delegation under fairly general conditions. Remarkably, the optimal interval delegation is completely determined by combining and modifying the solutions to a class of simple single-agent problems, where the other agent is assumed to report truthfully and choose his most preferred action.","This paper presents an analysis of a mechanism design problem with a principal (she) and two agents (he), and without monetary transfers. The principal needs to make two decisions, one for each agent, but the relevant information is dispersed between the agents. While each agent only cares about the decision for himself, the principal also cares about the interactions of the two decisions.====An application of our analysis is to the delegation problem within multidivisional organizations. As pointed out by Roberts (2004) and Alonso et al. (2008), multidivisional organizations exist primarily to coordinate the activities of their divisions. Coordinated decision making by the headquarter manager requires aggregation of the relevant information, which is usually dispersed among the individual division managers as they are best informed of their local conditions. But there is a conflict of interest between the headquarter manager, who cares more about coordination, and the division managers, who care more about adaptation: more coordinated decisions are less adapted to the local conditions of each division. In such an environment, how should the headquarter manager delegate to the division managers to reflect the trade-off between adaptation and coordination? This question is unexplored in the prior literature on authority allocation within multidivisional organizations.==== Our paper fills the gap as a direct application of our main result can shed light on the optimal design of delegation rules.====Formally, each of the two agents in our model has a quadratic-loss payoff function that only depends on his own state and the decision for him. Each agent's most preferred decision is equal to his state. By contrast, the principal's payoff function consists of three additively separable components. Two of them are called ====, which represent her potentially different preferences over each agent's decision and the corresponding state. In general, we allow incentive misalignment in the sense that these payoffs are different from the agents' ones. The third component is a supermodular function that only depends on the agents' actions. The complementarity of the two actions captures the principal's coordination motive: if one agent makes a higher decision, she would like the other agent to make a higher decision too. Thus, we refer to this component as the principal's ====.====The principal can commit to any deterministic dominant strategy incentive compatible mechanism, which can be implemented by a ==== mechanism. In such a mechanism, agents report their states to the principal and then the principal offers each agent a delegation set that depends on the other agent's report. After reporting and receiving his own delegation set, each agent chooses his favorite action from it. Our goal is to understand the principal's optimal contingent delegation.====To see the main problems faced by the principal in her design, consider the previously mentioned coordination problem in multidivisional organizations. If the headquarter manager only cared about whether the decisions of the local divisions were adapted to their local conditions and had no coordination concern at all, then she could simply grant full discretion and delegate all the decision rights to the local divisions, since their interests were perfectly aligned. However, this decision rule should not be optimal in the presence of the coordination motive, as the local divisions' fully adapted decisions may not be well coordinated, leading to a large coordination loss. To mitigate such miscoordination, the headquarter manager can give less discretion to the local divisions. By ruling out some decisions for a division, she can induce this division to coordinate with the other one at the cost of reduced adaptation of this division. Thus, the optimal level of discretion for each division must trade off the cost from reduced adaptation against the benefit from better coordination. The difficulty here is that each division's trade-off depends on the other division's decision, which in turn is determined by the discretion the other division is granted. Therefore, the optimal design must resolve both divisions' trade-offs jointly.====Our first main result, Theorem 1, sheds light on how these trade-offs are resolved jointly at the optimum. It characterizes the optimal ====, under which the contingent delegation sets that the principal offers to the agents are always intervals. We construct the optimal solution via a “two-step procedure.” The first step treats each agent's trade-off separately, while the second step deals with the joint design problem.====In the first step, we consider the principal's optimal interval contingent delegation problem for agent ====, assuming that agent −==== is granted full discretion. This involves a series of simple single-agent problems, in each of which the principal determines agent ===='s delegation interval to maximize the expected sum of her adaptation payoff from agent ==== and her coordination payoff, given that agent −===='s state is ==== and he chooses ====. We assume that for each ====, the optimal interval ==== is uniquely determined and non-degenerate.==== Both boundary functions ==== and ==== are nondecreasing in ====, because the principal would like agent ==== to take higher action to coordinate better with agent −==== when −==== takes a higher action. We refer to the pair of functions ==== as the ==== for agent ====, because it is obtained by assuming that agent −==== is never constrained. Panel (a) in Fig. 1 provides an illustration of the unilaterally constrained delegation rules for both agents. The square is the ====-plane.==== The blue and red curves represent the unilaterally constrained delegation rules for agents 1 and 2, respectively.====These two unilaterally constrained delegation rules together give the principal a contingent interval delegation ====. But intuitively it is not optimal, precisely because it neglects the joint design problem: changing from full discretion to delegation rule ==== changes agent −===='s behavior, which in turn affects agent ===='s coordination problem and makes ==== for agent ==== suboptimal. To see this, consider, for example, a sufficiently low ==== so that action ==== is never available to agent 2 under ====. Under this contingent delegation rule, agent 2's action will always be higher than what he would take under full discretion, i.e., ====. This implies that the delegation interval ==== for agent 1 is no longer optimal, because the principal would like to move this interval upward for better coordination.====Nonetheless, we resolve this issue in the second step by modifying ====, under the additional assumption that ==== and ==== intersect ==== and ====, respectively, only once in the ====-plane, as is the case in panel (a).==== Theorem 1 states that an optimal contingent interval delegation is immediately obtained by bounding the unilaterally constrained delegation rules with the intersections. The resulting contingent delegation is illustrated in panel (b).==== The curve ==== is the lower bound and ==== is the upper bound so that the delegation interval for agent ==== when −==== reports ==== is ====.====To gain some intuition on the construction of the optimal mechanism, consider again the example where ==== is sufficiently low so that action ==== is never available to agent 2 under ====. We employ an iterative process of modifications in this case by first increasing agent 2's action, ====, to the lower bound ====. This change of ==== implies that the delegation interval ==== for agent 1 is no longer optimal, and intuitively we change it to ====. If ==== is contained in the interval ====, we can stop further modifications and let ====. This is how we use the arrow to modify point ==== in panel (c) in Fig. 1. But if ==== is outside the interval, we need to change ==== to the boundary, and this triggers further modifications of agent 2's delegation interval. This iterative process continues until it converges to ====, as illustrated by the arrow starting from point ==== in panel (c). Consequently, the optimal delegation is flat over the corner.====Our second main result, Theorem 2, establishes sufficient conditions for the optimal contingent interval delegation in Theorem 1 to be optimal among all the contingent delegation mechanisms. These sufficient conditions are expressed in terms of the principal's adaptation and coordination payoffs and the state distributions. A more general result, which provides sufficient conditions for any given contingent interval delegation to be optimal and on which Theorem 2 is based, is also provided in Theorem 3 in the appendix. It extends the main sufficiency result in Amador and Bagwell (2013) to our two-agent setting.====Finally, we apply the above general results to study the previously mentioned optimal design problem within a multidivisional organization. Under the quadratic-loss specification of the principal's payoff function and log-concavity of the state distributions, all the conditions for Theorem 1, Theorem 2 are satisfied. Therefore, the optimal contingent interval delegation we found in Theorem 1 is indeed an optimal mechanism. Due to the simple structure of this optimal contingent interval delegation, a set of intuitive comparative statics results are easily obtained. For one example, if coordination becomes more important to the principal, then both divisions will receive less discretion. For another example, if one division becomes more important to the principal, then this division must be better off in that it will be granted larger discretion. But the other division will suffer as it will receive less discretion.====  Our work relates to two main strands of the literature. The first is the research on mechanism design without contingent transfers. In the single-agent setting, it is well known that such a problem is equivalent to the delegation problem. Holmström, 1977, Holmström, 1984 was the first to pose the general class of delegation problems. Since then, a number of other researchers, including Melumad and Shibano (1991), Martimort and Semenov (2006), Alonso and Matouschek (2008), Amador and Bagwell (2013), and Amador et al. (2018), have studied and characterized the solution to the single-agent delegation problem under various assumptions on the preferences and state distributions. This literature places particular emphasis on the optimality of interval delegation since it is the most natural form and is commonly observed in reality. By focusing on dominant strategy incentive compatible mechanisms, we establish a similar equivalence between mechanism design and delegation in our general framework with two actions and two agents.====To our knowledge, Alonso et al. (2014) were the first to study optimal mechanism design without contingent transfers in an environment with multiple actions and multiple agents. In their model, a principal allocates limited resources to three agents. Two of them are privately informed of their own ideal demand, and the ideal demand of the third agent is known to the principal. Agents are biased only in one direction so only a cap will be used in the optimal unilaterally constrained delegation rules and consequently in the optimal mechanism. Our analysis points out that the decomposition result holds with general functional form and, in particular, in the presence of biases in both directions.==== There are two other papers studying optimal non-monetary design with two agents and one action: Martimort and Semenov (2008) and Fuchs et al. (2022). Because the policy chosen by the principal is only one-dimensional, the models are more closely related to the single-agent case. For example, Fuchs et al. (2022) point out that when agents' type spaces are disjoint, the principal might find it optimal to delegate the decision right to just one agent.====The second strand studies authority allocation within multidivisional organizations. Similar to our setting, this literature assumes that multiple decisions must be coordinated and the relevant information for decision making is horizontally dispersed. However, related studies including Alonso et al. (2008), Rantakari (2008), Dessein et al. (2010), Friebel and Raith (2010), and Li and Weng (2017), assume a lack of commitment power in the sense that the organization can commit only to an ex ante allocation of decision rights, and explore strategic communication equilibria given an authority allocation mechanism in such settings.==== For example, Alonso et al. (2008) compare the efficiency of centralization, in which case the division managers communicate vertically with the headquarter manager who will make the decisions, and decentralization, in which case the division managers who will make their own individual decisions communicate horizontally with each other. While all these papers study equilibria under certain exogenously given mechanisms, we apply our main result to this environment to investigate the optimal mechanism under full commitment power. To the best of our knowledge, our paper is the first to study the optimal design of delegation rules to reflect the trade-off between adaptation and coordination in multidivisional organizations, although admittedly our framework simplifies the setup by assuming that division managers only care about themselves, while papers such as Alonso et al. (2008) allow agents also to care about coordination (just to a lesser degree).====The rest of the paper is organized as follows. Section 2 describes the model. Section 3 contains the analysis and our main results. In Section 4, we apply our general results to the multidivisional organization problem. Section 5 concludes. The proofs for Section 3 are deferred to the appendix. The proofs for Section 4 can be found in the online appendix.",Optimal contingent delegation,https://www.sciencedirect.com/science/article/pii/S0022053122001879,30 December 2022,2022,Research Article,44.0
Dellis Arnaud,"Dept. of Economics, Université du Québec à Montréal, Canada","Received 21 June 2021, Revised 30 September 2022, Accepted 19 December 2022, Available online 29 December 2022, Version of Record 10 January 2023.",https://doi.org/10.1016/j.jet.2022.105595,Cited by (0),"Whom should an interest group lobby in a legislature? I develop a model of informational lobbying, in which a legislature must decide on the allocation of district-specific goods and projects. An interest group chooses to search and provide information on districts' valuations of the goods. The setting is one of distributive politics, where the proposed allocation of goods and projects is endogenous to the information provided by the interest group. I characterize the information search strategy of the interest group. I furthermore establish that the relationship between informational lobbying and legislative majority requirement is non-monotonic. I also determine who gains and who loses from lobbying, identifying circumstances in which legislators would unanimously prefer to ban informational lobbying. Finally, I provide an informational rationale for why interest groups sometimes lobby legislative allies (friendly lobbying).","Two features pertain to interest group influence. One is the prevalence of legislative policymaking. Most policies are chosen, not by a single policymaker, but by a legislature composed of representatives elected in several districts. The other feature is the prevalence of lobbying as an instrument of interest group influence, where lobbying is defined as the act of providing information. Multiple accounts of policymaking in the U.S. (e.g., Bauer et al., 1963; Hansen, 1991; Drutman, 2015) assert that interest groups' influence often takes the form of persuasion through information provision. These accounts are substantiated by the Center for Responsive Politics (opensecrets.org), which reports that about 7 billion dollars were spent on lobbying over the cycle 2019-20, compared to 6.46 billion dollars of monetary contributions. Moreover, multiple studies (e.g., Wright, 1990; Ansolabehere et al., 2002) provide evidence suggesting that campaign contributions serve to gain access to legislators for interest groups to communicate their information. The prevalence of lobbying is associated with legislators' reliance on the information provided by interest groups, as noted in Hansen (1991: 5): “Lawmakers operate in highly uncertain electoral environments. They have an idea of the positions they need to take to gain reelection, but they do not know for sure. Interest groups offer to help... They provide political intelligence about the preferences of congressional constituents.”==== Because of the legislators' reliance on interest groups' information, lobbying can be an effective instrument of interest group influence (e.g., Igan and Mishra, 2014; Belloc, 2015). As Baumgartner et al. (2009: 124) writes: “There is evidence that organizational advocates are often successful in getting Congress to make policy decisions that are informed by research and the technical expertise that they provide.”====This paper investigates the question of which legislators an interest group should lobby. This question is empirically relevant as evidence suggests that most lobbying activities are targeted at the legislative branch of government,==== and that interest groups engage in selective persuasion, targeting some legislators while ignoring others (e.g., Bombardini and Trebbi, 2020). As You (2023: 5) writes:”... identifying the ==== and the ==== who are targeted is crucial to understanding interest groups' lobbying strategies when groups attempt to persuade politicians in a collective decision-making environment.” To investigate this question, I construct a model of interest group influence that includes both informational lobbying and legislative policymaking. The model is set in the context of distributive politics.==== Moreover, the legislative proposal is endogenous to the information provided by the interest group.====Intergovernmental lobbying is an example of a situation for which the model speaks. Municipalities in all U.S. states (except Hawai'i) are joining forces in state municipal leagues (e.g., League of California Cities, Texas Municipal League) and in a federal league (National League of Cities) to lobby state legislatures and Congress for, among other things, legislative earmarks, state grants, and funds for local infrastructure projects (e.g., transportation, housing).==== Local government lobbying constitutes a large share of lobbying activities at the state level. For example, Payson (2022: 13) writes: “In Texas, over half of the 1,741 registered lobbyists in the 2015 legislative session were working in some capacity for local governments... And in California, local governments have spent more on lobbying than any other interest group industry since at least 1999.” Likewise, Najmabadi (2019) writes about the importance of the role played by the Texas Municipal League: “The interest group representing Texas cities used to be one of the most powerful legislative forces at the Capitol. This session, it has become the GOP's most prominent adversary.” The analysis in this paper speaks for, among other things, the decision by a municipal league of which legislators to lobby for legislative earmarks and state-funded local public projects.====The model considers a legislature consisting of ==== legislators, representing different districts. The legislature must decide on the allocation of district-specific goods and projects (hereafter referred to as goods), which can be local public goods or pork-barrel projects, such as road construction, mass-transit projects, grants-in-aid, or recreational projects such as sports arenas and public libraries. Goods are financed by a national tax. One legislator serves as the agenda setter, proposing an allocation of goods across districts. The agenda setter can be, for example, the chair of the Appropriations Committee or of the Local Government Committee. Adoption of the agenda setter's proposed allocation requires the approval of at least ==== other legislators. I let ==== take a value between 0 (dictatorship of the agenda setter) and ==== (unanimity). Each district has a valuation of the goods which, to keep things simple, is either low or high. Districts are ex-ante heterogeneous, varying in their prospects of having a high valuation of the goods. Districts' valuations are ex-ante unknown to all, but an interest group that benefits from the provision of goods can search for information on districts' valuations. The interest group can be, for example, a municipal league advocating for legislative earmarks, Universities Canada advocating for research funding of various Canadian universities, or a professional union (e.g., U.S. Chamber of Commerce, American Association of State Highway and Transportation Officials, Alliance of Manufacturers and Exporters Canada). Lobbying is modeled as persuasion, where information takes the form of verifiable evidence. Information search is costly for the interest group. Search is sequential, with the interest group observing the outcome of its search on a district's valuation before deciding whether to search another district.====In equilibrium, the agenda setter forms a legislative coalition consisting of himself and ==== other legislators, whose districts have the highest expected valuations of the goods. Districts in the legislative coalition are offered goods, while those outside the legislative coalition receive no goods. A critical feature of the legislative choice is that the proposed allocation of goods and the composition of the legislative coalition are endogenous to the information provided by the interest group.====The interest group engages in selective persuasion, choosing strategically the districts in which it searches for information. More precisely, the interest group starts by searching districts with ‘moderate’ prospects of high valuation, and then moves towards districts with ‘extreme’ prospects, alternating between districts with better and worse prospects depending on the received information. To be more specific, I label districts (other than the agenda setter's) in decreasing order, with district 1 having the best prospects of high valuation, and district ==== having the worst prospects. The interest group starts its search process with district ====, that is, the district that has the best prospects of high valuation ====. If the interest group obtains the information that district ==== has a high valuation, then the interest group continues its search with district ====, that is, the next district with ==== of high valuation. Otherwise, the interest group moves instead to district ====, that is, the next district with ==== of high valuation. This process is iterated, with the interest group moving gradually towards districts 1 and ====, alternating between districts with better prospects and districts with worse prospects, depending on the information it receives. The equilibrium stopping rule prescribes that the interest group continues searching until one of two things happens. Either the interest group has obtained information of high valuation for ==== districts, in which case the agenda setter will be able to form a legislative coalition composed exclusively of known high-valuation districts. Or the interest group has obtained information of low valuation for ==== districts, in which case the agenda setter could have to include a known low-valuation district in the legislative coalition if the interest group were to continue searching districts.====The analysis yields three interesting implications. One concerns the relationship between information provision and legislative majority requirement, ====. I establish that this relationship is ====, increasing with ==== for low values of ==== and decreasing for high values of ====. I further show that legislators should make decisions under simple majority if they seek to maximize the minimum number of districts on which they get informed. Furthermore, the expected number of searched districts may reach its maximum under infra-, simple, or super-majority requirements, depending on districts' prospects of high valuation. This has an interesting implication for institutional design: while Diermeier and Myerson (1999) suggests that legislators in a unicameral legislature may want to adopt infra-majority requirements (by delegating authority to a leader) if they seek to maximize the expected amount of ==== they receive, my analysis suggests that legislators might be better off adopting simple- or super-majority requirements if they seek to maximize the expected amount of ==== they receive.====A second implication concerns the gainers and losers from lobbying. Informational lobbying has two effects in the context of distributive politics: (i) it induces changes in the composition of the legislative coalition; and (ii) it induces an increase in the total quantity of goods provided. I show that both the agenda setter and the interest group gain from lobbying since they benefit from the increase in the total quantity of goods provided. The legislators from districts 1 through ==== lose from lobbying since their district may no longer be included in the legislative coalition and, when this happens, their payoff drops further with the increased cost due to the rise in the total quantity of goods provided. The legislators from districts ==== through ==== may gain or lose from lobbying, since they benefit from the possibility that their district gets included in the legislative coalition, but lose from the increase in the total quantity of goods when their district is kept out of the legislative coalition. The analysis identifies circumstances in which, from an ex-ante perspective, all legislators (other than the agenda setter) lose from the possibility of lobbying and, therefore, would unanimously prefer to ban informational lobbying.====The third implication relates legislator targeting to the nature of lobbying. Empirical studies (e.g., de Figueiredo and Richter, 2014) find evidence that interest groups lobby both legislative allies (====) and opponents (====), and that friendly lobbying tends to prevail over confrontational lobbying. While confrontational lobbying makes sense in terms of persuasion, friendly lobbying does not. The present analysis shows that both friendly and confrontational lobbying can be rationalized in the context of distributive politics where the legislative proposal is endogenous to the information provided by interest groups. The analysis further identifies conditions under which friendly lobbying prevails over confrontational lobbying.====I consider several extensions of the model. In one extension, districts differ in the quality of information that the interest group can obtain. A second extension concerns the provision cost of goods. In a third extension, legislators or district-specific interest groups can decide to search for information on their own district's valuation, complementing the information search of the ‘national’ interest group considered in the model.==== Finally, I discuss an extension where valuations are correlated across districts.====While the present analysis is developed in the context of lobbying, it applies more generally to situations where a sender seeks to persuade a set of receivers. Collective decision-making institutions to which the model may apply are boards of directors or shareholders in firms, boards of governors in professional sports leagues (e.g., NHL), or coalition governments.====The remainder of the paper is organized as follows. Section 2 discusses the related literature. Section 3 describes the model. Section 4 characterizes the legislative choice. Section 5 analyses the interest group's search. Section 6 discusses three key implications of the analysis. Section 7 discusses several extensions to the model analyzed in the paper. Finally, Section 8 concludes. All proofs are in the appendix. A supplementary online appendix contains additional material.",Legislative informational lobbying,https://www.sciencedirect.com/science/article/pii/S0022053122001855,29 December 2022,2022,Research Article,45.0
"Liu Bin,Lu Jingfeng","School of Management and Economics and Shenzhen Finance Institute, The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen), 518172, China,Department of Economics, National University of Singapore, 117570, Singapore","Received 14 January 2022, Revised 15 December 2022, Accepted 19 December 2022, Available online 28 December 2022, Version of Record 6 January 2023.",https://doi.org/10.1016/j.jet.2022.105594,Cited by (1),"We allow negative prizes and investigate effort-maximizing prize design in rank-order contests with incomplete information. Endogenous participation arises due to less-efficient types' incentive to avoid punishments. The optimum features winner-take-all for the best performer and at most one punishment for the worst performer among all potential contestants, whenever they enter the competition. Based on this, we then (1) provide a necessary and sufficient condition for the optimality of pure winner-take-all without punishment; and (2) show that the optimal entry threshold increases with the total number of contestants and converges to the Myerson cutoff in the limit. Finally, we characterize the optimal entry-dependent prize structure, allowing the prize sequence to vary with the number of entrants. The optimal design must entail endogenous entry, and it harmonically integrates both winner-take-all and egality.","The practice of utilizing “sticks” jointly with “carrots” in incentivizing agents has long and widely been observed. In a contest environment, a positive prize can be viewed as a carrot and a negative prize can be viewed as a stick. Negative prizes (punishments) in contests are prevalent in practice. As noted by Liu et al. (2018) and Hammond et al. (2019), the widely used entry fees in contests are effectively negative prizes. Many firms use tournaments with punishments to incentivize employees—“General Electric, Metlife, Microsoft, American Express, AIG, Hewlett Packard, and Yahoo! have used tournaments that incorporate both rewards and punishment, with punishments coming in such forms as job reassignment, demotion, or even firing” (Newman and Tafkov, 2014). Examples of punishments in contests also include, for instance, an F grade for students, relegation in sports leagues, among others.====Prize allocation has long been recognized as a main instrument to incentivize contestants to exert productive effort. However, despite the fact that negative prizes (sticks) are widely adopted in practice, and are a powerful tool in eliciting effort supply, a vast majority of the prize design literature has been focusing on analyzing positive prizes (carrots), with only a few exceptions. A celebrated result is established by Lazear and Rosen (1981), who show that rank-order contests with negative prizes can achieve the first best in an environment of complete information. In environments with incomplete information, however, introducing negative prizes into the analysis inevitably entails the issue of endogenous entry of contestants,==== which would often make the analysis cumbersome and less tractable.====This paper analyzes prize design in rank-order contests with negative prizes and incomplete information. Rank-order contests are an important class of contest rules that are widely used in practice.==== They solely rely on the ranking of contestants' performance (i.e., relative performance), rather than the information on the level of performance (i.e., absolute performance), to allocate prizes/punishments. This advantage of rank-based prize structure has long been recognized in the literature (e.g., the discussion in Lazear and Rosen, 1981).==== Moldovanu and Sela (2001) pioneer the analysis of rank-order contests with incomplete information by modeling such contests as all-pay auctions with incomplete information. Such modeling approach, which is followed as well in this paper, has been a well-adopted workhorse model in the contest literature.====A complete characterization of the optimal rank-based prize design while allowing negative prizes in all-pay contests with incomplete information has remained an open question since Moldovanu and Sela's (2001) seminal work, which largely focuses on positive prizes. Nevertheless, several studies have made important progress by considering special prize structures. Moldovanu et al. (2012) study both cases with exogenous and endogenous entry. For the case of endogenous entry, they assume a single positive prize for the top performer and a fixed number of uniform negative prizes for the bottom entrants.==== Thomas and Wang (2013) and Kamijo (2016) focus on a single positive prize for the top performer and a single negative prize for the worst performer among all entrants in their analysis. Hammond et al. (2019) study prize-augmenting entry fees, which can be viewed as uniform negative prizes.====The goal of this paper is to provide a more complete answer to this important question by coming up with a tractable procedure and fully characterizing the optimal design of positive prizes and negative prizes in rank-order contests. In particular, we do not impose any further assumptions—such as the number of positive prizes, the number of negative prizes, and uniform negative prizes—on the prize structure. Our baseline analysis investigates an important and natural class of prize structures—prize structures that only depend on ranks of contestants' performance. Since in general the number of entrants is endogenous, we further analyze the optimal prize design in an extension, while allowing the prize sequence to depend on the number of entrants.====To be specific, our baseline analysis essentially allows prizes to be negative in the incomplete information all-pay auction model of Moldovanu and Sela (2001). Each contestant is endowed with his private type, which is his marginal cost of exerting effort. The organizer, who has a fixed budget, designs a prize structure ==== with ==== to maximize the total effort from ==== contestants, where ====, which can be nonnegative or negative, is the prize for the participant with the ====th highest effort. A nonparticipant receives no prize. Negative prizes in general lead to endogenous entry of contestants because of the participation constraint, so the actual number of entrants can be endogenous. The organizer must observe ex post budget constraints for all possible numbers of entrants, and she is allowed to value any leftover budget.====Notice that the prize structures in this baseline analysis depend only on ranks of contestants' performance. In particular, they are ==== of the effort levels and the number of entrants (==== prize structures). We start by focusing on such non-contingent prize structures in the baseline analysis—instead of directly analyzing the general prize structures that can additionally be contingent on the number of entrants—for the following reasons. (1) First, this baseline non-contingent prize structure naturally extends Lazear and Rosen (1981) and Moldovanu and Sela (2001). The former paper allows prizes to be both ==== to study rank-order contests in a ==== setting; while the latter paper focuses on ==== prizes to study rank-order contests in an ==== setting. In both studies, the prize structure is non-contingent. Our analysis of non-contingent prize structures can be viewed as the most natural extension of both papers by allowing ==== prizes in an ==== setting. (2) Second, this baseline non-contingent prize structure departs in a ==== way from that of Moldovanu and Sela (2001) while introducing negative prizes, which allows us to illustrate the role of negative prizes in a simplest setting. In particular, as will be clear later, negative prizes cannot be used to top up the first prize in the baseline analysis due to the ex post budget constraints, while this prize-augmentation becomes possible when the prize structure can further depend on the number of entrants. As such, this baseline setting allows us to study the role of negative prizes while eliminating the top-up effect. (3) Third, compared to contingent prize structures, such non-contingent prize structures are simple and easier to implement in practice, as typically, practical prize series are often fixed and do not vary with the actual number of participants. (4) Fourth, this baseline analysis serves as a benchmark for the analysis of the more complex and general structures that can be contingent on the number of entrants. (5) Finally, non-contingent prize structures are subject to less manipulation from the perspective of contestants.====In the baseline setting, the equilibrium endogenous participation induced by a prize structure is a threshold-entry, in which only contestants with types higher than a certain threshold enter the contest. We first identify the equilibrium bidding function and entry threshold for any given prize structure. Based on these, we find that the organizer's problem, for any fixed entry threshold, is essentially a linear programming, in the sense that all the functions in the problem—the objective function, the budget constraint, and the participation constraint—are linear in the ==== prizes. Nevertheless, coefficients in this linear programming problem vary in a highly intractable and nonlinear way as the induced entry threshold changes, which makes solving the problem challenging.====A typical procedure is to first identify the constrained optimum among all feasible prize structures that induce the same entry threshold, and then vary across all possible entry thresholds in the type space to pin down the universal optimum. However, this approach is rather infeasible because of the aforementioned arbitrary and intractable behavior of the coefficients in the linear programming problem, which generates complicacy in characterizing the optimum for a fixed entry threshold. We illustrate that the number of positive prizes and the number of negative prizes at the optimum for a fixed entry threshold can vary with the threshold. In particular, in general, multiple positive prizes and multiple negative prizes can arise as the optimum for fixed thresholds. Such arbitrariness in the number of prizes constitutes the main challenge of pinning down the optimal design.====We manage to tackle this difficulty by developing an innovative procedure. A first key step relies on discovering useful relations among the positive coefficients of prizes in the total effort function. This is made possible by observing a hazard rate dominance result of relevant order statistics. Specifically, these positive coefficients must be associated with higher prizes, and the ratios of these coefficients to their counterparts in the equilibrium entry condition are decreasing in ranks. Moreover, more coefficients become positive as the entry threshold gets higher. A second key step shows that when an entry threshold is sufficiently high such that all coefficients become positive, then the corresponding optimal prize structure can have only one negative prize. A third key step shows that for any fixed prize structure, we can construct an alternative prize structure to dominate it by varying only the last prize, which can induce the minimal entry threshold ==== that makes all coefficients in the total effort function positive. Therefore, the entry threshold ==== must be optimal and there can be only one negative prize at the optimum. A final step establishes that a single positive prize equal to the whole initial budget is universally optimal, relying on the key relations among ratios of coefficients in the objective function and the equilibrium entry condition, which is discovered in step one.====The optimal prize structure thus takes an elegant form of ====: A single positive prize ==== equal to the budget and a single negative prize ==== that supports the optimal entry threshold ====. In other words, the optimum features winner-take-all for the best performer and at most one punishment for the worst performer among all ==== potential contestants, whenever they enter the competition. We find that the opposite of the coefficient of ==== in the objective function can be interpreted as the marginal revenue gained by imposing an extra unit of punishment through the last negative prize. Therefore, the optimum is achieved when this marginal revenue is precisely zero. Based on this observation, we (1) further provide a necessary and sufficient condition for the optimality of pure winner-take-all (i.e., no negative prize); (2) find that the optimal entry threshold increases with ==== and converges to the Myerson cutoff (Myerson, 1981) in the limit.====Since the number of entrants is endogenous, a further question naturally arises: What would the optimal design be when the prizes can be contingent on the number of entrants while they do not depend their effort levels? We also fully pin down the optimum in this relaxed setting. We find that the optimal prize allocation rule must induce endogenous entry by excluding less efficient types. The prizes also vary dramatically upon the number of entrants. When everyone participates, all non-top performers are penalized by a finite uniform budget-augmenting negative prize and the top performer collects all the available prize budget; whenever the entry is partial, an egalitarian rule of sharing the initial budget applies to entrants. The distribution of more efficient types depends on the number of entrants. The optimal prize design provides an enhanced incentive for more efficient types, when the entry is full and thus it is more likely that there are more efficient contestants participating. When the number of entrants is low, the egalitarian rule ensures that entrants enjoy more surplus, which functions to maintain their participation incentive. The optimal prize design can be implemented by a winner-take-all contest with an appropriately set entry fee. With full entry, the best performer is rewarded the initial budget and all entry fees collected; with partial entry, the contest is canceled and all entrants share the available budget equally.====Our baseline analysis of the non-contingent prize structure addresses the open question of optimal prize design when negative prizes are introduced in Moldovanu and Sela's (2001) setting. In the literature of rank-order contests with incomplete information, to the best of our knowledge, only Moldovanu et al. (2012), Thomas and Wang (2013), Kamijo (2016), and Hammond et al. (2019) study this question. However, as mentioned above, all these papers focus on specific prize allocation rules and thus have not yet fully addressed the optimal design. Our optimal non-contingent design reveals that there is no loss of generality by assuming a single positive prize that equals the initial budget, which has been adopted by Moldovanu et al. (2012), Thomas and Wang (2013), and Kamijo (2016). However, our findings reveal that the restrictions they impose on negative prizes are indeed restrictive. Our paper allows full flexibility on the specification of prizes in both non-contingent and contingent prize structures. Moreover, in our analysis of the contingent prize design, a negative prize can be used to augment the prize budget, as in Fullerton and McAfee (1999), Liu et al. (2018), and Hammond et al. (2019).====In a closely related paper, Liu et al. (2018) adopt a mechanism design approach to investigate the same question studied in our paper.==== All types of contestants enter the contest at the optimum in their setting. They find that imposing an arbitrarily high entry fee and a minimum bid can extract nearly all surplus from contestants and achieve a level of total effort that is inducible in an environment in which all contestants are of the most efficient type with certainty. In their optimal contest, all collected entry fees, together with the initial budget, are awarded to the highest bidder if the bid is above the minimum bid. If everyone bids zero, the initial budget and collected entry fees are randomly allocated to each contestant to maintain the entry incentive of all types. Therefore, their optimal design crucially relies on the information on the level of bids,==== which is exactly the key difference between their model and ours—they allow information on the level of contestants' performance to be used in the mechanism, which is not feasible in rank-order contests.====Moldovanu and Sela (2001) conjecture (in their Section IV) that the effort level of Myerson's optimal seller revenue should be implementable by charging an appropriate entry fee while maintaining a single prize for the highest bidder among the entrants, which equals the organizer's initial prize budget. Since their prize design with entry fees is a non-contingent prize structure, our analysis of the non-contingent prize structure reveals that their prize design with entry fees is generally suboptimal, unless the optimal entry fee is zero. Moreover, our investigation of the optimal contingent prize structure indicates that the entry fees in Moldovanu and Sela (2001) can be better employed to top up the initial budget to reward top performers, as in Hammond et al. (2019). However, the rule of Hammond et al. (2019) can be further improved. In particular, when there is no full entry, a more lenient prize allocation rule should be adopted: All entrants should be treated equally.====Our paper contributes to the literature on optimal prize allocation in all-pay auctions with incomplete information. In addition to the pioneering work of Moldovanu and Sela (2001) and the papers mentioned above, other important contributions include the investigation of a two-stage all-pay auction framework (Moldovanu and Sela, 2006), the environment in which contestants care about their relative status (Moldovanu et al., 2007), endogenous contest success functions (Polishchuk and Tonis, 2013), the analysis of optimal crowdsourcing contests (Chawla et al., 2019), innovation contests (Erkal and Xiao, 2019), contests with entry costs (Liu and Lu, 2019), large contests (Olszewski and Siegel, 2020), and contests with convex costs (Zhang, 2022). In particular, Zhang (2022) characterizes a necessary and sufficient condition for the optimality of winner-take-all with convex effort costs. All these papers assume positive prizes.====Our paper is also related to the literature on prize design in all-pay contests with complete information. Recently, Ghosh and Hummel (2018) introduce cardinal information on contestants' performance to rank-order contests; Xiao (2019) studies ability grouping and characterizes the optimal prize structure; Fang et al. (2020) find that more unequal prize structures lead to lower effort provisions, when contestants are symmetric and the effort cost is convex; Letina et al. (2022) provide a general approach to study the contest design when the organizer can choose both the prize profile and the contest success function.====Several studies investigate the prize allocation in rank-order tournaments with complete information, pioneered by the seminal work of Lazear and Rosen (1981). Recent contributions include Akerlof and Holden (2012), Ales et al. (2017), Drugov and Ryvkin (2020, 2021), and Drugov et al. (2022). In particular, Drugov and Ryvkin (2020) fully characterize the optimal prize vector with risk-neutral agents and show that the optimum crucially depends on the shape of the hazard rate of the distribution of shocks—winner-take-all is optimal when the hazard rate is increasing, while awarding equal prizes to all but the worst performer is optimal when the hazard rate is decreasing; meanwhile, intermediate prize structures between these two extremes can be optimal for nonmonotone hazard rates. Drugov and Ryvkin (2021) generalize their analysis to the environment with risk-averse agents allowing nonseparable preferences, while Drugov et al. (2022) introduce an endogenous performance reserve into the Lazear-Rosen model and characterize the optimal prize schemes. Unlike our paper, in Drugov and Ryvkin (2020, 2021) and Drugov et al. (2022), prizes are nonnegative and there is no private information of abilities, so their analysis does not involve endogenous entry. Yet, in Drugov et al. (2022), the prize scheme can depend on the number of agents whose performance passes the reserve, which shares a common feature with our contingent prize structures.====Finally, our paper is also related to contests with endogenous entry, including Higgins et al. (1985), Kaplan and Sela (2010), and Fu et al. (2014) for the analysis of contests with costly entry, and the recent experimental study on information disclosure in Boosey et al. (2020a). All these papers assume that the agent's ability is common knowledge. Like in our paper, Liu and Lu (2019) assume that the agents' abilities are their private information in contests with endogenous entry, but they assume that prizes are nonnegative; Boosey et al. (2020b) study information disclosure in group contests with incomplete information, in which each player endogenously decides whether to join his group as a member. Broadly speaking, our paper is related to papers studying endogenous entry into one of several contests (e.g., Azmat and Möller, 2009; Konrad and Kovenock, 2012; Azmat and Möller, 2018; Morgan et al., 2018).====The rest of the paper is organized as follows. In Section 2, we set up the model. Section 3 presents the equilibrium analysis. We provide the analysis of the non-contingent optimal design with negative prizes in Section 4. Section 5 extends the analysis to contingent prize structures and Section 6 concludes. Technical proofs are relegated to the Appendix.",Optimal orchestration of rewards and punishments in rank-order contests,https://www.sciencedirect.com/science/article/pii/S0022053122001843,28 December 2022,2022,Research Article,46.0
"Blume Andreas,Lai Ernest K.,Lim Wooyoung","Department of Economics, University of Arizona, United States of America,Department of Economics, Lehigh University, United States of America,Department of Economics, The Hong Kong University of Science and Technology, Hong Kong","Received 27 January 2022, Revised 18 November 2022, Accepted 13 December 2022, Available online 19 December 2022, Version of Record 3 January 2023.",https://doi.org/10.1016/j.jet.2022.105593,Cited by (0),"We experimentally compare mediated (cheap) talk with direct (cheap) talk. Theory, guided by a characterization of equilibria in both environments, suggests that mediated talk has the potential to improve information sharing and welfare relative to direct talk. We sharpen the theory prediction by invoking ===='s (====) language-anchored level-==== analysis. In the experiment, we find that mediated talk can indeed facilitate information transmission. We also find, however, that this requires that the language employed conforms with the mediation mechanism: mediation mechanisms improve information sharing for a variety of conforming languages, but fail to do so with a nonconforming language. These experimental findings match the predictions from the language-anchored level-==== prediction.","Information transmission is a ubiquitous part of economic activity and inefficiencies affecting it entail potentially significant costs to society. A principal source of inefficiencies is the incentive for strategic manipulation resulting from divergence of interests among communicating parties (Crawford and Sobel, 1982). The extent of these inefficiencies depends on the rules and protocols that govern communication. In this paper we engage in a ==== exercise: we investigate whether introducing a (non-strategic) mediator can improve information transmission.====Various authors propose ways for organizations to improve internal information transmission by implementing communication protocols that mitigate misreporting. Harris and Raviv (2010) suggest the use of intermediaries, like the board of directors, to achieve better communication between management and shareholders. Laclau et al. (2020) demonstrate the benefits from requiring reporting through multiple channels and use it to rationalize the matrix organization as a management structure. Ambrus et al. (2013a) look at legislative committees as information intermediaries and show, building on work by Ivanov (2010), that with strong misalignment of interests a biased committee can improve information transmission between privately informed lobbyists and the legislature.====A common feature of the proposed protocols is that they enable the garbling of information. With a non-strategic intermediary that garbling can be achieved directly. With a strategic intermediary, it may be the result of (and require) the intermediary using a mixed strategy (see Ambrus et al., 2013b). There is little experimental work that would shed light on whether these garbling schemes are effective and on how to make them more effective. We see this paper as a beginning of a systematic investigation of communication protocols that mitigate misreporting incentives by facilitating the garbling of information. We focus on the most direct implementation of garbling in sender-receiver games, through a non-strategic mediator.====The rationale for the benefits of garbling in sender-receiver games is straightforward: A sender who is reluctant to provide information will be more willing to do so if it is known that information is degraded by garbling. The receiver prefers garbled information to no information. Under the right conditions both parties gain.====The potential for mediation to improve information transmission has long been recognized. Forges (1985) presents an example of an information transmission game in which communication is entirely ineffective with direct communication, while both sender and receiver can gain from communication via an appropriately chosen mediation scheme. The underlying logic is nicely illustrated by Myerson (1991) with a story of a sender and receiver communicating via a messenger pigeon. The sender has the choice of either sending the pigeon or not sending the pigeon. If the pigeon is sent, it gets lost with some probability. There are two sender types, one who prefers to be revealed and another who prefers to be concealed. There is an equilibrium in which the type who prefers to be revealed sends the pigeon and the other does not send the pigeon. When the pigeon does not arrive, the receiver does not know whether it was never sent or was sent but got lost. As a result, when the pigeon does not arrive the receiver remains uncertain about whether he is dealing with one type or the other. The type who prefers to be concealed remains concealed, and thus achieves deniability, whereas the type who prefers to be identified manages to get identified at least some of the time.====More recently Goltsman et al. (2009) have taken up the question of mediation in the context of the leading example in Crawford and Sobel (1982). They identify an efficiency bound for optimal mediation. Via the revelation principle (Myerson, 1982) this is also the bound for any other communication protocol, including, for example, repeated face-to-face communication as considered by Krishna and Morgan (2004). Blume et al. (2007) demonstrate that the efficiency bound can be attainted in a single round of communication through a simple noisy channel: the sender sends a message to the receiver that goes through with some probability as sent; otherwise the message is replaced by a random draw from some fixed distribution. The equilibria that achieve the efficiency bound with this noisy channel exhibit a structure reminiscent of the messenger pigeon example: (sets of) high types are sometimes pooled with (a set of) low types, and otherwise revealed.====We take mediation to the lab. To our knowledge, this is the first paper that experimentally compares direct with mediated cheap-talk communication. To keep the comparison manageable and induce salient incentives, we use a two-type incentive structure and a mediation rule that closely mirrors the one employed in the messenger pigeon example.====Our objective is twofold. First, we are interested in comparing the outcomes from direct talk, where the receiver observes the sender's message as sent, and mediated talk, where the sender's message is filtered through a noisy channel. Second, and intimately related, we are interested in how the language that is available to subjects affects mediation outcomes, where by “language” we refer to the framing of messages in our experiment. The question of whether mediation can improve on direct communication is a mechanism design problem. The language that is employed is part of the mechanism. It affects whether the mechanism is direct or indirect, whether desired outcomes can be supported with equilibria that require truth-telling, and whether desired outcomes can be supported with equilibria that require players to be obedient.====In an equilibrium analysis of communication games, the choice of language makes no difference at all (assuming that there is a one-to-one mapping between languages). To obtain sharper predictions, we complement and refine the equilibrium perspective with a level-==== analysis. In the level-==== analysis the language is central because, through pinning down level-0 behavior, it provides the anchoring of the level-==== hierarchy. Given the multiplicity of equilibria in our games, both with and without mediation, it is interesting to know whether language affects equilibrium selection.====Most of the prior experimental literature on direct talk frames messages in terms of payoff types or sets thereof. Framing messages in terms of actions is a natural, under-explored, alternative. With mediation, there are additional concerns: (i) the language used for messages sent to the mediator may or may not match the language used for messages received from the mediator; (ii) the language determines whether the mechanism is direct or indirect; and (iii) languages admitting truthful behavior may either ====, by admitting truthful equilibria, or may not have truthful equilibria, and thus fail to conform with the mechanism.====For the sake of experimental control, and to prevent ballooning sizes of strategy spaces, the languages of our experiment are primitive. They lack almost all distinguishing features of human language: there is no syntax; the semantics is not productive; there is no generative structure (see, for example, Chierchia and McConnell-Ginet, 2000). All our languages provide are minimal semantics that make it plausible for messages to refer to types, in one case, and to actions, in the other.====A natural frame to adopt with a mediator is that of a direct mechanism. In a direct mechanism the sender makes reports about her type to the mediator, using a language consisting of ==== (declarations of her types). Based on those reports, the mediator makes action recommendations to the receiver, using a language consisting of ==== (directions of which action to take).==== In contrast, with direct communication the spaces of sent and received messages are identical.====Accordingly, we consider five different mediated-talk scenarios: (i) a ====, as described above; (ii) a mechanism that uses directives for both inputs and outputs, which we call ====; (iii) a mechanism that uses declaratives exclusively, and has the language conform with the mechanism, which we term ====; (iv) a mechanism that also exclusively uses declaratives, but so that the language does not conform with the mechanism, which we refer to as ====; and, (v) a mechanism in which the declaratives language conforms with the mechanism but the garbling rate is insufficient to render mediation effective, which we refer to as ====. We compare these with each other and with the two direct talk scenarios, ====, in which messages are framed as directives, and ====, in which messages are framed as declaratives.====We obtain our theoretical predictions from augmenting a standard equilibrium analysis with a level-==== approach that, following Crawford (2003), is anchored at the focal meanings of messages: level-0 senders are taken to be forthright (i.e., truthful with a declaratives language and indicating their preferred response with a directives language) and level-0 receivers to be credulous (in the sense of best-responding to level-0 senders, following Crawford, 2003); higher-level behavior is obtained by iterating best replies. The level-==== prediction refines the equilibrium prediction. For direct talk, the equilibrium analysis by itself, without invoking level-==== reasoning, predicts pooling, which can be achieved with having messages be independent of the sender's type. With mediated talk, the equilibrium analysis by itself allows for both separation, where the two types send distinct messages, and pooling. The level-==== analysis is consistent with the equilibrium analysis: at all levels above level 0 players use equilibrium strategies. The level-==== prediction refines the equilibrium prediction: For direct talk it singles out exactly one message that will be sent. For mediated talk it predicts separation with conforming languages and pooling on a particular message with a non-conforming language.====We find that, in line with these theoretical predictions, the modal observed behavior of both senders and receivers converges to pooling with direct talk, to separation with mediated talk with conforming languages, and to pooling with mediated talk when the language is non-conforming. The agreement with the theoretical predictions extends to the details of behavior, notably message use. The difference in behavior between direct talk and mediated talk with a conforming language translates into payoff advantage of the latter over the former.====There is a stark difference in observed behavior under mediation depending on whether the language does or does not conform with the mechanism. Mediation mechanisms improve information sharing for a variety of conforming languages, but fail to do so with a nonconforming language. These experimental findings match the predictions from the language-anchored level-==== analysis. Strikingly, this is the case even when a whole array of alternative selection criteria (including iterative deletion of dominated strategies, strict equilibrium, Pareto efficiency etc.) make a unique common prediction that sharply disagrees with the language-anchored level-==== prediction.====There is a rich literature on sender-receiver game experiments with direct talk. We survey that body of work in Blume et al. (2020). The earliest paper in this stream is Dickhaut et al. (1995) who implement a discretized version of the Crawford-Sobel model. Much of this literature frames sender messages as reports of types or sets of types. An exception is Blume et al. (2001), who consider messages that are framed as action recommendations. In the present paper, one of the questions we ask is whether the framing of messages, the choice of language, has an impact on the performance of mediation mechanisms.====The experimental papers most closely related to ours are Nguyen (2016), Blume et al. (2019), and Fréchette et al. (2022). Nguyen and Fréchette et al. experimentally investigate Bayesian persuasion (Kamenica and Gentzkow, 2011).==== With Bayesian persuasion, like in the present paper, the receiver observes a garbled signal of the state of the world; unlike here, there is no private information and the sender can fully commit to a signaling rule that maps states of the world into signals.==== Blume et al. (2019) model and implement Warner's (1965) randomized response method in the lab. Under randomized response, garbling is entirely under the control of the sender, and for that to be incentive compatible it is necessary that the sender has a preference for compliance with the procedure, which may be in the form of deriving utility from truth-telling. While messages in the present paper are cheap talk, communication under randomized response amounts to costly signaling. Ours is the first paper that looks at the effects of garbling cheap-talk messages in sender-receiver games.====Casella et al. (2020) experimentally study mediation in a two-player conflict resolution game based on theoretical work by Hörner et al. (2015). Both players have private information, send messages, and take actions after the exchange of messages. Like in the present paper garbling of messages offers the promise of efficiency gains, in this case through a reduction in conflict. They find that mediation significantly affects behavior at the communication stage but does not reduce conflict.====Chassang and Zehnder (2019), building on Chassang et al. (2019), use an experiment to investigate the role of garbling in encouraging whistleblowers to come forward. In their environment exogenous garbling of messages is predicted to help create a deterrent: if whistleblowers' reports of observed misbehavior are received even when not intended, committing to retaliate becomes costly for the misbehaving party, whereas without garbling commitment to retaliation can be an effective off-path threat. In their experiment, Chassang and Zehnder (2019) find support for this prediction.====Our paper intersects with the broader literature on truth-telling and obedience in mechanism design. For some of the mechanisms we consider, messages to the mediator can be viewed as type reports. For others, messages from the mediator can be viewed as action recommendations. In our direct mechanism, we have both type reports and action recommendations. When messages are type reports, we can look at truth-telling behavior of senders. When they are action recommendations, we can ask whether receivers are obedient. The experimental literatures on using VCG mechanisms to deal with public goods problems (surveyed in Chen and Ledyard, 2010) and on using strategy-proof mechanisms to address school choice (e.g. Chen and Sönmez, 2006, and the survey by Hakimov and Kübler, 2021) have examined truth-telling behavior when it is a dominant strategy. The experimental literature on implementing correlated equilibria (e.g., Duffy and Feltovich, 2010) has asked whether players are obedient when given action recommendations that are exogenously generated from a correlated equilibrium distribution. We add to these perspectives, by (i) having truth-telling incentives (of senders) be contingent not only on the mechanism, but also on the behavior of receivers, and (ii) by having obedience incentives (of receivers) not only dependent on the mechanism and other players' actions, but also on the information revealed by senders. Neither senders nor receivers in our mechanisms have dominant strategies.====Our interest in language in mechanism design has antecedents in both the literature on auctions and the one on school choice. Masatlioglu et al. (2012) experimentally compare direct with indirect mechanisms by varying the language through which bidders generate their bids in a first-price auction. Bichler et al. (2022) is a recent contribution to the literature on bidding languages in combinatorial auctions. They propose and evaluate an alternative to the widely used enumerative exclusive or (XOR) bidding language. Bichler et al. (2014) use a laboratory experiment to compare simple bid languages with more expressive bidding languages. Calsamiglia et al. (2010) compare school choice mechanisms in which parents have access to a language that lets them fully express their ranking of schools with alternative languages in which parents are constrained to list a limited number of schools.====In the next section we introduce the communication protocols and the incentive structure. In Section 3 we discuss the theoretical predictions. Section 4 describes our experimental treatments and procedures. In Section 5 we report our findings and in Section 6 we discuss our findings and possible extensions.",Mediated talk: An experiment,https://www.sciencedirect.com/science/article/pii/S0022053122001831,19 December 2022,2022,Research Article,47.0
"Chatterji Shurojit,Kajii Atsushi","Singapore Management University, Singapore,Kwansei Gakuin University, Japan","Received 3 March 2022, Revised 16 September 2022, Accepted 3 December 2022, Available online 9 December 2022, Version of Record 15 December 2022.",https://doi.org/10.1016/j.jet.2022.105592,Cited by (0),"Do price forecasts of rational economic agents need to coincide in intertemporal perfectly competitive complete markets in order for markets to allocate resources efficiently? To address this question, we define an efficient temporary equilibrium (ETE) within the framework of a two period economy. Although an ETE allocation is intertemporally efficient and is obtained by perfect competition, it can arise without the agents' forecasts being coordinated on a perfect foresight price. With time-separable utilities, we show that there is a one dimensional set of ETE allocations for generic endowments. Moreover, these efficient allocations can be supported by forecasts that disagree up to one degree of freedom. Thus, strong as efficiency and perfect competition may appear, they do not imply perfect foresight, but they do add explanatory power to temporary equilibrium, since they select a small subset out of the Pareto efficient allocations, which generally have higher dimension.","Intertemporal trade in complete markets is known to achieve Pareto efficiency when the price forecasts of agents coincide and are correct. But this perfect alignment of forecasts does not appear to sit well with the spirit of perfect competition, since in a price-taking model agents are not expected to anticipate the market environment beyond the market prices they commonly observe; we therefore study intertemporal trade without requiring that price forecasts of heterogeneous agents coincide.====To address this issue precisely, we consider a sequence of commodity markets with no uncertainty, where there is a riskless bond market so that markets are complete. Specifically, we consider a two period (periods 0 and 1 respectively) pure exchange economy with at least two households, with finitely many perishable commodities in each period, and a riskless bond that pays in period 1 dollars. We ask what Pareto efficient allocations can be decentralized by a Walrasian model that respects the intertemporal structure, i.e., there be competitive spot markets for each period for the consumption goods available in that period, and a competitive market for the bond in period 0.====In period 0 each household optimizes given spot prices and the bond price observed in period 0, and its price forecast for the period 1 spot prices. The price forecasts of different households are allowed to be heterogeneous. The period 0 spot prices and the bond price are determined to clear the markets in period 0. Given the savings of the households from period 0, the period 1 spot prices emerge to clear the commodity markets in period 1; these market clearing spot prices will be in general different from the heterogeneous forecasts made by the agents in period 0. The resulting equilibrium is referred to as a temporary equilibrium (henceforth TE).====In this set up, if one ==== that the price forecasts of all agents coincide and agree with the period 1 market clearing prices, the resulting temporary equilibrium is referred to as a perfect foresight equilibrium (PFE). With the bond market, the markets under perfect foresight are complete, the ensuing equilibrium allocation coincides with an Arrow Debreu (AD) allocation and is Pareto efficient by the first fundamental theorem of welfare economics. This is of course an instance of the classic result formalized by Arrow (1964) and then elaborated by Radner (1972). Thus under perfect foresight, the AD allocations are the only ones that can be decentralized as Walrasian (temporary) equilibria. Moreover, by the theorem of Debreu (1970), there are finitely many AD allocations, generically in endowments. Therefore, generically in endowments, the set of Pareto efficient allocations that can be decentralized as Walrasian equilibria with perfect foresight is zero dimensional.====The PFE approach explains market prices and is able to address welfare issues, but it incurs a serious cost in that perfect foresight is assumed, rather than derived. The assumption of perfect foresight is extraordinarily strong as is expressed by various scholars; a case in point is Radner's own critique of perfect foresight.==== It goes without saying that this approach is absolutely inadequate for comparing the quality of price forecasts and explaining, among other issues, the use of policy tools that seek to influence the price forecasts of diverse economic agents. In spite of these obvious shortcomings, the pervasive use of this approach would appear to stem from the presumption that perfect foresight is indispensable to a market theory that addresses efficient intertemporal resource utilization and retains some predictive power.====Let us then ask if perfect foresight can be derived from some fundamental economic principle in the spirit of perfect competition as follows. First require that all the spot markets clear in the temporary equilibrium sense to maintain perfect competition for contemporaneous transactions. In particular, even when the households traded anticipating wrong prices in the past, describe how they consume and save competitively in every period. This first requirement seems natural and indispensable to address welfare issues in a transparent and tractable manner.====Secondly, assume that the exchange process exhausts all intertemporal gains to trade even though the forecasts do not necessarily end up getting fully aligned, so that the resulting sequence of consumption constitutes an intertemporally Pareto efficient allocation. We do so as it is our intention to investigate the extent to which forecast alignment is implied by the hypothesis that competitive markets allocate resources efficiently in an intertemporal setting. The two requirements raised so far in effect lead us to efficient temporary equilibria (ETE).====Thirdly, we require that market clearing prices induce time consistent behavior. To motivate this requirement, note that at PFE prices of periods 0 and 1, agents in period 1 do not wish to re-trade not only their period 1 consumption bundles, but also their period 0 consumption bundles even if they could. That is, agents do not regret their past choices, and in this sense they are retrospectively consistent. The time consistency of choices of this kind however does not necessarily hold at a TE; agents' period 0 consumption, optimized with respect to their forecasts, might be extremely inadequate if the market prices turn up to be very different from the forecasts. In order to put more discipline into and sharpen the focus of our investigation, we implicitly rule out such time inconsistent forecasts by directly imposing a retrospective consistency condition on ETE. Even though retrospective consistency is not a prescriptive device that agents can follow for generating forecasts,==== we interpret this time consistency property as a desirable axiom for an intertemporal solution concept from an ex post perspective. The property appears appropriate since it addresses in some measure a long standing criticism of the TE approach, namely, that it imposes hardly any restrictions on forecasts and it has little explanatory power. Moreover, it holds automatically if preferences are assumed to be time-separable in the sense that they are represented by a function of temporary utility levels. Time separability does not require time additivity nor recursiveness, so it allows a very general class which seems to cover most standard applications of dynamic economies.====In short, we propose an efficient temporary equilibrium with retrospective consistency (ETEC) as our solution concept. An allocation arising from an ETEC is by construction decentralized by market prices, but the underlying price forecasts are in principle not necessarily common. The question we pose is, must an ETEC necessarily be a PFE?====At first sight the answer might appear positive, under the standard set of assumptions on utility functions such as monotonicity, concavity, and differentiability. Observe first that the dimension of Pareto efficient allocations is one less than the number of the households, since it reflects the set of wealth transfers across households. On the other hand, at an ETEC, since the final consumption bundle must be attained in markets, each household's consumption bundle must satisfy some budget constraint. By market clearing one of these budget constraints might be redundant, but still these create additional restrictions at least as many as the dimension of Pareto efficient allocations. Recall that the set of AD equilibrium allocations can be found from Pareto efficient allocations and budget constraints by the second fundamental theorem of welfare economics using the Negishi method. As mentioned earlier, Debreu's generic finiteness theorem shows that the set of AD equilibria is zero dimensional generically. Therefore, the same logic seems to suggest that the set of ETEC allocations is zero dimensional, at least generically. Hence if an ETEC which does not entail perfect foresight ever exists, it must be an isolated case relying on some coincidence.====The surprise, the aforementioned logic notwithstanding, is that this conjecture is incorrect. More precisely, we show in our main result the existence of a one dimensional set of ETEC allocations around each Arrow-Debreu equilibrium allocation, generically in endowments whenever the utilities of households are time separable. For this purpose, we introduce the notion of a Quasi-ETEC, which is obtained by relaxing the role of forecasts in an ETEC. We establish a generic indeterminacy result for Quasi-ETEC for general utility functions: generically, Quasi-ETEC allocations constitute a one dimensional manifold around each PFE allocation. Since an ETEC is a Quasi-ETEC, the dimension of Quasi-ETEC allocations is therefore at most one for general utility functions. We then impose time separability of utility functions for establishing the equivalence of Quasi-ETEC with ETEC, to clarify the role of the assumption of time separability. Curiously enough, the degree of real indeterminacy does not depend on the number of households, while the dimension of Pareto efficient allocations is, generically, one less than the number of households.====Price forecasts in our model are not observable and indeterminate: in an ETEC which is not a PFE, households behave as if they have different price forecasts in mind. Since there is no equilibration among price forecasts of different households by definition, there tend to be some degrees of freedom about the individual price forecasts which support the same ETEC. But nonetheless one might want to ask if and how an ETEC allocation appears as if a consequence of aligned forecasts. As we will demonstrate in the process of establishing our generic indeterminacy result, when utility functions are time separable, an ETEC can be sustained with households' forecasts agreeing, and being correct, on second period relative prices but disagreeing on the inflation rate.====Some intuition behind one dimensional indeterminacy can then be given as follows. An ETEC allocation being efficient is associated with implicit income transfers among households. Since spot markets are complete, the transfers must be implied by the structure of forecasting errors. On the other hand, if the forecasts about the relative prices are set to be common and correct, the transfers must result from the realized rate of inflation being incorrect in the sense that it is different from that at a PFE (or else it constitutes a PFE), which provides just one degree of freedom.====Coming back to the question we posed above, namely whether or not an ETEC is necessarily a PFE, our answer is that decentralized markets are able to deliver a significantly larger set of acceptable Pareto efficient outcomes under less restrictive assumptions on forecasts. The extra degree of freedom due to allowance for heterogeneity of forecasts is at most one, and exactly one with time separability in our model, so the explanatory power is almost as strong as the perfect foresight approach, marking a stark contrast with the classical temporary equilibrium literature (e.g., Green (1973), Grandmont (1977)), which assumes, rather than derives, price forecasts and hence suffers from lack of explanatory power.====Moreover, without time separability, a Quasi-ETEC might not be an ETEC in a robust way; that is, efficiency and retrospective consistency imply perfect foresight in some economies. We provide a robust example to verify this assertion, while other examples suggest ETEC may not imply perfect foresight when the number of commodities in period 1 exceed the number of commodities in period 0, although we do not know how the class of such economies can be characterized at this point.====We conclude this introduction by some observations on the nature and limitations of our contribution. The early literature on TE mentioned above was primarily concerned with issues of the existence of an equilibrium with exogenously fixed forecast functions in models with money or competitive asset markets that included futures markets and markets for financial securities. Recent work that follows this methodology has focused on models where the forecasts can be interpreted as bounded rational forecasts (which may or may not embody some form of learning behavior): the proposed forms of forecasts deviate in some measure from the rational expectations hypothesis (RHH) but can better explain empirical observations that are at odds with the predictions of the RHH.==== The broad approach of this literature has been to allow some sorts of deviations of forecasts from the RHH that cause inefficiencies and then study the scope of policy interventions in making welfare improvements. Our approach is different in that our solution concept is allocation based: we impose the efficiency of allocations without restricting forecasts in order to demonstrate that forecasting errors are consistent with efficient markets and characterize the set of allocations that can be sustained by heterogeneous forecasts. This clarifies the parts of the Pareto set that can be decentralized via Walrasian markets in a dynamic set up and shows that strong as the efficiency and time consistency conditions may appear, they do not imply perfect foresight, but they do add explanatory power to TE, since they select a small subset out of the Pareto efficient allocations, which generally has a higher dimension.====A common interpretation of the PFE (as indeed of any Walrasian behavior based solution) is that it is an idealization of an out of equilibrium adjustment process that exhausts gains to trade and achieves full alignment of forecasts. This interpretation points to the literature that models out of equilibrium behavior that produces AD equilibrium outcomes.==== This literature has to inevitably contend with complex issues of how much strategic behavior to incorporate, how much bounded rationality is admissible, and the extent to which common knowledge assumptions are warranted on the induced state space of the adjustment process. The notion of ETEC is by no means exempt from such concerns: A complete theory of decentralizability via ETEC would require a fully articulated out of equilibrium adjustment process that would endogenously explain the exhaustion of gains to trade and the emergence of the requisite retrospectively consistent prices for the final allocation simultaneously. In view of the aforementioned issues, we do not attempt this task here and leave it for future work. Our analysis can be interpreted as one that identifies ETEC as those Pareto efficient allocations that can serve as the desired accumulation points of such adjustment processes and establishes that there is a larger but precise set of allocations that is consistent with efficient markets.====The paper is organized as follows. Section 2 specifies the model and the Definitions. Section 3 introduces the key auxiliary notion of a Quasi-ETEC. Section 4 provides a characterization result of Quasi-ETEC. Section 5 establishes the generic indeterminacy of Quasi-ETEC while Section 6 proves our main result on the indeterminacy of ETEC. Section 7 studies a class of homogeneous economies where the set of Quasi-ETEC can be explicitly characterized and discusses the role of time-separable utilities in our analysis. Section 8 concludes with observations on the role of forecasts, on extensions of our results, and discusses related literature.",Decentralizability of efficient allocations with heterogeneous forecasts,https://www.sciencedirect.com/science/article/pii/S002205312200182X,9 December 2022,2022,Research Article,48.0
"McAdams David,Song Yangbo,Zou Dihan","Fuqua School of Business and Economics Department, Duke University, United States of America,School of Management and Economics, The Chinese University of Hong Kong, Shenzhen, China,Economics Department, University of North Carolina at Chapel Hill, United States of America","Received 4 May 2022, Accepted 26 November 2022, Available online 6 December 2022, Version of Record 20 December 2022.",https://doi.org/10.1016/j.jet.2022.105591,Cited by (0),"During an infectious-disease epidemic, people make choices that impact transmission, trading off the risk of infection with the social-economic benefits of activity. We investigate how the qualitative features of an epidemic's Nash-equilibrium trajectory depend on the nature of the economic benefits that people get from activity. If economic benefits do not depend on how many others are active, as usually modeled, then there is a unique equilibrium trajectory, the epidemic eventually reaches a steady state, and agents born into the steady state have zero expected lifetime welfare. On the other hand, if the benefit of activity increases as others are more active (“social benefits”) and the disease is sufficiently severe, then there are always multiple equilibrium trajectories, including some that never settle into a steady state and that welfare dominate any given steady-state equilibrium. Within this framework, we analyze the equilibrium impact of a policy that modestly reduces the transmission rate. Such a policy has no long-run effect on society-wide welfare absent social benefits, but can raise long-run welfare if there are social benefits and the epidemic never settles into a steady state.","In 2005, a team of researchers led by Yale School of Medicine Professor Neel Gandhi descended on a rural hospital in KwaZulu Natal, South Africa to document the prevalence of drug-resistant tuberculosis (Gandhi et al., 2006). Of 542 patients diagnosed with active tuberculosis (TB), 53 had “extensively drug-resistant” (XDR) infections that were resistant to all of the first-line antibiotics typically used to treat TB as well as multiple second-line treatments. Worse yet, this XDR-TB strain was especially virulent: half of those with XDR-TB infection were dead within 16 days of identification, and only one survived for a full year.==== “Totally-resistant” TB strains that are untreatable with any known antibiotic have been identified in Italy, Iran, India, and elsewhere (Velayati et al., 2013; Khawbung et al., 2021). Fortunately, none of these nightmare pathogens has yet succeeded in launching a global pandemic. But once that does happen, and an untreatable pandemic-potential TB strain arrives in places like Europe and the United States, what will happen next? What course will the epidemic take? And what long-term impact will this novel pathogen have on society-wide welfare, including not just the direct harms due to the disease but also the indirect economic and psycho-social harms associated with efforts to avoid infection?====How a novel infectious disease such as untreatable TB spreads through a human population and how much harm it inflicts on people's health and prosperity depends on people's behavior, which itself changes during the course of the epidemic. This feedback between human behavior and pathogen transmission determines the equilibrium trajectory of the epidemic. The field of economic epidemiology seeks to further our understanding of equilibrium epidemics through models of behavior during an epidemic. Such models can be used to analyze the path of a novel infectious disease (Farboodi et al. (2021), Garibaldi et al. (2020), Keppo et al. (2020), McAdams (2020), Toxvaerd (2020), and references therein), to evaluate policy options for managing an unfolding epidemic (on optimal lockdown policies, see Acemoglu et al. (2021), Alvarez et al. (2021), Bethune and Korinek (2020), Jones et al. (2021), and Rowthorn and Maciejowski (2020)), and to quantify the social value of new vaccines and treatments (Makris and Toxvaerd (2020)), among many other things—but only if these models adequately capture the underlying economic-epidemiological environment.====Many assumptions about the economic-epidemiological environment are implicit in any economic-epidemic model, including (i) ecological and epidemiological assumptions about the disease process itself and (ii) economic assumptions about agents' information, interactions, and payoffs. McAdams (2021) surveys the recent Covid-inspired literature, categorizing economic-epidemic models based on their assumptions about immune response, manner of transmission, and economic impacts. Avery et al. (2021) provides an insightful discussion of several of these modeling dimensions, focusing especially on how agent heterogeneity can impact the qualitative features of equilibrium outcomes.====In this paper, we focus on the impact of an economic assumption that has not yet received much attention. Specifically, consider the social-economic activities that increase the risk of pathogen transmission (“transmissive activities” or simply “activity”). When an agent engages in these activities, does the benefit that they enjoy depend on whether others are also active? In other words, are the activities that drive transmission social in nature (such as working in-person at an office rather than virtually from home) or non-social (such as exercising in a gym). We show that incorporating social motivations into the economic model of an infectious-disease epidemic can have profound equilibrium implications for how the disease will progress and persist over time.====To illustrate the novel aspects of our analysis as clearly as possible, we employ an especially simple epidemiological model, a Susceptible-Infected-Removed (SIR) model with vital dynamics in which infected agents never recover from infection but may be “removed” due to death from the disease, agents also die at a constant rate from other causes, and there is a constant flow of newborn agents susceptible to infection. The special case in which no one dies from the disease is a Susceptible-Infected (SI) model. We develop our main findings first in the SI model, and then extend the analysis to allow for disease-induced death.====In the SI model, we first consider the benchmark case in which the economic benefits of activity do not depend on others' activity choices. For an epidemic that starts from an initial condition with low infection prevalence and sufficiently severe disease, we show:====Interesting differences also arise in terms of equilibrium comparative statics, with policy-relevant implications. For example, consider the long-run impact of a policy that somewhat reduces the transmission rate, such as improving ventilation, providing free masks, or developing an imperfect immunotherapy or vaccine. In the benchmark case without economic complementarities, such a policy changes how many people are infected in the unique SSE (ironically, ==== people are infected in the new steady state when the disease is sufficiently severe) but newborn agents continue to have zero expected lifetime welfare. The societal benefits of the new policy are therefore transitory in nature, undone by agents' equilibrium behavioral response. By contrast, if there are social benefits to activity and the epidemic never settles into a steady state, then the policy can increase long-run society-wide welfare.====The most novel aspect of our analysis is that non-converging epidemic trajectories can emerge in equilibrium once there are social benefits associated with transmissive activity. In addition to the ==== differences emphasized above, the possibility of non-converging equilibrium behavior can also have substantial ==== implications. For instance, in the numerical example illustrated in Fig. 4(c), about 90% of the population is infected in the unique steady-state equilibrium of the epidemic but non-converging equilibrium trajectories also exist in which only about 20% of the population is infected in the long run. Predictions and policy recommendations derived from models that abstract from economic complementarities and/or that restrict attention to equilibrium trajectories that converge to a steady state could therefore be substantially off-base.==== Like the vast majority of the recent Covid-inspired literature, this paper follows and builds on what we refer to as the “standard model,” introduced in Geoffard and Philipson (1996) (“GP”) and developed further by Reluga (2010) and others. In the literature following GP, agents know their own health status, transmission occurs whenever an infected person is randomly matched with a susceptible one, and the likelihood that any two agents are matched depends on how active they each choose to be. Most closely related is Toxvaerd (2019), who analyzes a Susceptible-Infected-Susceptible model with recovery and re-infection where strategic and forward-looking agents choose their individual level of exposure dynamically, under both centralized and decentralized decision making. By contrast, we focus on the decentralized case and work within a Susceptible-Infected-Removed model without recovery.====What distinguishes our paper from the rest of this literature is that we allow for economic complementarities of activity in a dynamic setting with forward-looking agents.==== We find that complementarities can have novel and profound qualitative and quantitative implications for the set of equilibrium epidemic trajectories. Most notably, we show that any equilibrium trajectory that enters a steady state is welfare dominated by other equilibrium trajectories that never converge but instead eventually ==== over time. Moreover, the difference in infection prevalence and welfare between oscillating and steady-state equilibria can be quite large in some cases. In environments where the benefits from activity depend on others' activity, such as when employees decide whether to work from home, analyses that abstract from complementarities and from non-convergent behavior may therefore generate inaccurate predictions and policy conclusions.====We appear to be the first in the economic literature==== to analyze non-converging equilibrium epidemic trajectories, but a variety of mechanisms have been identified that can lead to equilibrium multiplicity. Kremer (1996) and Chen (2012) provide two interesting examples, where multiplicity arises as a result of a more complex transmission technology. In a pioneering early paper, Kremer (1996) considers a model in which agents control how many encounters they have, but who they meet depends on who else is looking to meet. In that context, multiple equilibria naturally arise due to a selection effect: If few uninfected people are looking to meet, then most encounters will be with infected people and it is an equilibrium for the uninfected to avoid others. On the other hand, if many uninfected people are looking to meet, then each encounter poses less exposure risk and hence becomes more attractive for the uninfected. Within the standard model, Chen (2012) shows that multiple equilibria can exist if there is crowding in transmission, more precisely, if the “contact rate” (encounter rate per unit of others' overall activity, typically assumed constant) is decreasing in overall activity.====Also related is Philipson and Posner (1993 “PP”) and the insightful albeit relatively small literature that has followed, including Toxvaerd (2017) and Toxvaerd (2021). In their classic study of the AIDS epidemic, PP introduced a rich alternative modeling approach in which agents do not observe their own health status and transmission only occurs if, upon meeting, both agents consent to consummate their interaction. The need for mutual consent creates economic complementarities much as in our model, since the expected benefit and the exposure risk associated with activity both depend on others' willingness to consent. However, the implications of such complementarities on the equilibrium set have not hitherto received much attention in this literature. An earlier version of this paper, McAdams (2020), provides an algorithm to compute the set of equilibrium trajectories in a PP-esque model with asymptomatic infection and economic complementarities. However, the set of equilibria in that richer context is quite complex, making it difficult to draw clear insights from the analysis. For this reason, we focus here on a simpler model without asymptomatic infection.====The rest of the paper is organized as follows. Section 2 presents the model and some preliminary analysis. Section 3 considers the Susceptible-Infected model, corresponding to an untreatable disease from which people cannot recover but which does not kill them. Section 4 then extends the analysis to the Susceptible-Infected-Removed model, allowing infected agents to die from the disease. Section 5 considers a variety of equilibrium comparative statics, focused especially on equilibrium welfare during the endemic phase of the epidemic. Section 6 concludes.",Equilibrium social activity during an epidemic,https://www.sciencedirect.com/science/article/pii/S0022053122001818,6 December 2022,2022,Research Article,49.0
"Meisner Vincent,von Wangenheim Jonas","Technical University Berlin, Straße des 17. Juni 135, 10623 Berlin, Germany,University of Bonn, Institute for Microeconomics, Adenauerallee 24-42, 53113 Bonn, Germany","Received 22 June 2021, Revised 19 July 2022, Accepted 25 November 2022, Available online 2 December 2022, Version of Record 9 December 2022.",https://doi.org/10.1016/j.jet.2022.105588,Cited by (1),"Evidence suggests that participants in strategy-proof matching mechanisms play dominated strategies. To explain the data, we introduce expectation-based loss aversion into a school-choice setting and characterize choice-acclimating personal equilibria. We find that non-truthful preference submissions can be strictly optimal if and only if they are top-rank monotone. In equilibrium, inefficiency or justified envy may arise in seemingly stable or efficient mechanisms. Specifically, students who are more loss averse or less confident than their peers obtain suboptimal allocations.","Strategy-proof mechanisms offer a celebrated solution to the problem of matching prospective students to schools. For instance, deferred-acceptance (DA) mechanisms implement stable allocations, and top-trading cycles (TTC) implement efficient allocations. Consequently, such mechanisms are used in many existing school choice programs.==== By submitting their true preferences, students can maximize the probability of getting into their most preferred school without hurting their chances of admission to other schools. Unfortunately, growing evidence from both the field and the lab suggests that (especially, but not only) students with low priority tend to conceal their preferences for popular schools and mimic preferences for district schools despite the dominance of the truthful reporting. Hence, potentially, none of the desired properties such as efficiency and stability may be obtained.====We explain this puzzle with expectation-based loss aversion (EBLA, Kőszegi and Rabin, 2006, Kőszegi and Rabin, 2007). In our framework, the preference report is a channel to manipulate the expectations to which final match outcomes are compared. Ranking a popular school behind a less preferred school is always costly in terms of the expected match utility, as such a rank-ordered list (ROL) shifts a part of the match probability to an inferior school. However, it also mitigates disappointment, and not even trying to get into the popular school by dropping it completely shields off potential disappointment with respect to admission at this school. We characterize that ROLs are strictly rationalizable as a choice-acclimating equilibrium (CPE) in static strategy-proof mechanisms if and only if they satisfy a property we call top-rank monotonicity, which is a testable prediction.==== This theoretical foundation of commonly observed deviations is the first contribution of this paper.====Secondly, we show that these misrepresentations may give rise to justified envy and inefficiency in equilibrium. We analyze choice-acclimating Bayesian Nash equilibria when heterogeneously loss-averse students compete for scarce seats at elite schools. More specifically, loss-averse students decide to apply to their district schools over the elite schools if they are pessimistic about their admission chances. Consequently, weaker students with a lower degree of loss aversion (or higher degree of confidence), who submit true preferences, are accepted instead. In that sense, strategy-proofness does not “level the playing field,” voiding one of the crucial advantages prominently named by Pathak and Sönmez (2008). Our model also highlights a flaw in the empirical strategy to identify preferences reported to strategy-proof mechanisms as true. Regarding affirmative action policy, this insight is important because the observation that certain students do not rank certain schools does not necessarily mean that they prefer other schools.====In our model, students privately learn their match values for each school and their individual degree of loss aversion. Moreover, they receive a signal about their relative priorities compared to the other students at each school. Generally, given beliefs about the other students' priorities and strategies, a student's preference report corresponds to a lottery over match outcomes. For instance, by swapping two schools' ranks in the reported ROL, match probability mass is shifted from one school to the other. With respect to match utility alone, truthful reporting is a dominant strategy and, thus, induces a lottery that first-order stochastically dominates any lottery induced by any other ROL. Following the CPE framework by Kőszegi and Rabin (2007), the chosen outcome lottery constitutes the reference point. In addition to match utility, students receive psychological utility from comparing an outcome to the reference point. Since losses with respect to the reference point are weighted stronger than gains, any uncertainty in the match utility distribution generates a cost in expected utility.====As Kőszegi and Rabin (2007) have already proved, CPE allows for a preference for stochastically dominated lotteries, if an agent's loss aversion is sufficiently strong. Indeed, a loss-averse student may prefer to be matched with school ==== with certainty over being matched with the same school ==== with probability ==== and being matched with an even better school ==== with probability ====. Intuitively, the mere possibility of getting into ==== makes the realization of the more likely outcome ==== more painful. Not listing ==== abandons all hope so that this school does not enter the stochastic reference point and disappointment is avoided. Such motifs can explain the evidence, suggesting that low- and mid-priority students are prone to misrepresentations, but high-priority and optimistic students are not.====We draw on the extensive literature on matching mechanisms, but depart from the standard framework where preferences are only ordinal. In their seminal paper, Gale and Shapley (1962) introduce the deferred-acceptance mechanism as a solution to find the optimal stable matchings for the proposing side in the one-to-one matching problem. The dominance of the truthful strategy for proposers in DA and TTC mechanisms was established by Roth, 1982a, Roth, 1982b. Balinski and Sönmez (1999) show that DA is constrained efficient in the sense that no other fair mechanism Pareto-dominates it. Our model introduces a fundamentally different structure of incentives and questions all of these classical insights. Roth (1989) and Ehlers and Massó (2007) are the first to study matching with incomplete information.====Hassidim et al. (2017a) gather stylized facts about the pervasive misrepresentation of preferences in truthful mechanisms. Similar to Rees-Jones (2018) and Chen and Pereyra (2019) who analyze survey data, they find that “misrepresentation rates are higher in weaker segments of markets” and increase “when applicants expect to face stronger competition,” in line with the predictions of our model. In field data, misrepresentations are hard to identify since the true preferences are not observable. However, Hassidim et al. (2017b), Shorrer and Sóvágó (2017) and Artemov et al. (2020) exploit objective rankings in their data to expose “obvious misrepresentations” and find the same pattern.==== Artemov et al. (2020) and Hassidim et al. (2017b) find that 1–20% and 2–8% of obvious misrepresentations are ex-post costly, respectively. Shorrer and Sóvágó (2017) estimate that the 12–19% costly obvious misrepresentations amount to $3,000–$3,500 on average (unconditionally $347–$738 per misrepresentation). That is, even when restricting attention to obvious misrepresentations, consequential deviations can be observed.====Truthfulness is easier to detect in the lab where preferences are induced by the experimental design. While the pioneers Chen and Sönmez (2006) focused on a comparison of different mechanisms, more recently researchers have investigated patterns in preference manipulations. Hakimov and Kübler (2021) provide a well-structured overview of the current state of experimental research on matching markets. They document that rates of truthfulness seem to depend on multiple factors which should not impede the dominance of the strategy and which vary widely between studies. Rather than rooted in behavioral theory, most experimental studies are descriptive. For instance, Chen and Sönmez (2006) introduced the district-school bias and the small-school bias, which capture the tendency that safe district schools are ranked higher and small schools are ranked lower. We offer a theory to explain this pattern.====In contrast to our paper, where students deliberately submit incorrect preferences, misrepresentations have most commonly been interpreted as cognitive failures to identify the dominant strategy.==== Li (2017) points out that DA is not “obviously strategy-proof”,==== and shows that most “mistakes” vanish when replacing DA with sequential serial dictatorship. We find that the most common deviations documented by Li (2017) are indeed top-rank monotone. Hence, our model of non-standard preferences provides an alternative explanation for the observations in Li (2017). We believe that both explanations, cognitive mistakes and non-standard utility, are relevant in practice.====Combinations of behavioral theory and matching are still relatively rare. To the best of our knowledge, the first paper to consider non-standard preferences in matching is by Antler (2015) whose agents' preferences are directly affected by the reported preferences of others. Fernandez (2020) studies anticipated regret in deferred acceptance, and Zhang (2021) considers school choice with level-k reasoning. Dreyfuss et al. (2022) recently and independently raised the point that EBLA can help explain misrepresentations in DA. Alongside various differences in modeling choices, they focus on the individual decision problem and use empirical strategies to identify loss aversion in existing experimental data. In contrast, we take a deeper theoretical approach by deriving characterization results on rationalizable ROLs, analyzing strategic interaction, and evaluating remedy mechanisms. We discuss the distinction to our paper more carefully in Section A.1. Meisner (2022) proposes report-dependent utility as an explanation and differentiates our model from his by naming settings in which predictions of the models differ.====Gross et al. (2015, p. 22) document that parents who “did not get what they hoped for and felt this sense of frustration and false hope” question the legitimacy of centralized school choice. However, they also conclude that “only a small share of families probably experience the disappointment” because most applicants are matched to one of their top-ranked schools. To emphasize that disappointment already affects the submission of preferences, we employ the choice-acclimating personal equilibrium (CPE) concept introduced by Kőszegi and Rabin (2006). It essentially captures disappointment aversion similar to Bell (1985), Loomes and Sugden (1986) or Gul (1991), who model the reference point as the lottery's certainty equivalent. O'Donoghue and Sprenger (2018, Section 5) compare these models in detail. We choose CPE where outcomes are compared to the lottery's full distribution because it allows for “mixed feelings” and because it is unclear what the certainty equivalent of a lottery over real school placements is supposed to be. Comparing both approaches, Sprenger (2015) finds more support for Kőszegi and Rabin (2006) in the data.====EBLA is supported by evidence from the field, such as Crawford and Meng (2011) or Pope and Schweitzer (2011). Evidence from the lab is mixed. While the conflicting evidence of Ericson and Fuster (2011) and Heffetz and List (2014) is affirmatively mended by Heffetz (2021), who introduces an extra treatment causing expectations to “sink in,” the evidence on real-effort experiments with EBLA (Abeler et al., 2011; Gneezy et al., 2017) does not allow a clear verdict, yet. EBLA has been applied to a variety of economic models.====The remainder of the paper is structured as follows. In Section 2, we present the model environment, introduce reference-dependent preferences, and describe the appropriate equilibrium concept. In Section 3, we apply this decision-theoretic equilibrium concept to the individual decision problem in strategy-proof mechanisms. We introduce the attainability distribution as a reduced form summarizing all information relevant to determine the optimal ROL. Next, we characterize all rationalizable ROLs. In Section 4, we analyze strategic interaction in the static student-proposing deferred acceptance mechanism, and we show that a game-theoretic equilibrium exists. We then characterize equilibrium in a stylized setting with district and elite schools. In Section 5, we establish that only a sequential mechanism can solve potential instability issues of the static DA mechanism. All proofs are relegated to the appendix, Section A.3.",Loss aversion in strategy-proof school-choice mechanisms,https://www.sciencedirect.com/science/article/pii/S0022053122001788,2 December 2022,2022,Research Article,50.0
"Carnehl Christoph,Fukuda Satoshi,Kos Nenad","Bocconi University, Department of Economics and IGIER, Italy,Bocconi University, Department of Decision Sciences and IGIER, Italy,Bocconi University, Department of Economics, IGIER and CEPR, Italy","Received 20 April 2022, Revised 25 November 2022, Accepted 26 November 2022, Available online 1 December 2022, Version of Record 6 December 2022.",https://doi.org/10.1016/j.jet.2022.105590,Cited by (0),"We study social distancing in an epidemiological model. Distancing reduces the individual's probability of getting infected but comes at a cost. Equilibrium distancing flattens the curve and decreases the final size of the epidemic. We examine the effects of distancing on the outset, the peak, and the final size of the epidemic. First, the prevalence increases beyond the initial value only if the transmission rate is in the intermediate region. Second, the peak of the epidemic is non-monotonic in the transmission rate. A reduction in the transmission rate can increase the peak. However, a decrease in the cost of distancing always flattens the curve. Third, both a reduction in the transmission rate as well as a reduction in the cost of distancing decrease the final size of the epidemic. Our results suggest that public policies that decrease the transmission rate can lead to unintended negative consequences in the short run but not in the long run. Therefore, it is important to distinguish between interventions that affect the transmission rate and interventions that affect contact rates.","When faced with the possibility of contracting a hazardous disease, people undertake protective measures. They reduce social interactions due to the risk of meeting an infected person. Such behavior is not novel. During the plague pandemics, citizens would flee affected areas and wear costumes to protect themselves from the infection; long-beaked masks worn by physicians in the 17th century achieved particular notoriety. Public authorities eventually began to coordinate the response to epidemics. Famously, Venice required that the passengers on ships from affected areas confine themselves for forty days; thus, the term “quarantine” was minted.==== Such behavior calls for the explicit incorporation of human behavior in epidemiological models. Yet, the standard SIR model of epidemics, introduced by Ross and Hudson (1917) and Kermack and McKendrick (1927), assumes that individuals engage in as many interactions at the height of the epidemic as they do when the disease is barely present.====We study a tractable model of epidemics that incorporates social distancing and show that explicitly modeling human behavior has important consequences on the predicted trajectory of an infectious disease.==== Susceptible individuals non-cooperatively decide to which extent to reduce interactions at each point in time. Such distancing is costly but reduces the probability of getting infected. The cost of getting infected is fixed; building on the work of Chen (2012). We show that an equilibrium exists and that it is unique. If the disease spreads, the epidemic has a single peak: it propagates through the population until it reaches the peak prevalence, then it recedes and eventually dies out. Susceptible individuals distance throughout the epidemic, though the intensity of their distancing varies with the amount of actively infected individuals. Distancing affects three crucial and commonly discussed features: the conditions for an epidemic to start, its peak, and its final size.====First, we define a basic reproduction number taking distancing into account—the ====. It consists of the classical, epidemiological basic reproduction number, ====, multiplied by a behavioral term; a similar concept was introduced in Fenichel et al. (2011).==== We show that the disease propagates itself if and only if the behavioral basic reproduction number is larger than one. The novelty is that the behavioral basic reproduction number is concave in the transmission rate and that the disease spreads only for intermediate values of the transmission rate. If the transmission rate is too high, individuals distance with such fervor that the prevalence never rises above the initial seed of infection. This finding stands in stark contrast with the predictions offered by the SIR model without distancing where the infection spreads if the transmission rate is high enough; see for example Brauer and Castillo-Chavez (2012).====Second, we derive results about the peak prevalence of the disease. The peak prevalence is crucial to understand whether a disease might cause the health system to reach its capacity. For example, the 1918 influenza pandemic hit an unprepared health system which soon became overwhelmed; see Jester et al. (2018) and Schoch-Spana (2001). In March 2020—less than a month after the coronavirus erupted in Italy—, the healthcare system in Northern Italy was under such severe pressure that some pneumonia patients could not be treated.==== In order to avoid the active number of infected individuals exceeding the health care system's capacity, the goal became to ====. We show that an increase in the cost of distancing unequivocally leads to a reduction in distancing and, therefore, to a higher peak prevalence of the disease. However, peak prevalence is non-monotonic in the transmission rate. If the transmission rate is high enough for the disease to spread but not too high, an increase in the transmission rate leads to an increase in the peak prevalence. In contrast, when the transmission rate is sufficiently high, an increase in the transmission rate decreases the peak prevalence and causes flattening of the curve.====The above comparative statics lend themselves to two interpretations:====A large body of work that studies non-pharmaceutical interventions models these either as reductions in the transmission rate (see, for example, Kruse and Strack, 2022; Rachel, 2020a) or as directly imposing restrictions on the activity level of (a fraction of) individuals (see, for example, Acemoglu et al., 2021; Alvarez et al., 2021; Farboodi et al., 2021)—which are equivalent approaches in the SIR dynamics without behavior. Our results suggest that modeling individual distancing choices explicitly requires a careful choice of modeling interventions because their qualitative implications differ through the behavioral channel. On the one hand, those interventions affecting the rate at which the disease propagates conditional on meetings, e.g., mandatory mask mandates, should be modeled as a decrease in the transmission rate.==== On the other hand, those interventions that directly affect the incentives to distance, e.g., restaurant, bar or museum closures, should be modeled as a decrease in the cost of distancing; e.g., Fenichel et al. (2011) model a public policy intervention as a change in the payoff structure of contacts.====Third, we find that the possibly detrimental short-run effects of a decrease in the transmission rate disappear in the long run. Despite the non-monotonicity of the peak prevalence in the transmission rate, the total number of infected individuals throughout the epidemic is monotonically increasing in both the cost of distancing and the transmission rate. Our model predicts a smaller final size of the epidemic (i.e., less total infections) than the standard SIR model due to distancing. Indeed, the model converges to the SIR model without distancing when the cost of distancing grows and so does the final size of the epidemic.====With these findings, we highlight an important trade-off between short-run mitigation, i.e., flattening the curve to avoid an overburdened health system, and long-run size of epidemics when considering the transmission rate. This trade-off arises due to the varying degree to which behavior matters during an epidemic. At the peak, the infection risks are high and individuals' distancing decisions have a strong impact on the dynamics of the epidemic. When an epidemic fades out, however, behavior is of less importance as individual risks are low and the standard SIR mechanics dominate the behavioral effects. However, the trade-off disappears once policies are considered that directly affect distancing incentives of individuals and both short-run mitigation and long-run size of the epidemic are obtained with similar policies, i.e., lowering the cost of distancing.====In the final section, we discuss two of our main assumptions in more detail. We consider an extension of the model in which the cost of infection is endogenized. We show numerically that the non-monotonicity of peak prevalence in the transmission rate extends to that environment. In addition, we illustrate that the non-monotonicity in the transmission rate extends to a more general class of cost functions.==== Capasso and Serio (1978) introduced non-linear contact rates into the standard SIR model as a reduced form of modeling behavior. For a more recent overview of literature on non-linear contact rates, see Funk et al. (2010) and Verelst et al. (2016). A strong point for explicitly modeling behavior was made by Ferguson (2007).====Reluga (2010) and Fenichel et al. (2011) introduced preventive behavior into SIR models explicitly and provided numerical analyses of equilibrium trajectories.==== Fenichel (2013) studied the differences in incentives for distancing between a decentralized economy and an economy governed by a social planner. Chen (2012) introduced an SIR model with a constant cost of infection and derived conditions on the contact functions that deliver uniqueness of the Nash equilibrium in each period for a given prevalence of the disease; yet, he did not establish whether that leads to uniqueness of equilibrium trajectories. These papers do not provide analytical results about the equilibrium trajectories under social distancing.====Analytical results about equilibrium trajectories of the SIR model with distancing are few. Closest to ours is work by Dasaratha (2020), McAdams (2020), McAdams et al. (2021), Rachel (2020a) and Toxvaerd (2020). The last two analyze a model of behavior with a linear cost of distancing and an endogenous time-varying cost of getting infected. They derive the necessary conditions for an equilibrium and offer two different paths that satisfy the necessary conditions, but stop short from proving that these are indeed equilibria.==== Characterization of equilibria in their model, therefore, remains an open question. It remains unresolved whether an equilibrium in their model is unique and thus comparative statics are non-ambiguous.==== Dasaratha (2020) analyzes a model with a constant cost of infection (like ours), but where the infected individuals do not necessarily know whether they are infected. The complexity of his model requires that he mostly focuses on local results rather than on the entire path.==== McAdams (2020) and McAdams et al. (2021) propose a model in which an individual's benefit of social activities depends on the actions of other individuals and shows that complementarities in distancing choices may lead to multiplicity of equilibria. Gans (2022) explores the implications of directly imposing on a model of behavior that the effective reproduction number satisfies ====. Budish (2020) studies the optimal interventions (e.g., mask mandates and closure of large indoor gatherings) subject to ==== as a constraint in a static model. Avery (2021) studies the interaction between social distancing and vaccination. An excellent account of the rapidly growing literature is provided by McAdams (2021).====A wide pool of papers studies how policy interventions affect distancing and, through that, the spread of a disease. Farboodi et al. (2021) and Rachel (2020b) build on the work discussed above to study lockdown effectiveness and the possibility of a second wave occurring. Toxvaerd and Rowthorn (2020) compare individuals' and a planner's decisions to apply treatments and vaccinations as pharmaceutical interventions during an epidemic. Giannitsarou et al. (2021) provide numerical projections for the COVID-19 pandemic under waning immunity, based on a model with endogenous distancing. Acemoglu et al. (2021) and Brotherhood et al. (2020) study the importance of age composition in the COVID-19 pandemic. With the exception of Rachel (2020b), these papers focus on numerical solutions of rather involved models without establishing either the existence or the uniqueness of equilibria. While one can argue that the numerical algorithms are bound to lead to an equilibrium, or something close to it, the lack of uniqueness of equilibria reduces the credibility of welfare comparisons of various policies in those models.",Epidemics with behavior,https://www.sciencedirect.com/science/article/pii/S0022053122001806,1 December 2022,2022,Research Article,51.0
"Baccara Mariagiovanna,Lee SangMok,Yariv Leeat","Olin School of Business, Washington University in St. Louis, United States of America,Department of Economics, Washington University in St. Louis, United States of America,Department of Economics, Princeton University, United States of America","Received 27 April 2022, Revised 24 November 2022, Accepted 25 November 2022, Available online 1 December 2022, Version of Record 21 December 2022.",https://doi.org/10.1016/j.jet.2022.105587,Cited by (1),"We study dynamic task allocation when providers' expertise evolves endogenously through training. We characterize optimal assignment protocols and compare them to discretionary procedures, where it is the clients who select their service providers. Our results indicate that welfare gains from centralization are greater when tasks arrive more rapidly, and when training technologies improve. Monitoring seniors' backlog of clients always increases welfare but may decrease training. Methodologically, we explore a matching setting with endogenous types, and illustrate useful adaptations of queueing theory techniques for such environments.",None,Task allocation and on-the-job training,https://www.sciencedirect.com/science/article/pii/S0022053122001776,1 December 2022,2022,Research Article,52.0
"Eilat Ran,Neeman Zvika","Department of Economics, Ben Gurion University of the Negev, Israel,School of Economics, Tel Aviv University, Israel","Received 26 August 2021, Revised 22 September 2022, Accepted 30 October 2022, Available online 25 November 2022, Version of Record 5 December 2022.",https://doi.org/10.1016/j.jet.2022.105572,Cited by (1),"We study how the suspicion that communicated information might be deceptive affects the nature of what can be communicated in a sender-receiver game. Sender is said to ==== Receiver if she sends a message that induces a belief that is different from the belief that should have been induced in the realized state. Deception is costly to Sender and the cost is endogenous: it is increasing in the distance between the induced belief and the belief that should have been induced. A message function that induces Sender to engage in deception is not credible and cannot be part of an equilibrium. We study credible communication with state-dependent and state-independent Sender's preferences. The cost of deception parametrizes the sender's ability to commit to her strategy. Through varying this cost, our model spans the range from cheap talk, or no commitment (====, ====) to full commitment (====, ====).","Communication is indispensable for almost every economic and social interaction. Oftentimes, an agent with superior information conveys this information strategically to others in order to influence their behavior. If the interests of the parties are not perfectly aligned, then the informed agent may benefit from being dishonest. Yet, research shows that in many cases, in addition to possibly producing material gains, dishonesty is also costly.==== How does the cost of dishonesty affect the nature of what can be communicated between the parties? The answer to this important question depends, obviously, on the source and form of this cost.====What makes an untruth costly to communicate? If Alice and Bob both stand outdoors at midday, and Alice tells Bob that it is midnight, this is a lie. If, when exiting the dentist's office, Alice announces that she just had the best time of her life, this is also a lie. However, if Bob is a “reasonable person,”==== these lies do not change his perception of reality. Many would argue that lies that are not believed are costless.====In this paper we take the position that dishonesty is costly only to the extent that it undermines beliefs. Indeed, a common distinction between lying and deception is that a lie is “a statement that the speaker believes is false” whereas deception is a “statement – or action – that induces the audience to have incorrect beliefs” (Sobel, 2020).==== Accordingly, we assume that deception (rather than mere lying) is costly, and study how the suspicion that communicated information might be deceptive affects the nature of what can be communicated in equilibrium.====We study a standard model of communication between an informed Sender (she) and an uninformed Receiver (he) to which we add a cost of deception. Sender observes a certain variable and sends a message about it to Receiver who, upon receiving the message, updates his belief and takes an action. Receiver's beliefs on the relevant variable depend on the prior distribution, Sender's equilibrium strategy, and the actual message sent. Sender is said to deceive Receiver if she sends a message that is different from what she was supposed to send according to her equilibrium strategy, in a way that distorts Receiver's beliefs relative to his equilibrium expectations. We assume that the cost of deception to Sender is increasing in the “distance” between the belief induced by the message actually sent, and the belief that should have been induced under the message that was supposed to be sent in equilibrium.====The novelty in our approach is two-fold. First, we introduce a (belief-dependent) cost of deception into communication games. This allows us to investigate the effect that costly deception has on the information that is communicated in equilibrium. The form of deception cost employed reflects the emphasis we place on the fact that untruths are costly only to the extent that they affect Receiver's beliefs. Second, because the cost of deception in our model is measured relative to Receiver's equilibrium expectations, it is ====. This stands in contrast to other papers such as Sobel (2020) and Kartik (2009), in which truthfulness is evaluated relative to an exogenous standard. This distinction is important for two different reasons: (i) conceptually, deception can arguably only be evaluated relative to equilibrium expectations. After all, if Receiver does not anyway believe Sender, then he cannot be deceived. And, (ii) methodologically, as explained below, endogenous deception costs imply that communication need not be monotone, which requires the development of new proof techniques and generates new insights.====Our approach conforms with traditional equilibrium analysis. We say that Sender's strategy is credible if it does not induce Sender to engage in deception. Clearly, a strategy that is not credible cannot be sustained in equilibrium, and we are only interested in what can be communicated in equilibrium. However, although no deception occurs along the path of play, the mere possibility of deception has a large effect on the information that is conveyed by Sender. Indeed, equilibria are oftentimes interpreted as self-enforcing “social norms.” If a communication norm is established, it prescribes certain beliefs for the receiver. A deviating Sender understands that her Receiver will update his belief according to the norm, and that she is therefore deceiving him in a particular way. A conscientious Sender would be disturbed by such deception. The cost of deception in our model captures the magnitude of this disturbance.====The ability of Sender to deceive Receiver is closely related to Sender's ability to commit to her message strategy, in the sense of sending the specific message prescribed by the strategy and not a different message. A sufficiently large cost of deception implies “full commitment” of Sender to her message strategy. Such a commitment is obviously very valuable. It is a standard assumption in the literature on Bayesian persuasion (Kamenica and Gentzkow, 2011). By contrast, costless deception implies that Sender cannot commit to follow her message strategy and that, consequently, messages should be interpreted as mere “cheap talk” (Crawford and Sobel, 1982). Both of these extreme cases serve as useful benchmarks for us. They have both been extensively studied in the literature. Through varying the cost of deception in our model, it is possible to span the range from cheap talk, or no commitment, to full commitment.==== There are two players: Sender (she) and Receiver (he), and a binary state ====. The prior belief that ==== is one-third. Receiver has to choose an action ====. His payoff is given by ====. Hence, Receiver prefers to take action ==== if and only if his posterior belief that the state is ==== is at least one-half. The payoff for Sender is ====. She therefore prefers that Receiver chooses action ==== regardless of the state.====Suppose that Sender knows the state. If Sender has no qualms about deceiving Receiver, then it is impossible to sustain informative communication in equilibrium. This is because Sender would never send a message that would lead Receiver to choose ====. Expecting this, Receiver would ignore Sender's message, and choose the action ====, which is optimal given the prior belief. The payoff to Sender in this case is zero.====Suppose however that it is costly for Sender to deceive Receiver: if Sender deviates from the message that Receiver “anticipates” her to send given the state, then she incurs a cost. Suppose that this cost is equal to the difference between Receiver's posterior belief that the state is ==== that is induced by Sender's actual message, and the belief that should have been induced by Sender's anticipated message.====In this case, it is possible to sustain an equilibrium in which Sender reveals the state. In this equilibrium Sender sends message ==== in state ==== and message ==== in state ====. Receiver's posterior belief that the state is ==== following message ==== (resp. ====) is 1 (resp. 0), and so he takes action ==== (resp. ====). To verify that this is indeed an equilibrium, note that Sender cannot benefit from deceiving Receiver and sending message ==== in state ====. Sender's material gain from this deviation is one (because Receiver would take action ==== instead of ====). However, this gain is exactly offset by Sender's cost of deception (because Receiver's posterior belief that the state is ==== following this deception is one instead of zero). The (expected) payoff to Sender in this equilibrium is one-third.====At the extreme, if deception is infinitely costly for Sender, then she can obtain an even higher payoff. In this case, Sender has full commitment power and so, as shown by Kamenica and Gentzkow (2011), she can obtain an expected payoff of two-thirds by employing the following strategy: when the state is ====, send message ====; and when the state is ====, send messages ==== and ==== with equal probabilities. Given this strategy, Receiver takes action ==== following message ==== and action ==== following message ====. Because deception is infinitely costly, Sender cannot benefit from deviating and sending message ==== when she should send message ====.====In Section 2 we present the model and formally define the notion of deception costs. In Section 3 we study the implications of the possibility of costly deception in an environment in which Sender's payoff depends on the state of the world. To that end, we introduce the possibility of costly deception into the uniform-quadratic specification of Crawford and Sobel's (1982) model of strategic communication. The key difference between the equilibria that emerge in Crawford and Sobel's (1982) model and in ours is that in Crawford and Sobel (1982) ==== equilibrium is a partition equilibrium in which each element of the partition is an interval of Sender's types.==== By contrast, in our model Sender may induce a credible partition of the state space with ==== elements. However, we show that the ==== partition for Sender consists only of intervals. Our proof technique, which we explain in detail in Section 3, is substantially different from that of Crawford and Sobel (1982). This result allows us to explicitly solve for the optimal partition for Sender and describe how it relates to the optimal partition in Crawford and Sobel (1982). The fact that deception is costly facilitates more informative communication between Sender and Receiver compared to the most informative equilibrium in Crawford and Sobel (1982). In fact, we show that increasing the cost of deception is akin to decreasing the value of the parameter that measures the sender's bias in Crawford and Sobel's uniform-quadratic model.====In Section 4 we apply our definition of costly deception to a model in which Sender's payoff is independent of the state of the world. For this purpose, we introduce the possibility of costly deception into the payoff environment of Kamenica and Gentzkow's (2011) leading example. We provide a geometric characterization of Sender's highest equilibrium payoff in this model. We show that this highest payoff is obtained on a partial concavification of Sender's indirect payoff function with a bounded slope. We show that Sender's value is continuous in the cost of deception parameter but may be discontinuous in prior beliefs. We describe environments where communication involves either more or less garbling compared to the case of full commitment. Finally, we show that a lower cost of deception always hurts Sender and discuss the circumstances under which it either benefits or hurts Receiver.====In Section 5 we discuss two additional issues: an alternative tie-breaking rule for Receiver and the number of messages employed by Sender in an optimal credible message function. Section 6 concludes.",Communication with endogenous deception costs,https://www.sciencedirect.com/science/article/pii/S0022053122001624,25 November 2022,2022,Research Article,53.0
Kambhampati Ashwin,"Department of Economics, United States Naval Academy, United States of America","Received 8 March 2022, Revised 15 November 2022, Accepted 19 November 2022, Available online 25 November 2022, Version of Record 30 November 2022.",https://doi.org/10.1016/j.jet.2022.105585,Cited by (0),"A principal contracts with an agent, who takes a hidden action. The principal does not know all of the actions the agent can take and evaluates her payoff from any contract according to its worst-case performance. ==== showed that there exists a linear contract that is optimal within the class of deterministic contracts. This paper shows that, whenever there is an optimal linear contract with non-zero slope, the principal can strictly increase her payoff by randomizing over deterministic, linear contracts. Hence, if the principal believes that randomization can alleviate her ambiguity aversion, then restricting attention to the study of deterministic contracts is ==== loss of generality.","A principal writes an incentive contract for an agent. The agent takes a productive, but hidden, action. Unfortunately for the principal, she does not know all available actions. Which contract yields her the highest worst-case payoff?====In path-breaking work, Carroll (2015) sets forth a new paradigm to answer this question. He proves, very generally, that the optimal deterministic contract is linear — the agent receives a constant fraction of the output she produces. This simple contract contrasts with the more complicated, detail-sensitive contracts predicted by the standard, Bayesian principal-agent model.====At the same level of generality as Carroll (2015), this paper shows that the principal can strictly increase her worst-case payoff by randomizing over deterministic contracts, as long as there is an optimal (deterministic) linear contract with non-zero slope (Theorem 1). Hence, if the principal believes that randomization can alleviate her ambiguity aversion, then restricting attention to the study of deterministic contracts is ==== loss of generality.====Section 2 presents the model, Section 3 states and proves the main result, Section 4 considers two extensions, and Section 5 discusses a decision-theoretic justification for restricting attention to the study of deterministic contracts.",Randomization is optimal in the robust principal-agent problem,https://www.sciencedirect.com/science/article/pii/S0022053122001752,25 November 2022,2022,Research Article,54.0
"Bich Philippe,Teteryatnikova Mariya","Université Paris 1, Panthéon Sorbonne, UMR 8074, Centre d'Economie de la Sorbonne and Paris School of Economics, France,National Research University Higher School of Economics, Pokrovsky Boulevard 11, 109028 Moscow, Russia,Vienna University of Economics and Business, Vienna, Austria","Received 26 February 2020, Revised 16 June 2022, Accepted 15 November 2022, Available online 21 November 2022, Version of Record 23 November 2022.",https://doi.org/10.1016/j.jet.2022.105577,Cited by (0),"We extend standard tools from equilibrium refinement theory in non-cooperative games to a cooperative framework of network formation. First, we introduce the new concept of ====. It transposes the idea of “trembling hand” perfection to network formation theory and strictly refines the pairwise stability concept of ====. Second, we study basic properties of perfect pairwise stability: existence, admissibility and perturbation. We further show that our concept is distinct from the concept of strongly stable networks introduced by ====. Finally, we apply perfect pairwise stability to sequential network formation and prove that it enables a refinement of ====, a natural analogue of subgame perfection in a setting with cooperative, pairwise link formation.","The concept of pairwise stability for network formation was introduced by Jackson and Wolinsky (1996) as an attempt to describe the shape of social and economic networks that are likely to emerge. Basically, a network is a set of nodes and links, where links capture relationships between the nodes, such as, for example, friendships or co-author relationships between people.====The interesting feature of Jackson and Wolinsky's concept is that it takes into account both cooperative and non-cooperative aspects of link formation.==== Indeed, by definition, a network is pairwise stable if no two agents could gain from linking (a cooperative decision) and no single agent could gain from severing one of his or her links (a unilateral decision). This appealing mix of cooperative and non-cooperative aspects in the definition of pairwise stability, together with the concept's simplicity, make it a prominent and widely used concept. It has also become seminal for subsequent works on network formation.====However, pairwise stability has some limitations. First, the existence of a pairwise stable network is not guaranteed. This problem has been recently solved by Bich and Morhaim (2020) who showed that existence can be established for a large class of models if one considers ====. By definition, links in a weighted network have weights that are measured by real numbers between 0 and 1 and can be interpreted, for example, as strengths of relationships between agents or probabilities with which links are created. Deviations that increase this probability need consent of the involved agents, while deviations that decrease it can be done unilaterally. A pairwise stable network in this environment rules out any profitable deviation of either kind. Yet, even if we restrict attention to weighted networks, three important issues remain:====As a simple illustration of these issues, consider unweighted networks between three agents and assume that the payoff of all agents is 1 when the network is complete (i.e., all links are formed), and 0 otherwise.==== The empty network (for which no links are formed) and the complete network are pairwise stable: for the former, no pair of agents can benefit from creating a link, and for the latter, no agent has an incentive to delete a link. However, among these two networks, the complete network is a more reasonable prediction for stability. First, if no links are formed, any pair of agents have nothing to lose from creating a link, and they would strictly benefit from doing so if the other links are formed, too. Thus, in some sense, creating a link is a “weakly dominant” choice for every pair of agents. Second, if there is at least a small probability that all other links are formed, then any pair of agents has a strict incentive to form the link. This means that the empty network is not “robust” to small perturbations on other links. Yet, the concept of pairwise stability does not capture this difference in stability properties of the complete network (which is robust to perturbations and undominated) and the empty network (which is not robust and dominated).====In this paper, we prove that there exists a refinement of pairwise stability which addresses the three issues mentioned above (and in the considered example, identifies just the complete network as stable). This complements Bich and Morhaim (2020), which focused mainly on existence of a weighted pairwise stable network, and enhances the economic predictive power of their pairwise stability notion. To be precise, we introduce the concept of ==== that refines pairwise stability by transposing the idea of “trembling hand” perfection from non-cooperative games to a cooperative framework of network formation. Intuitively, a network is perfect pairwise stable if no agents have an incentive to modify the weight of any of their links, even if the weights of the other links are slightly perturbed.====We build a theoretical foundation for the concept of perfect pairwise stability. To that end, we provide its formal definition, prove that it is a refinement of pairwise stability and show that it satisfies three important properties – Existence (E), Admissibility (A) and Perturbation (P). The first property asserts that when payoff functions are continuous, quasiconcave and weakly monotonic, there always exists at least a weighted perfect pairwise stable network. The second property of Admissibility states that in a perfect pairwise stable network, no link weights are dominated, at least as soon as the payoff functions are multiaffine.==== Finally, the third property of Perturbation establishes for any weakly monotonic payoffs the equivalence between the fact that a network is perfect pairwise stable and that it is a limit of a sequence of completely weighted networks==== that are all pairwise stable in some “====-perturbed” setting.====Formulating and establishing these results requires allowing networks to be weighted. However, as illustrated in the example above, the perfection concept we propose also bears on the study of ==== networks, which are the main object of interest for most of the network formation literature. When payoff functions are defined only on unweighted networks, one simply has to extend the definition of payoffs to perturbed (weighted) networks, by using a canonical process that we call ====.==== The extended payoff functions satisfy the conditions needed to obtain Existence, Admissibility and Perturbation properties of perfect pairwise stable networks. Thus, one can define and study ==== perfect pairwise stable networks, using all the results of our analysis.====In the second part of the paper, we also introduce a new sequential setting for network formation, where agents can decide whether to form a link or not in a specified order. In this setting we define the notion of ==== and compare it to perfect pairwise stability. We prove that perfect pairwise stability permits the refinement of sequential pairwise stability.====Naturally, we are not the first to propose a refinement of pairwise stability. Other well known refinements are the concepts of strong stability by Jackson and Van den Nouweland (2005) and pairwise-Nash stability initially proposed by Jackson and Wolinsky (1996) and formally studied by Calvó-Armengol and İlkılıç (2009), İlkılıç (2004) and Gilles and Sarangi (2004). Our contribution differs from these conceptually in that we introduce a refinement methodology from a non-cooperative framework of game theory to a cooperative framework of network formation. Moreover, earlier refinements do not satisfy all of the properties (E), (P), (A). For example, strong stability refines pairwise stability by considering all deviating coalitions of two or more agents, which often imposes so many conditions on the outcome of network formation that a strongly stable network does not exist. The non-existence issue also arises for the concept of pairwise-Nash stability.====Importantly, we demonstrate that our notion of perfect pairwise stability cannot be seen as a trembling hand perfect Nash equilibrium of a conventional linking game à la Myerson (Myerson, 1991). To be precise, we show that the concept of perfect pairwise stability and perfect Nash equilibrium may lead to different sets of predictions, and one is not implied by the other. Therefore, our theory requires new constructions and proofs, beyond those existing for non-cooperative games and perfect Nash equilibria.====The paper is organized as follows. In Section 2, after some preliminaries where pairwise stability is defined, we introduce the concept of perfect pairwise stability. In Section 3, we derive the existence, admissibility and perturbation properties of perfect pairwise stable networks. In Section 4, we discuss the relationship of perfect pairwise stability with perfect Nash equilibrium of the Myerson network formation game. Finally, in Section 5, we introduce a sequential framework for network formation, define a concept of sequential pairwise stability and establish the relationship between that concept and our concept of perfect pairwise stability. The proofs of the results are provided in the appendix.",On perfect pairwise stable networks,https://www.sciencedirect.com/science/article/pii/S0022053122001673,21 November 2022,2022,Research Article,55.0
"Polanski Arnold,Vega-Redondo Fernando","University of East Anglia, United Kingdom of Great Britain and Northern Ireland,Bocconi University & IGIER, Italy","Received 26 February 2019, Revised 7 November 2022, Accepted 12 November 2022, Available online 17 November 2022, Version of Record 22 November 2022.",https://doi.org/10.1016/j.jet.2022.105576,Cited by (1),"We study how learning and influence co-evolve in a social network by extending the classical model of ==== in two fundamental ways:====(a) opinions are multidimensional and the learning time-span is arbitrary;====(b) the effective social network is endogenously shaped by opinion-based homophily.====Our analysis starts by establishing the existence of an equilibrium where, following (a)-(b), the learning outcome and the social network are jointly determined. This is followed by its characterization in some simple contexts. Next, we show that, at equilibrium, the strength of the link between any two agents is always given by its “support” – roughly, the amount of third-party (indirect) influence impinging on both agents. This result leads to the key insight that distinct groups may fail to integrate if their (possibly many) cross-group links lack sufficient support. Building on this, we identify sets of conditions for which social fragmentation is robust (i.e. dynamically stable) or even the unique equilibrium.","In this paper, we develop the idea that the network channeling social influence is shaped in conjunction with the learning process that unfolds on it – that is, we propose a model where the two dimensions, influence and learning, co-evolve. This perspective allows us to study the important issue of how social learning (e.g. the formation of individual opinions in a social context) is affected by the relaxation of the common, but unrealistic, assumption that the pattern of social influence stays fixed. Indeed, in the real world, “influence weights” are typically affected by the ongoing learning process and this may have a substantial and lasting effect on the final outcome. In particular, it may exacerbate the tendency of an initially cohesive society to become segregated into separate groups that hardly interact. This, in fact, has been highlighted as one of the distinct, and also worrying, features of modern societies because of the polarization and “echo chambers” it generates.====At the end of this Introduction, we discuss some related papers that analyze the coevolution of networks and opinions. In different contexts, they show that polarization can be a stable outcome due to the endogenously determined network structure. In contrast to these papers, our model stresses how a suitable (topological) notion of “network embeddedness” – which we call ==== – shapes the process through which opinions are formed, possibly ending up in a situation where they become solidly polarized. In this case, the society becomes fragmented into ==== opinion-independent subpopulations, across which no information or influence effectively flows.====Within our model, such a state of affairs may arise even if those groups (subpopulations) of agents are heavily connected across (i.e. they have many links between them). Indeed, we show that polarization always happens to be an entrenched (i.e. dynamically robust) possible outcome whenever those cross-group links do ==== enjoy the aforementioned support. For, because of homophily, it turns out that such a support is crucially needed if any amount of influence is to flow across inter-group links, thus breaking polarization. A better understanding this issue, we shall argue, should help identifying some of the key features and trade-offs underlying this important problem, hence suggesting possible avenues to tackle it.====As indicated, our learning model builds upon the well-known setup proposed by DeGroot, which has been studied in the economic literature by, among others, DeMarzo et al. (2003) and Golub and Jackson (2010, 2012). In the DeGroot framework, the opinion (or belief)==== held by an agent adjusts over time by combining linearly her immediately preceding beliefs and those of others. The vectors of weights specifying how each agent is affected by every other agent in the population define an influence network/matrix, which fully governs the overall social-learning process. Then, under the twin assumptions that the number of learning rounds is ==== and the influence network is ====, a standard result in this literature is that, under mild regularity conditions, the population converges to consensus – i.e. all agents end up holding the same opinion.====The situation, however, is interestingly different if, as we posit here, learning involves a ==== number of learning rounds and the influence network is ====. For, on the one hand, given the pattern of interpersonal influence that eventually materializes, its evolving topology must obviously play a key role in modulating how much divergence of opinions may persist after social interaction. And on the other hand, if significant opinion heterogeneity still remains, the force of homophily====) may shape a social network that exacerbates divergence.====The importance of homophily in determining how individuals construct their social network has been widely documented in the sociological literature. For a good early account of its pervasiveness in social environments, the reader is referred to the seminal work by Lazarsfeld and Merton (1954), whereas a more recent survey can be found in McPherson et al. (2001).====It is important to stress that, in this paper, homophily is to be regarded not as a normative postulate but as a ==== one. For example, from the point of view of social learning, it may well yield a suboptimal outcome if information gathering is the main objective. For, in this case, when choosing with whom to connect, agents should target those who are different from them and hence likely to hold complementary information – see Golub and Jackson (2012) or Lobel and Sadler (2013) for an illustration of this point in different contexts (DeGroot learning in the first case, Bayesian in the second).====In a nutshell, our homophily-based approach to endogenizing the network of social influence involves the postulate that the weight of each link should match the similarity of opinions/beliefs of the agents connected by it. Another important property that will also be assumed on the underlying environment is that it is rich enough to comprise opinions in multiple dimensions. Thus, for example, individuals may hold opinions on a number of different topics: economic, political, religious, etc. Or, even if restricted to just one such category, say the economic one, their concern may cover a wide range of different issues such as growth, unemployment, inflation, or income distribution. Such richness of the “topic space” is important in our context because it introduces the possibility of defining non-trivial correlations among the different opinion dimensions.==== And, as we shall explain, the similarity measure that underlies our notion of homophily is assessed in terms of whether agents display correlated deviations from “benchmark opinions” in the different dimensions.====Based on such a notion of similarity, our model posits that, given a certain (exogenously given) network of inter-agent observation, social influence is adjusted in the following simple manner:====A key result in this respect involves singling out a topological measure that characterizes the weighted pattern of influence that must prevail at ==== EIM. Specifically, it shows that, at equilibrium, the influence that any agent ==== exerts on another agent ==== must be proportional to the accumulated (indirect) influence that all third-party agents exert on ==== and ====. We call this magnitude the ==== of the relationship between ==== and ====.====An important consequence of the aforementioned result is that fragmentation of the population into groups may be hard to overcome by the mere establishment of observation links that could “bridge” influence across them. To understand this issue dynamically, we focus on a canonical and especially transparent context where two originally disconnected groups come into contact, i.e. establish observational links between them. Then we ask: Will they be able to rely on such cross-group observation to build up enough cross-group influence as well, thus becoming a more integrated population? We find that, even if the new bridging links are numerous, this can fail to happen for a number of interestingly different reasons. More specifically, we characterize how factors such as asymmetries in the group sizes or the learning span may play a key role in reinforcing segmentation, making it a dynamically robust state of affairs.====We close this introduction with a brief discussion of related literature. The general idea that the co-evolution of links and behavior underlies the dynamics of many interesting processes in social environments is widely stressed in the network literature.==== Concerning, more specifically, the phenomenon of social learning that is our focus here, early papers studying the network-influence interplay include Hegselmann and Krause (2002), Holme and Newman (2006), Crandall et al. (2008) and Pan (2010), while the paper by Flache et al. (2017) provides a useful discussion of the previous research in this area.====Within the more recent literature studying such an interplay, we illustrate its broad diversity by summarizing the papers by Melguizo (2019), Bolletta and Pin (2020), and Cerreia-Vioglio et al. (2021). All of them are concerned with the problem of polarization, but each one explores it from a somewhat different angle. Meguizo proposes a model where inter-agent homophily is defined in terms of a suitable similarity measure defined on multi-dimensional space of attributes. Her main conclusion is that disagreement persists in the long run if, and only if, homophily in at least one attribute grows sufficiently fast. Instead, Bolleta and Pin study a strategic model of polarization where, along an adjustment process, heterogeneous agents choose their actions and links to reach a suitable “compromise” between abiding by their own type and minimizing “dissonance” with neighbors. If the initial distribution of types is sufficiently spread out, there is a convergence toward a polarized social structure where the population is partitioned into separate network components. Finally, Cerreia-Vioglio et al. study a general (but non-strategic) learning framework where, in contrast with the DeGroot model, agents use opinion aggregators that possibly display non-linear features. None of these papers, however, study the endogenous interplay of network embeddedness and opinion evolution that is our primary concern here.====To end our literature review, it is worth mentioning that polarization in social networks may arise even if the influence network remains fixed and therefore no influence-network interplay can be the driving force at work. This is well exemplified by papers of Levy and Razin (2018, 2020), which highlight the role played by the phenomenon often labeled ====. More specifically, they show that learning ==== on a given network may also lead to the polarization of opinions/beliefs when agents neglect the correlation that results from being exposed, directly or indirectly, to non-disjoint sets of peers.====The rest of the paper is divided into three more sections. Section 2 presents the basic framework, Section 3 carries out the analysis, Section 4 concludes with a summary. For the sake of a smooth presentation, all formal proofs are gathered in the Appendix.",Homophily and influence,https://www.sciencedirect.com/science/article/pii/S0022053122001661,17 November 2022,2022,Research Article,56.0
"Li Fei,Song Yangbo,Zhao Mofei","Department of Economics, University of North Carolina, Chapel Hill, NC 27599, United States,School of Management and Economics, The Chinese University of Hong Kong (Shenzhen), Shenzhen 518172, China,School of Economics and Management, Beihang University, Beijing 100083, China","Received 8 August 2021, Revised 23 October 2022, Accepted 5 November 2022, Available online 11 November 2022, Version of Record 29 December 2022.",https://doi.org/10.1016/j.jet.2022.105575,Cited by (0),"We study adversarial information design in a regime-change context. A continuum of agents simultaneously chooses whether to attack the current regime. The attack succeeds if and only if the mass of attackers outweighs the regime's strength. A designer manipulates information about the regime's strength to maintain the status quo. Our optimal information structure exhibits local obfuscation: some agents receive a signal matching the regime's true strength, and others receive an elevated signal professing slightly higher strength. This policy is the unique limit of finite-signal problems. Public signals are strictly suboptimal, and in some cases where public signals become futile, local obfuscation guarantees the collapse of agents' coordination, making the designer's information disclosure time consistent and relieving the usual commitment concern.","Many economic problems with strategic complementaries are modeled as regime-change games where a status quo is overturned if enough agents attack it. Examples include speculation against a pegged currency, a run against a bank, and revolution against an authoritarian government.==== In these settings, a central element determining the agents' coordination outcome is the information structure. Therefore, a regime's defender will pursue information manipulation to collapse coordination and preserve the status quo to the largest possible extent. Depending on contexts, the regime's tool ranges from monetary policy (Angeletos et al. 2006), to stress testing (Inostroza and Pavan 2022), and to media outlets capture (Edmond 2013).====In this paper, we study an information design problem in a regime-change context. The designer can commit to any information policy of her choice. This framework unveils the fundamental trade-off of information manipulation and depicts the maximum value achievable in regime-change games. Our work makes two contributions. First, we characterize a simple optimal information policy in closed form. The characterization provides a benchmark to assess the role of numerous application-specific constraints in shaping optimal information policies.==== Second, we show that among possibly multiple policies to attain the unconstrained optimum, this policy is the limit of the unique solution to bounded-depth problems where only ==== levels of agents' strategic reasoning are up for manipulation. This exercise sheds light on the impact of realistic constraints, e.g., limitation of agents' cognitive abilities and complexity of signals, and proposes a selection criterion for the unconstrained problem.====In our model, an information designer faces a unit mass of agents who simultaneously decide whether to coordinate on an attack. Attacking is costly, and each attacker is rewarded if the status quo is overthrown. The strength of the status quo, namely the state, is randomly selected from an interval by nature and unknown to the agents. The status quo persists if and only if the total measure of attackers does not exceed its state. If the state is above one, it is ==== because the status quo persists under the attack of all agents; otherwise, it is ====. The information designer commits to a state-dependent information policy that sends a signal, which can be public or private, to each agent. Her objective is to maximize the probability of preserving the regime in her ====.====Adversarial information design captures the idea of robustness in the information design, but also poses a challenge: it breaks the applicability of the standard Bayes correlated equilibrium (BCE) method (Bergemann and Morris (2016) and Taneva (2019)), which implicitly selects the designer's favorite equilibrium. Key to constructing an optimal information policy is to recognize that regime-change games are supermodular. In these games, Milgrom and Roberts (1990) show that under each information structure, the adversarial (or smallest/lowest) equilibrium can be obtained by iterative elimination of strictly dominated strategies (IESDS). Consequently, adversarial information design in supermodular games can be treated as endogenzing the process of IESDS. As far as we know, this conceptual connection is first formally pointed out by Bergemann and Morris (2019), and has inspired several recent studies in other applications (see subsection 1.1 for a detailed discussion).====We explore this conceptual connection in regime-change settings to study adversarial information design as managing an endogenous IESDS process. It enables us to take advantage of a potentially infinite chain of state obfuscation. As a natural starting point, consider public persuasion where all agents are always sent identical signals. In some vincible states, the designer sends all agents the same signal as in invincible states, so that agents are scared off from attacking when they believe with sufficiently high probability that the true state is invincible. This idea of leveraging on the invincible states has been extensively studied by the literature, which is a natural analogy of the classical single-receiver persuasion (e.g. the jury example in Kamenica and Gentzkow (2011)). However, a coordination game allows the designer to manipulate information more subtly when not constrained to sending public signals: a state does not have to be truly invincible to be leveraged on, but needs only to convince agents of no sufficient coordination. In this spirit, the designer may leverage not only on the invincible states but on weaker states. This is an iterated, possibly infinite-step process enabled by the coordination nature of the base game: through obfuscating signals, some invincible states are the first tier of leveraged states to save certain weaker states; once these vincible states never face sufficiently coordinated attacks, they, in turn, become a “conditionally invincible” tier and may be leveraged on to save more vincible states, and so on. The linkages between these tiers are endogenously characterized by the designer's information policy, so our optimal policy must determine the number of such tiers, the states to be included in every tier, and how to interconnect them in agents' beliefs via obfuscating signals.====  The optimal information structure we identify has an important and novel property that we call ====. Specifically, the first tier contains all invincible states and sends signal ==== to all agents; the second tier is weaker than the first, and it sends ==== to a (randomly selected) proportion of agents and another signal ==== to others; the third is weaker than the second, and it sends ==== to a proportion of agents, and another signal ==== to others; and so on. The measure of each such proportion is deterministic; thus, although each agent receiving ==== remains uncertain about the true state, the measure of each signal sent conditional on states is fixed. Finally, the weakest tier, characterized by an endogenously determined threshold, always sends a self-identifying signal ====. In other words, except for the invincible and the weakest states, the information designer under each state executes a ==== policy, essentially revealing the actual tier to some agents but deceiving other agents by a slightly stronger tier. Heuristically, the designer treats some agents with loosely defined honesty but others with “alternative facts” that marginally distort the truth.====One practical context where this information policy can be effectively implemented is digital authoritarianism: governments worldwide are increasingly using advanced technology to consolidate their control over information and suppress dissent. The state in this application is a government's tolerance threshold for implementing an institution or imposing a regulation, i.e. the least fraction of opposing citizens that can force it to forfeit the scheme. Through censorship and the spread of disinformation, a government with “cyber sovereignty” may present each citizen with private and marginally different messages about the state.====The optimal local obfuscation collapses ==== among agents by creating both fundamental and belief uncertainty. The former refers to uncertainty about the regime's strength; while the latter refers to uncertainty about other agents' beliefs on the regime's strength. At the optimum, the regime-change outcome is characterized by a cutoff value of the regime's strength, i.e. the strongest state sending signal ====. While each agent's uncertainty about the regime's strength is limited, their higher-order belief uncertainty (which corresponds to the regime's state after each round of IESDS, i.e. whether enough agents can still be coordinated to overthrow the regime) remains. In fact, the only common knowledge among all agents is whether the regime's strength is above the cutoff. The remaining higher-order uncertainty makes agents' actions perfectly coordinated: An agent attacks the regime if and only if the regime strength is below the cutoff. In this case, the status quo fails.====Our construction can be viewed as an endogenous design of a series of “state infection,” an analogy proposed by Rubinstein (1989). We further show that to attain optimum, it suffices at every round of the process to extend the contagion to only the strongest currently uncontaminated states, up to a recursive belief-updating constraint. As a corollary, limitation on the designer's degree of freedom in controlling information – such as confinement to public signals or exogenous information – is the main potential source of sub-optimality compared to the unconstrained optimum. Notably, the term “local” indicates the adjacency between tiers of states, which differs qualitatively from its description of small signal perturbation in the classical email game (Rubinstein (1989)) or standard global games (Carlsson and van Damme (1993), Morris and Shin (2003)). Indeed, as we discuss later, this seemingly minor difference substantially distinguishes our construction from some recent information design work building on a similar “small-noise signal” infection argument.====The optimality of the ==== relationship between the regime-change outcome and the regime strength is a natural consequence of state monotonicity and strategic complementarity of regime-change games (Frankel et al. 2003) and echoes the intuitive constructions in previous studies (e.g., Goldstein and Huang 2016) and the equilibrium outcome under optimal public disclosure in some circumstances (see Inostroza and Pavan 2022).====  We then investigate how the ==== of agents determines the implementable outcome of an optimal information design. A practical way to evaluate bounded depths of manipulable reasoning is to impose the assumption of a finite signal space of information structures. Notice that this “level-====” obfuscation exercise substantially differs from the level-==== thinking in the behavioral economics literature (see, e.g., Alaoui and Penta (2016), De Clippel et al. (2019) and Crawford (2021)). For each information structure, we still look for Nash equilibria of the corresponding Bayesian regime-change game. That is, agents are capable of conducting infinitely higher-order strategic reasoning, but only the first K levels are up for manipulation. Hence, it captures (i) the designer's inability of flexible information control due to either signal design cost or communication obstacle, or (ii) the agents' bounded cognitive ability to comprehend/distinguish infinite signals.====We fully characterize the ==== optimal information policy in closed form when information design is constrained by the agents' level of reasoning up for manipulation. This differs from the benchmark unconstrained model that has multiple solutions (we discuss some of these policies in the analysis; also see the discussion regarding the implementation in Morris et al. (2020)).==== In particular, when the designer is capable only of manipulating agents' higher-order reasoning up to a finite level-====, a locally obfuscating policy producing ==== tiers in total is the unique optimal information structure. Thus, our result highlights the designer's advantage resulting from manipulating agents' higher-level reasoning and explicitly identifies the magnitude of this advantage as agents' depth of reasoning improves. Manipulating higher levels of reasoning benefits the information designer by creating more “conditionally invincible” states. It also indicates a one-to-one relation between the depth of manipulable reasoning and signal complexity: the level-==== locally obfuscating policy achieves the designer's unique optimum when agents are fully rational, but the designer has only ==== distinct signals at her disposal. The result holds for an arbitrary ====, making it a natural selection criterion that uniquely identifies our local obfuscating policy among policies that may achieve the designer's optimal outcome. As a practical implication, the designer should always adopt local obfuscation when constrained by agents' manipulable reasoning depth or signal availability.====We discover that the optimal level-==== obfuscation could lead to ==== among agents. To maximize the set of infected states using finite signals, it is optimal when the true state is slightly above the margin of collapse, to make attack conditionally dominated for only a fraction of agents. This is in sharp contrast to the results in our benchmark model as well as in the literature of adversarial information design in supermodular games (see subsection 1.1).====Local obfuscation has a unique advantage over public information structures, which manipulate agents' reasoning only up to the first level. We demonstrate this advantage in two ways. First, given a target set of persisting states, optimal local obfuscation allows a lower threshold of attacking cost to achieve the target than optimal public disclosure. The difference between the cost thresholds coincides with the conditionally expected strength of the persisting states below one. Second, when the measure of invincible states converges to zero, an optimal public information structure becomes futile, while optimal local obfuscation still manages to save a significant measure of vincible states. A sharp implication of this result is that when the attacking cost is sufficiently high but the measure of invincible states becomes almost negligible, virtually no state persists under public information disclosure, but all states persist under optimal local obfuscation, making the policy ex post optimal. It highlights the power of manipulating higher-order uncertainty: it is more likely to relieve us from the time-inconsistent commitment concern.====  Our framework allows the information designer the maximum degree of freedom to choose from all information policies, including ones with correlated and/or non-anonymous signals. Nevertheless, the optimal policy requires only a very simple implementation. It is essentially an anonymous and uncorrelated signal whose realization is at most binary given each possible state. In practical scenarios, the policy can be understood as either (i) i.i.d. private signals, (ii) one random signal with each realization covering a predetermined measure of randomly selected agents, or (iii) a combination between public signal and private endorsement ==== Alonso and Câmara (2016). The implementation simplicity sheds light on information manipulation practices in the digital era. For instance, recent studies (e.g., Guriev and Treisman 2019) have shown that a growing number of autocrats adopt information repression strategy, instead of terrorizing citizens in old-and-bloody style, to stabilize their governance. Our result begins a formal investigation that (i) unpacks the secret of such information repression and (ii) helps to understand this trend. To collapse citizens' hostile collective coordination, all it takes is to create minor belief uncertainty among citizens by dividing message recipients. Due to the penetration of social networks, this less bloody and more effective repression becomes increasingly appealing to autocrats.==== We apply our theory to explain the recent transition of dominant model autocrats. We argue that employing a divide-and-conquer type of information policy is less costly to make information disclosure time-consistent.",Global manipulation by local obfuscation,https://www.sciencedirect.com/science/article/pii/S002205312200165X,11 November 2022,2022,Research Article,57.0
"Ball Ian,Kattwinkel Deniz","Department of Economics, MIT, United States of America,Department of Economics, UCL, United Kingdom of Great Britain and Northern Ireland","Received 7 February 2023, Accepted 25 April 2023, Available online 26 May 2023, Version of Record 26 May 2023.",https://doi.org/10.1016/j.jet.2023.105666,Cited by (0),None,None,Corrigendum to “Role of linking mechanisms in multitask agency with hidden information” [J. Econ. Theory 145 (2010) 2241–2259],https://www.sciencedirect.com/science/article/pii/S0022053123000625,26 May 2023,2023,Research Article,59.0
"Kocherlakota Narayana,Toda Alexis Akira","Department of Economics, University of Rochester, United States of America,Department of Economics, University of California San Diego, United States of America","Received 30 December 2022, Accepted 4 January 2023, Available online 8 February 2023, Version of Record 8 February 2023.",https://doi.org/10.1016/j.jet.2023.105608,Cited by (0)," considers a deterministic economy with a finite number of immortal agents who trade a single asset subject to short-sales constraints. Proposition 4 shows that, in any equilibrium with a bubble, the present value of an agent's endowment is infinite if the agent's asset holdings do not converge over time to a limit. This note corrects an error in the proof of Proposition 4."," considers a deterministic discrete-time infinite-horizon exchange economy in which a finite number of immortal agents trade a single asset in sequential markets. The agents are subject to a common short-sales constraint. He shows via example that it is possible for equilibria to exist in these settings in which the asset's price has a positive bubble. The paper states that in any such equilibrium, some agent's endowment must have an infinite present value.====This result is primarily a consequence of Propositions 3 and ==== in the paper. The statements of both Propositions are valid, but the proof of ==== contains an error which we correct.",Corrigendum to “Bubbles and constraints on debt accumulation” [J. Econ. Theory 57 (1992) 245–256],https://www.sciencedirect.com/science/article/pii/S0022053123000042,8 February 2023,2023,Research Article,62.0
"Morgan John,Tumlinson Justin,Várdy Felix","University of California-Berkeley, United States of America,University of Exeter, United Kingdom of Great Britain and Northern Ireland,ISTO, LMU Munich School of Management, Germany,International Monetary Fund, United States of America","Received 21 December 2020, Revised 26 November 2021, Accepted 12 January 2022, Available online 28 January 2022, Version of Record 9 February 2022.",https://doi.org/10.1016/j.jet.2022.105414,Cited by (2),"We show that meritocracy, in the sense of accuracy of performance ranking, can be too much of a good thing: in contests with sufficiently homogeneous agents, it reduces output and is Pareto inefficient. In contests with sufficiently heterogeneous agents, discouragement and complacency effects further reduce the benefits of meritocracy. Perfect meritocracy may be optimal only for intermediate levels of heterogeneity.","If secular society has a creed, it is the creed of meritocracy: the belief that in the rat race of life rewards should go to the best performers, thereby unleashing society's full potential. Indeed, while many question how meritocratic society really is, few challenge the ideal.====The ‘rat race’ takes many forms: employees vie for promotions, students compete for Firsts, scientists contend for top journals' scarce pages, and so on. The importance of relative—rather than absolute—performance and the often starkly different rewards for winners and losers unify these disparate competitions. In the parlance of economics, they are all contests.====The case for meritocracy may seem self-evident: Not only would most consider an a-meritocratic contest, where winners are determined at random, unfair, it would also elicit little effort, since exertion would not improve a contestant's chance of winning.==== Nonetheless, we argue that meritocracy, in the sense of accuracy of performance ranking, can be too much of a good thing. Indeed, on reflection, increasing meritocracy cannot universally raise the performance of sufficiently diverse contestants, because it discourages the weak and makes the strong complacent. The former realize they now stand even less of a chance, while the latter rely ever more only slightly outperforming their weaker brethren to win a prize. In fact, only contestants ‘on the bubble’ (i.e., highly uncertain about winning or losing) are spurred on by greater meritocracy. A contest organizer intent on maximizing total output must account for these effects.====In homogeneous contests, discouragement and complacency effects do not arise. The intuition above then suggests that more meritocracy is always beneficial. For increasing marginal cost we show the opposite: beyond a certain point, more meritocracy is always harmful—it hurts the contest organizer by reducing total output, without benefiting contestants. This holds even though perfectly accurate performance ranking is costless in our model.====The intuition is as follows. When performance ranking is pure noise, nobody exerts any effort. Hence, injecting some meritocracy raises output. However, at a critical point ====, competition becomes so intense that contestants start ‘dropping out’—they put in no effort, lose, and earn zero payoff. Homogeneity implies that the remaining participants also earn zero in expectation. Thus, above the critical level of meritocracy, the contest organizer captures all the rents and should make the ‘pie’ as large as possible.====When contestants' marginal costs are increasing, Jensen's inequality implies that a given level of total output is produced most efficiently when all (homogeneous) contestants produce the same quantity. Duality of cost minimization and output maximization then implies that the output loss from contestants dropping out and producing zero must exceed the output gain from the remaining contestants working harder. Hence, the optimal level of meritocracy equals the critical level, while perfect meritocracy reduces total output and is Pareto inefficient. Under constant marginal costs, output losses exactly offset gains, so that any level of meritocracy greater than the critical level maximizes output and is Pareto efficient.====Now suppose the contest organizer optimizes across meritocracy and prize schedules, subject to a fixed prize budget. We first establish that, with homogeneous contestants, the organizer can do no better than to offer prizes of equal value. Then we show that within this class, ==== prize schedule can be optimal if the level of meritocracy is judiciously chosen. Subject to a technical condition the reverse result also holds: ==== level of meritocracy can be optimal, if we get to choose the number of (equal) prizes.====We also show that the optimal level of meritocracy positively relates to the number of prizes: contests with more, smaller prizes should be more meritocratic. In fact, perfect meritocracy is optimal iff the contest has a ‘participation-prize’ structure (i.e., almost everybody wins a prize). Intuitively, the only way to motivate contestants in an environment where almost all succeed is to make the performance ranking extremely accurate. At the other extreme, a winner-take-all structure is optimal iff the contest is almost a-meritocratic: the only way to motivate agents not to drop out when they know that almost nobody succeeds is to assign the prize almost at random.====Returning to (homogeneous-prize) contests with heterogeneous agents, first we prove that the homogeneous contest model is not a singularity: its results carry over to nearly homogeneous contests. Second, for general levels of heterogeneity, we show that the equilibrium effect of a rise in meritocracy on output can be partitioned into three, additively separable parts: attrition, competition, and heterogeneity. The attrition effect refers to the output loss from agents dropping out. The competition effect refers to the output gain from the average remaining contestant working harder. Finally, the heterogeneity effect corrects for the fact that a rise in meritocracy affects contestants ====, depending on their ability.====We find that, in the aggregate, discouragement of the weak and complacency of the strong dominate the spurring-on of the middle, such that the heterogeneity effect is negative. This implies that, as long as there is no attrition, heterogeneity reduces the benefits of meritocracy. Heterogeneity also gives meritocracy its best shot, however, because once it kicks in, attrition proceeds from the bottom of the ability distribution. This reduces the cost of agents dropping out when compared to the homogeneous case, and it makes perfect meritocracy potentially optimal for intermediate levels of heterogeneity. For high levels of heterogeneity, perfect meritocracy is once again strictly suboptimal, due to meritocracy-induced complacency among the most able.====Our analysis clarifies that whether meritocracy sharpens or softens competition depends on the heterogeneity of contestants: Under sufficient homogeneity, a rise in meritocracy pits contestants more harshly against each other, and thus it sharpens competition. Under sufficient heterogeneity, a rise in meritocracy discourages the weak and makes the strong complacent, and thus it softens competition. Nevertheless, perfect meritocracy reduces output in ==== cases: because of excessive competition in the homogeneous case and deficient competition in the sufficiently heterogeneous case.====We obtain these results in a novel, infinite player framework that approximates the finite-player contests traditionally analyzed in the literature. When the number of contestants is large, the threshold performance measure required to win a prize is essentially deterministic. This vastly simplifies the analysis, enabling the sharp results above. However, the intuitions and main results carry over to finite-player contests. For example, output is maximized in a Tullock (1980) contest when its discriminatoriness is increased just to the point where pure strategy equilibria cease to exist. If you go any higher, contestants start to mix over unequal efforts, which is inefficient with convex costs.====For ease of exposition, we have framed the discussion in terms of a principal who gets to increase the meritocracy of his contest. Examples include the replacement of human line judges with automated sensor technology in competitive tennis and the incorporation of attention tracking and micro-productivity data in employee performance assessments. But our results can also explain why a contest designer may want to decrease meritocracy, by adding noise to the performance evaluation process. Doing so increases total output in homogeneous contests, if the ‘natural’ level of measurement accuracy induces participants to drop out. It increases output in heterogeneous contests if, under the natural level, discouragement of the weak and complacency of the strong dominate encouragement of participants ‘on the bubble.’ The US Medical Licensing Examination program's recent decision to transition from reporting numeric scores to pass/fail for the Step 1 exam in the contest for medical residencies exemplifies the intentional introduction of noise.====The paper is organized as follows. Section 2 introduces and analyzes our baseline model with an infinite number of homogeneous contestants. In Section 3 we study the same model with heterogeneous contestants. Section 4 considers finite-player contests à la Lazear and Rosen (1981) and Tullock (1980). Section 5 places our findings in the extant literature. Section 6 concludes. Formal proofs have been relegated to Appendix A, while Appendix B contains the decreasing marginal cost case.",The limits of meritocracy,https://www.sciencedirect.com/science/article/pii/S0022053122000047,28 January 2022,2022,Research Article,64.0
Christensen Timothy M.,"Department of Economics, New York University, 19 W. 4th Street, 6th floor, New York, NY 10012, USA","Received 23 December 2020, Revised 17 August 2021, Accepted 10 January 2022, Available online 21 January 2022, Version of Record 28 January 2022.",https://doi.org/10.1016/j.jet.2022.105413,Cited by (0),"This paper derives primitive, easily verifiable sufficient conditions for existence and uniqueness of (stochastic) recursive utilities for several important classes of preferences. In order to accommodate models commonly used in practice, we allow both the state space and per-period utilities to be unbounded. For many of the models we study, existence and uniqueness is established under a single, primitive “thin tail” condition on the distribution of growth in per-period utilities. We present several applications to robust preferences, models of ambiguity aversion and learning about hidden states, and Epstein–Zin preferences.","Recursive utilities==== play a central role in contemporary macroeconomics and finance. Under recursive preferences, the value of a stream of per-period utilities is defined as the solution to a nonlinear, stochastic, forward-looking difference equation (or “recursion”). Despite the importance of recursive utilities, existence and uniqueness remains an unresolved issue as the recursions are typically not contraction mappings when state variables and per-period utilities are unbounded. In this paper, we derive primitive, easily verifiable sufficient conditions for existence and uniqueness of recursive utilities in stationary, infinite-horizon Markovian environments, with an emphasis on robust preferences, models of ambiguity aversion and learning about hidden states, and Epstein–Zin preferences. To accommodate models used extensively in macroeconomics and finance, we allow both the support of the Markov state vector and per-period utilities to be unbounded.====There are a large number of existence and uniqueness results for recursive utilities in models with compact state space, and possibly also bounded per-period utilities.==== However, many models used in macroeconomics and finance feature unbounded (i.e., non-compact) state spaces and unbounded utilities. For instance, the extensive long-run risks literature following Bansal and Yaron (2004) typically models state variables as vector autoregressive processes with unbounded shocks.==== A seemingly reasonable approach for models with non-compact state space is to truncate (i.e., compactifty) the state space and apply existing results for compact state spaces. After all, this truncation occurs implicitly when computing solutions numerically. However, truncation can materially alter the existence and uniqueness properties of the recursions we study. Knowing when the original model without truncation has a unique solution remains important for reconciling numerical solutions with the original (un-truncated) model envisioned by the researcher.====To illustrate this point, in Section 2 we present two empirically relevant examples to show how non-existence and non-uniqueness can arise under unboundedness. For both examples, we focus on a recursion arising under preferences for “robustness” (Hansen and Sargent, 1995, Hansen and Sargent, 2001; Hansen et al., 2006) and under Epstein–Zin preferences with unit intertemporal elasticity of substitution. The first example features a simplified version of the consumption growth process from Schorfheide et al. (2018), for which existence fails. The second example is from Bidder and Smith (2018) and Wachter (2013), for which uniqueness fails. When the state space is truncated, however, the recursion has a unique solution (irrespective of the truncation level) in both examples. This stark difference between the compact and unbounded case arises because the properties of the recursion depend delicately on the tail behavior of state variables and truncation, even at an arbitrarily high truncation level, materially alters tail behavior.====For many of the models we study, the single primitive sufficient condition we require for both existence ==== uniqueness is that the distribution of growth in per-period utilities has thin tails, in a sense we make precise. We verify this condition for robust preferences, models of ambiguity aversion and learning about hidden states, and Epstein–Zin preferences with unit intertemporal elasticity of substitution (IES). We consider both canonical linear-Gaussian environments which are pertinent to the long-run risks literature as well as environments featuring regime-switching and stochastic volatility.====As with much of the literature, we identify recursive utilities with fixed points of a nonlinear operator acting on a suitable function class. One strand of the literature on existence and uniqueness of (deterministic or stochastic) recursive utilities under unboundedness uses contraction mapping arguments for function classes defined via weighted sup-norms.==== However, it is not always easy to find a suitable weighting function under which operators defining recursive utilities are a contraction.==== Our arguments instead rely on monotonicity and concavity/convexity properties of the recursions we study, as with earlier work by Marinacci and Montrucchio (2010); see also Becker and Rincon-Zapatero (2017), Bloise and Vailakis (2018), and Ren and Stachurski (2020), primarily for the compact case.==== While our approach has some similarities with these earlier works, it differs in terms of the function class and technical arguments used so as to accommodate a broad class of empirically relevant models with unbounded state space. In particular, our arguments do not rely on certain topological properties of the space of bounded functions, such as the “solidness” of the cone of non-negative functions.====Our point of departure is to embed a transformation of the value function, such as its logarithm, in a class of unbounded but thin-tailed functions. The class is an exponential-Orlicz class used in empirical process theory in statistics (van der Vaart and Wellner, 1996) and modern high-dimensional probability (Vershynin, 2018).==== Exponential-Orlicz classes are naturally suited to the recursions we study, which involve the composition of exponential and logarithmic transforms and expected values.====The key high-level condition we use to establish uniqueness is that a subgradient (in the convex case) or supergradient (in the concave case) of the recursion is monotone and its spectral radius is strictly less than one. For many of the models we study, the recursion is convex and its subgradient is a discounted conditional expectation under a distorted law of motion. Verifying the spectral radius condition in these models amounts to checking a primitive thin-tail condition on the change-of-measure distorting the law of motion. We specialize this condition to particular models, deriving more primitive thin-tail conditions on the distribution of growth in per-period utility which are easy to verify: one simply has to know the tail behavior of the distribution.====To illustrate the usefulness of our results, we then present applications to three classes of models.====Section 4 studies a recursion arising under preferences for “robustness”, namely risk-sensitive preferences (Hansen and Sargent, 1995), multiplier preferences (Hansen and Sargent, 2001), constraint preferences (Hansen et al., 2006), and also under Epstein and Zin (1989) preferences with unit IES. There are currently no uniqueness results in the literature for this recursion allowing non-compact state space and unbounded utilities (see the discussion in Section 4), both of which are important for models in macroeconomics and finance. We establish new existence and uniqueness results under a single primitive thin-tail condition on utility growth. We verify this condition in canonical linear-Gaussian environments and environments featuring regime-switching and stochastic volatility, thereby establishing new existence and uniqueness results for such settings.====Section 5 considers models with learning. We study extensions by Hansen and Sargent, 2007, Hansen and Sargent, 2010 of multiplier preferences to accommodate both model uncertainty and uncertainty about hidden states, dynamic models of ambiguity aversion studied by Ju and Miao (2012) and Klibanoff et al. (2009), and Epstein–Zin preferences with unit IES and learning. There are currently no existence and uniqueness results in the literature allowing non-compact state space and unbounded utilities (see the discussion in Section 5). We establish existence and uniqueness under a single primitive thin-tail condition on utility growth. We verify the condition, and therefore establish existence and uniqueness results, for regime-switching environments (Ju and Miao, 2012) and Gaussian state-space models (Hansen and Sargent, 2007, Hansen and Sargent, 2010; Croce et al., 2015; Collard et al., 2018).====Finally, in Section 6 we examine Epstein–Zin recursive utilities with IES not equal to one. There are no uniqueness results for models with unbounded state space when risk aversion and intertemporal substitution are in a range normally encountered in the long-run risks literature (see the discussion in Section 6). Here we establish existence under an eigenvalue condition from Hansen and Scheinkman (2012) and a thin-tail condition on its corresponding eigenfunction. We verify this condition for linear-Gaussian environments which are pertinent to the long-run risks literature. Appendix A gives definitions of mathematical terms used in the main text. All proofs are in Appendix B.",Existence and uniqueness of recursive utilities without boundedness,https://www.sciencedirect.com/science/article/pii/S0022053122000035,21 January 2022,2022,Research Article,65.0
Herresthal Claudia,"University of Bonn, Adenauerallee 24-42, 53113 Bonn, Germany","Received 14 January 2020, Revised 19 November 2021, Accepted 20 December 2021, Available online 21 January 2022, Version of Record 29 January 2022.",https://doi.org/10.1016/j.jet.2021.105402,Cited by (3),"A decision maker faces a choice to withdraw or to retain a product but is uncertain about its safety. An agent can gather information through sequential testing. Players agree on the optimal choice under certainty, but the decision maker has a higher safety standard than the agent. We compare the case where testing is hidden and the agent can choose whether to disclose his findings to the case where testing is observable. The agent can exploit the additional discretion under hidden testing to his advantage if and only if the decision maker is sufficiently inclined to retain the product. Hidden testing then yields a Pareto improvement over observable testing if the conflict between players is larger than some threshold, but leaves the decision maker worse off and the agent better off if the conflict is smaller than this threshold.","Pharmaceutical companies have recently come under scrutiny for selective reporting of clinical trial outcomes.==== As a response to demands for greater transparency, several companies have pledged to register trials and report their outcomes in open online databases (Goldacre et al. (2017)). At first sight, it seems that such transparency would improve regulation because pharmaceutical companies can no longer hide trials with unfavorable outcomes. However, companies may also strategically respond by running fewer trials and this could mean that the regulator has to base his decisions on weaker evidence.====This paper analyzes how transparent information acquisition affects the likelihood of correct regulatory decisions and welfare. A decision maker (DM, e.g. a regulator) has to choose whether to withdraw or retain a product based on evidence gathered by an agent (e.g. a company). However, there is a conflict of interest between the players: the DM is more averse to retaining an unsafe product than the agent, that is, the DM has a higher safety standard than the agent. The agent can sequentially run costless tests up to some deadline at which point the DM makes his choice. We first consider a setting of ====, in which the agent's findings are publicly observed. We contrast this with a setting of ====, in which the DM neither observes for how long the agent has tested nor what the agent has found unless the agent chooses to disclose his findings.====One might expect that the DM is better off in equilibrium when testing is observable than when testing is hidden. After all, when testing is hidden, the agent can strategically hide evidence to influence the DM's choice. Provided the DM initially favors retaining the product, we show that the DM is indeed worse off under hidden testing if the conflict between players is small. However, if the conflict is large, then hidden testing yields a strict Pareto improvement over observable testing, that is, not only the agent but also the DM is strictly better off under hidden than observable testing.====This Pareto improvement arises for the following reasons. When testing is observable, the DM can react optimally to the evidence gathered, but the agent acquires evidence strategically. In particular, the agent can manipulate the DM's choice by stopping to test following certain findings while continuing to test following others. We characterize the unique equilibrium and show under which conditions the agent stops testing before the deadline even if further tests could be mutually beneficial. In particular, when the conflict is large and the DM initially favors retaining the product, the agent stops testing before the deadline if running further tests has the downside that the DM may come to favor withdrawal while the agent continues to favor retaining the product.====When testing is hidden, the agent can strategically withhold results from the DM. If the DM initially favors approval, it is indeed the case that the agent exploits this discretion to persuade the DM to retain the product more often than is optimal from the DM's perspective. However, since the agent has more influence over the DM's final choice than under observable testing, the agent has more incentives to keep testing. Moreover, the agent reveals to the DM the findings of the additional tests run under hidden relative to observable testing if and only if these tests convince the agent that withdrawal is optimal. Therefore, the additional tests trigger the DM to withdraw instead of retaining the product if and only if withdrawal is in both players' interest, resulting in a mutual benefit.====By contrast, if the conflict of interest between players is small instead, the DM is worse off and the agent better off under hidden than observable testing. The reason is that the agent becomes equally informed under each regime, but under hidden testing, the agent's selective disclosure can still lead the DM to make suboptimal choices.====A Pareto improvement from hidden testing would also exist in a static setting in which the agent commits ex ante to how long he will test. We use the model with sequential testing to show that a Pareto improvement from hidden testing arises even if the agent has no commitment power.==== Analyzing sequential testing also helps to illustrate the agent's trade-off more clearly as the agent is weighing up the advantages from testing versus stopping at each belief.====Our analysis can be applied to the context of postmarketing studies for medical drugs. These studies investigate a drug's safety or efficacy, or new indications of a drug after regulatory approval.==== Frequently, these studies are initiated by companies themselves, but they can also be requested by the regulator as a condition of approval. However, companies often do not carry out the requested studies or do not release all their findings (Glasser et al. (2007)), and it has been argued that regulators lack the power to enforce compliance.==== In light of this, demands for compulsory trial registration and reporting have been rising.====Our work predicts that registries help to improve regulatory decision making when the conflict of interest is small, but cause adverse welfare effects for all parties when the conflict of interest is large. In particular, without a trial registry, the company can run studies in private and withdraw the product if it finds that the product does not meet its own safety standards.==== In case the company does not withdraw the product, the regulator infers that at least the company's own safety standards are being met and, hence, the regulator will not see a reason to withdraw the product. By contrast, with a trial registry, a company with a relatively low safety standard is discouraged from performing such tests. The reason is that these tests could yield weak evidence that the product is unsafe, sufficient for the regulator to take the product off the market but not sufficient to fail the company's safety standards. Therefore, the company does not run tests that could uncover serious safety concerns, harming both itself and the regulator.====Our results can also be applied in the context of academic publishing. As various replication studies produced findings at odds with those in the initial studies, e.g. Open Science Collaboration (2015), calls for greater transparency in research increased, e.g. Nosek et al. (2018). One suggestion is to make registration of experiments and the use of result databases a condition for publication. However, as our paper points out, when all experiments need to be documented, a researcher with strong incentives to publish might be reluctant to further investigate a hypothesis that was supported by his existing experiments.==== The reason is that these additional experiments could contradict his earlier findings and shed some doubt on whether the hypothesis is true. This may not be sufficient to deter the researcher from wanting to publish his findings, but the editor may no longer find the evidence convincing enough and reject the paper. If the researcher could experiment in private, he would discover whether or not additional experiments strongly reject the hypothesis. In this event, he will not seek publication, thereby reducing the likelihood that a false finding is published.====The paper is structured as follows. Section 2 outlines the model. Observable testing is analyzed in Section 3, hidden testing in Section 4. The welfare comparison can be found in Section 5. Extensions and robustness checks are in Section 6. A detailed comparison with the existing literature can be found in Section 7. Section 8 concludes. Proofs can be found in the appendix.",Hidden testing and selective disclosure of evidence,https://www.sciencedirect.com/science/article/pii/S0022053121002192,21 January 2022,2022,Research Article,66.0
"Matsuyama Kiminori,Ushchev Philip","Northwestern University, USA,HSE University, St. Petersburg, Russia","Received 30 September 2020, Revised 8 December 2021, Accepted 13 January 2022, Available online 19 January 2022, Version of Record 24 January 2022.",https://doi.org/10.1016/j.jet.2022.105415,Cited by (2),"In existing models of endogenous innovation cycles, market size alters the amplitude of fluctuations without changing the nature of fluctuations. This is due to the ubiquitous assumption of CES homothetic demand system, implying that monopolistically competitive firms sell their products at an exogenous markup rate in spite of the empirical evidence for the procompetitive effect of entry and market size. We extend the Judd model of endogenous innovation cycles to allow for the procompetitive effect, using a more general homothetic demand system. We show that a larger market size/innovation cost ratio, by reducing the markup rate through the procompetitive effect, has destabilizing effects on the dynamics of innovation under two complementary sets of sufficient conditions; i) when the price elasticity is “not too convex” in price; and ii) when the demand system belongs to the two parametric families, “generalized translog” and “constant pass-through,” each of which features the choke price and yet contains CES as a limit case. Interestingly, the destabilizing effects become ==== as the demand system approaches to the CES limit within each family. We also discuss some cross-sectional implications in a multi-market extension. Because innovation/entry activities fluctuate more in larger markets, they are not always higher in larger markets than smaller markets. Furthermore, the sale of each product is more volatile in larger markets.","How does market size affect the dynamics of innovation? Many existing studies have already investigated the market size effect on innovation and long run growth.==== However, innovation is not only a source of long run growth. It is also a source of fluctuations because innovations tend to arrive in waves, as many have pointed out.==== Yet, little is known about the market size effect on the patterns of fluctuations in innovation and aggregate dynamics. In existing models of ====, market size merely affects the amplitude of fluctuations. Its potential effects on the patterns of fluctuations are muted by the ubiquitous assumption of the CES homothetic demand system for innovated products, which implies that monopolistically competitive firms sell their products at an exogenously constant markup rate, in spite of the empirical evidence of ====; see, e.g., Campbell and Hopenhayn (2005) and Feenstra and Weinstein (2017). That is, as more firms enter and compete against one another in a larger economy, they face more elastic demand for their products, which forces them to set their prices at lower markup rates. In the presence of such procompetitive effect, a larger market size relative to the innovation cost (or equivalently a smaller innovation cost relative to market size) and the resulting competitive pressures would make innovators more sensitive to changing market environments, thereby causing instability in the dynamics of innovation.====To capture this intuition, we extend the Judd (1985, section 4) model of endogenous innovation cycles to allow for the procompetitive effect. The Judd model offers an ideal setting for our purpose. First, it generates endogenous fluctuations along the unique equilibrium trajectory, unlike some other models of endogenous innovation cycles, which rely on expectational indeterminacy and multiple equilibria. Second, it is analytically tractable. Starting from any initial condition, its unique equilibrium trajectory can be obtained by iterating a skewed-V map (i.e., piecewise linear with two branches, decreasing in the lower branch and increasing in the upper branch). This class of maps generates a wide range of fluctuating patterns, including chaotic fluctuations, and yet it is simple enough to be characterized completely. In particular, one could study its properties by looking at a single constant number, ====, which we call the ====. This constant number determines how much past innovations discourage current innovations; it is the key factor that generates incentives for innovators to synchronize their activities and creates temporal clustering of innovation in the model. Under CES, this number is a monotonically decreasing transformation of the (exogenously determined) constant markup rate and hence it is independent of any other parameters. Yet, it captures the idea that greater competitive pressures lead to instability. This feature makes it possible to generate the destabilizing effects of market size by endogenizing the markup rate through the procompetitive effect of market size.====We generalize the Judd model by extending its CES homothetic demand system to a more general homothetic demand system, H.S.A., which stands for ====. It is one of the classes of homothetic demand systems studied in Matsuyama and Ushchev (2017), to which we further impose symmetry and gross substitutability and define over a continuum of input varieties to make it applicable to monopolistic competition, as in Matsuyama and Ushchev (2020a, section 3). The key feature of monopolistic competition under H.S.A. is that the price elasticity of demand curve for each product is a function of its “relative price,” which is defined as its own price divided by the price aggregator, which is ==== across all products. This common price aggregator fully captures the cross-price effects in the demand system, thus competitive pressures each innovator faces.====We have chosen this class of demand systems for the following reasons. First, they are ====. Although there have been many attempts to develop monopolistic competition models without CES, they have typically done so by making the demand system nonhomothetic.==== In order to ==== the procompetitive effect of a market size change, it is useful to avoid introducing the market size effect operating through nonhomotheticity.==== Furthermore, homotheticity makes it straightforward to extend it to multi-sector or multi-market settings. Second, under the additional assumption that the price elasticity function is increasing (i.e., the price elasticity goes up as one moves up along the demand curve; the so-called Marshall's second law of demand), H.S.A. exhibits the procompetitive effect.==== Third, H.S.A. contains as special cases both CES and homothetic translog, which has been used to introduce the procompetitive effect.==== Thus, H.S.A. allows us to perform robustness checks for these two demand systems. Fourth, as the name suggests, H.S.A. features a single aggregator, which serves as the sufficient statistic to capture all ==== each firm faces caused whether by entry of new firms or by pricing of other firms. Due to this single aggregator property, the Judd model under H.S.A. remains equally tractable as the original Judd model under CES. Indeed, its dynamics are still characterized by a skewed-V map.==== The only difference from the case of CES is that both the markup rate as well as the delayed impact of innovation, ====, become functions of the market size/innovation cost ratio. Thus, by investigating the properties of these functions, we can use the Judd model under H.S.A. as a simple way of studying how the market size/innovation cost ratio affects the patterns of fluctuations in innovation dynamics through its procompetitive effect.====In our analysis, we identify two complementary sets of sufficient conditions under which an increase in the market size/innovation cost ratio increases the delayed impact of innovation, ====, through the procompetitive effect, and hence it has the destabilizing effects on the dynamics of innovation. The first set of the sufficient conditions is that the price elasticity as a function of its relative price, is not “too convex,” that is, the price elasticity goes up when moving up along the demand curve, but not in a too accelerating way. The second set of sufficient conditions deals with the cases where the above sufficient condition fails due to the presence of the choke price. They are two parametric families within H.S.A., which we call “generalized translog” and “constant pass-through.” These two parametric families feature the choke price and yet contain CES as the limit case, which allows us to check the robustness of the results under CES. Interestingly, the destabilizing effects become ==== as the demand system approaches to CES. And the qualitative properties of the dynamics change ==== with an arbitrarily small departure from CES. This suggests that, even if an empirically estimated pass-through rate is close to one, using CES as an approximation could be misleading. We also discuss some cross-sectional implications in a multi-market extension. For example, because innovation/entry activities fluctuate more in larger markets, they are not always higher in larger markets than smaller markets. Furthermore, the sale of each product, conditional on surviving idiosyncratic obsolescence shocks, is more volatile in larger markets.====The rest of the paper is organized as follows. In Section 2, we revisit the Judd model under CES, derive a skewed-V map, which generates the equilibrium trajectory, and offer a full characterization of the properties. In doing so, we highlight its key features and explain the intuition why it generates endogenous fluctuations in innovation, why an increase in the (exogenous) constant elasticity of substitution between products has a destabilizing effect, and yet why it is independent of the market size/innovation cost ratio. In Section 3, we formally introduce symmetric H.S.A. demand systems with gross substitutes defined over a continuum of products. Then, we derive the dynamical system for the Judd model under H.S.A., which still features a skewed-V map. In Section 4, we introduce another assumption on H.S.A., Marshall's (weak and strong) 2==== law of demand, which generates the procompetitive effect under H.S.A. In Section 5, we present two propositions. Proposition 1 states that, under H.S.A. with the procompetitive effect, the delayed impact of innovation, ====, can take the same range of values as under CES, even though it now depends on the market size/innovation cost ratio. Proposition 2 and its Corollary state that the delayed impact of innovation, ====, is strictly increasing in the market size/innovation cost ratio if the procompetitive effect is combined with the “not too convex” condition. In Section 6, we present two parametric families within H.S.A., “generalized translog” and “constant pass-through,” both of which feature the procompetitive effect, the choke price, and contain CES as a limit case. In both families, an explicit calculation allows us to show that an increase in the market size/innovation cost ratio increases the delayed impact of innovation, ====, and hence has the destabilizing effects on the dynamics of innovation. Interestingly, the destabilizing effects become ==== as the demand system approaches to the CES limit within each family. In Section 7, we discuss some cross-sectional implications in a multi-market extension. We conclude in Section 8. Appendices A through E offer some relatively more technical materials.",Destabilizing effects of market size in the dynamics of innovation,https://www.sciencedirect.com/science/article/pii/S0022053122000059,19 January 2022,2022,Research Article,67.0
"He Wei,Li Jiangtao","Department of Economics, The Chinese University of Hong Kong, Hong Kong,School of Economics, Singapore Management University, Singapore","Received 10 February 2021, Revised 12 December 2021, Accepted 22 December 2021, Available online 28 December 2021, Version of Record 5 January 2022.",https://doi.org/10.1016/j.jet.2021.105403,Cited by (11),"We study the design of auctions when the auctioneer has limited statistical information about the joint distribution of the bidders' valuations. More specifically, we consider an auctioneer who has an estimate of the marginal distribution of a generic bidder's valuation but does not have reliable information about the correlation structure. We analyze the performance of mechanisms in terms of the revenue guarantee, that is, the greatest lower bound of revenue across all joint distributions that are consistent with the marginals. A simple auction format, the second-price auction with no reserve price, is shown to be asymptotically optimal, as the number of bidders goes to infinity. For markets with a finite number of bidders, we (1) solve for the robustly optimal reserve price that generates the highest revenue guarantee among all second-price auctions with deterministic reserve prices, and (2) show that a second-price auction with a random reserve price generates the highest revenue guarantee among all standard dominant-strategy mechanisms.","Traditional models in mechanism design make strong assumptions about the detailed knowledge of the designer in the economic environment. Subsequently, the theoretical conclusions can sometimes be fragile; mechanisms that are optimized to perform well when the assumptions are exactly true may still fail miserably in the much more frequent cases when the assumptions are untrue. The so-called Wilson doctrine holds that practical mechanisms should be designed without assuming that the designer has precise knowledge about the economic environment.====This paper studies a robust version of the single-unit auction problem where we relax the assumption about the auctioneer's knowledge in the payoff environment—the auctioneer has limited statistical information about the joint distribution of the bidders' valuations. In particular, we consider an auctioneer who has an estimate of the marginal distribution of a generic bidder's valuation but has non-Bayesian uncertainty about the correlation structure. Lacking the knowledge of the correlation structure, our auctioneer ranks mechanisms according to their revenue guarantee, that is, the greatest lower bound of revenue across all joint distributions that are consistent with the marginals.====Several motivations can be offered for considering the robustness to the correlation structure:====Within this correlation-robust framework, our analysis focuses on the following three dimensions:====To fix ideas and also to illustrate some of the motivations of our analysis, let us revisit the seminal paper of Myerson (1981) that studies optimal auction design in the independent private-value setting.====While we only considered two particular correlation structures, the analysis already suggests that in large markets, the second-price auction with the optimally chosen reserve price under the independent private-value model is more vulnerable than the second-price auction with no reserve price. Intuitively, the optimality of the second-price auction with reserve price ==== in the independent private-value model depends on the intricate tradeoff of the following two events: (1) the largest valuation is less than ==== (so that the reserve price is not favorable); and (2) the largest valuation is weakly larger than ==== conditional on that the second largest valuation is less than ==== (so that the reserve price is favorable). Thus, the second-price auction with reserve price ==== may not perform well if the correlation structure is misspecified. In contrast, the second-price auction with no reserve price generates an expected revenue that is equal to the expectation of the second largest valuation regardless of the correlation structure. As such, one might expect that the second-price auction with no reserve price is a reasonable mechanism given non-Bayesian uncertainty about the correlation structure.====Indeed, our first result, Theorem 1, establishes the robust optimality of the second-price auction with no reserve price in large markets among all dominant-strategy mechanisms.==== We show that the revenue guarantee of the second-price auction with no reserve price converges to the expectation of a generic bidder's valuation. Importantly, the expectation of a generic bidder's valuation is an ==== of the highest revenue guarantee in our framework; our auctioneer could never rule out the maximally positive correlation as a candidate for the joint distribution, and the expectation of a generic bidder's valuation is the full surplus under this particular correlation structure.====Although the robustness of a mechanism is a key concern, it is only one of several desiderata in practical mechanism design. Indeed, when selecting an auction format, the auctioneer might have to balance many different criteria. This perspective (of balancing multiple criteria) makes our result all the more appealing: besides having nice theoretical properties and being widely adopted in practice, the second-price auction with no reserve price is asymptotically optimal in the correlation-robust framework.==== Our analysis supports the use of the second-price auction with no reserve price in large markets from a novel robustness perspective, complementary to existing reasons.====To show that the revenue guarantee of the second-price auction with no reserve price converges to the expectation of a generic bidder's valuation, we need to solve the minimization problem in which Nature minimizes the auctioneer's expected revenue by choosing a joint distribution that is consistent with the marginals. While this is a non-trivial task, due to the functional form of the ex post revenue function of the second-price auction with no reserve price, there is strong intuition about the properties of the worst-case correlation structure: if bidder ==== has the highest valuation and bidder ==== has the second highest valuation, then all the other bidders have the same valuation as that of bidder ====. This is because (1) the choice of Nature for the other bidders' valuation does not matter for the ex post revenue for this particular realization; (2) since Nature is bounded by the marginal consistency constraint, choosing the same valuation as that of bidder ==== provides the maximum flexibility for Nature to reduce the auctioneer's ex post revenue for other realizations.====This intuition leads us to consider a candidate correlation structure that has a natural economic interpretation as the maximally positive correlation conditionally on the existence of a strong bidder. Let ==== denote the distribution of a generic bidder. One bidder, whom the seller believes is equally likely to be any one of the bidders, is a strong bidder whose value is drawn from ==== conditional on that her valuation is weakly higher than some threshold. Every other bidder is a weak bidder whose value is drawn from ==== conditional on that their values are weakly less than this threshold. Furthermore, all the bidders' valuations are maximally positively correlated.====To show that the candidate correlation structure is indeed a worst-case correlation structure, we adopt a duality approach. This step of our analysis is closely related to the optimal transport theory (see for example Villani (2003)). To wit, Nature's minimization problem can be interpreted as an optimal transportation problem in which Nature seeks to implement the transportation at minimal cost. A transportation plan is a joint distribution that is consistent with the marginals, and Nature's cost function is the ex post revenue function of the auctioneer. While the literature of optimal transport focuses on the case of two random variables, we work with multiple random variables. To be rigorous and self-contained, we prove an ====-dimensional generalization of the weak duality property in the Kantorovich duality theorem (see Villani (2003, Theorem 1.1.3)). This generalization is straightforward and follows from a modification of the original proof.====While Theorem 1 is a result on large markets, the second-price auction with no reserve price also performs well in small and moderate sized markets. We show (in Remark 1(d)) that for any marginal distribution, if there are ==== bidders, the difference of the full surplus and the revenue guarantee of the second-price auction with no reserve price is bounded above by ==== (when the set of valuations is normalized to ====). For a numerical example, suppose that each bidder's valuation is uniformly distributed on the ==== interval. The revenue guarantee of the second-price auction with no reserve price is 75% of the full surplus with 4 bidders, and is 90% of the full surplus with 10 bidders.====The auctioneer could potentially do better using other mechanisms in markets with a finite number of bidders. For practical purposes, we first consider a familiar class of auction forms, second-price auctions with deterministic reserve prices, that are both theoretically appealing and widely adopted in practice.==== Formally, we work with a maxmin optimization problem in which the auctioneer chooses a deterministic reserve price to maximize the worst-case expected revenue, where the worst case is taken over all joint distributions that are consistent with the marginals.==== Theorem 2 solves for the robustly optimal reserve price that generates the highest revenue guarantee among all deterministic reserve prices for any finite number of bidders. We then focus on a class of dominant-strategy mechanisms that we call standard dominant-strategy mechanisms (that is, mechanisms such that bidders who do not have the highest bid do not get the object), including second-price auctions with random reserve prices. Theorem 3 shows that a second-price auction with a random reserve price generates the highest revenue guarantee among all standard dominant-strategy mechanisms.====The remainder of the introduction reviews the related literature. Section 2 presents our model. Section 3 and Section 4 show that the second-price auction with no reserve price is asymptotically optimal. Section 5 solves for the robustly optimal reserve price for any finite number of bidders. Section 6 shows that a second-price auction with a random reserve price generates the highest revenue guarantee among all standard dominant-strategy mechanisms. Section 7 concludes the paper.",Correlation-robust auction design,https://www.sciencedirect.com/science/article/pii/S0022053121002209,28 December 2021,2021,Research Article,68.0
"Halim Edward,Riyanto Yohanes E.,Roy Nilanjan","Division of Economics, School of Social Sciences, Nanyang Technological University, 48 Nanyang Avenue, Singapore 639818, Singapore,Division of Economics, School of Social Sciences, Nanyang Technological University, SHHK-4-70/SHHK-05-56E, 48 Nanyang Avenue, Singapore 639818, Singapore,Department of Economics and Finance, College of Business, City University of Hong Kong, 83 Tat Chee Avenue, Kowloon Tong, Hong Kong","Received 12 January 2021, Revised 4 October 2021, Accepted 11 December 2021, Available online 17 December 2021, Version of Record 23 December 2021.",https://doi.org/10.1016/j.jet.2021.105400,Cited by (0),"). The presence of traders having induced motive to smooth consumption is not sufficient to eliminate price bubbles. Despite the asset being consistently priced higher than the equilibrium price, traders are able to share idiosyncratic risk and attain higher welfare. The co-existence of traders with income shocks along with those having no induced motive to trade does not hinder in the former smoothing their consumption stream. Our results hold for markets with and without aggregate risk.",None,Sharing idiosyncratic risk even though prices are “wrong”,https://www.sciencedirect.com/science/article/pii/S0022053121002179,17 December 2021,2021,Research Article,69.0
"Cheng Ing-Haw,Hsiaw Alice","University of Toronto, Rotman School of Management, 105 St. George St., Toronto, Ontario M5S 3E6, Canada,Brandeis University, International Business School, 415 South St., Waltham, MA 02453, USA","Received 18 September 2019, Revised 18 November 2021, Accepted 12 December 2021, Available online 17 December 2021, Version of Record 28 December 2021.",https://doi.org/10.1016/j.jet.2021.105401,Cited by (1),"Why do individuals interpret the same information differently? We propose that individuals form beliefs following Bayes' Rule with one exception: when assessing the credibility of experts, they double-dip the data and use already-updated beliefs instead of their priors. This “pre-screening” mechanism explains why individuals jointly disagree about states of the world and the credibility of experts, why the ordering of signals and experts affects final beliefs, and when individuals over- or underreact to new information. In a trading game, pre-screening generates excessive speculation, bubbles, and crashes. Our theory provides a micro-foundation for why individuals disagree about how to interpret the same data.","Disagreement is everywhere, over topics ranging from the causes of climate change to the consequences of stimulus spending. A core feature of many disagreements is that individuals disagree not just about the substance of their positions (“Do humans affect climate change?”) but also about the credibility of information sources such as experts that inform those positions (“How credible are scientists and their data?”). In debates about economics (“What is the value of stimulus spending?”), medicine (“Are vaccinations safe for children?”), and politics (“Was there interference in elections?”), one side typically expresses supreme confidence in their preferred sources while dismissing the other side's sources. Bayesian learning about an unknown state of the world using signals from an information source with uncertain credibility struggles to explain this type of disagreement when all agents observe the same signals and have common priors.====This paper proposes a departure from Bayesian learning called “pre-screening.” The central idea of pre-screening is that an agent recognizes that credibility is uncertain but mistakenly treats credibility as an “ancillary parameter,” or a parameter necessary to learn with some precision before applying Bayes' Rule to form final beliefs. A pre-screener first forms an updated first-stage belief about credibility. She then forms posterior beliefs by weighing the data using the updated belief about credibility instead of her prior and overlooks that this step “double-dips” the data. In contrast, a Bayesian weighs the data using only her prior beliefs because she carefully separates her priors from the likelihood of the data.====Consider the following example. An individual who is reasonably sure that he weighs 200 pounds steps on a scale that he believes is likely accurate, and the scale reads 300 pounds. Surprised by the reading, a Bayesian's posterior is that the scale is likely inaccurate but that there is some chance he weighs 300 pounds, as he carefully combines the likelihood of the data with his prior belief that the scale was accurate. In contrast, a pre-screener first infers that the scale is inaccurate upon seeing the reading and then combines the likelihood of the data with this updated belief as though he knew the scale was inaccurate all along. This process leads the pre-screener to discount the possibility that he might be 300 pounds too much, on the premise that the scale is inaccurate. Intuitively, the pre-screener thinks: “I now think the scale is not credible, and my beliefs should reflect that I saw non-credible signals.”====We motivate pre-screening by observing that “using the data twice” is an error that recurs in practice, particularly when the initial use is for a seemingly legitimate reason—in this case, reducing uncertainty about credibility when the principal object of interest is the state. For example, criticisms of statistical methods such as empirical Bayes and posterior Bayes Factor methods often center around the potential for double-dipping the data (Lindley, 1991; Aitkin, 1991). Similar criticisms exist of applied work in fields as varied as finance and neuroscience (Lo and MacKinlay, 1990; Vul et al., 2009).====Overlooking the use of updated data is plausible as evidence of hindsight bias suggests that individuals who have seen data tend to behave as if they “knew it all along” (Fischhoff, 1975; Hawkins and Hastie, 1990). A pre-screener overlooks her erroneous substitution of updated first-stage beliefs for priors, while a Bayesian carefully distinguishes her prior from subsequent data. Relatedly, evidence on the curse of knowledge (Camerer et al., 1989) suggests that individuals have difficulty conceptualizing what it was like to be uninformed in the past.====Section 2 introduces pre-screening. We assume that signal sources are data-generating processes that produce signals about an uncertain state and abstract from strategic motives to isolate the effects of biased learning. In the base model, we model a source's credibility as its accuracy in discerning the true state of the world (e.g., do economists accurately understand the economy).====Section 3 characterizes three implications of pre-screening for disagreement. First, pre-screening endogenously generates correlated disagreement. Suppose two agents with common priors observe signals with the same objective information content. If the agents are Bayesian, they never disagree. If the agents are pre-screeners, they disagree about the state if and only if they disagree about credibility and credibility objectively matters. Specifically, pre-screener ==== thinks the objectively-favored state is more likely than pre-screener ==== does if and only if ==== also thinks the source is more credible than ==== does.====Second, disagreement about credibility between two pre-screeners occurs if they see signals in different orders, even if they share common priors and see the same objective information content. The reason is that pre-screening, through the repeated substitution of updated beliefs about credibility for priors, leads to path-dependent beliefs whereby early signals have an outsized influence on the interpretation of later signals and thus final posterior beliefs. Bayesian beliefs, in contrast, do not depend on the order of signals.====Third, the model provides conditions for when pre-screeners endogenously over- and under-react to new signals that depend on how the signals affect first-stage beliefs about source credibility. Pre-screening thus provides a unified explanation for seemingly contradictory deviations from Bayes' Rule. For example, the literature suggests that agents exhibit confirmation bias and under-react to disconfirming news that contradicts their beliefs about the state (Rabin and Schrag, 1999). On the other hand, agents also over-react to news that is so disconfirming that it causes agents to re-evaluate their worldview or paradigm (Ortoleva, 2012; Galperti, 2019). Distinct from confirmation bias, pre-screeners can over-react or under-react to confirming and disconfirming news.====Section 4 characterizes pre-screeners' beliefs when there are multiple sources. Pre-screeners can disagree about sources' credibilities and the state even when they observe all sources' signals, as long as they encounter sources in different orders. The reason is that beliefs about an early source's credibility color the interpretation of later sources' signals. Thus, our theory explains why individuals disagree about states of the world and the credibilities of multiple sources of information.====Section 5 considers three extensions. First, we show that the possibility that sources may “slant” signals toward a given state can create further scope for error by pre-screeners. While the base model considers the case where agents do not know a source's accuracy, in reality agents may also be uncertain of whether sources have slant (e.g., whether scientists have an agenda). Second, we extend pre-screening to the case where a source may send multiple signals in one period and show how signal order can be important for persuasion. Third, we show that key results hold when we allow for fading memory.====Section 6 illustrates the implications of pre-screening for prices and trade. In a trading game similar to Harris and Raviv (1993), we compare outcomes when all traders are pre-screeners with outcomes when all traders are Bayesians. The game with pre-screeners features weakly more trade than the game with Bayesians since agents speculate against each other's beliefs in the sense of Harrison and Kreps (1978). Specifically, a pre-screener may buy an asset even if the price is greater than what she thinks it is worth to resell it later to other agents. This speculation is akin to traders “riding the bubble” (Abreu and Brunnermeier, 2003; Brunnermeier and Nagel, 2005). Sharp swings in prices akin to bubbles and crashes, as defined by Barberis (2018), can occur beyond what can be easily explained by a model with Bayesian agents with heterogeneous priors. The application to the trading game illustrates how pre-screening can generate several empirically-relevant dynamics within a unified framework.====Our main contribution is to offer a parsimonious micro-foundation for why individuals disagree about the interpretation of the same data. Our approach suggests that erroneous learning about credibility may play a central role in explaining the joint disagreement over substance and credibility in a wide range of settings. Section 7 summarizes pre-screening's key implications and how they differ from other approaches in the literature, including heterogeneous priors, inattention, correlation neglect, and confirmation bias. Concerning heterogeneous priors (Morris, 1995), our model provides a theory for such priors' origins. We conclude with a discussion of avenues for future research.",Distrust in experts and the origins of disagreement,https://www.sciencedirect.com/science/article/pii/S0022053121002180,17 December 2021,2021,Research Article,70.0
"Lahkar Ratul,Mukherjee Sayan,Roy Souvik","Department of Economics, Ashoka University, Rajiv Gandhi Education City, Sonipat, Haryana, 131029, India,Economic Research Unit, Indian Statistical Institute, Kolkata, India","Received 11 February 2021, Revised 13 November 2021, Accepted 3 December 2021, Available online 14 December 2021, Version of Record 28 December 2021.",https://doi.org/10.1016/j.jet.2021.105398,Cited by (4),"We consider a generalization of perturbed best response dynamics in population games with a continuum of strategies. The previous literature has considered the logit dynamic generated through the Shannon entropy as a deterministic perturbation. We consider a wider class of deterministic perturbations satisfying lower semicontinuity and strong convexity. Apart from the Shannon entropy, Tsallis entropy and Burg entropy are other perturbations that satisfy these conditions. We thereby generate the generalized perturbed best response dynamic with a continuum of strategies. We establish fundamental properties of the dynamic and show convergence in potential games and negative semidefinite games.","Perturbed best response dynamics are one of the most important classes of dynamics in evolutionary game theory. Best response is, of course, the fundamental behavioral norm in game theory. But due to multiplicity of best responses and the fact that the best response can change abruptly, the best response dynamic arising from this behavioral protocol is technically difficult to analyze (Gilboa and Matsui (1991), Hofbauer (1995)). Perturbed best response dynamics avoid this problem by considering a uniquely defined smoothed version of the best response which can then be analyzed using the standard theory of differential equations. Even if best responses are uniquely defined, perturbed best responses are important in their own right because there may be errors in agents' perception of payoffs or in implementing the best response. In such situations, perturbed best response may be a better model of behavior than the classical best response. For this reason, the notion of perturbed best response has also found applications in rationalizing data in experimental economics (Goeree et al. (2016)).====The logit dynamic (Fudenberg and Levine (1998), Chapter 4) is the prototypical representative of this class of dynamics obtained by perturbing the best response through the Shannon entropy function. A number of authors have since generalized the logit dynamic to the class of perturbed best response dynamics (Benaım and Hirsch (1999), Hofbauer and Hopkins (2005), Hofbauer and Sandholm, 2002, Hofbauer and Sandholm, 2007). As with most of evolutionary game theory, most of this literature on perturbed best response dynamics has been in the context of large population games with finite strategy sets.==== However, similar to classical game theory, various economic applications of evolutionary game theory are also more conveniently done with games with a continuum of strategies. Despite introducing certain measure theoretic complications related to, for example, defining the state space, continuous strategy models greatly simplify the characterization of Nash equilibrium and Pareto optimal states.==== Therefore, to make evolutionary game theory a more relevant technique for economic analysis, there is a need to develop this theory in the context of games with continuous strategy sets. This has been the main motivation behind the literature on extending the prominent evolutionary game dynamics from finite strategy to continuous strategy games. Papers in this area include Oechssler and Riedel, 2001, Oechssler and Riedel, 2002 and Cheung (2016) on the replicator dynamic, Hofbauer et al. (2009) on the Brown–von Neumann–Nash (BNN) dynamic, Cheung (2014) on the pairwise comparison dynamic, and Perkins and Leslie (2014) and Lahkar and Riedel (2015) on the logit dynamic.====The present paper is a contribution to this literature on continuous strategy evolutionary dynamics. Specifically, it focuses on the class of perturbed best response dynamics for such games. The earlier literature on this topic has been confined to the logit dynamic (Perkins and Leslie (2014), Lahkar and Riedel (2015)). The logit dynamic is generated when agents' payoffs are perturbed using the Shannon entropy and agents best respond with respect to these perturbed payoffs. Here, we generalize the logit dynamic by considering perturbations other than the Shannon entropy. Thus, instead of assuming any particular form of perturbation, we consider a broader class of perturbations that satisfy certain lower semicontinuity and strong convexity conditions. For example, apart from the Shannon entropy, the Tsallis entropy and the Burg entropy are other admissible entropies under these conditions. As far as we know, this is the first paper that considers such a generalization of perturbed best response dynamics in continuous strategy games. We do note, though, that our generalization is entirely with respect to deterministic perturbations unlike in the finite strategy literature where both deterministic and stochastic perturbations have been considered (Hofbauer and Sandholm (2002)).====Similar to the logit best response, we first perturb an agent's payoff using the general perturbation function and allow the agent to best respond. This generates the generalized perturbed best response (GPBR) and the associated dynamic. We establish the fundamental properties of the dynamic. Thus, we show that rest points of the dynamic, which are fixed points of the perturbed best response function and which we call perturbed equilibria, exist and converge to Nash equilibria of the underlying game when the perturbation becomes small. We also show that unique solution trajectories that are continuous with respect to initial states exist for this class of dynamics. Results of a similar nature have, of course, already been established for the logit dynamic (Lahkar and Riedel (2015)). But by extending these results beyond the logit dynamic, this paper establishes the idea of perturbed best response on sounder foundations in continuous strategy games.====This is important because one of main motivations behind looking for broader and more diverse classes of evolutionary dynamics is to ensure that the results of evolutionary game theory are not dependent upon the specific properties of a particular dynamic. Otherwise, those results will not be robust. Our findings are in line with this general quest in evolutionary game theory. Conclusions obtained from the application of the logit dynamic in continuous strategy games now become more credible since such results now generalize to the wider class of perturbed best response dynamics. This applies, for example, immediately to the results on evolutionary implementation in Lahkar and Mukherjee (2019) where the logit dynamic has been applied. These results now also hold for the entire class of deterministic perturbed best response dynamics.====Another ubiquitous pursuit in evolutionary game theory has been classes of games in which a wide variety of evolutionary dynamics converge to Nash equilibria. Nash equilibrium prediction in such games then becomes more credible even when agents are myopic as is the case in evolutionary game theory. Two such classes of games in which almost all well known evolutionary dynamics, both finite strategy and continuous strategy, do converge are potential games (Monderer and Shapley (1996), Sandholm (2001)) and negative semidefinite games (Hofbauer and Sandholm (2009)).==== Potential games are defined by a real valued function which summarizes payoffs in the game. Negative definite games are characterized by the property of “self–defeating externalities” due to which small groups of agents changing strategy find themselves at a relative payoff disadvantage. We show that our GPBR dynamic also converges in these two classes of games when their strategy sets are continuous. Convergence in this case is, of course, not to Nash equilibria but to their approximation, perturbed equilibria. Once again, such convergence has already been established for the continuous strategy logit dynamic. The present results show that such convergence arises from fundamental properties of perturbations like lower semicontinuity and convexity. This generalization is also in line with similar conclusions that arise in the case of finite strategy perturbed best response dynamics (Hofbauer and Sandholm (2007)).====Analyzing evolutionary dynamics for continuous strategy dynamics raises certain significant mathematical complications which makes the exercise a non–trivial extension of finite strategy dynamics. For example, an important question to resolve is the choice of topology in the space of population states, which takes a measure theoretic form. Following much of the literature (for example, Oechssler and Riedel, 2001, Oechssler and Riedel, 2002, Lahkar and Riedel (2015)) we apply the strong and weak topologies in our analysis. The weak topology is helpful in resolving questions like the existence of a perturbed equilibrium and convergence of solution trajectories of the perturbed best response dynamics to perturbed equilibria in potential and negative definite games. The strong topology, on the other hand, is required for results like the existence of solution trajectories of the dynamic.====Nor is the present analysis a straightforward extension of earlier papers on the continuous strategy logit dynamic. The absence of any specific functional form for the perturbation raises certain technical challenges that requires us make appropriate assumptions to make the analysis tractable. For example, we need assumptions like lower semicontinuity, strong convexity and the existence of a continuous density function for the perturbation to establish the existence of a perturbed equilibrium. Showing convergence of perturbed equilibria to Nash equilibria requires us to impose some restrictions (Conditions A1 and A2) on the perturbation function to ensure that a small change in the population state does not lead to a large change in the perturbation. Such assumptions also ensure that, like the logit dynamic, our general class of perturbed best response dynamics are forward invariant not just in the state space, but in the space of states with bounded density functions. Such results then allow us to construct appropriate Lyapunov functions that establish convergence in the classes of potential and negative semidefinite games.====Beyond its theoretical merit, is there any importance of our exercise from the point of view of applications? To answer this question, we consider a large population model of Cournot competition with a linear cost function. Such a game is a potential game. Moreover, due to the linearity of the cost function, the game has a convex set of Nash equilibria. Hence, it is possible that GPBR dynamics generated by the Shannon, Tsallis and Burg entropies converge to perturbed equilibria that, as the perturbation becomes small, approximate different Nash equilibria. We present certain simulations that suggest that is indeed the case. Therefore, the pattern of social behavior can depend upon the manner in which best responses are perturbed, particularly in situations where there is a continuum of Nash equilibria. This holds even in the limit as the perturbations become small, suggesting that our generalization of perturbed best response dynamics can have meaningful consequences for applications.====The rest of the paper is as follows. Section 2 defines preliminaries of the model. In Section 3, we analyze the perturbed best response and perturbed equilibria. This section also provides examples of perturbations beyond the Shannon entropy to which our analysis applies; namely, the Tsallis entropy and the Burg entropy. Section 4 establishes fundamental properties of the perturbed best response dynamic. In Sections 5 and 6, we show convergence in potential games and negative definite games respectively. Section 7 presents the application to the Cournot model and Section 8 concludes.",Generalized perturbed best response dynamics with a continuum of strategies,https://www.sciencedirect.com/science/article/pii/S0022053121002155,14 December 2021,2021,Research Article,71.0
Pivato Marcus,"THEMA, CY Cergy Paris Université, France","Received 20 November 2020, Revised 16 November 2021, Accepted 5 December 2021, Available online 9 December 2021, Version of Record 13 December 2021.",https://doi.org/10.1016/j.jet.2021.105399,Cited by (1),"How should we aggregate the ex ante preferences of ==== agents with heterogeneous beliefs? Suppose the state of the world is described by a random process that unfolds over time. Different agents have different beliefs about the probabilistic laws governing this process. As new information is revealed over time by the process, agents update their beliefs and preferences via Bayes rule. Consider a Pareto principle that applies only to preferences which remain stable in the long run under these updates. I show that this “eventual Pareto” principle implies that the social planner must be a utilitarian. But it does not impose any relationship between the beliefs of the individuals and those of the planner, except for a weak compatibility condition.","In a watershed 1955 paper, Harsanyi considered social decisions in the presence of risk. He showed that if all individuals and the social planner are expected utility maximizers, and the planner's ex ante preferences satisfy the Pareto property with respect to the individual ex ante preferences, then the planner is a “utilitarian”, in the sense that the social utility function is a weighted average of the individual utility functions. While the connection between this formal result and philosophical utilitarianism can be debated (Weymark, 1991), there can be no doubt that it has been highly influential in welfare economics.====Harsanyi formulated his result in the von Neumann-Morgenstern framework, where risks are described by objective probabilities. But around the same time, Savage (1954) developed a theory of decision-making under uncertainty, where each agent maximizes expected utility relative to an idiosyncratic probability distribution, interpreted as her “subjective beliefs”. This raised a question: does Harsanyi's result remains valid when the individuals and the planner are Savage-style subjective expected utility maximizers, perhaps with different beliefs? In an influential paper, Mongin (1995) answered this question in the negative: in the Savage framework, the planner can satisfy the ex ante Pareto axiom if and only if all agents have the ==== beliefs. Since such homogeneity of beliefs is empirically implausible and perhaps even normatively undesirable, Mongin interpreted this result as an impossibility theorem.====This seemed to deal a fatal blow to Harsanyi's project of founding utilitarianism in the theory of rational decisions. But in 2004, Gilboa et al. rescued Harsanyi by weakening the ex ante Pareto axiom so that it applied ==== to preferences between acts that depend upon events about whose probabilities all agents agreed. They showed that this restricted ex ante Pareto axiom was compatible with belief heterogeneity, but still strong enough to imply ==== that the social planner is a utilitarian, but ==== that her subjective beliefs are a weighted average of the subjective beliefs of the individuals.====As already noted by Mongin, 1995, Mongin, 1997, when individuals have heterogeneous beliefs, a naïve application of the Pareto axiom to their ex ante preferences may lead to cases of ====, where there is disagreement in both the utility functions and the beliefs of the individuals, but these disagreements “cancel out”, to create apparent agreement in their ex ante preferences over acts. The key insight of Gilboa et al. (2004) was to restrict the Pareto axiom to ==== such cases of spurious unanimity, by applying it only when the relevant underlying beliefs are in agreement. Gilboa et al.'s landmark result became the point of reference for all subsequent literature on social decisions under uncertainty (Chambers and Hayashi, 2006, Chambers and Hayashi, 2014; Alon and Gayer, 2016; Danan et al., 2016; Billot and Vergopoulos, 2016; Zuber, 2016; Qu, 2017; Desai et al., 2018; Sprumont, 2018, Sprumont, 2019; Hayashi and Lombardi, 2019; Ceron and Vergopoulos, 2019; Brandl, 2021; Dietrich, 2021; Billot and Qu, 2021).==== Most of these papers either employ non-SEU preferences or follow Gilboa et al. (2004) in weakening ex ante Pareto, so as to avoid cases of spurious unanimity while still axiomatizing a simultaneous aggregation of utilities and beliefs. To distinguish mutually beneficial financial trades from mere “betting”, Gayer et al. (2014) and Gilboa et al. (2014) consider weak ex ante Pareto conditions that are still stronger than the one proposed by Gilboa et al. (2004). But Mongin and Pivato (2020, §6) recently argued that Gilboa et al.'s restricted Pareto axiom is actually ====. The linear pooling of beliefs which they derive from this axiom might not be an asset, but a liability, because there are situations where linear pooling of beliefs is not desirable —in particular, it is not compatible with Bayesian updating under the arrival of new information. More fundamentally, Gilboa et al.'s restricted Pareto axiom is still vulnerable to a sort of “spurious unanimity” in ====: different individuals may assign the same probability to an event, but for different and incompatible reasons. For example: starting from the same prior, they might Bayes-update on different private information, but coincidentally end up assigning the same posterior probability to some event, even though their combined information would yield a ==== conditional probability for this event. Mongin and Pivato refer to this as ====.====These cases of spurious unanimity and complementary ignorance suggest that ex ante Pareto is a mistake, even in weakened form. Perhaps we should jettison it entirely, and fall back on a purely ex post approach. But in some situations, this is not possible. In a temporally extended decision problem with an infinite planning horizon, there is a progressive resolution of uncertainty over time, but this process never terminates in a state of final certainty. The ex post outcome on Monday evening becomes the ex ante situation on Tuesday morning, and the ex post of Tuesday evening becomes the ex ante of Wednesday morning, ====. So a purely ex post approach may be unavailable.====Even in decisions without an explicitly intertemporal element, we may never obtain total knowledge of the state of nature. Risse, 2001, Risse, 2003 and Hild et al., 2003, Hild et al., 2008 consider decisions under uncertainty with a non-atomic Boolean algebra of events, so that any event can always be split up into smaller events, encoding more precise information. They construct an example where ever-more-precise information can cause agents to reverse their preferences, and then reverse them again, repeatedly, forever. Their conclusion is that, for practical purposes, there is no such thing as ex post.====Meanwhile, in social decisions where all agents share the same probabilistic beliefs, and it is plausible that these beliefs are ====, the ex ante Pareto axiom is unproblematic, and even normatively compelling. Consider an insurable risk with a publicly known and well-established loss distribution (e.g., based on extensive actuarial data). If Alice is more risk-averse than Bob, then there are insurance contracts that Bob is willing to sell and Alice is willing to buy. The ex ante Pareto axiom explains why society should endorse such transactions. In this case, the axiom asserts a kind of ====: it says that if rational agents with correct beliefs can negotiate a mutually beneficial risk-pooling arrangement, then we should approve. A repudiation of ex ante Pareto would make it difficult to explain why insurance markets are socially valuable and should be facilitated, whereas markets for quack medicines or bets on sports events are not. So ex ante Pareto should not be ==== rejected, but rather, restricted to cases where it is “appropriate” because agents agree for the “right reasons”. This was precisely the justification given by Gilboa et al. (2004) for their restricted Pareto axiom. But as I noted earlier, their particular restriction is not appropriate in decision environments with changing information.====However, there is another important issue, which has not received sufficient attention in the literature on Bayesian social aggregation. Humans are fallible. Not only are their beliefs susceptible to future revision in light of new information, but their expected-utility calculations themselves could be inaccurate, because of misspecifications in their probabilistic beliefs or an imperfect understanding of the causal relationship between actions and consequences. Such fallibility is especially relevant in complex, multi-period stochastic decision problems. So an individual's preference for one policy over another is more credible if it is ====, in the sense that it has a margin of error. Likewise, a unanimous preference in a society is more persuasive if it is a unanimously ==== preference.====The present paper is a reaction to these concerns. I will consider a model of decision-making under uncertainty in which agents steadily receive more information over time, and update their beliefs and their preferences accordingly. Many contemporary social decision problems have this structure. Three obvious examples are anthropogenic climate change, emerging pandemics, and macroeconomic crises. These are all complex, poorly understood phenomena, unfolding over time. Different agents may have different beliefs about how these phenomena will evolve in the future, either because they assign different values to parameters in their scientific models, or because they use entirely different models. Due to different beliefs and different utility functions, different agents may have different preferences over policies. As time passes and new empirical data arrives (e.g. about weather patterns, infection and mortality rates, GDP trends, etc.), the agents may update their beliefs. They may revise their estimates of model parameters, or even discard certain models altogether in the face of new evidence. Thus, their policy preferences may change over time.====In particular, there might initially be unanimous consensus amongst the individuals that policy ==== is better than policy ====, but this consensus might crumble as the individuals learn new facts about the world. Thus, in retrospect, it would have been a mistake to apply the ex ante Pareto axiom to this ephemeral consensus. At the same time, the individuals might gradually converge on a unanimous consensus that policy ==== is better than policy ====. If this new consensus ==== over the long term, then it may be a suitable target for the ex ante Pareto axiom.====As earlier noted, the individuals might never obtain ==== knowledge about the underlying phenomenon. Thus, they might never converge to ==== agreement in their beliefs. But there might still be enough belief-convergence to support an enduring consensus that ==== is better than ====. Is such an enduring consensus a sufficient foundation for a Paretian social preference for ==== over ====? Not necessarily, because of the fallibility of individual preferences, discussed above. This enduring consensus would be more compelling if it was built from ==== preferences, each having a margin of error.====A unanimous, enduring, and robust preference for ==== over ==== provides a cogent Paretian justification for a social preference for ==== over ====. But does it justify an ==== social preference for ==== over ====? In light of individual fallibility, perhaps not. A more conservative Pareto principle would simply require a social planner to not ==== the individuals' robust, enduring consensus for ==== over ==== by developing a robust, enduring social preference for ==== over ==== instead.====In view of these considerations, I will restrict the Pareto axiom to cases where the individuals not only unanimously prefer one act to another, but these preferences are ====, and this unanimity ==== as the individuals acquire more and more information. This Pareto axiom prohibits the social planner's robust, enduring preferences from directly opposing a robust, enduring consensus of the individuals. I will show that this weak, asymptotic form of the Pareto axiom is necessary and sufficient for the social planner to be a utilitarian. But it does ==== imply that social beliefs are an aggregate of individual beliefs. (I argue that this should be seen as a strength, rather than a weakness; see §4.7.)====To obtain this utilitarian conclusion, I require only a weak compatibility between agents' beliefs. The agents may have heterogeneous beliefs, but there must be some probability distribution (perhaps not representing anyone's beliefs) which is ==== with respect to the beliefs of all agents (including the social planner). Roughly, this means that while agents can disagree about probabilities, there is some agreement about which events are ==== (i.e. have probability zero) or ==== (i.e. have probability one): an event deemed impossible by one agent cannot be deemed almost-certain by another.====The paper is organized as follows. Section 2 introduces some tools from probability theory. Section 3 contains the framework and main result. Section 4 contains further interpretive remarks and conceptual discussions. Appendix A contains the proof of the main result, while Appendix B contains the proofs of other statements made in the paper.",Bayesian social aggregation with accumulating evidence,https://www.sciencedirect.com/science/article/pii/S0022053121002167,9 December 2021,2021,Research Article,72.0
"Luo Xiao,Qiao Yongchuan,Sun Yang","National University of Singapore, Singapore,University of Electronic Science and Technology of China, China,Southwestern University of Finance and Economics, China","Received 3 August 2020, Revised 23 October 2021, Accepted 28 November 2021, Available online 9 December 2021, Version of Record 20 December 2021.",https://doi.org/10.1016/j.jet.2021.105396,Cited by (0),"). The notion of CE==== is fully characterized by undominated correlated equilibria that involve admissible actions only. Moreover, any CE==== distribution can be represented by an incentive-compatible direct-revelation mechanism; the set of CE==== distributions is a convex polyhedron that contains all perfect equilibria. Our paper thus provides a strategic foundation for undominated correlated equilibrium. We also show that the CE==== distribution is equivalent to a weak version of acceptable correlated equilibrium (ACE) as in ==== (and equivalent to ACE in two-person games).","In the classic paper “Subjectivity and Correlation in Randomized Strategies,” Aumann (1974) introduces the notion of correlated equilibrium (see also Aumann, 1987). The correlated equilibrium appears to be a useful solution concept if preplay communication is allowed for players in economic situations (see, e.g., Myerson, 1986b and Forges, 1986). A correlated equilibrium (CE) of a game is defined as a (pure-strategy) Nash equilibrium of a communication game in which players receive private messages from a correlation device or a mediator before the game is played. Among others, Aumann (1974) characterizes the effects of communication by showing the revelation principle for the concept of CE in games with complete information: ====Accordingly, for any equilibrium of any general communication mechanism, there is an incentive-compatible direct-revelation mechanism that is essentially equivalent. The revelation principle allows us to restrict attention to the class of canonical correlated equilibria that models a CE as a distribution on choice combinations (Aumann, 1974, 1987). Furthermore, the set of CE distributions is a convex polyhedron that contains all mixed Nash equilibria.====However, the “imperfection” problem might arise in equilibrium: For example, in a canonical communication game, a player would be not willing to follow an action recommended by the mediator if the player believes that there is any chance that opponent players will disobey the mediator's recommendations, notwithstanding the fact that every player is willing to obey the mediator's recommendations conditional on the absolutely certainty that opponent players are themselves willing to obey. To deal with the imperfection problem, Dhillon and Mertens (1996) offer the notion of a “perfect correlated equilibrium” (PCE), which is a trembling-hand perfect refinement (Selten, 1975) of CE. Dhillon and Mertens (1996) define a PCE as a perfect equilibrium in a corresponding communication game constructed using a correlation device—that is, there exists a sequence of completely mixed strategy profiles that converges to the equilibrium strategies to support each player's equilibrium strategy in the communication game (Definition 1). Dhillon and Mertens (1996) demonstrate, using an example, that their notion of PCE fails to satisfy the revelation principle in the sense that a PCE distribution cannot be induced by a canonical PCE. As Dhillon and Mertens (1996, p. 282) conclude, “the failure of the revelation principle, and that the set of perfect direct correlated equilibria is not convex (does not even contain the convex hull of the perfect equilibria) hence is a very unsatisfactory solution concept.”====The undesirable properties of PCE jeopardize the application of this notion in economics; for instance, characterization of PCE becomes rather complicated, because all possible message spaces must be taken into consideration in constructing a correlation device. As Gerardi and Yariv (2007, pp. 330-331) point out in voting situations, “Unfortunately, the literature on games with communication has not yet developed enough of a technical apparatus to deal with perfect equilibria. In fact, the characterization of the outcomes induced by perfect equilibria of arbitrary cheap talk extensions is still an open question (the research frontier is probably Dhillon and Mertens (1996), who provide an answer only for two-person games with complete information). We are thus less optimistic about finding general results when concentrating on this particular equilibrium notion at this point in time.”==== The idea of perfection is central to a game-theoretic analysis of strategic behavior in complex interactions (see, e.g., van Damme, 1991). In this respect, we offer a fundamental revelation principle for a reformulation of PCE in games with complete information.====This paper aims to resolve the failure of the revelation principle for CE in the presence of trembles in players' strategic behavior, posted in Dhillon and Mertens (1996). In doing so, we introduce a notion of correlated equilibrium with message-dependent trembles (CE====) (Definition 2). That is, we define the notion of CE==== as a message-dependent trembles refinement of Nash equilibrium in a corresponding communication game constructed using a correlation device, in which players can adopt fairly feasible and distinct theories of trembles in opponent players' strategic behavior. More specifically, the notion of CE==== requires that upon receiving a message, every player's equilibrium action remains optimal against message-dependent (possibly correlated) trembles in the equilibrium play of opponent players. Put differently, the notion of CE==== is a weaker version of perfect correlated equilibrium that allows a player to believe that (1) the other players may tremble conditional on his own message and (2) their trembles can be correlated.==== Because the notion of CE==== accommodates more flexible forms of trembles in players' strategic behavior, every PCE must be a CE====.====The main purpose of this paper is to show that the revelation principle holds for the notion of CE====—that is, every CE==== distribution can be induced by a canonical CE==== (Proposition 1(i)). Because each recommended action is an optimal response to a completely mixed action profile, it must be weakly undominated. We show that CE==== can be fully characterized by the notion of undominated CE in which no player's weakly dominated action is assigned with a positive probability (Proposition 1(ii)); moreover, the set of CE==== distributions is a convex polyhedron that contains all mixed perfect equilibria (Proposition 1(iii)). Consequently, in light of the revelation principle, we can without loss of generality limit attention to the canonical communication system in which the “obedient” Nash equilibrium behavior involves no inadmissible actions. Hence, our approach provides a strategic foundation for the notion of undominated CE. Our notion of CE==== is also related to Myerson's (1986a) notion of “acceptable correlated equilibrium” (ACE), which is defined through the canonical representation of CE. In this paper, we show that the CE==== distribution is equivalent to a weak version of ACE; in two-person games, it is ACE (Proposition 3).====In the canonical structure of communication, a mediator determines his recommendations according to the canonical correlation device, and the equilibrium strategies are that every player obeys the mediator's recommendations, even if every player also takes into consideration trembles in the correlated play of players. We show that strategy-planning incentive constraints can be replaced with action-planning incentive constraints under message-dependent trembles (Proposition 2). By the revelation principle for the notion of CE====, we obtain a straightforward definition of CE==== distributions over joint actions (cf. Remark 4).====The rest of the paper is organized as follows. In Section 2, we offer an illustrative example to explain the paper's key ideas and main results. Section 3 introduces the setup, notation, and definitions. Section 4 presents the central result of the revelation principle, and studies the relation between the notions of CE==== and ACE. Section 5 is devoted to discussions on related solution concepts through the lens of CE====. We also provide a simple version of deliberative voting in Gerardi and Yariv (2007) to explain the usefulness of our approach. Section 6 concludes. All proofs are relegated to Appendixes.",A revelation principle for correlated equilibrium under trembling-hand perfection,https://www.sciencedirect.com/science/article/pii/S0022053121002131,9 December 2021,2021,Research Article,73.0
"Bikhchandani Sushil,Mishra Debasis","Anderson School at UCLA, Los Angeles, United States of America,Indian Statistical Institute, Delhi, India","Received 25 February 2021, Revised 1 August 2021, Accepted 28 November 2021, Available online 6 December 2021, Version of Record 9 December 2021.",https://doi.org/10.1016/j.jet.2021.105397,Cited by (3),"It is well-known that optimal (i.e., revenue-maximizing) selling mechanisms in multidimensional type spaces may involve randomization. We obtain conditions under which deterministic mechanisms are optimal for selling two identical, indivisible objects to a single buyer. We analyze two settings: (i) decreasing marginal values (DMV) and (ii) increasing marginal values (IMV). Thus, the values of the buyer for the two units are not independent.====We show that under a well-known condition on distributions (due to ====), (a) it is optimal to sell the first unit deterministically in the DMV model and (b) it is optimal to bundle (which is a deterministic mechanism) in the IMV model. Under a stronger sufficient condition on distributions, a deterministic mechanism is optimal in the DMV model.====Our results apply to heterogeneous objects when there is a specified sequence in which the two objects must be sold.","We consider optimal, i.e., expected revenue maximizing, mechanisms for selling two identical units of an object to a buyer. The buyer's type (values for the units) is two dimensional and privately known to the buyer. We focus on two cases: decreasing marginal values and increasing marginal values. Thus, the buyer's values for the units are not independent.====A general solution to the optimal mechanism design problem for the sale of multiple indivisible products is unknown. Unlike the single product case, the optimal mechanism for selling two or more products may involve randomization (see Thanassoulis (2004), Manelli and Vincent (2006), Pycia (2006), and Hart and Reny (2015)). Our objective is to find sufficient conditions under which a deterministic mechanism is optimal among all mechanisms for selling two identical units, including random mechanisms.====We assume that the seller can commit to a mechanism. Implicit in this is the assumption that the mechanism can be objectively verified by both parties. As Laffont and Martimort (2002) emphasize, it is easier to verify a deterministic mechanism than a random mechanism. For instance, commitment by a seller to a random mechanism may not be credible in a one-shot interaction with a buyer. Perhaps this is a reason for the limited use of randomized selling methods.==== Under our sufficient conditions, the seller does not sacrifice optimality for credible commitment to a (deterministic) mechanism.====The assumption of homogeneous objects reduces the dimensionality of the price space and therefore the dimensionality of random allocation rules (compared to heterogeneous objects). While this represents a simplification of the problem of finding an optimal mechanism, the correlation of values in our paper increases complexity.====With homogeneous objects, there is a natural order of transactions: the second unit can be sold only after the first unit is sold. In some settings with two ==== objects, one of the two objects can be sold only after the other object is sold.==== For instance, the warranty on a product is only sold to a buyer who purchases the product. Another example is when a seller offers two versions of a product, basic or premium. The premium version of a product can be viewed as the basic version plus an upgrade. That is, the upgrade is sold only if the basic product is also sold. All our results apply to such settings.====We define a function of buyer (marginal) values,==== ====, which plays a key role in the analysis.==== The function Φ is a guidepost for making revenue improvements to any incentive compatible and individual rational mechanism. If Φ satisfies certain single-crossing conditions, then incentive compatibility and individual rationality are maintained in the improved mechanism. The function Φ depends only on the distribution of types.====With decreasing marginal values, we show that if Φ satisfies single-crossing in the horizontal direction (which corresponds to changes in ==== only), then there exists an optimal selling mechanism in which the first unit is sold deterministically. We refer to a mechanism in which the first unit is sold deterministically as a ====. Line mechanisms are completely described by the payment for the first unit and the probability of allocating the second unit to types on the vertical line ====, where ====. If, in addition to horizontal single-crossing, Φ satisfies single-crossing in the vertical direction (which corresponds to changes in ==== only) then there is an optimal mechanism which is ====, i.e., a line mechanism with at most one probabilistic value for allocating the second unit. Finally, if Φ satisfies diagonal single-crossing (along the diagonal boundary of the support of the distribution), in addition to horizontal and vertical single-crossing, then there exists an optimal mechanism that is deterministic.====Our results for increasing marginal values are under weaker conditions, in that horizontal single-crossing of Φ is sufficient for the existence of an optimal mechanism that is deterministic. In this optimal mechanism, the two units are bundled together and sold at a take-it-or-leave-it price.====We provide a class of distributions for decreasing marginal values, called the ordered decreasing values model, where our single-crossing conditions take a simple form and Φ is related to virtual utilities. Similarly, we introduce an ordered increasing values model with increasing marginal values.====To our knowledge, the function Φ is new to this literature. However, horizontal single-crossing of Φ is equivalent to a sufficient condition introduced by McAfee and McMillan (1988). A version of Φ may be useful in proving the optimality of deterministic mechanisms in other settings, such as the sale of heterogeneous objects in more general models.==== Early work on mechanism design with multidimensional types includes Rochet (1987), McAfee and McMillan (1988), Wilson (1993), Armstrong (1996), and Rochet and Choné (1998). As these papers focused primarily on divisible products, existence of deterministic mechanisms was not an issue.====Thanassoulis (2004), Manelli and Vincent (2006), Manelli and Vincent (2007), Pycia (2006), Pavlov (2011a), Pavlov (2011b), and Hart and Reny (2015) investigate the sale of indivisible, heterogeneous objects with independent, additive values. As already noted, it may be optimal to randomize in this setting. Moreover, as Hart and Reny (2015) show, the optimal revenue may not be monotone in the distribution of the buyer's type.==== Correlation between a buyer's values adds another layer of complexity and may increase the desirability of randomization. In a model with two heterogeneous goods and correlated values, Hart and Nisan (2019) show that mechanisms of bounded menu size, such as deterministic mechanisms, may yield a negligible fraction of the optimal revenue.====The optimality of deterministic mechanisms is investigated by Manelli and Vincent (2006), who obtains sufficient conditions in a model with two heterogeneous objects with independent, additive values; related papers include Menicucci et al. (2015) and Tang and Wang (2017). This question is also the focus of Malakhov and Vohra (2009) and Devanur et al. (2020) in a homogeneous objects model in which the buyer has the same privately known value for all units, but the number of units desired is privately known. In a general model, Haghpanah and Hartline (2021) obtain sufficient conditions for the optimal mechanism to be bundling, which is a deterministic mechanism. In a model in which buyers purchase one of two heterogenous objects, Pavlov (2020) obtains an optimal mechanism (which may be random) and shows that if the optimal mechanism is deterministic, then it takes the form of selling the units separately.====When there are two or more buyers, Chen et al. (2019) provide sufficient conditions for the existence of an optimal Bayesian incentive compatible mechanism that is deterministic. These conditions do not apply to our setting, where there is one buyer, or to dominant strategy incentive compatible mechanisms. Daskalakis et al. (2017) and Kleiner and Manelli (2019) characterize optimality for a multi-product monopolist using duality theory.====There is a literature on approximately optimal mechanism design, starting with the work of Chawla et al. (2007) and Hartline and Roughgarden (2009). Recent contributions include Dhangwatnotai et al. (2015), Hart and Nisan (2017), Babaioff et al. (2018), Hart and Nisan (2019), Hart and Reny (2019), Bhattacharya et al. (2020), and Babaioff et al. (2020). These papers identify simple mechanisms, which are often deterministic mechanisms, that guarantee a constant fraction of the optimal mechanism revenue. These guarantees are usually independent of the prior. Another related paper is Carroll (2017), which shows that posted-prices are robustly optimal for heterogeneous objects with additive values.====The rest of the paper is organized as follows. We investigate the decreasing marginal values model in Section 2. In Section 2.2, we provide a sufficient condition under which it is optimal to sell the first unit deterministically. Line mechanisms are characterized in Section 2.3 and sufficient conditions for the existence of an optimal mechanism that is deterministic are provided in Section 2.4. Necessary conditions for a specific deterministic mechanism to be optimal are presented in Section 2.5. Two special cases of decreasing marginal values, the ordered decreasing values model and the conditional decreasing values model, are presented in Sections 2.6 and 2.7 respectively. Our results for increasing marginal values are in Section 3. In Section 4, we describe the application of the results to heterogeneous objects and to a two-period model; we also show the optimal revenue is monotone under our sufficient conditions for deterministic optimality. All proofs are in an Appendix.",Selling two identical objects,https://www.sciencedirect.com/science/article/pii/S0022053121002143,6 December 2021,2021,Research Article,74.0
"Han Zhao,Tan Fei,Wu Jieran","Department of Economics, William & Mary, United States of America,Department of Economics, Chaifetz School of Business, Saint Louis University, United States of America,Center for Economic Behavior and Decision-Making, Zhejiang University of Finance and Economics, China,Academy of Financial Research and School of Economics, Zhejiang University, China","Received 29 April 2020, Revised 21 November 2021, Accepted 22 November 2021, Available online 26 November 2021, Version of Record 8 December 2021.",https://doi.org/10.1016/j.jet.2021.105395,Cited by (2)," that feature asymmetric information sets, endogenous signals, and higher-order expectations.","Incorporating information frictions in dynamic general equilibrium models has produced fruitful results that shed light on a wide range of questions in macroeconomics and finance [Angeletos and Lian (2016)]. Despite the progress made so far, solving dynamic models with endogenous information frictions remains a daunting task. To address this challenge, this article proposes a frequency-domain method of solving and characterizing macroeconomic and finance models with endogenous information frictions.====The presence of endogenous information is inevitable in most macroeconomic models with rational expectations. Households and firms form expectations conditional on endogenous economic conditions, whose decisions, in turn, impact the state of the economy. In models of the financial market, learning from endogenous asset prices is also an indispensable ingredient as these prices aggregate information. Another notion of information endogeneity emerges in disciplined bounded rationality models, such as rational inattention models, where the choice of information structure is endogenous.====While endogenous information constitutes an essential ingredient of the general equilibrium (GE) feedback mechanism between expectations and economic outcomes that produces propagation, persistence, and volatility [e.g., Angeletos and Lian (2018), Chahrour and Gaballo (2019)], it also complicates the model solution significantly. In models with endogenous information, an information fixed point exists between agents' perceived law of motion of the economy (including endogenous variables such as prices) and the actual law of motion, based on agents' actions and conditional expectations using the endogenous signals. The underlying GE effect, operating through the lens of endogenous learning, creates equilibria that in general admit no finite-state representation.====The complication is exacerbated in the environment of information heterogeneity. When an agent's view about economic fundamentals differs from that of other agents, the “forecasting the forecasts of others” problem gives rise to the role of higher-order expectations (HOEs) in shaping model dynamics. The recursion of HOEs implies that agents need to form an infinite order of expectations about what others believe when making decisions. When agents are not learning from endogenous variables, Huo and Takayama (2018) show that HOEs are tractable, and the model equilibrium permits a finite-state representation in the time domain. Therefore, the greatest challenge to solving and analyzing the model arises when information is both endogenous and heterogeneous.====The key contribution of this paper is to develop an analytic policy function iteration (APFI) method to address the challenge directly. We use a simple asset pricing model of Singleton (1987) to demonstrate the basic idea of our method (Section 2). We circumvent the issue with infinite-state representation in the time domain by treating the frequency domain as the appropriate state space.==== Our idea is straightforward and analogous to the classical method used to solve dynamic programming problems in the time domain. However, the mathematical foundation and numerical implementation of our approach differ substantially from standard policy function iterations.====We first provide three theorems that establish the theoretical foundation for the APFI method (Section 3). The first theorem characterizes the basis for functional approximations in the frequency domain. We use the set of rational functions to approximate the true equilibrium solution. The functional form of the solution within each iteration is known (as rational analytic functions). We show that any linear stationary equilibrium can be approximated arbitrarily well by a VARMA==== process. The second theorem uses the theory of analytic continuation to construct the appropriate state-space grid in the real unit interval ====. We apply the theory of the convergence of analytic functions to establish a convergence criterion for our algorithm. The third theorem constructs a fast and efficient method of computing conditional expectations in the frequency domain, which are typically hard to evaluate in the time domain. Specifically, we apply the discrete Fourier transform to compute conditional expectations under different information sets.====We then establish the baseline APFI algorithm along with numerical details that facilitate the computation (Section 4). Admittedly, frequency-domain methods have not been widely adopted by macroeconomists. To minimize the user's fixed cost, we provide a handy MATLAB-based, object-oriented toolbox called “z-Tran” that implements these procedures, serving as the paper's second contribution.==== This toolbox encapsulates all required frequency-domain methods via a user-friendly interface. An applied user can quickly input the model's linearized equilibrium conditions into the canonical form of the baseline APFI algorithm and test the model implications. We also allow experienced users to call each routine in the toolbox independently and modify the baseline APFI algorithm whenever appropriate.====The existing frequency-domain method is powerful in deriving analytical characterization of the equilibrium [e.g., Whiteman (1983), Tan and Walker (2015), and Huo and Takayama (2018)]. However, it also faces limitations due to the cumbersome symbolic algebra involved in the procedure. The APFI framework circumvents these limitations. With numerical efficiency and stability, it extends the applicability of existing methods from characterizing small, illustrative models to solving large, quantitative dynamic stochastic general equilibrium (DSGE) models with general information frictions. The APFI framework is also flexible in the choice of information structure, as our canonical representation nests nearly all examples considered in the literature, including full information, imperfect (exogenous or endogenous) information, and heterogeneous (dispersed or hierarchical) information.====In related research, Huo and Takayama (2018) characterize the analytical solution to incomplete-information models by applying the state-space method to obtain the Wold fundamental representation in the frequency domain. While Huo and Takayama (2018) focus primarily on exogenous information, our APFI approach is designed mainly for models with endogenous information where the signal process per se does not admit an exact VARMA representation. It is also suitable for models whose explicit solutions from algebraic derivation become infeasible.==== In this sense, our APFI framework complements the approach of Huo and Takayama (2018) and the two methods agree when information is exogenous.====Another influential work by Nimark (2017) develops a state-space method of solving dynamic models of dispersed information. His method truncates the (potentially) infinite-dimensional state vector of HOEs to a finite order and is widely adopted in the literature. One limitation of the truncation approach arises when agents in the model face asymmetric information frictions. That is, different groups of agents face ex-ante distinct information structures. In this case, the truncation strategy no longer works due to the explosion of cross-expectations among groups. Another limitation of the truncation approach emerges when the time series structure of the primitive model extends beyond the simple AR(1) recursion so that pinning down a suitable state space becomes tricky. In contrast, the APFI framework is not constrained by these limitations and works well for models with general information frictions.====In summary, the APFI framework is particularly useful for solving dynamic models with endogenous signals and substantial information heterogeneity. We demonstrate the reliability and flexibility of our method by applying it to study three macroeconomic models (Section 5). The first example solves a prototypical New Keynesian DSGE model similar to Melosi (2017). We highlight the sensitivity of model solutions to incomplete-information firms' endogenous signals. We then augment this DSGE model with a fiscal sector that features primary surplus and government debt and introduce incomplete-information households. We showcase four distinct fiscal effects of the primary surplus shock by allowing firms and households to observe differential, non-nested, and endogenous information. Our results provide new insight into how fiscal policy affects inflation.====The second example considers a HANK-type model of Angeletos and Huo (2021) with endogenous wealth distribution and incomplete information. We modify their original model by allowing different groups of households to be endowed with asymmetric, endogenous information sets. We examine how such asymmetric information frictions interact with the group heterogeneity and shape the model dynamics. To the best of our knowledge, the extension of introducing distinct, endogenous information frictions to different sectors (groups) of the economy is novel, which highlights the flexibility of our methodology.====Our baseline APFI algorithm requires an invertibility condition on the non-expectational block of the model system. Moreover, it cannot handle models with random walk dynamics. In the last application, we consider such a dispersed-information RBC model from Graham and Wright (2010). The model features non-stationarity and multiple equilibria, and it does not satisfy the invertibility condition. To circumvent the limitations of the baseline APFI algorithm, we design three extended APFI algorithms to solve this model. We also conduct a numerical experiment that compares the performance of our algorithms with the time-domain truncation method, which demonstrates the comparative advantages of the APFI framework in terms of accuracy, flexibility, and initial conjecture choice.",Analytic policy function iteration,https://www.sciencedirect.com/science/article/pii/S002205312100212X,26 November 2021,2021,Research Article,75.0
"Safra Zvi,Segal Uzi","Warwick Business School, University of Warwick, UK,Department of Economics, Boston College, USA","Received 31 March 2020, Revised 25 July 2021, Accepted 21 November 2021, Available online 26 November 2021, Version of Record 6 December 2021.",https://doi.org/10.1016/j.jet.2021.105393,Cited by (0),"We consider a risk averse decision maker who dislikes ambiguity as in the Ellsberg urns. We analyze attitudes to ambiguity when the decision maker is exposed to unrelated sequences of ambiguous situations. We discuss the Choquet expected utility, the smooth, and the maxmin models. Our main results offer conditions under which ambiguity aversion disappears even without learning and conditions under which it does not. An appendix analyzes compound gambles within the expected utility model and demonstrates how to rank them.","A patient suffers from a certain disease. The doctor offers two possible treatments. A standard, well investigated treatment ====, which with probability ==== leads to a good outcome and with probability ==== leads to a less favorable outcome, which is still better than no treatment.==== Alternatively, she offers him a new treatment ==== with somewhat ambiguous probabilities of success. It is however known that whatever the outcome, it improves over that of the standard treatment. Moreover, although the probabilities are not known for sure, they are believed to be somewhere around ====. Both treatments are preferred to no treatment and the question is which of the two to choose. The patient is ambiguity averse, and as the improvement in the outcomes of the new treatment is not much, he prefers the standard treatment with the known probability of success. In other words, ====.====The doctor does not have any information she did not share with the patient. Moreover, although she knows that she will see many patients like him, she believes that she won't gain any information about the probability of success of the new treatment, as this probability depends entirely on unobservable characteristics of the patients. Her preferences over risk and uncertain prospects are the same as the patient's (alternatively, she adopts the patient's preferences). Does it follow that she too will prefer the standard treatment to the new one?====Although they have exactly the same information and preferences and do not gain any new information by learning, there is one dimension in which the patient and the doctor are different, and this is the number of cases they face. The patient sees only one case, his. Ambiguity aversion can be explained as fear of the unknown. Many people believe that they are unlucky and therefore, if they choose the ambiguous prospect, they'll find out that the winning probabilities took a bad turn and are on the lower side of their expectations. But such people do not necessarily believe that they are always unlucky. Thus the doctor is ambiguity averse, but as she is facing many similar cases, her aversion to each case may diminish. Furthermore, this may lead her to prefer the new treatment over the standard one.====In this paper we formalize this discussion. Suppose that the doctor has to make a decision for ==== (identical) people. Denote ==== repetitions of the standard treatment by ==== and of the new treatment by ====, where both yield the sum of the outcomes of the respective treatments. Suppose that both are better than no treatment. We show that under some conditions, and for sufficiently large ====, ====. That is, ==== repetitions of the new treatment are, eventually, preferred over ==== repetitions of the standard treatment (Theorem 1, Theorem 3).====Next consider an alternative scenario involving, again, a patient and a doctor. This time, avoiding treatment does not lead to a bad outcome but may be costly, and only the ambiguous treatment ==== is available. Suppose that the probabilistic lottery ====, yielding the outcomes of ==== with the probabilities ====, has a positive expected value and that ==== and all its repetitions ==== are preferred to no treatment, no matter how small is its cost, while no treatment is preferred to ====. We show that under some conditions, ==== repetitions of the ambiguous treatment are eventually preferred to no treatment (Theorem 2).====Should society encourage, maybe even enforce, the use of the ambiguous treatment? Patients may be willing to pay the extra price for the unambiguous treatment if it exists, or to bear the cost of no treatment if an alternative treatment does not exist. But if society adopts the point of view of social planners and care takers (even if they do not have any better information), then it may opt out for the ambiguous treatment. Providing general answers to such questions is beyond the scope of the current paper but our aim is to show that, at least under some conditions, such questions are not meaningless.====Theorem 1, Theorem 2 of Section 3 analyze Choquet expected utility preferences (Schmeidler, 1989). Under some conditions, similar results hold in the smooth recursive utility model (Klibanoff et al., 2005), but under some other conditions they do not hold (Theorem 3 in Section 4). On the other hand, in the maxmin expected utility model (Gilboa and Schmeidler, 1989) similar results hold only under some extreme conditions (Theorem 4 in Section 5). We discuss some further issues and the literature in Section 6. All claims are proved in the appendixes.====Our analysis requires us to compare ambiguous acts with probabilistic lotteries. In the various models discussed in the paper probabilistic lotteries are evaluated by expected utility functionals, and require the proofs of some results which may be of independent interest. These results are grouped in Appendix B. Conclusion 1 highlights the importance of the degree of absolute risk aversion at −∞ for the analysis of compound gambles as ==== becomes large.",A lot of ambiguity,https://www.sciencedirect.com/science/article/pii/S0022053121002106,26 November 2021,2021,Research Article,76.0
"Giordani Paolo E.,Mariani Fabio","Department of Economics and Finance and School of European Political Economy, LUISS University, Italy,IRES/LIDAM, UCLouvain, Belgium,IZA, Bonn, Germany","Received 10 February 2020, Revised 12 November 2021, Accepted 16 November 2021, Available online 26 November 2021, Version of Record 1 December 2021.",https://doi.org/10.1016/j.jet.2021.105385,Cited by (0),"This paper provides a rationale for the revival of protectionism, based on the rise of the educated class. In a trade model with heterogeneous workers and entrepreneurs, globalization generates aggregate gains but has distributional effects, which can be attenuated through taxation. By playing a two-stage political game, citizens decide on ==== and the extent of redistribution. In this setting, trade liberalization is politically viable as long as the losers from trade are compensated through the redistributive mechanism. When skilled workers account for a large share of the population, however, there may be limited political support for redistribution, and those who are left behind by globalization – namely unskilled workers and importing-sector entrepreneurs – can form a coalition to impose protectionist measures. We then build a dynamic version of the model, where redistribution promotes social mobility. Our analysis suggests that globalization – by favoring the ascent of the educated class and thus eroding the political support for redistribution – may ultimately breed its own decline.","This paper relates the recent revival of protectionism observed in Western democracies to the rise of the educated class. We argue that the (endogenous) process of human capital accumulation, by eroding the political support for redistribution, may increase the demand for protectionism – if trade openness deepens inequality. As a result, modern societies tend to become progressively more inclined to “empty the baby out with the bath”, thus resisting globalization in spite of its possible beneficial effects.====As pointed out by Zeira (2021), the educated class has emerged, over the last few decades, as one of the major winners from the globalization process. For this reason, higher-educated voters have encouraged trade openness and tolerated the gradual rise in inequality. On the other hand, a non-negligible share of the “working class” has seen its status deteriorate with globalization and, in the absence of an adequate redistribution of the gains from trade, has drifted – together with other losers from globalization – towards a protectionist political stance. The idea that trade may bring about differential effects on the political attitudes of voters with different levels of education has received empirical support by Aksoy et al. (2018). In addition, Piketty (2018) observes that the progressive advancement of globalization and expansion of education may have substantially altered the nature of political competition, reducing the salience of previous class-based redistributive conflict in favor of new cleavages.====To rationalize this process, we first build a trade model in which the international exchange of goods generates aggregate gains but has distributional effects across workers and firms: while skilled workers and exporting-sector entrepreneurs benefit from globalization, unskilled workers and importing-sector entrepreneurs lose. Trade can thus exacerbate both “between-skill” and “between-industry” inequality, as highlighted by Grossman et al. (2017). Inequality, however, can be attenuated through taxation – by redistributing the gains from trade and thus making globalization Pareto-improving ====. Citizens play a two-stage political game, and decide by majority voting on both (the degree of) trade openness and redistribution. In this setting, an increase in the proportion of skilled workers weakens the political support for redistribution, as the median voter on taxation becomes wealthier. Therefore, the lack of redistribution prevents trade from being beneficial for all, and fuels the political opposition against globalization, with the losers from trade forming a protectionist coalition.====A dynamic extension of our model, built around an endogenous mechanism of social mobility, further reveals that globalization may breed its own decline. If human capital accumulation depends on public education, a high level of redistribution – which makes globalization politically viable in the first place – also drives an increase in the share of skilled workers. Eventually, however, the rise of the educated class weakens the political support for redistribution and thus favors the emergence of protectionist policies.====Before moving on, let us stress that any economic process susceptible – like international trade – of bringing about aggregate gains, while inducing distributional effects, may be opposed and potentially slowed down by the losers. One may think, for instance, of skill-biased technological progress that, according to Blanchard and Willmann (2018) among others, may bring about the same political conflict as globalization. Different from trade, however, skill-biased technological progress cannot be easily resisted (or reversed) by voting – and this may also explain why the former can be used as a scapegoat of the latter, as pointed out by Rodrik (2018).====It is also important to clarify that, throughout this paper, we look at trade openness as the main aspect of globalization and abstract from the international mobility of workers. In reality, the growing importance of international migration might also explain, at least partially, the change in political attitudes toward globalization – although this view receives only limited support from the empirical literature.==== Another dimension of globalization that could drive an increase in income inequality, but remains beyond the scope of our paper, is the internationalization of technological competition, as stressed by Cozzi and Impullitti (2016).",Unintended consequences: Can the rise of the educated class explain the revival of protectionism?,https://www.sciencedirect.com/science/article/pii/S0022053121002027,26 November 2021,2021,Research Article,77.0
"Frick Mira,Iijima Ryota,Le Yaouanq Yves","Department of Economics, Yale University, P.O. Box 208281, New Haven, CT 06520-8281, United States of America,CREST, Ecole Polytechnique, IP Paris, 5 Avenue Henry Le Chatelier, TSA 96642, 91764 Palaiseau Cedex, France,LMU Munich, Germany,CEPR","Received 26 July 2020, Revised 2 November 2021, Accepted 21 November 2021, Available online 25 November 2021, Version of Record 8 December 2021.",https://doi.org/10.1016/j.jet.2021.105394,Cited by (6),"We show how incorporating ===='s (====) notion of objective rationality into the ====-MEU model of choice under ambiguity can overcome several challenges faced by the baseline model without objective rationality. The decision-maker (DM) has a ==== preference ====, which captures the complete ranking over acts the DM expresses when forced to make a choice; in addition, we endow the DM with a (possibly incomplete) ==== preference ====, which captures the rankings the DM deems uncontroversial. Under the ====-MEU model, ==== has an ====-MEU representation and ==== has a unanimity representation à la ====, where both representations feature the same utility index and set of beliefs. While the axiomatic foundations of the baseline ====-MEU model, while we show that, for the baseline model, standard updating rules can be ill-defined.","A widely used model of choice under ambiguity is the ====-maxmin expected utility (====-MEU) criterion, dating back to Hurwicz (1951). This criterion represents a decision-maker's (DM's) preference ==== over (Anscombe-Aumann) acts ==== by considering the weighted average of each act's worst-case and best-case expected utility,==== according to some weight ====, closed and convex set ==== of beliefs over states, and nonconstant and affine utility ==== over outcomes. Unlike Gilboa and Schmeidler's (1989) maxmin expected utility criterion (i.e., the special case when ====), the general ====-MEU model does not assume that the DM is uncertainty-averse (Schmeidler, 1989). Instead, in line with various experimental evidence (see the survey by Trautmann and van de Kuilen, 2015), (1) allows the DM to display a mix of ambiguity-averse and ambiguity-seeking tendencies, and the weight ==== and set of beliefs ==== are often interpreted as simple parameterizations of the DM's ambiguity attitude and perception of ambiguity, respectively. This has contributed to the model's popularity in applied work, which has employed ====-MEU representations in both static and dynamic settings.====Despite its popularity, the foundations of the ====-MEU model are still not fully understood. In this paper, we point to several challenges that arise in the standard domain of preferences over acts, and show how incorporating the notion of objective rationality (Gilboa et al., 2010, henceforth, GMMS) into the model can overcome these challenges.====In Section 3, we begin by highlighting three main challenges in the standard domain. First, there is no known fully general axiomatic characterization of ====-MEU in terms of the DM's preference ==== over acts (Section 1.1 discusses existing work). The remaining two challenges are more fundamental. Second, as is well-known, the preference ==== does not uniquely identify ==== and ====, complicating the interpretation of these parameters as capturing the DM's ambiguity attitude and perception: In Proposition 1, we fully characterize the extent of multiplicity (building on Siniscalchi, 2006). Third, as we show in Example 1, the lack of identification of the model parameters creates the following problem for dynamic extensions of ====-MEU: Common belief-updating rules, such as prior-by-prior Bayesian updating of all beliefs in ====, are ill-defined at the level of preferences, as different representations of the same ex-ante preference ==== may give rise to different updated preferences.====Motivated by these challenges, we consider the following ==== model. We interpret ==== as the DM's ==== preference, which captures the complete ranking the DM expresses when forced to choose between any two acts. In addition, as in GMMS, we endow the DM with a (possibly incomplete) ==== preference ====, which models the rankings that appear uncontroversial to the DM. We then consider a joint representation of ==== and ====, where for some utility ====, set of beliefs ====, and weight ====:====In Section 4, we address the aforementioned challenges using the objectively founded ====-MEU model. We first show that this model admits a simple axiomatic characterization (Theorem 1). We impose Bewley's (2002) axioms on the objectively rational preference; that is, ==== satisfies all subjective expected utility axioms, except that completeness is only assumed for the ranking over constant acts. The subjectively rational preference is required to be invariant biseparable (Ghirardato et al., 2004); that is, ==== satisfies all subjective expected utility axioms, except that independence is only imposed for mixtures with constant acts. The final and key axiom, security-potential dominance, disciplines the completion rule from ==== to ====: We require the DM to subjectively prefer act ==== to act ==== whenever ==== is both “more secure” than ==== and has “more potential” than ====, where security and potential are defined in terms of the objective ranking against certain prospects.====Second, in contrast with the baseline model without objective rationality, the parameters ==== and ==== in Theorem 1 are uniquely identified. Thus, the interpretation of ==== and ==== as the DM's ambiguity attitude and perception is behaviorally founded, making it possible to conduct comparative statics of these parameters (Section 4.2).====Third, in contrast with Example 1, we show that prior-by-prior Bayesian updating of the objectively founded ====-MEU model admits well-defined preference foundations. Suppose the DM's ex-ante subjective and objective preferences have an objectively founded ====-MEU representation ====. Upon learning that the state of the world is contained in some event ====, the DM forms a conditional subjective preference ====. Theorem 2 characterizes when ==== admits an ====-MEU representation whose utility is ==== and whose set of beliefs ==== is derived from the unique ex-ante belief set ==== by prior-by-prior Bayesian updating. The key axiom imposes an intertemporal analog of security-potential dominance on the relationship between the ex-ante objective preference and the conditional subjective preference.====The contribution of Section 4 is not primarily technical (the results admit relatively simple proofs), but rather, to illustrate the methodological value of the objective rationality framework in shedding light on the ====-MEU model. As we discuss in Section 5, our approach is not restricted to ====-MEU. Indeed, we show that security-potential dominance characterizes linear completion rules for a broader class of incomplete preferences ==== beyond Bewley preferences. Just as for ====-MEU, this makes it possible to provide foundations and characterize belief updating for several other representations that are difficult to analyze based on the subjectively rational preference ==== alone.",Objective rationality foundations for (dynamic) ,https://www.sciencedirect.com/science/article/pii/S0022053121002118,25 November 2021,2021,Research Article,78.0
Zhou Hang,"School of Finance, Shanghai University of Finance and Economics, China","Received 30 November 2020, Revised 29 October 2021, Accepted 8 November 2021, Available online 12 November 2021, Version of Record 18 November 2021.",https://doi.org/10.1016/j.jet.2021.105384,Cited by (1),"This paper investigates the effect of strategic reasoning on financial markets with a level-==== thinking framework. A level-==== speculator performs ==== rounds of iterative reasoning to infer information from asset prices. In contrast to the static rational expectations equilibrium, the level-==== framework produces a unified theory of momentum and contrarian trading strategies. Besides, this paper discusses how the distribution of sophistication levels affects several market variables and it sheds new light on empirical patterns such as: (1) overreaction of asset prices, (2) the excess volatility puzzle, and (3) the excessive trading volume puzzle. Moreover, this paper explores whether the level-==== strategy converges to the rational expectations equilibrium.","Prominent finance professionals have embraced the idea that successful active investing requires thinking one step ahead of the market. For instance, Howard Marks, co-founder of one of the largest hedge funds investing in distressed securities, argues that “second-level thinking” is crucial for active investors to earn an excess return (Marks, 2011). He argues that securities are mispriced if traders use their simplistic and superficial ‘first-level thinking”. “Second-level thinkers” who understands their mistakes would exploit them and earn an abnormal return.====He offers one story to illustrate. When a company reports good news on future profit, first-level thinkers will buy its stock based solely on the good news. However, a more sophisticated second-level thinker would reason in the following way: if everyone is buying solely based on the good news, then good news turns bad as it leads to an overvaluation of the stock, making buying a bad decision.====In this story, two features characterize sophisticated active investors. First, they engage in iterated reasoning. Despite using their own information, they also reason about how others use the information they have. In particular, sophisticated traders think about how prices reveal the private information of others. Second, there is a hierarchy of sophistication levels in the minds of active investors. Namely, they believe that they outsmart other traders in the market by thinking one step ahead.====Market microstructure models fail to capture these two features because this approach constructs the rational expectations equilibrium (REE henceforth) as the Nash equilibrium. Intuitively, Nash equilibrium assumes that players are equally sophisticated as they seek the best response simultaneously. Therefore, players’ hubris and the behavior of thinking “one step ahead” are not captured in the Nash equilibrium. REE has additional drawbacks. For instance, the observed enormous trading volume is not compatible with the rational expectations models. Campbell (2017) argues that it is hard “for those models [the REE models] to generate trading volume comparable to what is empirically observed without assuming highly volatile supply shocks that are exogenous and unexplained.”====Behavioral game theorists have developed non-equilibrium models that capture the aforementioned features and outperform the Nash equilibrium in terms of predicting human behaviors in strategic environments. Among those theories, the level-==== thinking model assumes that players adjust their strategies through iterated best responses to less sophisticated strategies (Crawford et al., 2013). In this model, the rounds of reasoning represent the levels of strategic sophistication.====Empirical research finds that smart traders such as hedge fund managers have constantly profited from the behavioral biases of retail investors.==== Here is an illustration of the level-==== behavior in financial markets. A naive trader tends to buy a stock based on a good recent performance. A sophisticated investor with market power could exploit this bias by deliberately buying a stock and creating a price momentum to attract the naive traders. A more sophisticated trader understands how the game is played and exploits this opportunity by short-selling the stock before its price falls.====The level-==== model is naturally related to algorithmic trading. Algorithmic trading can be viewed as solving the best response given how other algorithms in the market work. An algorithm tries to identify a market anomaly and then it designs strategies to exploit this opportunity. This process, however, does not involve solving the mutual best responses. Moreover, we can view the evolution of algorithms as iterated best responses, which is the intuition behind the level-==== model.====Although level-==== reasoning seems to figure prominently in decision rules of finance practitioners, a level-==== theory of speculative trading is missing. This paper contributes to the literature by filling that gap. To be more concrete, I incorporate level-==== reasoning into a non-competitive financial market in the spirit of Kyle (1989).====The level-==== model introduces levels of strategic sophistication to Kyle (1989). I consider naive behavior as ignoring the information contained in asset prices for this trading game. Therefore, naive traders, i.e., level-0 speculators, design demand schedules conditioned on their private information only. Sophisticated speculators choose their trading strategies through iterated best responses.====First, the level-==== model provides a unified theory for momentum and contrarian strategies. In this paper, I define a momentum (resp. contrarian) strategy as an upward (resp. downward) sloping demand function.==== My definition of momentum strategy is conceptually consistent with the “market positive feedback strategy” as in De Long et al. (1990). The upward-sloping demand function preserves the positive feedback nature of a momentum strategy because asset positions and prices move in the same direction. I show that the level-1 speculators follow momentum strategies if they perceive prices as informative enough. Their reasoning is this: if the market-clearing price ends up high, the fundamental value of the asset should exceed its price because the naive speculators fail to infer others' information from prices. This reasoning process justifies their momentum strategies.====As for contrarian strategies, my definition keeps the negative feedback feature. I show that the naive strategy, consistent with the previous notion of naivety, is always contrarian. Besides, level-2 speculators also are contrarians: A level-2 speculator anticipates that an asset will be overvalued if most traders are speculating with momentum strategies. Therefore, their contrarian strategies exploit the profit by short-selling overvalued assets. The level-==== model thus captures the intuition in Warren Buffett's quote above. A sophisticated speculator should always reason one step further than the market and, possibly, take a contrary position. In terms of expected profits, I show that, on average, speculators on the minority side of the market make positive profits.====Second, I study the market implications when the distribution of sophistication levels varies. I show that the distribution of levels has an impact on market depth, the valuation of assets, price volatility, and expected trading volume. My results emphasize the “market destabilizing” role of the momentum strategy. I show that increasing the fraction of momentum speculators reduces market depth, and this explains empirical patterns such as the overshooting of asset prices, excess price volatility, and excessive trading volume.====Regarding asset valuation, my results show that in a market with sufficient momentum speculators, assets will be overvalued if the realized fundamental values beat the expectations, and they will be undervalued when the realized fundamental values miss the expectations. In particular, I show there are distributions of levels such that market depth is arbitrarily small, which leads to unbounded price volatility and trading volume.====The assumption that each trader thinks she is smarter than the market may sound unrealistic. However, as we can interpret the Kyle model, as one speculator trades against the average trader in the market, the hubris assumption implies that active investors think they outsmart the average trader. Given that this paper models speculative trading, this assumption might sound more realistic than it seems at first glance.====Besides, this paper contributes to the models of disagreement in the finance literature.==== To the best of my knowledge, this paper is the first to introduce depth of reasoning as the main mechanism for generating such disagreement. Finally, this paper studies strategic trading when common knowledge of private information is relaxed. A level-==== trader is consistent with ====-th order mutual knowledge of private information.====The structure of the rest of this paper is as follows. Section 2 formally introduces the level-==== model. Section 3 shows how the level-==== model unifies momentum and contrarian trade and other properties. Section 4 discusses some market implications of the level-==== model. Section 5 discusses related literature. Section 6 offers concluding remarks.",Informed speculation with k-level reasoning,https://www.sciencedirect.com/science/article/pii/S0022053121002015,12 November 2021,2021,Research Article,79.0
"Loertscher Simon,Muir Ellen V.,Taylor Peter G.","Department of Economics, Level 4, FBE Building, 111 Barry Street, University of Melbourne, Victoria 3010, Australia,Department of Economics, Stanford University, United States of America,School of Mathematics and Statistics, University of Melbourne, Melbourne, Australia","Received 3 July 2020, Revised 24 October 2021, Accepted 1 November 2021, Available online 10 November 2021, Version of Record 18 November 2021.",https://doi.org/10.1016/j.jet.2021.105383,Cited by (7),"Traders that arrive over time give rise to a dynamic tradeoff between the benefits of increasing gains from trade by accumulating traders and the associated cost of delay due to discounting. We analyze this tradeoff in a dynamic bilateral trade model in which a buyer and seller arrive in each period and draw their types independently from commonly known distributions. With symmetric binary types, the optimal market clearing policy can be implemented with posted prices and ex post budget balance, provided that it is optimal to store at least one trader. While optimally thick markets involve storing a small number of traders, their performance is nevertheless close to that of a large market. In particular, irrespective of the type distributions, two-thirds of the gains from increased market thickness can be achieved by storing just one trader.","This paper determines the optimal degree of market thickness in an infinite horizon model in which one buyer and one seller arrive in every period and develops a measure of market thickness that permits comparisons across different environments, thereby contributing to the nascent field of dynamic market making. Assuming that each buyer's value and each seller's cost is persistent, we consider a social planner whose objective is to maximize expected discounted social surplus. The basic tradeoff the planner faces is that increasing market thickness by storing traders increases welfare in future periods through increased gains from trade. However, this comes at the cost of delaying consumption.====The key forces at work are most transparent and tractable in a symmetric setting with binary types, where each trader's type is either ==== or ====. Trades between efficient types maximize the gains from trade, while trades between efficient and suboptimal types achieve less than half of this maximum. There are no gains from trade between suboptimal types. The optimal market clearing policy in this setting is very intuitive. Suboptimal types should never be stored as the new arrivals in the future can be no worse than these types. Likewise, it is optimal to execute any trade between efficient types as soon as it becomes available. Hence, the problem reduces to deciding when to forgo a trade between an efficient trader and a suboptimal trader and store the efficient trader. We refer to the optimal policy as a threshold policy, as it involves storing a number of efficient traders up to a ==== and not executing any suboptimal trades until this threshold is reached.====The optimal storage threshold is increasing in the discount factor ====. Provided it is positive, the optimal market clearing policy can be implemented using posted prices. Until the storage threshold is reached, the only trades that are executed are trades between efficient buyers and sellers, which can be implemented by simply posting a price equal to the average of their types. If the arriving buyer and seller are willing to trade at this price, an efficient trade is executed. If only one of these agents wants to trade, thereby revealing that its type is efficient, then this agent is either stored or it trades with a stored agent. Once the storage threshold is reached, a trade between an efficient and a suboptimal type is executed if the arriving agents consist of an efficient type of the kind stored (say, a seller) and a suboptimal type on the other side of the market (a buyer). Importantly, such a trade can still be implemented via a posted price by ensuring that this price is sufficiently favorable for the side of the market from which no agents are stored. In contrast, a posted-price implementation is not possible with a storage threshold of zero. In this case, the efficient policy induces ex post efficient trade in every period and the market-clearing prices depend on the arriving agents' types on both sides of the market.====The posted-price implementation shows that, whenever the optimal storage threshold is positive, the optimal market clearing policy can be implemented regardless of whether arriving agents are privately informed of their types. That is, there is no Myerson and Satterthwaite (1983) problem. Intuitively, with private information and a storage threshold of zero, implementing the ex post efficient allocation in every period requires that arriving trades involving efficient types are always executed. Identifying the efficient types requires that the planner pays an information rent to these types on ==== sides of the market in every period, potentially resulting in a deficit. With a positive storage threshold, suboptimal types only trade with positive probability in periods where the storage threshold has been reached and traders are stored on the ==== side of the market. Hence, the social planner pays an information rent to efficient types on ==== side of the market in any given period.====We show analytically and numerically that optimally thick markets typically involve storing a small number of traders. Specifically, as the discount factor ==== increases, the optimal storage threshold grows slowly, at a rate that is bounded by ====. Moreover, for a wide range of plausible parameterizations, the optimal storage threshold is a single digit. This raises the question of why optimal storage thresholds are typically small and optimally thick markets are seemingly thin. Addressing this question requires a quantitative measure of market thickness. The optimal threshold, while a natural candidate, is of limited use as it is specific to this dynamic setting and only offers an ordinal measure.====With this in mind, we propose a cardinal measure of market thickness. For each storage threshold we consider the average surplus per period under the stationary distribution, which is the standard welfare criterion for a perfectly patient social planner. The maximum gain from increasing market thickness is the difference between the average per-period surplus with an unbounded storage threshold and a storage threshold of zero.==== Our market thickness measure determines the proportion of this maximum gain that is achieved by a given threshold policy.==== This measure is independent of the discount factor, allowing us to meaningfully quantify how the discount factor affects the ==== degree of market thickness. According to our market thickness measure, a threshold policy with a threshold of one always achieves two-thirds of the maximum gain from increasing market thickness, and a threshold policy with a threshold of six achieves more than 90 percent of this maximum gain.====Many of our results related to threshold policies extend to continuous type distributions. Exploiting the static benchmark with a continuum of traders on each side of the market, buyers and sellers can still be categorized as “efficient” or “suboptimal” depending on which side of the Walrasian price their types are. Absent private information, two-thirds of the maximum gain from increasing market thickness is still achieved using a threshold policy with a threshold of one. Accounting for private information, we show that this gain is even larger. Posting the Walrasian price in any given period still allows the planner to execute trades between efficient agents. When the storage threshold is reached and a trade involving a suboptimal type may need to be executed, a second-best mechanism is required to determine whether there are any gains from trade.====In the final section of the paper we also consider the case of a profit-maximizing market maker and show that such a market maker induces an excessively thick market relative to the benevolent social planner. While many important questions (such as the nature of optimal mechanisms away from the binary type setting) remain open, this paper provides a starting point for studying efficient and profit-maximizing market making in dynamic setups.====Our paper relates to two strands of literature. The first includes Gresik and Satterthwaite (1989), Satterthwaite and Williams, 1989, Satterthwaite and Williams, 2002, McAfee (1992), Rustichini et al. (1994), and Cripps and Swinkels (2006) and analyzes double auctions in static settings. Motivated by the impossibility results of Vickrey (1961), Hurwicz (1972) and Myerson and Satterthwaite (1983), this literature has studied how quickly inefficiencies arising from private information vanish as the numbers of buyers and sellers increase. Our paper relates to this literature by providing a dynamic perspective, showing that benefits from increasing market thickness accrue even absent incentive problems, and by providing a measure of market thickness that is applicable across different mechanisms and environments.====The second strand is the recent literature on dynamic matching and mechanism design. The notions of periodic ex post incentive compatibility and individual rationality used in our mechanism design analysis were introduced by Bergemann and Välimäki (2010). We study a problem with a dynamic population of agents with persistent types, as do Parkes and Singh (2003), Gershkov and Moldovanu (2010) and Board and Skrzypacz (2016). Our contribution relative to these papers is that our focus is on the structure of the optimal allocation rule and deriving the optimal degree of market thickness. Within the dynamic matching literature, our paper is closest to Baccara et al. (2020), who, motivated by the problem of matching children and parents in an adoption “market,” consider a dynamic, two-sided matching problem. We adhere to the standard assumptions in the dynamic mechanism design literature of geometric discounting and quasilinear payoffs,==== which allows us to study a broad range of alternative questions and setups. The tradeoff between the benefits of increasing market thickness and the cost of delay has also been studied by Akbarpour et al. (2020) who, building on Ünver (2010) and Anderson et al. (2017), study efficiency in a dynamic matching model in which exchange possibilities have a network structure. This tradeoff also naturally arises in many market microstructure models (see, among other others, Vayanos (1999), Rostek and Weretka (2015) and Du and Zhu (2017)). Our paper complements this literature by deriving the optimal trading mechanism.====The remainder of this paper is organized as follows. Section 2 introduces the model. In Section 3 we characterize the optimal market clearing policy and optimal market thickness under a social planner with complete information. Section 4 deals with implementation when types are traders' private information. Section 5 concludes the paper. All proofs are provided in the appendix.",Optimal market thickness,https://www.sciencedirect.com/science/article/pii/S0022053121002003,10 November 2021,2021,Research Article,80.0
Li Qi,"Department of Economics, The Pennsylvania State University, 201 Old Main, University Park, PA 16802, United States of America","Received 13 April 2020, Revised 26 October 2021, Accepted 30 October 2021, Available online 9 November 2021, Version of Record 16 November 2021.",https://doi.org/10.1016/j.jet.2021.105381,Cited by (0),"This paper studies security design with adverse selection when verifiable retention is impossible due to market segmentation and price opacity across market segments. Rather than signaling through retention, sellers in the model signal quality through posted prices, which is feasible because the posted price affects buyers' search behavior and the equilibrium probability of selling. The optimal security design, in this case, is to break up the cash flow of an asset into several debt securities of increasing seniority. The size of senior relative to subordinated debts is affected by the equilibrium markup. Search frictions, as determinants of the markup, shape the endogenous creation of financial securities.","The literature on security design with adverse selection often focuses on verifiable retention and its signaling effect.==== Specifically, it is often assumed that a seller can promise to retain a state-contingent cash flow of his asset (a residual tranche, hereafter), and, more importantly, the seller's promise of retention can be verified by buyers. Since sellers with a worse asset typically incur a larger cost to keep a residual tranche, the size and shape of the residual tranche can, therefore, be used by buyers to infer asset quality. What is less known in the literature is the alternative scenario where buyers cannot verify retention. This paper, by focusing on security design without verifiable retention, is a step to fill the gap.====The lack of verifiable retention is a plausible characterization of many financial markets, in particular, the market for residential mortgage-backed securities (RMBS) and collateralized debt obligations (CDO). While verifiable retention may be consistent with reality in the corporate debt business, the evidence does not support that sellers can commit to retaining residual tranches in RMBS or CDO sectors. The offering process of RMBS and CDO is distinct from the traditional corporate syndication process where securities are sold on a predetermined day. With RMBS and CDO, primary market trades can occur over time, so sellers can hardly assure early buyers that residual tranches will not be sold later. Selling residual tranches is also facilitated by the creation of bankruptcy-remote special purpose vehicles in securitization processes. In practice, numerous examples exist where residual tranches of RMBS or CDO were sold, mostly to hedge funds.==== Not only is the sale of residual tranches rampant, it is also hard to detect since buyers do not observe sellers' holdings of individual securities.==== These features of RMBS and CDO sectors support my assumption of no verifiable retention.====In my model, the lack of verifiability is a result of market segmentation and price opacity across segments, which allow a seller to market each security, including the residual tranche, to a segment of buyers without being detected by the rest. The introduction of price opacity is motivated by the fact that in RMBS and CDO sectors, price quotes are often communicated bilaterally rather than publicly. With bilateral communication, a seller does not have to truthfully report the price he posts to a segment of buyers (say, hedge funds interested in high-risk securities) to another segment of buyers (say, pension funds interested in low-risk securities).====This paper explores market segmentation in the context of competitive search. In my model, each seller has one of two assets that differ in cash flow distribution. From the cash flow of his asset, a seller can design (at most) ==== securities and a residual tranche of arbitrary (weakly increasing) shapes. Each security, including the residual tranche, can be presented to a segment of uninformed buyers along with an ultimatum price. Buyers decide whether to search and for which type of contract. The selling probability is determined by demand over supply (also known as market tightness) through a matching function: a larger demand per unit of supply implies a higher probability of selling.====Since buyers cannot verify retention, sellers in equilibrium always attempt to sell the residual tranche. Still, non-trading arises out of rationing. In equilibrium, the selling probability is affected by sellers' choice of securities and prices. For a given security, good sellers (sellers with a better asset) will post a higher price and trade with a lower probability vis-a-vis bad sellers. Intuitively, rationing has to be more severe when the posted price is higher so that bad sellers do not overprice their securities. Good sellers self-select into posting a higher price because holding the security is less costly to them. In this way, prices help sellers to signal quality even if signaling through retention is unavailable.====The equilibrium explored in the paper features a surprisingly rich security structure: a set of ==== increasingly senior debt securities (“multitiered debts”). Each debt security is characterized by two (possibly degenerated) flat regions: the debt security pays zero when the cash flow is too low and a fixed amount when the cash flow is sufficiently high. Between the flat regions, payment from the debt increases 1-to-1 with the cash flow. (See Fig. 1 for an illustration when ====.) The security structure predicted by my model is consistent with market reality: issuing multitiered debts is prevalent in RMBS and CDO sectors.====The reason for issuing multitiered debts can be better understood if we discretize the cash flow state into a set of ==== points, with ====. We can split the cash flow into ==== increasingly senior debts that positively span the space of weakly increasing securities (“basis debts”). The security design problem boils down to sorting ==== basis debts into ==== bundles. Sellers want to bundle basis debts with similar information sensitivity because doing so alleviates rationing. In general, bundling basis debts reduces trade because it makes signaling harder by forcing sellers to sell each component with the same probability. Bundling basis debts of similar information sensitivity minimizes the reduction in trade because the selling probabilities of these debts are similar in the first place. When cash flow distributions are hazard rate ordered, the information sensitivity of basis debts monotonically decreases with its seniority. Thus, basis debts with consecutive seniority are grouped together, which again forms a set of increasingly senior debt securities.====The paper also speaks to the endogenous creation of senior debt through tranching. In the model, a reduction of markup makes signaling informationally-sensitive securities relatively easier because it reduces bad sellers' incentive to imitate. Consequently, sellers facing a low-markup environment will move marginal cash flow from a senior debt to a subordinated debt, as the latter is now easier to sell. Through this channel, economic shocks and policies that change the equilibrium markup affect the composition and the quality of primary markets. For example, a decline in aggregate matching efficiency that lowers markup will reduce the size of senior debts and, at the same time, improve their quality.====The remainder of this paper proceeds as follows. Section 2 discusses the related literature. Section 3 develops a model of security design with adverse selection and market segmentation. Section 4 demonstrates that the issuance of multitiered debt securities arises endogenously in equilibrium. Section 5 analyzes the effect of markup and search frictions on optimal security design. Section 6 discusses price transparency and endogenous market segmentation. Section 7 concludes.",Security design without verifiable retention,https://www.sciencedirect.com/science/article/pii/S0022053121001988,9 November 2021,2021,Research Article,81.0
"Choi Kyoung Jin,Jeon Junkee,Koo Hyeng Keun","Haskayne School of Business, University of Calgary, Canada,Department of Applied Mathematics, Kyung Hee University, Republic of Korea,School of Business, Ajou University, Republic of Korea","Received 29 January 2020, Revised 22 September 2021, Accepted 26 October 2021, Available online 8 November 2021, Version of Record 16 November 2021.",https://doi.org/10.1016/j.jet.2021.105380,Cited by (3),"We study the consumption and portfolio selection problem of economic agents who face ====: there is disutility from changing consumption levels. The derived preference exhibits ==== toward a consumption gamble with the previous consumption level being the reference point. The optimization problem involves a non-monotonic and non-concave utility function. We derive a closed-form solution by combining a duality method and the super-contact principle. We show that the consumption policy involves an inaction interval for the consumption-permanent income ratio, which are consistent with various empirical regularities about consumption. The effective risk aversion derived from agents' optimal portfolio choice exhibits an inverted U-shape in the inaction interval.","In this paper, we examine a model of intertemporal preference with loss aversion. We develop the model by introducing disutility (utility adjustment cost) from changing consumption levels. Disutility implies an ==== of consumption decisions in the following sense: if an agent increases or decreases consumption now, she cannot reverse the decision in the future without incurring ====. The utility costs may be attributable to the costs of learning (Brunnermeier, 2004), or the costs of information acquisition and processing (Sims (2003) and Reis (2006)).==== We solve a continuous-time consumption and investment problem in closed form. Optimal consumption induced by irreversibility exhibits excess smoothness, excess sensitivity, and the disappearance of both excess smoothness and excess sensitivity for large shocks, MPC (marginal propensity to consume) heterogeneity, and asymmetric sensitivities to positive and negative income shocks, consistent with empirical evidence. The investment behavior exhibits a ====-shaped relationship between risk aversion and wealth, and hence, time-varying risk aversion.====Consumption irreversibility is closely related to loss aversion. The utility function can be expressed as two equivalent forms: one with consumption irreversibility and the other with ==== (ILA). The equivalence is established by rewriting the agent's utility function with respect to the changes in consumption. It is necessary to use the first form for the solution analysis. The second is conceptually more useful when we discuss implications. The second form shows that the preference displays loss aversion toward a consumption change with the previous consumption level being the reference point. Note that the utility function representing the preference exhibits a minimal departure from the canonical time-separable utility function. Our model is perhaps the simplest among those displaying loss aversion toward a consumption gamble (Bowman et al. (1999), Kösgezi and Rabin, 2006, Kösgezi and Rabin, 2007, Kösgezi and Rabin, 2009, and Pagel (2017)), as the previous level of consumption is conceptually the simplest possible reference point.====Technically, the irreversibility or existence of adjustment costs makes the utility function ==== and ==== in consumption====; the optimization problem involving such an objective function is non-trivial. To tackle this difficulty, we first transform the original problem to a dual problem, using a martingale approach.==== Then, the dual problem is eventually reduced to a solvable ====: it can be properly handled by the super-contact principle often used for analyzing a firm's investment decisions (e.g., Abel and Eberly (1996) and Dixit and Pindyck (1998)). We provide optimal policies in closed form with a full verification of their optimality. The optimal policies are determined by three preference parameters for subjective discounting, risk aversion, and ILA. In particular, we investigate how ILA affects the optimal policies, and disentangle the effects of risk aversion and those of ILA.====The optimal consumption policy involves inaction, because of adjustment costs (Fig. 1). More precisely, it is characterized by two numbers, say ==== and ==== (with ====). When the ratio ==== of the previous consumption level to wealth, which we call the ====, is inside ====, called the ====, consumption is not adjusted. When the ratio is outside the interval, consumption is adjusted immediately so that the ratio is restored to the nearest boundary of the interval. The inaction interval becomes wider as ILA increases, i.e., the agent adjusts consumption less frequently as ILA increases.====There exist empirical facts about consumption that are regarded as puzzles: (i) excess smoothness (Deaton (1987), Campbell and Deaton (1989), and Jappelli and Pistaferri (2010)) (ii) excess sensitivity (Flavin (1981), Jappelli and Pistaferri (2010), and Fuch-Schündeln and Hassan (2016)) and (iii) disappearance of both smoothness and sensitivity with large wealth or income shocks (magnitude hypothesis (Browning and Collado (2001), Browning and Crossley (2001), Jappelli and Pistaferri (2010)), Scholnick (2013), Fuch-Schündeln and Hassan (2016)) (iv) MPC heterogeneity (Jappelli and Pistaferri (2014), Arellano et al. (2017), Christelis et al., 2019, Christelis et al., 2020, and Fuster et al. (2021)) and (v) the asymmetric sensitivity to positive and negative income changes (Shea (1995), Bowman et al. (1999), Jappelli and Pistaferri (2014), Bunn et al. (2018), Christelis et al. (2019), and Fuster et al. (2021)) (vi) a hump-shaped consumption pattern over the life cycle (Gourinchas and Parker (2002) and Fernández-Villaverde and Krueger (2007)) (vii) a significant drop of consumption after retirement (Banks et al. (1998), Bernheim et al. (2001), Hurd and Rohwedder (2006), Haider and Stephens (2007), Ameriks et al. (2007), Battistin et al. (2009), and Jappelli and Pistaferri (2010)).==== The consumption policy described in Fig. 1 is consistent with facts (i)-(v). The agent does not increase consumption immediately in response to a shock, when the magnitude of the shock is moderate enough that the consumption-wealth ratio changes slightly (as wealth changes) but does not go beyond the inaction interval. This property can provide a potential explanation for (i) excess smoothness, (ii) excess sensitivity, and (iv) MPC heterogeneity. We show by simulation that the consumption of agents with very low loss aversion exhibit excess smoothness and excess sensitivity. In a population consisting of such agents with cost of consumption adjustment, a substantial proportion does not adjust consumption and the probability that an agent adjusts consumption is important to measure MPC. There exists evidence from survey data that for positive small shocks, approximately 80% do not or are not willing to adjust consumption (Shapiro and Slemrod (1995), Shapiro and Slemrod (2003), Shapiro and Slemrod (2009), Christelis et al. (2020), and Fuster et al. (2021)).==== While such a good shock does not increase consumption now, it increases the probability of increasing consumption in subsequent periods. Therefore, consumption in subsequent periods is excessively sensitive to changes in income or wealth as well as to anticipated changes. Both excessive sensitivity and excess smoothness disappear for a large shock, as the consumption-wealth ratio immediately reaches a boundary of the inaction interval for such a large shock, consistent with (iii) the magnitude hypothesis. Finally, notice ==== in Fig. 1. This means that consumption exhibits (v) asymmetric sensitivity conditional on the consumption being adjusted: the marginal propensity to consume (MPC) with respect to negative shocks is larger than the MPC with respect to positive shocks. Our model does not explain facts (vi) and (vii). Overconfidence, implementation costs, time-inconsistency induced by expectations-based loss aversion, or other reasons such as home production, which we do not incorporate in our model, can provide their potential resolution, and we refer to Caliendo and Huang (2008), Huang and Caliendo (2011), Aguiar and Hurst, 2005, Aguiar and Hurst, 2007, and Pagel (2017) for explanations.====The optimal portfolio policy features a U-shaped relationship between financial wealth and the share of risky assets in wealth, called the ====. If ILA increases, the U-shaped relationship becomes more pronounced: the minimum risky share decreases with loss aversion, while its maximum is determined by risk aversion, and unchanged by loss aversion (Fig. 5(a)). When the agent adjusts consumption, she adjusts the portfolio without the concern about maintaining the current level of consumption: therefore, she has the same risk aversion as the non-loss averse agent. When the agent has not adjusted consumption, however, the agent is averse to a negative change in wealth, which makes it more difficult for her to maintain the current level of consumption; therefore, she is more risk averse than a non-loss averse agent.====The U-shaped relationship provides a new perspective on the debate in the literature on the relationship between the household's financial wealth and the risky share. Calvet et al. (2009) and Calvet and Sodini (2014) show evidence that the risky share tends to increase with financial wealth, which is consistent with the habit or commitment, or decreasing relative risk aversion (DRRA) models. However, Brunnermeier and Nagel (2008) and Chiappori and Paiella (2011) find a neutral or slightly negative relationship, providing evidence consistent with the standard CRRA model. From the perspective of our model, the empirical relationship is not robust to changes in wealth distribution. We show that the relationship between changes in the risky share and changes in wealth can be positive or negative, dependent upon the cross-sectional distribution of the wealth-to-consumption ratio. If the distribution is concentrated in the right tail, the relationship tends to be positive, and the opposite is true otherwise.====Attitudes toward risk are characterized by the ==== (RCRRA), i.e., the level of relative risk aversion inferred by an outsider who observes the risky investment of the agent, but assumes the agent is not loss-averse. The RCRRA has an inverted U-shape relationship with the consumption-wealth ratio and wealth, due to its inverse relationship with the risky share. The RCRRA is higher than the actual coefficient of relative risk aversion (CRRA) of the underlying felicity function inside the inaction interval, and is equal to the CRRA at its boundary points. The inverted U-shape becomes wider and higher as ILA increases. That is, the maximum value of RCRRA increases with ILA, while its minimum value is fixed as the CRRA at each boundary (Fig. 5(b)). An individual with high ILA and low risk aversion can appear to be highly risk averse to a small shock (i.e., extreme risk aversion), but appear to be aggressive to a large shock, or consecutive shocks that make the consumption-wealth ratio stay near a boundary (i.e., excessive risk-taking) in time. Such an individual's consumption-wealth ratio moves within the inaction interval; her RCRRA is high when the moderate good and bad shocks alternate.====: There exists vast literature on loss aversion, including reference dependence, and thus, we do not attempt to summarize it here (see Kahneman and Thaler (2000), Barberis and Thaler (2003), Barberis and Huang (2008), and O'Donoghue and Sprenger (2018) for surveys). The key difference between our model and existing models is that the reference point in our model is the previous consumption level.==== Bowman et al. (1999) study a two-period model of consumption and savings with loss aversion. Our model is a continuous-time generalization of their model for when the weight is given to the most recent consumption in calculating the reference point.====Fuster et al. (2021) propose a model similar to ours. They consider an explicit mental adjustment cost in a two period model and show that the model has a substantial explanatory power for empirical results derived from survey data beyond the explanation of the classical model with precautionary savings and liquidity constraints, including the importance of the extensive and intensive margins for MPC measurement, responses to news about income shocks, and asymmetric sensitivities to positive and negative shocks. In this study we consider an infinite horizon continuous time model with mental adjustment costs. We consider a portfolio selection problem and derive optimal policies in closed form, whereas they consider only consumption choice, and rely on numerical simulations.====Pagel (2017) studies life-cycle consumption/savings decisions when the agent has the expectations-based loss aversion proposed by Kösgezi and Rabin, 2006, Kösgezi and Rabin, 2007, Kösgezi and Rabin, 2009, and shows that the expectations-based loss aversion generates excess smoothness and sensitivity of consumption. Thanks to its anchoring with past consumption, our model exhibits stricter rigidity in optimal consumption, as implied by the inaction interval.==== Pagel (2017) does not consider the portfolio selection problem as we do in our model. Furthermore, our model admits closed-form expressions for optimal consumption policies, whereas multi-period expectations-based loss aversion models rarely admit closed-form expressions.====There is extensive literature on loss aversion estimation since Kahneman and Tversky (1979). Recent empirical studies document a large degree of heterogeneity in loss aversion. For example, Ert and Erev (2013) and Yechiam (2019) show that estimation results can depend upon both the context and the magnitude of losses. Walasek et al. (2018) use meta-analysis to obtain an estimate of loss aversion 1.31 with confidence interval ==== much lower than 2, the original estimate from Tversky and Kahneman (1991). Chapman et al. (2018) show that more than half of a large representative sample of the US population is loss tolerant and more educated people tend to exhibit greater loss aversion, implying great diversity in loss aversion. Furthermore, despite significant efforts to study loss aversion toward monetary rewards (see Karle et al. (2015)) few studies examine loss aversion toward consumption. An exceptional study by Hardie et al. (1993) shows that consumers exhibit loss aversion toward quality with the last purchase as a reference point, providing indirect evidence for loss aversion toward consumption. More recently, Foellmi et al. (2019) use macroeconomic data to show evidence for loss aversion in countries. Therefore, our model should not be viewed as describing the universal behavior of a representative individual in an economy, rather as that of individuals who exhibit loss aversion toward satisfaction from consumption. We explore the implication of heterogeneity in loss aversion in our companion paper Choi et al. (2020).====The original motivation of our study is the classical work of Duesenberry (1949), in that our model builds on the assumption of the partial irreversibility of consumption decisions. His critique==== is closely related to habit formation, and thus, has significantly contributed to the development of habit literature. Sundaresan (1989), Constantinides (1990), and Detemple and Zapatero (1991) have developed habit models, demonstrating that they be consistent with the high equity premium. Based on earlier studies Abel (1990) and Campbell and Cochrane (1999), in a seminal study, have proposed an equilibrium model with external habit stock and shown that it could match the observed asset pricing moments. Chan and Kogan (2002) have extended the external habit model to accommodate heterogeneous agents. Furthermore, habit formation in consumption has been widely applied in both finance and macroeconomics. For example, it has been applied to explain the positive relationship between growth and saving, to explain the excess volatility in the current account, or to study the responses of outputs or spending and inflation to monetary shocks (see Carroll et al. (2000), Fuhrer (2000), Gruber (2004), and Smets and Wouters (2007)). However, empirical evidence of habit in the data is mixed. The evidence is strong in the macro data, but weak in the micro data (Dynan (2000), Havranek et al. (2017), and Carroll et al. (2020)). Our model, with loss aversion (or mental adjustment costs) to consumption changes, can be regarded as an alternative form of habit (Dybvig (1995) and Reis (2006)).==== A part of our results resembles those from the habit models. For example, both our model and habit models generate time-varying risk aversion. However, classical habit models neither imply asymmetry and heterogeneity in consumption responses to income (wealth) shocks, nor are they consistent with the magnitude hypothesis.====Our model belongs to a large class of models with adjustment costs for consumption. Substantial literature has developed in this vein (Grossman and Laroque (1990), Flavin and Nakagawa (2008), and Chetty and Szeidl, 2007, Chetty and Szeidl, 2016, Fuster et al. (2021)). These models and our model share common features: consumption adjustments are infrequent, exhibiting excess smoothness and excess sensitivity, and these properties disappear when consumption responds to large shocks. The extant literature mostly model the costs as fixed or proportional to the value of the good, while we model them as proportional to changes in utility.==== With the nature of fixed adjustment costs, these models in general do not exhibit the asymmetric sensitivity of consumption to downward adjustments and upward adjustments, whereas our model does.==== Furthermore, the extant models admit only numerical solutions. Our model is much more tractable and admits closed-form solution. In particular, we can derive an analytic relationship showing the hump-shaped risk attitude, whereas the previous literature derives the relationship only numerically (e.g., Chetty and Szeidl (2016)). However, one can glean useful insights from the literature. For example, Chetty and Szeidl (2016) compare the commitment model, which is a variant of the adjustment cost model, and the habit model. Their comparison is also relevant to ours, as both models belong to the same large class.====Finally Dybvig (1995), Riedel (2009) and Jeon et al. (2018) study models in which consumption decisions are completely irreversible, i.e., consumption is not allowed to decline over time (other than a predetermined depreciation), and Shrikhande (1997) and Watson and Scott (2015) study models with mental adjustment costs.==== All these models are special cases of our model. We provide further discussions on these models as well as habit and commitment models in the main body of this paper.====The rest of this paper is organized as follows: Section 2 explains the discrete-time utility setup with consumption irreversibility and its equivalent form with IRA, and provides their continuous-time versions. Section 3 describes the agent's problem and provides the solution analysis. We investigate the model implications for the consumption policy in Section 4, and for the portfolio choice and the risk attitude in Section 5. Section 6 briefly explains an extension to the pure exchange equilibrium. Section 7 concludes the paper. All the proofs are provided in the Appendix.",Intertemporal preference with loss aversion: Consumption and risk-attitude,https://www.sciencedirect.com/science/article/pii/S0022053121001976,8 November 2021,2021,Research Article,82.0
Do Jihwan,"Economics and Management School, Wuhan University, Wuhan, 430072, China","Received 26 June 2020, Revised 26 September 2021, Accepted 28 October 2021, Available online 8 November 2021, Version of Record 16 November 2021.",https://doi.org/10.1016/j.jet.2021.105382,Cited by (0),"This paper provides a theoretical explanation of “cheating and compensation on-path of play” using a canonical repeated game model of price-fixing collusion. The novel mechanism relies on firms playing mixed strategies allowing for both the monopoly price and undercutting the monopoly price to happen with positive probability, together with a compensation scheme that punishes a price-cutter. For an intermediate range of discount factors, the mechanism is optimal in a restricted class of equilibria, and such price-cutting and compensation are necessary parts for any symmetric collusive equilibrium.","Many price-fixing cartels operate under some form of compensation scheme to enforce collusion. For example, the citric acid and lysine international cartels in the 1990s used a “buy-back system”, under which a firm whose sales exceeded its quota purchases output from those who sold less than their quotas. Similarly, various cartels such as those in vitamins, graphite electrodes, aluminum, and steel also enforced their collusion through either direct or indirect compensation rules.==== In their study on cartel duration, Levenstein and Suslow (2011) emphasize such a collusive use of inter-firm transfers in stabilizing cartels and state that “many cartels—a third of our sample—adopt formal compensation rules.”====Importantly, compensation schemes were often not only agreed upon but also actually implemented in the course of collusion. In the citric acid cartel, Haarmann & Reimer purchased approximately 7000 tons of citric acid in 1992 from ADM for the deficit of ADM's sales beneath its quota.==== In the vitamins cartel, similar compensatory purchases were made in 1996 and 1997 by Roche and BASF when they sold more than their allotted quotas.==== The lysine cartel's use of a buy-back system was described as “typical” by a former U.S. Deputy Assistant Attorney General who was knowledgeable in the operation of price-fixing conspiracies.====If compensation schemes can function as a punishment against offending members and enforce compliance with a collusive agreement, why do firms need to conduct transfers in equilibrium? One may consider the heterogeneity of firms in production costs, in which case cartel members can support efficient allocation of production by providing proper compensation to less efficient firms for reducing their supplies (Miklós-Thal, 2011). However, this type of inter-firm transfer differs from fines or buy-back systems that are used to penalize offending members. Alternatively, Harrington and Skrzypacz, 2007, Harrington and Skrzypacz, 2011 show that if actual sales are subject to unobservable demand shocks, rendering monitoring imperfect, then a collusive equilibrium ==== price-cutting can be supported by, as well as necessitates, implementing asymmetric punishments through transfers when realized sales differ from the agreed-upon quotas. In other words, firms agree to pay fines for excessive sales in the absence of price-cutting because doing so can prevent its actual occurrence.====In this paper, we provide an alternative explanation—firms agree to compensate others in the ==== of price-cutting in order to prevent a complete breakdown of collusion. This explanation is inspired by important empirical observations suggesting that cheating and price-cutting often occur or at least are strongly suspected among cartel members in many price-fixing cartels. During the second phase of the citric acid cartel (beginning in mid-1993), for instance, suspicions of price-cutting began to arise, and in particular, Jungbunzlauer continued to be accused of undermining the price agreements by other members.==== Similar problems were also documented in the lysine and vitamins cartel cases, where firms often attribute the continuous gap between the target and actual prices to price-cutting by other members.==== The Sugar Institute in the 1920s and OPEC also provide good examples of widespread cheating behaviors in cartels.==== To the best of our knowledge, only the paper by Bernheim and Madsen (2017) addresses such deliberate price-cutting and business stealing in an (optimal) collusive equilibrium, but the authors, by ruling out the possibility of inter-firm transfers, do not explain the role of compensation schemes.====We consider a canonical model of repeated price competition with homogeneous goods, symmetric firms, and perfect monitoring where collusion on the monopoly price is not sustainable. We show that some degree of collusion is achievable in equilibrium when firms can transfer part of their profits to others at the end of each period, enabling compensation from a price-cutter to its victims (i.e., asymmetric punishments). A novel mechanism is constructed relying on firms playing mixed strategies allowing for both the monopoly price and price-cutting to happen with positive probability, together with a compensation scheme that punishes price-cutters. The constructed equilibrium is optimal in an important subclass of subgame perfect Nash equilibria, and such compensations and price-cutting are necessary parts for any symmetric equilibrium with some degree of collusive surplus. Based on these findings, we also provide several implications regarding how the details of an optimal collusive agreement will change in response to an increase in a discount factor.====A first step in grasping the intuition behind our results is to understand why the actual implementation of a compensation scheme can be necessary and feasible in equilibrium. The answer is not straightforward in that inter-firm transfers must be self-enforced. At one extreme, sufficiently patient firms can support any desirable level of collusive outcomes by a shadow threat of a price-war punishment that is never implemented in the colluding phase. In this sense, a necessary condition for a compensation scheme to play a non-trivial role would be that perfect collusion is not sustainable. As another extreme, suppose that it is impossible to discipline price-cutting incentives at an agreed price level through the price-war punishment. Then, introducing compensation schemes will not provide additional help unless a price-cutter is hurt more severely with the payment of compensation than engaging in a price-war. In this case, however, the price-cutter will renege on its compensation to others, and this type of double-deviation will remain as a more profitable option.====An answer proposed in this paper is that firms collude on a distribution of prices, not only requiring coordination on the collusive price but also allowing for some degree of price-cutting. With such randomization, the usual price-cutting below the monopoly price no longer guarantees high profits, decreasing incentives for deviations. In turn, however, this generates endogenous asymmetry in realized sales and profits across colluding members, implying that the expected profit attached to each price level might substantially differ. This ex-post asymmetry is then neutralized by using a compensation scheme, giving room for a winning firm to compensate losing firms without causing a complete breakdown of collusion. In this way, collusive outcomes can be sustained with the help of on-path price-cutting and inter-firm compensation schemes.====The equilibrium characterized in this paper is summarized in the left panel of Fig. 1. As illustrated, the equilibrium strategy divides a range of prices into three parts: (i) the monopoly (target) price level corresponding to no price-cutting; (ii) on-path price cuts that trigger inter-firm compensation; and (iii) off-path price cuts that cause a breakdown of a cartel. The right panel in the figure shows that the cartel's surplus under optimal collusion is continuously increasing in the discount factor. This is in sharp contrast to those without price randomization or a compensation scheme, exhibiting discontinuity of equilibrium profits and prices at the threshold discount factor ====.",Cheating and compensation in price-fixing cartels,https://www.sciencedirect.com/science/article/pii/S002205312100199X,8 November 2021,2021,Research Article,83.0
"Fu Qiang,Wu Zenan,Zhu Yuxuan","Department of Strategy and Policy, National University of Singapore, 15 Kent Ridge Drive, 119245, Singapore,School of Economics, Peking University, Beijing, 100871, China","Received 7 November 2020, Revised 18 October 2021, Accepted 20 October 2021, Available online 3 November 2021, Version of Record 16 November 2021.",https://doi.org/10.1016/j.jet.2021.105377,Cited by (5),"The generalized multiple-prize nested lottery contest framework has been broadly applied to model noisy competitions that award prizes to multiple recipients. Assuming homogeneous and risk-neutral players, previous studies have typically solved for the symmetric strategy profile that satisfies the first-order condition as the equilibrium solution to the game. The literature has yet to formally establish equilibrium existence because of a technical challenge caused by the presence of multiple prizes. The associated payoff structure dismisses the key property of contests as aggregative games and nullifies the usual approach for equilibrium analysis. We develop an alternative approach to ascertain the property of players' payoff functions without assuming homogeneous and/or risk-neutral players, which enables us to establish equilibrium existence. We then consider a setting that allows for incomplete information and develop an indirect approach to establish equilibrium existence of the ==== contest game.","Contest-like situations are common in the modern socioeconomic landscape: Contenders expend costly and nonrefundable efforts to vie for a limited number of prizes and they are rewarded based on their relative performance instead of on absolute output metrics. A wide variety of competitive activities resemble a contest, such as R&D races (Loury, 1979; Lee and Wilde, 1980), electoral campaigns (Snyder, 1989), influence politics (Che and Gale, 1998), and internal labor markets inside firms (Lazear and Rosen, 1981).====The game theoretical frameworks that model players' strategic behavior in contests can largely be classified into two broad categories: perfectly discriminatory contests and imperfectly discriminatory contests.==== The former allows the player with the highest effort or bid to prevail with ====, which refers to an all-pay auction (Hillman and Riley, 1989; Baye et al., 1993, Baye et al., 1996). The latter abstracts the situation whereby one's win or loss depends not only on his effort but also on ==== and ====. In particular, the generalized lottery contest is predominantly adopted to model noisy winner-selection mechanisms: For a given effort profile ====, one wins with a probability====where the function ====, conventionally called the impact function, converts a player's effort ==== into his effective output in the contest.==== The two modeling frameworks provide drastically different predictions for equilibrium behavior and yield contrasting implications for contest design.====A natural extension is to allow prizes to be awarded to multiple players.==== Multiple prizes can readily be accommodated in an all-pay auction, as players are precisely ranked by their bids and each is awarded a prize according to his respective rank (see, e.g., Clark and Riis, 1998a; Barut and Kovenock, 1998; Bulow and Levin, 2006; Siegel, 2009, Siegel, 2010, Siegel, 2014; Xiao, 2016; Fang et al., 2020). Embedding multiple prizes into a generalized lottery contest was less straightforward until Clark and Riis (1996) proposed the multi-prize nested lottery contest model. Despite its broad applications (see, e.g., Azmat and Möller, 2009; Brown, 2011; Fu and Lu, 2009, Fu and Lu, 2012b; Fu and Wu, 2021), the numerous studies based on the model have provided little in terms of formal ways to shed light on its game theoretic fundamentals. This paper provides a general analysis of equilibrium existence in generalized multi-prize nested lottery contest games to close this long-existing gap.====  A generalized multi-prize nested lottery contest can be conveniently described as a sequential lottery process. Suppose that ==== prizes are available to ==== players, with each eligible for at most one prize. Players commit to their effort entries ====, and the winner of the first prize is determined by the ratio-form contest success function (CSF, henceforth) (1). This winner is immediately removed from the pool of players eligible for the second prize; the recipient of the second prize is drawn from the rest of the players, and the probability of winning it—conditional on not having won the first prize—is given by the ratio of his effective output to the sum of those who remain in the pool. This process is repeated until all ==== prize recipients are distributed. Despite the intuitive and convenient analogy to a sequential lottery process, Fu and Lu (2012a) demonstrate that the contest is uniquely underpinned by a (simultaneous) noisy ranking system à la (McFadden, 1973, McFadden, 1974), with the probability of one's being ranked in the ====th place equal to that of being selected for the ====th draw in the sequential lottery process.====Prior analysis has largely been incomplete. Assuming complete information and homogeneous, risk-neutral players, the majority of studies solve for the symmetric effort profile that satisfies first-order conditions and simply adopt the solution as the equilibrium prediction. The literature nevertheless has yet to verify whether this convenient solution fulfills the requirement of Nash equilibrium, or to establish equilibrium existence in general.====  The multi-prize nested lottery contest fundamentally differs from its single-prize variant in terms of the underlying game theoretic structure. Previous results on equilibrium existence in contests and discontinuous games in general do not carry over.====Recall the ratio-form CSF (1) and consider, for simplicity, a standard lottery contest with ====. Discontinuity in players' payoff functions arises at origin, i.e., ====. The analysis of a contest typically relies on its properties as an aggregative game to verify equilibrium existence: For a given prize value and an effort cost function, one's payoff depends only on his individual output ==== and the total effort ====, which enables the powerful tool of backward-reply correspondence (see Selten, 1970; Novshek, 1985; and Acemoglu and Jensen, 2013) in equilibrium analysis and, subsequently, share correspondence (Cornes and Hartley, 2005) in contests. However, this approach hits a roadblock in a multi-prize contest, in which one's payoff depends not only on his effort and the total effort, but also on the distribution of the effort profile. To see this, note that the ex ante probability of a player ===='s winning the second prize, which we denote by ====, is given by====where ==== is the set of players involved in the contest: ==== is the probability of player ===='s being picked in the second draw conditional on another player ===='s winning the first prize, while ==== is the probability of player ===='s being picked in the first draw. The deviation from aggregative games nullifies the conventional approach to establishing equilibrium existence in contests.====Furthermore, the multi-prize generalization compounds the discontinuity in payoffs. Consider, for instance, a five-player contest that awards three positive prizes. In addition to the origin, discontinuity arises at an effort profile ==== as well: Player 3 would expect a jump in his payoff if he exerts an infinitesimal effort, which assures him of a prize.====Reny (1999) develops the famous thesis that a pure-strategy Nash equilibrium would emerge in a discontinuous game if the game is ==== with convex and compact strategy spaces and payoffs are ==== in players' strategies. However, this theorem cannot directly be applied. Write a player's payoff function as====where ==== is the probability of his winning each prize ====, ==== is his valuation of prize ====, ==== is his effort cost, and ==== is an increasing and (weakly) concave Bernoulli utility function that represents his risk preference. With risk-neutral players and a single prize, the winning probability as given by (1) is well behaved, as it is concave in one's own effort. In contrast, with two or more prizes, the probability ==== is concave only for the first prize, i.e., ====. The property of the payoff function ====—as an aggregation—is thus elusive.====  In this paper, we first establish the existence of equilibrium in a complete-information setting in which players differ in their prize valuations, utility functions, impact functions, and effort cost functions. We then proceed to an incomplete-information setting and establish equilibrium existence in a Bayesian contest game.====More specifically, in the complete-information setting, we borrow Reny's (1999) better-reply security condition to establish equilibrium existence. We first develop a novel approach to verifying the concavity of a player's payoff in this model: Instead of writing the expected payoff function (2) as a linear combination of one's probabilities of winning each prize as above, we rearrange it as a linear combination of the probabilities of ====. The rearrangement illuminates the nature of a player's payoff structure in this game, which allows us to circumvent the usual analytical challenge and establish payoff concavity.====Further, we identify a lucid and intuitive sufficient condition (i.e., Assumption 1 in Section 2) that ensures better-reply security in the game. The condition requires that the numbers of ==== for each player—i.e., the prizes that provide positive incentives to a player—should be equal among all. We demonstrate that this condition plays a critical role and that better-reply security may fail to hold when it is violated.====We then relax the assumptions of complete information to consider the contest as a discontinuous Bayesian game. It allows for a rich information structure, in which players' private types may involve their prize valuations, utility functions, impact functions, and effort cost functions. The results from the recent literature on equilibrium existence in incomplete-information discontinuous games—i.e., He and Yannelis (2015) and Carbonell-Nicolau and McLean (2018)—do not directly apply to our context. In particular, the multi-prize nested lottery contest fails to meet the requirement of ==== in every ex post game, which ensures upper semicontinuity of the sum of payoffs on players' behavioral strategies. We develop an indirect approach that bridges He and Yannelis (2015) and Carbonell-Nicolau and McLean (2018) to our setting. Specifically, we first construct an alternative game that modifies the original contest game by manipulating its “tie-breaking” rule. The constructed game is shown to meet the requirement of He and Yannelis (2015) and Carbonell-Nicolau and McLean (2018), and thus an equilibrium exists. We then demonstrate that the equilibrium of the constructed game is also that of the original game, which closes the loop.====  Our paper belongs to the literature that explores the equilibrium fundamentals of imperfectly discriminatory contests. A large number of studies have been devoted to verifying the existence and/or uniqueness of Nash equilibrium in complete-information single-prize contests with risk-neutral players. These studies include Pérez-Castrillo and Verdier (1992), Szidarovszky and Okuguchi (1997), Cornes and Hartley (2005), Alcalde and Dahm (2010), Ewerhart (2015), and Feng and Lu (2017). Esteban and Ray (1999) and Brookins and Ryvkin (2016) establish equilibrium existence and uniqueness in group contests, while Franke and Öztürk (2015) and Xu et al. (2020) verify this in network contests. Lagerlöf (2020) applies the classic result of Reny (1999) to study a hybrid contest in which each player takes two actions, with one to be costly regardless of the outcome (all-pay) and the other to incur expenses only to the winner (winner-pay). While all the aforementioned studies assume risk-neutral players, Skaperdas and Gan (1995), Cornes and Hartley, 2003, Cornes and Hartley, 2012, Yamazaki (2009), and Treich (2010) proceed to settings with risk aversion.====This research stream has extended to contests with incomplete information. Hurley and Shogren (1998b), Schoonbeek and Winkel (2006), Wärneryd (2003), Rentschler (2009), Zhang and Zhou (2016), and Denter et al. (2020) allow for one-sided asymmetric information. Hurley and Shogren (1998a), Malueg and Yates (2004), Schoonbeek and Winkel (2006), Fey (2008), and Serena (2021) consider two-sided incomplete information in bilateral contests.==== Wasser (2013a, 2013b), Ewerhart (2014), Einy et al. (2015), and Ewerhart and Quartieri (2020) allow for private information with three or more players. Einy et al. (2017) establish equilibrium existence and uniqueness in a Tullock contest in which symmetrically informed players are uncertain about their common prize value and common effort cost function. In particular, Einy et al. (2015) and Ewerhart and Quartieri (2020) assume rich information structures and allow each player's payoff to depend on his private type in multiple dimensions, e.g., prize valuations, impact functions, and effort cost functions. The former establishes equilibrium existence, while the latter also verifies the uniqueness.====All the aforementioned papers assume a single prize. To the best of our knowledge, our paper is the first to explore equilibrium existence in noisy multi-prize contests, and the analysis contributes notably to the contest literature. First, as previously mentioned, the shift from a single-prize contest to a multi-prize one fundamentally changes the nature of the game, dismissing the regularity inherent in aggregative games. As a result, the conventional results and approaches for single-prize contests do not extend. Second, our analysis illuminates players' payoff functions in multi-prize contests. Our approach and the identified regular properties may well be useful for future equilibrium analysis of this game. Third, our analysis encompasses a broad spectrum of modeling variations based on the framework of multi-prize nested lottery contest. For instance, the Bayesian game setting considered in Section 4 accommodates an enriched information structure and allows players to be privately informed.====Our study also complements the recent literature on equilibrium existence in discontinuous games with incomplete information. Three papers relate closely to our work. First, our analysis of incomplete-information contests builds on He and Yannelis (2015) and Carbonell-Nicolau and McLean (2018), who notably extend Reny's (1999) equilibrium existence result to Bayesian games. As mentioned above, our contest game deviates from their settings because it lacks the key property of aggregate upper semicontinuity, which prevents direct application of their results. Our approach restores the relevance of their existence results in our context and broadens the scope of their applications. The technique may well be useful in future research for other forms of games that do not immediately match the requirements of He and Yannelis (2015) and Carbonell-Nicolau and McLean (2018). Second, our paper is also conceptually linked to Olszewski and Siegel (2020). They develop an alternative tool to verify equilibrium existence in discontinuous games with ties and relate its applications largely to contest games. Their study applies more to perfectly discriminatory contests, while ours focuses on imperfectly discriminatory contests.====The rest of the paper is structured as follows. Section 2 describes the generalized multi-prize nested lottery contest. Section 3 establishes the main result regarding equilibrium existence in contests with complete information. Section 4 extends the model to accommodate incomplete information and establishes equilibrium existence, and Section 5 concludes. Appendix A collects the proofs that are not provided in the main text. Appendix B provides an example that demonstrates how a key property of the model relates to an assumption.",On equilibrium existence in generalized multi-prize nested lottery contests,https://www.sciencedirect.com/science/article/pii/S0022053121001940,3 November 2021,2021,Research Article,84.0
Stark Oded,"University of Bonn, Germany,University of Warsaw, Poland","Received 24 March 2022, Revised 31 May 2022, Accepted 25 August 2022, Available online 11 November 2022, Version of Record 11 November 2022.",https://doi.org/10.1016/j.jet.2022.105543,Cited by (0),"Fifty years ago Eytan Sheshinski constructed a composite measure of social welfare in which income per capita enters positively, and income inequality enters negatively: social welfare was defined as a strictly increasing function of the product of income per capita and one minus the Gini coefficient. In the case of a population of two persons whose incomes are distinct, Sheshinski states that social welfare depends only on the lower income, which reduces the social welfare function to the Rawlsian social welfare function. We show that this is not true: social welfare depends on both incomes, and there is no congruence with the Rawlsian perspective.",None,Corrigendum to “Relation Between a Social Welfare Function and the Gini Index of Income Inequality” ,https://www.sciencedirect.com/science/article/pii/S0022053122001338,11 November 2022,2022,Research Article,86.0
"Dasgupta Ani,Ghosh Sambuddha","International Maritime Business Department, Massachusetts Maritime Academy, 101 Academy Drive, Buzzards Bay, MA 02532, United States of America,Economics Department, Boston University, 270 Bay State Road, Boston, MA 02215, United States of America,Economics, Shanghai University of Finance and Economics, 111 Wuchuan Road, Shanghai 200433, China","Received 8 September 2020, Revised 7 May 2021, Accepted 26 June 2021, Available online 18 August 2021, Version of Record 2 December 2021.",https://doi.org/10.1016/j.jet.2021.105312,Cited by (0),"We propose a simple geometric construct called ‘self-accessibility’ to study discounted infinitely repeated games with perfect monitoring. Self-accessibility delivers payoffs by computing action paths that keep all continuation payoff vectors close to the target payoff. It unifies the analysis of games with symmetric and asymmetric discounting, dispenses with public randomization, and generates pure on-path strategies. We first use it to simplify ==== (====, ====) and generalize their results to asymmetric discounting. Next, we use self-accessibility to find all payoff profiles that are realizable via some path at some (possibly asymmetric) discount vector. Finally, we offer an easily-verifiable sufficient and almost necessary condition for a payoff profile to arise in a subgame-perfect equilibrium, and use self-accessibility to explicitly construct the corresponding equilibrium strategies. We show that achieving cooperation via intertemporal trade among unequally patient players may require very different arrangements from those proposed earlier by ==== for two players using public randomization.","To what extent can we deter myopic behavior by self-interested agents? What kind of strategies are required to do so? These questions are central to the literature on repeated games. Under symmetric discounting comprehensive answers to them are available. However, many applications call for asymmetric discounting====: not all agents have the same discount factor, either because of different time preferences or because they face different rates of interest in the capital market.====We propose a simple geometrical construct, ====, to study discounted repeated games with perfect monitoring and asymmetric discounting. It is useful for identifying payoff vectors with three key properties: realizability, sequential individual rationality, and supportability. A payoff vector is ==== if it is the (normalized discounted) payoff of a path of actions at some (possibly asymmetric) discounting profile. It is ==== (SeqIR) at that discounting profile if such an action path gives every player continuation payoffs that are at least his minmax value. Finally, it is ==== at that discounting profile if such a path can be sustained in a subgame perfect Nash equilibrium (SPNE).====We now define our key concept. A set of payoffs is self-accessible for a discount factor vector if for any point in the set there is a pure action profile that can be played such that the induced continuation payoff vector also lies in the set. Why is this construct useful? First, realizability of any point in a self-accessible set follows from recursively applying the definition (the continuation payoff vector is itself in the self-accessible set, etc.) Second, by construction, only pure action profiles are used, obviating the need for public randomization devices (PRDs). Third, continuation payoffs are in the set by construction; thus, if we can enclose a strictly individually rational target payoff in a small enough self-accessible set, SeqIR is automatic.====With symmetric discounting, we prove that all closed balls in the interior of the feasible set are self-accessible above an easily calculated==== discount factor bound. Furthermore, allowing for asymmetric discounting, if each player's discount factor exceeds this bound, we can find a self-accessible ellipsoid inside the ball. Given any point inside the FSIR (feasible and strictly individually rational) set of the stage game, our ability to find a self-accessible set containing it implies, using the third property mentioned in the paragraph above, that this point can be obtained in equilibrium if all players are patient, using a unified approach for symmetric and asymmetric discounting. No condition is imposed on ‘relative discounting’, i.e. the ==== closeness of players' discount factors to unity; as long as each discount factor exceeds the bound, the argument works.====Staying with asymmetric discounting, we now turn to points outside the feasible set. First, we show that any payoff vector in the interior of the smallest closed rectangle that contains the feasible set ==== of the stage-game is realizable for some discount factor profile. Thus the set of realizable payoffs is contained in this rectangle, and contains the interior of the rectangle. To study SeqIR and supportability, we next introduce two closely related conditions that are easy to check directly from the payoff matrix. A payoff ==== satisfies ==== (WD) if we can renumber the players in such a way that for each ==== there is a feasible point giving player ==== the amount ==== while giving both him and those before him weakly individually rational payoffs (those after may get less than their individually rational payoffs). Further, if these feasible points lie in the ==== of ==== and for ==== and his predecessors provide payoffs that are ==== individually rational, we say the vector ==== satisfies ==== (SD). We prove that WD is necessary for supportability, while SD is sufficient.====It is worth noting that we establish our sufficient conditions by explicitly constructing both equilibrium and punishment paths, as well as compatible discount factor profiles. For points outside the FSIR set, our strategies are novel and are best described as ‘turnpike strategies’ for the following reason. The equilibrium path consists of two phases: a finite-period turnpike phase when the continuation payoffs are outside the FSIR set, and an infinite-period post-turnpike phase when they are inside. If a deviation takes place in the turnpike phase, after punishing the deviant, play returns to the point on the turnpike phase where the deviation took place and continues on the turnpike phase, with rewards for punishing the deviant being deferred to the post-turnpike phase.====In the rest of this introduction we discuss the relevant literature, and place our contribution in relation to it. For symmetric discounting the seminal paper of Fudenberg and Maskin (1986), hereafter FM 1986, showed that FSIR payoffs are supportable at a high enough common discount factor; however, they assumed the existence of PRDs. Clearly, it is problematic to argue that players have access to a correlating device fine enough to realize all FSIR payoffs. A plethora of interesting and counterintuitive things happen as soon as one disallows PRDs. For example, Yamamoto (2010) shows that for large enough common discount factors, the set of SPNE payoffs can be non-convex, and non-monotonic in the common discount factor. For a class of two-player ==== games, Olszewski (1998) shows that the undiscounted folk theorem does not hold without PRDs.====Fudenberg and Maskin (1991), hereafter FM 1991, deliver the first folk theorem without PRDs. They make use of Sorin's Lemma (see Sorin, 1986) which addresses realizability, but does not guarantee SeqIR. FM 1991 build on this result to restore SeqIR; as some non-constructive steps are involved, their approach, unlike that in FM 1986, does not give an easy way to calculate a discount factor bound sufficient to support a target payoff vector. In contrast, self-accessibility delivers such a bound; this might be important from a practical standpoint as the whole argument of repeated interaction facilitating cooperation among selfish individuals is more plausible when these bounds are known to be lower.====The first paper to systematically study repeated games with asymmetric discounting was Lehrer and Pauzner (1999), hereafter LP 1999. Under perfect monitoring and assuming the existence of PRDs, they characterize the Pareto-frontier of the limiting realizable set of 2-player games as players become patient (i.e. each player's discount factor approaches unity) while fixing relative patience (taken as ratios of logarithms of discount factors).==== LP 1999 also address the equilibrium problem by providing upper and lower bounds of the equilibrium payoff set (as well as its limit as players become patient).====To identify Pareto optimal payoffs in a two-player repeated game, LP 1999 uses paths where the relatively impatient player gets high payoffs at the beginning, perhaps at the cost of poor payoffs for the relatively patient player; thereafter, the path switches to actions that reward the latter for his initial sacrifice. Such paths effect mutually beneficial trade: it is as if the patient player initially gives a loan to the impatient one, and gets paid back later. However, in equilibrium, the impatient player still must receive at least his minmax value as continuation payoff in every period of the loan-repayment phrase. Maximizing the patient player's payoff in the feasible set subject to giving the impatient one at least his minmax value typically requires correlated actions. This is why a PRD is used in LP 1999. They conjecture that it might be possible to remove PRDs using techniques similar to those in FM 1991; however, we offer a counterexample showing that the building block of the FM approach, Sorin's Lemma, fails under asymmetric discounting.==== Additionally, LP acknowledge that their methods do not extend to more than two players even in the presence of PRDs; indeed with three or more players, equilibrium paths may exhibit an intertemporal tradeoff pattern very different from the ==== pattern (See Section 5.4) on pure action profiles that characterizes the two-player case.====Chen and Takahashi (2012), under the same conditions on patience as LP 1999, show that for any number of players a sequence of action profiles can be supported in equilibrium if all its continuation payoffs are uniformly bounded away from minmax values, provided any two players have either non-equivalent stage-game payoffs (i.e., one is not an affine transformation of the other) or different discount factors.==== However, without PRDs, their folk theorem approximates any target payoff, but may not deliver it exactly.====Repeated games with imperfect public monitoring are studied using ‘self-generation’, a technique originally advanced in Abreu et al. (1990), and subsequently extended by Fudenberg and Levine (1994) and Fudenberg et al. (1994). Using this technique, Sugaya (2015) extends LP 1999 to prove a comprehensive folk theorem that applies to any number of players, perfect and imperfect public monitoring, and possibly asymmetric discounting, without using PRDs.==== He too, works with discount factor vectors exhibiting fixed ‘relative patience’. Specifically, he shows that the limiting sets of SeqIR payoffs and equilibrium payoffs are identical as players become more ‘absolutely patient’ while ‘relative patience’ parameters — given by pairwise ratios of ==== — are either constants or converging to constants.====Clearly, self-accessibility is a recursive notion based on dynamic programming. Restricting self-generation to pure-action profiles while letting go of incentive compatibility gives us self-accessibility; thus neither notion nests the other. Leaving incentives out of the definition allows us to provide them separately using stick-and-carrot schemes à la Abreu (1988), making our proofs fully constructive. In contrast, self-generation is designed to compute equilibrium payoff sets, and typically does not provide explicit equilibrium strategies or discount factor bounds for supporting a specific payoff vector.====To put the contributions of this paper in context we note that the existing literature on asymmetric discounting stands on a very different footing from the symmetric-discounting literature in two ways. First, the latter makes it easy for the applied economist to rule out certain payoffs as impossible in the repeated game; but the former does not provide simple necessary or sufficient conditions in terms of the stage game parameters for a payoff profile to be supportable in equilibrium. Our conditions WD and SD, on the other hand, are easily checked necessary and sufficient conditions, respectively. Breaking with the existing literature on asymmetric discounting, we don't fix relative discounting ====, since we wish to answer the question: “Can a given payoff profile be supported for some form of asymmetric discounting?”==== Relative discounting emerges ==== in our work. When we show that a certain payoff vector is realizable or supportable by picking a specific discount factor profile for which the claim is true, we also show that the claim continues to hold with another discount factor profile which has the same discount rate ratios but where all players are now more (absolutely) patient.====Second, unlike the literature on symmetric discounting, existing folk theorems under general discounting do not explicitly construct equilibrium strategies to support a specific payoff vector. Sugaya (2015) relies on fixed point arguments to identify payoffs, rather than constructing strategies. LP 1999 and Chen and Takahashi (2012) ==== that we are already given an equilibrium path that is (strictly) SeqIR and only then construct an equilibrium strategy to support such a path. In contrast, we construct from scratch the equilibrium path and the strategy profile to support a target payoff vector satisfying SD. In this aspect too, we have followed the classical tradition as exemplified by FM 1986.====The paper is structured as follows. Section 2 formally introduces the model and notation, and then, using a numerical example, previews our results on asymmetric discounting. Section 3 defines self-accessibility, briefly explains its significance, and studies the structure of self-accessible sets under symmetric and asymmetric discounting. Section 4 proves a folk theorem for points in the interior of the FSIR set. Section 5 explores realizability. Section 6 introduces the weak and strict diagonalizability conditions and uses them to analyze supportability, building thereby on Section 4. Section 7 concludes. All proofs are collected in the appendix.",Self-accessibility and repeated games with asymmetric discounting,https://www.sciencedirect.com/science/article/pii/S0022053121001290,18 August 2021,2021,Research Article,87.0
"Araujo A.,Vieira S.,Parra C.","Instituto Nacional de Matemática Pura e Aplicada, Estrada Dona Castorina 110, Rio de Janeiro, Brazil,FGV EPGE Brazilian School of Economics and Finance, Praia de Botafogo, 190, Rio de Janeiro, Brazil,Ibmec, Av. Presidente Wilson, 118, Rio de Janeiro, Brazil,UERJ, R. São Francisco Xavier, 524, Maracanã, Rio de Janeiro, Brazil","Received 12 April 2021, Revised 21 April 2022, Accepted 24 July 2022, Available online 12 September 2022, Version of Record 12 September 2022.",https://doi.org/10.1016/j.jet.2022.105525,Cited by (0),"In ====, C. Schottmüller proposes a method for solving the principal's maximization problem. Using this method, he solves three examples. We show that the solutions presented in that paper are suboptimal. For each of the examples, we show how to build an optimal decision function that yields a larger expected payoff for the principal. These decision functions are monotonic but not strictly monotonic. Therefore, they open up the possibility of bunching. In addition, we identify that the source of the problem is (====, Theorem 1, sixth bullet point) that is not correct. The sufficiency conditions for strict monotonicity of the decision function ==== that are presented in ==== are also incorrect."," studies monotone solutions to a screening model in which the single-crossing condition does not hold. He conducts a meticulous and interesting analysis. First, he characterizes the results for monotone solutions. Then, he concentrates on strictly monotone and continuous solutions, presenting an algorithm to find such solutions. Finally, he gives the sufficient conditions for a solution to be strictly monotone and continuous so that the algorithm can be used. To illustrate this method, he uses the algorithm to solve three numerical examples, the first in the main body of the paper and the other two in the web appendix.====In this paper we show that the sufficiency conditions for strict monotonicity of the decision function ==== that are presented in ==== are incorrect. The sufficient conditions in these two propositions are derived using (====, Theorem 1, sixth bullet point). However, this sixth bullet point from Theorem 1 contains a subtle error: the changed decision function ==== used in its proof may not be implementable when ==== is constant at an interval containing ==== which invalidates the result in this case.====Taking this into account, we modified the solution proposed by Schottmüller, now allowing the decision function ==== to be constant for the lower types. And the result is that, for each of the examples, we manage to build an optimal decision that yields a higher expected payoff for the principal. Therefore, in the three examples solved by ====, the solutions he found are suboptimal.====We use two approaches in each example. Our numeric approach discretizes the principal's problem and solves that problem using a nonlinear optimization package. The other, an analytical approach, uses calculus of variations. The results of these two approaches coincide. Moreover, these results differ from those presented in ====.====The following is the Supplementary material related to this article.",Corrigendum to “Adverse selection without single crossing: Monotone solutions” [J. Econ. Theory 158 (2015) 127–164],https://www.sciencedirect.com/science/article/pii/S0022053122001156,12 September 2022,2022,Research Article,90.0
"Crès Hervé,Tvede Mich","New York University in Abu Dhabi, PO Box 129188, Abu Dhabi, United Arab Emirates,University of East Anglia, Norwich Research Park, Norwich, NR4 7TJ, United Kingdom","Received 3 June 2019, Revised 13 February 2021, Accepted 16 June 2021, Available online 24 June 2021, Version of Record 12 January 2022.",https://doi.org/10.1016/j.jet.2021.105305,Cited by (1),"We study the formation of opinions in a bipartite network of firms' boards and directors theoretically. A director and a board are connected provided the director is a board member. Opinions are sets of beliefs about the likelihood of different states of the world tomorrow. Our basic assumption is that boards as well as directors aggregate opinions of each other: a production plan is better than another for a board (director) provided every director (board of which she is a member) finds it better. Opinions are stable provided aggregation does not result in revision of opinions. We show that for connected networks: opinions are stable if and only if they are unambiguous and identical; and repeated aggregation leads to stable opinions. Hence, there will eventually be a single society-wide intersubjective “truth”.","In the present paper we develop a formal theory of formation of opinions in networks. Our theory uses connectedness of networks to explain consensus in these networks.==== We have looked at all 800 decisions taken in 2014 in general assembly meetings of the 40 largest French corporations (CAC 40) leaving out six decisions that were put on vote without the approval of the boards and obtained very little support (2-16%). On average every decision was supported by 94.9% of the voters. For a ==== defined as the highest ==== for which ====% of the decisions were supported by at least ====% of the votes the number is 87.6%. Decisions on collective issues have an index of 89.4%, decisions on individual compensation an index of 78.8% and decisions on appointments an index of 87.5%. On average general assembly meetings are well attended. Indeed for the period 2014-2017 on average about 1000 shareholders were present and further about 13600 shareholders were represented by proxies.====Unless markets for goods and assets are believed to be perfectly competitive and complete much less consensus should be expected in assemblies of shareholders. Indeed for anybody familiar with the social choice literature, these numbers are striking. An illustrative example of disagreement is ==== individuals sharing a cake where everybody just cares about the size of her own slice: for all allocations of the cake ==== individuals would agree to split the slice of the last individual; and the stability index would be zero.====The boards of corporations in CAC 40 have a total of 501 directors of which 57 are members of two boards, 14 are members of three boards and nobody is a member of four or more boards. In Fig. 1 we show the network with corporations being nodes and common board members being links, so there are 40 nodes and 99 links. A closer look at the boards in the CAC 40 corporations reveals that they are all connected directly or indirectly. Since the average number of links per firm is just below 2.5, connectedness of the network is striking.====We develop a theory of opinion formation in bipartite networks. Our theory uses one of the striking features, namely the connectedness of the network, to explain the other striking feature, namely the very high consensus index. Perhaps it would have been more natural to focus on either general assembly meetings or boards, but as general assembly meetings appoint boards and boards put up proposals to general assembly meetings we suggest the high consensus in general assemblies carries over to boards. A suggestion supported by casual evidence.====Several studies of networks of boards and directors have found that the networks tend to be connected (Burt, 2006; Davis, 1996). The median firm in Fortune 500 in the 1980s shared board members with seven other firms and some firms shared board members with 40 or more firms (Davis, 1996). It has been pointed out that firms benefit from being connected because thereby expertise and opinions of board members in other firms becomes indirectly available (Davis et al., 2002; Mace, 1971).==== In our economy decisions in firms have to be made today about production tomorrow. The state tomorrow is unknown so production is uncertain. In every firm a decision about production has to be taken. These decisions are equivalent to choosing beliefs for which firms should maximize profit. There are two types of agents, namely boards and directors. The decision in a firm is taken by the board that consists of some directors who can be members of one board or several boards. Every director has an opinion about how likely the different states are. These opinions can be unambiguous, in which case the director has a unique subjective belief about the likelihood of the different states, or ambiguous, in which case the director has multiple subjective beliefs.====Despite differences in opinions among directors, boards have to take decisions. We impose a rather mild condition, namely the Pareto principle, on aggregation processes of opinions in a board: If every director considers one production plan at least as good as another, and a least one director considers it better, then the board finds it better too. The opinions of boards are intersubjective in that they are formed by a group of individuals, namely the directors, rather than by a single individual. Therefore the decisions taken in boards have a tinge of objectivity. We assume directors are affected by the decisions taken in boards: opinions of directors are outcomes of aggregation processes of the decisions in which they are involved. Like in boards, we impose the Pareto principle on aggregation processes within directors, but we include an element of reflexivity: If every board, in which a director is a member, ==== considers one production plan at least as good as another, and at least one board ==== considers it better, then the director finds it better too.====Since boards aggregate opinions of directors and directors aggregate opinions of boards, mutually interdependent aggregation processes take place in our model. Networks can be used to represent connections between firms and directors: a firm and a director are directly connected when the director is in the board of the considered firm. A firm and a director are indirectly connected when the director is in the board of a firm in which another director is a member of the board of another firm in which... another director is in the board of the considered firm. We focus on connected networks in which every firm and every director are directly or indirectly connected. However at the end of the paper we briefly discuss how our findings extend to disconnected networks.====Stability is the notion of equilibrium for opinions. Indeed, opinions are ==== provided they satisfy the Pareto principle with respect to aggregation. Our main results concern stable opinions. We find that opinions are stable with respect to aggregation if and only if they are identical and unambiguous (Theorem 1). Hence stability of opinions is equivalent to a single society-wide intersubjective “truth” or a shared belief. Intuitively the Pareto principle implies aggregated opinions are convex combinations of opinions and every opinion is given positive weight. Because every agent has an aggregated opinion of other aggregated opinions and so on, stable opinions are identical and unambiguous.====Since directors meet other directors in boards it could be argued that they should be affected solely by the opinions of other directors rather than decisions in boards. We extend the characterization of stable opinions to include the case where directors are affected by other directors rather than boards (Corollary 1) as well as the dual case where boards are affected by other boards rather than directors (Corollary 2).====We have modeled our economy as an Actor-Network. It is flat: boards are formed by directors and directors are formed by boards; and, there is no aggregate level above them. Theorem 1 strengthens such an interpretation of our world. Opinions of agents depend on the opinions of their connections whose opinions depend on the opinions of their connections and so on: Directors and boards are their connections. Consequently it does not make sense to study an agent in isolation. Agents are treated symmetrically. Boards and directors aggregate opinions they experience from the other type of agent. They can be seen as direct or indirect collectives of collectives (Breiger, 1974; Simmel, 1955): directors are collectives of boards which are collectives of directors which are collectives of boards and so on. However, rather than applying Actor-Network Theory to a specific topic like decisions in boards of the CAC 40 corporations we construct a formal Actor-Network aimed at capturing the mutually interdependent aggregation processes in boards and directors. And rather than going into the specifics of how directors and boards interact and react, we merely assume that aggregation processes satisfy the Pareto principle.====The characterization of stable opinions is non-constructive like existence theorems (Debreu, 1959), it is merely saying that nobody feels the need to revise her opinion as a result of social interaction if and only if there is a shared belief. There is no explanation of how opinions become stable. Therefore we consider dynamic mutually interdependent aggregation processes where boards revise their opinions as a consequence of directors revising their opinions and directors revise their opinions as a consequence of boards revising their opinions and so on. We find that dynamic mutually interdependent aggregation processes converge to stable opinions (Theorem 2). As a converse we show that if opinions are stable, then there are mutually interdependent dynamic aggregation processes that converge to these opinions (Theorem 3). Hence stable opinions and limits of mutually interdependent dynamic aggregation processes are identical.====Naturally aggregation can be interpreted as deliberative processes in boards and directors. Therefore stable opinions can be seen as reflective equilibria (Rawls, 1971; Rorty, 1990; Daniels, 2011). In the context of reflective equilibrium, Theorem 1, Theorem 2 are extremely strong. Deliberation leads to a shared belief so deliberation removes all ambiguity and resolves all conflicts. However, in our world opinions are completely flexible in that agents are willing and able to revise them based on their experiences. And revision of opinions occurs in accordance with the Pareto principle so revision pushes toward agglomeration rather than polarization. As mentioned previously we discuss aggregation in disconnected networks at the end of the paper.==== The seminal model of influence and aggregation of information and opinions in networks is DeGroot (1974). It is a discrete time dynamic model where a group of agents starts with initial opinions and then periodically updates them by taking weighted averages of the opinions of their neighbors according to a fixed vector of weights. Interactions between agents are hence captured through a matrix ==== whose ====-entry is ====, the degree of confidence of agent ==== in the opinion of agent ====. Conditions of convergence of the updating process are thoroughly studied in Jackson (2008) and Golub and Jackson (2010).====DeMarzo et al. (2003) propose a microfoundation of DeGroot's model as a repeated naive maximum likelihood estimation procedure of an underlying parameter that captures a form of persuasion bias. The interpretation gave rise to a growing literature on aggregation of information on networks, relaxing the linearity in the naive-updating rules of the agents. Some recent contributions, like Molavi et al. (2018) consider social learning where agents both receive signals about an underlying state of the world and naively combine the beliefs of their neighbors; others, like Cerreia-Vioglioa et al. (2020), study the long-run opinions as the size of the society grows to infinity. Both papers take an axiomatic approach, postulating some properties of the opinion aggregators.====Our model differs from this literature in two ways. First, the opinions of our agents (whether individual or collective) do not rely on ‘information’ in a strict sense: they do not receive signals about some ‘true’ state of the world; their opinions are purely subjective. Second, although our approach is axiomatic, we do not use aggregators, but just postulate the Pareto principle on how opinions are updated. The fact that our graph is bipartite, with boards aggregating the opinions of directors, and conversely directors aggregating the opinions of boards, introduces some additional structure that allows connectedness to result in a unique unambiguous opinion.==== In Section 2 we outline the structure with time and uncertainty and describe the agents, boards and directors, and their relations. In Section 3 we introduce the Pareto principle for aggregation. In Section 4 we introduce our notion of stability of opinions and characterize stable opinions. In Section 5 we characterize the outcomes of repeated social interaction and discuss a weaker version of the Pareto principle. In Section 6 we discuss our results and some extensions. Proofs are gathered in Appendix A.",Aggregation of opinions in networks of individuals and collectives,https://www.sciencedirect.com/science/article/pii/S0022053121001228,24 June 2021,2021,Research Article,92.0
Banerjee Kuntal,"Department of Economics, College of Business, Florida Atlantic University, Boca Raton FL 33431, United States of America","Received 4 January 2022, Revised 29 June 2022, Accepted 11 July 2022, Available online 9 August 2022, Version of Record 9 August 2022.",https://doi.org/10.1016/j.jet.2022.105519,Cited by (1),"This paper corrects Theorem 1 in ====. An example of a preference intensity relation that has a preference intensity function representation but satisfies neither translocation consistency nor separability is presented. An equivalence between the class of binary relations satisfying weak order, reversal and lateral consistency and those that are representable by preference intensity functions is established.","This paper corrects the characterization of a class of preference intensity relations by ====. Theorem 1 of ==== establishes the equivalence between the preference intensity relations satisfying the axioms of weak order, reversal and translocation consistency and the existence of a preference intensity function representing such relations. Gerasimou also establishes the same equivalence with the axioms weak order, reversal, and separability. However, the theorem is incorrect – we show by means of an example (====, section ====) that a preference intensity function representation can exist without the underlying relation satisfying translocation consistency. This shows that the equivalence claimed in Theorem 1 of ==== between statements 1 and 3 is incorrect. Moreover, the same example also fails to satisfy separability showing that the equivalence between statements 2 and 3 is also incorrect. A new characterization of preference intensity relations that have a preference intensity function representation is provided.====Notation in this paper is consistent with ==== as such a reader familiar with the original work can simply skip ahead to sections ==== and ==== without losing much by way of content.",Corrigendum to “Simple preference intensity comparisons” [J. Econ. Theory 192 (2021) 105199],https://www.sciencedirect.com/science/article/pii/S0022053122001090,9 August 2022,2022,Research Article,94.0
Konishi Teruki,"Faculty of Economics, Kyoto University, Japan","Received 7 April 2022, Revised 14 July 2022, Accepted 15 July 2022, Available online 1 August 2022, Version of Record 1 August 2022.",https://doi.org/10.1016/j.jet.2022.105521,Cited by (0),This note reexamines the condition of Proposition 4 in ==== and then provides an alternative proof of the proposition without relying on any additional parameter conditions. The generalized version of Proposition 4 now shows that the dynamics in an economy with a finite patent length ==== involve nonmonotonic convergence of the equilibrium path.,"It is challenging to explore the dynamic properties of a growth model with a finite patent length. Because of the complexity of differential-difference equations, it is not always possible to solve a continuous-time growth model with a finite-period patent policy analytically. ==== construct a discrete-time endogenous growth model with a finite patent term and analyze the dynamic properties, under certain parameter conditions, with higher-order difference equations. This note reexamines the condition of Proposition 4 in ==== and then provides an alternative proof of the proposition without relying on any additional parameter conditions. The generalized version of Proposition 4 now shows that the dynamics in an economy with a finite patent length ==== involve nonmonotonic convergence of the equilibrium path.",Corrigendum to “Dynamic analysis of patent policy in an endogenous growth model” [J. Econ. Theory 132 (2007) 306–334],https://www.sciencedirect.com/science/article/pii/S0022053122001119,1 August 2022,2022,Research Article,95.0
"Dominiak Adam,Tserenjigmid Gerelt","Department of Economics, Virginia Tech, United States of America","Received 25 September 2019, Revised 30 March 2021, Accepted 8 April 2021, Available online 17 April 2021, Version of Record 12 January 2022.",https://doi.org/10.1016/j.jet.2021.105256,Cited by (4),"In this paper, we study choice under growing awareness in the wake of new discoveries. The decision maker's behavior is described by two preference relations, one before and one after new discoveries are made. The original preference admits a ==== representation. As awareness grows, the original decision problem expands and so does the state space. Therefore, the decision maker's original preference has to be extended to a larger domain, and consequently the new preference might exhibit ambiguity aversion. We propose two consistency notions that connect the original and new preferences. ==== requires that the original states remain unambiguous while new states might be ambiguous. This provides a novel interpretation of ambiguity aversion as a systematic preference to bet on old states than on newly discovered states. ==== requires that the relative likelihoods of the original states are preserved. Our main results axiomatically characterize a ==== (MEU) representation of the new preference that satisfies the two consistency notions. We also extend our model by allowing the initial preference to be MEU, and characterize reverse full-Bayesianism, which is an extension of the reverse Bayesianism of ==== to MEU preferences.","When modeling choice behavior under uncertainty, economists take for granted that the description of the underlying decision problem – including the states of nature, actions and consequences – is fixed. However, in many real life situations, a decision maker (henceforth, DM) makes new discoveries that change the decision problem. New scientific insights, novel technologies, new medical treatments, new financial instruments, or new goods emerge on an almost daily basis. Such discoveries might reveal contingencies of which the DM was unaware.==== As awareness grows, the DM's universe (i.e., state space) expands and this might affect her preferences. In this paper, we explore how the DM's beliefs and tastes evolve.====We provide a theory of choice under growing awareness in which a subjective expected utility (SEU) preference (Anscombe and Aumann, 1963) extends to a maxmin expected utility (MEU) preference (Gilboa and Schmeidler, 1989). While the extended preference captures ambiguity aversion, it inherits some properties of the initial SEU preference. Our theory provides a novel interpretation of ambiguity aversion. In particular, ambiguity arises because the DM treats new and old states differently; there is no exogenous information about states. In contrast, in the Ellberg experiments exogenous information about states is provided and ambiguity arises since the DM treats states with known and unknown probabilities differently.====To illustrate changes in beliefs due to growing awareness, consider a patient who suffers from a disease and needs to choose an appropriate treatment. There are two standard treatments, ==== and ====. Each treatment leads to one of two possible outcomes: a success or a failure. The patient believes that each treatment is successful with probability ====.====Suppose now that the patient consults her doctor and discovers that there is a new treatment ====, which may be successful or not. Since this treatment is new, the patient faces a new decision problem. How does she extend her original beliefs to evaluate the new treatment? First, the patient needs to form new beliefs about the conceivable outcomes of the new treatment in order to evaluate it. Second, the new discovery might causes the patient to reevaluate her original beliefs regarding the outcomes of the standard treatments ==== and ====.====Given the discovery of treatment ====, the patient's original preferences might change fundamentally. In particular, she may not be able to come up with a unique probability that treatment ==== will be successful, and therefore she may be cautious about the novel treatment. To discipline the effect of new discoveries, we consider two consistency notions between the patient's behavior before and after the discovery.====Behaviorally, our consistency notions can be described by the way the standard treatments are evaluated after awareness has changed. Our first consistency notion requires that treatments ==== and ==== are still evaluated with respect to a (unique) probability measure over old states, yet the measure might change. Under this consistency notion, the values of ==== and ==== might change after the discovery. However, our second consistency notion requires that the ranking over the old treatments does not change as awareness grows. In general, the two consistency notions are independent.==== Our goal is to axiomatically characterize each consistency notion by connecting the patient's initial and new preferences.====To study growing awareness formally, we introduce a general framework that can cover other widely studied approaches to model growing awareness such as Karni and Vierø (2013) (KV, henceforth) and Heifetz et al., 2006, Heifetz et al., 2008, Heifetz et al., 2013. Since we closely follow KV's framework, it is useful to contrast our approaches. They construct state spaces explicitly by invoking the approach of Schmeidler and Wakker (1990) and Karni and Schmeidler (1991): a state specifies the unique consequence that is associated with every act. Within this approach, KV have introduced an elegant theory of choice under growing awareness called ====. They focus on SEU preferences and characterize the evolution of probabilistic beliefs in the decision theoretic framework of Anscombe and Aumann (1963). However, under reverse Bayesianism, growing awareness does not affect the SEU form of preferences and thus the theory precludes ambiguity.====In contrast, our theory allows the DM's behavior to change fundamentally as awareness grows.==== While being originally a SEU maximizer, the DM might become ambiguity averse in an expanded universe. More specifically, the DM's behavior is described by two preference relations, one before and one after a discovery is made. The initial preference takes the SEU form. One can think of this assumption as follows: the DM is relatively familiar with the original decision problem and therefore she has come up with a (unique) probability measure over the states. As awareness grows, the extended preference admits a MEU representation, capturing ambiguity in an expanded state space.====Our main results behaviorally characterize the evolution of an original SEU preference to a new, extended MEU preference under both consistency notions. The first consistency notion, called ====, requires that the new events that correspond to the old states are revealed to be unambiguous by the new MEU preference; only the new states may be ambiguous.====An extended MEU preference that satisfies Unambiguity Consistency is characterized by a novel axiom called ==== (NUI) (see Theorem 1). It states that the DM can hedge against the ambiguity of the new acts (e.g., the new treatment ====). However, mixing them with the old acts (e.g., the standard treatments ==== or ====) cannot be used as a hedging strategy.====Under Unambiguity Consistency, new discoveries might affect the DM's ranking over the old acts since her probabilistic beliefs (as well as her risk preference) might change. For instance, after the discoveries of the new treatment ====, the patient might believe that success under treatment ==== is more likely than under treatment ====, leading her to strictly prefer the old treatment ==== over ====.====Our second consistency notion, called ====, requires that the new extended preference maintains the relative likelihoods of the original states. Preserving the original likelihoods might be reasonable in choice situations where the original preference is supported by hard facts or objective information. Likelihood Consistency is characterized by ==== (BAC), which requires that the DM's ranking of old acts (standard treatments ==== and ====) is not affected by growing awareness (Theorem 2).====Due to growing awareness the state space expands. We are particularly interested in two types of state space expansions, ==== and ====. Under refined expansions, the DM becomes aware of “finer descriptions” of the original states. However, under genuine expansions, the DM becomes aware of “completely new” states.==== Axiomatic characterizations of the two consistency notions do not distinguish these two separate cases explicitly. Regardless, the two cases can be behaviorally disentangled, which could be difficult when both preferences are SEU. When the new treatment ==== is discovered, the state space is refined: each original state is extended by indicating whether treatment ==== leads to a success or a failure. In this context, our theory implies that the old treatments ==== and ==== are unambiguous acts while the new treatment ==== is ambiguous. Consequently, an ambiguity averse DM tends to prefer the old, unambiguous acts to the newly discovered ones.====However, in the context of genuine expansion of the state space ambiguity aversion will be exhibited differently. Suppose that the patient discovers that the standard treatments ==== and ==== might cause a health complication. In this case, the original state space is extended by completely new states indicating whether treatments ==== and ==== lead to the health complication or not (i.e., genuine expansion). If the patient perceives ambiguity about the completely new states, then the standard treatments become ambiguous acts. Therefore, the patient prefers mixtures between the old treatments ==== and ==== over ==== (and ====).====At a fundamental level, our theory provides a novel interpretation of the widely-studied ambiguity phenomenon. Typically, as in the classical Ellsberg experiments, ambiguity is exogenously created. That is, subjects are informed about exogenous probabilities for some events in a given state space, and for other states such information is missing. The task is to elicit subjects' attitudes towards ambiguity. A systematic preference for betting on known probability events rather than betting on unknown probability events is understood as aversion towards the exogenous ambiguity.====In our theory, the DM perceives ambiguity about states of which she was originally unaware. In other words, an expanding universe can be seen as a “source” of ambiguity. As awareness grows, the DM cannot extend her old subjective belief uniquely so that her new beliefs are represented by a set of probability measures. Therefore, in our theory ambiguity aversion is displayed differently as a preference for betting on old, familiar states rather than betting on the newly discovered states.====To show that Likelihood Consistency and Unambiguity Consistency can be extended to the setting where one MEU preference evolves to another MEU preference, we extend our model by allowing the initial preference to be a MEU preference (see Section 5). This extension also illustrates that our framework can be extended to settings with more than two periods. Under the reverse Bayesianism of KV, the initial prior is the Bayesian update of the extended prior. Interestingly, under our extension of Likelihood Consistency, called ====, the initial set of priors is the full-Bayesian (i.e., prior-by-prior) update of the extended set of priors. Hence, our reverse full-Bayesianism is an extension of the reverse Bayesianism of KV to MEU preferences.====The rest of the paper is organized as follows. Section 2 presents the basic setup and illustrates how new discoveries expand the original state space. In Section 3, we discuss the SEU and MEU representations of the original and new preferences, and provide our definitions of consistent evolution of beliefs. In Section 4, we provide representation theorems that characterize our two consistency notions. In Section 5, we consider the case where the initial preference is MEU. In Sections 4.4 and 5.3, we study dynamic consistency of MEU preferences. In Section 6, we study a parametric version of our MEU representation. A brief overview of the literature on choice under (un)awareness is provided in Section 7. The proofs are collected in Appendix A.",Ambiguity under growing awareness,https://www.sciencedirect.com/science/article/pii/S0022053121000739,17 April 2021,2021,Research Article,96.0
"Esponda Ignacio,Pouzo Demian,Yamamoto Yuichi","UC Santa Barbara, United States of America,UC Berkeley, United States of America,Hitotsubashi University, Japan","Received 3 June 2022, Revised 21 June 2022, Accepted 28 June 2022, Available online 29 July 2022, Version of Record 29 July 2022.",https://doi.org/10.1016/j.jet.2022.105513,Cited by (0),This note corrects errors in ====.,None,Corrigendum to “Asymptotic behavior of Bayesian learners with misspecified models” [J. Econ. Theory 195 (2021) 105260],https://www.sciencedirect.com/science/article/pii/S002205312200103X,29 July 2022,2022,Research Article,97.0
Szőke Bálint,"Federal Reserve Board, United States of America","Received 25 September 2019, Revised 23 December 2020, Accepted 18 February 2021, Available online 1 March 2021, Version of Record 12 January 2022.",https://doi.org/10.1016/j.jet.2021.105225,Cited by (1),I estimate and evaluate a model with a representative agent who is concerned that the persistence properties of her baseline model of consumption and ,"Survey expectations are commonly thought to provide valuable information about ‘subjective’ beliefs.==== Interestingly, these measures often display large and systematic differences compared to ‘objective’ forecasts from estimated statistical models. By way of illustration, Piazzesi et al. (2015) provide evidence that financial forecasters' expectations about future interest rates are formed as if the forecasters believed that the dynamics of yield curve were more persistent than they appear with hindsight. Although findings like this are important first steps to understanding belief formation, they leave unanswered the question: why do beliefs deviate from the observable dynamics?====Building on the robust control model of Hansen et al. (2020), this paper offers a potential answer by presenting a particular mechanism of belief formation and proposing a general method to evaluate its plausibility as an explanation for the seeming inconsistencies between survey expectations and realized time series. Robust control theory supplies a model of subjective beliefs, providing testable predictions about how these beliefs deviate from forecasts derived from fitted statistical models. In particular, model implied beliefs distort physical probabilities by overweighting states with adverse utility consequences, yielding expectations that are ==== relative to the observable state dynamics.====Importantly, while this theory departs from rational expectations, it provides a set of powerful cross-equation restrictions between the decision maker's preferences, beliefs, and environment. Studying an endowment economy with a robust representative household that faces an endowment stream subject to long-run risk, I use these restrictions along with data on interest rates, consumption, and inflation to estimate the model parameters. In doing so, I switch viewpoint and consider robust control theory as a particular model for the stochastic discount factor. Using the estimated model, I derive implied interest rate expectations and contrast them with analogous forecasts from the ==== (BCFF) survey. Assuming that these are good proxies for beliefs allows me (1) to test the model's prediction about pessimistic beliefs, and (2) to gain insights into how expectations are formed.====I find that the model implied belief well approximates the average forecast bias of professional forecasters for various maturity and forecast horizons. The model can also reproduce the finding of Piazzesi et al. (2015) that, under the subjective (or survey) belief, both the level and the slope of the nominal yield curve appear to be more persistent than what the observed yields suggest. On the other hand, the volatility of model implied expected yield changes falls short of the survey analogues, which indicates that the increased persistence that I estimate from observed prices might be somewhat excessive from the viewpoint of the surveys.====My decision maker acknowledges that her ==== can only approximate her environment, so, rather than using a single model, she considers a ==== of alternatives that are difficult to distinguish from the baseline. Seeking a decision rule that is robust to these alternative models, she ends up discovering a ==== to which she responds optimally. Being a probability distribution that justifies the robust decision rule as a best response to beliefs, this worst-case model can be interpreted as the decision maker's subjective belief. Hansen and Sargent (2001) represent a set of models as a collection of likelihood ratios whose relative entropies with respect to a baseline are bounded by a ==== parameter encoding the decision maker's desired ====. Hansen et al. (2020) refine this setting by introducing a new object, ====, a nonnegative state dependent function, meant to guarantee that certain parametric models are included in the set. Twisting the set toward parametric alternatives embodies the decision maker's ====.====I use a multivariate extension of their model with a quadratic ==== that is well-suited to direct the agent's misspecification concerns to the ==== of the baseline dynamics. As a comparison, I also consider the special case of constant ====, which gives back the original Hansen and Sargent (2001) model with ==== that has well-known close connections with recursive utility of Duffie and Epstein (1992). The question then arises as to how important the state dependence of ==== might be. I demonstrate that it is essential to replicate robust features of the interest rate data. In particular, however large I set the risk aversion parameter, recursive utility with a unitary elasticity of substitution cannot generate an upward sloping average nominal yield curve—at least when the agent's belief is described by a (well-fitting) statistical model assumed in this paper.====In contrast, by allowing for state dependence in ====, the model not only replicates the observed average nominal yield curve, but it also generates substantial fluctuations in nominal yields with long maturities, features that are strongly backed by the data, but that a constant ==== fails to account for. Fluctuations in long-term yields are implications of state dependent, counter-cyclical market prices of model uncertainty==== that emerge from the agent's concerns that the persistence of the baseline dynamics is misspecified. The estimated worst-case suggests that she is most worried that inflation is more persistent and that the correlation between consumption growth and lagged inflation is more negative than in her baseline model. This invites a reinterpretation of the ‘bad news’ channel of Piazzesi and Schneider (2007) in terms of model uncertainty.====Combining recursive utility with sufficiently high risk aversion and a particular long-run risk feature of their data set, Piazzesi and Schneider (2007) obtain an upward sloping nominal yield curve. Underlying this result is their finding that inflation predicts low future consumption growth. My failure to replicate a positive term premium with recursive utility follows from the fact that the forecasting ability of inflation is more modest in my sample, signaling its ==== in the data.==== Misspecification concerns about that feature therefore seem plausible and indeed, this is what I find by looking at the data through the lens of the estimated worst-case model. One advantage of this interpretation, emphasized by Barillas et al. (2009), is that implausibly large risk aversion parameters can be replaced with plausible concerns about robustness to (specific kinds of) misspecifications. Calculating a detection error based measure associated with the estimated worst-case model, I show that the agent's belief is indeed reasonable in the sense that it is not easily rejectable by the data.====  The paper contributes to a literature that aims to estimate and empirically evaluate models with robustness, that is, models in which (1) agents exhibit uncertainty aversion, and (2) the set of priors is tightly constrained by statistical discrimination based considerations. Prominent examples are Hansen et al. (1999), Cagetti et al. (2002), Bidder and Smith (2012), Bhandari et al. (2019), and Hansen and Sargent (2020). More broadly, this paper relates to a literature focusing on the quantitative importance of ambiguity aversion in macroeconomics with a prime example being Ilut and Schneider (2014). Although similar in spirit, their paper differs from mine in that it is built on the alternative decision theoretic framework of Epstein and Schneider (2003), which gives rise to more permissive restrictions on the agent's set of priors than those used here.====The closest to my paper is the work by Bhandari et al. (2019). Similar to the present setting, they extend the model of Hansen and Sargent (2001) in ways that imply time variation in the worst-case drift distortion, and interpret the worst-case as a subjective belief, which enables them to utilize survey expectations in their analysis. However, instead of using surveys to evaluate the model's predictions, they use them to identify exogenous variations in the degree of ambiguity. Another key difference is that they estimate a full-blown general equilibrium model with endogenous consumption. While this allows them to investigate the impact of robustness on aggregate macro dynamics, due to the complexity of the model, they must rely on approximations. In contrast, the endowment economy and my functional form assumptions guarantee a tractable linear quadratic framework, at the cost of limiting my focus to how robustness affects asset prices and beliefs.====Similar to this paper, Hansen and Sargent (2022) consider a decision maker who entertains multiple parametric models (so called “structured models”), but they construct the set of models relative to which the agent seeks robustness in a different way. Instead of restricting the drift distortion process by imposing a (state-dependent) upper bound on its intertemporal average as I do, they restrict the set of (structured) models by introducing an instant-by-instant constraint in order for this set to be rectangular in the sense of Epstein and Schneider (2003). Like my setting, their formulation gives rise to interesting state-dependence in uncertainty prices, but the instant-by-instant constraint renders the computation of the worst-case model difficult. In contrast, my formulation leads to a tractable non-linear state space model that I can readily estimate with maximum likelihood.====An important feature of my paper's decision problem is that the set of models is considered to be a result of careful deliberation, that the decision maker does not seek to improve over time through learning.==== Rather than struggling with slow learning and fully embracing one of the ‘wrong’ models, my decision maker designs a decision rule that works well under a set of models. Hansen and Sargent (2007) and Hansen and Sargent (2010) incorporate learning into robust decision problems. Similar to my formulation, Hansen and Sargent (2010) study an investor who entertains two parametric models of consumption growth, one with substantial growth rate persistence, the other with little such persistence. However, instead of using these models to construct a set of distributions around a baseline which is twisted toward particular parametric misspecifications, the investor in Hansen and Sargent (2010) uses observations on consumption growth to update her posterior over the two models and expresses her misspecification concerns by pessimistically tilting that posterior distribution. This pessimistic tilting leads to countercyclical uncertainty prices and makes the investor act as if good news is temporary and bad news is persistent. Estimating and evaluating the asset pricing implications of their model is an interesting challenge, which is beyond the scope of this paper.====A well-known difficulty of the long run risk model of Bansal and Yaron (2004) is that in order to match key asset pricing moments, it relies on a highly persistent predictable component in aggregate consumption and dividend growth rates, even though the estimated persistence levels of these growth rates are typically much lower. In addition, the assumed high persistence leads to several counterfactual implications for consumption growth—excessive higher-order autocorrelations of consumption growth or excessive predictability of future consumption growth by the market-wide price-dividend ratio. The robust control model of this paper is not subject to these criticisms, because the model which is key for asset pricing (worst-case model) is in principle different from a model that fits the consumption series well (baseline model). In this sense, my paper is very much in the spirit of Bidder and Dew-Becker (2016) who offer a reinterpretation of the long run risk channel in terms of model uncertainty.====  The rest of the paper is structured as follows. Section 2 summarizes the essential features of robust preferences and the particular refinement used in this paper. It also discusses the functional form assumptions and solves the planner's problem to derive formulas for the equilibrium yield curve. Section 3 proposes a two-step maximum likelihood procedure to estimate the model parameters, reports details about the estimation and discusses the results. In Section 4, I compare model implied beliefs with survey expectations using some informative moments. Section 5 concludes with remarks on possible extensions. The appendices contain derivations and further results.",Estimating robustness,https://www.sciencedirect.com/science/article/pii/S0022053121000429,1 March 2021,2021,Research Article,98.0
"Filiz-Ozbay Emel,Gulen Huseyin,Masatlioglu Yusufcan,Ozbay Erkut Y.","University of Maryland, Department of Economics, United States of America,Purdue University, Krannert School of Management, United States of America","Received 30 September 2019, Revised 10 January 2021, Accepted 18 February 2021, Available online 25 February 2021, Version of Record 12 January 2022.",https://doi.org/10.1016/j.jet.2021.105224,Cited by (1),"We investigate experimentally preferences between different ambiguous processes generated by two-color Ellsberg urns. By providing symmetric information on urns with different numbers of beads and keeping the information on the most optimistic, pessimistic, and equal probability of winning possibilities the same, we elicit subjects' preferences for the size of an ambiguous urn. Subjects prefer the bets from the ambiguous urns with more beads. We analyze the role of ambiguity aversion and ratio bias of subjects in this behavior. We study the restrictions that our findings impose on the existing ambiguity models.","In a two-color urn thought experiment of Ellsberg (1961), a decision maker (DM) prefers betting on an urn with 5 Black and 5 White beads (the risky urn) rather than an urn with a total of 10 Black and White beads with an unknown composition (the ambiguous urn). Ellsberg's thought experiment has been widely confirmed in numerous laboratory experiments (see e.g., Camerer and Weber, 1992 and Machina and Siniscalchi, 2014 for detailed surveys). In order to explain such behavior, normatively or prescriptively appealing theories of ambiguity have emerged (see Gilboa and Schmeidler, 1989, Schmeidler, 1989, Ergin and Gul, 2009, Klibanoff et al., 2005, Neilson, 2010, and Seo, 2009 and see also Machina and Siniscalchi, 2014 for a survey of ambiguity models). These models commonly predict preferences for betting on known distributions rather than unknown ones.====In this paper, we investigate whether the number of beads in the Ellsberg's two-color urn matters. Note that none of the well-known theories of ambiguity is crafted to make a prediction regarding the urn size. Consider a DM who decides to place a bet between two ambiguous Ellsberg's two-color urns, one with ==== beads and another with ==== beads with ====, for which the composition of Black and White beads in either urn is unknown. Other than the total number of beads in each urn, no information about the urns is provided to the DM. Would the DM prefer the urn with ==== or ==== beads, or would she be indifferent between them?====Our laboratory experiments investigate size effect under ambiguity and its interaction with the ambiguity attitude and ratio bias. We elicit subjects' preferences between ambiguous urns with different sizes, risky urns with different sizes, and risky and ambiguous urns with the same size. See Fig. 1 for the illustration of how this design elicits different types of size preferences. Our results indicate that there is a preference for larger size when comparing ambiguous urns; and the preference for the larger urn is mainly driven by ambiguity averse subjects. Some subjects exhibited preferences for the urn size even when they compare two risky urns with different sizes but equal chances of winning on each; indicating ratio bias. We further study in detail the contribution of ambiguity aversion and ratio bias to the preferences for a larger urn under ambiguity.====The DM may be indifferent between betting on ambiguous urns with different sizes, as the provided information on each urn is the same, or, alternatively, she may prefer betting on one ambiguous urn rather than the other, as she may think that her chance of winning on one of them is higher. For example, say the colors of the beads in an urn are determined by throwing them into two adjacent black and white paints from a distance of 20 feet, as in the example of Einhorn and Hogarth (1985). Given this process, the composition with all black beads or all white beads may be more “probable” in a 2-bead urn than in a 1000-bead urn, i.e., a DM may think that there must be at least some beads painted in the color that she had bet on among those 1000 beads. Hence, an ambiguity averse DM may exhibit a preference for a larger urn. Alternatively, the DM may dislike having a large number of possibilities in an uncertain situation (see Einhorn and Hogarth, 1986). While in a risky urn with two beads and 50% chance of winning, there is only one possibility of color composition (i.e., 1 black and 1 white beads), in an ambiguous urn with 2 beads there are 3 color composition possibilities. Similarly, in an ambiguous urn with 1000 beads, there are 1001 possibilities. It may be harder for the DM to contemplate 1001 possibilities than 3 possibilities, and hence, a larger urn may be perceived as less desirable. In that case, an ambiguity averse DM may exhibit a preference for a smaller urn. In this paper, we elicit subjects' preferences on Ellsberg's two-color urns with different number of beads. If the urn size is playing a role in the decision, this information, as a measure of the size of the ambiguous state space or complexity of the source of ambiguity, can be incorporated into the ambiguity models.====In many decision problems, individuals decide among different ambiguous situations where the size of the possible possibilities may have an effect on the perception of ambiguity. For example, imagine a DM selecting a day laborer outside a home improvement retailer, where workers congregate, for a job that does not require many qualifications (see Valenzuela, 2003 for the U.S. day laborer market). Suppose that each worker is either good or bad. The DM does not know the distribution of workers' types, and she will pick the first worker in the line at the retail location. If more workers gather at one retail location than another, would the size of the crowd (as the size of an Ellsberg urn) matter for the DM even if this information did not indicate anything about the chance of getting a better service? Alternatively, consider a situation where a prize is randomly assigned to a lucky winner, similar to Charlie's decision problem in the story of “Willy Wonka & the Chocolate Factory.” Charlie wants to buy a Wonka chocolate bar because some of these bars include a Golden Ticket for a full tour of a mysterious chocolate factory as well as a lifetime supply of chocolate. Charlie may go either to a small store that carries few Wonka Bars or to a giant store that carries a lot of Wonka Bars. Which store would Charlie prefer to purchase his Wonka Bar? Our experiment explicitly addresses this type of questions in a context-free environment, focusing on the number of different compositions of winning/losing possibilities controlling for all other effects.====Based on our experimental findings, we revisit the existing models and discuss what our results impose on these models. We argue that multi-prior models and source models are too flexible and they can explain any behavior in our setup. On the other hand, two-stage models such as the smooth ambiguity model of Klibanoff et al. (2005) can accommodate our data on size effect under ambiguity and its interactions with ambiguity attitude.==== Finally, we provide some calibrations based on the smooth ambiguity model for the size premium under ambiguity in order to quantify our findings.====There are other ambiguity experiments that require subjects to compare different ambiguous processes (see Pulford and Colman, 2008, Halevy and Feltkamp, 2005, Abdellaoui et al., 2011, Epstein and Halevy, 2018, and Chew et al., 2017). Among those, Chew et al. (2017) is the most closely related design to ours as they also vary the possible compositions of the ambiguous urns and their decision problems complement ours. We discuss these experiments in detail in Section 5. Pulford and Colman (2008) repeated the standard Ellsberg experiment varying the urn sizes. Although our focus is to understand the preferences between two ambiguous urns with different sizes, we also ask the standard Ellsberg questions varying the urn size. We confirm their findings for 2- and 10-bead urns. However, when the size of the urn is very large, particularly 1000 - a size that they do not investigate-, we find that the percentage of subjects choosing the risky urn is significantly smaller. We discuss the differences between our design and theirs in more detail in Section 5.====Finally, we conduct an empirical investigation to assess the extent to which our evidence on the set of possible compositions of an ambiguous urn being a relevant component of decision exists in an application where ambiguity models are often used. Specifically, we investigate whether investors take the number of assets held by a mutual fund into account while investing. The resolution of uncertainty regarding a fund's performance is admittedly not identical to our abstract two-color Ellsberg urn in the experiment. However, in loosely speaking, one may argue that the number of assets held by a mutual fund may be considered as a measure for the size of the ambiguous state space in this environment. Controlling for the commonly used determinants of fund flows, we find that investors prefer mutual funds with more holdings even though this does not result in better investment performance.====The paper is organized as follows. Section 2 describes our experimental design, and Section 3 presents the results. Section 4 discusses some existing models in the context of our decision problems. Section 5 summarizes other experiments that are closely related to ours. Section 6 discusses the relevance of the number of assets in a mutual fund in an investment decision based on an empirical analysis. Section 7 concludes. Additional analysis that is not included in the main text is presented in Appendix A, B, and C. The instructions for the experiment are presented in Appendix D.",Comparing ambiguous urns with different sizes,https://www.sciencedirect.com/science/article/pii/S0022053121000417,25 February 2021,2021,Research Article,99.0
"Szydlowski Martin,Yoon Ji Hee","University of Minnesota, 321 19th Ave S, Minneapolis, MN 55455, USA,University College London, Department of Economics, Drayton House, 30 Gordon St, Kings Cross, London WC1H 0AX, United Kingdom","Received 13 September 2019, Revised 17 December 2020, Accepted 18 February 2021, Available online 25 February 2021, Version of Record 12 January 2022.",https://doi.org/10.1016/j.jet.2021.105229,Cited by (1),"We study a continuous-time principal-agent model in which the principal is ambiguity averse about the agent's effort cost. The robust contract generates a seemingly excessive pay-performance sensitivity. The worst-case effort cost is high after good performance, but low after bad performance, which leads to ==== and ==== respectively and provides a new rationale for performance-sensitive debt. We also characterize the agent's incentives when the contract is misspecified, i.e., he is offered the robust contract, but his true effort cost differs from the worst case. Then, termination can induce shirking, the strength of incentives is hump-shaped, and agents close to firing prefer riskier projects, while those close to getting paid prefer safer ones. This feature resembles careers in organizations, most notably risk-shifting and the quiet life.","Firms need to align the interests of managers and shareholders, but the environment they operate in is prone to change. Events such as the entry of competitors, the arrival of a new technology, or a shift in customer demands are often impossible to anticipate. Their impact on an individual manager's productivity and his daily duties can be intricate, which makes it difficult to quantify and to communicate to others.==== Consequently, firms write employment contracts with little knowledge about how the relationship with the manager and the contract's ability to provide incentives may change in the future.====In this paper, we model the firm's lack of knowledge as ambiguity and we develop a theory of robust dynamic contracting. The firm (the principal) does not know the arrival times, the impact, or the probability distribution of shocks to the environment. It only knows the range which particular realizations may take, and offers the manager (the agent) a contract which provides incentives and maximizes its payoff under the worst case. We characterize the impact of ambiguity aversion on the shape of the optimal contract, and derive the dynamics of the worst-case process. Our main example is when the ambiguity is about the manager's effort cost. For instance, the difficulty of his work could vary depending on the competitive pressure faced by the firm or his own abilities may change over time.====The optimal contract has several novel features, which are all driven by the dynamics of the worst case effort cost. First, the contract is divided into an over- and an undercompensation region. After sufficiently high performance, the worst case effort cost is high, and, in expectation, the agent receives higher payouts than in the case without ambiguity. After low performance, the worst case is that his effort cost is low, and the agent is undercompensated. This result is driven by the dynamics of the worst case, which changes depending on how close the contract is to termination. Hence, our paper provides a new explanation for why high performing managers receive seemingly excessive compensation, a question which has received significant attention in both the popular press and the academic literature.==== In our setting, the reason is not managerial power (e.g. Zwiebel (1996)), board capture (e.g. Hermalin and Weisbach (1988)), or the exploitation of information rents, (e.g. Jensen (1986)), but the principal's ambiguity aversion.====Since both over- and undercompensation regions arise in the same optimal contract, our model generates career trajectories. In expectation, a manager with a good track record will continue to receive excessive compensation unless he encounters bad luck in the form of a path of negative outcomes. Although a manager with a bad record can expect to move up to the overcompensation region as long as he exerts effort, the likelihood of advancement is lower than without ambiguity, while the chance of being fired is higher. Similarly, a young manager starts out undercompensated but on average reaches the overcompensation region once enough time has passed. The speed of this advancement depends on his performance record. High performing managers reach the overcompensation region faster, while low performing ones may reach it more slowly and therefore remain undercompensated for long periods of time, or get fired. These features resemble seniority and entrenchment, but they arise because of the interaction of ambiguity aversion and incentive provision.====Ambiguity aversion also introduces a disconnect between the manager's pay-performance sensitivity (PPS) and his current effort cost. In a dynamic contract without ambiguity, the pay-performance sensitivity is proportional to the current effort cost.==== In our setting, the firm is forced to set the pay-performance sensitivity at the highest level at all times, since it expects the manager to shirk under the worst case otherwise. Essentially, ambiguity aversion generates a precautionary motive for the principal and the manager receives excessive incentives, compared to both the contract without ambiguity and to his realized effort cost. This result offers a new answer to the puzzle raised by Murphy (2003). They find that many managers receive stock options, even when their individual impact on the firm does not seem to be large enough to warrant them.====Finally, the optimal contract can be implemented with performance-sensitive debt. A changing interest rate is necessary because the drift of the agent's continuation value changes under the worst case. The performance sensitive debt is hence used to adjust the firm's cash flows in the over- and undercompensation regions. This interpretation is new and differs from current justifications for performance sensitive debt such as Piskorski and Tchistyi (2011). The implementation consisting of equity and credit lines in DeMarzo and Sannikov (2006) is no longer optimal.====Our notion of ambiguity corresponds to “Type I Ambiguity” in Hansen and Sargent (2012). In that paper, a Ramsey planner does not know the “true model,” and believes that the private sector knows the true model. For any policy, the planner evaluates ==== her own payoffs and the private sector's incentives under the worst-case model, which reflects her concerns about robustness and her belief that the private sector knows the true model. The analog holds in our paper. The agent knows the true evolution of the effort cost (represented by a probability measure) and the principal chooses a contract which maximizes her payoff under the worst-case measure. As in Hansen and Sargent (2012), both the principal's and the agent's payoffs are evaluated under the worst-case measure. This setting has an intuitive interpretation as a game between a principal, an agent, and a malevolent nature. Each agent is endowed with an effort cost process, which represents how well his skills are suited to the principal's project. The principal posts a contract, but is uncertain about which agent she is being matched with. A malevolent nature chooses the match between principal and agent, to minimize the principal's value.====Alternatively, we can understand the contract as being optimal under the principal's ==== preferences. Then, the contract may fail to provide incentives if the true measure differs from the principal's worst case (see Hansen and Sargent (2012), p. 432 for a discussion). We study this interpretation in Section 5. There, we alter the model and assume that the agent evaluates his payoffs under a reference measure, while the principal remains ambiguity averse. Generally, the reference measure differs from the principal's worst case, so the principal and agent disagree about the evolution of effort costs. This setting corresponds to “Type II Ambiguity” in Hansen and Sargent (2012).====This alternative setting yields novel predictions. Contrary to a long line of literature on dynamic contracts,==== termination does not motivate the agent to work and instead may induce shirking. Intuitively, the agent's value from exerting effort depends on the payments he is promised to collect in the future. However, when the firing probability is high, the agent is unlikely to collect on the promised rewards, and thus the value of working is low. If the effort cost under the true measure is sufficiently high, the agent shirks. Importantly, shirking occurs even though the pay-performance sensitivity is high.====When the agent shirks, his continuation value drifts downwards. Thus, he expects to be fired as time passes and only a sequence of lucky realizations of the Brownian noise allows him to collect payments. In line with this intuition, agents who shirk are risk-loving. That is, they prefer a project with higher volatility in output, or equivalently a contract with a higher PPS. By contrast, agents who exert effort are risk averse. They can expect to enter and then to stay in the overcompensation region forever if the noise in the output process is sufficiently small, and thus they will always prefer to bear less risk. These results are again driven by the principal's ambiguity aversion. Without ambiguity, by contrast, the agent is always risk-neutral in the PPS. Thus, the misspecified contract provides incentives for risk-shifting at the bottom (Jensen and Meckling (1976)) and conservatism at the top (Bertrand and Mullainathan (2003)). That is, if the agent could select the riskiness of the project, he would choose a risky project when his continuation value is low, and a safe project when his continuation value is high.====Our results crucially depend on the fact that ambiguity is about the effort cost. To show this, we study ambiguity about the firm's average productivity in Section 6.1. The features we have described above do not appear, because ambiguity about the productivity does not interact with providing incentives. As a result, the worst-case productivity is static and at the lowest possible level. The PPS is proportional to the agent's effort cost and there are no over- and undercompensation regions. In the misspecified contract, the agent still exerts effort and the implementation of DeMarzo and Sannikov (2006) is optimal. In both the main model and Section 5, the principal is ambiguity averse while the agent is not. In Section 6.3 we extend our model so that ==== the principal and the agent are ambiguity averse. While agent's worst-case differs from the principal's worst-case, the results are qualitatively similar to the ones in Section 5.",Ambiguity in dynamic contracts,https://www.sciencedirect.com/science/article/pii/S0022053121000466,25 February 2021,2021,Research Article,100.0
Hill Brian,"GREGHEC, HEC Paris & CNRS, France","Received 12 September 2019, Revised 25 November 2020, Accepted 29 January 2021, Available online 8 February 2021, Version of Record 12 January 2022.",https://doi.org/10.1016/j.jet.2021.105209,Cited by (4),"This paper develops a belief update rule under ambiguity, motivated by the maxim: in the face of new information, retain those conditional beliefs in which you are more confident, and relinquish only those in which you have less confidence. We provide a preference-based axiomatisation, drawing on the account of confidence in beliefs developed in ","Reasons for going beyond the Bayesian representation of beliefs by probability measures abound. Whether it be decision makers' observed non-neutrality to ambiguity (Ellsberg, 1961), the purported unjustifiability of the Bayesian requirement of belief precision (Gilboa et al., 2009, Gilboa et al., 2012; Bradley, 2014) or the difficulty of forming warranted beliefs satisfying the Bayesian tenets in real decisions (Cox, 2012; Gilboa and Marinacci, 2013), many have argued for non-probabilistic representations of belief. But how should such non-Bayesian beliefs be updated?====Two sorts of situations pose particular challenges for the update of non-Bayesian beliefs: complete ignorance and surprising events. A venture capitalist faced with an entirely new drug, based on a never-tested, revolutionary technology, is in a situation of ====. In terms of the popular multiple prior model (Gilboa and Schmeidler, 1989), her beliefs concerning the probability of cure under the drug can be characterised by the set of all (relevant) priors, i.e. the set ====. Yet a central determinant of her investment decision will be how she updates on the basis of early tests giving cure rates on relatively small samples. For an example of ====, consider an investor wedded to a fine-tuned model of the stock market that has great difficulty explaining, say, the events in a financial crisis—the opening quote, given fours days after BNP Paribas suspended redemptions from three funds, is an example from the 2008 financial crisis. Such an investor typically has precise probabilistic beliefs about future asset returns and prices (conditional on their past values), which give extremely low probability to transpired events; the issue is how to update them. Going by the growing body of evidence that ambiguity increases in a financial crisis (e.g. Caballero and Krishnamurthy, 2008; Ilut and Schneider, 2014), a common reaction is to keep an open mind about potential alternative models, and accordingly revert to more open sets of probabilities over future asset prices.====Despite the importance of these situations for motivating non-Bayesian beliefs (e.g. Gilboa et al., 2009; Levi, 1974), existing accounts of update struggle to adequately capture learning in them. On the one hand, the main update rules for multiple priors tend to deal with complete ignorance in an ‘extreme’ way (Gilboa and Marinacci, 2013), for instance by retaining the completely ignorant ==== priors in the previous example, or jumping directly to a perfectly precise posterior belief (see Section 4.1). Neither seems to reflect how a typical venture capitalist would, or should, react. On the other hand, multiple prior update rules typically coincide with Bayesian conditionalisation when priors are precise, and hence suffer from the same issues concerning updating on surprising events—and ==== on null events—which plagues the Bayesian account (e.g. De Bondt and Thaler, 1985, De Bondt and Thaler, 1987). They tell the investor in the opening quote to stick to his original model in the face of the conflicting evidence, rather than keeping an open mind.====The current paper takes up this double challenge, developing and behaviourally characterising a novel account of the update of non-Bayesian beliefs. It is, to our knowledge, the first to cope comfortably with both complete ignorance and surprising events (see Table 1). Moreover, it is general enough to recover ‘classic’ multiple prior update rules, such as Full Bayesian and Maximum Likelihood update, as well as more recent suggestions, including that used by Epstein and Schneider (2007), as special cases. As such, it provides a unified analysis of them. It also recoups existing approaches to updating on null events, such as Conditional Probability Systems, as a special cases, whilst preserving a strong relationship between ex ante and ex post preferences, which they lack. In so doing, our account provides new perspectives on these existing approaches. Finally, the account also has solid normative credentials, being built on a reasonable story about how beliefs should be updated in the face of contrary evidence.====Conceptually, our account taps into an intuition as to ==== beliefs may be non-Bayesian: decision makers may be more or less ==== in different beliefs. This ‘second-order’ aspect is something that the Bayesian model has trouble rendering properly, whilst it can be captured, and related to preferences, in some non-Bayesian models (e.g. Hill, 2019). Formally, we represent confidence in beliefs using ==== (Hill, 2013). A confidence ranking is a nested family of sets of probability measures, where different sets are understood as representing the beliefs, or probability judgements,==== held by the decision maker at different levels of confidence. The larger a set of probability measures, the fewer probability judgements hold for all measures in it; so ==== sets in a confidence ranking involve ==== beliefs in this sense, and accordingly correspond to ==== levels of confidence. Structures of this sort have long been employed in econometrics (e.g. Manski and Nagin, 1998; Manski, 2013).====Whilst previous work has connected confidence to preferences (Hill, 2013, Hill, 2019), the account developed here recognises that it ==== has a role to play in update. Put succinctly: in updating beliefs, ==== those conditional beliefs in which you are ==== confident, and relinquish only those in which you have less confidence. Formally, this is reflected in the following update rule (see Section 2.4 for details): given prior confidence ranking Ξ, on learning the event ====, the posterior confidence ranking is==== where ==== is a decreasing function and, for every set of probability measures ==== and event ====, ==== is the well-known Full Bayesian update defined as follows:====The ==== function ==== assigns a probability value to every set in the confidence ranking, and hence implicitly to every confidence level. In so doing, it effectively specifies a set of probability measures for each confidence level, namely those which assign ex ante probability to ==== greater than the ====-value for that level. These can be thought of as representing the conclusions the decision maker is warranted to deduce from the observation of ==== with that much confidence: any probability measure giving a value to ==== that is less than this threshold ‘gets it too wrong’ to be considered plausible at that confidence level. Since ==== is decreasing, this set is larger for higher confidence levels: the conclusions that can be drawn from the data with high confidence are weaker than those that can be drawn with lower confidence. For instance, the observation of drug trials on 100 patients, out of which 75 were cured, warrants high confidence that the probability of cure for a randomly selected new patient is 0.25 or higher, but more limited confidence that this probability is 0.60 or higher. Probability thresholds are reminiscent of significance levels in hypothesis testing, and indeed there is a sense in which the proposed update rule retains the spirit of classical statistical reasoning (see Section 5).====Update rule (1) is visually illustrated on Fig. 1. For every confidence level, the prior beliefs held at that level are represented by the appropriate set of probability measures in Ξ, whereas the conclusions that can be drawn from the data with that level of confidence are summarized by the set of probability measures singled out as ‘reasonable’ by ====. If these are compatible—if the two sets of probability measures overlap—the update rule (1) retains all of these as posterior beliefs at that confidence level—it takes the intersection. This corresponds to the maxim that conditional beliefs held or conclusions drawn with high confidence are, as far as possible, retained. By contrast, at lower confidence levels, where the (more precise) initial beliefs may contradict the (stronger) conclusions drawn from the data with that much confidence, neither are retained. In cases of conflict with observation, it is the beliefs held with low confidence, if any, that are withdrawn. Finally, the update rule conditions the structure obtained on the learnt event ====.====The update rule can be illustrated on the ‘surprising events’ investor example above. Suppose that the investor was using a model specification====say, a set of dynamic stochastic equations—==== and a vector of relevant parameter values ==== to determine the stochastic process relating future vectors of asset returns and prices ==== to previous ones ====.==== Letting, for any model specification ==== and parameter values ====, ==== be the conditional probability measure over future values of ==== in solution,==== the investor's initial beliefs are represented by ====. However, this gives no information about how confident he is in these beliefs, nor about his relative confidence in the model specification ==== as compared to the parameter values ====. Suppose, for the purposes of illustration, that he is more confident in the parameter values ==== than in the model specification ====. His confidence in beliefs can thus be modelled by a three-level confidence ranking, ====, where ==== is a set of models containing ==== and ==== is a set of possible parameter settings containing ====.==== According to the bottom element of this confidence ranking, asset returns and prices are determined by the model ==== with parameters ====: this captures the stated ex ante beliefs, independently of the confidence with which they are held. At the next level up, all probability measures in the set correspond to the same parameter values ====, but different model specifications. This captures a judgement that he is more confident in the former than the latter. Finally, neither the belief about the model nor that concerning the parameters are retained at the highest confidence level. This confidence ranking is drawn in black in Fig. 1.====On observing an economic event ==== (e.g. ==== or a set of possible values for ====, given the sequence ====), the investor can assign to each confidence level a probability threshold, determining which measures can be ruled out as ‘having got the prediction about ==== too wrong’ with that much confidence. For instance, he could apply thresholds of 0.05, 0.01 and 0.001 for the low, medium and high confidence levels respectively.==== The sets of probability measures giving a probability to the observation higher than the threshold are shown in red in Fig. 1.====The confidence ranking resulting from update rule (1) is indicated by the blue shaded sets in Fig. 1.==== In this example, at the top two confidence levels, the intersection of the sets is non-empty, and these yield the posterior beliefs. It follows that all the prior conditional beliefs are retained—for instance, at the medium confidence level, the belief in parameter values ==== is retained. Perhaps new beliefs are added—in the example, the posterior beliefs at the medium confidence level are more precise than they were ex ante, corresponding to the investor retaining some alternative model specifications as potentially more reasonable. The probability measure at the bottom confidence level in the prior confidence ranking assigns a probability to ==== that is below the corresponding threshold, so the beliefs specific to that level are dropped on learning. In the face of what, under his prior beliefs about the model and parameters, is a 25-standard deviation event of the sort in the opening citation, the investor retracts those beliefs in which he is least confident: in this case, those concerning the model specification. The only beliefs held at the low confidence level ex post are those inherited from higher confidence levels.====Note that the posterior beliefs held at the lowest confidence level will typically not correspond to a single probability measure, or a single model specification-parameter value pair, but will rather remain open between several competitive models. In belief representation terms, the posterior beliefs are not precise, though the prior ones are (at the low confidence level). So this update rule captures the reaction of ‘keeping an open mind’ after surprising events. Note also that it is the investor's ==== confidence (represented by the confidence ranking) and its interaction with his confidence in the conclusions drawn from the observation (represented by the probability-threshold function) that determine which beliefs are retained. Since the investor in this example has more confidence in the parameter values, this belief is retained—and the belief about the model specification dropped—on update. This is perfectly coherent with the previously stated maxim exhorting him to retain beliefs in which he is more confident, at the price of those held with less confidence.==== This example thus illustrates how the proposed approach can cope with surprising events.====In this paper, we provide a behavioural characterisation of a generalisation of (1) that we call ==== All the parameters, and in particular that playing the role of ====, are revealed from preferences. Special cases, including a version of rule (1), are also axiomatised. We show that this update rule can deal not only with surprising events but also with updating on complete ignorance. This is summarised in Table 1,==== which also lists a range of existing update rules that can be recovered as special cases of confidence update.====The paper is organised as follows. Section 2 sets out the framework, the confidence model and update rules. Section 3 contains the main results of the paper, characterising general and specific versions of confidence update, and considering its comparative statics. Section 4 brings out the contributions of the proposed approach with respect to the issues of update from complete ignorance and on surprising or null events, including null events in game-theoretical reasoning. Section 5 discusses extensions and ==== the relationship to Bayesian and classical statistical reasoning. Proofs and other material are to be found in the Appendix.",Updating confidence in beliefs,https://www.sciencedirect.com/science/article/pii/S0022053121000260,8 February 2021,2021,Research Article,101.0
"Luo Yulei,Nie Jun,Wang Haijun","Faculty of Business and Economics, University of Hong Kong, Hong Kong,Research Department, Federal Reserve Bank of Kansas City, USA,School of Mathematics and Shanghai Key Laboratory of Financial Information Technology, Shanghai University of Finance and Economics, China","Received 30 September 2019, Accepted 24 January 2021, Available online 28 January 2021, Version of Record 12 January 2022.",https://doi.org/10.1016/j.jet.2021.105204,Cited by (0),"This paper studies how the two types of uncertainty due to ignorance, parameter and model uncertainty, jointly affect strategic consumption-portfolio rules, ====, and welfare. We incorporate these two types of uncertainty into a recursive utility version of a canonical ==== model with uninsurable labor income and unknown income growth, and derive analytical solutions and testable implications. We show that the interaction between the two types of uncertainty plays a key role in determining the demand for ==== and risky assets. We derive formulas to evaluate both marginal and total welfare costs of ignorance-induced uncertainty and show they are significant for plausible parameter values.","Most macroeconomic and financial models assume agents have a good understanding of the economic models they use to make optimal decisions. However, there is plenty of evidence that ordinary investors may be ignorant about certain aspects of the economic model, including the structure of the model, the parameters, and the current state of the model economy that their decisions are based on. For example, some households may not know the basics of risk diversification when making financial decisions (van Rooij et al., 2011)====; some people may lack financial literacy, meaning that they do not have the necessary skills and knowledge to make informed and effective investment decisions (Lusardi and Mitchell, 2014); some investors may have incomplete information about the investment opportunity set (Brennan, 1998); and some individuals may not have full information about their own income growth (Guvenen, 2007). Such ignorance generates pervasive uncertainty for agents when they make economic and financial decisions.====Hansen and Sargent (2015) propose that using ignorance is a useful way to model different types of uncertainty by specifying the details that the decision maker is ignorant about. They use a simple Friedman one-equation tracking model to illustrate this idea. They mainly discussed two types of ignorance: (i) the agent is ignorant about the conditional distribution of the state variable in the next period and (ii) the agent is ignorant about the probability distribution of one key parameter (i.e., the response coefficient in their paper) in an otherwise fully trusted model. In other words, the first type of ignorance represents ==== (or MU) as the agent does not know the distribution of shocks, while the second type of ignorance represents ==== (or PU) as the agent does not know model parameters. (When parameters are stochastic, we may also view parameter uncertainty as state uncertainty.)====Inspired by Hansen and Sargent (2015), in this paper we study how these two types of ignorance affect intertemporal consumption-saving and asset allocation decisions —a core focus of modern macroeconomics and finance. Our central goal is to provide a ==== framework to study how these two types of ignorance (or the two types of uncertainty induced by ignorance: parameter and model uncertainty) interact with each other to affect the optimal consumption-saving-portfolio decisions as well as the equilibrium welfare implications. We derive analytical solutions to separate different forces which determine consumption, precautionary saving, and asset allocation. In addition, our framework also includes some other important factors such as incomplete markets, which are proven to be important for determining consumption, saving, and asset allocations (Wang, 2003, 2009).====Specifically, we construct a continuous-time Merton (1971)-Wang (2009)-type model with uninsurable labor income and unknown income growth in which investors have recursive utility and face model uncertainty. We follow Hansen and Sargent (2007) to introduce model uncertainty by incorporating the preference for robustness (RB). In robust control problems, agents are concerned about the possibility that their true model is misspecified in a manner that is difficult to detect statistically; consequently, they make their decisions as though the subjective distribution over shocks was chosen by an evil agent to minimize their utility.==== In our model economy, investors not only have incomplete information about income growth but are also concerned about the model misspecification. Compared with the full-information rational expectations (FI-RE) case in which income growth is known, parameter uncertainty due to unknown income growth creates an additional demand for robustness. In our recursive utility framework, we also disentangle two distinct aspects of preferences: the agent's elasticity of intertemporal substitution (the EIS, attitudes towards variation in consumption across time) and the coefficient of absolute risk aversion (the CARA, attitudes toward variation in consumption across states), which are shown to have different roles in driving consumption-saving and portfolio choice decisions.==== As will be explained below, our model delivers not only rich theoretical results but also testable implications.====This paper has four main findings and contributions. First, we provide analytical solutions to this rich framework with both types of uncertainty (PU and MU) to study strategic consumption-portfolio rules in the presence of uninsurable labor income. Using the closed-forms solution, we are able to inspect the exact mechanism through which these two types of induced uncertainty interact and affect the demand for risky assets and precautionary saving.==== Specifically, we find that the precautionary saving demand and the strategic asset allocation are mainly affected by the ==== coefficient of risk-uncertainty aversion ==== that is determined by the interaction between the CARA (====), the EIS (====), and the degree of RB (====) via the formula: ====. This expression clearly shows that both risk aversion and intertemporal substitution play roles in determining the amount of precautionary savings and the optimal share invested in the risky asset, but without model uncertainty, only risk aversion matters in determining these two demands.====Second, we use our analytical results (as summarized by Proposition 2 in Section 4) to separate different forces such as MU, PU, and the interaction between MU and PU in determining precautionary saving and the demand for risky assets, which represent the two core parts of saving and portfolio-choice decisions. We show that the share of precautionary saving due to MU and PU together is very large at reasonable levels of model uncertainty. For example, when the degree of model uncertainty (as measured by the detection error probability (DEP)) equals 0.2, MU and PU together can account for about 80% of total precautionary saving for plausible parameter values.==== Out of this, about 46% comes from model uncertainty through the robust control channel and 54% comes from MU through robust filtering channel (or the interaction between PU and MU), while PU itself slightly ==== precautionary saving. We theoretically prove and explain why PU has such a negative impact on precautionary saving. In general, we show that the importance of the interaction between MU and PU increases with the robustness parameter, highlighting the importance of modeling the interaction between these two types of uncertainty when studying consumption-saving-portfolio choice problems. To the best of our knowledge, our paper is the first to provide a unified framework with analytical solutions to separate these different forces.====On the demand for risky assets, we show that its total demand decreases with the amount of model uncertainty due to robustness, and can be decomposed into three components: (i) the traditional speculation demand, (ii) the learning-induced hedging demand, and (iii) the income-hedging demand. We find that the first and second components are determined by the robust control and robust filtering channels respectively, while the third component is independent of both channels. In addition, this decline is largely driven by the increase in the degree of robustness through the robust control channel, though the relative importance of the robust filtering channel increases with the degree of robustness.====Third, we quantitatively test the model implications on risky-asset holding using household-level data. In particular, we conduct a calibration exercise to match the observed risky-asset holdings by different educational groups in the data. The calibrated results suggest that less educated households face more model uncertainty measured by the detection error probability, a commonly used statistical tool to measure the amount of model uncertainty in the robustness literature.==== This result is consistent with the recent empirical finding using survey data by Lusardi and Mitchell (2014) which show that highly educated people are usually more financial literate than low educated people. In addition, we show that the interaction between the two types of uncertainty is the key to explain the data. In particular, if we shut down the model uncertainty channel, the model with only parameter uncertainty cannot explain the observed patterns of asset holdings.====Fourth, we show that the welfare cost of ignorance in general equilibrium can be sizable.==== We provide formulas to evaluate both the marginal and the total welfare costs due to the two types of ignorance-induced uncertainties. Specifically, when ==== (or DEP =0.25), a 10% increase in ==== leads to a welfare cost equivalent to about 2% percent reduction in initial consumption. In addition, this welfare loss increases with the amount of parameter uncertainty. Furthermore, if we remove all uncertainty in the model, the welfare gains could be as large as 23% of initial consumption.====The remainder of this paper is organized as follows. Section 2 provides a literature review. Section 3 describes our model setup, introducing key elements step by step. Section 4 presents key theoretical results. Section 5 presents quantitative results. Section 6 examines the welfare implications of ignorance. Section 7 concludes.","Ignorance, pervasive uncertainty, and household finance",https://www.sciencedirect.com/science/article/pii/S0022053121000211,28 January 2021,2021,Research Article,102.0
"Klibanoff Peter,Mukerji Sujoy,Seo Kyoungwon,Stanca Lorenzo","Department of Managerial Economics and Decision Sciences, Kellogg School of Management, Northwestern University, Evanston, IL, USA,School of Economics and Finance, Queen Mary University of London, London E1 4NS, UK,Business School, Seoul National University, Seoul, South Korea","Received 31 August 2019, Accepted 24 January 2021, Available online 27 January 2021, Version of Record 12 January 2022.",https://doi.org/10.1016/j.jet.2021.105202,Cited by (4),"The ====-MEU model and the smooth ambiguity model are two popular models in decision making under ambiguity. However, the axiomatic foundations of these two models are not completely understood. We provide axiomatic foundations of these models in a symmetric setting with a product state space ====. This setting allows marginals over ==== to be linked behaviorally with (limiting frequency) events. Bets on such events are shown to reveal the i.i.d. measures that are relevant for the decision maker's preferences and appear in the representations. By characterizing both models within a common framework, it becomes possible to better compare and relate them.","In decision making under ambiguity, an important concern is modeling and discriminating between “perception” of ambiguity and ambiguity attitudes. Two popular models that have been described as allowing for a distinction between the two are the ====-MEU model and the smooth ambiguity model. The ====-MEU model ranks acts ==== according to the criterion==== while the smooth ambiguity model ranks acts ==== according to==== In the former, ambiguity perception is captured by the set ==== and attitudes are described by the parameter ====. For the latter, the second-order measure ==== captures ambiguity perception while the curvature of ==== describes the ambiguity attitude. Part of their popularity is explained by the ability of separating between the two. However, the axiomatic foundations of these two models are not yet completely well-understood. Importantly, the fact that there is no axiomatization of the two models in a common framework has inhibited comparison of the two.==== Existing axiomatizations of the ====-MEU model (e.g., Ghirardato et al., 2004; Kopylov, 2003; Gul and Pesendorfer, 2015), as the discussion in Section 1.1 describes, characterize different special cases of the model. The axiomatization of the smooth ambiguity model in Klibanoff et al. (2005) has been criticized for using second order acts (e.g., see the comment by Epstein, 2010 and the reply by Klibanoff et al., 2012). Seo's (2009) related axiomatization did not use second order acts, but his result is not able to uniquely separate the function ==== from the prior ====.====In this paper, we axiomatize these two models in a common framework under a symmetry assumption on preferences. In particular, when the state space Ω has the product structure ====, we axiomatize a version of the ====-MEU model that takes the form==== where ==== is a finite set of probability measures over ====, ====, and ==== is a non-constant, affine utility function. In words, in this special case of the ====-MEU model the set ==== only contains i.i.d. probability measures having marginal distributions contained in the set ====. The smooth ambiguity model we axiomatize under symmetry takes the form==== where ==== is a probability measure over ====, ==== is a non-constant, affine utility function and ==== is continuous and strictly increasing. Furthermore, if the support of ==== is not finite then ==== must satisfy a Lipschitz-type condition.====The symmetric setting we consider is natural for many empirically grounded applications of ambiguity. Indeed, many economic models impose constraints on the agents' preferences so that they reflect some type of calibration of perceived ambiguity to external data (see e.g., Hansen and Sargent, 2008 for motivation and discussion). For instance, in an asset pricing model, the modeler may want to impose the restriction that an investor seeks to make her portfolio robust against only a limited set of stochastic processes that pass certain tests of inference on past data. Typically, such tests rest on the assumption that past and current data generating processes are (at least, conditionally) exchangeable, thus invoking symmetry.====To illustrate the implications of our results, we consider three thought experiments:====Consider an individual with preference ≻ who can bet on two sources of uncertainty. The first is an urn with 100 balls divided in an unknown way between black balls and white balls. The other source of uncertainty is the return on TESLA stock. More precisely, the individual can bet on the results of repeated draws from the urn and repeated daily returns of the stock. The state space is ====, where ==== is an interval containing the possible daily returns. The individual has to choose among the following bets:====Let ==== denote the events that 50% to 60% and 20% to 30% of the daily returns are between 0% and 1%, respectively, and let ==== be the event that 30% to 40% of the balls drawn from the urn are black. If an individual follows the smooth ambiguity model, ==== implies that ====. This is a consequence of the smooth ambiguity model as in (4) necessarily satisfying the sure-thing principle when restricted to bets over long-run frequencies. To see that this requirement forces ==== when ====, notice that ==== and ==== are constructed from ==== and ==== by changing the common payoff on the event ==== from $0 to $90. On the other hand, if an individual uses the ====-MEU model as in (3), then the pattern of preference ==== and ==== is allowed. To illustrate, let ==== be the identity, ====, and the set of measures, ====, be ==== for some ==== with ====, ==== and ====. These ====-MEU preferences imply that ==== and ====, a violation of the sure-thing principle when restricted to bets over long-run frequencies. To see that ====, observe that ==== and ====, while ====. However, ==== due to the fact that ====, which pays $90 if ==== or ==== occurs, provides a hedge against ambiguity, while ====, which pays $100 if ==== or $90 if ==== occurs, does not. Indeed, according to all measures in ====, the event ==== occurs with probability ====, meaning that ==== is an unambiguous bet evaluated like a fifty-fifty lottery between $90 and $0. In contrast, since according to ==== both ==== and ==== occur with probability ====, while according to ==== neither occurs, whether ==== will pay more than $0 is ambiguous. Since ====, the individual prefers to accept the lower payoff of $90 on ==== instead of $100 on ==== in exchange for this decrease in ambiguity.====The previous example illustrated a way in which the smooth ambiguity model is more restrictive than the ====-MEU model when applied to bets depending on long-run frequencies. Our next example illustrates the reverse – a way in which the ====-MEU model is more restrictive than the smooth ambiguity model, even when applied to bets depending on long-run frequencies. Suppose that the individual has to choose among the following bets:====Finally, for general acts that do not necessarily involve long-run frequency events, the ====-MEU model excludes other types of behavior that the smooth ambiguity model does not. To illustrate, suppose that the individual is told that the urn contains equal numbers of black and white balls. Consider the following bets:====An advantage of characterizing these two models in the same framework is that it becomes possible to compare them on their whole domain of preferences. As we will see, the difference between the symmetric versions of the two models is that the ====-MEU model satisfies Certainty Independence and Relevant Range, while the smooth ambiguity model need not, and when it is not expected utility, cannot. Conversely, the smooth ambiguity model must satisfy axioms of subjective expected utility when restricted to acts whose payoffs depend only on events based on limiting frequencies, while the ====-MEU model need not, and cannot unless it is expected utility for all acts. While Certainty Independence and axioms equivalent to subjective expected utility are familiar from the existing literature, the Relevant Range axiom is novel to this paper.",Foundations of ambiguity models under symmetry: ,https://www.sciencedirect.com/science/article/pii/S0022053121000193,27 January 2021,2021,Research Article,103.0
"Gong Aibo,Ke Shaowei,Qiu Yawen,Shen Rui","Department of Economics, University of Michigan, United States of America,Institute of Accounting and Finance, School of Accountancy, Shanghai University of Finance and Economics, China,Shenzhen Finance Institute, School of Management and Economics, Chinese University of Hong Kong, Shenzhen, China","Received 27 August 2019, Accepted 24 January 2021, Available online 27 January 2021, Version of Record 12 January 2022.",https://doi.org/10.1016/j.jet.2021.105201,Cited by (0),"We study strategic trading with a market maker who does not know the joint distribution of public information and an asset's value, and hence cannot interpret information properly. Following a public event, a probabilistically informed trader who knows the distribution and liquidity traders trade. The market maker adopts a robust linear pricing strategy that has the best worst-case payoff guarantee. We show that such a strategy is equivalent to a two-step learning procedure, and characterize the unique linear equilibrium. Expected equilibrium prices exhibit underreaction to public information. If the trading frequency is arbitrarily high, the market maker fully reveals the distribution in the price eventually.","Economic theory often assumes that the joint distribution of random variables is common knowledge. For example, participants in financial markets know the distribution of asset returns, employers know the distribution of employees' unobserved ability, and consumers know the distribution of a new product's quality. In practice, however, such distributions are often unknown, in which case information will be difficult to process even if it is public and precise. For instance, when a publicly traded company discloses information or a central bank cuts interest rates, it is often unclear to many market participants how much of the information is already anticipated and priced in.====When the distribution is unknown, people face ==== (see Ellsberg (1961)). A seminal paper by Gilboa and Schmeidler (1989) characterizes an axiomatic model that exhibits ambiguity aversion, the ==== model. In this model, people maximize the ==== (across all plausible distributions) payoff guarantee; that is, people behave in an optimal way that is robust to the unknown distribution. This model and, more generally, the maxmin principle have been widely used in many applications (see Section 8).====We follow this approach and examine how people react to ==== information in a simple model of strategic trading. Specifically, an asset is traded in the market. Market participants are the ====, the market maker, and liquidity traders. None of them has private information about the value of the asset ====. They receive the same signal ==== from a public event at the beginning. Then, on each of the trading dates, all traders submit market orders to trade and the market maker determines a price of the asset at which the orders are traded. The probabilistically informed trader trades to maximize profit, and liquidity traders trade for idiosyncratic reasons.====After the public event, the probabilistically informed trader (she) can update her belief about ==== based on the joint distribution of ==== and ====. The joint distribution is normal and the mean of ==== is ====.==== However, not all market participants know how to translate ==== into information about ====. As noted above, it is often unclear what public information should have been anticipated, or even whether the information is positive or negative given the price. Therefore, we consider a situation in which the market maker (he) knows everything about the joint distribution except ====—that is, what to expect from ==== in the first place.====Without knowing ====, the market maker does not know how to form the posterior of ==== based on the public information, but he prices the asset in a way that is robust to the unknown ====—he chooses a pricing strategy that has the best worst-case (across all possible values of ====) payoff. We assume that the market maker wants to set a fair price for traders, and his payoff is the sum of his current and future price errors (differences between prices and ====) evaluated according to some general loss function. Due to the asymmetric information about ====, the probabilistically informed trader may manipulate her orders to affect the market maker's behavior, and the market maker's aversion to mispricing determines how she takes the orders into account as she sets prices.====We study the linear equilibrium of such a model, called the ==== (RP) equilibrium.==== First, we show that given any linear trading strategy of the probabilistically informed trader, the market maker's ==== optimal robust linear pricing strategy is equivalent to the following two-step learning procedure.==== On each trading date, based on the received orders (and the initial public signal), the market maker first estimates the unknown parameter ==== optimally using the ==== (BLUE), which depends on the probabilistically informed trader's strategy. Then, the market maker uses the estimated joint distribution to update his belief about ==== and lets the price be the conditional expectation of ====. The two-step learning procedure resembles how people deal with unknown distributions in practice: They often estimate the distribution. As Hansen (2007) emphasizes, it is important to understand how real-time distribution estimation affects people's behavior and equilibrium outcomes.====This result enables us to characterize the unique dynamic linear RP equilibrium indirectly by characterizing the unique ==== equilibrium. In a dynamic BLUE equilibrium, instead of finding the optimal robust linear pricing strategy, the market maker is assumed to adopt the two-step learning procedure. Every dynamic linear RP equilibrium is equivalent to a dynamic BLUE equilibrium, and vice versa.====The indirect characterization of the dynamic linear RP equilibrium is useful in many ways. First, a key variable in the dynamic BLUE equilibrium is the BLUE of ====, which does not appear in the definition of the dynamic linear RP equilibrium. The BLUE of ==== can be thought of as an auxiliary variable in the dynamic linear RP equilibrium, which turns out to help us understand the structure of the equilibrium better—not only the market maker's behavior, but also the probabilistically informed trader's.====Second, we show that the dynamic linear RP equilibrium exhibits two properties by analyzing the dynamic BLUE equilibrium. First, on average, the equilibrium prices exhibit ==== under the true joint distribution; that is, the expected prices are less sensitive to public information than in a benchmark model in which the market maker knows ====, and as time goes by the expected prices move toward the price in the benchmark model. Classic economic theories posit that the market is efficient and public information will rapidly be fully reflected in prices. The benchmark model is consistent with this. In sharp contrast, a large amount of empirical evidence finds underreaction to public events. For example, stock returns often experience post-earnings announcement drift.==== Thus, our theory provides an explanation for underreaction.====The second property of the dynamic linear RP equilibrium addresses a basic question. Without knowing ====, the market maker faces ambiguity. What does the market maker do with ambiguity as he solves the dynamic robust pricing problem? Will ambiguity be eliminated? We show that if the trading frequency is arbitrarily high, the market maker will learn ==== in the dynamic BLUE equilibrium in the end—or, equivalently, as the market maker implements the optimal robust pricing strategy in the dynamic linear RP equilibrium, eventually ==== will be revealed in the price and the public information will be fully incorporated into the price.====Last, we examine how the above findings rely on our assumptions about the market maker's behavior. First, we compare our model to a model with a Bayesian market maker. The Bayesian market maker also does not know ==== but treats it as a random variable and has a prior over it. We show that underreaction does not always arise in the Bayesian case. Second, we analyze how the equilibrium behavior changes if the market maker uses some estimator of ==== other than the BLUE in the first step of the two-step learning procedure. We find that multiple equilibria exist, and the probabilistically informed trader does not necessarily benefit from the fact that the market maker uses a “suboptimal” estimator.====The structure of the paper is as follows. Section 2 describes the setup that applies to all models in our paper. The benchmark model is introduced in Section 3, the static model in Section 4, and the dynamic model in Section 5. Section 6 studies underreaction and market efficiency in the dynamic model. In Section 7, we analyze how our findings depend on the assumptions about the market maker's behavior. Section 8 discusses related literature, and Section 9 concludes.",Robust pricing under strategic trading,https://www.sciencedirect.com/science/article/pii/S0022053121000181,27 January 2021,2021,Research Article,104.0
"Lensman Todd,Troshkin Maxim","Department of Economics, Massachusetts Institute of Technology, Cambridge, MA 02142, USA,The Wharton School, University of Pennsylvania, Philadelphia, PA 19104, USA,Department of Economics, University of Exeter, Exeter EX4 4PU, UK","Received 6 May 2019, Accepted 24 January 2021, Available online 27 January 2021, Version of Record 12 January 2022.",https://doi.org/10.1016/j.jet.2021.105206,Cited by (3),"We study the implications of ambiguity for optimal fiscal policy in macro public finance environments with heterogeneous agents and private idiosyncratic shocks. We describe conditions under which ambiguity implies that it is optimal to periodically reform policies. Periodic reforms lead to simplified optimal policies that are not fully contingent on future shocks; at times they also lose dependence on the full history of past shocks. These simplified policies can be characterized without complete backward induction when the time horizon is finite. However, linear policies can be far from optimal. We also show that equilibria in decentralized versions of these economies are not generally efficient, implying a meaningful role for government provision of insurance, unlike in conventional environments with a narrower view of uncertainty.","A sizable and growing literature shares the following approach to social insurance, redistribution, and normative questions in dynamic economies more generally:==== Start with a friction, typically private information about idiosyncratic shocks, and characterize friction-constrained allocations that maximize an ex ante objective, typically social welfare. The optimal policies are then the ones that implement constrained-optimal allocations. Crucially, the policy designer and the agents in the economy are commonly assumed to know the data-generating process for the shocks (“rational expectations”). Three broad outcomes are closely associated with this assumption:====Our goal is to characterize broad properties of optimal policies that are robust with respect to incomplete knowledge of the stochastic process for shocks. To do so, we remove the assumption of exact knowledge of future distributions of shocks and instead allow a broader view of uncertainty. That is, the agents in the economy face both risk in the conventional sense of stochastic, heterogeneous skills, as well as (Knightian or model) uncertainty in the sense that agents entertain multiple possible distributions of future skills, commonly referred to as beliefs, models, or priors. The approach we take to modeling risk and uncertainty, with aversion to both, follows the approaches in macroeconomics and finance (e.g., Hansen and Sargent, 2001; Epstein and Schneider, 2003).====We consider an otherwise conventional heterogeneous-agents environment with idiosyncratic and potentially persistent shocks to skills and beliefs. The data-generating process for shocks is arbitrary, and it is not known to anyone in the economy. Each period, every agent draws a shock and forms a set of distributions that he believes may represent the data-generating process for shocks in the next period. We impose a simple condition on these beliefs, and we keep the environment virtually agnostic about any learning that may map histories of observations and current distributions into updated beliefs about the future. To aid intuition, we present arguments using a recursive maxmin expected utility representation of preferences, with arbitrary belief updating rules.====The economy has a government that seeks to provide social insurance against risk and uncertainty, as well as a degree of redistribution. It is constrained by the same uncertainty about the distribution of future shocks, so the government is not an abstract entity with perfect knowledge of the data-generating processes. Rather, the government is interpreted concretely as having at best the information that all of the agents in the economy have combined.====To make it transparent that the main force behind the results is uncertainty, we first develop them in a baseline finite-horizon environment with a finite number of agents, in which agents' idiosyncratic shocks are publicly observable (Section 2). We then extend our results to the setting in which shocks are privately observed by the agents (Section 3). To show that competitive equilibria are not generally efficient, we compare outcomes in this private-information economy to those in its decentralizations (Section 4). Finally, we further examine the simplicity of optimal policies by discussing conditions under which linear policies can be optimal (Section 5).====Briefly, the intuition for the main findings is as follows. First, we show that it is optimal for the government to periodically ====, i.e., that the (constrained) efficient level of welfare can be delivered with allocations that are redesigned as needed after the economy realizes a new set of shocks. Our condition on beliefs is that today each agent allows for the possibility of everyone believing tomorrow that some shocks will not be realized in the future. Such shocks can then be ignored by a non-paternalistic government when designing a welfare-maximizing policy, as long as this does not pose an issue for feasibility. If tomorrow agents' actual beliefs differ (i.e., a situation described by the epigraph takes place), then a Pareto improvement can be found. In other words, the government may find it possible in subsequent periods to improve everyone's welfare by redesigning the continuation allocations, i.e., by reforming. Every agent foresees this possibility but does not find it necessary to preempt it since their welfare weakly increases as a result.====By the same logic, broader uncertainty implies that optimal policies are ==== in the sense that they are not fully contingent on future shocks. Optimal allocations may also lose dependence on past shocks whenever a reform provides a big enough improvement to the status quo allocation. In addition, because they are periodically reformed, optimal allocations can be constructed by solving what we call a reform problem. The reform problem is recursive in nature, with the previously designed policy as a state variable, and provides an algorithm for characterizing the optimum period by period, without solving for a complete sequence of fully state-contingent policy functions. When the time horizon is finite, the optimality of reforms implies that optimal allocations can be characterized without the full backward induction ordinarily required. Despite these simplifications, restrictive assumptions are required for linear or even affine policies to be optimal, e.g., that agents supply labor inelastically and believe that idiosyncratic shocks are independently distributed.====Finally, broader uncertainty creates a potentially meaningful role for government provision of insurance. In decentralizations of the private-information economy, agents contract with competitive firms to provide labor and capital in exchange for consumption. We show that whether competitive equilibria are efficient depends crucially on the relationship between the firms' uncertainty and the government's uncertainty, as expressed through its feasibility constraint. If firms are more uncertain about the data-generating process than the government, they may be unwilling to provide the same degree of insurance as the government. In this sense, uncertainty may act as a friction that impedes the efficient operation of competitive markets. Nevertheless, by the same intuition as in the government's problem, any insurance that decentralized economies do provide can still be simplified and periodically reformed.",Implications of uncertainty for optimal policies,https://www.sciencedirect.com/science/article/pii/S0022053121000235,27 January 2021,2021,Research Article,105.0
"De Filippis Roberta,Guarino Antonio,Jehiel Philippe,Kitagawa Toru","European Banking Authority, France,Department of Economics, University College London (UCL), United Kingdom of Great Britain and Northern Ireland,Paris School of Economics, France","Received 20 September 2019, Revised 7 December 2020, Accepted 1 January 2021, Available online 26 January 2021, Version of Record 12 January 2022.",https://doi.org/10.1016/j.jet.2021.105188,Cited by (9),"In our laboratory experiment, subjects, in sequence, have to predict the value of a good. The second subject in the sequence makes his prediction twice: first (“first belief”), after he observes his predecessor's prediction; second (“posterior belief”), after he observes his private signal. We find that the second subjects weigh their signal as a ","Suppose you are contemplating the possibility of investing in a new project. Since it seems, ====, equally likely that it succeeds or fails, you ask for the opinion of an independent advisor. After collecting some information on the project, he evaluates the probability of success to be 70%. On the basis of this recommendation only, clearly your belief on the probability of succeeding depends on how much you trust your advisor's ability. If you fully trust him, you may agree with him and evaluate the probability of success to be 70% as well. If you do not think he has done a good job, or you suspect he is not so talented as you originally thought, you may even completely discard his view and keep your prior belief of a 50% probability of success. For the sake of the example, let us assume you trust him, although not completely, and assess that the project will succeed with probability 65%. You now receive further information on the project, independently of your advisor's information. The information is of the same quality as that received by your advisor, but is negative, that is, if you had to base your evaluation on this information only, you would update your prior belief to a value lower than 50% (actually, 30%, since the information is of the same quality). How would you now use this information to make inferences on the quality of the project? Would you change your mind on your advisor's ability? And how would you revise your 65% belief?====One way of reasoning is that the negative private information you receive (contradicting the advisor's view) makes it more doubtful that he used his information correctly. You should, therefore, discount the advisor's evaluation even more than before and mainly rely on your information. Since you now trust him less, your belief based on his advice only would be less than 65%. Moreover, you have now negative information, which pushes your belief further down. For instance, if you completely lost trust in your advisor's ability, you would now totally discard his view and on the basis of his advice only you would evaluate the probability of success to be 50%; considering also the negative content of your information, you would evaluate the probability of success to be 30%.====While this reasoning seems intuitive, one has to be careful in applying it, at least if one wants to respect standard Bayesian updating.==== Indeed, a Bayesian agent, once expressed his belief of 65%, would simply update it by considering the probability of the received information conditional on the project being a success or a failure. To the extent that the information received by the advisor and yourself are independent conditional on the profitability of the project, the 65% probability summarizes all the relevant information in order to update the belief. Given the values we have used in our example, a Bayesian agent would never give a valuation of 30%; rather, he would update the 65% belief to 44.3%.====Decision problems like the one we have just, informally, described are very common in many applications. Traders in financial markets typically try to infer information, say about asset fundamental values, from the order flow and also use their own private information. Companies make investment decisions using consultants and their own internal profitability studies. Editors of scientific journals make their editorial decisions on the basis of referee reports and of their own reading of the papers.====In this work we aim to study human behavior in a controlled experiment in which subjects have to make similar types of decisions. Our purpose is to analyze how well human subjects' behavior conforms to Bayesian updating when they have to make inferences from a private signal and from the decision of another human subject, and, if appropriate, to understand the determinants of deviations from it. To be specific, we ask subjects to predict whether a good is worth 0 or 100 units, two events that are, a priori, equally likely. A first subject receives a noisy symmetric binary signal about the true value realization: either a “good signal”, which is more likely if the value is 100; or a “bad signal”, which is more likely if the value is 0. After receiving his signal, the subject is asked to state his belief on the value being 100. We then ask a second subject to make the same type of prediction based on the observation of the first subject's decision only. Finally, we provide the second subject with another, conditionally independent, signal about the value of the good and ask him to make a new prediction.====The main result of our study is that when at time 2 subjects observe their private signal, they update their belief in an “asymmetric way.” When the signal is in agreement with their first belief (e.g., when they first state a belief higher than 50% and then receive a signal indicating that the more likely value is 100), they weigh the signal as a Bayesian agent would do. When, instead, they receive a signal contradicting their first belief, they put considerably more weight on it (i.e., ==== they had observed more than one signal, or ==== the signal had a higher precision than it actually has).====This asymmetric updating is incompatible with standard Bayesianism, even allowing for subjective precisions of the signals, in the vein of Savage (1954), as long as the two pieces of information are independent conditional on the value of the good. Given that the conditional independence follows from principles of logic, we conclude that there is a need to go beyond the Bayes/Savage paradigm. Moreover, as we will demonstrate, the asymmetric updating: ====) cannot be explained by known psychological biases such as base-rate neglect or confirmation bias; ====) is not due to risk preferences; ====) is inherently related to social learning (indeed, it does not occur in control, individual decision making, treatments).====Another main contribution of the paper is to provide a simple explanation for such asymmetric updating, by building on the literature on ambiguity and multiple priors (and, in particular, on the branch devoted to belief updating). We propose that, like in models with multiple priors, the subject at time 2 may entertain several possible theories about time 1 subject's “rationality” (where a subject is considered “rational” if he chooses an action higher than 50 when he receives a good signal and lower than 50 when he receives a bad signal; a “noise” subject, in contrast, chooses any action between 0 and 100 independently of whether he receives a good or bad signal).==== Moreover, each time he has to make a decision, he selects the theory that maximizes the likelihood of the realized observations. Based on this selected theory about the rationality of the predecessor, subject 2 updates his belief in a standard Bayesian fashion (possibly using subjective representations of the precision of the signals). We label this mode of updating Likelihood Ratio Test Updating model, a generalization of the Maximum Likelihood Updating model.====Intuitively, this explains the asymmetry we observe for the following reason. Imagine a subject observing the predecessor taking an action greater than 50 (i.e., an action that presumably comes from a good signal, indicating the value is 100). Suppose he considers that the event is most likely under the prior that the predecessor is rational and, therefore, chooses his own action (his “first belief”) accordingly. After he observes a private signal confirming his first belief (that the value is more likely to be 100), the subject remains confident that the predecessor was rational, that is, sticks to the same prior on the predecessor's rationality. He updates on that prior belief and so the weight he puts on the signal seems identical to that of a Bayesian agent. Consider now the case in which he receives a signal contradicting his first belief (i.e., a bad signal, indicating that the more likely value is 0). In such a case he now deems it an unlikely event that the predecessor was rational. In other words, he selects another prior belief on the predecessor's rationality, giving a much higher probability to his predecessor being noise. Once he has selected this new prior on the predecessor's rationality, he updates on the basis of the signal realization. This time it will look like he puts much more weight on the signal, since the signal first has made him change the prior on the rationality of the predecessor (becoming more distrusting) and then update on the basis of that prior. It is important to remark that it is as if the subject put different weights on the signal depending on its realization, not that he consciously uses different weights.====If instead of considering the Likelihood Ratio Test Updating model one were to consider the Full Bayesian Updating model, a popular alternative model of updating with multiple priors, we establish that the agent would behave as if putting a ==== (rather than higher) weight on his own signal when it is contradicting relative to when it is confirming. Thus, whereas the Likelihood Ratio Test Updating model can explain our experimental finding, the Full Bayesian Updating model cannot.====More formally, we offer statistical and experimental evidence in support of this theory. We perform a careful econometric analysis and find evidence in support of this theory against the Bayesian (Savage) approach and the Full Bayesian Updating model. As for experimental evidence, in a treatment we directly test how human subjects update on the rationality of the predecessor. Again, we find that the updating is at odds with the Savage approach and the Full Bayesian Updating model and in line with our model.====Our findings are of direct relevance for theory purposes. In particular, to the best of our knowledge, we are the first to use multiple priors to model beliefs about rationality of others and provide experimental evidence in its support. We show that ambiguity is relevant not just when it is about the composition of an urn (which has inspired an enormous literature, following Ellsberg's famous experiments), but also in very common and economically relevant decision problems. In our analysis, we also contribute to the debate on how to update multiple priors (an area in which there is very little theoretical consensus) and propose a novel identification strategy to distinguish between different theories of updating.====It should be clear that our results have implications for applied work. In applications, the Savage approach has been widely used. For instance, in behavioral finance, researchers model overconfidence by allowing subjective precisions of the signals (an agent considers his own signal more informative than the others'). While this is left for future research, our finding suggests the need to allow for multiple priors, which may unveil important phenomena, e.g., in the way economic agents react to good and bad news depending on the prevailing current belief, market conditions or state of the economy (recession or boom).====The paper is organized as follows. Section 2 describes the model, the Perfect Bayesian Equilibrium predictions, as well as the predictions of the Likelihood Ratio Test Updating model and of the Full Bayesian Updating model. Section 3 presents the experiment. Section 4 contains the results of the main treatments. Section 5 illustrates the control treatments. Sections 6 and 7 present econometric and experimental evidence in support of the Likelihood Ratio Test Updating rule. Section 8 discusses the related literature and Section 9 concludes. The Appendix contains additional material and the instructions.",Non-Bayesian updating in a social learning experiment,https://www.sciencedirect.com/science/article/pii/S0022053121000053,26 January 2021,2021,Research Article,106.0
"Turchick David,Xiong Siyang,Zheng Charles Z.","Department of Economics, University of São Paulo, São Paulo, SP, Brazil,Department of Economics, University of California, Riverside, CA, United States,Department of Economics, University of Western Ontario, London, ON, Canada","Received 1 April 2022, Accepted 1 May 2022, Available online 3 June 2022, Version of Record 3 June 2022.",https://doi.org/10.1016/j.jet.2022.105476,Cited by (0),One of the proofs in ==== has a subtle mistake. This note provides a simple correction without changing the structure of the proof or altering any assumption in the said paper.,None,Corrigendum to “Core equivalence theorem with production” [J. Econ. Theory 137 (1) (2007) 246–270],https://www.sciencedirect.com/science/article/pii/S0022053122000667,3 June 2022,2022,Research Article,109.0
"Samuelson Larry,Schmeidler David","HEC Paris, France,Tel Aviv University, Israel,Department of Economics, Yale University, United States of America","Received 30 June 2019, Revised 7 May 2020, Accepted 22 November 2020, Available online 26 November 2020, Version of Record 12 January 2022.",https://doi.org/10.1016/j.jet.2020.105166,Cited by (1),"Beginning with Robert Aumann's 1976 “Agreeing to Disagree” result, a collection of papers have established conditions under which it is impossible for rational agents to disagree, or bet against each other, or speculate in markets. The subsequent literature has provided many explanations for disagreement and trade, typically exploiting differences in prior beliefs or information processing. We view such differences as arising most naturally in a “large worlds” setting, where there is no commonly-accepted understanding of the underlying uncertainty. This paper develops a large-worlds model of reasoning and examines how agents learn in such a setting, with particular interest in whether accumulated experience will lead them to common beliefs (and hence to agree, and to cease trading). No learning rule invariably ensures learning, leaving ample room for persistent disagreement and trade. However, there are intuitive learning rules that lead people with different models of the underlying uncertainty to a common view of the world ==== the data generating process is sufficiently structured.",None,Learning (to disagree?) in large worlds,https://www.sciencedirect.com/science/article/pii/S0022053120301599,26 November 2020,2020,Research Article,110.0
"Erdil Aytek,Kitahara Minoru,Kumano Taro,Okumura Yasunori","University of Cambridge , UK,Osaka City University, Japan,Yokohama National University, Japan,TUMSAT, Japan","Received 4 February 2021, Revised 25 March 2022, Accepted 20 April 2022, Available online 13 May 2022, Version of Record 10 June 2022.",https://doi.org/10.1016/j.jet.2022.105470,Cited by (0),"We identify an error in Proposition 3 by ====. We then recover the result by replacing the substitutability condition on priority structures with a new condition which we call ====. We further show that the priorities in both applications in ==== satisfy the bridging property, which in turn ensures that their Propositions 5 and 6 are still valid.","Partly motivated by diversity goals in school choice, ==== introduce a model where schools compare alternative cohorts of student intake according to some distributional metric (possibly a measure of diversity). Importantly, to maintain generality and flexibility, they allow such comparisons to have ties between many alternatives, and show how ==== can capture these comparisons (i.e., priority rankings over sets of students). They introduce a modified deferred acceptance procedure (MDA) to show that a ==== (i.e., one that ====) assignment exists.====In general, there are multiple stable assignments, and some assignments might serve students better than other assignments in the sense of Pareto dominance. A stable assignment is called ==== if no other stable assignment Pareto dominates it (from the students' perspective). Noting that the outcome of MDA is not necessarily constrained efficient, they explore the possibility of recovering constrained efficiency by using a construction which they call ====. They claim, in their Proposition 3, that given acceptant and substitutable school priorities, if a stable assignment does not admit a PSIC, then it is constrained efficient. They then rely on this sufficiency claim to establish an algorithm to compute a constrained efficient assignment in two distinct classes of applications; namely ==== and ====.====We illustrate, by way of counterexample, that their Proposition 3 is not correct. Since they rely on this proposition to establish the relevant results for their applications (Propositions 5 and 6, and Corollaries 1 and 2), failure of the proposition leads to a gap in the proofs of these latter results.====To complete that gap and recover these results, we introduce a new technical condition on priorities: the ====. This allows us to revise the statement of Proposition 3 as follows. Given acceptant priorities which satisfy the bridging property, if a stable assignment does not admit a PSIC, then it is constrained efficient. We then show that priorities in both classes of applications satisfy the bridging property. Hence, we verify that Propositions 5 and 6, and Corollaries 1 and 2 in ==== are still valid. We also show that bridging is a necessary condition for Proposition 3 to hold: if bridging is not satisfied for at least one school's priorities, then it is possible for a stable assignment to be constrained inefficient even though it does not admit a PSIC.",Corrigendum to “Efficiency and stability under substitutable priorities with ties” [J. Econ. Theory 184 (2019) 104950],https://www.sciencedirect.com/science/article/pii/S0022053122000606,13 May 2022,2022,Research Article,111.0
"Lopomo Giuseppe,Rigotti Luca,Shannon Chris","Duke University, United States of America,University of Pittsburgh, United States of America,UC Berkeley, United States of America","Received 18 February 2019, Revised 17 May 2020, Accepted 22 June 2020, Available online 1 July 2020, Version of Record 12 January 2022.",https://doi.org/10.1016/j.jet.2020.105088,Cited by (1),"This paper studies a robust version of the classic surplus extraction problem, in which the designer knows only that the beliefs of each type belong to some set, and designs mechanisms that are suitable for all possible beliefs in that set. We derive necessary and sufficient conditions for full extraction in this setting, and show that these are natural set-valued analogues of the classic convex independence condition identified by ==== (====, ====). We show that full extraction is neither generically possible nor generically impossible, in contrast to the standard setting in which full extraction is generic. When full extraction fails, we show that natural additional conditions can restrict both the nature of the contracts a designer can offer and the surplus the designer can obtain.","This paper studies a robust version of the classic surplus extraction problem. In the standard setting, when agents' private information is correlated with their beliefs, a designer can typically extract all or virtually all information rents in a broad range of environments, as shown in the pioneering work of Crémer and McLean (1985, 1988) and McAfee and Reny (1992). We consider a robust version of this problem, in which the designer knows only that the beliefs of each type belong to some set, and designs mechanisms that are suitable for all possible beliefs in that set. We derive necessary and sufficient conditions for full extraction in this setting, and show that these are natural set-valued analogues of the classic convex independence condition identified by Crémer and McLean (1985, 1988). We show that full extraction is neither generically possible nor generically impossible, in contrast to the standard setting in which full extraction is generic. When full extraction fails, we show that natural additional conditions can restrict both the nature of the contracts a designer can offer and the surplus the designer can obtain.====The classic full extraction results of Crémer and McLean (1985, 1988) and McAfee and Reny (1992) create a puzzle for mechanism design because they suggest that private information often has no value to agents. In addition, these results suggest that complicated stochastic mechanisms are typically optimal for the designer. Both predictions seem unrealistic. In the literature that followed, a number of papers showed that full extraction breaks down under natural modifications to the setting. In particular, risk aversion, limited liability, budget constraints, collusion, and competition are among the reasons that full extraction may fail (see Robert (1991), Laffont and Martimort (2000), Peters (2001), or Che and Kim (2006), for example). On the other hand, relaxing the standard setting to include these features often eliminates other desirable properties, while also typically predicting complex mechanisms that bear little resemblance to those frequently used in practice. In this paper, we show not only that our robustness requirements can limit the designer's ability to fully extract rents, but also that when this happens the designer can be limited to simpler and more realistic mechanisms.====Under the notion of robustness in this model, whether or not full extraction is possible depends in part on the amount of uncertainty about agents' beliefs. If agents' beliefs are given precisely, the model reduces to the standard problem, and full extraction is possible generically. In particular, in this case full extraction holds if agents' beliefs satisfy the convex independence condition of Crémer and McLean (1988) (which is satisfied on a generic set). Starting from such beliefs, we show that full extraction continues to hold with uncertainty about these beliefs, provided uncertainty is sufficiently small. Full extraction fails when uncertainty is sufficiently large, however. We show that full extraction holds in general if beliefs satisfy a natural set-valued version of convex independence. When this condition is satisfied, the mechanisms that achieve full extraction are variants of the full extraction mechanisms constructed by Crémer and McLean (1988). When full extraction fails, however, we show that the designer can be restricted to simple mechanisms: natural conditions limit the designer to offering a single contract, and can make a deterministic contract optimal for the designer.====We follow McAfee and Reny (1992) in considering a reduced form description of the surplus extraction problem. In a prior, unmodeled stage, agents play a game that leaves them with some information rents as a function of their private information. This game could be an auction, a bargaining game, or a purchase from a seller, for example. Private information is summarized by the agent's type. The current stage also has an exogenous source of uncertainty, summarized by a set of states, on which contract payments can depend. For some applications, it is natural to take the state space to be the set of types, although we follow McAfee and Reny (1992) in allowing the state space to be arbitrary. We assume that both the set of types and the state space are finite, as in the original environments considered by Crémer and McLean (1985, 1988).==== Each type determines both the information rent and a set of beliefs over the state space.====Our main results define and characterize full extraction in this model. We start with notions of full extraction motivated by robustness to uncertainty about agents' beliefs; these notions require incentive compatibility and individual rationality conditions to hold for all possible beliefs of agents. We then connect these notions to choice behavior under Knightian uncertainty, as in the foundational model of Bewley (1986) (see also Bewley (2002)). We show that these notions of extraction are closely related, and that the same set-valued analogues of convex independence provide necessary and sufficient conditions for full extraction in each case. Our results can thus be interpreted as incorporating robustness to the designer's misspecification of agents' beliefs, or robustness to agents' perceptions of uncertainty. We show that full extraction is neither generically possible nor generically impossible in this setting, and that this result holds under both topological and measure-theoretic notions of genericity. This is in contrast with the standard environment in which each type is associated with a single belief, either because the designer is not concerned about misspecification or because agents do not perceive uncertainty, and convex independence is satisfied for a generic subset of type-dependent beliefs whenever there are at least as many states as types.====We also explore limits on the complexity of contracts the designer can offer. When convex independence is satisfied and full extraction holds, the designer typically offers a menu of contracts with as many different contracts as types. When convex independence fails, however, the designer can be restricted both in the complexity of contracts offered and in the surplus that can be extracted. In the spirit of our earlier work (Lopomo et al. (2009)), we show that this is particularly salient when the beliefs associated with different types intersect. We adapt the notion of fully overlapping beliefs introduced in Lopomo et al. (2009), which gives a notion of richness of the beliefs common to several types, to this setting with finitely many types. When beliefs are fully overlapping, we show that the designer is limited to offering a single contract. Under additional conditions on the designer's beliefs and objective, we show that a single deterministic contract can be optimal for the designer.====Our paper is related to several strands of literature on robustness and ambiguity in mechanism design problems more generally, and on surplus extraction more specifically. We adapt the basic framework of our earlier work in Lopomo et al. (2009) to the general setting of McAfee and Reny (1992), and restrict attention to problems with a finite set of states and types. Our earlier work instead considers more general mechanism design problems in settings with an infinite set of types, and gives conditions under which incentive compatible mechanisms must be simple, in particular ex-post incentive compatible. The key condition we introduce in Lopomo et al. (2009) is a version of fully overlapping beliefs. We illustrate using the leading example of epsilon-contamination, which we adapt here in section 3. As in the current paper, we argue that the results of Lopomo et al. (2009) can be interpreted either as robustness or ambiguity. Jehiel et al. (2012) adopt the model of robustness introduced in Lopomo et al. (2009), and show that ex-post implementation is generically impossible in the epsilon-contamination example. Chiesa et al. (2015) also adapt the model of Lopomo et al. (2009) to a setting with a finite set of types, and focus on the performance of Vickrey mechanisms in either dominant or undominated strategies. Fu et al. (2017) also consider the problem of surplus extraction with finitely many types when the designer does not have full information about the distribution of buyers' valuations. They consider auctions in which the seller can observe samples from a finite set of possible distributions of buyers' valuations, and can condition the mechanism on these samples. They give tight bounds on the number of samples needed for full extraction using dominant strategy incentive compatible mechanisms. Albert et al. (2019) study related sampling and estimation problems for a monopolistic seller facing a single buyer, and consider mechanisms that relax incentive and participation constraints by only requiring that these hold with high probability. Their focus is primarily on computational aspects of finding optimal mechanisms in this class. They give conditions under which the optimal such mechanism can be computed in polynomial time, and under which it can be learned using a polynomial number of samples from the buyer's distribution. Instead, we consider the more general setting of McAfee and Reny (1992), and assume that the designer does not have access to samples from the distribution of buyers' rents, while knowing the set of possible such distributions. We then give conditions under which full surplus extraction is possible regardless of buyers' beliefs.====Our paper also connects with a growing body of work on mechanism design under ambiguity. This includes Bodoh-Creed (2012), Bose et al. (2006), and Wolitzky (2016), which extend standard mechanism design problems to maxmin expected utility agents, and Bose and Renou (2014) and De Tillio et al. (2016), which allow for maxmin expected utility agents facing ambiguity about aspects of the mechanism. In particular, Wolitzky (2016) derives a necessary condition for social choice rules to be implementable with maxmin expected utility agents, and provides a characterization of when efficient trade is implementable in bilateral trade, extending the classic Myerson-Satterthwaite theorem (Myerson and Satterthwaite (1983)). Most closely related is Bose and Daripa (2009), who study auction design and surplus extraction in a model with ambiguity. They consider an independent private values setting in which bidders are assumed to have maxmin expected utility with beliefs modeled using epsilon-contamination. They show that the seller can extract almost all of the surplus by using a dynamic mechanism which is a modified Dutch auction. This is in contrast to results for optimal auctions without ambiguity, or to optimal static auctions with ambiguity, which leave rents to all but the lowest types (see Bose et al. (2006)). We focus instead on an analogue of the correlated beliefs setting of Crémer and McLean (1988), for which full extraction holds generically under unique priors using static mechanisms. Our results show that full extraction can still hold with ambiguity, using variants of the static mechanisms of Crémer and McLean (1988), but that full extraction is no longer a generic feature of the model with ambiguity.====Our paper is also related to a substantial literature exploring the limits of the classic full extraction results. As discussed above, risk aversion, limited liability, budget constraints, collusion, and competition have all been shown to be reasons full extraction might fail. Our results can be understood as showing that robustness to sufficiently imprecise information about beliefs or sufficient ambiguity might be other reasons for the failure of full extraction. Another important recent strand of work questions whether the conclusion that full extraction holds generically is robust to alternative models of agents' beliefs and higher order beliefs, to the relationship between payoffs and beliefs, or to the notion of genericity used. Neeman (2004) focuses on the relationship between payoff and beliefs, and notes that full extraction fails when different rents can be associated with the same beliefs. Heifetz and Neeman (2006) show that the type spaces in which this is ruled out, and thus “beliefs determine preferences” are required and full extraction is possible, are not generic in a measure-theoretic sense within the universal type space. Barelli (2009) and Chen and Xiong (2011) argue that whether such type spaces are generic or not depends on the notion of genericity used, and show that such type spaces are instead topologically generic. Chen and Xiong (2013) show that full extraction also holds generically, again in a topological sense. Chen and Xiong (2013) establish their generic result by showing that full extraction mechanisms are robust to sufficiently small changes in priors, in that for each ====, if a mechanism from a particular class extracts all but at most ==== surplus for a given prior, then the same mechanism also extracts all but ==== surplus for all priors in a sufficiently small weak-==== neighborhood of the original prior. Our paper complements these results by showing that full extraction can hold robustly in a related sense. In our setting, full extraction is neither generically possible nor generically impossible, regardless of whether topological or measure-theoretic notions of genericity are invoked.====The paper proceeds as follows. In section 2, we set up the basic model and definitions, and discuss the model of incomplete preferences. In section 3, we give the leading example to illustrate the main ideas and results of the paper. In section 4 we develop the general model, and give our main results characterizing sufficient and necessary conditions for full extraction. We also connect these results to choice behavior under Knightian uncertainty, and provide alternative results for notions of full extraction motivated by such behavior. We also consider limits to the designer's ability to extract rents when these results do not apply. In section 5 we consider the genericity of full extraction. Finally, in section 6 we consider several extensions of our main results. We explore the performance of given extraction mechanisms when beliefs are perturbed, and show that several of our main results carry over to other standard models of ambiguity, including versions of maxmin and alpha-maxmin expected utility.",Uncertainty and robustness of surplus extraction,https://www.sciencedirect.com/science/article/pii/S0022053120300831,1 July 2020,2020,Research Article,115.0
"Levy Gilat,Razin Ronny","LSE, United Kingdom","Received 27 June 2018, Revised 5 May 2020, Accepted 28 May 2020, Available online 8 June 2020, Version of Record 12 January 2022.",https://doi.org/10.1016/j.jet.2020.105075,Cited by (5),"We suggest a framework to analyse how sophisticated decision makers combine multiple sources of information to form predictions. In particular, we focus on situations in which: (i) Decision makers understand each information source in isolation but are uncertain about the correlation between the sources; (ii) Decision makers consider a range of bounded correlation scenarios to yield a set of possible predictions; (iii) Decision makers face ambiguity in relation to the set of predictions they consider. We measure the bound on correlation scenarios by using the notion of pointwise mutual information. We show that the set of predictions the decision makers considers is completely characterised by two parameters: the Naïve-Bayes interpretation of forecasts (correlation neglect), and the bound on the correlation between information sources. The analysis yields two countervailing effects on behaviour. First, when the Naïve-Bayes interpretation of information is relatively precise, it can induce risky behaviour, irrespective of what correlation scenario is chosen. Second, a higher correlation bound creates more uncertainty and therefore potentially more conservative behaviour. We show how this trade-off affects behaviour in different applications, including financial investments, ==== and CDO ratings. For the latter, we show that when faced with complex assets, decision makers are likely to behave in ways that are consistent with complete correlation neglect.","When confronted with multiple forecasts, we often have a better understanding of each forecast separately than we do of how the sources relate to one another. This is apparent in many situations when experts or organisations make predictions. In the finance literature this has been long recognised.==== Jiang and Tian (2016) point to several problems in estimating correlation, including the lack of sufficient market data, instabilities in the correlation process and the increasingly interconnected market patterns. The US financial crisis inquiry (FCIC) report from 2011 cites the acknowledgment of the rating agency Moody's that “In the absence of meaningful default data, it is impossible to develop empirical default correlation measures based on actual observations of defaults.”====In this paper we suggest a framework to model how sophisticated individuals combine forecasts in complex environments. In particular, we focus on situations in which: (i) Decision makers understand each information source in isolation but are uncertain about the correlation between the sources; (ii) Decision makers consider a range of bounded correlation scenarios to yield a set of possible predictions; (iii) Decision makers face ambiguity in relation to the set of predictions they consider.====In particular, we consider an environment in which an agent observes forecasts about a potentially multidimensional state of the world, ====. For each element ==== in ====, the agent observes possibly multiple forecasts, each a probability distribution over Ω. To combine the multiple forecasts into a prediction about ====, the agent considers a set of possible joint information structures that could have yielded these forecasts. We allow for two types of correlation in the consideration set of the agent: across the fundamentals (the elements in ====), and across the predictions (e.g., due to biases in polling techniques). For each joint information structure in this set, that is consistent with the multiple forecasts, the agent derives a Bayesian prediction over the state of the world. This process yields a set of predictions about ==== that is the focus of our analysis. For example, if the decision maker only considers joint information structures which satisfy (conditional) independence across forecasts, then the unique prediction that arises is the Naïve-Bayes (NB) belief.====Our main modelling assumption is to use a bound on the ==== (PMI) of information structures as the bound on the correlation scenarios the decision maker considers. PMI relates to the distance between the joint distribution and the independent benchmark, that is, the multiplication of the marginal distributions. The higher is the bound on the PMI, the more correlation levels can be considered. As we show, modelling the perceptions of individuals about correlation in this way is general, distribution free, and allows us to complete the model by using attitudes towards ambiguity as a model of decision making when decision makers face multiple predictions.====We characterise the set of predictions of a decision maker who considers scenarios with bounded correlation structures. We show that the set of predictions is convex and compact, and is monotonic (set-wise) in the PMI bound. Moreover, it can be fully characterised by two sufficient statistics: The PMI-bound and the Naïve-Bayes belief that assumes independence.====The above results allow us to analyze the ambiguity of decision makers vis a vis the set of predictions they have generated. In particular, the convexity of the set of predictions allows us to treat them as a set of priors. Moreover, the (set) monotonicity implies a metric by which ambiguity increases as the PMI bound increases. Specifically, the model implies that larger ambiguity over correlation structures can translate to larger ambiguity over the state of the world.====In contrast to the effect of a higher PMI bound, we show that as the NB prediction becomes more informative the set of predictions shrinks. In particular, fixing the PMI-bound when the NB belief becomes degenerate, the set of predictions shrinks and converges to the NB prediction. In an application to the evaluation of financial assets we formalise the notion that the complexity of securities distorts its evaluation towards correlation neglect. Focusing on CDOs, we show that as the number of individual mortgages in a CDO increases, its evaluation becomes highly dependent on the NB belief. Thus, the evaluation of complex assets might suffer from complete correlation neglect even when, ex ante, experts allow for a wide family of correlation scenarios.====Our model therefore highlights an intuitive relation between the set of correlation structures the decision maker considers and her confidence in the decision she takes. First, when the NB belief is relatively precise, the decision maker behaves as if she completely neglects correlation, which, as already explored in the literature, implies more extreme beliefs.==== Second, correlation will affect confidence through a Knightian notion of uncertainty. A decision maker who is ambiguity averse and who entertains different possible models of correlations will tend to behave more cautiously. An increase in the set of possible correlation structure may lead to more cautious behaviour.====To illustrate the implication of the tension between these two effects we analyse several applications. We consider a simple investment application in which investors observe past investments by others and reassess their positions. We show that when the number of investors is low, investors with a high correlation bound will reduce their initial risky positions due to the cautiousness effect described above. However, when the number of investors is large, the observation of others' beliefs might imply that the NB belief becomes more precise. In turn this will result in investors substantially increasing their risky behaviour. These results can shed light on behaviour before and after the 2008 financial crisis. First, the bounds on the correlation scenarios might have increased post 2008, contributing to a shift from more risky to more cautious behaviour. Second, the volume of trade in a market can be linked to the precision of the NB belief; a low volume of trade will indicate a less precise belief, resulting in more cautiousness. In addition, as mentioned above, CDO pricing might have changed drastically before and after 2008 due to similar forces.====Finally, we study the implications of our model to group decisions. We study a jury of individuals that deliberate (that is, exchange their beliefs) and then vote. This is indeed an environment in which individuals are exposed to the same evidence and hence the perception of correlation is relevant. Moreover, jurors are obliged to deliberate and exchange information. We show how the decisive voter is determined by their correlation bounds as well as by their preferences (threshold of doubt). We contrast the normative properties of our model to those of the literature. We show that juries can both over-acquit as well as over-convict and that jury size affects decisions in novel ways.====Our results contribute to several strands of the literature. First, our results are complementary to Epstein and Halevy (2019) who also study the relation between uncertainty about correlation and ambiguity. They consider preferences over lotteries that could either depend on outcomes of draws from one urn or from two urns. They show in their experiments that subjects exhibit stronger aversion to ambiguity when considering lotteries over the two urns rather than over each one separately, implying ambiguity aversion with relation to correlation across urns. They consider a model which allows for multiple “sources” of uncertainty, capturing risk, bias and correlation. Our model, in the prism of their analysis, considers predictions of decision makers who already observe draws from different individual “urns”, and therefore focuses only on ambiguity over correlation.====Second, we contribute to a recent large literature on correlation neglect, i.e., a behavioural assumption that individuals neglect taking account of possible correlations between multiple sources of information.==== Enke and Zimmerman (2013), Kallir and Sonsino (2009) and Eyster and Weizsäcker (2011) show how correlation neglect arises in experiments, with the latter two focusing on financial decision making. The papers on correlation neglect assume, in some environments, a misspecified model held by the decision makers, inducing wrong beliefs. In our framework the decision maker has a set of potentially misspecified models, with bounded degrees of correlation. Our results show that not only this too can end with a wrong belief, but that the wrong belief is the one associated with complete correlation neglect. That is, even when decision makers consider bounded correlation, when the naïve interpretation of the data is very informative, they all behave as if they have correlation neglect.====Finally, our results contribute to the literature in finance, which has since the 2008 crisis, considered extensively the issue of the uncertainty about correlation in default rates as well as across stress tests.==== Our framework rationalises the procedures employed by rating agencies and investment banks when these evaluate complex assets. Risk analysis in financial firms that evaluate CDOs uses the individual level data of the loans making up a CDO, and then considers different correlation scenarios. As the FCIC report documents, “The M3 Prime model let Moody's automate more of the process... Relying on loan-to-value ratios, borrower credit scores, originator quality, and loan terms and other information, the model simulated the performance of each loan in 1250 scenarios.” Indeed in practice, the set of scenarios that are considered by this analysis has a particular structure; treating forecasts as independent is often used as a benchmark.==== Around this benchmark, the level of correlation implicit in these scenarios is typically bounded. Tractability and simplicity imply that the different models used for generating correlation only allow for modest levels of correlation, using a small number of correlation parameters. When dealing with large numbers of components (e.g., the number of loans in a CDO), this implies bunching many assets to have the same correlation patterns (the “homogeneous pool” problem).====Our results indicate that cautious or risky financial decisions can be a result of the tension between ambiguity aversion and the Naïve-Bayes updating rule. There are many anecdotes illustrating that decision making in organisations is sometimes akin to ambiguity attitudes, where either “optimists” or “pessimists” prevail. Anil K Kashyap's paper prepared for the FCIC observes that before the crisis there was an: “...inherent tendency for the optimists about the products to push aside the more cautious within the organisation”. After the crisis became apparent, “pessimism” prevailed: “Moody's officials told the FCIC they recognised that stress scenarios were not sufficiently severe...analysts took the “single worst case” from the M3 Subprime model simulations and multiplied it by a factor in order to add deterioration.”",Combining forecasts in the presence of ambiguity over correlation structures,https://www.sciencedirect.com/science/article/pii/S0022053118303144,8 June 2020,2020,Research Article,116.0
"Auster Sarah,Kellner Christian","Department of Decision Sciences and IGIER, Bocconi University, Italy,Department of Economics, University of Mannheim, Germany,Department of Economics, University of Southampton, United Kingdom of Great Britain and Northern Ireland","Received 29 March 2019, Revised 6 December 2019, Accepted 4 May 2020, Available online 15 May 2020, Version of Record 12 January 2022.",https://doi.org/10.1016/j.jet.2020.105072,Cited by (4),"We study the properties of Dutch auctions in an independent private value setting, where bidders face uncertainty over the type distribution of their opponents and evaluate their payoffs by the worst case from a set of probabilistic scenarios. In contrast to static auction formats, participants in the Dutch auction gradually learn about the valuations of other bidders. We show that the transmitted information can lead to changes in the worst-case distribution and thereby shift a bidder's payoff maximizing exit price over time. We characterise the equilibrium bidding function in this environment and show that the arriving information leads bidders to exit earlier at higher prices. As a result, the Dutch auction systematically generates more revenue than the first-price auction.","The descending price (or Dutch) auction and the first-price sealed-bid auction are known to be strategically equivalent in the canonical model, regardless of whether bidders are risk-neutral or risk-averse, whether values are affiliated or independent, etc. An important assumption underlying the canonical model is that bidders are subjective expected utility maximizers and, as such, quantify the uncertainty over other bidders' private information with a single prior belief (or behave as if they do). Yet, in practice, bidders often do not know with precision the distribution of their opponent's private valuation, especially when transactions are infrequent and past data points are scarce. This paper revisits the properties of the descending price auction and shows that the consideration of uncertainty about the distribution of bidders' valuations has important implications for equilibrium bidding and revenue properties.====We study a setting with two bidders, whose private valuations are drawn independently from a continuum of values, according to some unknown distribution. From the viewpoint of a bidder, the descending price auction takes the form of an optimal stopping game in continuous time. At each point in time, the bidder decides whether to terminate the auction and pay the currently displayed price or whether to wait for a lower price. We depart from the benchmark model by assuming that bidders demand robustness with regard to the uncertainty they perceive. Their preferences are represented by the maxmin expected utility model (Gilboa and Schmeidler, 1989).==== Accordingly, bidders evaluate each bid by the worst case from a set of distributions that they view as possible.====An important feature of the Dutch format is that it gradually transmits information to the participants: given a bidding strategy, players learn about their opponent's valuation from the observation that the auction has not yet been terminated. In particular, as time progresses, bidders revise their beliefs towards lower and lower types. We assume that bidders update each of the distributions and consider the worst case over the set of updated distributions. Updating is thus prior by prior, also referred to as full Bayesian updating. Apart from a smoothness condition, we do not impose any restrictions on the set of distributions over which bidders minimise and, therefore, do not rule out that the worst-case scenario for a bidder, identified by one of the distributions in the set, changes over time. When such changes occur, bidders face a time-inconsistency problem: their current optimal exit point may no longer be optimal at a later point in time.==== We assume that bidders are sophisticated in that they correctly anticipate how their future selves will act and optimise accordingly.====We start the analysis with an important preliminary result: provided the other bidder follows a strictly monotone strategy, a bidder locally evaluates his expected payoff from continuing the auction at a given point in time with the distribution function that has the ====. The reversed hazard rate for a probability distribution with cumulative distribution function ==== and density function ==== is described by the ratio ==== (see Fig. 1). It determines the rate by which the probability of winning the auction falls at a given point in time conditional, on reaching that time.====When the maximal reversed hazard rate belongs to different distributions at different points of the support, bidders' preferences over exit prices change over time. The fact that time is continuous makes such preference reversals challenging. To address this issue, a new approach is followed: we construct an auxiliary distribution function with the property that its reversed hazard rate agrees with the maximal reversed hazard rate in the set of distributions over which bidders minimise, illustrated in Fig. 2. Using the auxiliary probability distribution, we follow a standard first-order condition approach to derive the candidate equilibrium bidding function; as if both bidders had a single prior belief corresponding to the auxiliary distribution. Finally, to verify the equilibrium, we leverage the fact that bidders foresee the behaviour of their future selves and use a backward induction argument.====The auxiliary distribution function method not only provides a surprisingly simple characterization of the equilibrium bidding function but also allows for a clean comparison of the properties of the equilibrium outcome of the Dutch auction with that of other auction formats. We show that the auxiliary distribution first-order stochastically dominates the lower envelope of the set of distributions over which bidders minimise, strictly so whenever there is no single distribution in the set that maximizes the reversed hazard rate at each point of the support. Using this feature, we can show that the Dutch auction generates more revenue than the first-price auction, where the ranking is strict if and only if the first-order stochastic dominance is strict. Combining our result with those of the earlier literature (e.g. Lo, 1998) allows us to conclude that the Dutch auction dominates the other standard auction formats in terms of revenue, strictly so for a large class of sets of probability distributions.====Why does strategic equivalence between the Dutch auction and the first-price auction fail? At each point in time, a bidder evaluates his utility of ending the auction at a later time with the conditional distribution under which winning the auction is least likely. Hence, he uses the lower envelope of the set of updated distribution functions to evaluate his continuation payoff. In the first-price auction, bidders receive no information, which means that they use the lower envelope of the set of unconditional distributions. The same is true at the start of the Dutch auction. However, once the price starts descending, bidders discard the possibility of the opponent's valuation being very high and, hence, no longer care about the upper tail of the distribution. Instead they are concerned about the probability of losing the auction if they wait a little longer and evaluate their continuation payoffs with the distribution that maximizes this probability. Since this distribution is in general not the same as the one used in the beginning of the auction, exiting immediately becomes more attractive than it would have been from an ex-ante point of view. The gradual disclosure of information in the Dutch auction thus leads bidders to end the auction earlier at higher prices, which then benefits the auctioneer. Finally, we extend our results to the case where the auctioneer sets a reserve price and show that the optimal reserve price in the Dutch auction is smaller than in the first-price auction.====The contribution of this paper is twofold. First, following Bose and Daripa (2009, more about this below), we strengthen the rationale for the use of descending prices. Besides actual Dutch auctions (e.g., for flowers, treasury bonds, etc.), descending prices appear informally in many situations, such as in the housing market, where asking prices of listed homes decline until a buyer is found. Lucking-Reiley (1999) provides an interesting empirical study on the revenues generated by the different auction formats, using field experiments that auction off collectible trading cards over the Internet. In each experiment, two identical cards are sold: one using a dynamic format (Dutch or English auction), the other using the strategically equivalent static format (first- or second-price auction). While the English auction and the second-price auction produce roughly the same revenue, the Dutch auction generates 30 percent higher revenues than the first-price auction, in line with our theory.==== Given that our revenue result holds regardless of the set of probability distributions representing bidders' beliefs—or the seller's knowledge thereof—choosing a descending price auction over another standard format can be viewed as a robust way of increasing revenue. Secondly, we make a methodological contribution by demonstrating a new way of deriving optimal strategies for games in continuous time with preference reversals related to worst-case robustness: after identifying the characteristics of the distribution functions that matter for local optimality conditions, an auxiliary distribution function that shares these characteristics with the worst case is found, mapping the problem into a simpler one with a single distribution. This method is not restricted to dynamic auctions but can be applied to other stopping problems.==== There is a growing literature on ambiguity and auctions. While most work in that literature is focused on static settings,==== some papers have considered dynamic auctions as well. Karni (1988) is the first to show that the equivalence of first-price and descending price auctions relies on bidders being dynamically consistent, however without providing a description of equilibrium bidding when dynamic consistency fails. Nakajima (2011) finds that if preferences exhibit the typical choice pattern of the Allais paradox, the Dutch auction will result in higher bids and thus higher revenue. Conversely, Weber (1982) studies a non-expected utility preference of a form that displays a reversal of the typical Allais choices, which could be more empirically relevant for small stakes. For such non-expected utility preferences, the first-price auction generates higher revenues. Gosh and Liu (2019) study sequential first-price auctions for multiple units of the same good with ambiguity-averse bidders. They show that higher degrees of ambiguity are reflected in larger price drops between the auctions. Closest to our paper is the work by Bose and Daripa (2009), who are the first to emphasize the benefits of using descending prices when agents are ambiguity-averse. We discuss the relation to their work in detail in Section 5.3.====The Dutch auction is an instance of a dynamic game of incomplete information with ambiguity-averse players. Some recent papers discuss general solution concepts and their properties. Battigalli et al. (2019) explore self-confirming equilibria in dynamic games with players whose preferences are represented by the smooth ambiguity model of Klibanoff et al. (2005). Like us, they permit dynamic inconsistency and assume that players are sophisticated. Also Hanany et al. (2018) consider multistage games with incomplete information and smoothly ambiguity-averse players. In contrast to Battigalli et al. (2019), they require equilibria to satisfy sequential optimality and show that such games are as if players update in a dynamically consistent way. In their framework, suitably extended to continuous time, the Dutch and first-price auctions thus remain strategically equivalent. Finally, Pahlke (2018) considers a setting in which players have maxmin expected utility and update their beliefs prior by prior, as do we. In contrast to our framework, she allows the set of probability distributions over which players minimize to depend on the game that is played in such a way that dynamic consistency is maintained within the game. For our setting, this requires that players expand the initial set of probabilities ==== so as to include the auxiliary distribution function ==== when faced with the Dutch auction but not when playing a first-price auction. The interpretation changes in this case, while the main insights do not.====There is also some work on dynamic games with ambiguity-averse preferences in applications unrelated to auctions. For instance, Kellner and Le Quement (2017, 2018) study the effects of exogenous and endogenous ambiguity in a cheap-talk setting; (Beauchene et al., 2019) consider the case of persuasion. As in our work, both consider consistent planning equilibria with prior-by-prior Bayesian updating.====The paper proceeds as follows. In Section 2, we describe the environment, in particular the auction game and the solution concept. Section 3 derives the bidding equilibrium, providing the main constructive steps, while Section 4 presents the revenue result and explains the role of reserve prices. Section 5 discusses the case of more than two bidders, the role of the updating rule and sophistication, and the contribution with respect to previous work by Bose and Daripa (2009). Section 6 concludes.",Robust bidding and revenue in descending price auctions,https://www.sciencedirect.com/science/article/pii/S0022053120300685,15 May 2020,2020,Research Article,117.0
"Grant Simon,Rich Patricia,Stecher Jack","Australian National University, Canberra, Australia,Hamburg University, Hamburg, Germany,University of Bayreuth, Bayreuth, Germany,University of Alberta, Edmonton, Canada","Received 3 March 2020, Accepted 6 March 2020, Available online 10 March 2020, Version of Record 12 January 2022.",https://doi.org/10.1016/j.jet.2020.105027,Cited by (3),"We provide a theory of decision under ambiguity that does not require expected utility maximization under risk. Instead, we require only that a decision maker be probabilistically sophisticated in evaluating a subcollection of acts. Three components determine the decision maker's ranking of acts: a prior, a map from ambiguous acts to equivalent risky lotteries, and a generalized notion of certainty equivalent. The prior is ====, defined over the inverse image of acts for which the decision maker is probabilistically sophisticated. Ambiguity preferences are similar to Hurwicz, depending on an act's best- and worst-case interpretations. The generalized certainty equivalent may, but need not, come from a Bernoulli utility. The ability to combine appealing theories of risk and ambiguity at will has been sought after but missing from the literature, and our decomposition provides a promising way forward.","Decision makers are said to face ambiguity when they do not have a uniquely defined prior over some decision relevant events. For example, climate change projections from different models provide credible but divergent estimates of the probability distribution of climate sensitivity (see the discussion in Heal and Millner, 2014). How a decision maker (DM) responds to ambiguity (as when evaluating climate policies) is a distinct question from how the DM makes calculated risks with known probabilities. For instance, it may be that a DM is calculating and level-headed when assessing climate change but a maniacal fiend when she enters a casino.====Our goal is to isolate the role of ambiguity, enabling researchers to study it without committing to a particular theory of decision under risk. Attitudes toward known risks can be studied in some collection of problems on which the DM is probabilistically sophisticated (Cohen et al., 1987; Machina and Schmeidler, 1992)—that is, on which known probabilities guide her actions, regardless of whether she follows expected utility or some other theory of decision under risk. Whether the DM is Bayesian in forming beliefs is a different question from whether she has a Bernoulli utility function (Machina and Schmeidler, 1995) (or, employing the terminology of Nehring (2007), the DM may be probabilistically-sophisticated without necessarily being utility-sophisticated). Requiring no more than probabilistic sophistication can reassure a researcher that findings about behavior under ambiguity are not artifacts of an underlying assumption about attitudes toward risks.====This is easier said than done: there have been a slew of negative results showing that, under natural assumptions, existing models of decision under ambiguity force the researcher to make a choice. Adopt expected utility theory as the decision model for risk, or commit to a world in which all non-degenerate events are ambiguous. Beginning with Marinacci's (2002) results on the multiple priors model, this finding was shown by Strzalecki (2011) to apply to the variational model (Maccheroni et al., 2006), and generalized by Cerreia-Vioglio et al. (2012) to a wider collection of theories of decision under ambiguity.====We do not wish to interpret these formal results as implying that we cannot model all of the agent types that we would like. It remains desirable to jointly model non-EU risk preferences and common ambiguity preferences, as various attempts show. For example, Nakamura (1995) provides a joint axiomatization of rank dependent utility (regarding risk) and Choquet expected utility (regarding ambiguity). Dean and Ortoleva (2017) propose a possible hedging axiom that could drive non-EU behavior under both risk and ambiguity. Similarly, Lleras et al. (2019) characterize preferences which can equally be interpreted as loss averse (non-EU regarding risk) or as MEU (ambiguity averse).====Success in this direction, however, remains tied to specific theories of decision under both risk and ambiguity. In principle, however, these preference types are independent, and a DM might display any combination of risk and ambiguity attitudes. It is therefore desirable to build a theory which is agnostic regarding both types of attitude, and in which their effects are isolated. This paper aims to provide such a theory: the roles of the DM's prior probability, attitude toward ambiguity, and attitude toward known risk are disentangled. In the spirit of Machina and Schmeidler (1995), we aim to separate Hurwicz (1951) from Bernoulli, while keeping Bayes.====We address each of these pieces in turn. Our first step is to consider a collection of acts thought of as risky. Since the DM is probabilistically sophisticated, she has a prior probability over the different consequences of a risky act. We can therefore work with the inverse image along the prior of each prize, and restrict attention to lotteries with objective probabilities.====We use risky acts as our building blocks for studying ambiguous acts. Following the approach in the expected uncertain utility (EUU) model of Gul and Pesendorfer (2014) (foreshadowed partly in Jaffray, 1989), we replace each ambiguous act with a closed interval of risky acts. The lower and upper bounds on these intervals correspond to the most extreme cases of how the ambiguity might be resolved. The DM's ambiguity preferences are captured by her ==== (monetary prize) for each corresponding interval of prizes.====The last piece is the DM's ==== function, which reflects her risk preferences. The sure equivalent takes as its input simple lotteries over monetary prizes, built from the prior and the unambiguous equivalent. It maps each such lottery to the prize which the DM views as equivalent.====These three pieces give our main result. Given probabilistic sophistication, we show that if a DM satisfies axioms similar to those of Gul and Pesendorfer (2014), along with a strengthening of Savage's (1954)'s postulate P3 that embodies a notion of (stochastic) independence between risk and ambiguity, then the DM's preferences can be represented with the composition of our three components. This provides a way to generalize Gul and Pesendorfer's (2014) Expected Uncertain Utility (EUU), which we call a Generalized Uncertain Utility (GUU) representation.====A simple example illustrates how these three pieces combine, applying them step-by-step to determine the DM's certainty equivalent. This lets us rank arbitrary acts. The example is related to Machina's (2009) Reflection Effect, which causes problems of one kind or other for most major theories of decision under ambiguity (Baillon et al., 2011).==== Both risk and ambiguity are present, and by addressing the two separately, we are able to account for common behavior (see L'Haridon and Placido, 2010, for experimental evidence).====Let there be four states of the world, referred to as ====, ====, ====, and ====. The events ==== and ==== are risky, each occurring with probability 0.5. Beyond these, the DM has no further information about any nontrivial events. In keeping with our motivation, we can imagine that the setting is one of choice given anticipated climate change. The ambiguous event could be whether a particular important species will still be around in 10 years. The risky event could be whether an emerging biotechnology will be viable at the end of the same 10-year period (taking the liberty of assuming that we can measure this).====For concreteness, consider the acts in Fig. 1. The first observation regarding this choice problem is that some popular models of decision under ambiguity require indifference between ==== and ====, and between ==== and ====, while strict preferences are intuitive and common (Machina, 2009; L'Haridon and Placido, 2010). The second important observation is that the choice between ==== and ==== is equivalent to the choice between ==== and ====; ==== is a mirror image of ====, and similarly ==== is a mirror image of ====. However, Machina (2009) notes that shifting 0 to 4 in event ====, then shifting 4 down to 0 in event ==== allows us to obtain act ==== from ==== and ==== from ====. This sequence of common outcome tail shifts suggests that a DM who prefers ==== to ==== should prefer ==== to ====. Experimentally, however, most subjects' choices instead reflect the informational symmetry; here, the preference between ==== and ==== would match the preference between ==== and ==== (L'Haridon and Placido, 2010).====Our approach both preserves the indifferences arising from the informational symmetry and allows for strict preferences between ==== and ==== (as well as their counterparts). In order to show how our theory works, we show how an agent's underlying ambiguity preferences would yield an act ranking. Act ==== gives an uncertain prize of ==== with probability 1/2 and an uncertain prize of ==== with probability 1/2. Let ==== be the prize in the set of outcomes ==== that the DM views as equivalent to the ambiguous prize ranging from ==== to ==== (where ====). For example, suppose that the DM views an ambiguous outcome between ==== and ==== as equally desirable as receiving ====, similar to the Cobb-Douglas ambiguity preferences Binmore (2009) proposes (see the discussion in Grant et al., 2019). Then we can replace ==== with a lottery paying ==== with probability 1/2 and ==== with probability 1/2. Similarly, we can replace ==== with a lottery paying ==== with probability 1/2 and ==== with probability 1/2. Now let ==== be the sure equivalent. We rank the acts ==== and ==== by comparing==== where ==== is the certainty equivalent for a given act ====.====Clearly there is nothing that requires the certainty equivalents of ==== and ==== to be the same in this case. If the DM is close to a risk-neutral expected utility maximizer, she would prefer ==== to ====. But she might prefer ==== to ==== either because she is risk averse and an expected utility maximizer, or for many reasons that would be consistent with non-EU decisions under risk: ==== has a higher security level (Gilboa, 1988; Jaffray, 1988; Cohen, 1992), it may appeal to disappointment averse decision makers (see Gul, 1991), and so on.====A final feature of our approach is worth noting here. A concern about models involving both ambiguity and known risks is that it may be possible for a DM to hedge ambiguity (see Raiffa, 1961). One way to address this is to relax the requirement that risky events form a ====-algebra, instead imposing only that they form a ====-system (Epstein and Zhang, 2001). This idea is taken up in the context of probabilistic sophistication by Chew and Sagi (2008), and is the approach we follow here.====A formal definition of a ====-system appears in Section 2, but at this stage it is enough to note that it is similar to a ====-algebra but with the important difference that it is required to be closed only under complements and countable disjoint unions; it need not be closed under intersections. For instance, we can imagine a DM who is concerned about climate change and faces decisions related to the weather in Canberra. The following example shows in this context why we should not expect the set of risky events to be closed under intersections.====Notice that the event “June's average maximum temperature is above its historic mean” and the event “either both June's average maximum temperature and its rainfall are above their respective historic means or both are below”==== are both risky (with probability 1/2). However, their intersection, the event “both June's average maximum temperature and its rainfall are above their respective historic means”==== has unknown probability and hence is not risky. That is, the set of risky events is not closed under intersection although casual inspection reveals that the above collection of risky events is closed under complements and disjoint unions. A ====-system is thus the more appropriate structure to represent the collection of risky events.====The rest of the paper is organized as follows. We begin with the basic set-up in Section 2. We then present in Section 3 the formal definition of the family of Generalized Uncertain Utility maximizers which is followed in Section 4 with our set of axioms and representation theorem. Section 5 provides one way to endogenize the collection of risky events that were taken as given in our axiomatization. We achieve this by employing Epstein and Zhang's (2001) behavioral definition for classifying ==== events. In Section 6 we present a finite state version of the model. We conclude in Section 7 with two examples that demonstrate the two broad ways in which the family of GUU maximizers extends Gul and Pesendorfer's (2014) family of EUU maximizers. Unless stated otherwise, proofs appear in the appendix.",Bayes and Hurwicz without Bernoulli,https://www.sciencedirect.com/science/article/pii/S0022053120300326,10 March 2020,2020,Research Article,119.0
"Cerreia-Vioglio Simone,Maccheroni Fabio,Marinacci Massimo","Università Bocconi and IGIER, Italy","Received 10 April 2019, Revised 30 April 2019, Accepted 5 May 2019, Available online 28 June 2019, Version of Record 12 January 2022.",https://doi.org/10.1016/j.jet.2019.05.003,Cited by (3),"We study how changes in ==== affect ambiguity attitudes. We define a decision maker as decreasing (resp., increasing) absolute ambiguity averse if he becomes less (resp., more) ambiguity averse as he becomes richer. Our definition is behavioral. We provide different characterizations of these attitudes for a large class of preferences: monotone and continuous preferences which satisfy risk independence. We then specialize our results for different subclasses of preferences. Inter alia, our characterizations provide alternative ways to test experimentally the validity of some of the models of choice under uncertainty.","Beginning with the seminal work of David Schmeidler, several choice models have been proposed in the past thirty years in the large literature on choice under uncertainty that deals with ambiguity, that is, with Ellsberg-type phenomena.==== At the same time, many papers have investigated the economic consequences of ambiguity. Our purpose in this paper is to study a basic economic problem: How the ambiguity attitudes of a decision maker change as his wealth changes. In other words, our purpose is to study absolute and relative ambiguity attitudes in terms of changes in ====. This is an alternative and complementary approach to some of the existing literature, discussed at the end of this introduction, which in contrast studies how ambiguity attitudes change in terms of utility shifts rather than wealth shifts. Put differently, this literature studies absolute and relative ambiguity attitudes in terms of changes in ====.====We are motivated by the fact that the relationship between wealth and agents' risk attitudes plays a central role in many fields of Economics (e.g., portfolio allocation problems and insurance demand). For example, in his seminal work (Arrow, 1971, p. 96), Arrow, in discussing measures of absolute and relative risk attitudes, mentions that “The behavior of these measures as wealth changes is of the greatest importance for prediction of economic reactions in the presence of uncertainty”.====To the best of our knowledge, no systematic study has been done in exploring a similar relation between ==== and ==== attitudes, despite the large and growing use in applications of models that are nonneutral toward ambiguity. In this context, Arrow's comment would seem to apply all the more. The challenge of our work, compared to the analysis done under risk by Arrow and Pratt, is that their study has been restricted to the expected utility model. Under ambiguity, instead, there are by now several alternative models, thus moving the analysis well beyond expected utility. In characterizing how ambiguity attitudes change with wealth, our results might provide some guidance in choosing between these models, as the standard theory of absolute risk aversion of Arrow and Pratt provides guidance in the choice of the von Neumann-Morgenstern utility function. For example, our results will show that a researcher who believes that agents are not constant absolute ambiguity averse – be that due to experimental evidence and/or personal introspection as for Arrow's assumption of decreasing absolute risk aversion – can rule out the use of some models: for example, ====-maxmin, Choquet expected utility, and variational preferences under risk neutrality. Similarly, for a researcher relying on the smooth ambiguity model, behavioral assumptions on absolute and relative ambiguity attitudes translate into corresponding choices of the model's parameters. For instance, if risk attitudes are assumed to be CRRA and risk averse, as common in Macroeconomics,==== and relative ambiguity attitudes are assumed to be constant as well (irrespective of the prior ====), then our results yield that ==== must be either CARA or CRRA, depending on the von Neumann-Morgenstern function being either the logarithm or the power function.====Finally, our work provides alternative and useful methods to falsify models of choice under ambiguity, as well as testable implications. For example, on the one hand, under the assumption that agents are CARA,==== falsifying our notion of constant absolute ambiguity attitudes yields that preferences cannot be invariant biseparable preferences (e.g., ====-maxmin and Choquet expected utility). On the other hand, under the assumption that agents are CRRA,==== if we observe that the share invested in the uncertain asset is not constant with wealth, then we can conclude that preferences cannot be invariant biseparable.====Given any formalization of points 1 and 2, one can say that ≿ is decreasing absolute ambiguity averse if ambiguity aversion is higher at lower wealth levels, that is, ==== yields that ==== is more ambiguity averse than ====. Clearly, when preferences are defined over lotteries, this approach exactly mirrors how decreasing absolute risk attitudes are defined. Conversely, under ambiguity the above approach might lead to different definitions depending on which formal notion one translates points 1 and 2 into. In our case, preferences over final wealth levels are modeled by a binary relation ≿ on ====. Given a wealth level ==== and an act ====, we define by ==== the act whose final monetary outcomes are those of ==== shifted by ==== (see Section 2.1, for a formal definition). Thus, as for point 1, we define preferences at wealth level ==== by==== As for point 2, we rely on the comparative notion of Ghirardato and Marinacci (2002).==== Finally, in a similar fashion, we also define the notions of increasing and constant absolute ambiguity aversion (see Definition 3).====Before proceeding, we reiterate that there are other, rather different, approaches that formalize absolute ambiguity attitudes in terms of ==== rather than ==== (see the related literature below). Since our goal here is to talk about wealth effects, we will always talk about these attitudes as wealth decreasing (resp., increasing, constant) absolute ambiguity averse attitudes and we will refer to them as ====-DAAA (resp., ====-IAAA, ====-CAAA), thus adding the “wealth” qualifier to our notion.",Ambiguity aversion and wealth effects,https://www.sciencedirect.com/science/article/pii/S002205311930050X,28 June 2019,2019,Research Article,122.0
"Borovička Jaroslav,Stachurski John","New York University, United States of America,NBER, United States of America,Research School of Economics, Australian National University, Australia","Received 29 June 2020, Revised 10 February 2021, Accepted 18 February 2021, Available online 1 March 2021, Version of Record 3 March 2021.",https://doi.org/10.1016/j.jet.2021.105227,Cited by (1),"We obtain an exact necessary and sufficient condition for the existence and uniqueness of equilibrium asset prices in infinite horizon, discrete-time, arbitrage free environments. Using local spectral radius methods, we connect the condition, and hence the problem of existence and uniqueness of asset prices, with the recent literature on stochastic discount factor decompositions. Our results include a globally convergent method for computing prices whenever they exist. Convergence of this iterative method itself implies both existence and uniqueness of equilibrium asset prices.","One fundamental problem in economics is the pricing of an asset paying a stochastic cash flow with no natural termination point, such as a sequence of dividends. In discrete-time no-arbitrage environments, the equilibrium price process ==== associated with a dividend process ==== obeys==== where ==== is the sequence of single period stochastic discount factors.==== Two questions immediately arise in connection with these dynamics:====These questions have become more pressing for two reasons. First, models of dividend processes and state price deflators are becoming more sophisticated, in an ongoing effort to better match financial data and resolve outstanding puzzles (see, e.g., recent iterations of the models in Campbell and Cochrane (1999), Barro (2006), or Bansal and Yaron (2004)). This complexity makes questions 1–2 challenging, especially in quantitative applications with discount rates close to the growth rates of underlying cash flows. There have been few sufficient conditions proposed that (a) imply existence and uniqueness of equilibrium prices, (b) are weak enough to be useful in modern quantitative analysis, and (c) are practical enough to implement in interesting applied settings.====The second reason that questions 1–2 have become more pressing is the accumulating evidence that nonlinearities embedded in the original models matter for quantitative analysis. For example, Pohl et al. (2018) and Lorenz et al. (2020) show that the log-linearization techniques used to solve asset pricing models can lead to large distortions in the equity premium and price volatility. These findings increase the need for practical methods for investigating the underlying structure of modern asset pricing models.====In this paper, we introduce a condition for existence and uniqueness of equilibria that is both weak enough to hold in realistic applications—in fact necessary as well as sufficient—and practical in the sense that testing the condition focuses on a single value. This value is referred to below as the ====. To illustrate key ideas, consider the case of stationary dividend growth, which is the standard assumption in quantitative applications. Seeking a stationary price-dividend ratio, we rewrite (1) as====Let ====. For this class of models, the stability exponent is==== where ==== is the expectation of the ====-period pricing kernel adjusted for dividend growth. Uncertainty in the discount process ==== and dividend growth ==== is driven by an irreducible state process, and ==== takes expectations over the unique stationary distribution.====We show that, in this setting, existence and uniqueness of an equilibrium price process is exactly equivalent to the statement ====. In addition, successive approximations converge globally to equilibrium prices if and only if ====. We also show that convergence of this algorithm itself implies that the limit is an equilibrium and, moreover, that ====. Therefore, convergence from a single initial condition implies that the limit is an equilibrium, and that these equilibrium prices are unique and globally attracting.====Interpreting the condition ==== is straightforward. Let ==== denote the price of a claim on the dividend paid out ==== periods ahead at the current state ====, normalized by the current dividend. The pricing of these so-called dividend strips has been analyzed extensively, see, e.g., van Binsbergen et al. (2012) or van Binsbergen et al. (2013). Due to irreducibility, ==== can be replaced by ==== for limiting events, so ==== for large ====. The condition then states that, asymptotically, prices of long-horizon dividend strips decay to zero at a geometric rate.====The intuition is particularly simple in the case where dividends are stationary. We can then replace ==== in (3) with ====. Now ==== is the price of a risk-free zero-coupon bond with maturity ==== at current state ====, so ==== is the corresponding yield to maturity. Since ==== for large ====, the condition ==== means that, in the limit, yields on risk-free long bonds are positive. This indicates a fundamental preference for current payoffs over future payoffs, which generates finite, well defined prices for stationary infinite horizon cash flows.====While ==== has a natural interpretation, it is striking that this condition is necessary as well as sufficient for existence, and hence exactly characterizes the set of models with well defined equilibrium prices. This result rests on irreducibility mentioned above, which is a mild condition, and a “local spectral radius” result due to Zabreiko et al. (1967) and Forster and Nagy (1991). Using this result, we show that, for any positive cash flow with finite first moment, the asymptotic mean growth rate of its discounted payoff stream is equal to the principal eigenvalue of an associated valuation operator, which is in turn equal to the exponential of ====. If the principal eigenvalue equals or exceeds unity, then the sum of expected discounted payoffs grows without bound.====An operator-theoretic way to understand our results is to view ==== as a random “contraction factor” for an operator that has, as its fixed point, an equilibrium price function. If there is a constant ==== with ==== with probability one, then valuation equations (1) and (2) imply this operator will be a contraction of modulus ====, yielding existence of a unique equilibrium. However, in most applications, ==== holds on a set of positive probability, due to the fact that payoffs in bad states have high value. Thus, a direct one step contraction argument is problematic. Hence we adopt the weaker condition ====, which requires instead that ==== holds on average over the long run, and show that this is both necessary and sufficient.====We also discuss methods for calculating ==== when no analytical solution exists. Similar to Backus et al. (1989), we show that, when the state space is finite, the rate of decay of prices of long-term dividend strips and hence ==== can be calculated using numerical linear algebra. For other cases, we propose a Monte Carlo method that involves simulating independent realizations of the pricing kernel. This method is inherently parallelizable, sufficiently accurate for the applications we consider, and well suited to settings where the state space is large.====As one illustration of the method, we consider a model of asset prices with Epstein–Zin recursive utility, multivariate cash flows and time varying volatility studied in Schorfheide et al. (2018). Hitherto no results have been available on existence and uniqueness of equilibria in the underlying theoretical model. We show that ==== holds at and in the neighborhood of the benchmark parameterization in a global numerical approximation of the model. This indicates existence of a unique set of equilibrium prices, along with a globally convergent method of computing them. The fact that our conditions are necessary as well as sufficient allows us to examine how far this positive result can be pushed as we shift parameters relative to the benchmark.====We also encompass and extend the classic result of Lucas (1978), who studied a model with infinite state space and SDF of the form==== Here ==== is a stationary consumption process, ==== is a state independent discount component and ==== is a period utility function. Using a change of variable, Lucas (1978) obtains a modified pricing operator with contraction modulus equal to ====, and hence, by Banach's contraction mapping principle, a unique equilibrium price process. His theorem is a special case of our main result.====While Lucas (1978) frames his study in a space of bounded functions, our analysis admits unbounded solutions. This is achieved by embedding the equilibrium problem in a space of candidate solutions with finite first moments. Such a setting is arguably more natural for the study of forward looking stochastic sequences, since the forward looking restriction is itself stated in terms of expectations. Adopting this setting allows us to generalize the existence and uniqueness results for equilibrium prices obtained in Calin et al. (2005) and Brogueira and Schütze (2017), which extend Lucas (1978) by allowing for habit formation and unbounded utility.====Our work is also connected to the literature on stochastic discount factor decompositions found in Alvarez and Jermann (2005); Hansen and Scheinkman (2009); Hansen (2012); Borovička et al. (2016); Christensen (2017); Qin and Linetsky (2017) and other recent studies. These decompositions are used to extract a permanent growth component and a martingale component from the stochastic discount process, with the rate in the permanent growth component being driven by the principal eigenvalue of the valuation operator associated with stochastic discount factor. We show that the log of this principal eigenvalue is equal to ==== in our setting, using the local spectral radius result discussed above.====In addition, our work is related to Pohl et al. (2019) and Christensen (2020), who provide conditions for existence and uniqueness of recursive utilities in settings where the state space and rewards are unbounded. While the objects in play are different (recursive utilities vs asset prices), the techniques are related because both sets of problems treat forward looking recursions over unbounded state spaces driven by exogenous state processes. The connection can be summarized as follows: Our results are not applicable for most recursive utility problems concerning existence and uniqueness, where nonlinearities require specialized techniques (e.g., the Orlicz space methods in Christensen (2020) or Jensen-type bounds in Pohl et al. (2019)). At the same time, our methods have comparative advantage for asset pricing, because they exploit positivity and affine structure (which follow from nonexistence of arbitrage). This allows us to obtain conditions for existence and uniqueness that are necessary as well as sufficient.==== In addition, we use this same no arbitrage structure, combined with properties of positive linear operators, to translate spectral radius conditions over valuation operators into the analytically and computationally convenient stability exponent ====. Finally, we offer techniques for computing ==== analytically, as well as via linear algebraic methods and through Monte Carlo.====The rest of the paper is structured as follows. The main results are presented in Section 2. Sections 3–5 treat applications with stationary dividend growth and Section 6 concludes. Appendix A discusses models with stationary dividends, rather than stationary dividend growth. A discussion of numerical methods for implementing our test can be found in Appendix B. Long proofs are deferred until Appendix C. Computer code that replicates our numerical results and figures can be found at ====.",Stability of equilibrium asset pricing models: A necessary and sufficient condition,https://www.sciencedirect.com/science/article/pii/S0022053121000442,1 March 2021,2021,Research Article,123.0
"de la Croix David,Pommeret Aude","IRES/LIDAM, Université catholique de Louvain and CEPR, London, United Kingdom,IREGE Université Savoie Mont Blanc, France","Received 12 April 2020, Revised 9 February 2021, Accepted 22 February 2021, Available online 1 March 2021, Version of Record 12 March 2021.",https://doi.org/10.1016/j.jet.2021.105231,Cited by (3),Having children is like investing in a risky project. Postponing birth is like delaying an ,"Having a child increases risk. This is especially true as far as future income is concerned. The career costs of motherhood include both first-order moment effects on wages and employment but also second-order moment effects as the following examples show. Mothers are subject to possible atrophy of skills due to random interruptions (Adda et al., 2017), a risk of not getting promoted from temporary to permanent jobs (Guner et al., 2017), more frequent occupation and workplace changes (Lundborg et al., 2016), lost earnings opportunities with possibly lower wages, and a possibility of discrimination (Correll et al., 2007). In addition, parents also endure an increasing risk in sickness absence (Angelov et al., 2013). This pattern is likely to be reinforced when children have special needs, or mitigated when children are easy to manage.====Beyond the issues of income and career, there is increased uncertainty affecting spending and utility flows. Many examples can be found in the literature: childrearing reduces women's social network size and alters the composition of men's networks (Munch et al., 1997); if network size is associated with insurance, smaller network implies less opportunities to face adversity. Childrearing may have long-term health consequences such as urinary incontinence, weight gain, etc., but also positive health consequences, such as reduced chances of having some types of breast cancer; and having a baby causes a substantial decline in the average couple's relationship (Doss et al., 2009). The most extreme case of risk incurred when being a mother is of course that of maternal mortality. The consequences of this risk for fertility have been studied in detail: exploiting variations in mortality risks across US states and cohorts, Albanesi and Olivetti (2014) show that the growth in fertility was highest for US states and cohorts of women that experienced the greatest reduction in maternal mortality. Albanesi and Olivetti (2016) show that improvements in maternal health reducing maternal mortality and morbidity are important to explain the joint evolution of married women's labor force participation and fertility in the United States during the twentieth century.====Although the literature is full of examples stressing an increase in uncertainty following the birth of a first child, it does not treat it as such (except for the maternal mortality risk). It indeed focuses on first-order moments – such as the effect of having a child on the mean wage, the employment rate, etc. – without acknowledging the risk component. Miller (2011) finds that delaying motherhood leads to a substantial increase in labor market earnings, of 9% per year of delay. This benefit goes through an increase in wages of 3% and an increase in work hours of 6%. Herr (2016) looks at the specific effect of first birth on wages. For women who entered the labor market before having children, she finds a clear monotonic relationship between delayed first birth and higher long-run wages. Budig and England (2001) look at the effect of having children on wages and employment. They find a wage penalty for motherhood of approximately 7% per child. One-third of the penalty is explained by years of past job experience and seniority, because motherhood interrupts women's employment, leading to breaks, more part-time work, and fewer years of experience and seniority. The authors guess that the remaining two-thirds of the motherhood penalty may arise from the impact of motherhood on productivity and/or from employer discrimination. Note that all these studies are based on the National Longitudinal Survey of Youth (NLSY79) which is the data set we use in our quantitative analysis as well. However, using an event-study framework, Kuziemko et al. (2018) show that substantial and persistent employment effects of motherhood in U.K. and U.S. are not anticipated by women.====In this paper, we develop a theory in which motherhood increases income risk explicitly. We stress that it is of particular importance for the optimal age at childbearing. We focus on how to model increased income risk, how to measure it in the data, and whether it matters for household choices. The main idea we develop is that if having a child is irreversible and affects expected future earnings through risk, waiting (postponing birth) has a value (option value). A robust result of option theory is that the riskier an investment project, the worthier it is to wait (Dixit and Pindyck, 1994). In a different context, we also obtain that the option value of postponing birth increases with risk. Beyond income risks, the value of waiting interacts with fecundity (the biological clock) and the availability of assisted procreation techniques.====Besides our innovation to model income risk as a function of motherhood, our model shares some characteristics with Adda et al. (2017), namely skill atrophy, intertemporal budget constraint, and risk aversion. Apart from the risk aspect, their model is richer than ours (they also consider occupational choices and marital status) and needs to be solved numerically, using indirect inference. Our approach is more parsimonious in order to allow for analytical resolution and therefore a clear grasp of the mechanisms. It can indeed be solved explicitly using stochastic optimal control and optimal control with regime switches (Boucekkine et al., 2013). Our theory highlights how the timing of the first birth depends on financial uncertainty and on the risk of infertility. The model also allows to distinguish between three types of childlessness: voluntary, natural (primary sterility), and childlessness due to postponement. It is a very first attempt to account for risk-increasing maternity and a natural extension would be to consider occupational choices and marital status.====Despite its parsimony we feel that bringing the model to data and quantifying its main mechanism are very insightful. We thus conduct a quantitative analysis, identifying the structural parameters of the model using the National Longitudinal Survey of Youth (NLSY79). This survey follows the lives of a sample of American youth born between 1957–1964 from Round 1 (1979 survey year) to Round 25 (2012 survey year). It started in 1979 with a sample of women aged 14 to 22, who were interviewed regularly from then on. Two-thirds of the sample was still observed at the end of the childbearing years, at which point 84 percent had children, which allows to study the effect and timing of childbearing on wages and employment. We show that mothers face a higher income risk than childless women. Although risk decreases with education, the risk differential between mothers and childless women increases with the education level, which partly explains why educated parents have children later.====Finally, we use the model to investigate the effect of two policies. First, introducing a hypothetical insurance against motherhood-related risks appears to be a very strong tool to reduce the age at first birth for the more educated. The empirical literature (see Gauthier (2007) for a survey, and d'Albis et al. (2015)) suggests that well-designed public policy can affect the timing of fertility, including childcare provision and lump-sum financial incentives. In unequal societies, having a well-developed market for nannies and babysitters might play the same role (Hazan and Zoabi, 2013). Our model contributes to this literature by stressing the importance of reducing not only the average opportunity cost of having children, but also the “risk opportunity cost” by helping mothers when things go wrong.==== Second, we simulate the effect of free and highly effective medically assisted procreation, which amounts to make women three years younger. This policy delays the age at first birth by less than one year for the higher education categories, and reduces childlessness, but not more than the insurance policy. Our results on assisted procreation are in line with Sommer (2016) as she finds that the introduction of IVF technology (calibrated on 2012 IVF success rates) increases the number of births but is not sufficient to compensate for the effect of the increased earning risk observed on the period studied. On the whole, our results indicate that insurance against motherhood-related risks seems more effective than artificial procreation to advance births.====There exists a literature on the optimal timing of births. A first approach is deterministic and the dynamic structure is simple, with only a choice between early and late childbearing, as in Low (2013). In her model, women can trade one more year of job experience or training for having babies early in life (and getting married). The interest of the static structure is to allow to solve for equilibrium on the marriage market, and to study its properties analytically. Pestieau and Ponthière (2014, 2015) propose a dynamic model in discrete time in which parents can have children early or late (binary choice). Here again the simple dynamic structure allows to provide a general equilibrium analysis. An early dynamic model of fertility can be found in Heckman and Willis (1976). They focus on the proximate determinants of fertility. In their approach, it is costly not to have children (cost of contraception). The other costs are not modelled. Their model suggests that a woman's reproductive history depends on the sequence of contraception decisions a couple makes. The authors notice that “the optimal decision making that they have specified requires a couple to solve a stochastic dynamic programming problem at the beginning of each month from marriage to menopause.” Later, Cigno and Ermisch (1989) focus on the interaction between physiological and financial considerations in a deterministic framework. The interactions between demographics and economics are studied by d'Albis et al. (2010) and de la Croix and Licandro (2013) in dynamic deterministic models in which women choose the time of birth. They show how the growth rate of the population is affected by this choice. Compared to all these approaches, we neglect general equilibrium effects and the marriage market aspect, but we model the time dimension more precisely, as the trade-off between fecundity and income depends crucially on age, and is not the same at 25, 35, or 40.====Even existing structural stochastic models do not explicitly make risk depend on motherhood. Francesconi (2002) and Sheran (2007) account for some uncertainty, but it takes the form of taste, technological, and/or birth control shocks that are not affected by labor or fertility decisions. For instance, Francesconi (2002) estimates the structural parameters of a finite-horizon, discrete-choice model on a sample of married women from the National Longitudinal Survey (NLS) of Young Women (1968–1991), and shows that a short interruption of full-time work is less harmful for the earnings profile than a part-time experience during childrearing. Using the same data set and the same type of model, Sheran (2007) shows that a childcare subsidy is likely to reduce women's education level, but increase their time spent working. It should be noted that even if these papers study the joint decision of female labor supply and fertility using dynamic life-cycle models, their objective is not to study childbearing decisions, but rather the consequences of children on labor-related choices in order to better predict the effect of public policies that are likely to affect both decisions. Sommer (2016) studies the decision to have children and accounts for earning risks, but again, childbirth does not affect risk: mothers and childless women face the same shocks and the same asset accumulation. Note however that due to motherhood, women may decide to spend less time at work, which in fact reduces their sensitivity to these shocks. In this case, having children provides insurance, which is in line with the “old age security” hypothesis (Nugent, 1985) based on the idea that children are a security asset.==== Sommer (2016) finds that having children is considered as a consumption commitment, and her model explains half of the decrease in the number of births between 1970 and 1990 when the US labor market risk was high.==== In addition, she finds that fertility and earnings risks amplify each other as far as the number of births is concerned, even if the infertility risk leads women to have children earlier. Demographers have also written extensively on childbearing postponement. When they aim at analyzing economic uncertainty, their preferred approach is to include unemployment rates as a forcing variable in their empirical studies (Hoem, 2000, Meron and Widmer, 2002, and Pailhé and Solaz, 2012).====The paper is organized as follows. The theory is exposed in Section 2. The main analytical results are provided in Section 3. The quantitative part, including calibration simulation and policy, is in Section 4. Extensions regarding the possibility of having a second child or the distinction between married and single mothers are provided in Section 5. Section 6 concludes.","Childbearing postponement, its option value, and the biological clock",https://www.sciencedirect.com/science/article/pii/S002205312100048X,1 March 2021,2021,Research Article,124.0
"Fu Hu,Haghpanah Nima,Hartline Jason,Kleinberg Robert","Department of Computer Science, University of British Columbia, Canada,Department of Economics, Penn State University, United States of America,EECS Department, Northwestern University, United States of America,Department of Computer Science, Cornell University, United States of America","Received 16 May 2019, Revised 8 February 2021, Accepted 19 February 2021, Available online 26 February 2021, Version of Record 1 March 2021.",https://doi.org/10.1016/j.jet.2021.105230,Cited by (3),"We study whether an auctioneer who has only partial knowledge of the distribution of buyers' valuations can extract the full surplus. There is a finite number of possible distributions, and the auctioneer has access to a finite number of samples (independent draws) from the true distribution. Full surplus extraction is possible if the number of samples is at least the difference between the number of distributions and the dimension of the linear space they span, plus one. This bound is tight. The mechanism that extracts the full surplus uses the samples to construct contingent payments, and not for statistical inference.","Cremer and McLean (1988) design an auction that generically extracts the full surplus for selling an indivisible product to one of many potential buyers. The auction given by Crémer and McLean requires full knowledge of the distribution of buyers' valuations, but in reality the auctioneer rarely has access to such detailed information. This paper studies the possibility of full surplus extraction when the auctioneer has only partial knowledge of the distribution.====In our model, the true distribution of buyers' valuations is unknown to the auctioneer and all buyers. The true distribution belongs to a finite commonly-known set of possible distributions. The auctioneer has access to a finite number of independent draws, or ====, from the true distribution. Before observing the samples, the auctioneer commits to a mechanism that specifies the allocation and payments based on the bids and the realized samples. After the buyers bid, samples are revealed and the allocation and the payments are specified.====Does there exist a mechanism that extracts the full surplus for each possible distribution? The auctioneer cannot simply ask the buyers to report the true distribution and punish them if they disagree, since the buyers do not know the true distribution either. Thus full surplus extraction is impossible without samples, since the auctions that extract the full surplus for different distributions may be different. Even with samples, it may seem impossible to extract the full surplus since, if samples are used for inference, the auctioneer needs infinitely many samples to infer the true distribution with certainty.====We show that it is in fact possible to extract the full surplus. We identify the number of samples that are necessary and sufficient for doing so. The number of samples is equal to the number of distributions in the set, minus the dimension of the linear space spanned by them, plus one.==== This number is at least one and at most the number of distributions minus one. If the distributions are linearly independent, the number of samples is one. In particular, for any two distributions, one sample suffices.====The mechanism that extracts the full surplus uses the samples to construct contingent payments, and not for statistical inference. The mechanism is a second price auction plus side payments. The side payment of each buyer depends on others' bids and the realized samples. The second price auction maximizes the total surplus. The surplus is fully extracted if each buyer's interim expected utility is zero for each value of the buyer and each distribution, that is, the expected side payment equals the expected utility from the second price auction.====The problem is thus to verify whether a solution to a system of linear equalities exists, where the variables are the side payments. A solution exists if for each buyer, the rows of the following matrix are linearly independent: The matrix specifies, for each distribution and value of the buyer, indexing the rows, the conditional probability of each profile of others' values and samples, indexing the columns. In the special case where the true distribution is known (the set of distributions is a singleton), the matrix is an extension of the one constructed by Cremer and McLean (1988), where the columns are not only indexed by profiles of others' values, but also samples. In general, our matrix simply stacks such matrices, one for each distribution in the set, on top of each other.====A main technical step in our proof is to identify how many samples are required to guarantee linear independence. The key observation is that for a given distribution and value of a buyer, the profile of others' values and each sample are independently distributed. Thus, each row of the matrix is an outer product of several vectors, i.e., a vector that specifies the conditional probabilities of others' values, and multiple copies of a vector that specifies sample probabilities. We identify exactly how many times the vector of sample probabilities must be multiplied by itself to guarantee linear independence. The fact that this is achieved with relatively few samples is one of the main insights of our paper. Samples are used to create linear independence.====There are two ways to interpret our results. The first interpretation views full surplus extraction as a critique of the features of commonly studied auction models. In this view, our work reexamines these features in order to identify one that is responsible for the unsettling prediction of full surplus extraction. Instead of identifying an assumption that is responsible, we identify one that may be weakened. This assumption is that the true distribution is commonly known. Other papers have shown that full surplus extraction is not generically possible if the assumptions of risk neutrality and unlimited liability are relaxed (Robert, 1991), if buyers can collude (Laffont and Martimort, 2000), or if more than one auctioneer compete (Peters, 2001).====The second interpretation of our results views full surplus extraction as a proof of concept. In this view, the possibility of full surplus extraction demonstrates the power of contingent payments to design robust mechanisms. With only partial knowledge of the distribution, one can design mechanisms that perform as well as the mechanisms that have full knowledge of the distribution. The existing literature on contingent payments (Hansen, 1985; DeMarzo et al., 2005) suggests that an auctioneer can increase profit by relating payments to observable events that are correlated with buyers' values (e.g., revenue sharing in auctions for oil lease). In contrast, our work highlights the role of contingent payments to obtain robustness.",Full surplus extraction from samples,https://www.sciencedirect.com/science/article/pii/S0022053121000478,26 February 2021,2021,Research Article,125.0
"Gersbach Hans,Mamageishvili Akaki,Tejada Oriol","CER-ETH – Center of Economic, Research at ETH Zurich and CEPR, Zürichbergstrasse 18, 8092 Zurich, Switzerland,CER-ETH – Center of Economic, Research at ETH Zurich, Zürichbergstrasse 18, 8092 Zurich, Switzerland","Received 18 July 2019, Revised 8 October 2020, Accepted 19 February 2021, Available online 24 February 2021, Version of Record 12 May 2021.",https://doi.org/10.1016/j.jet.2021.105228,Cited by (6),"We analyze the effect of handicaps on turnout. A handicap is a difference in the vote tally between alternatives that is added to the vote tally generated by voters. Handicaps are implicit in many existing democratic procedures. Within a costly voting framework with private values we show that turnout incentives diminish considerably across the board if handicaps are large, while low handicaps yield more mixed predictions. The results extend beyond the baseline model—e.g. by including uncertainty and behavioral motivations—and can be applied to the design of Assessment Voting. This is a new voting procedure where ==== some randomly selected citizens vote for one of two alternatives, and the results are published; ==== the remaining citizens vote or abstain; and ==== the final outcome is obtained by applying the majority rule to all votes combined. If the size of the first voting group is appropriate, large electorates choose the majority's preferred alternative with high probability, and average participation costs are moderate or low.","In this paper we investigate the effect of handicaps on turnout incentives for large elections. A ==== is a difference in the vote tally between the available alternatives that ==== strategic citizens take as predetermined when they decide, first, whether to turn out and, second, what alternative to vote for, and that ==== is added to the vote tally generated by the (strategic) voters. Handicaps arise in one-round voluntary voting with the majority rule when some votes are manipulated, some citizens can publicly commit their vote ahead of voting day, or information about the ongoing voting outcome is released. In either case the difference in the vote count between alternatives before voting either starts or continues takes the form of a (possibly stochastic) handicap. Handicaps are also at work in sequential voting, in voting procedures where a qualified majority is required, and in the signature-gathering procedures that regulate popular initiatives.====To assess the properties of handicaps, we consider a society composed of strategic, risk-neutral citizens called upon to choose one of two alternatives, say ==== and ====. Each citizen's preferred alternative is private information and is independently drawn from a common Bernoulli distribution. For each citizen there is also a common cost ==== that materializes if and only if s/he votes. There is also a handicap, denoted by ====, which is an exogenously given integer added to the vote tally created by the citizens who end up casting a vote. If ==== (====), alternative ==== (====) requires at least ==== more votes than alternative ==== (====) to be implemented. Thus we say that ==== (====) is ==== with respect to ==== (====) if ==== (====). The probabilities of preferring ==== to ==== and vice versa, the cost parameter ====, and the handicap ==== are common knowledge. Due to our focus on large electorates we further assume that the number of citizens follows a Poisson distribution (Myerson, 1998, Myerson, 2000). We then analyze the symmetric-type Nash equilibria in undominated strategies of the simultaneous-move game, denoted by ====, in which each citizen decides whether or not to turn out and the alternative with the most votes (handicap included) is implemented.==== Ties are broken by fair randomization.====We find that if the handicap is sufficiently large in absolute value, i.e., if ====, then game ==== has only one equilibrium, in which no citizen casts a vote (referred to hereafter as the ==== equilibrium). In such cases the outcome is determined solely by the (exogenously given) handicap. This is a fairly general result, since it holds for our baseline setup and also if there is aggregate and/or individual uncertainty—and in particular asymmetry of information—, polls are manipulated, the reckoning of pivotal probabilities is biased, or some minority voters conform to voting for the alternative leading in the polls. If, by contrast, ====, then alongside the no-show equilibrium game ==== typically has at the same time equilibria in which the supporters of both alternatives choose positive turnout probabilities and equilibria in which the supporters of only the handicapped alternative do so. These equilibria entail positive turnout levels and yield very different outcomes, which means that mixed predictions cannot be avoided for handicaps that are low in absolute value.====These results are useful in understanding the series of examples of existing democratic procedures set out above. But our analysis can also be applied to the optimal design of a new voting procedure which we call ==== (AV) and specifies the following course of events:====Clearly, the outcome of the first voting round of AV takes the form of a handicap. Accordingly, the results set out earlier are informative on the outcome of the second voting round and hence on the overall outcome of AV. Consider, in particular, the case where the size of AG is chosen to be sufficiently large, its composition is random, and all members of this group vote and they do so for their preferred alternative. Then AV implements the alternative preferred by the majority of the population with a probability arbitrarily close to one. This is because, due to the law of large numbers, the probability that the alternative preferred by the majority of the population collects ==== more votes than the other alternative goes to one with the size of AG. In such cases, the outcome of the first round of AV becomes the outcome of the entire voting procedure. As we show for general handicaps, the turnout level of the second voting round is the result of a cost-benefit analysis made by the citizens participating in this voting round, but it is not necessary for citizens to be deprived of their right to vote. This adds to the appeal of AV from a democratic perspective, as this voting procedure then satisfies the “one-person-one-vote” principle.====For setups where voting is costly and citizens have private values it is well known that the preferences of the majority are often ==== implemented through one-round voluntary voting. The reason for this is the ====. This effect ensues from members of the minority displaying higher turnout levels than members of the majority because they suffer less from the positive externalities associated with voting. By contrast, AV implements the majority opinion almost surely, and it does so at an average participation cost comparable to that of one-round, voluntary voting.==== Note that while one-round compulsory voting also chooses the (==== preferred) majority alternative with arbitrarily high probability, it does so at the highest possible participation cost. It thus transpires that AV is an efficient mixture of voluntary and compulsory one-round voting schemes. In fact, AV implements the socially optimal solution asymptotically as the (expected) size of the electorate goes to infinity. Our results can also be used to rationalize a wider use of direct democracy, provided that it is possible to have a small, different subset of the population vote on each issue at hand before the rest of the population (otherwise it is too costly).====Finally, we extend the analysis of handicaps by expanding the baseline setup in three main directions. First, we allow some citizens to incur no voting costs (say, because they are partisan) or even like voting (say, because of some sense of moral duty). We distinguish two polar cases depending on whether or not partisanship is correlated with preferences. Second, we allow voting costs to differ across supporters of the different alternatives. Third, we consider the case of three or more alternatives. All these extensions extend the boundaries of the costly-voting paradigm. They are also of paramount importance to the appeal of AV as an actual voting procedure, since its main (theoretical) strength is that, if designed properly, it can provide the citizenry with the right incentives to turn out.====The paper is organized as follows: In Section 2 we discuss our contribution to several strands of the literature. In Section 3 the (baseline) model is introduced. In Section 4 we analyze the equilibria of game ====. In Section 5 we apply our results to the design of AV. In Section 6 we analyze some extensions of our baseline model—the proofs are in Appendix B. In Section 7 we reinterpret our results in the framework of existing democratic procedures. Section 8 concludes. The proofs pertaining to the main sections of the paper are in Appendix A.",The effect of handicaps on turnout for large electorates with an application to assessment voting,https://www.sciencedirect.com/science/article/pii/S0022053121000454,24 February 2021,2021,Research Article,126.0
"Herweg Fabian,Müller Daniel","University of Bayreuth and CESifo, Faculty of Law, Business and Economics, Universitätsstr. 30, D-95440 Bayreuth, Germany,University of Würzburg, Faculty of Economics, Sanderring 2, D-97070 Würzburg, Germany","Received 22 June 2020, Revised 28 October 2020, Accepted 18 February 2021, Available online 24 February 2021, Version of Record 1 March 2021.",https://doi.org/10.1016/j.jet.2021.105226,Cited by (12),", ====) and salience theory (====) is a special case of salience theory (====), which itself is a special case of generalized regret theory (====).","The analysis of decision making under risk is a core topic in microeconomics. The neo-classical workhorse model is ==== (EUT), which was first introduced by Bernoulli (1738) and obtained an axiomatic foundation by von Neumann and Morgenstern (1947). Relatively early on, EUT was criticized—most prominently by Allais (1953)—for its failure to predict a significant share of observed individual choices and, thus, also average behavior in a variety of choice situations. The most prominent non-EUT alternative theory for decision making under risk is ==== (Kahneman and Tversky, 1979), which rests on the three building blocks of probability weighting, reference dependence, and loss aversion. While being able to rationalize many choice anomalies under risk, prospect theory—and in particular probability weighting—was criticized to rely on a set of rather complex and ad hoc assumptions that lack a solid psychological foundation.====With ====, Bordalo et al. (2012) propose a theory for choice under risk that bases the probability weighting process on the psychologically motivated mechanism of salience. Specifically, salience theory posits that a decision maker's attention in a pairwise choice situation is unknowingly drawn to those states of the world in which the respective payoff combinations of the two feasible choice options stand out (i.e., are salient). This directional influence on attention is hypothesized to lead to the decision maker placing disproportionally much (little) weight on those states of the world in which the outcomes of the two choice options are perceived as very different (rather similar); i.e., the decision maker over-weighs (under-weighs) the occurrence probability of states with salient (non-salient) outcome combinations. Whether a given state's outcome combination is salient or not is assumed to be determined by the interplay of contrast and level effects, which accounts for the idea that the human perceptional apparatus is attuned to detect differences rather than absolute values and to perceive changes on a log scale (Weber's law).====Salience theory's core assumption that within-state comparisons of outcomes are a key determinant of choice behavior is shared by ==== (Loomes and Sugden, 1982, Loomes and Sugden, 1987)—another non-EUT theory for choice under risk.==== Regarding pairwise choice, regret theory posits that, after uncertainty about the true state of the world has been resolved, the experienced utility derived from receiving the chosen alternative's outcome in that state depends not only on this outcome alone, but also on the outcome that the other choice option, which was not chosen, would have yielded in the realized state of the world. If the decision maker had done better (worse) by choosing differently, she suffers from regret (rejoices). The anticipation of these ex post feelings of regret and rejoicing, which arise from within-state comparison of outcomes across choice options, is hypothesized to be factored into ex ante decision making. Specifically, it is presumed that the decision maker has a desire ex ante to avoid ex post feelings of regret. Notably, in contrast to salience theory, regret theory assumes that the decision maker perceives the occurrence probabilities of the different states of the world correctly and without any distortion.====While the respective psychological motivation underlying regret and salience theory is different, both theories have a significant overlap when it comes to their explanatory potential (Loomes and Sugden, 1982, Loomes and Sugden, 1983; Bordalo et al., 2012). Both regret theory and salience theory can explain the common consequence effect and the common ratio effect, which go back to Allais (1953).==== Furthermore, both theories can rationalize the reflection effect identified by Kahneman and Tversky (1979), the preference reversal phenomenon documented by Grether and Plott (1979), as well as the inherent instability of risk attitudes as reflected in the simultaneous preference for gambling and insurance or in the so-called ==== in Tversky and Kahneman (1992). Given this overlap in rationalizable choice patterns, we believe it to be desirable to aim for a thorough analytical comparison of these two theories.====The rest of the paper is structured as follows: After reviewing the related literature in Section 2, we explain the choice situation, regret theory, and salience theory in Section 3. Section 4 and Section 5 contain our main results: While “generalized” regret theory (Loomes and Sugden, 1987) contains salience theory as a special case, salience theory itself contains “original” regret theory (Loomes and Sugden, 1982) as a special case. Section 6 concludes. In Appendix A, we relate our more general salience model to smooth and rank-based salience theory as introduced by Bordalo et al. (2012) and we briefly compare the existing approaches that extend regret theory and salience theory beyond pairwise choice.",A comparison of regret theory and salience theory for decisions under risk,https://www.sciencedirect.com/science/article/pii/S0022053121000430,24 February 2021,2021,Research Article,127.0
"Bils Peter,Duggan John,Judd Gleason","Dept. of Political Science and Dept. of Economics, Vanderbilt University, United States of America,Dept. of Political Science and Dept. of Economics, University of Rochester, United States of America,Dept. of Politics, Princeton University, United States of America","Received 15 April 2019, Revised 14 January 2021, Accepted 16 February 2021, Available online 22 February 2021, Version of Record 4 March 2021.",https://doi.org/10.1016/j.jet.2021.105223,Cited by (0),We study a model of repeated elections that features privately informed politicians and ideologically extreme lobby groups. We establish existence of a class of perfect ,"Democratic theorists have long celebrated elections as a mechanism to discipline officeholders, while also highlighting the dangers posed by organized groups with particular interests, or “factions.”==== Elections enable voters to remove incumbents who enact unpopular policy, but officeholders face considerable pressure from policy-motivated interest groups. Lobbying is a prominent way that groups influence policy, and a correspondingly large literature in political economy studies its impact on policy outcomes (Grossman and Helpman, 1994; Kang, 2015).==== Although lobbying can distort policy if the preferences of powerful interest groups diverge from those of the public at large, the threat of losing re-election encourages officeholders to choose policies that are sufficiently popular with voters, and the electoral consequences of policy choices have the potential to feed back into lobby group efforts. In this article, we propose a dynamic model to study how lobbying by ideological interest groups interacts with electoral accountability to shape policy in democratic systems.====Existing theoretical work typically considers these forces separately. On one hand, elections have been shown to moderate policy through several channels, including electoral competition (Black, 1948; Downs, 1957) and dynamic policy responsiveness by incumbents (Banks and Duggan, 2008; Duggan and Forand, 2021). On the other hand, a large literature suggests that lobbying distorts political motives by shifting policy choices away from the median voter (Grossman and Helpman, 1996; Fox and Rothenberg, 2011). Nevertheless, lobby groups evidently have reason to consider the electoral prospects of their policy initiatives: if a group advocates a policy choice that risks an electoral loss, then it must compensate the officeholder for those electoral costs; moreover, if the incumbent loses re-election, then the new officeholder may be less receptive to the group's lobbying. In particular, the new officeholder may be lobbied by groups from the opposite ideological extreme, which increases the policy stakes and highlights the endogeneity of lobbying incentives due to repeated interaction over time. Overall, analyzing the effect of interest groups on policymaking in democracies involves highly complex, dynamic incentives and, consequently, difficult analytical challenges.====We provide a formal analysis of elections over time that allows us to trace causal mechanisms that are crucial for understanding the role and effects of money in electoral politics, and we find that the interaction of accountability and lobbying has a stark effect on ideological cohesion within parties. If money is highly effective or office incentives are large, then all politicians from the same side of the ideological spectrum choose the same policy, a phenomenon we refer to as “strong parties.” Furthermore, we highlight conditions under which a dynamic version of the median voter theorem holds, so that the electoral mechanism nullifies the adverse effects of lobbying: if office incentives are high, then equilibrium policies converge to the median as the effectiveness of money becomes small. Conversely, we also provide conditions such that the centrifugal effect of lobbying offsets the centripetal effect of elections and produces arbitrarily extremist policy outcomes: when the effectiveness of money becomes large, the most polarized equilibria become arbitrarily extremist. Another implication of our results is that lobbyist expenditures are non-monotonic in the effectiveness of money, and they are highest when money is moderately effective.====The operation of dynamic incentives in the model is subtle, and our analysis reveals counterintuitive possibilities when the effectiveness of money is not large. As the effectiveness of money increases, it becomes less costly for a lobby group to pull policy outcomes in the direction of its preference, and the direct effect on policy outcomes is that they become more extreme. This applies when a politician is initially lobbied to a policy in the win set and is pulled to a more extreme policy in the win set, and when a politician is initially lobbied to a losing policy and is pulled to a more extreme losing policy. However, there is also an indirect effect through the endogeneity of lobbying incentives: when an incumbent is replaced by a challenger, there is a chance that the opposing lobby group pulls the new officeholder to worse policies, making both groups more willing to compromise. The indirect effect can dominate: we give a numerical example in which the effectiveness of money grows, and greater equilibrium compromise ==== the welfare of the median voter.====As well, it is intuitive that within an equilibrium, lobby groups will pull politicians to policies that are more extreme than they would otherwise choose. Insofar as policy payoffs are concerned, this insight is correct. However, lobby groups have an extra incentive to compromise that officeholders lack: if an incumbent is replaced by a challenger, then in addition to future policy payoffs, lobby groups must anticipate future payments to politicians, which are of course endogenous. We give an example in which the moderating influence of continuation payments offsets the extremal influence of policy incentives, and some politician types who would choose losing policies are lobbied to winning ones. For such a politician type, the default choice in the absence of a lobby offer is to shirk, by choosing her ideal policy and foregoing re-election, but the active lobby group pushes the politician to compromise and pays to compensate the politician, in order to avoid the risk of future payments to politicians.====In our model, policymaking occurs repeatedly over an infinite time horizon. In each period, a lobby group makes an offer to an incumbent politician; the incumbent either accepts this offer and chooses the proposed policy, or she rejects the offer and chooses policy independently; and there is an election between the incumbent and a challenger. Voters observe the policy choice of the incumbent, but not the ideology of either politician. We analyze stationary equilibria, in which the incumbent always chooses the same policy if re-elected, so that voters face the choice between a known incumbent and a relatively unknown challenger. The incumbent politician may face a trade off between the short term gains from choosing her ideal policy and the long term gains of compromising her choice, choosing a more moderate policy in order to gain re-election. A lobby group, anticipating voter choices and politician incentives, can make an offer to the incumbent politician that consists of a desired policy and a transfer to the politician. We model this transfer as a monetary payment from the lobby group to the politician, but it may more generally represent resources that are desirable to the politician, such as (unmodeled) campaign contributions, drafting of model legislation, or promises of future revolving-door opportunities.====These strategic incentives determine a centrally located “win set,” which consists of policy choices that are sufficient for re-election: if the incumbent chooses a policy in the win set, then she is re-elected, and otherwise, she is replaced by the challenger. Given the win set, in the absence of an offer from a lobby group, the incumbent optimally chooses a winning policy if she is moderate, while more extreme politicians may choose their ideal points, foregoing re-election. This policy choice in lieu of a lobby offer is a “default” policy that serves as a reversion point in the lobbying stage. Anticipating the default policy choice, one of two lobby groups makes an offer to the incumbent that consists of a proposed policy and a monetary payment to the politician. We assume that liberal politicians are lobbied by the left interest group, while conservative politicians are lobbied by the right group; and for simplicity, we assume that the contract, if accepted by the politician, is binding. The proposed policy maximizes the payoff of the lobby group, subject to the reservation payoff of the officeholder, and the monetary payment compensates the politician for moving policy from her default choice.====We establish existence of a simple lobbying equilibrium, in which politicians and voters use stationary strategies and such that choices are optimal at every point in the game, and we provide results on strongly partisan equilibria, on policy extremism as the effectiveness of money becomes large, and convergence to the median as the effectiveness of money approaches zero. The existence proof relies on a fixed point approach, but it is novel in that we do not impose, ex ante, any structure on equilibria: rather than anticipating the form of equilibria in advance (and build that into the existence proof), the argument takes place in a space of “continuation distributions,” which are consistent with a large class of strategy profiles. This strengthens our characterization results, and it suggests a technique that may be of general use in the analysis of complex, dynamic models. The dynamic median voter theorem connects our game-theoretic analysis to the social choice theory literature, and it illustrates the attraction of the median, even in the presence of lobby groups with incentives to pull policy outcomes to the extremes of the policy space. Finally, our results on the effectiveness of money establish that lobbying can precipitate extreme policies as the role of money in elections grows large, but that in some cases, lobbying can have a positive influence on voter welfare. This suggests that caution should be taken in setting restrictions on political contributions, and it points to the importance of the further study of the linkages between money and policy.====The analysis of lobbying in this paper contributes to the literature on electoral accountability and builds on the repeated elections framework of Duggan (2000),==== in which lobby groups are not modeled and the incumbent politician chooses policy independently in each period. As in our paper, voters observe policy choices but do not observe the preferences of politicians, so that elections are subject to pure adverse selection. Early studies of electoral accountability include Barro (1973), who studies an electoral model in which there is one type of politician and voters observe policy choices, and Ferejohn (1986), who analyzes a pure moral hazard setting, where policy choices are not perfectly observable. In the pure adverse selection context, closer to our work, Bernhardt et al. (2004) study the effect of term limits; Bernhardt et al. (2009) add partisanship to the model by assuming that challengers can be drawn from different pools, depending on their partisan affiliation; and Bernhardt et al. (2011) add valence to the model, so that a politician's type is composed of two components, valence (which is observed) and her ideal point (which is not). The pure adverse selection model is extended to the multidimensional setting by Banks and Duggan (2008), who establish a dynamic median voter theorem in one dimension. Our median voter result reinforces that of the latter paper by showing that convergence to the median policy obtains even when lobby groups have incentives to pull policy away from the median.====Much of the previous literature on lobbying and elections studies models in which donations from interest groups increase a politician's probability of winning the election (Austen-Smith, 1987; Baron, 1989). There is also a prominent literature studying lobbying as an instrument to buy votes (Groseclose and Snyder, 1996; Banks, 2000; Dal Bó, 2007; Dekel et al., 2009), as well as a large formal literature in which lobbying provides information to politicians (Potters and Van Winden, 1990, Potters and Van Winden, 1992; Austen-Smith and Wright, 1992, Austen-Smith and Wright, 1994).==== We focus on lobbying as a means for interest groups to directly influence policy content via quid pro quo transfers.==== While quid pro quo exchanges are against the law, there is substantial evidence that politicians are able to maneuver around these restrictions in practice (De Figueiredo and Garrett, 2004).==== Austen-Smith and Wright (1994) examine how two lobbies may attempt to influence the same politicians to try and offset one other. We abstract from this possibility by assuming that only one lobby group is active at a given time, and that it only attempts to influence politicians on the same side of the median. In our context, however, this does not appear to be an onerous assumption, as it captures the empirical regularity that interest groups tend to lobby ideological allies.====Another branch of the literature uses the common agency approach to study lobbying. Grossman and Helpman (1996) analyze a static model of campaign finance, in which interest groups contribute to political parties to gain influence or serve electoral motives, and Grossman and Helpman (1994) use the common agency framework to explore how special interests affect trade policy. Martimort and Semenov (2007) consider how officeholder decisions are affected, in an election-free setting, by contributions from competing lobbying firms that have opposing ideological preferences. However, these papers do not consider the dynamic incentives inherent in the repeated elections framework.====Closest to the analysis of this paper is Snyder and Ting (2008), who also study a model of repeated elections with lobbying, but the papers differ in many important ways. First, Snyder and Ting (2008) assume that politicians are purely office motivated, so that they do not face a trade off between policy and re-election. An implication of their assumption is that if two policies fail to gain re-election, then a politician is indifferent between them. This leads to uninteresting stationary subgame perfect equilibria in which the incumbent politician always chooses the ideal point of the interest group and voters always remove the incumbent in favor of a challenger. Such equilibria cannot occur in our model, because politicians care about policy. Second, and more subtly, Snyder and Ting (2008) assume that politicians differ only in their innate benefit from holding office, and that this type is revealed to voters after a politician's first term of office. Thus, once a first-term incumbent is re-elected, the politician no longer has an incentive to signal her type. Third, and perhaps most importantly, Snyder and Ting (2008) assume that lobby groups are short-lived, and that in each period there is a single lobby group with ideal policy drawn independently over time. In contrast, we analyze the influence of two competing lobby groups that persist over time and that care about the future consequences of policy decisions, not just pertaining to the incumbent's re-election chances, but anticipating the ideology of future challengers and the effect of lobbying on future policies.====In Section 2, we describe the model of repeated elections with lobbying. The simple lobbying equilibrium concept is defined in Section 3, and in Section 4, we establish equilibrium existence and provide a characterization in terms of cutpoints in the space of policies. Section 5 presents results on electoral incentives and strong partisanship. In Section 6, we examine the consequences for policy outcomes when the effectiveness of money becomes large. Section 7 concludes, and proofs are contained in the appendix.",Lobbying and policy extremism in repeated elections,https://www.sciencedirect.com/science/article/pii/S0022053121000405,22 February 2021,2021,Research Article,128.0
"Landry Peter,Webb Ryan","Rotman School of Management, University of Toronto, Canada,Department of Management, University of Toronto, Mississauga, Canada,Department of Economics, University of Toronto, Canada","Received 18 July 2019, Revised 12 January 2021, Accepted 8 February 2021, Available online 19 February 2021, Version of Record 15 March 2021.",https://doi.org/10.1016/j.jet.2021.105221,Cited by (15),"We present a theory of multi-attribute choice founded in the neuroscience of perception. Valuation is formed through a series of pairwise, attribute-level comparisons implemented by ==== — a form of relative value coding observed across sensory modalities and in species ranging from honeybees to humans. Such “pairwise normalization” captures a broad range of behavioral regularities in multi-attribute choice, including the compromise and asymmetric dominance effects, the diversification bias in allocation decisions, and majority-rule preference cycles.","Standard choice theories presume that an individual's valuation of an alternative does not depend on the set of alternatives under consideration. However, a large empirical literature has revealed several violations of such “context-independence.” For example, simply adding an alternative to a choice set can alter preferences among existing alternatives (see Rieskamp et al., 2006, for a review). Empirical demonstrations of context effects can be found in both laboratory experiments (beginning with Huber et al., 1982, and Simonson, 1989) and in field data (e.g. Doyle et al., 1999; Geyskens et al., 2010), and extend to many types of decisions — including consumer choice, choices among lotteries, doctors' prescription decisions, perceptual decisions, and mate selection, to name just a few.====Though less familiar to behavioral researchers, context-independence is also challenged by an established neuroscience literature (beginning with Hartline and Wagner, 1952) demonstrating that the brain encodes information in relative, not absolute terms. For example, the neural activity encoding the value of an alternative decreases (indicating a reduced valuation) as the value of another alternative rises (Louie et al., 2011; Holper et al., 2017). This pattern of neural activity is consistent with ====, a well-documented neural computation that, in its simplest conceivable form, merely re-expresses some input value ==== — which may represent the utility of an alternative, or the intensity of sensory stimuli (such as the brightness of a pixel) — relative to another input ==== as ==== (see Rangel and Clithero, 2012; Carandini and Heeger, 2012, and Louie et al., 2015, for reviews). Indeed, the prevailing neuroscience literature conceptualizes such “division by neurons” as an arithmetic operation that is actually performed in the brain.====Why would our brains not just encode ==== independently of ====? The reason is thought to stem from biological constraints. The brain has a limited number of neurons, each with a bounded response range. Thus, information must be compressed within these bounds. A relative value encoding is then needed to ensure this compression is well-calibrated to the choice environment (a point previously noted in the economics literature by Robson, 2001, and Rayo and Becker, 2007; also see Netzer, 2009; Woodford, 2012; Robson and Whitehead, 2018, and Frydman and Jin, 2020). A relative encoding using the divisive normalization computation has been shown to optimally mitigate choice mistakes subject to these biological constraints (Steverson et al., 2019; Webb et al., 2020a). Put simply, divisive normalization efficiently facilitates the perception of both large and small differences on a common scale — e.g. helping to distinguish “one dollar from two dollars and one million dollars from two million dollars” (Carandini and Heeger, 2012).====In this paper, we explore whether divisive normalization — an inherently context-dependent computation — might relate to context-dependent behavior. To do so, we adapt the “====” divisive normalization model to the multi-attribute choice setting where behavioral research on context-dependence is mainly focused. Let ==== be an alternative with ==== denoting its ==== attribute values. These may be observable values or utility-transformed values, as discussed in Section 3.1. The decision-maker's valuation of ==== according to our basic ==== model is normalized relative to other alternatives in the choice set ==== as:==== This formulation is “pairwise” in the sense that each term reflects an attribute-level comparison (normalization) of ==== to some other alternative ====. Pairwise comparisons have long been a feature of multi-attribute choice theories (e.g. Tversky and Simonson, 1993) and have substantial empirical support from eye-tracking research in multi-attribute choice environments; in one such study, Noguchi and Stewart (2014) find that “alternatives are repeatedly compared in pairs on single dimensions.”====Our modeling approach demonstrates how neuroscience may prove useful to economists as a source of candidate functional form representations to consider in model selection (as suggested by Bernheim, 2009). Arguably the simplest, standard multi-attribute choice model is an additive model, ====. This additive model provides a common foundation for many leading multi-attribute choice theories that address context-dependence (e.g. Tversky and Simonson, 1993; Kivetz et al., 2004a; Koszegi and Szeidl, 2013; Bordalo et al., 2013; and Bushong et al., 2019). These theories typically replace each term in the summation with a function of ==== that also depends on the set of alternatives. Similarly, our theory modifies the additive model by applying pairwise normalization to each attribute value (effectively replacing ==== with ====). This formalizes pairwise normalization in its most elemental form, isolated from other factors that may influence choice, and with minimal parametric freedom. Despite its simplicity, the model's predictions capture a broad range of context-dependent behavioral regularities that are only partially captured by prevailing multi-attribute choice theories. These predictions are summarized in Table 1, where ‘====’ indicates that the model robustly captures the associated behavior (i.e. never predicts the opposite or no effect under conditions for which it would be expected), ‘S’ means the model sometimes predicts the behavior and sometimes predicts the opposite effect, and ‘N’ means the model does not predict the behavior.====The rest of this paper proceeds as follows. Section 2 provides background on the neurobiological basis of our model and the behavioral regularities that it captures. Section 3 presents the theoretical model. Section 4 examines how a preference between two alternatives can be affected by a third alternative, and relates these effects to the notion advanced by Tversky and Russo (1969) and Natenzon (2019) that similar alternatives are “easy to compare.” Section 5 considers choices among alternatives defined on three dimensions. Section 6 considers various allocation problems. Section 7 explores a generalization of the model. Section 8 clarifies the behavioral role of pairwise comparisons in our model and elaborates on the varying representations of attribute-level comparisons in the relevant theoretical literature.",Pairwise normalization: A neuroeconomic theory of multi-attribute choice,https://www.sciencedirect.com/science/article/pii/S0022053121000387,19 February 2021,2021,Research Article,129.0
"Krasikov Ilia,Lamba Rohit","Higher School of Economics Moscow, Russian Federation,Pennsylvania State University, United States of America","Received 16 February 2019, Revised 7 January 2021, Accepted 10 January 2021, Available online 18 February 2021, Version of Record 2 March 2021.",https://doi.org/10.1016/j.jet.2021.105196,Cited by (5),"Financial constraints preclude many surplus producing economic transactions, and inhibit the growth of many others. This paper models financial constraints as the interaction of two forces: the agent has persistent ==== and is strapped for cash. The wedge between the optimal and efficient allocation, termed distortion, increases over time with each successive “bad shock” and decreases with each “good shock”. At any point in the contract, an endogenous number of “good shocks” are required for the principal to provide some liquidity and then eventually for the contract to become efficient. Efficiency is reached almost surely. The average rate at which contract becomes efficient is decreasing in persistence of shocks; in particular, the iid model predicts a quick dissolution of financial constraints. This speaks to the relevance of modeling persistence in dynamic models of agency. The problem is solved recursively, and building on the literature, a technical tool of finding the minimal subset of the recursive domain that houses the optimal contract is further developed.","Long term economic transactions are often marred by financial constraints. A sizeable body of empirical work documents the wide prevalence of financial constraints, their micro impact on firms size and growth, and macro impact on the misallocation of capital in an economy.==== With an aim to provide theoretical constructs to these empirical regularities, Kiyotaki (2012), in an elegant note, advocates “a mechanism design approach to illustrate how different environments of private information and limited commitment generate different financial frictions.” In the spirit of the said agenda, this paper posits financial constraints as a product of the interaction between (i) persistent private information, and (ii) limitations on the ability of agents to generate timed cash flows.====We study a dynamic screening problem with Markovian shocks where the principal offers history dependent allocations and transfers to the agent. If the agent has a cash reserve or pledgeable assets, the principal will ask the agent to post a bond or deposit collateral. Broadly, optimal distortions are then ====, going from maximal to zero, and optimal payments are ====, maximally delayed to the extent possible. However, in many real situations, e.g. in supply contracts, managerial compensation, provision of public goods and regulation, the agent may not have the requisite cash to post a bond or collateralize existing assets. This has implication for both the optimal structure of distortions, and the sequential breakup of payments.====Taking inspiration from the literature on financial contracting, we model the aforementioned situation by restricting the stage (or per-period) utility to be positive. The idea being that the agent requires, in the least, the amount of cash that covers the consumption/production decisions in every period. The economic force generated by the interaction of this stronger feasibility restriction and private information is termed as the ====. This is because if there is no private information, the efficient allocation is implementable, and in the presence of a bond or collateral, efficiency is achievable much more easily through maximal backloading of payoffs. So, it is the interaction of the two forces together that produces financial constraints. Further, we show that ==== in private information, an empirically relevant feature of the model, makes this interaction even richer in terms of constraining the optimal allocation.====The big picture question is: when do these financial constraints bind and when they do, what dynamic distortions do they generate? In asking and then trying to answer this question, we provide a deeper understanding of the role of financial constraints in dynamic mechanism design, and a deeper understanding of the role of persistence in agency frictions in dynamic financial contracting.====The rest of this introduction is divided into three parts. First is the structure of the optimal contract and a plausible mechanism that implements it. Second is unpacking the economic content of the novel elements – (i) cash versus no cash constraints, (ii) interpretation of positivity of stage utility, and (iii) the role of persistence in agency frictions in generating financial constraints. Third, is an overview of the literature. We expound upon each after briefly describing the model.====The formal model is as follows. A big firm (principal) repeatedly producing a final good contracts with a smaller firm (agent) that supplies an important input. Each period, the small firm privately observes either a low (“good shock”) or high (“bad shock”) marginal cost. After being drawn from a prior, costs evolve according to an exogenous two state Markov process. Preferences are quasi-linear. The small firm requires a constant cash flow to cover its costs of production, hence the stage utility must be positive: we say that the agent is thus ====. The big firm is tasked with designing a contract which sets supply of inputs by the small firm, and payments for its production. Both parties can commit to a dynamic contract.====. A Pareto-optimal contract chooses allocations and transfers that satisfy incentive compatibility and cash-strapped constraints to maximize the profit of the big firm while ensuring a minimum ex ante payoff for the small firm. Fig. 1a depicts a typical sequence of technology shocks. For a history of cost realizations ==== and current cost ====, let ==== and ==== be the allocation and expected utility of the small firm. At this point, if the marginal cost of incentive provision is zero, then ====, that is the (statically) efficient quantity is supplied. If it is positive, then ==== where ==== measures the history dependent optimal distortion. As is standard, the low cost type always supplies the efficient quantity: ====.==== On the other hand, each “bad shock” increases optimal distortions: ====.==== In addition, the realization of a “good shock” decreases the optimal distortion: ====. An endogenous number of consecutive “good shocks”, say ====, is required for the optimal distortion to reach zero. For every additional “bad shock”, as distortions increase, this number increases: ====. Once the optimal distortion reaches zero it stays at zero, that is, efficiency is an absorbing state. In the long run, the efficient contract is supplied almost surely.====With reference to Fig. 1a, the expected utilities of both the low and high cost types go up after a “good shock” and go down after a “bad shock”. That is, as long as the contract is inefficient: ====. Two thresholds on the vector of expected utilities divide the evolution of the optimal contract into three regions - illiquidity, liquidity and efficiency; see Fig. 1b. The contract typically starts in the illiquid region – both incentive and cash-strapped constraints bind. A low cost type either keeps the contract in illiquidity or can transition it to liquidity. A high cost type decreases the expected utility of the small firm which keeps it illiquid. After an endogenous number of low cost realizations, the expected utility of the small firm reaches a critical threshold at which the cash-strapped constraint becomes slack. This is called the liquid region. Liquidity is not an absorbing state, a high cost realization can push the small firm back into illiquidity. The liquid region forms a penultimate zone towards efficiency. Once liquid, the realization of one more low cost pushes expected utility of the small firm beyond the second threshold into the absorbing state of efficiency.====At a technical level, we use the recursive approach to characterize the optimal contract. More specifically, we construct a “shell”, the ==== of the recursive domain which houses the optimal constrained contract. The recursive domain is too large to make crisp predictions about the exact structure of dynamic distortions. We show that as long as the optimal contract is inefficient, the expected utility of the agent must always lie in this shell. It allows us to show all the aforementioned monotonicity properties of the evolution of the optimal contract. We also provide a simple price-theoretic explanation of the construction of the shell.====A combination of ==== and eventual ==== implements the optimal contract. In the illiquid region, the cash-strapped constraint binds and the big firm only provides working capital to the small firm. Through a sequence of consecutive low cost realizations, the small firm has to earn its way into liquidity. In the liquid region, the big firm promises to take over the small firm on the realization of one more low cost type for a determinable strike price. Thereafter, the small firm operates in-house, producing the efficient quantity.==== Allowing for a long-term contract helps mitigate the problem of agency frictions by backloading payoffs. Financial constraints, though, restrict the extent of backloading. Dynamic distortions in our framework are an additive sum of two effects: backloading of payoffs and illiquidity due to financial constraints; the latter increases with each “bad shock”, overturning the standard result of decreasing distortions in dynamic mechanism design. Efficiency is still a certainty, though the path towards it is much more constrained in comparison to the model sans financial constraints.====We also reconsider the interpretation of the positivity of stage utility as a limited liability constraint for small businesses. It is clear that the cash strapped constraint is welfare reducing from the perspective of total welfare (or surplus), but is it “beneficial” for the agent? Consider the principal profit maximizing contract on the Pareto frontier in which the big firm has all the bargaining power. The ex ante expected utility of the small firm from the contract is determined endogenously as part of the optimum. We show that in the iid limit the ex ante expected utility of the agent is higher in our model than in the benchmark, and in the perfect persistently limit the ranking can reverse for certain parameters. This points to a cautious interpretation of the positivity of the stage utility as a ==== constraint, which is the standard in the literature.====Finally, we take this model as representative of firm dynamics in an economy with financial constraints and numerically show how persistence in agency frictions makes a marked difference to the substantive predictions of the model. We make three broad points. The fraction of financially constrained firms in the short-run is monotonically increasing in the persistence of technology shocks. The average rate at which firms converge to the state of being unconstrained is decreasing in persistence; in particular, the iid model predicts a quick dissolution of financial constraints. And, variance in the total value of both constrained and unconstrained firms is larger with persistence. The standard dynamic financial contracting literature that operates in the iid world would miss all these, empirically important, comparative statics.==== This paper sits at the intersection of at least two strands of theoretical models: dynamic mechanism design with serially correlated information (see surveys by Vohra (2012), Krähmer and Strausz (2015a), Pavan (2016), and Bergemann and Välimäki (2019)) and dynamic financial contracting with iid technologies (see surveys by Biais et al. (2013), and Sannikov (2013). Three ingredients interact to determine the structure of dynamic inefficiencies: correlation in agency frictions, strength of feasibility restrictions, and permissibility of termination. The overarching role of each combination of ingredients is to create frictions in dynamic contracting that lead to realistic qualitative predictions.====Table 1, Table 2 enlist the most ====; Table 1 features screening and Table 2 features cash flow diversion as the underlying agency friction. Within each table, papers are classified along inclusion/exclusion of the three aforementioned modeling ingredients. In terms of long-term predictions, once the recursive problem is appropriately set up, it can be shown that in each of the papers the optimal contract converges to the efficient allocation in the absence of the termination clause, and it converges either to efficiency or termination in the presence of the termination clause.==== The key economic force that leads to this result is backloading of information rents to the extent possible.====At a high level, ours is the first paper to precisely characterize the short-run predictions in terms of the monotonic nature of dynamic distortions: “good shocks” monotonically push the allocation towards efficiency and “bad shocks” take it away from it. As noted in Table 1, it is the first paper to analyze a dynamic screening model (as opposed to a cash flow diversion or moral hazard problem) with both persistence in private information and financial constraints, nudging the literature on dynamic mechanism design to explicitly incorporate financial constraints. Further it (i) identifies the minimal subset of the recursive domain that houses the optimal contract; (ii) clarifies the connection between limited liability and being strapped for cash; (iii) provides an explicit characterization of the optimal contract in the perfectly persistent limit which shows “good shocks” have a stronger effect on distortions than “bad shocks”, this fact underlies the long-term efficiency result; (iv) solves for the optimal contract in continuous time, which seeks to unify the literatures on cash flow diversion and screening, since the models converge to the same limit in continuous time; and (v) explores the implications of persistence in agency frictions on firm dynamics.====The two most closely related papers are Battaglini (2005) and Krishna et al. (2013). Battaglini (2005) studies a similar screening model, but where the agent has cash to post a bond. More specifically, only the total expected utility over time is required to be positive in every period. The structure of short-run distortions are thus quite different – the contract becomes efficient forever as soon as the agent assumes a “good shock”, and it has decreasing distortions along the history of constant “bad shocks”. In a departure from that paper, and more generally the literature on dynamic mechanism design, our paper explores the implications of cash constraints for the agent with persistent private information.====Krishna et al. (2013) study the same model as ours, repeated screening with the cash-strapped constraint, but where the agent's types follow an iid process. Since theirs is a special case of our model, all our results also hold in their setup. However, the focus of the paper is on long-term efficiency. We build upon their work in at least three ways. First, the monotonicity of allocation rule, even for the iid model is novel to our paper. Second, the Markov model is technically much harder to solve, as has already been noted in simpler dynamic mechanism design models without financial constraints.==== Third, persistence adds greater empirical relevance to the analysis, as is evident from the applications of standard dynamic mechanism design models to public finance (see Stantcheva (2020)).====Clementi and Hopenhayn (2006) and Fu and Krishna (2019) both study the problem of cash flow diversion by the agent in a repeated setting, the former looks at an iid technology and the latter at a Markovian one. A simple way to map their framework into ours would be to change the time structure: At the start of every period the agent commits to a production plan after which his cost type is realized. The type is reported, agreed upon input quantity is supplied, and the agent is compensated for by the principal. The interpretation here is that agent does not know whether his cost would be low or high when he makes the production decision. Despite being a low cost type, he can misreport to be a high cost type, supply some portion of the produced quantity and sell the rest in the black market – a diversion of the economic surplus. While these models produce similar long-term predictions, the short-run structure of the optimal contract here is quite different than the screening literature.====Our paper is also related to the recent work by Guo and Hörner (2018): They consider a dynamic principal-agent model with persistent private information where preferences are perfectly aligned, transfers are not allowed and the principal wants to maximize efficiency. The optimal contract converges almost surely either to permanent allocation (efficiency) or permanent non-allocation (immiseration), driven by the fact that both front-loading and backloading of payoffs can occur at the optimum. In our framework, preferences are misaligned, the expected utility is continuously backloaded, and the optimal contract always converges to efficiency. A technical aspect we share with Guo and Hörner (2018) is the characterization of a subset of the recursive domain that houses the optimal contract, which allows us to make precise statements about the short and long run properties.====Financial constraints have also been explored in the sequential screening literature pioneered by Courty and Li (2000). For example, Krähmer and Strausz (2015b) consider a sequential screening model with ex post participation constraints. They show that with these additional constraints the optimal contract is static and does not illicit the agent's information sequentially. One way to map their framework into ours would be to consider the two period version of our model, and require the first period allocation to be (exogenously) zero. Then, the cash-strapped constraints require that no payments can be charged in the first period. As a consequence the optimal contract replicates the “static optimum”. In contrast our model highlights that multi-period interactions can extract private information in an incentive compatible fashion, even with stronger feasibility restrictions.====Finally, the cash-strapped constraint breaks the linearity of transfers across time. The spirit of this exercise is shared by other related works: Amador et al. (2006), and Halac and Yared (2014) study models of delegation. Thomas and Worrall (1990), Garrett and Pavan (2015), Luz (2015), and Arve and Martimort (2016) consider dynamic models of private information where the agent is risk averse. Krasikov et al. (2019) analyze a dynamic screening model with individual rationality, but where the principal is more patient than the agent. In all these papers, there is a cost to moving transfers or payments across time.",A theory of dynamic contracting with financial constraints,https://www.sciencedirect.com/science/article/pii/S0022053121000132,18 February 2021,2021,Research Article,130.0
"Glazer Jacob,Rubinstein Ariel","The Coller Faculty of Management, Tel Aviv University, Israel,Department of Economics, Warwick University, United Kingdom of Great Britain and Northern Ireland,School of Economics, Tel Aviv University, Israel,Department of Economics, New York University, United States of America","Received 22 October 2020, Accepted 31 January 2021, Available online 15 February 2021, Version of Record 18 February 2021.",https://doi.org/10.1016/j.jet.2021.105211,Cited by (1),"We study methods for constructing a story from partial evidence where a story is defined as a path along a finite directed graph from the origin to a terminal node. Each node in the graph represents a possible event. A ==== receives evidence, i.e. a subset of events consistent with at least one story, and expands it into a coherent story. The analysis focuses on a stickiness property whereby if the story builder believes in a particular story, given a certain set of facts, then he believes in it given a broader set of facts consistent with the story.","An individual often receives partial evidence about some chain of events and develops it into a complete and coherent “story”. In constructing the story, the individual is aware of the constraints on how the story can develop. We are interested in methods of constructing a full story from partial evidence. Given that people often fail to apply Bayesian reasoning even in very simple situations, the approach taken is non-Bayesian.====In our setting, a story is a sequence of events, i.e., a path along a finite directed graph (without cycles), which starts from the origin and continues along the graph until it reaches a terminal node. Each node in the graph represents a possible event. A ==== receives some evidence in the form of a subset of events consistent with at least one story and expands it into a full and coherent story that he believes to be true. Thus, the story builder holds a point belief rather than a probabilistic belief about the real story.====The formalization of the concept of a “story builder” makes it possible to define and analyze a variety of procedures. Prominent among them is the order-based story builder who has in mind an ordering over the set of possible stories and chooses the story that maximizes that ordering, given the evidence. The ordering might embody a belief about the likelihood of the stories, in which case he chooses the most likely one. Alternatively, the ordering might reflect wishful thinking, and in that case the story builder selects the best story that does not conflict with the evidence.====Whereas the order-based story builder approaches the situation holistically, other types of story builders construct the story in steps, sequentially adding events according to some rule. For example, the story builder might have in mind a probability measure over the set of possible stories and advances from the origin by selecting the most likely event at each stage, given the evidence and the path he has chosen so far.====Much of the analysis centers around a principle we call Story Stickiness which states that if, after receiving a particular evidence set, the story builder believes in some story, then he continues to do so if he receives the same evidence set together with an additional piece of evidence consistent with the story.====If the graph is a tree, then Story Stickiness is satisfied only by an order-based story builder. If not, then there are story builders that satisfy the property but are not order-based. In most of the paper we assume that the story builder is naive and does not take into account the source of the evidence. However, we also comment on a story builder who believes that the evidence is presented by a party with a vested interest.====The most closely related model is that of Sadler (2021), in which an agent encounters information in the form of propositions that are either true or false. A proposition is identified by the subset of states in which it is true while a belief is a set of propositions held by the individual. An individual is modeled as an updating function that determines a belief as a function of a previous belief and a new proposition. This formalization provides a language for specifying a number of updating rules that are not necessarily consistent with Bayesian reasoning.====In Bjorke (2019), the set of “states” is a product set. The individual receives information about some components of the vector and possesses a “focal state” function (analogous to our story builder) that completes any subset of characteristics so as to become a full vector. An individual chooses a “focal state” for every subset of values. Bjorke (2019) focuses on two functions: “most likely” and “most distinctive”. For each of them, he investigates the following problem: An informed party can inform the agent regarding some of the true state's components and convinces the individual to believe in a different state. Given a true state, what are the states that the informed party can make the individual to believe in? Eliaz and Spiegler (2020) focus on a decision maker who interprets objective data about the realizations of variables in terms of a causal model (a Bayesian network). The decision maker in their framework receives statistical data and builds a model (a narrative) that organizes the data.",Story builders,https://www.sciencedirect.com/science/article/pii/S0022053121000284,15 February 2021,2021,Research Article,131.0
"Nguyen Anh,Tan Teck Yong","Tepper School of Business, Carnegie Mellon University, United States of America,College of Business, University of Nebraska-Lincoln, United States of America","Received 29 April 2020, Revised 4 January 2021, Accepted 7 February 2021, Available online 12 February 2021, Version of Record 18 February 2021.",https://doi.org/10.1016/j.jet.2021.105212,Cited by (20),"We study a model of ==== persuasion in which the Sender commits to a signal structure, ==== observes the signal realization, and then sends a message to the Receiver at a cost that depends on both the signal realized and the message sent. Our setup weakens the Sender's commitment to truthfully reveal information in ==== persuasion. We provide sufficient conditions for full communication by the Sender in the Sender-preferred equilibrium, and these conditions are satisfied under many commonly studied communication games. Under these conditions, the Sender's (lack of) commitment in the persuasion problem is quantified as a communication cost to induce a belief distribution for the Receiver. We apply this approach to study test design and information provision by lobbyists.","Many economic situations involve an agent wanting to influence the action of a decision maker. When monetary transfers are not possible, the agent can instead strategically control the decision maker's information to influence her beliefs and thus affect the actions that she takes. Kamenica and Gentzkow (2011) (hereafter KG) model this as a “Bayesian persuasion” problem in which the agent (Sender, he) designs an information structure that generates information about the underlying state to the decision maker (Receiver, she).====In this paper, we study a variant of a Bayesian persuasion model with the innovation that new information is transmitted to the Receiver by the Sender through potentially costly messages. The Sender first commits to a signal structure that generates a signal about an unknown state. Upon ==== observing the realized signal, the Sender sends the Receiver a message at a cost that depends on both the message and the signal realization. The Receiver then updates her belief and takes an action that affects the utility of both players.====Our model differs from a canonical Bayesian persuasion model in two substantial ways. First, when information misrepresentation by the Sender is possible but costly, our setup weakens the Sender's commitment to truthfully revealing new information to the Receiver, which is a key assumption in the Bayesian persuasion literature. This setup is of particular relevance to persuasion activities that require expert interpretation or the preparation of new information. For example, a drug company can commit to a type of scientific research on its drug, but the results require expert interpretation, which is susceptible to misrepresentation.====Second, the “belief-based” approach reduces a typical Bayesian persuasion problem to choosing a distribution of posteriors for the Receiver subject to only the posteriors averaging back to the prior (“Bayes plausibility”). Therefore, the signals used to encode the respective posteriors are fully interchangeable in Bayesian persuasion. In contrast, because the Sender's communication cost depends on the identity of the realized signal, our model also captures the Sender's choice of signal to use to encode each posterior belief in the signal structure.====To illustrate the implication of the second feature, consider a situation in which a drug company wants to persuade the FDA to approve a new drug by first announcing a type of news to search for regarding the drug. Let ==== (====) be the probability that the announced news is found when the drug is good (bad). Say that it is a “positive news test” if ====, and it is a “negative news test” if ====. News has a different effect in the two types of tests. In a positive news test, the arrival of news improves the belief about the drug, whereas the lack of news worsens the belief; in contrast, in a negative news test, the arrival of news worsens the belief, whereas the lack of news improves it.====If the drug company's problem is modeled as a Bayesian persuasion problem, then the drug company is assumed to be committed to truthfully revealing the test outcome to the FDA, or (equivalently) the FDA can directly observe the outcome. In this case, the choice between a positive or a negative news test is irrelevant because the belief-based approach reduces the problem to choosing the distribution of two posteriors, and any distribution of two posteriors can be induced by a set of ==== that has the property of ==== (i.e., positive news test) or ==== (i.e., negative news test).====Suppose now that the FDA relies on the drug company to report its test outcome (i.e., the setup in this paper), and it is infinitely costly for the drug company to fabricate news, whether positive or negative, but concealing news is always costless. In this case, the type of test becomes important. In a negative news test, the bad posterior is attached to a readily manipulated signal (because any news can be costlessly hidden). Since the drug company always wants to generate the good posterior, its “no news” message in a negative news test is never credible to the FDA. However, this credibility problem is absent from a positive news test because the bad posterior is attached to a signal that the drug company is committed to truthfully report (because it cannot fabricate news when none is found). Therefore, the solution to the persuasion problem now involves not only pinning down the distribution of posterior beliefs (as in Bayesian persuasion) but also the property that new information for persuasion must be generated by a positive news test.====More generally, in this paper, we define a ==== as the triple of (1) the signal space for generating new information; (2) the message space for communicating the new information to the Receiver; and (3) the message costs associated with each signal realization. We consider how the persuasion activity is also affected by the message technology as opposed to only the players' preferences over actions, which has been the focus of the Bayesian persuasion literature. In the example above, the signals and messages available in the message technology are “news” and “no news.” A positive (negative) news test is represented by a signal structure that attaches a favorable belief to the “news” (“no news”) signal, and the messaging cost after each signal realization renders the use of a negative test infeasible. Our main model considers more general forms of message technologies that allow for abstract signal and message spaces and cover a wide range of communication games.====In our model, the Sender faces two interacting problems: what information to generate and what information to reveal to the Receiver. Because the information design/acquisition problem is followed by strategic communication, any subgame-perfect equilibrium involves characterizing two interconnected belief distributions: the Sender's belief distribution generated by his chosen signal structure and the Receiver's belief distribution in the equilibrium of the communication subgame. With communication taking place through costly messages, the two players' beliefs can differ in equilibrium, thus complicating the problem.====Our first contribution is providing a set of sufficient conditions (Condition 1) on the message technology for the Sender-preferred equilibrium to be supportable by full communication between the Sender and the Receiver (Proposition 1). These conditions are satisfied under many natural communication games and do not put any restrictions on the preferences of the Sender or the Receiver. Armed with Proposition 1, our second contribution is showing that the persuasion problem reduces to finding ==== the (Sender-)optimal belief distribution held by the Receiver ex post while obeying a constraint for full communication in the equilibrium of the communication subgame (Proposition 2). This constraint manifests as a cost for the Sender to induce each belief distribution for the Receiver and is derived from a cost-minimization problem of choosing signals and messages to encode beliefs such that it is incentive-compatible for the Sender to truthfully reveal information. Therefore, the solution characterization consists of a two-step process that is reminiscent of the solution to a standard discrete-action moral hazard problem====: first, determine the cost for each belief distribution; second, optimize over the belief distributions while accounting for its cost.====The costs to sustain various belief distributions can be interpreted as the Sender's commitment power (or lack of it) under the message technology. If the cost of a belief distribution is infinite, then it implies that the message technology does not allow the Sender to credibly transmit such information to the Receiver. In Proposition 3, we provide a broad class of message technologies with the feature that the commitment power afforded to the Sender is simply the set of the feasible signal structures — i.e., the cost of a belief distribution is always either zero or infinite. Therefore, under such message technologies, the problem is equivalent to doing “KG persuasion” within a constrained set of signal structures. We illustrate its applications using some examples.====The remainder of the paper proceeds as follows. We discuss the related literature in the next subsection. Then, in Section 2, we introduce our model. In Section 3, we study issues related to full communication. Subsequently, in Section 4, we show how the problem reduces to finding the optimal belief distribution with a cost attached to each distribution and provide some properties of this cost function. In Section 5, we consider two examples, and in Section 6, we provide an extension with an information misrepresentation cost. Finally, in Section 7, we conclude. All omitted proofs are found in Appendices A and B.",Bayesian persuasion with costly messages,https://www.sciencedirect.com/science/article/pii/S0022053121000296,12 February 2021,2021,Research Article,132.0
"Li Yingkai,Pei Harry","Department of Computer Science, Northwestern University, United States of America,Department of Economics, Northwestern University, United States of America","Received 3 August 2020, Revised 7 February 2021, Accepted 10 February 2021, Available online 12 February 2021, Version of Record 19 February 2021.",https://doi.org/10.1016/j.jet.2021.105222,Cited by (1),"We examine a patient player's behavior when he can build reputations in front of a sequence of myopic opponents. With positive probability, the patient player is a commitment type who plays his Stackelberg action in every period. We characterize the patient player's action frequencies in equilibrium. Our results clarify the extent to which reputations can refine the patient player's behavior and provide new insights to entry deterrence, business transactions, and capital taxation. Our proof makes a methodological contribution by establishing a new concentration ====.","Economists have long recognized that individuals, firms, and governments can benefit from good reputations. As shown in the seminal work of Fudenberg and Levine (1989), a patient player can guarantee himself a high payoff when his opponents believe that he might be committed to play a particular action. Their result can be viewed as a refinement, which selects the patient player's optimal equilibria in many games of interest.====This paper studies the effects of reputations on the patient player's behavior instead of his payoffs, which have been underexplored in the reputation literature. Existing works on reputation-building behaviors restrict attention to particular equilibria or games with particular payoff functions. By contrast, we identify tight bounds on the patient player's action frequencies that apply to all equilibria under more general payoff functions. Our results clarify the extent to which reputations can refine the patient player's behavior and provide new insights to applications such as entry deterrence, business transactions, and capital taxation.====We analyze a repeated game between a patient player and a sequence of myopic opponents. The patient player is either a strategic type who maximizes his discounted average payoff, or a commitment type who plays his optimal pure commitment action (or ====) in every period. The myopic players cannot observe the patient player's type, but can observe all the actions taken in the past.====We examine the extent to which the option to imitate the commitment type can motivate the patient player to play his Stackelberg action. Theorem 1 characterizes tight bounds on the discounted frequencies with which the strategic-type patient player plays his Stackelberg action in equilibrium. We show that the maximal frequency equals one and the minimal frequency equals the value of the following linear program: Choose a distribution over action profiles in order to minimize the probability of the Stackelberg action subject to two constraints. First, each action profile in the support of this distribution satisfies the myopic player's incentive constraint. Second, the patient player's expected payoff from this distribution is no less than his Stackelberg payoff. The first constraint is necessary since the myopic players best reply to the patient player's action in every period. The second constraint is necessary since the patient player can approximately attain his Stackelberg payoff by imitating the commitment type. In order to provide him an incentive not to play his Stackelberg action, his continuation value after separating from the commitment type must be at least his Stackelberg payoff.====The substantial part is to show that these constraints are not only necessary but also sufficient. Our proof is constructive and makes a methodological contribution by establishing a novel concentration inequality on the discounted sum of random variables that bounds the patient player's action frequencies (Lemma A.1).====Theorem 2 identifies a sufficient condition under which a distribution of the patient player's actions is his action frequency in some equilibria of the reputation game. In a number of leading applications such as the product choice game and the entry deterrence game, our sufficient condition is also necessary, in which case Theorem 2 fully characterizes of the set of action frequencies that can arise in equilibrium.====Our results provide new insights to classic applications of reputation models. For example, in the product choice game of Mailath and Samuelson (2006, Figure 15.1.1 on page 460),==== our results imply that a policy maker can increase the frequency of high effort by subsidizing consumers for purchasing low-end products or by taxing consumers for purchasing high-end products. Intuitively, these policies increase the consumers' demand for high effort when they purchase the high-end product, which in turn increases the frequency of high effort in the worst equilibrium. In the entry deterrence game of Kreps and Wilson (1982) and Milgrom and Roberts (1982a), our results imply that a small amount of subsidy to potential entrants for entering the market makes a reputation-building incumbent more aggressive in fighting entry, but a large amount of subsidy eliminates the incumbent's fighting incentives.====Our results contribute to the reputation literature by clarifying the role of reputations in refining the patient player's behavior. This is complementary to the result of Fudenberg and Levine (1989) that studies how reputations refine the patient player's payoff. Existing works on players' reputation-building behaviors restrict attention to particular equilibria or particular payoff functions. For example, Kreps and Wilson (1982) and Milgrom and Roberts (1982a) characterize sequential equilibria in entry deterrence games. Schmidt (1993) characterizes Markov equilibria in repeated bargaining games. Bar-Isaac (2003), Phelan (2006), Ekmekci (2011), Liu (2011), and Liu and Skrzypacz (2014) restrict attention to supermodular games or ==== games. By contrast, we characterize tight bounds on the patient player's action frequencies that apply to all equilibria. Our results are more general in terms of payoffs, which only require the patient player's optimal commitment payoff to be greater than his minmax value and that his optimal commitment outcome is not a stage-game Nash equilibrium.====Cripps et al. (2004) show that when the monitoring structure has full support, the myopic players eventually learn the patient player's type and the strategies converge to an equilibrium of the repeated complete information game. However, their results do not characterize the speed of convergence or players' behaviors in finite time, and hence do not imply what players' discounted action frequencies are. Ekmekci and Maestri (2019) study players' reputation-building behaviors in stopping games where a patient uninformed player chooses between continuing and irreversibly stopping the game in every period. By contrast, the uninformed players in our model are myopic and their action choices are reversible. Pei (2020a) provides sufficient conditions under which the patient player has a unique on-path behavior. Unlike our model that restricts attention to private value environments but allows for general stage-game payoffs, his result requires nontrivial interdependent values and monotone-supermodular stage-game payoffs.====Section 2 sets up the baseline model. Section 3 states our main results. Section 4 applies our results to several applied models of reputation formation and discusses the results' practical implications. Section 5 discusses our modeling assumptions as well as issues related to taking our predictions to the data. Section 6 concludes. The proofs of our results can be found in the appendix.",Equilibrium behaviors in repeated games,https://www.sciencedirect.com/science/article/pii/S0022053121000399,12 February 2021,2021,Research Article,133.0
"Araujo Luis,Minetti Raoul,Murro Pierluigi","Michigan State University, United States of America,Sao Paulo School of Economics-FGV, Brazil,Luiss University, Italy","Received 9 March 2020, Revised 29 January 2021, Accepted 31 January 2021, Available online 4 February 2021, Version of Record 9 February 2021.",https://doi.org/10.1016/j.jet.2021.105210,Cited by (1), that inject liquidity into the lending sector enhance the stabilizing effects of credit relationships but have ambiguous welfare consequences.,"The relationship between the financial sector and the corporate sector has undergone profound transformations in recent decades. Various phenomena have been indicated as sources of a dilution of credit relationships and of a trend towards “arm's-length” financial systems.==== On the supply side, credit relationships have been challenged by mounting competition among financial institutions and from capital markets and by increasing geographical distance between banks and firms (Boot and Thakor, 2000; Krozner, 2015; IMF, 2006). Further, the growing ability of financial institutions to securitize and resell loans has allegedly diluted their incentive to engage in credit relationships with firms (Wang and Xia, 2014; Berndt and Gupta, 2009). On the demand side, firms' increasing tendency to accumulate large cash holdings has reduced their dependence on financial institutions for day-to-day liquidity provision, possibly attenuating the intensity of credit relationships (Bates et al., 2009; Pinkowitz and Williamson, 2001).====While these phenomena have been extensively documented, we still have limited theoretical understanding of how credit relationships can affect the role of the credit sector in aggregate economic activity. Does the relationship structure of the credit sector influence the equilibrium allocation and its welfare properties? Is a relationship-oriented credit sector an attenuator or amplifier of changes in economic conditions? How do credit relationships shape the effects of conventional and unconventional monetary policies? Clearly, studying the aggregate effects of credit relationships can not only advance our understanding of the role of the credit sector in the economy at large but also yield insights for financial reforms. In the aftermath of the Great Recession, scholars and policy-makers debate whether the observed trend should be promoted or whether efforts should be made to preserve more relationship-oriented financial systems (IMF, 2006, 2012; ECB, 2014).====This paper takes a step towards examining these issues. To investigate the above phenomena, we build an economy with credit relationships and liquidity constraints. The model economy borrows elements from a broad literature in which trading frictions endogenously motivate a need for liquidity (see Lagos et al., 2017, for a review). In the economy, firms obtain liquidity borrowing from lenders or holding internal liquidity (cash). Liquidity is essential for firms because the capital assets of their distressed projects need to be restructured in a decentralized market with search frictions. We model credit relationships following Diamond and Rajan (2000, 2001a, 2001b) and an extensive financial intermediation literature thereafter: relationship lenders can acquire expertise on firms' projects, reducing the costs of operational and financial restructuring in case of distress. Crucially, the incentive of relationship lenders to acquire expertise on firms' projects depends on their involvement in liquidity provision and, hence, on their stake in the restructuring of projects. The larger the share of a project financed by loans rather than by firms' cash holdings, the stronger the incentive of a relationship lender to acquire expertise on the project. Thus, when demanding cash firms face a trade-off. On the one hand, larger precautionary cash holdings reduce their dependence on costly loans. On the other hand, firms' cash holdings dilute lenders' involvement in projects and, hence, lenders' restructuring effort.====After characterizing firms' and lenders' decisions, we study the aggregate effects of credit relationships. We first show that in equilibrium relationship loans entail a cost premium over firms' cash holdings: this premium reflects firms' appetite for relationship loans, that is, firms' desire to incentivize relationship lenders' restructuring effort by better involving them in project financing. This relationship finance premium can be suboptimally high in steady state. However, we find that in a region of parameters the relationship finance premium can be suboptimally low relative to lenders' effort and firms can overborrow in credit relationships and overinvest in their projects. In the latter scenario, the monetary authority finds it optimal to compensate for the underpricing of relationship loans and boost their cost by setting a positive policy rate, that is, departing from the Friedman rule.====We then examine how credit relationships influence the impact of economic conditions (probability of project distress) through the interaction between firms' cash holdings and lenders' restructuring effort. The influence of credit relationships depends on the response of lenders' effort to economic conditions. Worse economic conditions raise the probability that lenders' restructuring expertise becomes useful, incentivizing their effort. Worse economic conditions, however, lead to a lower participation of lenders in project financing, disincentivizing their effort. When economic conditions are not too poor and financial restructuring costs are not too high, the first effect dominates and lenders respond to worse economic conditions stepping up their restructuring effort. In this scenario, credit relationships attenuate the impact of economic conditions. When instead economic conditions are poor and financial restructuring costs are high, lenders respond to deteriorated economic conditions by reducing their effort. Yet, as long as the effort reduction is moderate, credit relationships may still attenuate the impact of economic conditions, as the net benefit of lenders' restructuring effort rises. These effects arise also when the policy rate is adjusted optimally.====Taken together, our results thus imply that, while credit relationships can lead to overinvestment, they can enhance the resilience of investment to economic conditions. To get a sense of these trade-offs, after characterizing the mechanisms analytically, we develop a numerical example calibrated to data on Italian firm-bank credit relationships.==== In the last part of the paper, we study whether unconventional monetary policy (liquidity injections into lenders or into firms) influences the aggregate effects of credit relationships. We find that a credit policy that injects liquidity into lenders can enhance the stabilizing role of credit relationships by altering the mix relationship loans-cash holdings and promoting bankers' effort. However, in some scenarios liquidity injections into lenders harm the welfare effect of credit relationships, which would be better served by liquidity injections into firms.====  This paper is related to search models of money with credit.==== In this literature, Rocheteau et al., 2018a, Rocheteau et al., 2018b are probably the most related papers. Rocheteau et al. (2018b) consider a setting in which banks can extract surplus from firms' investments. Through precautionary money holdings, firms reduce their reliance on bank funding and banks' surplus extraction. Rocheteau et al. (2018a) also explore the endogenous formation of lending relationships. Relative to these papers, we focus on the intensive margin (endogenous intensity) of credit relationships and investigate the interaction between banks' role as liquidity providers and their endogenous effort in acquiring expertise on projects in a monetary economy. Banks' effort in acquiring expertise on projects is a key feature of relationship finance (Berger and Udell, 2002, 2006; Boot and Thakor, 2000). In particular, following the approach of Diamond and Rajan (2000, 2001a, 2001b), we specify relationship banks' expertise as restructuring skills. By analyzing the interaction between banks' liquidity provision and their restructuring effort, we uncover a complementary mechanism through which firms' money holdings can affect output and welfare. In our economy where banks' effort is non-contractible, while firms' money holdings reduce firms' reliance on costly bank credit, they also dilute banks' involvement in projects and hence their incentives to acquire restructuring expertise. By contrast, banks' liquidity provision motivates their acquisition of expertise, that is, banks' effort and intermediated liquidity (credit) are complements. In this setting, factors that modify firms' money holdings influence banks' effort in credit relationships, affecting investment, output and welfare.====Financial intermediaries are also at the center of the literature that stresses financial, rather than trading, frictions. Several studies investigate the financial disintermediation process in difficult times (Gertler and Kiyotaki, 2010; Diamond and Rajan, 2008; Lorenzoni, 2008). Yet, other studies highlight that in difficult times lenders may protect firms on which they have acquired expertise (Beck et al., 2018; Hoshi et al., 1990; Hachem, 2011). This paper can help reconcile these views showing that the composition of liquidity between loans and firms' cash holdings is crucial to determine whether credit relationships enhance the resilience to economic conditions.====Finally, it is worth pointing out that, in specifying credit arrangements between banks and firms, we follow a large literature in banking on the extensive use of credit line facilities (see, e.g., Berger and Udell, 2006; Campello et al., 2011, and references therein). As this literature stresses, by establishing a partial commitment of banks to make liquidity available at a given interest rate, credit lines partially protect firms from the risk that in difficult times banks extract surplus by threatening to withhold liquidity. In addition to capturing the diffusion of credit line facilities, our specification helps better isolate the endogenous interaction between banks' restructuring effort and the composition loans-firms' cash holdings from the cash holdings motive driven by banks' hold-up power (see, e.g., Rocheteau et al., 2018b, and the above-mentioned studies).====Before proceeding, it is useful to note that, while the model applies naturally to banks, it could also be applied to specialized financial intermediaries (e.g., finance companies or investment funds) which also engage in relationship finance. The remainder of the paper unfolds as follows. In Sections 2–3, we describe and solve the model. Section 4 studies the aggregate effects of credit relationships. We first characterize their impact on welfare and on the optimal policy rate. We then study their impact on the resilience to economic conditions. Section 5 examines how unconventional monetary policies influence these effects. Section 6 studies robustness and extensions, with an emphasis on the specification of the restructuring process. Section 7 concludes. Technical proofs and details on the numerical example are relegated to the online Appendix.","Relationship finance, informed liquidity, and monetary policy",https://www.sciencedirect.com/science/article/pii/S0022053121000272,4 February 2021,2021,Research Article,134.0
Mauersberger Felix,"University of Bonn, Germany","Received 11 February 2019, Revised 12 January 2021, Accepted 19 January 2021, Available online 28 January 2021, Version of Record 28 September 2021.",https://doi.org/10.1016/j.jet.2021.105203,Cited by (6),This paper introduces a learning-to-forecast laboratory experiment based on a New-Keynesian macroeconomy that is particularly close to the model's ,"The topic of inflation control has seen a surge of interest in the light of the Covid-19 crisis, which has raised concerns about both inflationary and deflationary shocks (e.g., Guerrieri et al., 2020; Jaravel and O'Connell, 2020). One of the most influential policy recommendations for inflation control has been the “====”. This particular principle implies that interest rates should actively respond to inflation (see, e.g., Taylor, 1993; Woodford, 2003). Traditionally, theorists have propagated the Taylor principle, because it has been argued to help uniquely pin down inflation under rational expectations.====This paper contributes to a relatively recent literature which has challenged the Taylor principle. It provides an empirical test of the Taylor principle based on a laboratory experiment that is particularly close to the microfoundations of a New-Keynesian model. The advantage of using an experimental approach is that no specific ==== assumptions need to be used to model agents' expectations.==== My laboratory experiment questions the use of the Taylor principle as the predominant paradigm. Instead, my results highlight that, in order to stabilize the economy, interest rates should respond particularly aggressively to undesired inflation.====Doubts about the Taylor principle were raised even when the widely used paradigm of rational expectations was retained. For example, Cochrane (2011) shows that even if the Taylor principle is fulfilled, the New-Keynesian economy has multiple rational expectations solutions. These “non-fundamental” solutions are explosive and cannot be ruled out by any transversality condition. McCallum (2009) deviates from rational expectations and assumes that agents have to learn the parameters of their environment. Given this assumption, he showed that only the fundamental equilibrium corresponding to the minimum-state variable solution is stable and the “non-fundamental” solutions proposed by Cochrane are unstable. Cochrane (2009, 2011) argues, however, that McCallum's result relies on the observability of the monetary policy shocks. Evans and McGough (2018) show that, regardless of the observability assumption, the fundamental equilibrium is robustly stable under adaptive learning. Gabaix (2020), introducing partial myopia into a New-Keynesian model, shows that there is a unique bounded equilibrium, even if the Taylor principle is not satisfied.====Assenza et al. (2021) recently published a laboratory experiment investigating the Taylor principle and different strengths in the monetary authority's response to inflation. These authors also cast doubt on the Taylor principle, but they conclude that “a reaction coefficient [on inflation of] ==== [...] is sufficient to ensure convergence to the target.” Importantly, in their experiment, Assenza et al. (2021) ask subjects to forecast ==== outcomes, such as inflation and the output gap. Macroeconomic outcomes are then generated via computer, using subjects' forecasts in the three-equation standard textbook New-Keynesian model, containing a “dynamic IS” equation, a “Phillips curve” and an interest rate rule.====However, Preston (2005) and Woodford (2013) highlight the following important point (which is taken into account in my experimental design): The causal structure of the microfounded New-Keynesian framework does not imply that agents merely forecast inflation and output gap one period ahead. Based on the model's microfoundation, subjects forecast their ==== optimal consumption and prices instead of aggregate outcomes. Under rational expectations (RE), the distinction between forecasting aggregate or individual-specific outcomes does not matter, and the framework used in this study is equivalent to the textbook three-equation model. Yet, under non-rational expectations, agents can experience disperse individual outcomes. Subsequently, agents' expectations are determined by those individual experiences. In other words, it may make a difference whether: a) agents make forecasts about the expenditure of their individual household and the price of their particular firm (and these forecasts are subsequently aggregated); or whether b) agents make forecasts about an aggregate variable from the beginning.====From an experimentalist's perspective, there is a trade-off between an experimental economy's simplicity and accuracy in reflecting the causal structure of the underlying framework. The perhaps simplest experiment based on a New-Keynesian model is the learning-to-forecast study by Pfajfar and Žakelj (2014). They asked subjects to forecast only inflation, and everything else, including output gap forecasts, was calculated electronically using the three-equation New-Keynesian model. Assenza et al. (2021) and Hommes et al. (2019b) also base their design on the three-equation New-Keynesian model, but they asked their subjects to forecast both inflation ==== the output gap. Other experiments asked subjects to both predict and optimize (see, e.g., Bao et al., 2013; Bernasconi and Kirchkamp, 2000; Petersen, 2015). This paper retains a ==== setup, where subjects' only decisions are forecasts of economic variables.==== Sticking to a learning-to-forecast experiment has the advantage that all deviations from rationality can be attributed to individuals' forecasting decisions. Thus, the analyst does not have to disentangle whether deviations from rationality are due to the failure to forecast rationally or to the failure to optimize.====While my results are similar to Assenza et al. (2021) in certain aspects, they display important differences. The two similarities to Assenza et al. (2021) that should be highlighted are: first, I find that disobeying the Taylor principle results in divergence and instability; second, I find that obeying the Taylor principle is necessary, but not sufficient for macroeconomic stability. Yet, how my findings differ from Assenza et al. (2021) is striking: While these authors conclude that “a reaction coefficient [on inflation of] ==== [...] is sufficient to ensure convergence to the target”, my experimental results do not support this conclusion. I show that adopting a coefficient on inflation of 1.5 in the Taylor rule entails a high likelihood that dynamics do not converge to the fundamental steady state. Specifically, convergence to the rational expectations steady state in the experimental economies constructed for this study is achieved with a coefficient on inflation of 3. Thus, my experimental results are in line with papers in the adaptive learning literature such as Orphanides and Williams (2007) and Ferrero (2007), which call for a more aggressive response to inflation.====The behavioral explanation of the dynamics observed in the experiments somewhat differs from previous learning-to-forecast setups due to the different nature of the data-generating process. In a framework where subjects are tasked with forecasting aggregate market outcomes, such as the textbook three-equation New-Keynesian model, subjects try to coordinate their expectations to the rational expectations equilibrium (REE). In the framework adopted in my experiments, each agent needed to forecast a different outcome. Moreover, agents' individual forecasts tended to have a considerable influence on their individual outcomes, while aggregate forecasts tend to have only a small influence. To some extent, this may allow subjects to adopt an individual strategy without giving much consideration to other subjects' behavior. From a researcher's perspective, subjects appear to make random, idiosyncratic movements.==== This finding is consistent with a large literature that has already documented random elements in agents' decisions (see, e.g., Agranov and Ortoleva, 2017; Gabaix, 2019; Woodford, 2020).====Fortunately, the monetary authority can mitigate randomness in people's behavior and achieve coordination to the REE if it adopts a particularly aggressive response to inflation. An active (passive) response to inflation introduces negative (positive) feedback into the underlying system. Positive (negative) feedback between an expectation for a variable ==== and an outcome variable ==== prevails when a higher forecast for ==== yields a higher (lower) realized outcome of ====, ceteris paribus. Under positive feedback, any non-rational deviation from the REE induces outcomes to move in the same direction as forecasts. Thus, under positive feedback, the irrational behavior is at least ==== self-confirming. Conversely, under negative feedback, the outcome moves in the opposite direction of any irrational deviation of forecasts from equilibrium. That way, negative feedback causes a sizable forecast error, giving agents incentives to adjust their forecasts back into the direction of the equilibrium. The more aggressively the central bank responds to inflation, the stronger the negative feedback in the system. Strong negative feedback incentivizes agents to coordinate their behavior. Such coordination then reduces the idiosyncrasies, and therefore the noise, in agents' forecasts.====I propose a behavioral model based on endogenous noise that can predict the different dynamics observed in the experiments ex-ante. Khaw et al. (2017) show that the higher the Bayesian posterior variance, the more noise there is in their subjects' behavior. I use a novel modeling approach to capture this stylized fact.==== The link between randomness and uncertainty, as captured by the Bayesian posterior, has been formalized in the operations research literature through the Thompson Sampling model (Thompson, 1933). In this model, agents try to learn the underlying state of the economy by using Bayes' rule. However, instead of optimally using that posterior, agents make a random draw from the subjective posterior distribution. While the Thompson Sampling model has normative implications on how to solve the trade-off between exploration and exploitation in the bandit problem, it is also consistent with theories of the brain. It can be interpreted as a model of cognitive limitations (see Mauersberger, 2021, for more details).",Monetary policy rules in a non-rational world: A macroeconomic experiment,https://www.sciencedirect.com/science/article/pii/S002205312100020X,28 January 2021,2021,Research Article,135.0
"Mangin Sephorah,Julien Benoît","Research School of Economics, Australian National University, Australia,School of Economics, UNSW Business School, UNSW Sydney, Australia","Received 4 February 2020, Revised 14 January 2021, Accepted 24 January 2021, Available online 27 January 2021, Version of Record 1 February 2021.",https://doi.org/10.1016/j.jet.2021.105208,Cited by (7),When is entry efficient in markets with search and matching frictions? This paper generalizes the well-known Hosios condition to dynamic environments where the expected match output depends on the market tightness. Entry is efficient when buyers' surplus share is equal to the ==== plus the ==== (i.e. the elasticity of the expected match surplus with respect to buyers). This ensures agents are paid for their contribution to both ==== and ,"Consider a search-and-matching model in which buyers and sellers are matched according to a frictional matching process and there is free entry on one side, e.g. buyer entry. There are two standard externalities related to entry: the congestion and thick market externalities. The former is a negative externality that arises because greater buyer entry reduces the matching probability of each buyer. The latter is a positive externality that arises because greater buyer entry increases the matching probability of each seller. Hosios (1990) asked the question: When is entry ====?====The answer provided by Hosios (1990) is remarkably simple: entry is efficient only when buyers' share of the joint match surplus equals the elasticity of the matching function with respect to buyers. This result is now widely known as the “Hosios condition.”==== When this condition holds, markets internalize the search externalities that arise through the frictional matching process. When it fails, markets do not internalize these externalities, leading to either inefficiently high or inefficiently low entry.====The Hosios condition has proven to be widely applicable across a broad range of search-and-matching models. However, it does not guarantee efficiency in settings where the ==== – i.e. the expected output conditional on matching – depends on the market tightness or buyer-seller ratio.==== In such environments, entry affects not only the number of matches but also the expected match output. An additional externality arises – which we call the ==== – that may be positive or negative. The output externality is not internalized by the Hosios condition: entry may be inefficiently high or low when this condition holds.====This paper provides a generalization of the Hosios (1990) condition to a wide class of dynamic search-and-matching models where the expected match output may depend on the market tightness. Our simple, intuitive generalization provides a ==== for understanding the efficiency of entry across a broad range of models which may appear quite different on the surface.====To see why our generalization is necessary, consider an environment with buyer entry. Entry is efficient only when buyers are paid their marginal contribution to the social surplus. If the expected match output is exogenous, buyers need only be paid for their effect on ====, i.e. on the number of matches, and the standard Hosios condition applies. If the expected match output is endogenous, however, buyers must also be compensated for their effect on ====, i.e. on the expected value of the joint match surplus. A generalization of the Hosios condition is thus required.====Our main result is that entry is constrained efficient only when buyers' surplus share equals the ==== plus the ==== (i.e. the elasticity of the expected match surplus with respect to buyers). We call this simple condition the “generalized Hosios condition”. When this condition holds, both the standard search externalities and the output externality are internalized. Whether or not this condition holds in a particular market depends on how prices are determined.====The importance of the generalized Hosios condition is particularly clear in search-theoretic models of the labor market.====First, the Hosios condition is used to determine the efficient level of vacancy entry and thus unemployment. In environments where labor productivity depends on the market tightness, however, the standard Hosios condition does not guarantee efficiency. If this condition is mistakenly used to determine the efficient level of vacancy entry, unemployment may be inefficiently high or low. The generalized Hosios condition is required to ensure that firms are compensated for the effect of job creation on both unemployment and labor productivity.====Second, the Hosios condition is often used to calibrate models in which wages are determined by Nash bargaining. In environments where the standard Hosios condition suffices for efficiency, it is possible to ==== this condition (and thus efficiency) by using a Cobb-Douglas matching technology and setting firms' bargaining parameter equal to the constant matching elasticity, e.g. Shimer (2005). Importantly, the generalized Hosios condition does not allow this calibration trick. In environments where this condition is necessary for efficiency, we cannot simply restore efficiency by a particular choice of matching technology and bargaining parameter. This is because the surplus elasticity – unlike the matching elasticity – is always endogenous.====In this sense, the inefficiencies that obtain when the generalized Hosios condition fails are harder to eliminate. However, we find that it is possible to decentralize the efficient allocation through directed or competitive search.==== Section 2 presents a simple, motivating example. Section 3 derives our main result, the generalized Hosios condition. Section 4 presents some examples. Section 5 discusses how to apply the condition. Section 6 concludes. The Appendix contains omitted proofs. The Online Appendix includes a generalization of our results to more general matching and output technologies, as well as additional examples.",Efficiency in search and matching models: A generalized Hosios condition,https://www.sciencedirect.com/science/article/pii/S0022053121000259,27 January 2021,2021,Research Article,136.0
"Cole Richard,Tao Yixin","Courant Institute, New York University, 251 Mercer St, New York, NY 10012, United States of America","Received 10 December 2019, Revised 3 September 2020, Accepted 14 September 2020, Available online 26 January 2021, Version of Record 1 February 2021.",https://doi.org/10.1016/j.jet.2021.105207,Cited by (2),We also investigate the communication complexity for finding a Pareto Efficient and envy-free allocation.,"How can one partition a collection of indivisible goods so as to achieve both efficiency and fairness, when the recipients may have non-additive utilities, and further the possible allocations may be restricted? By efficiency we mean Pareto Efficiency and by fairness we intend envy-freeness. We first note that there is no deterministic allocation achieving both Pareto Efficiency and envy-freeness, as can be seen by considering the case of allocating one item among two players, one of whom must end up being envious.====Instead, we allow for randomization in the allocations, and measure the outcome in terms of expected, i.e. Von Neumann–Morgenstern utility. Of course, randomness is commonly used in many resource allocation settings. These include school settings, when it is used to break ties for places in over-demanded schools; at universities, where it is used to provide an order or ranking by which students choose over-demanded goods including university housing and seats in courses; it is used for assigning potential bads such as jury service and military call ups. Randomization is used in these settings because monetary transfers are considered undesirable and because the goods are indivisible.====Hylland and Zeckhauser (1979) solved this problem in the case that the legal allocations are matchings. Their solution was based on computing a CEEI equilibrium to allocate fractional shares of the goods, followed by a suitable randomized rounding procedure that maintained expected utilities. A subsequent generalization by Budish et al. (2013) considered the allocation of multiple goods subject to restrictions as would be needed for example in course assignment, where the restrictions correspond to bounds on the number of courses per student, the room capacities, etc. They showed that only certain types of restrictions could be managed. In addition, Budish et al. (2011) gave an implementation of a different scheme for course scheduling based on an approximate CEEI notion.====Hylland and Zeckhauser's solution determines item prices. This does not suffice in general, as was noted in (Budish et al., 2013) and as we illustrate in Example 1.1 below. However, a pricing solution does exist for this example if one uses bundle prices. Bundle prices tend to be harder to compute, and in what circumstances they exist is not clear. Our second example below introduces restrictions on the allowable allocations, and shows that with these restrictions there need not be a bundle pricing either. But in both cases Pareto Efficient and envy-free allocations exist.====Our construction allows the feasible allocations to be constrained, if we impose an anonymity limitation on the allowable allocations. This arises because our construction, when faced with a candidate allocation in which there is envy, will seek to reduce the envy by performing partial swaps of the agents' individual allocations. For this to be possible, every permutation of a feasible allocation must also be feasible, and this is what we mean by anonymity.====It is helpful to specify allocations in terms of partitions of the set of items. Suppose there are ==== players. Then a partition is a particular division of the items into ==== disjoint bundles, but without an allocation of the bundles to the players. An allocation based on a partition ==== is a distribution of the ==== bundles in ==== to the ==== players. Because of the anonymity limitation, as there are ==== players, a single partition yields ====! possible allocations.====The next example shows that even with partition-independent bundle pricing, if the collection of allowable allocations is constrained, then there may be no pricing supporting a Pareto Efficient and envy-free allocation.====In contrast, we show that for any ==== utility functions, including negative-valued utilities, one can obtain a Pareto Efficient and envy-free allocation. Here, partition-based utility functions mean the players' utilities can depend on both the allocations they receive and the whole partition. In addition, we can impose restrictions on the allocations so long as they remain anonymous.====Our solution works by constructing a mapping from the space of mixed allocations and weight vectors to itself. We then apply the Kakutani fixed-point theorem (Kakutani, 1941) to obtain a fixed point. Finally, we prove that the fixed point corresponds to a mixed Pareto Efficient and envy-free allocation. The proof is inspired by (Svensson, 1983; Barbanel, 2005; Weller, 1985).====We conclude the paper by asking how readily one can calculate a Pareto Efficient and envy-free allocation, in terms of the unavoidable communication cost, referred to as the communication complexity of the problem. In particular, we show that even with two players, if their utility functions are submodular, in general, calculating such an allocation will require ==== bits to be communicated between the players, where ==== is the number of items to be allocated, which rapidly becomes infeasible as ==== increases.",On the existence of Pareto Efficient and envy-free allocations,https://www.sciencedirect.com/science/article/pii/S0022053121000247,26 January 2021,2021,Research Article,137.0
"Levy Yehuda John,Veiga André","Adam Smith Business School, University of Glasgow, Glasgow, UK,Imperial College Business School, South Kensington Campus, London, UK","Received 21 June 2019, Revised 10 January 2021, Accepted 12 January 2021, Available online 20 January 2021, Version of Record 29 January 2021.",https://doi.org/10.1016/j.jet.2021.105198,Cited by (0)," (AG) define a notion of equilibrium that always exists in the ==== (RS) model of competitive insurance markets, provided costs are bounded. However, equilibrium predictions are fragile: introducing an infinitesimal mass of high-cost individuals discretely increases all prices and reduces coverage for all individuals. We study sensitivity w.r.t. cost bounds by considering sequences of economies with increasing upper bounds of cost, and determining whether their equilibria converge. We present sufficient conditions under which AG equilibrium exists when cost is unbounded. For simple insurance markets, we derive a necessary and sufficient condition for existence: surplus from insurance increases faster than linearly with expected cost. This condition is empirically common. If the condition fails, a higher bound on cost results in market unraveling: all prices diverge and, in the limit, an AG equilibrium does not exist. We use these results to show that the equilibrium for an insurance market with an unbounded continuum of types is characterized by a simple differential equation. We also provide examples of non-existence for a (single-product) market for lemons with unbounded cost.","The Rothschild and Stiglitz (1976, henceforth RS) model of competitive insurance markets has (at least) two limitations. First, there need not exist a pure-strategies Nash equilibrium. Second, equilibrium is sensitive to assumptions about the upper bound of the cost distribution: introducing an infinitesimal mass of high cost individuals discretely increases the equilibrium price of all contracts (Mailath et al. (1993)). Azevedo and Gottlieb (2017, henceforth AG) suggest a notion of equilibrium that always exists in the RS context. By tackling existence, the AG equilibrium concept allows us to focus on the second limitation, sensitivity to cost bounds. In fact, AG equilibrium is only guaranteed to exist when cost is bounded. Moreover, AG equilibrium predictions are similarly sensitive to cost bounds. This limits the policy usefulness of insurance models because it is often unclear what is a reasonable assumption for the upper bound of cost. Moreover, it is unclear what economic environments feature the sensitivity of the RS setting.====This article derives conditions under which screening markets with adverse selection (and, in particular, insurance markets) have AG equilibria which are robust w.r.t. cost bounds. We do so by considering sequences of truncated economies where cost is bounded, and progressively relaxing this truncation. Our measure of robustness w.r.t. cost bounds is whether the equilibria of the truncated economies converge. That is, whether an equilibrium exists for the limit economy with unbounded cost.====Our motivation is ==== that unbounded costs are particularly relevant or realistic. Instead, we take existence of equilibria as our measure of whether a model's predictions are sensitive to assumptions about the support of costs. If equilibria of bounded economies converge to an equilibrium of the unbounded economy, then model predictions are robust w.r.t. assumptions on cost bounds. Conversely, if assumptions about the support of cost can have an unbounded impact on equilibrium predictions, this can be diagnosed by determining that the limit economy has no equilibrium.====We first consider the setting described in AG. Intuitively, an AG equilibrium is a set of prices and choices such that: a) individuals optimize; b) each contract breaks even; and c) choices and prices of non-traded contracts are robust to small perturbations in fundamentals. In this general setting, we provide sufficient conditions for equilibrium existence when cost is unbounded. However, our most general existence result has a limitation: it does not impose conditions directly on model primitives.====We then focus on the case of insurance markets, as in RS. We allow individuals to differ in risk and risk aversion (but a single parameter determines both, so types are effectively one-dimensional), and assume costlier types have higher marginal willingness to pay for insurance generosity.====In the two-type RS model, there exists a unique AG equilibrium which predicts the same allocation as the pure strategies Nash equilibrium (when it exists). We provide a novel characterization of the (unique) AG equilibrium for an arbitrary number of (possibly unbounded) discrete or continuous types. We then show that, if cost is unbounded, the price of full insurance is also unbounded in any equilibrium.====Our main contribution is a condition on model primitives which is both necessary and sufficient for existence of equilibrium when cost is unbounded. Intuitively, this condition requires that surplus from insurance increases faster than linearly with expected cost. The condition divides insurance economies into “robust” and “fragile” w.r.t. cost bounds. For fragile economies, as cost becomes unbounded, the market unravels: the price of each alternative increases without bound and the chosen level of coverage of each type converges to zero. For fragile economies with unbounded cost, an equilibrium does not exist. Conversely, economies that are robust w.r.t. cost bounds have an equilibrium no matter what assumption is imposed on the support of cost.====This condition is intuitive and empirically relevant. For instance, if utilities are CARA and wealth shocks Gaussian, equilibrium exists if the variance of wealth shocks increases (asymptotically) faster than linearly relative to the mean of these shocks. This condition is satisfied in the empirical findings of Handel et al. (2015) and, more broadly, it seems empirically common that individuals with higher expected cost experience higher variance in insurable shocks (Brown et al. (2014); Hendren (2013)) and therefore obtain higher surplus from insurance. Therefore, insurance economies robust w.r.t. cost bounds seem empirically common. The results further imply that, if individuals differ only in their cost (as in RS), and costs are unbounded, then the economy is fragile: AG equilibrium does not exist.====We use these results to characterize the equilibrium of an insurance market with an unbounded continuum of types, and show it is defined by a simple differential equation. We also characterize the equilibrium for an economy with unbounded discrete types.====We then briefly consider markets for lemons (Akerlof (1970); Einav et al. (2010)), where there is a single non-zero insurance contract available. Even in such simple settings, unbounded costs can result in equilibrium non-existence.====We extend several results to more general insurance markets. For instance, unbounded costs imply unbounded prices even in more general settings where types are truly multidimensional, so there is pooling of multiple types in each contract. This result identifies a large class of economies with unbounded costs where equilibrium does not exist.====Our results are useful in two ways. First, when model primitives are well known, we provide a novel characterization of equilibrium, showing under which conditions a wider distribution of cost types causes a market to unravel. Second, when model primitives are uncertain, we identify conditions under which assumptions about the support of the cost distribution have a large impact on equilibrium.====Section 2 summarizes the setting and results of AG. Section 3 describes our general existence result. Section 4 specializes the model to insurance markets. Section 5 considers markets for lemons. Section 6 generalizes several results. Section 7 concludes. All proofs are in the Appendix (including the Online Appendix), although we provide some intuition for the proofs in the main text.",Competitive insurance markets with unbounded cost,https://www.sciencedirect.com/science/article/pii/S0022053121000156,20 January 2021,2021,Research Article,138.0
Salamanca Andrés,"Paris School of Economics, France","Received 16 December 2019, Revised 24 November 2020, Accepted 6 January 2021, Available online 19 January 2021, Version of Record 27 January 2021.",https://doi.org/10.1016/j.jet.2021.105191,Cited by (10),"This paper characterizes optimal mediation in sender-receiver games. We assume that the mediator's objective is to maximize the ex-ante welfare of the sender. Mediated equilibria are defined by a set of linear incentive constraints. The Lagrange multipliers associated with these constraints yield shadow prices that are used to construct “virtual utility functions” that intuitively characterize the signaling costs of incentive compatibility. Importantly, we characterize the value of an optimal mediation plan (====) through the concavification of the sender's indirect virtual utility function over posterior beliefs. This result provides necessary and sufficient conditions under which a candidate mediation plan is optimal. An additional result establishes a bound on the number of messages that the sender must convey to achieve the value of mediation.","In economic and social interactions, one person often possesses private information that is relevant to other individuals' decisions. For instance, a firm's CEO usually has information about its profitability that is important to its shareholders. In financial markets, analysts possess information that is pertinent to investors. In labor markets, job candidates know their own abilities, which are relevant to prospective employers. These are just a few examples of information that is private yet relevant to others. Communication is thus crucial for both the informed individual and the decision-maker. Information transmission can be arranged as cheap-talk (plain conversation) or be mediated by a neutral third-party. The purpose of this paper is to characterize optimal mediation protocols that maximize the ex-ante welfare of the informed individual. That is, we are interested in communication schemes that are optimal behind a veil of ignorance (i.e., before any private information is acquired).====The model of this paper is a ==== Bayesian game between a privately informed ==== and an uninformed ==== who must implement an action affecting the welfare of both individuals. The sender's information is represented by her ====, which is randomly chosen according to a commonly known prior probability distribution. This interaction situation is called the ====. Notably, besides the discreteness of the types and action spaces, our model imposes neither the widely assumed hypothesis of supermodularity on the utility functions nor regularity conditions on the distribution of types.====We are concerned with the (non-cooperative) solutions of the basic game when the players exchange information using a ====. Any such communication device consists of a protocol in which the sender reports a type to a neutral trustworthy mediator, who then recommends an action to the receiver. A Nash equilibrium of the mediated game is called a ====. In this paper, a mediation plan will be called ==== if it induces a mediated equilibrium that maximizes the sender's (ex-ante) payoff among the mediated equilibria generated by all possible mediation plans. Our primary interest will be to characterize the ====—the value to the sender of an optimal mediation plan. This will allow us to provide necessary and sufficient conditions under which a candidate mediation plan is optimal.====Under mediation, the sender's report is not verifiable either by the mediator or by the receiver, which allows the sender to strategically manipulate her private information. Moreover, the mediator's recommendation is not binding; that is, the receiver is free to choose any action different from the recommended one. A mediation plan with an equilibrium where the sender always reports her type truthfully and the receiver always follows the recommendation is called ====. In general, a mediated game can have many distinct equilibria even when the mediation plan is incentive compatible. Fortunately, the Revelation Principle applies here, and so an optimal mediation plan can be found among the incentive-compatible mediation plans without a loss of generality (see Myerson, 1982; Forges, 1986).====Incentive-compatible mediation plans are described by a system of linear constraints guaranteeing that the sender has no incentive to lie about her actual type—====—and the receiver has no incentive to disobey the mediator—====. Consequently, finding an optimal incentive-compatible mediation plan is a linear programming problem. For such an optimization problem, a Lagrangian function can be constructed by multiplying the truth-telling incentive constraints by variables called ==== (or Lagrange multipliers) and adding them into the objective function. These dual variables yield shadow prices for the “signaling costs” associated with truth-telling incentive compatibility. By incorporating the signaling costs into the sender's utility function, we define the ==== of the sender (see Myerson, 1991, ch. 10).==== The virtual utility is a distorted utility scale that exaggerates the difference between the utility of the sender's actual type and the utilities of her other types. Considering virtual utilities rather than real utilities, we obtain a ==== in which there are no truth-telling incentive constraints. As Bergemann and Morris (2016) pointed out, the fictitious game is a standard Bayesian persuasion problem. Thus, following the concavification approach in Kamenica and Gentzkow (2011), the sender's ==== (i.e., the value to the sender of an optimal information structure) in the fictitious game can be derived from the concave envelope of her indirect virtual utility function over posterior beliefs. Our first main result (Theorem 1) can be stated as follows: ====This result provides a way to reinterpret the ex-post inefficiencies associated with incentive compatibility. Rather than saying that truth-telling forces the sender to incur signaling costs, we may say that, in response to the difficulty of gaining the receiver's trust, the sender transforms her actual preferences into a virtual utility that achieves an optimal separation from her jeopardizing types. Just as economists have used shadow prices of resource constraints to gain insights into the marginal costs of scarcity, Theorem 1 shows how the shadow prices of truth-telling incentive constraints elucidate the nature of signaling costs in situations where misrepresentation is problematic.====We further exploit the concavification approach used in Theorem 1 to characterize an optimal mediation plan through a constrained splitting of the prior belief (====). Provided that none of the truth-telling incentive constraints are binding, Carathéodory's theorem implies that there always exists an optimal mediation plan involving at most ==== different posterior beliefs, where ==== is the number of sender's types. Moreover, the corresponding distribution of posteriors is given by the unique solution of a well-determined system of linear equations asserting that the expected posterior belief must equal the prior belief (====). When misrepresentation is problematic, truth-telling incentive compatibility may increase the number of recommendations that the mediator needs to transmit and, therefore, the number of induced posterior beliefs (see Example 1). As a result, Bayes plausibility may define an underdetermined system of equations with infinitely many solutions.==== Fortunately, Theorem 1 implies a relationship between the truth-telling incentive constraints and their corresponding dual variables, which is known as ====. This relation asserts that whenever a dual variable is strictly positive, the corresponding truth-telling incentive constraint must be binding. Complementary slackness provides the additional restrictions that are required to determine the correct distribution of posteriors. The optimal mediation plan can thus be obtained from the identified distribution of posteriors using Aumann and Maschler's (1995) “splitting lemma”.====As we have argued above, in order to satisfy incentive compatibility, the mediator may need to convey additional recommendations that he would not make in the absence of adverse selection problems. The following question naturally arises: How many different recommendations (resp. posteriors) are transmitted (resp. reached) in an optimal mediation plan? Our second main result (Theorem 2) shows that the smallest number of recommendations that the mediator must convey does not exceed ====. Indeed, by the Fundamental Theorem of Linear Programming, the ==== truth-telling incentive constraints (each of the ==== types does not want to deviate to the other ==== types) increase the number of sufficient recommendations from ==== to ====. We also show that this bound might be tight (see Example 2).====Before concluding the paper, we present a brief analysis of an extension of our approach to the problem of designing optimal mediation plans by a third party who is given effective control over all communication channels between the sender and receiver. Finally, we conclude the paper by comparing our mediation approach to cheap-talk and Bayesian persuasion. We focus on the issues of sender's commitment and incentive compatibility.====The rest of the paper is organized as follows. In Section 2 we frame our contributions in light of the relevant literature. Section 3 introduces the basic setup and formally defines an optimal mediation plan. Section 4 exploits the virtual utility approach to obtain Theorem 1. Theorem 2 is stated in Section 5. Section 6 extends our results to mediation plans that are optimal from a third-party perspective. Finally, we conclude the paper in Section 7 with a comparison of our mediation approach to other information transmission schemes.",The value of mediated communication,https://www.sciencedirect.com/science/article/pii/S0022053121000089,19 January 2021,2021,Research Article,139.0
"Manjunath Vikram,Westkamp Alexander","Department of Economics, University of Ottawa, Canada,Department of Management, Economics and Social Sciences, University of Cologne, Germany","Received 12 March 2019, Revised 18 September 2020, Accepted 11 January 2021, Available online 18 January 2021, Version of Record 26 January 2021.",https://doi.org/10.1016/j.jet.2021.105197,Cited by (8),"We study the balanced exchange of indivisible objects without monetary transfers when agents may be endowed with (and consume) more than one object. We propose a natural domain of preferences that we call ====. In this domain, each agent's preference over bundles of objects is responsive to an ordering over objects that has the following three indifference classes, in decreasing order of preferences: desirable objects, objects that she is endowed with but does not consider desirable, and objects that she neither is endowed with nor finds desirable.====For this domain, we define a class of individually rational, Pareto-efficient, and strategy-proof mechanisms that are also computationally efficient.","We study the problem of reallocating indivisible objects without monetary transfers.==== Unlike much of the earlier work on this problem, we consider situations where each agent may be endowed with and consume more than one object. Our contribution is to define a natural domain of preferences and a class of individually rational, Pareto-efficient, strategy-proof, and computationally efficient mechanisms. Our positive result is in marked contrast with the impossibility results that abound in the literature on multi-object exchange without monetary transfers (Sönmez, 1999; Biró et al., 2018).====Our interest in these exchange problems stems from our search for a solution to the problem of shift-reallocation. Millions of people in many different professions, from physicians to retail workers, engage in shift work. Shift plans are often made months in advance and scenarios like the following are common: A medical practice consists of four specialist consultant doctors A, B, C, and D. This practice is responsible for ensuring that emergency medical services in their specialty are available to a given hospital at all times. That is, each week, one of the four doctors is to be designated as being ====. Being on call is not desirable for these doctors. However, it is necessary for their practice to maintain privileges at the hospital. Since it is a chore that they must perform, in the interest of fairness, they agree to share the weeks equally so that each member of the group is on call every fourth week.==== To facilitate planning, the call schedule is made six months at a time, taking the doctors' preferences into consideration. For instance, the schedule from January 1 to June 30 is announced in December, and is created on the basis of the doctors' preferences as of December. Thus, on January 1, each of the four doctors is responsible for their assigned weeks until June 30. While this initial assignment may be Pareto-efficient with regards to the doctors' December preferences, six months is a long time. At some later point, say the beginning of March, there may be scope for re-optimization based on current preferences. It may well happen that Dr. A would like to attend a conference the week of April 5, Dr. D would like to help at a clinic in a remote area on June 16, and Dr. C would like to go on vacation on May 23. If each of these doctors is obliged to be on call for the respective week, a three-way trade could improve welfare in regards to current (as of March) preferences. We are interested in the design of mechanisms to identify such trades optimally and provide agents with incentives to reveal their private information about scheduling conflicts truthfully.====As the above example illustrates, we are interested in the problem of ==== shifts, or indivisible objects more generally, from fixed endowments rather than designing the initial schedule itself.==== This problem is relevant for many workers: given a fixed schedule, a worker may wish to engage in other activities (like work for other firms, further training or education, vacation) that are incompatible with her currently assigned responsibilities; at the same time, workers will have typically already made some commitments that limit the set of new shifts that they can take on. As long as there are possibilities of shocks to preferences or opportunities to make desirable commitments over the duration of the schedule, workers are bound to find themselves in situations where there are gains from trading pre-assigned shifts.====We define the domain of ==== preferences, which suits applications like shift exchange, as follows. Each agent's preference over bundles of objects is responsive==== to an ordering over objects that has three indifference classes:====We focus on situations in which endowments are commonly known and exchange is balanced in the sense that each agent ends up with the same number of goods as she is endowed with. In the context of shift exchange, this mean that all participants know the initial schedule of shifts and that a worker's total workload is fixed at the number of shifts in the initial schedule.====Trichotomous preferences are a natural restriction since we consider balanced exchange. If an agent is endowed with ==== objects and there are a total of ==== objects, then there are ==== different bundles she could end up with. Even for very modest sized problems, this is an unreasonably large number: with just ten agents, each of whom is endowed with only three objects, there are 4060 different bundles that each of them could consume. That it is impractical to expect an agent to even know, much less communicate, preferences over such a large set has been previously noted in the literature (Milgrom, 2009, Milgrom, 2010, Milgrom, 2011; Othman et al., 2010; Budish and Kessler, 2018). Asking each agent to submit a rank-order list over objects is far more tractable. This is the approach taken, for instance, in the National Resident Matching Program: hospitals have preferences over bundles of doctors but submit a ranking of individual doctors. If hospitals' preferences are responsive, this simplification in the message space does not change the set of equilibrium outcomes compared to when hospitals may actually submit full rankings over bundles (Milgrom, 2010; Perez-Richet, 2011). Even if we restrict preferences to be responsive, the total number of objects ==== may be large. Ranking all of the objects, let alone communicating them to the mechanism, may be difficult. Indeed, there is evidence in the marketing literature that the larger the set a consumer chooses from, the less intense her preferences may be (Greenleaf and Lehmann, 1995; Dhar, 1997; Iyengar and Lepper, 2000; Chernev, 2003). So we have posited that each agent partitions these objects into three sets as we have described above.====The objects of our study are mechanisms that associate every profile of preferences with an allocation. We are interested in mechanisms that satisfy the following desiderata:====While we define these three properties for general preference domains, we define a stronger form of individual rationality for trichotomous preferences. ==== says that for each agent, each component of the bundle that she receives is at least as desirable as some object in her endowment. In other words, her bundle ought to be composed only of objects in the top two indifference classes. On the domain of trichotomous preferences, component-wise individually rational mechanisms have the advantage of being “simple” in the sense that the only information an agent needs to convey is the set of objects that she finds desirable. On the other hand, if a mechanism is merely individually rational, it may depend on more information about how an agent compares bundles. After all, the desirable set alone says nothing about how she compares, for instance, a bundle comprised of objects in both the first and third indifference class against her endowment. We show that ignoring such information does not come at the expense of Pareto-efficiency (Proposition 1).====Our main contribution is to define a new class of mechanisms satisfying these desiderata.==== A ==== (CIRP) mechanism is parameterized by a fixed ordering of the agents, independent of their reports, and picks a final allocation of goods in accordance with the following sequential process:====Apart from their desirable allocative and incentive properties, we also show that CIRP mechanisms are computationally efficient. More precisely, taking agents' desirable sets and a priority order over the agents as inputs, a CIRP allocation can be computed in ==== time, where ==== is the total number of objects, ==== is the number of agents, and ==== is the largest number of objects an agent may be endowed with.====While the characterization of a maximal domain of preferences where our desiderata are compatible is beyond the scope of this paper, we discuss to which extent our approach does or does not extent beyond the trichotomous domain.",Strategy-proof exchange under trichotomous preferences,https://www.sciencedirect.com/science/article/pii/S0022053121000144,18 January 2021,2021,Research Article,140.0
Gerasimou Georgios,"School of Economics & Finance, University of St Andrews, Scotland, United Kingdom","Received 27 April 2020, Revised 7 January 2021, Accepted 12 January 2021, Available online 15 January 2021, Version of Record 19 January 2021.",https://doi.org/10.1016/j.jet.2021.105199,Cited by (4),"We propose and analyze a general model of simple preference intensity comparisons. The model encompasses those that belong to the utility-difference class, has transparent behavioural underpinnings and features purely ordinal uniqueness properties. Its empirical content is characterized by an easily testable condition on dual behavioural data that include choices and additional observables with intensity-revealing potential that are often elicited in experimental/empirical work, such as survey ratings, response times or willingness to pay.","The idea that decision makers not only have preferences over the choice alternatives of interest but may also be capable of ranking differences in preference intensities between those alternatives has been present -often implicitly- in many areas of economics, including decision theory and experiments, welfare and public economics, political economy, voting, matching and market design. All existing modelling approaches in this large and diverse body of work assume that an individual's preference intensities are ultimately expressible on a cardinal or pseudo-cardinal utility scale. This paper shows instead how it is possible to model the decision maker as able to make simple and purely ordinal preference intensity comparisons such as ==== without assuming that such comparisons can also be quantified in any way.====Our approach takes Paul Samuelson's (1938) bivariate formulation of neoclassical cardinal utility representations as a starting point and extends it in a natural and intuitive but hitherto unexplored direction. Specifically, we propose the general class of ==== that mimic ordinal utility ones by associating each ==== of alternatives with a numerical value in a way that preserves the intensity ordering. In particular, letting ==== be such a function, ==== holds if and only if ==== is preferred to ==== at least as much as ==== is to ====. The key novelty of such preference intensity representations that formally distinguishes them from utility-difference ones (or bivariate representations that are formally equivalent to those) is a property that we refer to as ====. This requires that whenever ==== is preferred to ==== and ==== to ====, then ==== is preferred to ==== no less than ==== is to ==== or ==== is to ====, i.e. ====. The condition ensures conceptual harmony between and within the preference and preference intensity relations, and also establishes transitivity of the former without forcing –as is done by utility-difference models– that the value capturing the intensity difference between ==== and ==== be the ==== of the values of the intensity differences between ==== and ==== and between ==== and ====. By not imposing this additivity requirement, the model is characterized by the standard ==== and ==== axioms together with an intuitive ==== one.====Then, generalizing Echenique and Saito (2017) who motivated their analysis based on choice and response-time data, we also show that the testable ==== axiom that we introduce characterizes ==== datasets that comprise finitely many binary choices and an additional menu-specific observable or foregone resource with intensity-revealing potential such as survey-based intensity ratings, response times or willingness to pay. When rationalizable in this way, the relevant dataset can be thought of as being generated by a decision maker whose preferences and intensity comparisons are representable by a preference intensity function that is strictly monotonic in the observed resource. By using preference intensity functions instead of difference-preserving utility functions as its building block, this more general notion of rationalizability can account for substantially more behaviour without sacrificing completeness, transitivity or conceptual harmony between the revealed preference and intensity relations.====The next section introduces preference intensity relations and the relevant notation. The following section defines the benchmark class of utility-difference representations for such relations and summarizes some important conceptual and analytical challenges that are associated with existing manifestations of this modelling approach. Motivated by this discussion, Section 4 introduces and axiomatically characterizes preference intensity functions, clarifying why such representations are genuinely ordinally unique and how they include utility-difference representations as special cases. Section 5 presents the revealed preference intensity analysis and the last section concludes. All proofs appear in the Appendix.",Simple preference intensity comparisons,https://www.sciencedirect.com/science/article/pii/S0022053121000168,15 January 2021,2021,Research Article,141.0
Gollier Christian,"Toulouse School of Economics, University of Toulouse-1 Capitole, France","Received 19 November 2020, Accepted 3 January 2021, Available online 14 January 2021, Version of Record 18 January 2021.",https://doi.org/10.1016/j.jet.2021.105189,Cited by (2),"Suppose that the conditional distributions of ==== (resp. ====) can be ranked according to the ====-th (resp. ====-th) risk order. Increasing their statistical concordance increases the ==== degree riskiness of ====, i.e., it reduces expected utility for all bivariate utility functions whose sign of the ==== cross-derivative is ====. This means in particular that this increase in concordance of risks induces a ==== degree risk increase in ====. On the basis of these general results, I provide different recursive methods to generate high degrees of univariate and bivariate risk increases. In the reverse-or-translate (resp. reverse-or-spread) univariate procedure, a ==== degree risk increase is either reversed or translated downward (resp. spread) with equal probabilities to generate a ==== (resp. ====) degree risk increase. These results are useful for example in asset pricing theory when the trend and the volatility of consumption growth are stochastic or statistically linked.","The theory of stochastic dominance has been developed almost five decades ago initially to determine the conditions under which all (risk-averse) individuals dislike a specific change in risk.==== Rothschild and Stiglitz (1970) have characterized the second-degree risk increase through either an integral condition and a more intuitive approach using the concept of mean-preserving spreads. For any integer ====, Ekern (1980) has more generally defined the ====-th degree risk increase as any change in risk that is disliked by any von Neumann-Morgenstern individual whose ====-th derivative has the same sign as ====. For ==== from 2 to 4, this corresponds to the notions of risk aversion, prudence and temperance, respectively. Ekern characterizes the ====-th degree riskiness through an integral condition, but he failed to provide an intuitive approach associated to it. For a long time, economists knew little about the ====-th degree risk increase beyond Ekern's integral condition. A breakthrough came when Eeckhoudt and Schlesinger (2006) and Eeckhoudt et al. (2009) found an intuitive way to decompose a higher-degree risk increase into lower-degree risk increases through a process known as risk apportionment. In this paper, I propose to extend their methodology to its logical ultimate generalization.====Suppose that the ====-th degree riskiness of ==== and the ====-th degree riskiness of ==== are uncertain. This is modeled by assuming that ==== is parametrized by ====, and that an increase in ==== implies a ====-th degree risk increase of ====, which denotes the random variable that is distributed as ==== conditional on ====. Similarly, assume that ==== is parametrized by ====, and that an increase in the risk measure ==== implies a ====-th degree risk increase of ====. The uncertainty affecting the riskiness of these two random variables is represented by a joint distribution function for ====. I demonstrate that increasing the concordance of this pair of random variables, in the sense of Tchen (1980) and Epstein and Tanny (1980), raises the ====-th degree riskiness of ====.====To illustrate, consider the special case in which the pair of risk measures ==== has a support limited to ====. In other words, the riskiness of ==== or ==== can be either “low” or “high”, in the sense of the ====-th or ====-th degree risk respectively. Consider also the special case in which ==== is initially perfectly negatively correlated, i.e., its distribution puts equal probabilities on the two white circles in Fig. 1. This is a case in which ==== sums up two variables whose conditional distributions systematically combine a low risk with a high risk. Suppose now that one switches to perfect positive correlation for ====, as represented by the two black circles in that figure. The generic result stated above implies that this shift in distribution represented by the arrows in Fig. 1 makes ==== riskier than the initial one, in the sense of the ====-th degree risk order. This is the idea developed in Eeckhoudt and Schlesinger (2006) and Eeckhoudt et al. (2009). In short, when two versions of ==== and ==== must be combined in two equally likely states, it is always safer to apportion the riskier version of these two lotteries in different states, thereby compounding more risk with less risk. This simple concept of risk apportionment provides an intuitive approach to high orders of risk increases, as did the concept of mean-preserving spreads for second-degree risk increases.====The basic idea conveyed in the recent literature on risk apportionment is that making different harms more statistically concordant deteriorates welfare ex ante. But the result by Eeckhoudt et al. (2009) is a special case of this idea because it is limited to two states for ====, equal state probabilities and perfect (negative or positive) correlations. In this paper, I remove these appendages of the existing theory of risk apportionment to empower the initial idea that increasing the concordance in riskiness (rather than harms) deteriorate welfare. The initial and final distributions of ==== could be anything as long as they satisfy the restrictions of the concordance order defined by Tchen (1980). The generality of the results presented in this paper and their link with the existing literature are made explicit by observing that any increase in concordance can be described by a sequence of marginal shifts in probability masses described in Fig. 1. This generates more powerful and general results, as shown in Theorem 1, Theorem 2, and in the various applications developed in this paper. I also go one step further in the generalization of the theory by extending it to non-monetary outcomes, in the spirit of Richard (1975), Tchen (1980), Epstein and Tanny (1980), Eeckhoudt et al. (2007) and Meyer and Strulovici (2013).====The fundamental structure of the existing literature is based on the state-dependence of the riskiness of two lotteries. How should these risk intensities be apportioned across states in this framework? This paper makes clear that, in this framework, the answer to this question is based on the key concept of comparative concordance. Indeed, all existing special cases of my generic results illustrate the idea that raising the concordance of risk intensities of two random variables increases the risk intensity of their combination. Moreover, Theorem 1 in this paper claims that more concordance is the weakest restriction in the re-apportionment of risk intensities that guarantees that this combination becomes riskier.====Generalizing a theory has costs and benefits. The main cost is to make the result more complex and therefore potentially less intuitive. The impressive success of Eeckhoudt and Schlesinger (2006) comes precisely from the simplicity and elegance of their analysis, that is yet strong enough to yield the expected utility equivalence result. I preserve this elegance by exhibiting different recursive methods to build high order risk increases. However, one must recognize at the same time that the restrictive assumptions mentioned in this introduction have limited their applicability to other fields. More than ten years after the first publications of this new theory, researchers in finance and macroeconomics still fail to take notice of it. This paper offers an attempt to break this standoff.====The asset pricing literature on long run risks pioneered by Bansal and Yaron (2004) provides an interesting field of applications for the decision-theoretic results presented in this paper. For example, as is well-known, the persistence to the shocks affecting the trend of consumption growth magnifies the long run risk, and is an illustration of the fact that increasing the concordance between two first degree risks raises the second degree risk of their sum. A less well understood phenomenon emerges when persistent shocks to the volatility of growth are introduced, which helps solving the equity premium puzzle. In fact, this is an illustration of the result that introducing positive concordance to two second degree risks increases the fourth degree riskiness of their sum. As already shown by Gollier (2018), a positive serial correlation in volatility raises the equity premium only if the representative agent is temperant, i.e., has a utility function whose fourth derivative is negative. Following Tinang (2017), we can also examine the case of introducing a negative concordance between the trend and the volatility of consumption growth, which can be shown to yield third degree risk increase of future consumption.==== In another direction, this new literature on high degree risk orders has been useful to determine the impact of a change in risk on optimal decisions such as saving, portfolio, insurance and self-protection choices.====These results provide a simple iterative procedure to test for the sign of any derivative of the utility function. For example, a ====-th degree risk increase can be obtained from a ====-th degree risk increase though a simple “reverse-or-translate” randomized actions: with probability 0.5, one reverses it, and with probability 0.5, one translates it to smaller wealth levels. This is an application of Theorem 2 with ====, where the first degree risk increase on ==== takes the form of a sure reduction of wealth. For example, as shown by Chiu (2005) in case ====, a simple mean-preserving spread can be obtained by a “reverse-or-translate” procedure that randomizes a simple first-degree risk reduction at some wealth level (i.e. an upward transfer of probability mass) with the symmetric first-degree risk increase at some lower wealth level. Using this second-degree risk increase as the new basic ingredient, a third-degree risk increase can be obtained from a new “reverse-or-translate” randomization of its reversal at some wealth level and of its implementation at some lower level. This recursive method is not new, but it has been obscured in the literature by a misleading terminology. In Eeckhoudt and Schlesinger (2006) and Eeckhoudt et al. (2009) for example, the qualifier “good” or “bad” was used to describe the ====-th degree change in risk, a terminology that requires knowing the sign of the ====-th derivative of the utility function. This means that the recursive method required knowing the sign of all derivatives up to ==== to generate a meaningful result. In reality, Theorem 2 shows that one should replace the qualifier “good” or “bad” by “safer” or “riskier” in the sense of the ====-th order, a solution that abstains us from restricting the sign of the successive derivatives of the utility function to perform the recursive method described here. In short, high orders of risk aversion are about a preference for the disaggregation of risks, not about the disaggregation of bads.====Theorem 2 can also be applied recursively with ====, thereby allowing to generate all odd (starting with ====) and even (starting with ====) orders of risk. It uses a randomization procedure that I call “reverse-or-spread” that randomizes between a reversal of a risk increase and its spread. For example, a fourth-degree risk increase can be obtained from a second-degree risk increase through a “reverse-or-spread” procedure that randomizes between its reversal at some wealth level and its spread around that wealth. We illustrate the randomized reverse-or-spread procedure in Table 1, starting from the bottom with a mean-preserving spread that transfers a probability mass ==== from outcome 0 to outcome -1 and +1 in a symmetric way. I build a fourth degree risk increase by combining a reversal and a spread of this second degree risk increase. With probability 1/2, this spread is reversed, which leads to a transfer of probability mass described in row “reverse” of ====. With probability 1/2, the initial mean-preserving spread is spread, with a probability 1/2 to be translated by 1 to the left, and with probability 1/2 to be translated by 1 to the right. This leads to a transfer of probability mass described in row “spread” of ====. The net transfer of probability mass is described in row “total”, which describes a fourth degree risk increase. The iteration to ==== is also described in this table. As an illustration, this implies that lottery ==== is sixth-degree riskier than lottery ====.====In Section 2, I summarize the basic concepts and properties of m-th degree risk orders and of comparative concordance. Section 3 is devoted to the derivation of the basic results of the paper. In Section 4, I describe some new applications of Theorem 2 in the Gaussian world. Based on these results, Sections 5 and 6 are devoted to the development of recursive procedures to construct high orders of risk increases, respectively in the univariate and bivariate cases.",A general theory of risk apportionment,https://www.sciencedirect.com/science/article/pii/S0022053121000065,14 January 2021,2021,Research Article,142.0
"Ma Qingyin,Toda Alexis Akira","International School of Economics and Management, Capital University of Economics and Business, 121 Huaxiang Zhangjia Crossing, Fengtai, Beijing 100070, PR China,Department of Economics, University of California San Diego, 9500 Gilman Dr, La Jolla, CA 92093, USA","Received 13 July 2020, Revised 5 January 2021, Accepted 6 January 2021, Available online 13 January 2021, Version of Record 19 January 2021.",https://doi.org/10.1016/j.jet.2021.105193,Cited by (9),"Empirical evidence suggests that the rich have higher ==== than do the poor. While this observation may appear to contradict the homotheticity of preferences, we theoretically show that that is not the case. Specifically, we consider an income fluctuation problem with homothetic preferences and general shocks and prove that consumption functions are asymptotically linear, with an exact analytical characterization of asymptotic marginal propensities to consume (MPC). We provide necessary and sufficient conditions for the asymptotic MPCs to be zero. We calibrate a model with standard constant relative risk aversion utility and show that zero asymptotic MPCs are empirically plausible, implying that our mechanism has the potential to accommodate a large saving rate of the rich and high wealth inequality (small Pareto exponent) as observed in the data.","Empirical evidence suggests that the rich have higher propensity to save than do the poor.==== This fact implies that the rich have lower marginal propensity to consume (MPC), which has important economic consequences. For example, when the rich have lower MPC, the consumption tax, which is a popular tax instrument in many countries, becomes regressive and may not be desirable from equity perspectives. MPC heterogeneity also implies that the wealth distribution matters for determining aggregate demand and hence monetary and fiscal policies (Kaplan et al., 2018; Mian et al., 2020).====Why do the rich save so much? Intuition suggests that canonical models of consumption and savings that feature (identical) homothetic preferences are unable to explain the high saving rate of the rich: in such models, consumption (hence saving) functions should be asymptotically linear in wealth due to homotheticity, implying an asymptotically constant saving rate. A seemingly obvious explanation for the high saving rate of the rich is that preferences are not homothetic.==== However, non-homothetic preferences have some undesirable theoretical properties. First, they are inconsistent with balanced growth (whereas many aggregate economic variables such as real per capita GDP are near unit root processes), at least in basic models in which preference parameters are constant. Second, non-homothetic utility functions have more parameters than homothetic ones, which introduces arbitrariness in model specification and calibration.====In this paper we theoretically show that the intuition of “homotheticity implies (asymptotic) linearity” is only partially correct. We consider a standard income fluctuation problem with (homothetic) constant relative risk aversion (CRRA) preference but with capital and labor income risks in a general Markovian setting. We prove that the consumption functions are asymptotically linear in wealth, or the asymptotic marginal propensities to consume converge to some constants.==== While this statement is intuitive, there is one surprise: we obtain an exact analytical characterization of the asymptotic MPCs and prove that they can be zero. The asymptotic MPCs depend only on risk aversion and the stochastic processes for the discount factor and return on wealth, and are independent of the income process. Furthermore, we derive necessary and sufficient conditions for zero asymptotic MPCs. When the asymptotic MPCs are zero, the saving rates of the rich converge to 1 as agents get wealthier. Thus, we provide a potential explanation for why the rich save so much, and we do so with standard homothetic preferences.====To prove that consumption functions are asymptotically linear with particular slopes, we apply policy function iteration as in Li and Stachurski (2014) and Ma et al. (2020). Since agents cannot consume more than their financial wealth in the presence of borrowing constraints, a natural upper bound on consumption is asset, which is linear with a slope of 1. Starting from this candidate consumption function, policy function iteration results in increasingly tighter upper bounds. On the other hand, we directly obtain lower bounds by restricting the space of candidate consumption functions such that they have linear lower bounds with specific slopes. We analytically derive these slopes based on the fixed point theory of monotone convex maps developed in Du (1990), which has recently been applied in economics by Toda (2019) and Borovička and Stachurski (2020). Finally, we show that the upper and lower bounds thus obtained have identical slopes, implying the asymptotic linearity of consumption functions with an exact characterization of asymptotic MPCs.====To assess the empirical plausibility of our new mechanism, we numerically solve a partial equilibrium model with CRRA utility and capital income risk calibrated to the U.S. economy. We find that with moderate risk aversion (above 3), the asymptotic MPCs become zero, the saving rates of the rich are increasing and approach 1, and the implied wealth Pareto exponent is close to the value in the data.====Our paper is related to the theoretical studies of the income fluctuation problem, which is a key building block of heterogeneous-agent models in modern macroeconomics.==== Chamberlain and Wilson (2000) study the existence of a solution assuming bounded utility and applying the contraction mapping theorem. Li and Stachurski (2014) relax the boundedness assumption and apply policy function iteration. Benhabib et al. (2015) consider a special model with CRRA utility, constant discounting, and ==== and mutually independent returns and income shocks to study the tail behavior of wealth. Ma et al. (2020) allow for stochastic discounting and returns on wealth in a general Markovian setting and discuss the ergodicity, stochastic stability, and tail behavior of wealth. Carroll (2020) examines detailed properties of a special model with CRRA utility, constant discounting and risk-free rate, and ==== permanent and transitory income shocks.====While the main focus of these papers is the existence, uniqueness, and computation of a solution, we focus on the asymptotic behavior of consumption with general shocks. Carroll and Kimball (1996) show the concavity of consumption functions in a class of income fluctuation problems with hyperbolic absolute risk aversion (HARA) utility,==== which implies asymptotic linearity. However, they do not characterize the asymptotic MPCs as we do. As an intermediate result to examine the wealth accumulation process, Proposition 5 of Benhabib et al. (2015) characterizes the asymptotic MPC of a special model described above. Carroll (2020) also intuitively discusses the asymptotic linearity of the consumption function in a model without capital income risk, and points out in Appendix A.2.2 and Fig. 6 the possibility of zero asymptotic MPCs, although that case requires a negative interest rate. Our contribution relative to these results is that we obtain a rigorous and complete characterization of asymptotic MPCs in a general setting (including capital income risk and Markovian shocks), analyze the necessity of these advanced features in generating zero asymptotic MPCs, and show through a numerical example that they are empirically plausible.====The rest of the paper is organized as follows. Section 2 introduces a general income fluctuation problem, proves the asymptotic linearity of consumption functions with homothetic preferences, and discusses some examples. Section 3 applies the theory to a calibrated model and shows that zero asymptotic MPCs are empirically plausible and generate high wealth inequality (small Pareto exponent). Appendices A and B contain the proofs. Replication files for Section 3 are available at ====.",A theory of the saving rate of the rich,https://www.sciencedirect.com/science/article/pii/S0022053121000107,13 January 2021,2021,Research Article,143.0
"Eliaz Kfir,Spiegler Ran,Thysen Heidi C.","School of Economics, Tel-Aviv University, Israel,David Eccles School of Business, The University of Utah, United States of America,Economics Dept., University College London, United Kingdom of Great Britain and Northern Ireland,CFM, United Kingdom of Great Britain and Northern Ireland,London School of Economics, United Kingdom of Great Britain and Northern Ireland","Received 27 March 2020, Revised 30 October 2020, Accepted 7 January 2021, Available online 13 January 2021, Version of Record 19 January 2021.",https://doi.org/10.1016/j.jet.2021.105192,Cited by (7),"We study strategic communication when the sender's multi-dimensional messages are given an interpretation by the sender himself or by a proxy. Interpreting messages involves the provision of some data about their statistical state-dependence. Interpretation can be selective: different kinds of data interpret different sets of message components. The receiver uses this data to decipher messages, yet he does not draw any inferences from the kind of data he is given. In this way, strategic interpretation of messages can influence the receiver's understanding of their equilibrium meaning. We show that in a two-action, two-state setting, the sender can attain his first-best payoff when the prior on one state exceeds a threshold that decays quickly with message dimensionality. We examine the result's robustness to the critique that our receiver does not attempt any inferences from selective interpretations.","In the simplest textbook model of strategic communication, originated by Crawford and Sobel (1982), a “sender” privately observes a state of Nature and chooses a costless message from some given message space. Then, a “receiver” observes the message and takes an action that affects both parties' payoffs. A hallmark of this conventional approach is that messages have no intrinsic meaning; their content - namely, their statistical relation with the underlying state - is established in Nash equilibrium of the sender-receiver game. According to the standard steady-state interpretation of this solution concept, the receiver has access to a “dataset” that fully reveals the statistical relation between states and messages.====In this paper we revisit the basic sender-receiver model and relax the assumption that the receiver is fully capable of interpreting equilibrium messages. We focus on settings in which the receiver has two available actions, ==== and ====. In each state of Nature, exactly one of these actions is appropriate. The prior probability of the states for which ==== is the appropriate action is ====. The receiver's sole objective is to select the appropriate action. This familiar setting is borrowed from Glazer and Rubinstein, 2004, Glazer and Rubinstein, 2006 or Kamenica and Gentzkow (2011). For most of the paper, we follow these papers by also assuming that the sender always wants the receiver to play ==== (but we also examine an alternative, “zero-sum” specification).====By default, our receiver lacks access to any data regarding the state-message mapping, and therefore cannot decipher messages by himself. He is like a tourist in a foreign country who does not understand its language or cultural codes. However, if an “interpreter” handed him a “====” containing data regarding the statistical mapping from states to the sender's messages, he would have some ability to interpret the message he receives.====Our model makes room for the ==== supply of such dictionaries. The sender himself - or a ==== who acts as an ==== on the sender's behalf - chooses a dictionary from some feasible set. He can condition the dictionary on the state and the message. Thus, different messages may be accompanied by different dictionaries, and the same message may be paired with different dictionaries in different states. Each dictionary provides ====, yet possibly selective statistical data regarding the sender's state-message mapping (given by the sender's strategy). The receiver uses this data to update his belief given the message. Crucially, our basic model assumes that the receiver lacks any other means for extracting the meaning of messages (we relax this assumption in Section 4). Consequently, he ====, since this would require some data regarding the ==== distribution of messages, dictionaries and states - data the receiver does not have.==== Consequently, the sender can manipulate the receiver's beliefs beyond what is feasible under rational expectations.====Strategic interpretation of messages - in the sense of providing selective statistical data about their meaning - is pervasive in real-life situations, whether the messages are cheap talk or hard-information disclosures. Consider an employee who wants to exert effort only when sufficiently sure he is not about to be fired. He is summoned to the General Manager's office to hear about his prospects at the company. After the meeting is over, the HR manager (who was present at the meeting) explains that when the GM says to an employee “you have a future in the company”, this means a 50% chance of keeping his job. This is an interpretation of the GM's verbal message. It is selective because it ignores other aspects of the GM's communication, e.g. his body language. Alternatively, the HR's interpretation could focus on the latter: “The GM's handshake was feeble; this is definitely bad news”.====Another example involves a tenure case that is brought in front of a university promotions committee. Although the candidate submits his CV, committee members outside his discipline cannot decipher the connection between the candidate's quality and indicators such as the number of publications, conference lectures or supervised students. The candidate's department chair will offer an interpretation by providing statistical data about researchers in comparable departments (including their subsequent academic performance, which indicates their “true quality”). If the chair's objective is misaligned with the university committee's, the data he provides may be strategically selective. In a similar vein, imagine a foreign candidate for a graduate program. The candidate submits his grade transcript, yet the admission committee does not know the grades' meaning. A faculty member writing a recommendation letter on the candidate's behalf may provide such an interpretation, by describing the grade distribution for a selected subset of courses.====Finally, suppose the sender is a political party and the receiver is a representative voter. The party's message is multi-dimensional, where each component describes public pronouncements by a different party member. A political commentator interprets the party's message in some media outlet. He does so by providing historical data about the match between the public pronouncements of selected party organs and the underlying reality.====These are all examples of selective interpretations where the receiver is presented with partial statistics about the sender's state-dependent, multi-dimensional message. These interpretations can be strategic when the interpreter's interests are misaligned with the receiver's. We analyze the sender's choice of messages when he takes their subsequent strategic interpretation into account. For instance, the way a political party structures the public statements made by its members will be shaped by its expectation of how a media outlet that is biased in its favor will interpret these statements.====One could argue that in these examples, the statistical data the interpreter provides need not be perfectly credible or unbiased. However, because they are quantitative and verifiable, they are more likely to be credible than cheap-talk messages like “you have a future in the company”. At any rate, we abstract from this consideration; our analytical task is to quantify the effect of strategic provision of ==== messages and their interpretation on the sender's ability to attain his objective, assuming perfect credibility of the statistical data these interpretations involve. In the course of this paper, we will consider various kinds of partial statistics that strategic interpretations can entail.====We present our basic model in Section 2, where we define a dictionary as a non-empty subset of the components of a ====-dimensional message. The dictionary enables the receiver to learn the state-dependent joint distribution of these components. We assume that the interpreter's preferences fully coincide with the sender's. For expositional convenience, our formal exposition regards them as a ==== player who ==== to a state-dependent joint distribution over messages and dictionaries. Neither of these two assumptions is necessary for our main findings. (In our informal description, we occasionally refer to the interpreter as a distinct agent who shares the sender's preferences.)====In Section 3, we present our main result, which characterizes the maximal probability of persuasion as a function of ==== and ====. In particular, we show that the sender can attain full persuasion, as long as ==== is above a cutoff ==== given by a simple formula that makes use of Sperner's Theorem and decays quickly with ====.====Our assumption that the receiver draws no inferences from the dictionary he is given raises natural questions. First, does the dictionary itself convey information about the underlying state? The answer is negative: The sender-optimal strategy we construct has the property that the distribution over dictionaries is state-independent. Second, would the receiver be “suspicious” of a dictionary that does not cover all message components? We address this question in Section 4, while insisting on sender strategies that induce a state-independent dictionary distribution.====In Section 4.1, we perturb the model by assuming that the sender has a lexicographically secondary preference for small dictionaries. We also introduce a refinement of the sender's strategy: if the sender's interests were aligned with the receiver's, he would want to play a strategy that induces the same observed distribution over dictionaries. Thus, if the receiver had independent access to data about the distribution of dictionaries, he could reconcile the observed use of selective dictionaries with a benevolent sender. Under this refinement, we show that full persuasion is attainable if and only if ====. The sender's strategy only interprets ==== message components.====In Section 4.2, we modify the definition of dictionaries. When a dictionary ==== is provided, this now means that the receiver learns the state-dependent distribution of ==== ==== the state-dependent distribution of ====. Thus, the interpreter is forced to provide statistical data about the behavior of ==== message components, though in a format that can break them into two disjoint sets. Under a mild assumption on how the receiver extrapolates a belief from these pieces of data, we show that full persuasion is attainable whenever ==== exceeds a cutoff that decays quickly with ====. The lesson from these two variants of our basic model is that strategic interpretation can produce effective persuasion without generating excessive “suspicion” regarding its selectivity.====Section 5 picks up the theme of Section 4.2 and present an example that illustrates a richer notion of dictionaries, which involves data about other slices of the joint state-message distribution. We show how this richer specification can enhance the sender's ability to attain full persuasion. In Section 6 we perform partial analysis of our basic model when the two parties have diametrically opposed preferences. We discuss related literature in Section 7.",Strategic interpretations,https://www.sciencedirect.com/science/article/pii/S0022053121000090,13 January 2021,2021,Research Article,144.0
Uribe Martín,"Columbia University, United States of America,CEPR, United Kingdom,NBER, United States of America","Received 3 May 2020, Revised 7 January 2021, Accepted 10 January 2021, Available online 13 January 2021, Version of Record 19 January 2021.",https://doi.org/10.1016/j.jet.2021.105195,Cited by (5),"This paper establishes the existence of endogenous cycles in infinite-horizon open economy models with a flow collateral constraint. It shows that for plausible parameter configurations, the economy has a unique equilibrium exhibiting deterministic cycles in which periods of debt growth are followed by periods of debt deleveraging. In particular, three-period cycles exist, which implies by the Li-Yorke Theorem the presence of cycles of any periodicity and chaos. The paper also shows that deterministic cycles are absent in the Ramsey optimal allocation providing a justification for macroprudential policies even in the absence of uncertainty.","Financial frictions have been shown to amplify the business cycle. This paper argues that they can also be its engine. It studies a model of an open economy with a flow collateral constraint, whereby external debt is limited by the value of tradable and nontradable income. This environment has been extensively used to shed light on important issues in open economy macroeconomics such as inefficient credit booms, overborrowing, and sudden stops. However, the related literature has limited attention to economies driven by exogenous stochastic disturbances. The contribution of the present paper is to show that the mere presence of the financial friction can engender cyclical fluctuations. To highlight this result we abstract from any source of uncertainty and characterize perfect foresight equilibria. Furthermore, we focus on parameterization for which the equilibrium is unique.====The first result of the paper is a full analytical characterization of the debt policy function (i.e., the choice of current debt as a function of the level of past debt) in infinite-horizon environments in which agents are impatient (====) and lines of credit are tied to income. These two features are defining elements of the literature to which this paper contributes. To the best of our knowledge, this is the first paper to achieve this task. Although numerical characterizations in stochastic environments do exist. The paper shows that the debt policy function is continuous but nonmonotonic. Importantly, it shows that the maximum of the debt policy function lies above the 45-degree line and occurs at a level of debt below the steady state, that is, below the largest constant level of debt that satisfies the collateral constraint and guarantees positive consumption. This characteristic of the debt policy function gives rise to endogenous equilibrium dynamics in which the economy oscillates around the steady state level of debt and suffers from recurring inefficient credit booms followed by debt deleveraging.====The second contribution of the paper is to show that for conditions that obtain under plausible calibrations, the aforementioned oscillatory dynamics are periodic, which means that the economy perpetually fluctuates around the steady state without ever converging to it. The economy exhibits cycles of periodicity three. By the Li and Yorke (1975) theorem, this implies the existence of cycles of any periodicity and chaos. The endogenous cycles identified in this paper share a number of features of business cycles observed in emerging market economies. In particular, during the expansionary phase of the cycle, external credit grows, domestic absorption expands, the real exchange rate appreciates, and the current account deteriorates. At some point, the financial constraint binds, the credit boom comes to a stop, there is widespread debt deleveraging, the real exchange rate depreciates, and the current account experiences a reversal.====The emergence of endogenous cycles has to do with three key features of the class of models to which this paper belongs. One is that agents are impatient in the sense that their subjective discount rate exceeds the market discount rate (====). This feature drives agents to front load consumption. Absent financial frictions, household debt would rise monotonically and approach the natural debt limit. The second is a financial friction taking the form of a collateral constraint. The third feature of the model that is key for the emergence of endogenous cycles is the well known fact that when collateral depends on equilibrium prices, the collateral constraint creates a pecuniary externality. In the present model collateral is the sum of tradable and nontradable income so that the relative price of nontradables in terms of tradables affects the value of collateral measured in units of tradable goods. By this externality, agents fail to internalize the full costs of temporarily borrowing beyond the maximum level of debt that is sustainable in the long run. In particular, they fail to see that their individual borrowing, in the aggregate, fuels the credit boom by raising the value of collateral through real exchange rate appreciation and that their deleveraging by depreciating the real exchange rate exacerbates the credit crunch. Debt deleveraging has a cleansing effect, as debt levels must fall significantly below the level that is sustainable in the long run. At this point, impatient consumers feeling financially stronger embark on another credit boom and the story repeats itself.====The third result of the paper is to show that endogenous debt cycles are inefficient in the sense that they imply greater fluctuations in consumption than is socially optimal. We characterize analytically the debt policy function of the Ramsey planner. This characterization is also novel as only numerical approximations for stochastic economies are presented in the related literature. As is well known, the Ramsey planner behaves like an individual who becomes more patient in periods in which the collateral constraint is slack in the current period but binding in the next. Thus she puts more weight on the future costs of deleveraging than do private households. We present conditions under which it is optimal for the Ramsey planner to eliminate endogenous cycles altogether.====Finally, we characterize the associated optimal capital control policy and show that the planner puts capital control taxes into place when next-period debt in the laissez-faire economy exceeds the level of debt that is sustainable in the long run. This result is a refinement of an existing one in stochastic versions of the present economy, namely, that the Ramsey planner imposes capital controls in the current period when the probability that the collateral constraint will bind under the optimal allocation in the next period is strictly positive. This finding shows that capital controls can be optimal in deterministic environments.====This paper is related to a large and growing literature on financial constraints in open economy models. The type of flow collateral constraint we study was introduced in open economy models by Mendoza (2002) to understand sudden stops caused by fundamental shocks. The pecuniary externality that emerges in this framework and the consequent room for macroprudential policy is studied in Korinek (2011), Lorenzoni (2008), Bianchi (2011), Benigno et al. (2013, 2016), Schmitt-Grohé and Uribe (2017a, 2017b), Dávila and Korinek (2018), and Jeanne and Korinek (2019), among others. Multiple equilibria have been identified in models with stock collateral constraints, where debt is limited by the value of capital or land, by Jeanne and Korinek (2019) and Schmitt-Grohé and Uribe (2017a) and in models with flow collateral constraints, where debt is limited by income, in Schmitt-Grohé and Uribe (forthcoming). By contrast, the present paper focuses on parameterizations for which the equilibrium is unique and characterizes conditions under which there exist deterministic cycles taking the form of periodic equilibria or chaos.====The paper is also related to a closed-economy literature showing that financial frictions can give rise to endogenous instability in both infinite-horizon and overlapping-generations economies. Benhabib et al. (2016) show the existence of chaotic equilibrium dynamics when the financial friction takes the form of limited enforcement in the banking sector. Woodford (1989) shows that periodic equilibria and chaos can occur when the financial friction takes the form of market segmentation whereby workers are hand-to-mouth consumers and firms finance investment from retained earnings. Beaudry et al. (2020) characterize periodic equilibria in a New Keynesian model with consumer default risk and bank monitoring costs. Suarez and Sussman (1997) show the possibility of boom bust cycles in models with asymmetric information in financial markets and Dong and Xu (2020) in a model with banks and financially constrained heterogeneous firms. Matsuyama (2007) emphasizes agency problems in credit markets as a source of cycles. Azariadis et al. (2016) show the existence of self-fulfilling credit cycles in a model with unsecured corporate debt, Benhabib et al. (2018) in an economy with adverse selection in credit markets, and Cui and Kaas (forthcoming) in a model with corporate default. Gorton and Ordoñez (2014, 2020) and Chousakos et al. (2020) analyze overlapping generations frameworks in which financial constraints are informational in nature and information and productivity comove in such a way as to create credit-led boom bust cycles. Gu et al. (2013) show that endogenous credit cycles may arise in an economy with endogenous debt constraints due to limited commitment to repay. Martin and Ventura (2012) and Miao and Wang (2018) introduce financial frictions in models of rational bubbles to generate booms and busts in net worth and asset prices.====The remainder of the paper is organized as follows. Section 2 presents the model. Section 3 shows that in equilibrium the financial constraint must bind in an infinite number of periods and characterizes the steady state of the economy. Section 4 provides an analytical characterization of the debt policy function. Section 5 derives conditions under which deterministic cycles exist. Section 6 characterizes the Ramsey allocation and establishes conditions under which it is optimal to eliminate endogenous cycles. Section 7 concludes.",Deterministic debt cycles in open economies with flow collateral constraints,https://www.sciencedirect.com/science/article/pii/S0022053121000120,13 January 2021,2021,Research Article,145.0
"Galeotti Andrea,Ghiglino Christian","Department of Economics, London Business School, United Kingdom,Department of Economics, University of Essex, United Kingdom,GSEM Geneva, Switzerland","Received 10 November 2017, Revised 26 December 2020, Accepted 6 January 2021, Available online 13 January 2021, Version of Record 19 January 2021.",https://doi.org/10.1016/j.jet.2021.105194,Cited by (5),"Cross-ownership smooths firms' idiosyncratic shocks but affects their portfolio choice and, therefore, their risk-taking position. The classical intuition on the role of pooling risk in raising welfare is valid when ownership is evenly dispersed. However, when the ownership of some firms is concentrated in the hands of a few others, deeper integration leads to excessive risk-taking and volatility and, consequently, it results in lower aggregate welfare.","Back in the 1930s, the influential work of Berle and Means (1932) reported that ownership of capital of large corporations in USA was dispersed among small shareholders and that 44% of the largest 200 corporations were under effective management control. This picture was scrutinised within the corporate finance literature over the years.==== After the Second World War, the increase in stock market participation of households, buying directly companies' shares, made the dispersed ownership structure even more salient. But from 1980 households' participation in the stock market started to be channeled via the acquisition of mutual funds actively managed by an institution. Davis (2008) investigated the implication of this trend to ownership structure. He reported a widespread increase in cross-ownership accompanied by an increase in ownership concentration. Harford et al. (2011) provide similar insights. Fichtner and Heemskerk (2020) revised and confirmed these earlier empirical results, taking into account the growth of passive funds. In a recent study, He and Huang (2017) show that the fraction of U.S. public firms that are cross-held has increased from below 10% in 1980 to about 60% in 2014.====In view of the increase in ownership concentration, empirical research has investigated whether the picture of “separation between ownership and control” depicted by Berle and Means (1932) is still relevant for the modern economy. Many influential papers have confirmed a ==== separation between ownership and control, even when the network of ownership is highly concentrated. This may occur because even sizeable cross-holdings are still too small to create real control (see Harford et al. (2011)). Additionally, legal restrictions and conflict of interests make it costly for companies to interfere on management decisions (Davis (2008)). On the other hand, there is some indirect evidence of collusion across companies with a large share of cross-holding, e.g., He and Huang (2017). We recognise that establishing a causal link between ownership and control is difficult as there could be many mechanisms at work. But it seems uncontroversial that, even if not complete, there is a fair amount of division between ownership and managerial control.====Motivated by these empirical results, we develop a model to study the implications of cross-ownership for firms' portfolio choice and welfare. A collection of firms is located in a network of cross-holding. We focus on cross-holding in the form of shares, but other instruments that channel the performances of one firm on other firms can be included. The network of cross-holding reflects the claims that each firm has on the value of other firms. There is full separation of who makes these claims (the shareholders) and the decision maker of a firm (the manager who has full control). The manager of a given firm is assumed to be risk averse and has the choice to invest in projects of different risks. For example, a manager can invest on outgoing projects to expand the current capacity or can finance new projects that are riskier. The investments by firms' managers and the network of cross-holding together define the distribution of returns. To simplify the analysis we assume that firm's decision makers have mean-variance preferences and that every firm can invest its endowment in a risk-free asset or in a distinct risky project.====We begin by deriving a summary measure that aggregates all direct and indirect claims induced by the cross-holding network: we refer to this as (the matrix of) ownership. Ownership keeps track of indirect claims of cross-holding and determines the set of final bilateral transfers. In Proposition 1, we characterise decentralised firms' risk-taking behaviour. In Proposition 2, we find the social optimum for risk-taking of firms, when constrained by a given network of cross-holding. These two results clarify who are the firms that take too much or too little risk relative to socially optimal investment and how this depends on their network location. We explain this next.====Portfolio choice in a cross-ownership network leads to two competing forces–one involving diversification and the other risk-shifting. Cross-ownership allows firms to diversify and invest in high return projects despite their risk. But cross-ownership also skews the incentives of how much risk firms take because those that make portfolio decisions do not bear all of the risk they take on. Indeed, a firm will absorb some of the risk taken by the firms it has shares in. How these two effects shape the firm's portfolio choice depends on the local structure of its network of ownership. Low self-ownership incentivises firms to take too much risk. Firms whose ownership is concentrated in the hand of a few others also take too much risk as the risk taken is shifted to a few neighbours. In contrast, firms with high self-ownership and whose ownership is dispersed take too little risk.====At the aggregate level, the cross-ownership network that maximizes aggregate utility is one where self-ownership is minimized and each firm's ownership is uniformly distributed across all other firms (Proposition 3). This is achieved in a complete and symmetric cross-holding network and, in this setting, it is analogous to a perfect insurance scheme. Yet, empirical research has documented highly concentrated cross-ownership networks in the modern economy. Complementary to the body of research discussed above, network scientists, working in the intersection between computer science, economics and finance, investigated the structure of cross-ownership networks within and across different countries. Glattfelder and Battiston (2009) mapped ownership networks focusing on the stock markets of 48 countries. In a subsequent work, Vitali et al. (2011) studied transnational corporations, including both listed and non-listed companies around the world. This analysis revealed that cross-ownership networks have a bow-tie structure, see Fig. 1 for a stylised example.====The bow-tie structure provides a taxonomy to group firms in three categories based on their local network of ownership. It also clarifies the nature of asymmetries in cross-ownership networks. Firms that have many shares of other firms in their portfolio but that do not raise equity by issuing their own shares belong to the in-section. Firms located in the core of the bow tie have shares of other core firms and of out-section firms, but they are also cross-held by in-section firms. The out-section firms do not cross-hold other firms, but raise capital by issuing shares that are mainly acquired by the core firms. Glattfelder and Battiston (2009) and Vitali et al. (2011) pointed out that the core section is tightly connected and firms in the out-section are highly exposed to the performance of firms in the core section. The bow-tie describes cross-ownership across transnational firms. However, it also resembles many properties of the ownership structure of traditional national corporations. National corporations are often organized in an almost tree-like structure with monotonous flow of value. For example, see La Porta et al. (1999), who mapped the ownership structures of the 20 largest publicly traded firms in each member of a list of 27 wealthy economies.====Based on this evidence, we study the portfolio choice in a bow-tie network. We parameterize the bow-tie structure and derive the corresponding matrix of ownership. We show that both core and out-section firms can over or under invest relative to the social optimum and we provide conditions for both cases (Proposition 4). We also show that deeper integration among core-section firms increases the welfare of core-firms, because it allows more diversification among them. However, it has a non-monotonic effect on the welfare of in-section firms. In fact, deeper integration can also trigger a reduction in aggregate welfare (Proposition 5). The negative effect of deeper integration in a bow-tie network occurs when there are only a few firms in the in-sector, and each holds major shares of core firms. In such a case, the integration of core firms is analogous to increasing diversifications across core firms, so the core firms take on a lot of risk. But this risk is shifted mainly to the few in-section firms, and this lowers their welfare substantially.====We also illustrate the role of diversification and concentration with the analysis of the Allianz AG cluster (reported in La Porta et al. (1999)). We start from the empirical cross-holding network and, based on that, derive the ownership structure. We observe that the centre of the cluster, Allianz AG, takes more risk than peripheral firms. We then perform a thought experiment in which the shares of Allianz AG are concentrated in a single hand, and show that this reduces social welfare.====We then develop further the analysis of social welfare. We derive an expression for the welfare of networks in which linkages are small (thin networks). In thin networks, welfare is well approximated by only the first two layers of investments. We show that being thin is a sufficient condition, independently of the structure of cross-holding, for integration to increase welfare (Proposition 6). Yet, also in thin networks the variance across investments reduces the welfare benefits that more integration creates.====Finally, we relax the assumption that the returns of projects are uncorrelated. We show existence of an equilibrium and provide sufficient conditions for uniqueness. In the case of weak positive correlation we find that correlation typically mitigates risk taking. More precisely, we show that the larger is the partial insurance that ==== provides to other firms, the larger is the moderating effect of positive weak correlation on firm ===='s risk-taking.====We build on two important strands of research. The first line of work is the research on cross-holdings and linkages.==== From this literature we borrow the formalization of cross-holding networks. The second strand is the literature on portfolio choice. In a complete market setting, any uncertainty on returns is washed out and only expected returns matter. However, when markets are incomplete, maybe because access is restricted, risk matters.==== This motivates a richer model of firm risk-taking choices. We build on a prominent strand of the literature that has used the portfolio model of Pyle (1971) and Hart and Jaffee (1974). Within this framework, firms are assumed to behave as competitive portfolio managers, taking prices and yields as given and choosing their portfolio (composition of their balance sheets and liabilities) in order to maximize the expected utility of the firms' financial net worth.====An important assumption of our model is that we take the cross-ownership structure, and therefore the exposure to the risk-taking behaviour of other firms, as given. Our analysis and result should be interpreted as the study of how exogenous regulations and rigidities in cross-holding affect the level of investment of a firm in a risky project that is not directly accessible to other firms. This is in line with a large literature that has focused on situations in which not all the elements of a firm's balance sheet can be chosen. In particular, Rochet (1992) reevaluates the work of Koehn and Santomero (1980) and Kim and Santomero (1988) in a model in which the firm equity capital is fixed, in the short run over which the model spans; this reflects the real distinction in the way equity capital can be altered in the short run relative to other securities.====In the recent work on contagion in financial networks, attention has focused on the role of the distribution of shocks and the architecture of networks in the case of bankruptcy.==== There are two distinguishing features of our work. First, the origin of the shocks – the investments in risky projects – is itself an object of individual decision making.==== Second, the economics system is not “damaged”, i.e., there is no risk of bankruptcy. Thus, the focus of our work is, first, on how the network of linkages shapes the level of risk-taking by agents and, second, on how it spreads the rewards of the risky choices across different parts of the system. Our work on the effects of integration and on optimal network design should be seen as complementary to the existing body of work.====Section 2 introduces the model. Section 3 presents our main results. Section 4 extends the main result to correlation across firms. Section 5 concludes. The proofs of the results are presented in the Appendix.",Cross-ownership and portfolio choice,https://www.sciencedirect.com/science/article/pii/S0022053121000119,13 January 2021,2021,Research Article,146.0
"Cui Zhiwei,Weidenholzer Simon","School of Economics and Management and MoE Key Laboratory of Complex System Analysis and Management Decision, Beihang University, Beijing 100191, PR China,Department of Economics, University of Essex, Wivenhoe Park, Colchester CO4 3SQ, United Kingdom","Received 22 January 2019, Revised 23 December 2020, Accepted 30 December 2020, Available online 12 January 2021, Version of Record 26 January 2021.",https://doi.org/10.1016/j.jet.2021.105187,Cited by (12),"We consider a model of social coordination and network formation where agents decide on an action in a coordination game and on whom to establish costly links to. We study the role of passive connections; these are links that other agents form to a given agent. Such passive connections may create an endogenously arising form of lock-in where agents don't switch actions and links, as this may result in a loss of payoff received through them. When agents are constrained in the number of links they form, the set of ==== includes action-heterogenous strategy profiles, where different agents choose different actions. Depending on the precise parameters of the model, risk-dominant, payoff-dominant, or action-heterogenous strategy profiles are stochastically stable.","We propose a novel explanation for why we sometimes observe multiple technology standards being adopted at the same time. This explanation does not require any sort of heterogeneity among agents but instead is based on the idea that agents may become locked into their action choices when the interaction structure among them is determined by a process of network formation. To solidify ideas, consider an agent deciding on which kind of technology standard to adopt. Typically, this agent is better off if she interacts with somebody using the same technology standard, thus giving rise to a coordination game. In addition to her action chosen, her payoff depends on the choices of her interaction partners. These interaction partners can be distinguished in two groups, those she actively chooses to interact with and those who actively choose to interact with her, i.e. those who she passively interacts with. While agents have a say over the composition of the former group, they typically have much less control over who belongs to the latter.====The relative importance of benefits received through passive interaction depends on the context of the interaction among agents.==== For instance, consider a set of agents who can decide whether to adopt a VHS recorder or use the Betamax standard. Forming a link in this context represents borrowing a video cassette from another agent. While this act carries positive payoff to the borrower (the active side of the interaction) there is little or no benefit to the lender (the passive side of the interaction). In other circumstances there are, however, clear benefits for the passive side of an interaction. For instance, consider software developers who can choose among multiple programming languages (e.g. Python and Java). In this context, forming a link indicates initiating a collaboration. Clearly, both parties benefit, regardless of who initiated the link, and the benefits are higher if both use the same programming language.====We study the role of payoff received through such passive connections in a model of social coordination and network formation similar to the ones presented in Goyal and Vega-Redondo (2005) and Staudigl and Weidenholzer (2014). In contrast to previous work, action-heterogenous network configurations, where different actions are adopted by different agents, are Nash equilibria and sometimes may even be stochastically stable. More specifically, in our model there is a population of agents who decide on an action in a ==== coordination game and who choose their active interaction partners via establishing costly links to them.==== The coordination game captures a conflict between efficiency- and risk- considerations, encapsulated by one equilibrium being payoff dominant and the other being risk dominant. In line with Staudigl and Weidenholzer (2014) and in contrast to Goyal and Vega-Redondo (2005), we focus on a scenario where agents are constrained in the number of links they may form, thereby reflecting technological constraints in link formation or decreasing marginal benefits from socializing. For instance, in software development this is the case if the number of feasible collaborations is small compared to the pool of potential collaborators. Unlike Staudigl and Weidenholzer (2014) and in line with Goyal and Vega-Redondo (2005) agents also receive payoff from passive connections.====We argue that under constrained interactions the payoff received from passive connections may create an endogenous form of lock-in. To fix ideas, consider an agent's optimal choice in a population where both actions are used by other agents. If this agent has no passive connections (or there is no payoff earned from these as in Staudigl and Weidenholzer (2014)), she will choose the payoff dominant action and exclusively link up to agents choosing it (as the payoff dominant action gives the highest possible per interaction payoff). In contract, if this agent has passive connections from agents choosing the risk dominant action, then it may be optimal to choose the risk dominant action too and link up to other agents choosing it. The reason for this is that the payoff dominant action could result in a lower payoff received through passive connections. In this sense passive connections may endogenously create a situation where agents become locked into choosing the risk dominant action. When interactions are constrained, the resulting network may feature separated network components. Hence, agents may face different distributions of actions among their neighbors and consequently find it optimal to choose different actions. Thus, under passive payoffs and constrained interactions action-heterogenous profiles may be Nash equilibria (see Proposition 1). In contrast, this cannot happen if interactions are either unconstrained and payoff is earned from passive connections, as in Goyal and Vega-Redondo (2005), or if there is no payoff from passive connections (regardless whether interactions are constrained or not) as in Staudigl and Weidenholzer (2014). In the former case, the network will be fully connected. In the latter case, the actions of passive connections do not enter into payoffs. In both cases, all agents face the same distribution of actions among their (potential) interaction partners and necessarily have to choose the same action. It is thus the combination of constrained interactions ==== passive payoffs that leads to the emergence of action-heterogenous profiles as Nash equilibria.====We proceed by considering a myopic best response process in discrete time where a revising agent chooses links and actions so to maximize the payoff from the previous period. We characterize the absorbing sets of this dynamic process and, in doing so, provide a refinement of the set of Nash equilibria. In addition to profiles where all agents choose the same action, certain action-heterogenous profiles turn out to be absorbing (see Proposition 2). Interestingly, in action-heterogenous profiles the subnetwork among agents choosing the risk dominant action has to be fully connected, putting an upper bound on the number of agents choosing the risk dominant action. This is because all risk dominant agents need to receive sufficiently many passive links to be locked in.====Finally, we provide a discussion on the impact of passive connections on the long run outcome of our model. To this end, we consider a perturbed version of the process where agents with small probability make mistakes and choose actions and links different to the ones specified by the myopic best response. Following Kandori et al. (1993) and Young (1993) we identify stochastically stable states by assessing the relative robustness of the absorbing states to mistakes. The combination of passive payoffs and constrained interactions has important consequences for the dynamics of the model and for the transition among the various absorbing sets. To appreciate our results it is again useful to consider the models of Goyal and Vega-Redondo (2005) and Staudigl and Weidenholzer (2014) as a benchmark. When interactions are unconstrained the network will be fully connected regardless of whether there is payoff from passive interactions or not. Thus, as in Kandori et al. (1993), the question which convention will be selected comes down to a comparison of the size of the basin of attraction of the two actions; a comparison won by the risk dominant action. When interactions are constrained and there is no payoff from passive connections, payoff dominant profiles are stochastically stable: whenever there is a small cluster of agents choosing the payoff dominant action, agents want to choose the payoff dominant action and link up to agents using it. As some agents may not have any incoming links, this mechanism is also present in our paper. However, there is a further force at play. Starting from profiles where everybody chooses the payoff dominant action, risk dominant actions are able to spread contagiously through parts of the network. This is similar to the spread of risk dominant actions in fixed interactions structures (see Ellison, 1993, Ellison, 2000 and Morris (2000)).==== The crucial difference is that in the present context the interaction structure among agents arises endogenously. Moreover, the network evolves at the same time as agents adjust their actions. This constitutes another difference to the fixed interaction case and has important consequences for the number of agents who change their action as a result of the initial mistake; effectively putting an upper bound on the size of a subnetwork through which a risk dominant action may spread. In this sense, in the present setting the network plays a crucial role for the propagation of actions and evolves as agents adjust their links. In contrast the transition among various absorbing states in previous work are rather mechanic with no functional role for the network. In both Goyal and Vega-Redondo (2005) and Staudigl and Weidenholzer (2014) first a certain fraction of the population switches and then everybody follows suit.====Our main results are obtained by determining the relative importance of these dynamic forces for the transition among the various absorbing sets. We identify parameter ranges for which payoff dominant-, risk dominant-, and action heterogenous- are (uniquely) stochastically stable. The exact prediction depends on the size of the population, the number of links agents may support, the basin of attraction of the risk dominant action, and on the degree of payoff dominance. In a nutshell, whenever the payoff dominant action earns a sufficiently high payoff when played against itself, then network configurations where everybody chooses it are stochastically stable (Proposition 3). If the basin of attraction of the risk dominant action is sufficiently large, either risk dominant- or action-heterogenous- network configurations are stochastically stable (Proposition 4). In this case, action-heterogenous network configurations are uniquely stochastically stable if the population is large enough. This is because the transition from action-heterogenous network- to risk dominant network- configurations becomes more difficult in large populations. We further demonstrate (by means of Example 2) that sometimes even action-heterogenous networks where there are links between agents choosing different actions are stochastically stable.====Our results are in stark contrast to previous work where universal coordination on one convention always occurs and may offer an explanation why sometimes multiple technology standards are observed at the same time. Interestingly, this happens in a world where agents are ex-ante homogenous (so that in principle there shouldn't be a reason to expect action-heterogenous outcomes) and is driven by the endogenously formed interaction structure among agents. Our results draw on several crucial factors. Firstly, interactions are constrained so that agents only interact with a small subset of the overall population. Such constrained interactions may arise endogenously when linking costs are convex or benefits of interaction are concave in the number of links (as e.g. in the extensions discussed in Jackson and Watts (2002) or Staudigl and Weidenholzer (2014)) or simply be the result of technical restrictions in the linking technology (as is sometimes the case on online platforms such as facebook or Twitter). There is also ample of empirical evidence (see e.g. chapter 3 in Jackson (2008)) suggesting that usually the number of links agents form is small relative to the overall population. In this light, constrained interactions often seem to be a better description of agents' linking choices. Secondly, the emergence of action-heterogenous network configurations rests upon a conflict between payoff- and risk- dominance. Payoff dominance means that one technology is inherently better than the other (when used against itself). Risk dominance means that a technology offers a payoff advantage when used against a mixed group of interaction partners. As such, risk dominance is (in addition to its inherent properties) induced by the benefits received by users interacting with users using different standards as captured by the notions of inward- and outward compatibility.==== Lastly, there have to be benefits from passive connection. As argued above, this will depend on the nature of the interacting under consideration.",Lock-in through passive connections,https://www.sciencedirect.com/science/article/pii/S0022053121000041,12 January 2021,2021,Research Article,147.0
"Chung Kim-Sau,Liu Erica Meixiazi,Lo Melody","Hong Kong Baptist University, Hong Kong,Duke University, United States of America","Received 5 September 2020, Accepted 1 January 2021, Available online 8 January 2021, Version of Record 14 January 2021.",https://doi.org/10.1016/j.jet.2021.105186,Cited by (0),"This paper studies how sellers behave when their consumers have difficulty in detecting small differences. These consumers pose a problem because, even if a good deal exists, they cannot appreciate it if it is barely better than their outside options. This creates a role for a second deal, either marketed by the same seller or by another seller, even when consumers are homogeneous in their tastes. If the same seller markets the second deal, it will strategically position the first as a good deal, and the second as a bad deal, and use the bad deal to help consumers appreciate the good deal. If another seller markets the second deal, the two sellers will become specialized as, respectively, good-deal and bad-deal providers. Both sellers free-ride each other. The good-deal provider is happy because the bad deal helps consumers appreciate its good deal; while the bad-deal provider hides behind the presence of the good deal and manages to make a sale some of the time.","Adding a slightly worse option can sometimes help decision makers compare the top two contenders, while adding a much worse one cannot. Ketcham et al. (2015) use Medicare Part D data to study what nudges people to switch their Medicare prescription drug plans. They find that adding plans that are significantly worse does not help the switch, while adding plans that are slightly worse does.====To understand why this may happen, consider the following illustrative example. Suppose you are to choose between two cups of coffee, A and B. You'd prefer the one with less sugar, but they taste more or less the same, so without further information you can only randomly pick one. You may or may not end up having the one with more sugar. Now comes a third cup of coffee, C, which is discernably sweeter than both A and B. This third cup of coffee apparently does not provide any help for your decision between A and B. Then comes a fourth cup, D, which is sweeter, but not much so. Specifically, it is discernably sweeter than A, but tastes more or less the same as B. With the help of D, you can now infer that B must be somewhere between A and D in terms of sugar content, and that A is the one with the least amount of sugar. Happily, you choose A over B.====Adding a slightly worse option helps because, when decision makers have difficulty in detecting small differences, they will struggle to compare similar alternatives. These difficulties arise more often when the values of the given options are themselves difficult to calculate,==== when alternatives differ in multiple dimensions,==== or when the decision maker is cognitively overloaded. The consequences of these difficulties loom larger when the decisions are irreversible, either because a product cannot be returned once purchased, or because the value cannot be known until (long) after the product is consumed.====In this paper, we explore how sellers behave when their consumers have difficulty detecting small differences between options. These consumers pose a problem because, even if a good deal exists, they cannot appreciate it if it is barely better than the alternatives. If there is a monopolist who can market only one deal, its profit necessarily decreases the more consumers suffer from this difficulty (Proposition 2). This creates a role for a second deal, either marketed by the same monopolist or by another seller, even when consumers are homogeneous in their tastes.====If the same monopolist markets the second deal, it will strategically position the first as a good deal and the second as a bad deal, and use the bad deal to help consumers appreciate the good deal (Proposition 6). If this monopolist can market many deals, all of them but one will be positioned as bad deals, and mainly serve the purpose of helping consumers appreciate the (single) good deal (Proposition 7). If another seller markets the second deal, the two sellers will become specialized as, respectively, good-deal and bad-deal providers. Both sellers free-ride each other. The good-deal provider is happy that the bad deal helps consumers appreciate its good deal; while the bad-deal provider hides behind the presence of the good deal and manages to make a sale some of the time (Proposition 8).====In the context of Medicare prescription drug plans, Abaluck and Gruber (2011) find that people sometimes choose inferior plans. However, Ketcham et al. (2016) find that “the odds of choosing an inferior plan decline between 2006 and 2010 despite increasing availability of inferior plans” (p. 3934). These findings are not be surprising in light of our results. Suppose consumers are heterogeneous in their difficulty in detecting small differences. Then a bad deal can help some consumers, but not all, appreciate a good deal. Some consumers still erroneously purchase bad deals. However, when there are more and more bad deals, each helping a different group of consumers appreciate the good deal, the odds of purchasing the good deal will increase.====In order to model consumers' difficulty in detecting small differences, we follow an approach inspired by the decision-theoretic literature on intransitive indifference.==== Specifically, each consumer has a ==== (jnd). If two alternatives offer utilities that differ by less than this jnd, the consumer will not be able to tell them apart.==== Another popular approach often followed in the literature is called ====, where a decision maker partitions alternatives into a few categories and cannot tell apart different alternatives within the same category.==== This approach differs from ours in that, when two alternatives fall on opposite sides of a partition line, the decision maker will be able to tell them apart no matter how close they are to each other.====The consumers' difficulty in detecting small differences is one reason why they may not be responsive to competing offers. Other reasons for this lack of responsiveness that have been studied in the behavioral industrial organization literature include inattention (de Clippel et al. (2014)) and framing (Piccione and Spiegler (2012)). A recurring theme in this literature is that, when multiple sellers compete, consumers' surplus decreases with their ability to make comparisons. This phenomenon arises in our setting as well (Proposition 9). It will be interesting to investigate whether some common economic forces are at work. We leave this inquiry for future research.====Consumers' difficulty in detecting small differences also partly explains why the introduction of a third option affects their preference between the first two—this is a phenomenon generally known as the ====. Two prominent special cases of the context effect are the attraction effect (Huber et al. (1982)) and the compromise effect (Simonson (1989)). The attraction effect arises when the appeal of ==== vis-à-vis ==== increases in the presence of a third option ==== that is dominated in every dimension by ==== but not by ====. Ok et al. (2015) axiomatizes a reference-dependent choice model that can explain the attraction effect. Our consumer, however, cannot be modeled as a reference-dependent decision maker, because his behavior violates Ok et al.'s (2015) No-Cycle Condition.==== The compromise effect arises when an option that is mediocre in every dimension is chosen over any other option that is the best in one dimension but the worst in another. de Clippel and Eliaz (2012) axiomatizes a reason-based choice model that can explain both the attraction and the compromise effects. Our consumer, however, cannot be modeled as a reason-based decision maker, because his behavior violates de Clippel and Eliaz's (2012) Existence-of-a-Compromise Condition.====Another explanation of the context effect is that the very act of introducing the third alternative is endogenous, and hence carries extra information (Kamenica (2008)). Kamenica's (2008) explanation can be readily separated from ours by a laboratory experiment where the third alternative is introduced by a robot instead of a strategic player. If a subject's ranking between the first two alternatives is still affected, the experimental result would lend support to our explanation.====Our result that a monopolist would market different deals with different values resembles Salop's (1977) classical result that a monopolist may permit a non-degenerate distribution of price for the same product. However, the mechanisms are different. In Salop (1977), consumers differ in their price elasticities, and the monopolist would like to charge those who have lower price elasticities a higher price. If consumers who have lower price elasticities also have higher search costs, then the monopolist can screen for these consumers by permitting a non-degenerate price distribution, because consumers who search less will on average pay a higher price. In our model, consumers' difficulty in detecting small differences is not correlated with any other characteristic, and hence screening does not play a role in our result.====Our model is also similar to Rubinstein (1993) in that different types of consumers differ neither in their preferences nor in the information they possess, but rather in their ability to process information. Like Salop (1977), Rubinstein's (1993) monopolist also permits a non-degenerate price distribution. However, in terms of the underlying mechanism, Rubinstein (1993) is closer to Salop (1977) than to us. In particular, Rubinstein (1993) shows that, when consumers differ in their ability to process information, with less able consumers also being more costly to serve, the monopolist can screen out these costly-to-serve consumers by offering a non-degenerate price distribution.====We found that, in the case of multiple sellers, different sellers offer different deals with different values. This result resembles that of Salop and Stiglitz (1977), Varian (1980), and Rob (1985). However, our finding that both the good-deal and the bad-deal providers free-ride each other does not have a counterpart in these studies.====Finally, our paper is also related to Natenzon (2019), who looks at an environment where the choice between two deals may be affected by the presence of a third one. We differ in two aspects. First, while we focus on sellers' strategic choices over what alternatives to offer, Natenzon (2019) assume an exogenously fixed distribution of these alternatives. Second, in Natenzon (2019), the consumer finds some pair of alternatives easier to compare than other pairs. Such asymmetry is absent in our paper, where every pair is ==== similar, and contextual inference comes solely from the utility gap.",Selling to consumers who cannot detect small differences,https://www.sciencedirect.com/science/article/pii/S002205312100003X,8 January 2021,2021,Research Article,148.0
"Stachurski John,Zhang Junnan","Research School of Economics, Australian National University, Australia","Received 4 May 2020, Revised 30 December 2020, Accepted 3 January 2021, Available online 8 January 2021, Version of Record 14 January 2021.",https://doi.org/10.1016/j.jet.2021.105190,Cited by (4),This paper extends the core results of discrete time infinite horizon ==== to the case of state-dependent discounting. We obtain a condition on the discount factor process under which all of the standard optimality results can be recovered. We also show that the condition cannot be significantly weakened. Our framework is general enough to handle complications such as recursive preferences and unbounded rewards. Economic and financial applications are discussed.,"Researchers in economics and finance routinely adopt settings where the subjective discount rate used by agents in their models varies with the state. For example, Albuquerque et al. (2016) study an asset pricing model in which the discount rate is perturbed by an AR(1) process. They show that the resulting demand shocks help explain the equity premium puzzle. Mehra and Sah (2002) find that small fluctuations in agents' discount factors can have large effects on equity price volatility. Schorfheide et al. (2018) and Gomez-Cram and Yaron (2020) likewise embed state-dependent discount factors into Epstein–Zin preferences to generate realistic asset prices and returns.====State-dependent and time-varying discount rates are also common in studies of savings, income and wealth. An early example is Krusell and Smith (1998). In related work, Krusell et al. (2009) model the discount process as a three state Markov chain and show how discount factor dispersion helps their heterogeneous agent model match the wealth distribution. Fagereng et al. (2019) use time-varying discount rates and portfolio adjustment frictions to explain the positive correlation between savings rates and wealth observed in Norwegian panel data. Hubmer et al. (2020) model discount dynamics using a discretized AR(1) process.====State-dependent discounting is also found in analyses of fiscal and monetary policy. For example, Eggertsson and Woodford (2003) study monetary policy in the presence of zero lower bound restrictions with dynamic time preference shocks. Woodford (2011) considers the government expenditure multiplier in a similar environment. Eggertsson (2011) and Christiano et al. (2011) study the effect of fiscal policies at the zero lower bound on interest rates, while Nakata and Tanaka (2020) analyze the term structure of interest rates at the zero lower bound when agents have recursive preferences. In all of these models, state-dependent variation in discount rates plays a significant role.====In addition, state-dependent discounting is often used in studies of macroeconomic volatility. For example, Primiceri et al. (2006) argue that shocks to agents' rates of intertemporal substitution are a key source of macroeconomic fluctuations. Justiniano and Primiceri (2008) study the shifts in the volatility of macroeconomic variables in the US and find that a large portion of consumption volatility can be attributable to the variance in discount factors. Additional research in a similar vein can be found in Justiniano et al. (2010), Justiniano et al. (2011), Christiano et al. (2014), Saijo (2017), and Bhandari et al. (2013).====The standard theory of dynamic programming over infinite horizons (see, e.g., Blackwell (1965), Stokey et al. (1989), or Bertsekas (2017)) does not accommodate state-dependent discounting. Instead, it assumes either zero discounting (and considers long-run average optimality) or a constant and positive discount rate, which corresponds to a discount factor strictly less than one. This implies that, in the canonical setting, the Bellman operator satisfies the conditions of Banach's contraction mapping theorem, which in turn provides the foundations for the standard optimality theory.====We reconsider the standard theory when the constant discount factor ==== is replaced by a discount process ====, so that time ==== payoff ==== is discounted to present value as ==== rather than ====. Here ==== is the initial condition of an exogenous Markov state process that drives evolution of the discount factor. We replace the traditional condition ==== with a weaker “eventual discounting” condition: existence of a ==== such that ====. For a finite irreducible state process, this is equivalent to existence of a ==== such that ====, where ==== is the unconditional expectation.====We show that, when eventual discounting holds, (i) the value function satisfies the Bellman equation, (ii) an optimal policy exists, (iii) Bellman's principle of optimality holds, and (iv) value function iteration and Howard policy iteration (Howard, 1960) are both convergent. When ==== is constant at ====, eventual discounting holds at ====, so these results capture the standard theory as a special case.====Our conditions do not rule out ==== with positive probability. One example of why this matters is provided by the New Keynesian literature, where the discount factor is often allowed to temporarily attain or exceed unity, so that the zero lower bound on the nominal interest rates binds. For example, Christiano et al. (2011) admit a shock where ==== in their study of the government spending multiplier. Similarly, Hills et al. (2019) analyze tail risk associated with the effective lower bound on the policy rate in a model where the discount process is a constant multiple of a discretized AR(1) process that regularly generates value of ==== exceeding unity. Fig. 1 illustrates by showing a simulated time path of ==== using their parameters.====We discuss the eventual discounting condition at length in the paper, giving several equivalent conditions. One of these involves a bound on the spectral radius of a discounting operator. This connects our work to a strand of literature in finance that studies the long-term factorization of stochastic discount factors using eigenfunctions of valuation operators (see, e.g., Hansen and Scheinkman (2009), Hansen and Scheinkman (2012), and Qin and Linetsky (2017)). Drawing on these ideas, Borovička and Stachurski (2020) and Christensen (2020) connect the spectral radius of valuation operators with existence and uniqueness of recursively defined utilities. However, neither of these papers provides results on optimality or dynamic programming.====To handle unbounded rewards, we extend two approaches that have been developed previously for the case of constant discounting. The first one treats homogeneous programs in the spirit of Alvarez and Stokey (1998) and Stokey et al. (1989, Section 9.3). The second uses a local contraction method pioneered in Rincón-Zapatero and Rodríguez-Palmero (2003) and further developed by Martins-da Rocha and Vailakis (2010) and Matkowski and Nowak (2011). In each case, we show how the eventual discounting condition can be adapted to handle these extensions.====In addition, we study dynamic programming with Epstein–Zin utilities, where rewards are unbounded above and the Bellman operator is not a contraction in the short or long run under standard metrics. To solve the problem, we extend earlier work by Marinacci and Montrucchio (2010), Bloise and Vailakis (2018), and Becker and Rincón-Zapatero (2018), which exploits the monotonicity and concavity of the aggregator, to allow for state-dependent discounting. We show that, in the case of Epstein–Zin utility, the eventual discounting condition must be adapted to compensate for the role played by elasticity of intertemporal substitution.====Other papers have analyzed dynamic programming problems where discount rates can vary. For example, Karni and Zilcha (2000) study the saving behavior of agents with random discount factors in a steady-state competitive equilibrium. Cao (2020) proves the existence of sequential and recursive competitive equilibria in incomplete markets with aggregate shocks in which agents also have state-dependent discount factors. In the mathematical literature, various issues in dynamic programming with state-dependent discounting have been studied; see, for example, Jasso-Fuentes et al. (2020) and the references therein.==== However, these papers assume that the discount process in the dynamic program is bounded above by one or by some constant less than one.==== This is too strict for many applications, as discussed above.====Our work is related to Toda (2019), who investigates an income fluctuation problem in which the agent has CRRA utility. He obtains a necessary and sufficient condition for the existence of a solution to the optimal saving problem with state-dependent discount factors. Ma et al. (2020) relax the CRRA restriction by constructing optimality results via a consumption policy operator. Their results are specialized to optimal savings with additively separable rewards and do not apply to problems that involve discrete choices, endogenous labor supply, durable goods, or other common features. In contrast, the theory below is developed in a general dynamic programming setting, where the state spaces are arbitrary metric spaces.====In addition, the consumption policy operator, around which the theory in Toda (2019) and Ma et al. (2020) is constructed, is defined from the Euler equation, which characterizes the solution in their setting. However, many recent applications of state dependent discounting use recursive preferences (see, e.g., Albuquerque et al. (2016), Basu and Bundick (2017), Schorfheide et al. (2018), Nakata and Tanaka (2020), or de Groot et al. (2020)), implying that the Euler equation contains the value function and the consumption policy operator methods break down. Our theory extends to recursive preferences and illuminates the role of elasticity of intertemporal substitution on eventual discounting.====The rest of this paper is structured as follows. Section 2 sets out the model and provides our main results. Section 3 gives applications. Section 4 reviews our key assumption. Sections 5 and 6 treat extensions. Section 7 concludes.",Dynamic programming with state-dependent discounting,https://www.sciencedirect.com/science/article/pii/S0022053121000077,8 January 2021,2021,Research Article,149.0
"Curatola Giuliano,Faia Ester","University of Siena, Italy,Goethe University Frankfurt, Germany,CEPR, United Kingdom of Great Britain and Northern Ireland","Received 16 January 2019, Revised 30 October 2020, Accepted 29 December 2020, Available online 5 January 2021, Version of Record 11 January 2021.",https://doi.org/10.1016/j.jet.2020.105175,Cited by (2),"Financial crises are anticipated by leverage build-up and asset price booms and followed by sharp de-leveraging and asset price burst. Leverage pro-cyclicality, debt margins counter-cyclicality and heightened asset price volatility are often hard to reconcile with credit frictions models, with and without occasionally binding constraints. We show that a model in which the anticipatory effects of occasionally binding collateral constraints interact with borrowers' time-varying risk-attitudes (modeled through gain-loss reference dependent utilities) and with borrowers/lenders risk-attitudes heterogeneity can explain those facts. Simulations through global methods show that the model can also match numerous statistics characterizing the asset price and leverage cycles.","Leverage cycles are characterized by pro-cyclicality of asset prices and leverage, on top and beyond that of credit, and countercyclically of margins.==== This is even more evident around financial crises which are usually anticipated by a boom with leverage build-up and asset price growth and are followed by sharp de-leveraging and asset price bursts. Models of credit frictions, with and without occasionally binding collateral constraints, generate credit pro-cyclicality, counter-cyclical leverage and generally a-cyclical margins. They cannot match asset price business cycle statistics in absence of exogenously fed risk shocks.==== In this paper we show that leverage and asset price pro-cyclicality can be obtained by the interaction of anticipatory effects of occasionally binding collateral constraint and a borrower's counter-cyclical stochastic discount factor (SDF hereafter), which captures his willingness to bear risks. We obtain the latter with S-shaped reference-dependent preferences. Those preferences, featuring increasing willingness to bear risks in booms and loss aversion in recessions, are particularly well suited to capture borrowers' behavior around 2007, who were levering up and taking excessive risk prior to the crisis and sharply de-leveraging after that. The interaction between those preferences and occasionally binding constraints works as follows. In booms borrowers become increasingly risk-tolerant, hence their willingness to bear risk raises and they assign higher values to future collateral. This in turn relaxes the collateral constraint (beyond the level justified by the economic boom), leverage raises and the margin or down-payment falls.==== The opposite happens after negative shocks. The shadow price of debt or the Lagrange multiplier on the collateral constraint co-moves counter-cyclically with the down-payment. The shadow price of debt in equilibrium is given by the difference between lenders and borrowers SDFs, hence it proxies the premium that borrowers transfer to lenders for bearing the risk of future fluctuations in collateral. With S-shaped preferences the difference in SDFs is time-varying and counter-cyclical.==== Since in booms borrowers are increasingly more risk-tolerant relatively to lenders, their asset valuation raises and lowers the shadow price of debt. Besides accounting for the joint pro-cyclicality of asset price and leverage cycles, the non-linearities arising in our model from the combination of time-varying risk-attitudes, kinked utilities and occasionally binding collateral constraints account well for equity premia and Sharpe ratios and for the heightened asset price volatility.====Specifically we build a model with borrowers facing occasionally binding collateral constraints==== and exhibiting gain-loss and reference-dependent preferences,==== with an endogenous supply of debt and with time-varying differences in risk-attitudes between borrowers and lenders. We uncover the economic channels described above both through analytical derivations and numerical simulations. We show the main intuition analytically in a simple binomial model. For the infinite horizon economy we derive the asset price, equity premia, the Sharpe ratio and the equilibrium level of leverage==== and show how our model elements can rationalize the cyclical facts about asset price and leverage described above.====To validate our model empirically and against other alternative models, we simulate it with a global method, based on policy function iteration.==== First, through an event study analysis we show that the model can generate endogenously the leverage/de-leverage cycle and asset price boom/burst around a crisis, even in face of standard shocks. Second, we compute moments for up to 18 different variables characterizing the asset price and the leverage cycles and some consumption moments. We compare them first to the data equivalent for two countries, namely the US and the UK, and second to the moments generated by four alternative models (with only collateral constraints, with only S-shaped preferences, with high risk-aversion under standard or S-shaped preferences). In sum our model can match quantitatively the leverage pro-cyclicality and the shadow price counter-cyclicality for the intuition explained above. It can explain the equity premium, its volatility and the asset price volatility due to the model non-linearities.==== It can also explain well the empirical Sharpe ratio,==== which is typically hard to match, since the increasing risk-sensitivity of the borrower toward the lower tail raises the required premia in face of upcoming downfalls. Our model performs better in data matching than the alternative models, to which we compare ours.====The rest of the paper is divided as follows. Section 2 discusses related literature. Section 3 describes the model. Section 4 describes the channels through analytical derivations. Section 5 describes the simulation method, the calibration and presents numerical results. Section 6 concludes. Appendices follow.",Divergent risk-attitudes and endogenous collateral constraints,https://www.sciencedirect.com/science/article/pii/S002205312030168X,5 January 2021,2021,Research Article,150.0
"Au Pak Hung,Chen Bin R.","Department of Economics, Hong Kong University of Science and Technology, China,School of Economics, Huazhong University of Science and Technology, China","Received 2 April 2019, Revised 10 December 2020, Accepted 14 December 2020, Available online 4 January 2021, Version of Record 11 January 2021.",https://doi.org/10.1016/j.jet.2020.105172,Cited by (0),Evidence for positive ,"Team production and team-based compensation schemes are pervasive in the workplace. According to evidence cited by Bandiera et al. (2013), 52% of US firms use teamwork, while in 47% of British establishments, more than 90% of the workforce is organized in teams.==== Given the widespread use of teams, it is important for both firm management and economists to understand how teams should be structured and organized for production efficiency. In this paper, we study when and how a firm endowed with a pool of agents with different levels of ability should assign them into teams.====The existing literature on the economics of teams has extensively studied the benefits and costs associated with their uses. It is well-known that team production can be more efficient than individual production due to positive peer effects. The possible sources of these peer effects include ==== (Marschak and Radner, 1972; Alchian and Demsetz, 1972), ==== (Jackson and Bruegmann, 2009; De Grip and Sauermann, 2012) and ==== (Mas and Moretti, 2009; Falk and Ichino, 2006). On the other hand, the use of teams is often plagued with ==== among members (Alchian and Demsetz, 1972; Holmstrom, 1982). These considerations are significant in determining not only when teams should be used, but also how they should be structured if they were to be used. First, the implication of externalities in production technologies on the optimal team structure is straightforward: if team members' abilities are complementary (substitutable) in production, then positive (negative) assortative matching is optimal (see, for example, Koopmans and Beckmann, 1957). Second, if knowledge transfer is expected to be important, firms should match trained workers with untrained workers, or match junior workers with senior workers to increase their interaction (Cornelissen, 2016). Third, the free-riding incentives in teams can be mitigated by matching an agent who demands a large bonus with another agent who can only contribute little to the likelihood of team success. Putting these observations together, the design of team structure involves a non-trivial trade-off between exploiting the technological externalities and minimizing the cost of incentive provision.====While peer pressure has been noted as an important source of peer effects both in the theoretical and empirical literature, its effects on how teams should be organized have not been investigated. In this paper, we intend to fill this gap by showing that peer monitoring and sanction is another channel through which team composition affects the incentive costs. The seminal article by Che and Yoo (2001) provides a solid game-theoretical foundation for the mechanism of peer monitoring and sanctions, and illustrates how the principal could exploit this mechanism to mitigate free-riding incentives and save agency costs. The idea is as follows. As agents work closely with others, they have more precise observations about their peers' effort than the principal. If the principal adopts a team-based compensation scheme that positively links the agents' pay to their peers' successes, it is a credible threat for an agent that if any shirking is detected by his/her peers, the whole team would engage in mutual punishment by shirking in the future. The use of dynamic incentives mitigates the agents' moral hazard problem and helps the principal save agency costs. In our analysis, we study how the effectiveness of this peer monitoring mechanism depends on the team composition.====In our baseline model in Section 2, we abstract away from any technological externality and free-riding among team members to focus on the channel of peer monitoring in determining the optimal team structures. To this end, we assume that each agent's production is technologically independent, and each agent's production outcome is individually verifiable. The principal has access to a pool of agents with (potentially) different abilities, and has to decide their contracts, as well as their assignment into teams of two agents. We find that to exploit the peer monitoring mechanism most effectively, the principal should match an agent who can impose a ==== on his partner with another agent whose inherent ====. As the former agent is usually of high ability and the latter agent is of low ability (as is the case in which ability and effort are complements), negative assortative matching would minimize the overall incentive costs. Therefore, the principal could benefit from maximizing the diversity of members in a team by grouping high-ability agents with low-ability agents. This finding echoes the policy recommendation of Mas and Moretti (2009) that “the optimal mix of workers in a given shift is the one that maximizes skill diversity” in settings in which workers repeatedly interact.====Our baseline model is purposefully simplified to illustrate transparently the implications of peer monitoring on the optimal team structures. It has abstracted away from a number of realistic considerations. First, precise individual performance measures are often difficult to come by in a team. Second, when only a team signal is available, the inference of individual contribution could depend on team composition. Third, the firm can often design how production is organized, and assign workers into different positions or tasks. Section 3 studies an extended model that incorporates these considerations. We begin with analyzing the team formation problem in a setting in which production is undertaken in teams of two agents, and only the joint output of each team is contractible. The optimal team assignment is the outcome of the tradeoff between the regulation of the free-rider problem, effective signal inference, and exploitation of the peer monitoring mechanism. We find that while the free-rider problem is attenuated by positive sorting, signal inference and peer sanctions are more effective with negative sorting. Moreover, a high degree of correlation between the agents' production is found to intensify the impact of negative sorting on signal inference, and a high discount factor accentuates the threat of peers' sanctions. Negative sorting is therefore optimal if the team production technology exhibits strong correlation and/or the common discount factor is sufficiently high.====The insight obtained by studying team assignment can shed light on the job design problem. We are interested in the following question: given a pair of agents, would the principal find it more profitable to put them together in a team, or to assign them to individual production? On the one hand, individual production eliminates free riding in a team; on the other hand, its adoption is analogous to positive sorting (as each agent is teamed up with his own self). Building on the insight from the team assignment problem discussed above, team production is therefore superior to individual production if the benefit of negative sorting in team formation is sufficiently significant. This observation suggests two conditions that favor teams. One is a large discount factor, which makes the mechanism of peer monitoring and sanctions highly effective. Another condition is a large difference in the ability levels of the agents, which heightens the impact of sorting on the total agency cost. These conditions are in interesting contrast to those in Che and Yoo (2001), which concern the production technology. In a similar setting featuring homogeneous agents, they find that teams dominate individual production only if the team technology exhibits either team synergy or mutual sabotage.==== In contrast, our setting allows for heterogeneous agents working with production technologies that feature neither team synergy nor sabotage. Our analysis thus contributes to the understanding on job design by highlighting the importance of workforce characteristics and composition, and providing a simple theoretical explanation for why diversity in the workforce can favor the adoption of teams.====This paper is organized as follows. A literature review is given below. In Section 2, we set up a baseline model, in which each agent works on technologically independent tasks, and solves the optimal contract and team assignment. Section 3 studies the optimal job design, and how agents should be sorted into teams when team production is adopted. The connection between our theory and the empirical evidence is discussed in Section 4. Section 5 conducts several robustness checks, including analysis of alternative models in which effort choice by the agents is continuous or a competitive labor market is introduced. Section 6 concludes. All proofs are relegated to Appendix A.",Matching with peer monitoring,https://www.sciencedirect.com/science/article/pii/S0022053120301654,4 January 2021,2021,Research Article,151.0
"Lackner Martin,Skowron Piotr","TU Wien, Vienna, Austria,University of Warsaw, Warsaw, Poland","Received 23 October 2019, Revised 6 November 2020, Accepted 16 December 2020, Available online 29 December 2020, Version of Record 19 January 2021.",https://doi.org/10.1016/j.jet.2020.105173,Cited by (21),"This paper is an axiomatic study of consistent approval-based committee (ABC) rules. These are multi-winner voting rules that select a committee, i.e., a fixed-size group of candidates, based on approval ballots. We introduce the class of ABC scoring rules and provide an axiomatic characterization of this class based on the consistency axiom. Building upon this result, we axiomatically characterize three important consistent multi-winner rules: Proportional Approval Voting, Multi-Winner Approval Voting and the Approval Chamberlin–Courant rule. Our results demonstrate the variety of ABC scoring rules and illustrate three different, orthogonal principles that multi-winner voting rules may represent: proportionality, diversity, and individual excellence.","A ==== rule selects a fixed-size set of candidates—a ====—based on the preferences of voters. Multi-winner elections are of importance in a wide range of scenarios, which often fit in, but are not limited to, one of the following three categories (Elkind et al., 2017; Faliszewski et al., 2017). The first category contains multi-winner elections aiming for ====. The archetypal example of a multi-winner election is that of selecting a representative body such as a parliament. The second category comprises multi-winner elections with the goal that as many voters as possible should have an acceptable representative in the committee. Consequently, there is no or little weight put on giving voters a second representative in the committee. This goal may be desirable, e.g., in a deliberative democracy (Chamberlin and Courant, 1983; Dryzek and List, 2003). Voting rules suitable in such scenarios follow the principle of ====. The third category contains scenarios where the goal is to choose a fixed number of best candidates and where ballots are viewed as expert judgments. Here, the chosen multi-winner rule should follow the ==== principle. An example is shortlisting nominees for an award where a nomination itself is often viewed as an achievement.====We consider multi-winner rules based on approval ballots, which allow voters to express ====. An approval ballot thus corresponds to a subset of (approved) candidates. A simple example of an approval-based election can highlight the distinct nature of proportionality, diversity, and individual excellence: There are 100 voters and 5 candidates ====: 66 voters approve the set ====, 33 voters approve ====, and one voter approves ====. Assume we want to select a committee of size three. If we follow the principle of proportionality, we could choose, e.g., ====; this committee closely reflects the proportions of voter support. If we aim for diversity and do not consider it important to give voters more than one representative, we may choose the committee ====: it contains one approved candidate of every voter. The principle of individual excellence aims to select the strongest candidates: ====, ====, and ==== have most supporters and are thus a natural choice, although the opinions of 34 voters are essentially ignored. We see that these three intuitive principles give rise to very different committees.====It is relatively easy to explain what proportionality, diversity, and individual excellence means when the voters' preferences have a specific structure as in the above example. There, for any two voters, their approval sets are either the same or disjoint; this is equivalent to saying that the voters and candidates can be divided into disjoint groups so that each group of voters approves a single group of candidates (intuitively, such a group of candidates can be viewed as a political party)—we thus call such preference profiles ====. However, specifying how a multiwinner voting rule should act on party-list profiles does not—on its own—provide comprehensive guidance for choosing committees for the general model. To achieve that, one needs to rely on more general principles. Our analysis is thus based on four basic principles (framed as axioms):====In this work, we focus on approval-based committee (ABC) ranking rules, i.e., rules that produce a ranking of all committees, rather than only a set of winning committees. This model is very versatile since with ABC ranking rules one can easily combine the societal evaluation of committees with additional requirements one would like to impose on the structure of the committee. For example, suppose the goal is to select a committee subject to certain diversity constraints (such as an equal number of men and women). In such scenarios, a ranking rule can be applied directly: among the committees that satisfy the diversity constraint, one can simply select the committee that appears highest in the societal ranking.",Consistent approval-based multi-winner rules,https://www.sciencedirect.com/science/article/pii/S0022053120301666,29 December 2020,2020,Research Article,152.0
"Gul Faruk,Pesendorfer Wolfgang","Princeton University, United States of America","Received 17 November 2019, Revised 19 September 2020, Accepted 23 October 2020, Available online 11 December 2020, Version of Record 18 December 2020.",https://doi.org/10.1016/j.jet.2020.105129,Cited by (7),We introduce a new theory of belief revision under ambiguity. It is recursive (random variables are evaluated by backward induction) and consequentialist (the conditional expectation of any random variable depends only on the values the random variable attains on the conditioning event). Agents experience no change in preferences but are sensitive to the timing of resolution of uncertainty. We provide three main theorems: the first characterizes our rule and relates it to standard ==== updating; the others show that the dynamic behavior of an agent who adopts our rule is maxmin expected utility with an arbitrary set of priors.,"Consider the following Ellsberg-style experiment: a ball is drawn from an urn consisting of blue and green balls of unknown proportions and a fair coin is flipped. The decision maker is ambiguity averse and receives $1 if the ball is blue and the coin comes up heads or if the ball is green and the coin comes up tails. Prospect 1 (Fig. 1) describes a version of this experiment in which the agent first learns the color of the ball drawn and then the outcome of the coin flip whereas in Prospect 2, she learns the outcome of the coin flip before the ball is drawn from the urn.====Prospect 1 describes a situation in which the coin flip hedges the ambiguity of the draw and, thus, we expect its value to be the same as the value of a $1 bet on the outcome of a fair coin. This interpretation conforms with the definition of hedging in standard models of ambiguity (Gilboa and Schmeidler (1989), Schmeidler (1989), Klibanoff et al. (2005)). In particular, all axiomatic models set in the Anscombe-Aumann framework assume that the coin flip hedges the ambiguity in Prospect 1. The situation is different in Prospect 2; in this case, it seems plausible that the coin flip does not hedge the ambiguous draw from the urn. If it did, in many situations, agents could eliminate ambiguity by randomizing over acts, as Raiffa (1961) argues.====The implicit assumption underlying much of the ambiguity literature is that ex ante randomization does not hedge while ex post randomization does. We explicitly make this assumption.==== Thus, an ambiguity averse decision maker may prefer Prospect 1 to Prospect 2 even though these two prospects differ only in the order in which uncertainty resolves.====The main contribution of this paper is a model of belief updating consistent with the description above. Specifically, we formulate an updating rule that ensures that rolling back payoffs in the ball-first tree (Prospect 1) yields values consistent with hedging while the coin-first tree (Prospect 2) yields values consistent with no hedging. In our model, a totally monotone capacity describes the decision maker's initial perception of uncertainty. However, since the decision maker is sensitive to the timing of resolution of uncertainty, her ex ante evaluation of gradually resolving random variables need not be within the class of Choquet expectation functionals. Instead, such evaluations are within the class of maxmin expectation functionals. Moreover, for any maxmin expectation functional, there is some totally monotone capacity and a suitable timing of resolution of uncertainty such that the resulting ex ante evaluation is arbitrarily close to the given maxmin expected utility functional.====Our theory is recursive and consequentialist. Recursivity means that random variables that resolve gradually are evaluated by backward induction. There is no “preference change” and no preference for commitment (i.e., there is no dynamic inconsistency). Consequentialism means that the conditional expectation of any random variable depends only on the values the random variable attains on the conditioning event. However, our belief revision rule cannot, in general, satisfy the law of iterated expectation since the decision maker is sensitive to the manner in which uncertainty resolves. As the opening example illustrates, this sensitivity to the order of resolution of uncertainty is necessary to match the standard notion of hedging in the ambiguity literature.====As in Kreps and Porteus (1978) and Gul et al. (2019), our model posits an intrinsic preference for how uncertainty resolves. Whether the coin toss occurs first or the ball is drawn first matters despite the fact that no decision is made in between these two events. In the Kreps-Porteus model, the agent cares about the timing of resolution of uncertainty because she values knowing earlier or because she prefers to stay uninformed longer. Hence, the carriers of utility are the beliefs that the agent has at a given moment in time. Therefore, the amount of time that elapses between the coin toss and the draw from the urn matters in the Kreps-Porteus model. In our model, the agent cares about the ==== of the resolution of uncertainty. Hence, she cares about whether the coin is tossed first or the ball is drawn first but not about how much time elapses between these two events.====The primitives of our model are random variables. These random variables could be interpreted as the composition of a general act and a cardinal utility index. A ==== associates a real number, ====, with each random variable ==== and represents the overall value of that random variable (or the utility of the underlying act). Our first result (Proposition 1) ensures that simple evaluations are Choquet integrals with respect to a totally monotone capacity.====To study updating, we define two-stage random variables. In the first stage, the decision maker learns the element of a partition ==== of the state space. In the second stage, the agent learns the state. A ====, ====, is the value of a random variable ==== when the partition is ====. Even though the agent takes no action at the intermediate stage, the compound evaluation of ==== depends on the way uncertainty resolves. Assume, for the moment, that we have an updating rule and let ==== denote the conditional evaluation of ==== given ====. As usual, we define ==== to be the random variable ==== such that==== Then, we can state our recursivity assumption as follows:==== that is, the compound evaluation is computed by backward induction; first for every state ==== in ====, we set ==== equal to the conditional expectation ==== of ==== and then, we evaluate ====. The missing ingredient in this formulation is the new updating rule ====.====Our main result, Theorem 1, characterizes a new updating rule assuming the following weakening of the law of iterated expectation: ==== for all ==== implies ====. We can paraphrase this condition as saying that ====. The law of iterated expectation implies this condition but the converse is not true. In particular, this condition allows for the possibility that all news is good news. To see why ambiguity can lead to such violations of the law of iterated expectation, consider again the introductory example in Fig. 1, above. Let Prospect 3 be the version of Prospect 1 in which the coin toss and the draw from the urn are ==== and assume that simultaneous randomization does not hedge the ambiguous draw from the urn. Thus, the ambiguity averse agent assigns the same value to Prospects 2 and 3. If ==== is the value of a $1 bet on the coin, then we expect the value of Prospect 3 to be less than ====. By contrast, in Prospect 1 the conditional value of the prospect is ==== under either outcome of the draw from the urn and, therefore, all news is good news. In Prospect 1, uncertainty resolves in a manner that eliminates ambiguity which leads to the seemingly paradoxical situation that all news is good news.====In Theorem 1, we assume that the agent's information is a partition of the “payoff relevant” state space, that is, on the domain of the random variables in question. In section 4, we allow for a more general information structure that may not be of a partition form. Thus, agents receive signals that may affect the likelihood of states without necessarily ruling them out. We call compound evaluations with this more general information structure ====. These general compound evaluations are better suited for studying situations in which the analyst does not know the information structure of the agent. Our second main result shows that, given our updating rule, every general compound evaluation is a maxmin rule with some set of priors. Our third main result is the converse of our second: every maxmin evaluation can be approximated arbitrary closely by some general compound evaluation. Hence, our model provides an interpretation of maxmin expected utility as Choquet expected utility with gradual resolution of uncertainty.",Evaluating ambiguous random variables from Choquet to maxmin expected utility,https://www.sciencedirect.com/science/article/pii/S0022053120301228,11 December 2020,2020,Research Article,153.0
Horan Sean,"Université de Montréal and CIREQ, C-6018 Pavillon Lionel-Groulx, 3150 rue Jean-Brillant, Montréal QC, H3T 1N8, Canada","Received 13 November 2018, Revised 27 November 2020, Accepted 28 November 2020, Available online 9 December 2020, Version of Record 23 December 2020.",https://doi.org/10.1016/j.jet.2020.105171,Cited by (8),"In the late 1950's, Luce developed two theories of imperfect utility discrimination that have had a lasting effect on economics. The first (====) initiated the literature on incomplete preferences while the second (====) laid the foundations for the literature on discrete choice.====I propose a general model of utility discrimination that unifies Luce's two theories while addressing the main limitations of each. I show that two conditions from ===='s (====) monograph characterize behavior consistent with this model.","Over a short period in the 1950's, Luce proposed two models of decision-making that have had a lasting effect on economics: the deterministic ==== model (1956), which formalized the concept of “just noticeable differences” and initiated the literature on incomplete preferences; and the stochastic ==== model (1959), which laid the foundations for the multinomial logit model (McFadden, 1974) and the subsequent literature on discrete choice.====While the literatures that developed from these two models have little in common, the models themselves share the same basic goal: to capture “imperfect” utility discrimination. The semi-order model may be viewed as a theory of ==== discrimination where the decision-maker cannot distinguish between alternatives whose utilities are sufficiently close. In turn, the strict utility model may be viewed as a theory of ==== discrimination where the decision-maker becomes less likely to mistakenly choose an inferior alternative as its relative utility decreases.====Both models have well-known limitations. The semi-order model makes no predictions about choice between alternatives whose utilities are sufficiently close. If the decision-maker cannot distinguish perfectly between two alternatives, then the model simply treats her as indifferent. In turn, the strict utility model makes predictions that are not so easy to reconcile with the prevalence of options that are never chosen in real choice data. However small its relative utility, the model requires an alternative to be chosen with strictly positive probability when it is available.====I propose a model, called the ====, that unifies Luce's two models of imperfect discrimination. From a set of alternatives, the decision-maker first eliminates alternatives whose utility she perceives to be relatively small. Then, as in the strict utility model, she selects each of the remaining alternatives with probability proportional to its utility. This two-stage model connects Luce's theories in a fundamental way: the semi-order used to eliminate alternatives in the first stage is ==== the utility function that determines choice probabilities in the second.==== The model also avoids the main limitations of Luce's models—yielding testable implications when utilities are close while also permitting alternatives to be chosen with zero probability.====My main result (Theorem 1) shows that two conditions from Luce's monograph (1959) characterize the stochastic semi-order model: his ====, which includes an “often ignored” (Luce, 2008) condition for zero-probability choices; and a stochastic transitivity requirement that significantly weakens Block and Marschak's (Block, 1960, p. 190) ====.====Several recent papers (Ahumada and Ülkü, 2018; Cerreia-Vioglio et al., 2016; Dogan and Yildiz, 2016; Echenique and Saito, 2019; Lindberg, 2012; McCausland, 2009) study ==== with the same kind of two-stage structure as the stochastic semi-order model. In each of these models, the decision-maker uses some preliminary criterion to filter out alternatives before using Luce's strict utility model to select among the remaining alternatives.====Broadly, the stochastic semi-order model differs from these models on two dimensions. One is the connection between the two stages. The only other models that connect the two stages of decision-making are Echenique and Saito's ==== and McCausland's ====. The other difference is the first stage. Models from the literature generally require more structure than the stochastic semi-order model. Many even require the first stage to maximize a weak order (Cerreia-Vioglio et al., 2016; Dogan and Yildiz, 2016; Lindberg, 2012). This makes the resulting behavior indistinguishable from Block and Marschak's ==== model.====Besides this recent work on lexicographic Luce models, the stochastic semi-order model is also related to the long-standing literature on paired comparisons.==== This literature, which dates back to Fechner (1860) and Thurstone (1927), seeks to extend deterministic notions of preference (and choice consistency) to the stochastic setting. Theorem 2 shows that the stochastic semi-order model can be specialized to the setting of paired comparisons by replacing Luce's Choice axiom with the ==== (due to Luce, 1959; Luce and Suppes, 1965).====Finally, my work is related to the literature on interval representations of semi-orders. Theorem 3 (which is used to establish Theorem 1, Theorem 2) shows that an irreflexive binary relation is a semi-order based on a weak order if and only if the pair satisfies a ==== requirement. While this type of requirement has a long history dating back to Fishburn (1970), Roberts (1971a), and Bordes (1979), mixed transitivity has only been studied systematically in recent work on pairs of binary relations (Giarlotta and Greco, 2013; Nishimura and Ok, 2018).====After defining the stochastic semi-order model in Section 2, I present my main result in Section 3. In Section 4, I discuss the related literature and present some additional results. Proofs are relegated to the Appendix and some extensions are treated in an Online Appendix.",Stochastic semi-orders,https://www.sciencedirect.com/science/article/pii/S0022053120301642,9 December 2020,2020,Research Article,154.0
"Sadzik Tomasz,Woolnough Chris","UCLA, United States of America","Received 24 November 2020, Accepted 26 November 2020, Available online 4 December 2020, Version of Record 22 December 2020.",https://doi.org/10.1016/j.jet.2020.105169,Cited by (1),This paper studies the effect of informed trading on prices in a dynamic model with two-dimensional ,"Information in financial markets takes many forms. Some traders may be better informed directly about the asset's fundamental value, such as a firm's higher-than-expected profits. In addition, they may have information about the liquidity pressures on the asset, which could arise through a number of situations, such as financially constrained speculators liquidating their positions, institutions rebalancing portfolios to manage risk, or buying mania by unsophisticated “individual investors”. Strategic trading on superior information about value has been well researched, and it is known to be a force that stabilizes prices and improves market efficiency. However, strategic trading with superior information about both value and liquidity pressures is not so well understood.====In this paper, we study the effect of an informed trader on prices in a dynamic model of asset trade with two-dimensional information. In our model, trade occurs in continuous time among liquidity traders, a competitive pool of market makers, and an informed trader. Liquidity traders' order flow is affected by a persistent shock. Market makers are risk-neutral, and can service the liquidity needs with no risk premium. The only friction in the model is informational, as the informed trader has private information about both the asset's fundamental value and the liquidity shock.==== This rich, two-dimensional form of private information gives the informed trader rich opportunities to exploit his advantage in the process of dynamic trade.====Despite the challenges posed by two-dimensional dynamic signaling in a nonstationary setting, we provide an analytical characterization of the unique linear Markov equilibrium. This characterization allows us to establish the following four key findings of the paper. First, rather than cushioning liquidity shocks, in equilibrium, the informed trader trades in the same direction, exacerbating liquidity pressure even further. Second, we show that this trading leads to price overshooting: given, say, a positive demand shock, prices are always higher when the informed trader knows about it (as compared to when the shock is unexpected). Third, price volatility increases towards the end of trade, and by the close, both the fundamental information and the liquidity information are revealed to the market. Finally, we show that significant price distortions caused by informed trade persist in the limit, where liquidity shocks are arbitrarily small. We elaborate on these four points below.====In the linear Markov equilibrium that we characterize (Theorem 1), the informed trader's order flow depends linearly on the extent to which the fundamental value and the mean liquidity order flow exceed the market makers' estimates. We establish that at each point in time, the trader acts very differently on the two forms of information. As in Kyle (1985), the fundamental part of the trade always tends to correct the price – buying an underpriced and selling an overpriced asset. However, his speculation on liquidity shocks is always destabilizing – buying along with a buying pressure, and vice-versa (Proposition 4, Proposition 5). For example, if the asset is initially priced correctly, but there is a positive demand shock, the informed trader starts buying and “pumps” up the price, making an initial loss. Before the price peak is reached, while the market still underestimates the extent of the demand pressure, the fundamental part of his strategy takes over, and he begins to “dump”. With a negative demand shock, the informed trader first sells along, and then buys back the asset.====In the paper, we show that this exacerbation of liquidity shocks translates into destabilized prices. In a static model, say, positive demand by liquidity traders moves the price up, but the increase is smaller if the informed trader knows more about the shock (Section 2.1). In other words, informed trade dampens the shock's effect on the price. In our dynamic model, however, the price path is strictly higher if we increase the informed trader's expectation of the shock (Proposition 6). Thus, the informed trade has an unambiguously destabilizing effect on the price. This novel result is in direct contrast to the standard view on the role of informed arbitrage in stabilizing prices, dating back to Friedman (1953).====The equilibrium strategy of the informed trader may be described as front-running a demand shock. When the shock is positive, the informed trader pumps the price up to service it later, at inflated prices; when the shock is negative, he pushes the price down to service it later, at depressed prices. We stress that the multidimensional asymmetric information is essential to create the endogenous price impact of trade in this capital-unconstrained market. For example, in case of a positive shock, it lets the initial buying pressure be misattributed to the improved fundamentals and drive up the price. On the other hand, the market's learning is the only endogenous constraint on the price destabilization.====As in Kyle (1985), in our model, all private information is fully revealed to the market by the end of trade (Proposition 3). In the case of nonfundamental information about liquidity, this requires a novel arbitrage argument. Moreover, different from the models with just fundamental shocks, the price impact and volatility are variable over time and increase towards the end of trade (Section 3.1). The amplitude is increasing in the uncertainty about the liquidity shocks. In particular, price is highly volatile at the end of trade, when it returns to the fundamental value. The intuition is that the informed trader wants to postpone his arbitrage in order to delay the market's learning, and profit from servicing the exogenous liquidity needs longer.====Finally, our results carry over to the limit, as we look at models with vanishing magnitude of liquidity shocks (Proposition 7, Proposition 8). While the shocks are too small to have any significant effect on their own, the destabilization and price overshooting take an extreme form: the limiting equilibrium distribution of prices and the informed trader's asset holdings depend on the shocks as if they were substantial. For example, when persistent liquidity shocks have variance close to zero, a shock of one standard deviation above the mean would still result in a large 0.433 standard deviation increase in the price, driven almost exclusively by the informed trade. The distribution changes discontinuously from the linear Markov equilibrium in the model with just fundamental information (Kyle (1985)), where the insider leans against the wind and always corrects the price. In other words, the smart money essentially creates the price hike.====The result implies that the stabilizing strategic trade is a fragile outcome. The intuition is that in the continuous-time model with only fundamental shocks (Kyle (1985)), Kyle's “constant lambda” (or, price impact) means that the cost of any destabilizing round-trip trade is zero. Consequently, any information about liquidity shocks will tip front-running to be profitable. Moreover, as cheap destabilization relies on the high volume of trade at any instant, this suggests that the increasing liquidity and tightness of modern markets are a mixed blessing. While they facilitate arbitrage, they may also facilitate strategic destabilization.==== Jacklin et al. (1992), Romer (1993) and Avery and Zemsky (1998) were among the first to realize the effect of multidimensional uncertainty on the nonmonotone dynamics of prices. These models had no large strategic players and, thus, no manipulation.====This paper lies at a meeting point of the literatures on manipulation with multidimensional information and on price destabilization via front-running strategies.====The literature on manipulation has shown cases in which a strategic player, exploiting multidimensional uncertainty, can sometimes trade in the wrong direction. In the static setting, Lambert et al. (2018) show that an informed trader may buy when his signal suggests a low value, if the signal also suggests heavy selling (and low price). However, given the static nature of the model, a risk-neutral strategic trader will never trade against his forecasted mispricing: although it is possible that ====, it cannot be the case that ====, based on the trader's filtration.====In the dynamic setting, a strategic player may trade in the wrong direction to create a “smoke screen”. An informed trader may benefit from the uncertainty about whether he is present, possibly adding noise to his trade (Allen and Gale (1992), Allen and Gorton (1992), Fishman and Hagerty (1995), Chakraborty and Yılmaz (2004)).==== Foster and Viswanathan (1994) show that when two traders have nested information, the better-informed one trades heavily on the common signal early on to slow down the release of his superior information about the value. The informed trader in Guo and Ou-Yang (2014), who knows the mean reverting (rather than persistent) liquidity demand, trades against the liquidity trader's position for the same reason. Choi et al. (2019) examine a model, in which a strategic informed trader interacts with a constrained investor, whose portfolio rebalancing provides a justification for persistence in liquidity trade. The strategic trader initially has only private fundamental information, which leaves private information single-dimensional along the equilibrium path, and so precludes the destabilization as in our paper.====In our model, the informed trader will also trade in the wrong direction as long as he responds to the nonfundamental information about the liquidity shocks (==== in Theorem 1). We establish that he not only responds to the shocks but always exacerbates them (====). This drives the main contributions of the paper: the destabilizing pump-and-dump pattern of prices and asset holdings; time-dependent price volatility; and (possibly extreme) price overshooting.====De Long et al. (1990a) is the first paper to feature destabilizing strategic trade, in which informed traders pump the price up to set off demand by trend followers. Brunnermeier and Pedersen (2005) have strategic traders front-run a known sell-out by a distressed institutional trader, which leads to price overshooting.==== The result relies on the exogenous constant price impact, and the long- and short-selling constraints. We show how price impact and the novel constraint on front-running, given by the market's learning, arise endogenously with multidimensional uncertainty. When price impact stems from risk aversion instead, with one-dimensional asymmetric information about endowment shock only, the informed trader may first sell and then buy back shares, but there is no evidence of price overshooting (Vayanos (2001)).====In a model related to ours, but with two periods and independent liquidity shocks, Bernhardt and Taub (2008) show that the informed trader first front-runs and then trades against the second-period shock. In their model, informed trade both smooths the effect of the liquidity shock across time and stabilizes the price.==== Intuitively, the informed trader trades at negatively impacted prices at each time, with cost quadratic in order size. When the destabilizing strategy may not be spread over many periods, the cost of destabilization is prohibitively high.====Finally, while Kyle (1985), Back (1992) and Ostrovsky (2012) allow for asymmetric information about the fundamental value only, we show that the arbitrage argument can be extended to show revelation of both fundamental and non-fundamental information.",Rational destabilization in a frictionless market,https://www.sciencedirect.com/science/article/pii/S0022053120301629,4 December 2020,2020,Research Article,155.0
Dutta Prajit K.,"Columbia University, United States of America","Received 19 August 2019, Revised 27 November 2020, Accepted 28 November 2020, Available online 3 December 2020, Version of Record 4 March 2021.",https://doi.org/10.1016/j.jet.2020.105170,Cited by (0),"This paper considers a new model of repeated bargaining over a flow. In ====, once an agreement is reached the game ends. Here, the game continues and the agreement can be re-negotiated at cost in any period. It is shown that, with finite memory, there is a unique equilibrium which converges to the Nash Bargaining Solution in the limit. The novel equilibrium feature is the search for ====; players reject agreements that give them “too much”. Whilst rejection and delays are never observed on the equilibrium path in a stationary environment, they are observed in a non-stationary one. A folk theorem emerges with infinite memory."," - This paper considers a new model of ==== bargaining and proposes a theory of ====. The model is based on two key features. First, bargaining is over a flow rather than a stock; there is a (fresh) cake in every period. Second, though an agreement can be continued costlessly onto the next cake, it is costly to re-negotiate an existing agreement.====Both features are important for real-world bargaining. Consider the first - bargaining over a flow. Kennan (2001) notes - “Repeated bargaining relationships are important in many economic contexts. An obvious example is the continuing relationship between a union and an employer. Repeated contracts also arise in international trading relationships, and in intermediate product industries.” Surprisingly, repeated bargaining has received limited attention with the literature primarily focused on single bargain models, starting with Rubinstein (1982), models where the first agreement ends the bargaining game. The few exceptions include Hart and Tirole (1988), Muthoo (1995, 1999), Kennan (2001) and Strulovici (2017).====The second feature - that agreements can be continued costlessly - is embedded in many contracts as an “Evergreen Clause” which “allows for an agreement to continue for a defined period if the existing agreement is not renegotiated or properly canceled within a specified time. Evergreen Clauses can be found in both consumer and commercial contracts, including Residential Lease Agreements, Advertising Contracts, and many other service-based agreements.”====To incorporate these two features, I study a repeated bargain version of the complete information two-player alternating move Rubinstein (1982) model. In every period ==== there is a new cake. Also, in every period, only one player— the proposer— takes an action. The proposer picks a share - a utility number - for Player 1, ====. The pair of utilities, ====, then becomes the status quo, the proposal on the table, for the period ==== cake. If the proposer in that period matches the status quo by picking the same share for Player 1, i.e., if ====, then we have an agreement on the ==== cake and the two players receive ==== in that period. Otherwise, there is disagreement, the ==== cake is wasted and the players receive ====.====The model nests the single bargain Rubinstein model if it is further required that after the very first agreement ====, all subsequent proposals ====, ==== must equal ====. In that case, an agreement can never be renegotiated.====Here though, even after an agreement is reached with ====, it can be renegotiated by the period ==== mover proposing ====. That is costly in that period ==== payoffs then become ====. On the other hand, the agreement can be costlessly continued onto the ==== cake by the proposer choosing ====. This is the second feature, the Evergreen Clause.==== - Due to these two features, any agreement has to be forward-looking and may, consequently, be different from single bargain equilibria. It is well-known that in a single bargain model like Rubinstein (1982), the only proposal that a current mover will turn down is one in which she gets too little. She turns such a proposal down because, if she agrees to it, it becomes a permanent agreement and so she would rather reject it and counter-propose a better agreement for herself.====When the bargaining is over a flow, and agreements are not permanent, there is an additional reason to turn down a proposal - because the agreement may be short-lived. And it may be short-lived because it gives the current mover not too little but rather “too much”. In doing so, it gives the other player - the mover next period - too little and creates an incentive for him to then re-negotiate the agreement next period thereby making it short-lived. Anticipating that, the current mover might pre-emptively turn down a generous proposal in favor of one that is not as good but is long-lived. This turning down of generous proposals is what I call “compromise”.====To make matters a bit more precise and to see the exact trade-offs at work, suppose that, at time ====, Player 1 is deliberating on whether to accept a proposal that gives her a high ====. Suppose she knows that Player 2 - whose share is thereby low - would reject it in the next period ====, i.e., she knows that the agreement will be short-lived. Indeed, when Player 2 picks a new share, he likely would propose a share ==== that is minimally acceptable to Player 1. So accepting the generous proposal ==== would lead to a stream of payoffs ====.====An alternative for Player 1, anticipating all this, is to reject the high offer ==== and instead propose an agreement ==== that is maximal to her (and minimally acceptable to Player 2). Proposer power implies that ====. That would lead to a stream of payoffs ====.====With convex preferences, a stable moderately good agreement stream ==== turns out to be better than ==== - one that fluctuates between a high and a low share. “====”, turning down generous offers, is the equilibrium strategy.====I first show that if we restrict attention to strategies with finite memory (including Markov-Perfect Equilibria), then there is a unique SPE and it has the compromise feature discussed above. Furthermore, the limit outcome - as the discount factor converges to 1 - is the Nash bargaining solution. Second, once we allow infinite memory strategies, there is, however, a folk theorem. In particular, therefore, agreements and disagreements and delay in agreements of arbitrary length, can all emerge.====One potential criticism to address is that, in the stationary deterministic model studied here, generous offers and their rejection are not observed on the equilibrium path (after, possibly, the first period). In Section 5, I show that if we extended the model to allow for, either, non-stationarity or uncertainty, then compromise would indeed be observed on the equilibrium path.====In Section 5.1 I study a growing cake, a model in which there is a positive probability that the next cake will be ==== times bigger, ====.==== When the cake grows, the proposer could find herself in the fortunate situation of having inherited an agreement that now looks particularly attractive; he, Player 2, is only required, say, to hand over ==== to Player 1 and keep the balance but now the balance is ==== rather than ====. If agreements could not be re-negotiated, then he would happily keep the windfall. However, given the likely consequent re-negotiation and following the logic of compromise above, he would instead break the agreement, take 0 in the current period and propose a larger share for his opponent to have ==== starting in the next period.====In Section 5.2, I consider a model in which there is “noise”, similar to that in trembling-hand perfection and also related to the model of uncertainty in the Nash demand game (Nash (1953)). In that stochastic model, because of the noise, compromise also appears on the equilibrium path because mistakes generate generous offers (which are then rejected).====In terms of literature, the paper builds on the ==== model initiated by Rubinstein (1982). Surprisingly, there are only a few papers on ====. For complete information, there is Muthoo (1995) - see also Muthoo (1999) - and that is closest to the current paper. The main difference in model is that in Muthoo (1995) there no link between contiguous cakes. The paper is analyzed at length in Section 6.====Hart and Tirole (1988) and Kennan (2001) consider Repeated Bargains but within incomplete information. The link across periods is persistent private information. Finally, in Political Science, there is an extensive ==== literature that has followed Baron and Ferejohn (1989).====The reader might also wonder to what extent compromise is driven by the particular bargaining protocol studied here. I believe the phenomenon is general. All that is important is that re-negotiation can lead to a worse long-term outcome (an ==== rather than ====). And that would be true in any bargaining model with “proposer power”, a proposer getting a larger share than the proposed. In the alternating move model, proposer power kicks in with the very next cake. It should be clear though that as long as the opponent has ==== move in the future, the logic would still apply. Equally, and this is shown in Dutta (2020), it is not important that the re-negotiation cost, driven here by the Evergreen Clause, be large. Indeed, it is shown there that the result is qualitatively true for any positive re-negotiation cost.====The Model is in Section 2. Sections 3 and 4 discuss, respectively, finite and infinite memory equilibria. Section 5 has the extensions to non-stationarity and uncertainty. A literature review is in Section 6.",Compromise is key in infinitely repeated bargaining with an Evergreen Clause,https://www.sciencedirect.com/science/article/pii/S0022053120301630,3 December 2020,2020,Research Article,156.0
Bahel Eric,"Department of Economics, Virginia Polytechnic Institute and State University, Blacksburg, VA 24061-0316, USA","Received 4 February 2019, Revised 20 November 2020, Accepted 23 November 2020, Available online 26 November 2020, Version of Record 1 December 2020.",https://doi.org/10.1016/j.jet.2020.105168,Cited by (3),"For the class of cooperative games with transferable utility, we introduce and study the notion of hyperadditivity, a new cohesiveness property weaker than convexity and stronger than superadditivity. It is first established that every hyperadditive game is balanced: we propose a formula allowing to compute some core allocations; and this leads to the definition of a single-valued solution that satisfies core selection for hyperadditive games. This new solution coincides with the Shapley value on the subclass of convex games. Furthermore, we prove that the bargaining set of a hyperadditive game is equal to its core. It is shown that many well-known economic applications satisfy hyperadditivity. Our work extends (and gives a unifying explanation for) various results found in the literature on network games, assignment games and convex games. In addition, some new results are derived for these respective families of games.","Cooperative games with transferable utility (TU games, for short) allow to model a wide array of economic problems where side payments between agents are possible. Their solution concepts include the ====, which is the set of allocations that no coalition of players can improve upon, and the Shapley value, a single-valued solution characterized by a few natural axioms (see Shapley, 1953). Among other applications, these solution concepts have been used to describe (i) the reallocation of endowments in an economy [see for instance Shapley and Shubik (1969a), Wilson (1978)], (ii) bankruptcy and bargaining problems between economic agents (Gul, 1989; Montez, 2014), (iii) matching between firms and workers (Crawford and Knoer, 1981; Kelso and Crawford, 1982) or matching between tasks and machines (Bahel and Trudeau, 2019a).====A distinguished class of TU games is the family of ====, whose characteristic functions are supermodular. These convex games, which were first studied by Shapley (1971), exhibit some remarkable properties. For example, every convex game has a nonempty core (a property not guaranteed as soon as one drops convexity). Moreover, in a convex game, the Shapley value obtains as the average of all extreme core allocations. For TU games that are not convex, a major drawback of the Shapley value is that it generally does not fall in the core.====Another well-known family of TU games, which contains all convex games, is the class of superadditive games —see for instance Young (1985) and Solymosi (1999). However, it turns out that the core of a superadditive game may well be empty and, even if the core is nonempty, the Shapley value typically does not produce a core allocation in superadditive games (see Example 4).====Most economic applications of TU games are superadditive (bargaining, matching, networks, production economies, voting, etc.); but as pointed out above, superadditive games do not have the nice properties found in convex games. The main contribution of the present paper is to bridge this gap by introducing and studying a new class that is contained in the family of superadditive games. The interest of this new class lies in the facts that (a) it meets many of the properties of the class of convex games, (b) it encompasses most of the aforementioned applications of TU games, (c) we define a new solution concept that is a core selection on the class. These games, which we call ====, are formally defined in Section 2; and we then proceed to prove the points (a), (b) and (c) above.====Specifically, the notion of hyperadditivity is defined using the concepts of reduced game (Davis and Maschler, 1965) and marginal game (Núnẽz and Rafels, 1998). Given a superadditive TU game with player set ==== and characteristic function ====, call ===='s ==== the quantity ====. Then define the reduced game with player set ==== and characteristic function ==== such that ====, for every subset ==== of ====. Call this reduced game ==== the ==== associated with ====. In turn, one can compute the reduced game ==== obtained by using the marginal contribution of some player ==== to the game ====, and so forth. A game is then called ==== if these successive marginal games are all superadditive.====We prove in Proposition 1 that every convex game is hyperadditive, thus establishing that convexity is a stronger requirement than hyperadditivity. Moreover, we show that every hyperadditive game has a nonempty core (Theorem 2). Precisely, we define a new single-valued solution concept (the ====) which always falls in the core of hyperadditive games.==== Therefore, the average marginal value (unlike the Shapley value) is a core selection on the class of hyperadditive games. Interestingly, it is also shown that this new value coincides with the Shapley value on the subclass of convex games.====These findings mean that many properties exhibited by the Shapley value on the class of convex games carry through to the wider class of hyperadditive games if one extends the restriction of the Shapley value (to the set of convex games) by using our average marginal value. We also establish the equivalence between the bargaining set and the core for hyperadditive games. Maschler et al. (1971) showed that the bargaining set and the core coincide in convex games. The same result has been shown for assignment games (Solymosi, 1999) and veto games (Bahel, 2016). By stating that the core and the bargaining set coincide in every hyperadditive game (see Theorem 3), the present paper provides a unifying explanation for these three seemingly unrelated results.====We show in Section 6 that veto games, shortest path games and minimum cost arborescence games (including minimum cost spanning tree problems) are all hyperadditive. In Section 7 we focus on assignment games and show that they satisfy ====, a weaker requirement which is sufficient to guarantee many properties exhibited by hyperadditive games (see Proposition 5). As pointed out by Núnẽz and Rafels (2003), a marginal game of an assignment game is typically not an assignment game (it is not even superadditive in general). Regardless, we show in the proof of Proposition 11 that it has the same core (up to a geometric translation) as some suitably constructed assignment game. Our analysis of assignment games through the lens of quasi-hyperadditivity allows to better understand many results found in this literature.",Hyperadditive games and applications to networks or matching problems,https://www.sciencedirect.com/science/article/pii/S0022053120301617,26 November 2020,2020,Research Article,157.0
Sadler Evan,"Columbia University, United States of America","Received 28 January 2019, Revised 26 October 2020, Accepted 22 November 2020, Available online 26 November 2020, Version of Record 1 December 2020.",https://doi.org/10.1016/j.jet.2020.105167,Cited by (1),"Evidence suggests that individuals and firms are sometimes more innovative when subject to low-powered incentives. I offer an explanation based on a characteristic feature of creative work: hitting dead ends. An agent works on successive ideas, each of which may lead to a breakthrough with some probability. At any time, the agent may abandon her current idea, incurring delay to come up with a new one. Larger rewards and greater impatience cause the agent to spend more time on each idea and to work on lower quality ideas. I subsequently consider a planner who can choose ==== or subsidy policies to maximize the value of research ====. If ==== are large and relatively certain, then optimal policy favors the use of subsidies. If spillovers are concentrated among the highest quality ideas, then optimal policy favors taxes. The results highlight why policy makers should care about the structure of incentives for innovation beyond simply encouraging more overall investment.","Investments in innovation produce positive spillovers, so there is scope for social policy to encourage more innovation.==== However, the best way to do this is far from obvious because innovative output responds to incentives in unusual ways. Empirical work shows that low-powered incentives, meaning rewards to success that are smaller and less immediate, are strongly associated with greater innovation. In a corporate context, long-term incentives for research and development heads (Lerner and Wulf, 2007), longer stock option vesting periods (Yanadori and Marler, 2006), golden parachutes (Francis et al., 2011), and a failure-tolerant culture (Tian and Wang, 2014) are all associated with firms creating more patents and more heavily cited patents. Ederer and Manso (2013) present evidence from a laboratory experiment that long-term incentives and tolerance for failure help motivate innovation, and Azoulay et al. (2011) show how grants that are more failure tolerant, and give researchers more flexibility over what projects to pursue, increase the output of academic scientists. I ask two questions in this paper: Why do low-powered incentives encourage innovation, and how should this feature impact policy?====Unpacking the first question, we can distinguish two margins on which innovators make choices: whether to pursue innovation, and if they do, how to pursue it. Existing explanations for the association between low-powered incentives and innovation focus on the first margin. I offer a new perspective that focuses on the second. The theory in this paper centers on a distinctive feature of creative work: dead ends. As any experienced researcher knows, not every new idea works as intended. A pharmaceutical company explores hundreds if not thousands of compounds for each new drug that comes to market. A serial entrepreneur might suffer several failures and bankruptcies before coming up with a successful business plan. An academic scientist may try multiple experimental designs before convincing evidence emerges for or against a hypothesis. In each case, an agent decides not just how hard to work, but also how long to keep trying before searching for a new approach.====The option to search for a new idea leads to a tradeoff: do I suffer delay to start on a more promising project, or do I hold out a bit longer, hoping for a quick success? Because of this tradeoff, high-powered incentives may discourage creativity. High rewards make delay more costly, which leads to more time spent on dead ends and lower quality ideas. Impatience (i.e., a higher discount rate) has an analogous effect: the agent works longer on fewer ideas with lower average quality. This effect has nuanced implications for policymakers seeking to encourage innovation. Successful ideas require two distinct inputs, effort and creativity, and different incentives can shift the input mix in either direction. Beyond simply encouraging more investment in innovation, a savvy planner should also consider how incentive structures affect the ==== of innovation.====My formal analysis extends the exponential bandit framework of Keller et al. (2005) to model the innovative process. An agent successively comes up with ideas and works on each until either a breakthrough occurs or she gives up to start on a new idea. Every idea is good with independent probability ==== and has a value ==== drawn independently from a continuous distribution—==== is the payoff the agent receives if she has a breakthrough while working on the idea. If an idea is good, a breakthrough arrives at a constant positive rate. If an idea is bad, a breakthrough never arrives. Working on an idea incurs a flow cost of effort, but at any moment, the agent can give up and spend time searching for a new idea—this option is the key departure from previous work.==== Following a breakthrough, the agent likewise searches for a new idea.====I find it useful to distinguish two phases in the production of breakthroughs: the working phase, during which the agent exerts effort on an idea, and the creative phase, during which the agent searches for a new idea. We can view the two phases as complementary productive inputs—the agent's problem is choosing an optimal input mix. Raising the cost of one input relative to the other results in less use of that input. In particular, if creativity becomes more expensive, then the agent spends more time working on each idea and works on lower quality ideas.====One obvious way to raise the relative cost of creativity is to reduce the flow cost of effort in the working phase. Because the ratio of the reward ==== to the flow cost ==== is what determines the optimal policy, we get the same effect if we scale up the value distribution—higher powered incentives, meaning higher rewards for success, lead the agent to spend more time on each idea. An increase in the agent's discount rate has a similar effect. Since searching entails delay, increasing the agent's discount rate reduces the option value of search. As a result, an impatient agent is more reluctant to give up on her current idea and is willing to work on ideas with lower value.====This mechanism through which low-powered incentives lead to more innovation is distinct from explanations based on the familiar exploration-exploitation tradeoff. First, a different choice margin drives the result. Here, the question is not whether the agent explores a risky option or exploits one with known payoffs—the agent is going to experiment. The only question is, on what? The two approaches present the agent with qualitatively different outside options—a safe option with a known reward versus a random draw of a new idea—yielding different comparative statics.====In models based on the exploration-exploitation tradeoff (e.g., Manso, 2011), high rewards for early success make gathering information more costly, so the agent gravitates towards the outside option. For instance, publication bonuses might lead a scientist to produce minor improvements on a well established topic instead of pursuing more radical breakthroughs. Moreover, the incentive to focus on safe research topics is even greater if the scientist is impatient. In the present paper, impatience and high rewards make the outside option ==== due to the delay cost. If our scientist is committed to making more radical breakthroughs, then a publication bonus, or greater impatience, leads the scientist to spend more time on each project and explore fewer ideas overall. Another difference is that the exploration-exploitation tradeoff only explains why low-powered incentives encourage innovation in early periods—experimenting now is only valuable if it can lead to a large reward later, which requires high-powered incentives in later periods. The mechanism I describe can explain why permanently lower rewards may lead to more innovations with higher average value.====The effect of incentives on the process of innovation underscores new considerations for policymakers. I subsequently explore how a planner might use subsidies and taxes to maximize some objective. I assume a success of value ==== to the agent generates a social value ====, and the planner can either provide a subsidy to the flow cost of effort or impose a tax on the agent's reward. Because the agent's experimentation policy entails the choice of when to search versus when to work, and not whether to experiment, the planner's problem is not simply one of encouraging more effort. Different policies can shift the agent's input mix in either direction, leading the agent to either exert more effort on fewer ideas or to exert less effort while exploring more ideas.====Optimal policy depends both on the size of spillovers and the curvature of the function ====. When the social value of innovations is higher and less uncertain—meaning ==== is relatively concave—the planner should favor subsidies. When overall spillovers are lower, and they are concentrated among the highest quality ideas, then taxation better aligns the agent's incentives with the planner's objective. In practice, subsidies may involve direct financing for certain research projects or tax deductions for R&D investment, and my results offer a rationale for why subsidies should be more or less prevalent across different settings. Taking a different interpretation of the tax intervention, this finding has implications for the allocation of property rights in innovative ventures.====This paper makes three contributions. First, the analysis offers a new rationale for why low-powered incentives are associated with innovation. For agents that are committed to creating breakthroughs, the question is not whether they exert effort or not, but how that effort gets directed. Such agents trade off the value of working now against the opportunity cost of coming up with a new idea. Low-powered incentives increase the value of this option relative to continuing on the current idea, so the agent explores more ideas. Second, I point out that how spillovers relate to the agent's payoff has implications for social policy. Subsidies and taxes shift the agent's effort in opposite directions, each may be appropriate in different settings. Separate from encouraging more agents to pursue innovation, a planner should consider how the incentive structures affect the way these agents conduct their projects. Finally, building on the workhorse exponential bandit model, I introduce a modification that addresses important limitations in the experimentation literature. Considering the possibility of quitting to come up with new ideas may prove fruitful in other applications.",Dead ends,https://www.sciencedirect.com/science/article/pii/S0022053120301605,26 November 2020,2020,Research Article,158.0
"Dziubiński Marcin,Goyal Sanjeev,Minarsch David E.N.","Institute of Informatics, Faculty of Mathematics, Informatics and Mechanics, University of Warsaw, Poland,Faculty of Economics and Christ's College, University of Cambridge, United Kingdom,Fetch.AI, United Kingdom","Received 13 March 2020, Revised 9 November 2020, Accepted 15 November 2020, Available online 24 November 2020, Version of Record 27 November 2020.",https://doi.org/10.1016/j.jet.2020.105161,Cited by (9),"This paper develops a theoretical framework for the study of war and conquest. The analysis highlights the role of three factors – the technology of war, resources, and contiguity network – in shaping the dynamics of appropriation and the formation of empires.","A recurring theme in history is that of war and conquest.==== This motivates a number of questions. What are the circumstances under which rulers will choose to fight? What is the optimal timing of attack, now or later? When will the resource advantage of a ruler translate into domination over neighbors? What are the limits to the size of the empire? The goal of this paper is to develop a theoretical framework to address these questions.====We consider a setting with a number of ‘kingdoms’. Every kingdom is endowed with resources and controlled by a ruler. Rulers desire to expand territory and acquire more resources. A ruler can wage a war on neighboring kingdoms. The winner of a war takes control of the loser's resources and his kingdom; the loser is eliminated. The probability of winning a war depends on the resources of the combatants and on the technology of war that is defined by a ====.==== As the winning ruler expands his domain, he may be able to access and attack new kingdoms. The neighborhood structure between kingdoms is reflected in a ====. We model the interaction between rulers as a dynamic game and study its (Markov Perfect) equilibria.====We start by establishing that there exists a pure strategy Markov Perfect equilibrium and the equilibrium payoffs are unique. This sets the stage for a study of how the main parameters – resources, the contiguity network, and the contest function – affect the dynamics of war and peace.====Consider two rulers ==== and ====, with resources ==== and ====, and suppose ====. When they fight, the expected payoff of ==== is given by ====, where ==== is the contest success function that defines the probability of winning for ruler ====. The contest success function is said to be ==== if fighting is profitable for ==== (and unprofitable for ====), i.e., ====, for any ==== and ==== such that ====. The technology is said to be ==== if fighting is unprofitable for ==== (and profitable for ====), ====, for any ==== and ==== such that ====.==== The technology shapes the optimal timing and an optimal target of attack. When the technology is rich rewarding, ==== is optimal: attacking the two rivals in sequence is preferable to attacking the merged kingdom. In the poor rewarding setting, ==== is optimal: attacking the larger kingdom formed after two rivals have fought is best. Moreover, with a rich (poor) rewarding technology it is optimal for a ruler to attack opponents in increasing (decreasing) order of resources. Equipped with these results, we turn to the study of equilibrium dynamics.====Theorem 1 shows that with three or more rulers a rich rewarding technology implies that ==== rulers (even the poorest ones) find it optimal to attack a neighbor as soon as possible. Thus, we are in a world with incessant warfare, the violence only stops when all opposition is eliminated. When the network is connected, all opposition is eliminated only with the hegemony of a single ruler.==== The arguments underlying this result are fairly general. We start by defining a ==== ruler: this is a ruler who has a ‘full attacking sequence’ (involving all other opponents), such that at each point he is stronger than the opponent. Clearly, at any point in time, the richest ruler is a strong ruler. It follows from the rich rewarding property that, if everyone else is peaceful, then such a strong ruler has a strict incentive to fight every other ruler. Next consider the case when other rulers may also wish to attack: does the strong ruler still have an incentive to implement a full attacking sequence? Given the no-waiting property identified above, it follows that the strong ruler has a dominant strategy: a full attacking sequence. So, there is always at least one ruler who wishes to fight to the finish. Anticipating this, and given the no-waiting property, every ruler, no matter how poor, has an incentive to fight a neighbor. Thus in a connected network, in equilibrium, eventually there will be only one ruler left. Remarkably, this result does not depend on the topology of the network, as long as it is connected. Even very sparsely connected networks cannot prevent conflict escalation.====Turning to the role of resources and networks, we note that a ruler with relatively limited resources may be strong because the network enables him to accumulate resources by fighting weaker opponents. On the other hand, a ruler with relatively high resources may be surrounded by stronger opponents and have very limited opportunities to become a hegemon. This is the basic level on which the distribution of resources, the topology of the network, and the position in the network determines the probability of becoming a hegemon. For ease of exposition, consider the well known Tullock Contest Function: the probability of ruler ==== winning is ====, for some ====. It can be shown that the function is rich rewarding if ==== and poor rewarding if ==== (and rulers are indifferent between war and peace if ====). When ==== is sufficiently large, the probability of a weak ruler becoming a hegemon becomes negligible and the key factor affecting the probability of becoming a hegemon is whether the ruler is strong or weak. Within the set of strong rulers, those who have ‘exclusive’ access to weak kingdoms, have a significantly greater probability of becoming the hegemon (relative to their strong rivals). We show that the dynamics of appropriation have powerful redistribution effects: in particular, they tend to take resources away from the richest kingdoms and the poorest kingdoms and toward the middle resource kingdoms.====We then turn to a study of poor rewarding contest success functions. Observe that, by definition, a poor ruler gains from fighting a rich rival. However, in this setting, waiting is better: so the poorer ruler would prefer to wait and allow for opponents to become large before engaging in a fight. This gives rise to the prospect of peace. To make progress we divide the analysis into two parts. To start, consider resource distributions with a single rich ruler: if this ruler is sufficiently rich then his kingdom becomes an ‘irresistible’ prize; all other rulers have a strict incentive to fight to acquire the rich kingdom. So peace cannot be sustained and the outcome is hegemony. Next, consider the case where no ruler is very rich. Here we show that perpetual peace and a phase of war followed by peace may be sustained in equilibrium. The key to sustaining peace is the threat of imminent war. The equilibrium has the following structure: no ruler wishes to fight a single fight because, once this fight is undertaken, all rulers have an incentives to fight till the finish. It is this latter phase of war that makes war today unattractive. These arguments are summarized in Proposition 4. We illustrate through examples that the role of inequality is more general: across a range of networks and contest success functions, peace is more likely when resources are more similar. And, we illustrate through examples, that the dynamics of appropriation in the poor rewarding setting are ‘equalizing’. This is most clearly seen when Tullock parameter ==== is close to 0: across a range of networks, the equilibrium payoffs of all rulers are then more or less equal.====To summarize, the analysis suggests that in the baseline model, rich rewarding technology creates powerful incentives for war: starting in a situation with multiple kingdoms, the dynamics are characterized by incessant fighting; the expansion of a kingdom and, consequently, the size of the empire, is limited by the connectivity of the network. By contrast, if the technology is poor rewarding, the dynamics are considerably more complicated. War followed by hegemony is possible, but peace with multiple kingdoms may also be a long run outcome.====The interest turns next to other factors that would potentially act as restraints on war. We consider an extension of the model in which rulers choose short attack sequences only: this accommodates the idea that rival rulers can become active once a ruler begins an attack sequence. We show that the incentives to wage war remain strong in this setting: hegemony is still the norm. We then study the possibility of rivals forming an alliance to resist the aggression of an active ruler. When a ruler is picked to fight, the other rulers can form an alliance: this alliance puts together resources of all members to defend attack against any of them. The study of such defensive alliances delineates the circumstances under which a ‘balance of power’ can help restrain aggression and limit hegemony. Next we turn to costs of war in terms of lost resources. We consider a model in which rulers lose a proportion of resources in war. The analysis shows that costs of war significantly alter the incentives of rulers to wage war. A richer ruler will only wish to attack a poorer ruler if the resource differences are neither too small nor too large. This creates the possibility of ‘buffer’ states: poor kingdoms that are located in between large powerful neighbors and prevent the progression of conflict. Alliances and costs of war thus highlight ways in which the framework can be enriched in ways that accommodate forces that limit the scope of hegemony.====We now place our paper in the context of the literature and clarify its contributions. Our paper studies the dynamics of war and peace and the formation of empires; related work includes Hirshleifer (1995), Jordan (2006), Krainin and Wiseman (2016), Levine and Modica, 2013, Levine and Modica, 2016, and Piccione and Rubinstein (2007). In an early paper, Hirshleifer (1995) showed that ‘anarchy’ or multiple opponents could be sustained in a dynamic setting only if the technology satisfies ====.==== We show that ==== does indeed lead to the emergence of a hegemon, but our analysis goes beyond this insight along a number of dimensions: we develop a non-cooperative and dynamic game with many far-sighted players; we consider general contest functions, and there is a network structure which shapes the sequence of attack strategies and the scale of empires. Our analysis introduces new concepts: rich/poor rewarding contest success functions and strong/weak rulers. They enable us to address a range of different questions, such as the timing and monotonicity of optimal attack strategies, and how the prospects of individual rulers depend on the network and on the nature of the contest success function.====The theoretical framework combines elements from the literature on contests, on resource wars, and on networks. We now discuss the relationship between our paper and these literatures.====There is a large literature on contests, for surveys see Konrad (2009) and Garfinkel and Skaperdas (2012). We consider a general model of multi-player contests inspired by the axiomatic work of Skaperdas (1996).==== In recent work, Konrad and Kovenock (2009), Groh et al. (2012), and Anbarcı et al. (2018) study multi-player sequential contests. In these papers the contest takes the form of an all-pay auction. The interest is in how individual heterogeneity and the sequential contest structure determine aggregate efforts and winning probabilities. By contrast, in our model, we abstract away from effort so that we can study the dynamics of conflict with general contest success functions and networks. To the best of our knowledge, the results on rich/poor rewarding contest success functions and strong/weak rulers are novel in the context of this literature.====The role of resources in shaping violent conflict is an active field of study, see e.g., Acemoglu et al. (2012), Caselli et al. (2015), and Novta (2016). This literature provides evidence for appropriation of resources as a major motivation for war. The theoretical work is mostly limited to two players or to symmetric models; for an overview of the theory, see Baliga and Sjöström (2012). Our paper contributes to this literature by studying the cumulative dynamics of appropriation and the expansion of territory within a contiguity network, and by linking these dynamics to major episodes of world history.====Finally, our paper is a contribution to the recent literature on conflict and networks, see e.g., Franke and Öztürk (2015), Hiller (2017), Kovenock and Roberson (2012), Huremović (2015), De Jong et al. (2014), Jackson and Nei (2015), and König et al. (2017). For an overview see Dziubiński et al. (2016). Our paper advances this literature by presenting a dynamic game of inter-connected conflict that takes place over a network. De Jong et al. (2014) introduce a model of conflict with resources and a network. The principal difference between their paper and our paper is that they impose conflict exogenously: links are picked at random and rulers ==== fight. By contrast, in the present paper, the choice of waging a war or being at peace is the central object of study. This leads to a different mode of analysis with a focus on how networks and resources shape the incentives to fight, the optimal sequence of fights, and how that determines the long run outcomes.====The rest of the paper is organized as follows: Section 2 presents the basic model, Section 3 studies the incentives to fight and the optimal timing of attack, Section 4 presents the results on equilibrium dynamics, Section 5 discusses three variants of the model: one where sequences of attack are limited to a single fight, one where rulers can form defensive alliances, and one where we allow for losses in war. The proofs of the main results are presented in an Appendix. The proofs of results in the extensions to the model are presented in the On-Line Appendix.",The strategy of conquest,https://www.sciencedirect.com/science/article/pii/S002205312030154X,24 November 2020,2020,Research Article,159.0
"Jovanovic Boyan,Prat Julien","Economics Department, NYU, United States of America,CREST, CNRS, Institut Polytechnique de Paris, France","Received 20 January 2020, Revised 29 October 2020, Accepted 30 October 2020, Available online 18 November 2020, Version of Record 23 November 2020.",https://doi.org/10.1016/j.jet.2020.105158,Cited by (0),"Cyclical patterns in earnings can arise when contracts between firms and their workers are incomplete, and when workers cannot borrow or lend so as to smooth their consumption. Effort cycles generate occasional large changes in earnings. These large changes are transitory, consistent with recent empirical findings.","Recent analyses of large panel data sets reveal that earnings fluctuate in ways that cannot be captured by the linear models commonly used in the literature on earnings dynamics. In particular, earnings data show occasional changes far larger than predicted by a log-normal distribution – see Guvenen et al. (2015) and Arellano et al. (2017), henceforth GKOS and ABB respectively.====These unusual events are routinely attributed to external shocks such as layoffs triggering falls off the job ladder. Since large income fluctuations reduce the welfare of risk-averse agents, it seems natural to conjecture that they are not initiated by them. We show that career-concerned agents may in fact prefer to go through periods of low and high effort. We call these oscillations ====.====We change the career concerns model of Holmström (1999) by assuming that the agent is risk-averse and cannot borrow or lend. Since reputational investment pays off only in the future, a worker's incentive to maintain or improve her reputation depends on her discount factor. If she is risk averse, her discount factor depends negatively on her consumption growth. When the worker cannot smooth her consumption by other means,==== this force can then give rise to cycles in her effort, and thus in her earnings. The mechanism works as follows: When effort is low, the worker's consumption is low relative to her future consumption which means that her discount factor is also low. This reduces her incentive to create a good reputation and low effort is self fulfilling. In other words, the discount factor and reputational concerns are low because consumption growth is high. Conversely, in a period of high effort, current consumption is higher than future consumption, which means that the discount factor is also large and so high effort is self fulfilling too.====The model delivers several insights. First, evidence has shown that tangible investment is inhibited by liquidity constraints – Fazzari et al. (1988). We find that the opposite may holds, at least as far as reputational investment – an intangible – is concerned. Under the set of calibrated parameters, cyclical paths may entail higher average reputation capital than does the rest point solution of the model, although cyclical paths exist only if the agent cannot borrow or lend.====Second, in terms of pure theory, we establish the existence of a new class of equilibria. Repeated games are known to have equilibria that shift from period to period with no change in the payoff functions in the stage game – e.g., Green and Porter (1984), Mailath and Samuelson (2006, Sec. 4.3.1). In dynamic games with type uncertainty of which ours is an example, Markovian equilibria can also entail fluctuating effort – e.g., Board and Meyer-ter-Vehn (2013) and Cisternas (2018). With a risk averse agent, however, we show that the Holmström model acquires a cyclical equilibrium.==== Moreover, cyclical equilibria can arise even though the agent has a publicly known retirement date.====Third, we explore the model's quantitative properties using micro-data on earnings dynamics to calibrate its parameters. The variance coefficients can be recovered from Lange's (2007) estimates about the speed of employer learning. In the calibration of the model with stochastic cycles, income growth decomposes into small, fully persistent changes and larger, mean-reverting changes (see Fig. 7). These predictions are in line with recent evidence from U.S. panel data on earnings: GKOS and ABB detect significant deviations from lognormality, with a small but noticeable share of individuals experiencing very large changes. They also find that the degree of persistence appears to be non-linear in the size of the shocks as large shocks exhibit much stronger mean reversion than small shocks. Hubmer (2018) shows that a life-cycle version of the standard job ladder model can capture the large negative skewness and high excess kurtosis of earnings data. Our analysis indicates that endogenous fluctuations in effort can also help to explain these large deviations from the log-normal framework, while addressing the systematic correlation between the persistence and the size of income shocks.====Finally, the model can generate a ==== relation between exogenous income noise and the volatility of earnings: Income can become more volatile when reputational concerns rise in response to a reduction in the noise accompanying the relation between effort and output. Agency theory suggests that a reduction in noise raises the incentives to provide effort, and we show that this can trigger a rise in the volatility of earnings. For cycles in effort to exist, incentives to provide effort cannot be too weak. Noise with which the agent's output is observed deters effort and it also obstructs the formation of cycles. This bears on the question of why income inequality has risen – Song et al. (2015), henceforth SPGBW. Paradoxically, we find that an increase in inequality could have in part been caused by a ==== in exogenous volatility. In particular, a decline in the noisiness of technology will strengthen reputational concerns and may raise equilibrium income volatility. Hence what SPGBW measure as a rise in the volatility of observed income may in fact be the result of a decline in exogenous output noise.====The plan of the paper is as follows. Section 2 lays out the model. Section 3 characterizes the rest point of the model where effort remains constant, while Section 4 explains how and when cycles may emerge. We parametrize the model in Section 5 and explore its quantitative implications. Then we review major hypotheses for endogenous cycles in Section 6.1 so as to highlight the novelty of reputation cycles. Several extensions are discussed in Section 6.2, while Section 7 concludes. All the proofs are relegated to the Appendix.",Reputation and earnings dynamics,https://www.sciencedirect.com/science/article/pii/S0022053120301514,18 November 2020,2020,Research Article,160.0
Heumann Tibor,"Instituto de Economía, Pontificia Universidad Católica de Chile, Chile","Received 19 May 2019, Revised 5 November 2020, Accepted 6 November 2020, Available online 17 November 2020, Version of Record 20 November 2020.",https://doi.org/10.1016/j.jet.2020.105156,Cited by (1),"There is a continuum of agents, each of whom trades a divisible asset via demand function competition. Individual valuations are determined by payoff shocks that are correlated across agents. Agents observe multi-dimensional signals about the payoff shocks; it is only assumed that the signals are normally and symmetrically distributed. We give three results about this economy. First, an equilibrium exists. Second, the equilibrium is constrained inefficient; a higher total surplus could be attained if agents submitted different demands. Third, a constrained-efficient outcome can be implemented by setting an appropriate capital-gains tax. The second result identifies a new type of inefficiency that only arises when agents observe multi-dimensional signals; the third result identifies the taxation policy that allows correcting this inefficiency.","In financial markets, an agent's valuation of an asset is determined by a combination of multiple asset-specific shocks (e.g., dividend shocks) and multiple agent-specific idiosyncratic shocks (e.g., shocks to the agent's hedging needs, liquidity needs, regulatory constraints, etc.). It is natural to assume that agents observe different signals about the different shocks that affect their valuation. Thus, it is also natural to assume that agents observe multi-dimensional signals.====The presence of multi-dimensional signals suggests that, in general, there will be a limit to how much information can be aggregated by the asset's price. If all information were public, then the equilibrium price would reflect the agents' average expected valuation, and the quantity they purchase would reveal the idiosyncratic component of their valuations. However, this outcome will seldom be achieved if agents observe multi-dimensional private signals about the various shocks that determine their valuation. For example, efficiently distributing the asset across agents requires that each agent's demand be perfectly correlated with the idiosyncratic component of their valuations. Yet, in this case, the asset's price will not reflect the agents' private information about asset-specific shocks; after all, such information is not reflected in the agents' demand for the asset. Hence, the price would not reveal the agents' average expected valuation.====In this paper, we address the following questions. Are the trading strategies used by agents efficient? What taxation policy (if any) can increase the total surplus?====The model consists of a continuum of agents trading a divisible asset under demand function competition. The asset is supplied by a competitive supplier that has increasing marginal costs. Agents bear a quadratic asset-holding cost and an agent's marginal valuation is determined by a shock, which we refer to as the payoff shock. The payoff shocks can exhibit any degree of correlation across agents; the fact that payoff shocks are not perfectly correlated captures the fact that the valuation is determined by asset- and agent-specific shocks. Each agent observes multi-dimensional private signals about each of the payoff shocks. Agents are symmetric, and the information structure is Gaussian; we impose no additional assumptions on the information structure. We study the symmetric linear equilibria of the demand function competition game. The model is mathematically equivalent to a rational expectations equilibrium; it is a classic model of trading under asymmetric information.====Our first theorem states that a linear equilibrium exists. This result does not rely on any assumptions about the information structure beyond symmetry and normality. To the best of our knowledge, the existence of an equilibrium in a linear-quadratic demand function competition game has been established only for the case of agents observing either one-dimensional signals or certain types of two-dimensional signals (examples from the literature are discussed in the paper). Our existence proof differs from those found in the literature because the multi-dimensional nature of the information structure impedes explicitly solving for the coefficients in a linear equilibrium. Instead, we write the equilibrium conditions as a polynomial system of equations, and then reduce these equations to finding the roots of a single-variable polynomial of degree ====, where ==== is the number of signals each agent observes.====We then examine whether the equilibrium outcome is efficient. To this end, we compare the total surplus generated by an equilibrium – that is, the sum of the agents' welfare and the profits of the competitive supplier – with the maximum surplus that can be generated across all linear demand functions. The set of all linear demand functions provide a benchmark in which the trading mechanism remains fixed, but agents' behavior is not constrained to be an equilibrium. This benchmark describes the set of outcomes that might be achieved by a policy, such as implementing tax instruments, that modifies agents' incentives but does not change the trading mechanism. Our approach is akin to that of analyzing the solution to a game in which agents behave as a team (as in Vives (1988); Angeletos and Pavan (2007)).====Our second theorem shows that every linear equilibrium of the game is generically inefficient. More precisely, for almost every value of the supply's slope, the total surplus generated by an equilibrium demand function is strictly less than the maximum surplus that can be attained by linear demand functions. The equilibrium is inefficient because the weight that the agents place on the different private signals they observe is different than that in the surplus-maximizing demand function. This type of inefficiency arises only when agents observe multi-dimensional and is thus different from those identified by the literature (which we explain in due course).====Our third theorem shows that every surplus-maximizing demand function is the equilibrium of an economy in which the agents' payoffs are subject to a tax. Hence, the newly identified inefficiency can be corrected using an appropriate type of tax. This tax reduces an agent's profits (resp. losses) when he buys an asset at a price lower (resp. higher) than the first-best price. We interpret this instrument as a capital gains tax and as a tax benefit that allows for the discounting of capital losses. The intuition underlying this result is that a positive capital-gains tax reduces the gains from buying (or selling) an asset because the price does not perfectly reflect the average valuation. Thus, such a tax, increases how much agents trade to satisfy their idiosyncratic needs.====A remarkable property of this tax is its simplicity. The equilibrium is inefficient because the weights that the agents place on the different signals they observe is different than the socially optimal ones. Hence, one might expect that it would be necessary to impose taxes that depend on the realization of specific signals, which would lead to a number of tax instruments that increase with the number of signals. However, this is not the case; a single tax is sufficient to implement an efficient outcome.",Efficiency in trading markets with multi-dimensional signals,https://www.sciencedirect.com/science/article/pii/S0022053120301496,17 November 2020,2020,Research Article,161.0
"Kim Jaehong,Li Mengling,Xu Menghan","The Wang Yanan Institute for Studies in Economics (WISE), Xiamen University, Xiamen, 361005, China,Department of Economics, School of Economics, Xiamen University, Xiamen, 361005, China","Received 28 April 2019, Revised 5 November 2020, Accepted 12 November 2020, Available online 17 November 2020, Version of Record 23 November 2020.",https://doi.org/10.1016/j.jet.2020.105159,Cited by (7),"The ever-increasing shortage of organs for transplantation has motivated many innovative policies to promote the supply of organs. This paper proposes and analyzes a general class of deceased organ allocation policies that assign priority on organ waiting lists to voucher holders to promote deceased donor registration. Priority vouchers can be obtained by self-registering for donation or through family inheritance. In an overlapping generations framework, we find that extending the donor priority benefits to future generations can improve the aggregate donation rate and social welfare. In particular, giving higher priority to voucher inheritors who register for donation is always beneficial regardless of the levels of population growth and care for future generations. By contrast, the efficacy of granting priority to nondonors with inherited vouchers depends on these two sociodemographic factors because of potential free-riding incentives.","Organ transplantation is the most effective treatment for end-stage organ failure. However, the shortage of transplantable organs is a severe global problem.==== Because of both legal constraints and ethical concerns, monetary compensation for living or deceased organ donations is not feasible for promoting organ supplies in most parts of the world (Becker and Elias, 2007; Roth, 2007; Gordon et al., 2015; Healy and Krawiec, 2017). Hence, the supply of transplantable organs depends solely on voluntary donations. Thus, the question of how nonfinancial instruments can be used to promote organ donation is extremely important for both saving lives and improving social welfare.====The goal of this paper is to introduce and analyze a general class of voucher mechanisms to incentivize organ donation and enhance social welfare. In practice, most deceased donations are not directed to specific patients but are instead allocated through a priority queue or waiting list. The design of organ allocation priority rules can affect the scarcity of organs by influencing the incentives to register for deceased donations (Kessler and Roth, 2012).==== Promoting deceased donations is vital for several reasons. First, deceased donations account for the majority of the total organ supply, approximately 62% in the United States in 2019,==== whereas living donation rates appear to have stagnated or even declined (Rodrigue et al., 2013). Second, a deceased donor generates considerably more benefits by providing multiple organs.==== Third, concerns are emerging over the long-term risks of living donations (Mjøen et al., 2014).====The organ allocation mechanisms considered in this paper include the donor priority rule as a baseline policy, which provides priority on waiting lists to previously registered donors in the event that they need organ transplants.==== Both in theory and in the laboratory, Kessler and Roth (2012) find a significant positive impact of the donor priority rule on deceased donor registration. This rule was implemented in Singapore in 1987 (Iyer, 1987) and was introduced in Israel in 2008 (Lavee et al., 2010), in Chile in 2013 (Zuniga-Fajuri, 2015) and, more recently, in China in 2018.==== This rule has also been consistently proposed in other countries, including the United States (Chan, 2020), the United Kingdom (Gray, 2013), and Canada (Burkell et al., 2013).====This paper generalizes the notion of donor priority to ==== to analyze a broad class of organ allocation mechanisms with vouchers, as motivated by several recent policy initiatives that are currently in practice or proposed. While the donor priority rule gives higher priority to the donor, it is reasonable to consider the extension of such a priority benefit to one's family members. Indeed, this idea is featured in many practical policies. For deceased organ allocations, current policies in Israel and China grant priority to a (registered) donor's family members in addition to the donor (Lavee et al., 2010; Quigley et al., 2012).==== Similar extended family benefits are featured in the advanced donation program for kidney exchanges in the United States. The program allows a living kidney donation to be performed in advance and then generates a priority voucher to be redeemed later by a specified recipient when needed (Veale et al., 2017).==== Another prominent example is the family replacement program for blood donations in China and other countries. During blood shortage periods, a patient in need of blood can be given the option to recruit family members or friends to donate blood to enable the blood to be used immediately (Sun et al., 2016; WHO, 2016). More recently, Kominers et al. (2020) propose and analyze similar voucher-based incentive schemes for plasma donations from recovered COVID-19 patients.====To take into account these extended family benefits, we develop a general organ donation and allocation framework where anyone registered as a deceased organ donor receives a priority voucher that can be redeemed for higher priority in receiving organs by either the donor or his family members. The notion of priority vouchers allows us to consider a broad class of organ allocation mechanisms with vouchers that feature various terms and conditions with regard to voucher durations, redemption policies, and transfer rules, as well as the extent of the priority granted to beneficiaries.====Specifically, we analyze and compare a parametric family of voucher mechanisms in a unified framework to increase organ donation by providing greater flexibility and increased opportunities for donation. These mechanisms are characterized by the extent of allocation priority assigned to individuals based on their donation status and voucher inheritance status. Among this general class of mechanisms, two typical examples are closely related to existing policies in different countries. In our benchmark mechanism, a ==== can be redeemed only by the donor, which is similar to the donor priority studied by Kessler and Roth (2012) and has been implemented in countries such as Singapore and Chile.==== The second type, referred to as an ====, provides greater flexibility and coverage such that the voucher may be either redeemed directly by the donor or later inherited by his child. The idea of an extended voucher jointly captures the donor priority and extended family benefits, as in the Israeli and Chinese organ allocation policies, from a broad perspective (Lavee et al., 2010), where allocation priority is granted to registered donors and voucher inheritors regardless of their donation status. More generally, organ allocation mechanisms can distinguish inherited vouchers from self-generated vouchers and, at the same time, allow for additional priority gains when individuals holding inherited vouchers take further steps to register for donation. The latter essentially incorporates a “renewal” feature in the sense that an inherited voucher is more rewarding when the inheritor “renews” its validity by enrolling as a donor, and it can potentially serve as an additional incentive device in terms of stimulating the aggregate donation incentives. Accordingly, we characterize a family of ====, which are indexed by the degrees of priority extension to a voucher inheritor based on his donation status.====The main objectives of this paper are to compare the equilibrium donation rates and aggregate welfare under the parametric family of extended voucher mechanisms relative to the basic voucher mechanism and to delineate the conditions under which an extended voucher mechanism achieves superior performance. To account for the inheritance feature of the extended voucher mechanisms, our dynamic model considers organ donation decisions with overlapping generations (OLG) and dynastic utility functions.====Our main findings from the theoretical framework are threefold. First, we establish that the aggregate donation rate in an extended voucher mechanism can always be improved relative to the basic voucher mechanism by giving higher priority to voucher inheritors who register for donation. In contrast, the impact of granting priority to nondonors with inherited vouchers, such as the current practice in China and Israel, crucially depends on the sociodemographic factors, including the level of population growth and the degree of care for future generations (Theorem 1). The underlying intuition highlights the economic force at the heart of our analysis throughout this paper. An extended voucher obtained by self-registering for donation can either benefit the donor himself with an improved rank on the waiting list or his offspring through voucher inheritance. This enlarged coverage of an extended voucher leads to an immediate improvement over the basic voucher in terms of incentives to donate. However, in the absence of a voucher renewal requirement, the resulting receipt of a voucher for free can reduce one's donation incentive to the extent of the priorities granted to nondonors holding inherited vouchers. In other words, the inheritance feature of extended vouchers may lead to a “free-rider” problem, whereby individuals enjoy a higher priority in receiving organ donations without contributing to the total organ supply. When the population growth rate is high, the potential free riders become a small part of the population. Hence, the negative impact from free-riding incentives can be mitigated by extending the benefits to offspring. Meanwhile, with a high degree of care for future generations (i.e., parental altruism), the gain to the indirect beneficiaries of one's donation decision also reduces the intention to free ride.====Second, we show that the welfare consequence of giving higher priority to voucher inheritors in an extended voucher mechanism is always positive (Theorem 2). Specifically, the aggregate welfare effects can be decomposed into a direct increase in the expected value of extended vouchers and an indirect change in the expected utility of nonvoucher inheritors. While the indirect effect may be negative because of crowding-out by free riders in the competition for organ allocations, we show that the direct positive effect of voucher value increment always dominates. Finally, we find that a policy change from the basic voucher mechanism to an extended voucher mechanism always improves the aggregate donation rate and welfare in the short term (Theorem 3). This improvement is due to the absence of free riders in the initial stages after introducing an extended voucher policy.====To facilitate more concrete discussions on practical policy designs, we further evaluate different voucher mechanisms based on numerical simulations by calibrating our model using U.S. heart donation and transplantation statistics. The simulation results from these numerical exercises not only corroborate our theoretical analysis but also provide a better understanding of the efficacy of different voucher mechanisms from a practical perspective. More importantly, we explore the implications of different levels of population growth and care for future generations that correspond to the situations in different countries for optimal voucher mechanism design in terms of incentivizing the donation rate or improving welfare. These discussions can potentially offer important insights into and practical guidance on the effectiveness of implementing voucher mechanisms in different countries.====Overall, our theoretical predictions and numerical results support voucher renewal by inheritors. This finding resolves criticisms of Israel's policy of granting priority to candidates on organ waiting lists who have not enrolled as deceased organ donors themselves but have immediate family who have done so (Lavee and Brock, 2012; Quigley et al., 2012). It also provides an economic rationale for recent proposals to eliminate such proxy priority benefits (Zaltzman, 2018; Berzon, 2018).====The remainder of this paper proceeds as follows. In the rest of this section, we review the related literature and discuss our contributions. Section 2 presents the model and the benchmark mechanism. In Section 3, we introduce a general class of extended voucher mechanisms and analyze a specific example. Section 4 analyzes the parametric family of extended voucher mechanisms and Section 5 provides simulation results. Section 6 concludes and discusses possible model extensions. All of the proofs are in Appendix A.",Organ donation with vouchers,https://www.sciencedirect.com/science/article/pii/S0022053120301526,17 November 2020,2020,Research Article,162.0
Rahi Rohit,"Department of Finance, London School of Economics, Houghton Street, London WC2A 2AE, UK","Received 17 June 2020, Revised 3 November 2020, Accepted 6 November 2020, Available online 12 November 2020, Version of Record 17 November 2020.",https://doi.org/10.1016/j.jet.2020.105155,Cited by (3),"We study the market for a risky asset with uncertain heterogeneous valuations. Agents seek to learn about their own valuation by acquiring ==== and making inferences from the equilibrium price. As agents of one type gather more information, they pull the price closer to their valuation and further away from the valuations of other types. Thus they exert a negative learning externality on other types. This in turn implies that a lower cost of information for one type induces all agents to acquire more information. Private information production is typically not socially optimal. In the case of two types who differ in their cost of information, we can always find a Pareto improvement that entails an increase in the aggregate amount of information, with a higher proportion produced by the low-cost type.","We study the market for a risky asset in which agents have private correlated valuations for the asset. Each agent collects private information about his own valuation, and the equilibrium price reflects some of this information. Our aim is to investigate the externalities that arise in this setting, and in particular how they affect the equilibrium allocation of private information and the welfare of market participants.====Heterogeneity in valuations can be due to different uses that agents have for the asset, motivated by speculation, hedging or liquidity considerations, because of differing investment opportunities or constraints, or for purely behavioral reasons. Alternatively, we can think of the agents as producers in different industries, and interpret the asset as an input into an industry-specific stochastic production technology.==== In these examples, each agent belongs to a group (e.g. producers who operate in the same industry) that is distinguished by its own uncertain valuation for the asset, has access to private information about this valuation, and seeks to glean the wisdom of the crowd regarding the same valuation from the equilibrium price.====We analyze competitive rational expectations equilibria in a linear-normal model. To understand the mechanics of this model, suppose there are two types of agents, with uncertain valuations ==== and ====. Agents of type ==== (====) choose the precision ==== of a private signal about their valuation ====, at a cost that is increasing in the precision. For any given choice of precisions, ==== and ====, the price function takes the form ====, for some constant ====.==== The optimal choice of ==== by agents of type ==== in turn depends on how much these agents learn about ==== from the price.==== Assuming that the correlation between ==== and ==== is nonnegative, agents of type 1 learn more, and agents of type 2 learn less, about their own valuation the greater is the ratio ====.====Now consider an equilibrium ====, and suppose there is a decrease in the cost of information for type 1. This induces type 1 agents to collect more information (increasing ====), thus reducing price informativeness for type 2. As a result, type 2 agents collect more information as well (increasing ====). This reinforces the incentive of type 1 agents to accumulate more information, increasing ==== even further. The resulting feedback loop leads to an equilibrium in which both types gather more information. The effect is more pronounced for type 1 agents: ==== is higher at the new equilibrium. Consequently, type 1 agents learn more from the price and type 2 agents learn less.====More generally, this ==== property, whereby a lower cost of information for one type results in more information production by all types, holds if the economy exhibits strategic complementarities in information acquisition. At the new equilibrium, the type whose cost is reduced learns more from the price while all other types learn less. There are strategic complementarities in the two-type case if and only if the correlation between the valuations of the two types exceeds a (negative) lower bound ====. With arbitrarily many types, strategic complementarities arise if pairwise correlations exceed ==== and, in addition, do not vary too much.====Next, we turn to the question of social optimality of private information acquisition. In particular, we examine the welfare effects of a change in the precision vector ==== in the neighborhood of an equilibrium. There are two factors at play here. All else equal, agents are better off if they are better informed. At the same time, they stand to gain more from trade the greater the distance between their own valuation and the overall market valuation, given by the equilibrium price. For each type, the overall welfare effect can be written as the sum of a ==== and a ====.====There is a fundamental tension between these two effects. The price ==== is more informative about the valuation ==== only if it tracks ==== more closely. To take an extreme example, if ====, the price is fully revealing for type ====, but type ==== agents gain nothing from trading at this price; indeed, their optimal trade is zero.====This tradeoff can be seen most clearly in a symmetric economy in which all types have the same cost of information and the correlation between valuations is the same for any pair of types. Such an economy has a unique equilibrium at which all types choose the same precision. At this equilibrium, the learning and gains from trade effects are collinear but opposite in sign. Moreover, the gains from trade effect dominates the learning effect, so that the types that are better off after perturbing ==== are precisely those for whom price informativeness is lower. Price informativeness cannot be lower for all types, however, and hence a perturbation of ==== cannot make all types better off.====The possibility of a Pareto improvement arises in the non-symmetric case. We consider an economy with two types who differ in their cost of information. For example, suppose type 2 has the lower cost. Then there is a unique equilibrium at which ====. The low-cost type produces more private information as intuition would suggest. But a Pareto improving allocation of information can always be found. It entails more information production in the aggregate, a higher proportion of which is acquired by the low-cost type, i.e. a higher ==== and a higher ====.====To summarize, our comparative statics results imply that negative learning externalities across types lead to information acquisition decisions that are clustered together — if one type gathers more or less information, other types respond by moving in the same direction. Our welfare results show that this clustering is excessive from the social point of view.====A growing strand of literature starts from the premise that agents have correlated private valuations for a traded asset, and each agent has private information about his own valuation. In a seminal contribution, Vives (2011) studies strategic supply function competition among agents facing an uncertain cost.==== Vives (2014) uses a perfectly competitive version of this model to study information revelation in the market for a risky asset. Rostek and Weretka, 2012, Rostek and Weretka, 2015 extend the Vives (2011) setup to investigate the effect of market size on information aggregation and market power. Glebkin (2019) considers the case of two types, one of which consists of large strategic traders while the other is perfectly competitive, to analyze the interplay between liquidity and price informativeness. Bergemann et al. (2020) and Heumann (2020) introduce multidimensional signals into the Vives (2011) model; the first paper retains the strategic interaction of Vives (2011), while the second considers the perfectly competitive case. In Babus and Kondor (2018), dealers engage in bilateral trading in a network.====These papers employ a linear-Gaussian framework with exogenously specified valuations that vary across agents, just as in the present paper, but with more stringent assumptions on the correlations between these valuations. Vives, 2011, Vives, 2014, Bergemann et al. (2020), Heumann (2020), and Babus and Kondor (2018) assume that the correlations are the same for any pair of agents or agent types; this is also true in Glebkin (2019) since there are only two types. Rostek and Weretka, 2012, Rostek and Weretka, 2015 present a convincing argument for a general correlation structure, but restrict their analysis to the “equicommonal” case, wherein the average correlation between the valuation of a trader and that of the remaining traders is the same for all traders. Moreover, the symmetry assumptions imposed in all these papers ensure that price informativeness is the same for all agents, with the exception of Babus and Kondor (2018) who use an aggregate measure of constrained informational efficiency.==== This is a key difference with respect to our setup where price informativeness can change in opposite directions for agents with different valuations, and learning spillovers play an important role in both comparative statics and welfare.====Private information is exogenous in the papers cited above, apart from Vives, 2011, Vives, 2014. While these two papers differ in terms of market structure (imperfect vs perfect competition), the informational properties of the equilibrium are the same. As long as the marginal cost of information is sufficiently low, so that agents acquire at least some information, the price reveals the average signal, which for any individual agent is a sufficient statistic for the information of all other agents (Vives calls this a “privately revealing” equilibrium). In other words, there are no learning externalities in this setting. The perfectly competitive economy of Vives (2014) is ex post efficient regardless of how much information agents collect in equilibrium (this follows from the “private revelation” property and the first welfare theorem).====Learning externalities take center stage in Rahi and Zigrand (2018) (henceforth RZ), which serves as our point of departure, and from which we borrow some results (Lemma 2.1, Proposition 2.2 and Lemma 3.1) on rational expectations equilibrium with exogenously given signal precisions. The two papers diverge on the question of private information production. RZ study a binary information acquisition decision, wherein agents either acquire a piece of information at some cost or remain uninformed. In the present paper, we allow agents to choose the precision of their signal at a cost that increases in the precision. We assume that there is no fixed cost so that all types acquire some information. As such, our results complement those of RZ. Our welfare results, in particular, provide a different perspective on the Pareto inefficiency of the equilibrium allocation of private information. RZ show that discouraging information production can be Pareto improving if private signals are sufficiently noisy. In this paper, in contrast, the precision of private signals is endogenous, and there always exists a Pareto improvement that involves an increase in the total amount of information. We discuss this point further at the end of Section 5.====There is a large literature on the social value of public information in a pure exchange economy. More information can reduce risk-sharing opportunities (or, indeed, destroy them altogether, as in Hirshleifer (1971)). If markets are incomplete, it can also allow agents to construct better hedges. The overall impact on agents' welfare can be in any direction (Gottardi and Rahi (2014)). Much less is known about the welfare properties of asset markets in which information is endogenous and asymmetric. Most of the rational expectations literature relies on exogenous noise trade and hence does not provide a suitable framework for welfare analysis. There are some papers with fully optimizing traders but, apart from a few exceptions,==== they do not ask if the amount of information produced by agents is socially optimal.====A number of papers feature a complementarity in information acquisition that arises because prices become less informative as more agents acquire information. The underlying mechanism differs across these papers. In Barlevy and Veronesi, 2000, Barlevy and Veronesi, 2008 price informativeness falls with the incidence of informed trading because the asset payoff is negatively correlated with the noise trade, in Ganguli and Yang (2009) and Manzano and Vives (2011) because agents have two sources of information (about the asset payoff and the asset supply), in Goldstein et al. (2014) because agents with different investment opportunities trade on the same information in opposite directions, and in Breon-Drish (2012) due to non-normality of shocks. These papers use complementarity in information acquisition as a vehicle for generating multiple equilibria. In contrast, strategic complementarities in our paper actually describe the “well-behaved” case — if there are two types (which is the closest analog to the papers discussed here), the economy exhibits strategic complementarities if and only if the correlation between the valuations of the two types is not too negative; moreover, for such an economy, there is a unique equilibrium. The issue is partly a terminological one. We use strategic complementarities as a label to describe the case where higher information production by one type lowers price informativeness for the other type. But this is precisely the case in which higher information production by a given type ==== price informativeness for that type itself. It is worth noting that the multiplicity of equilibria in Rahi and Zigrand (2018), which has the same flavor as the multiplicity found in the papers discussed above, requires a within-type complementarity which is ruled out by the across-type complementarity assumption that we make in this paper.====Another line of research investigates the interaction between private and public information in a coordination setting in which agents wish to align their actions with the actions of others. Some of this work touches on themes that run through the present paper. For example, Colombo et al. (2014)) show that increasing the informativeness of an exogenous public signal reduces the incentive of agents to collect private information. But our framework is quite different from theirs. The asset trading stage in our model cannot be reduced to a coordination game, and the public signal is the equilibrium price, which is endogenous. The welfare problem that we analyze is also more involved as agents in our setting are not ex ante identical; welfare depends not only on how much information is produced but also by whom it is produced.====We now lay out a brief road-map for the rest of the paper. In the next section, we describe the basic setup and the price function for given signal precisions for each type. We endogenize these precisions in Section 3. In Section 4 we characterize equilibrium for the two-type case. A welfare analysis follows in Section 5. In Section 6, we provide sufficient conditions on the primitives for the economy to exhibit strategic complementarities. This forms the basis of the existence and comparative statics results in Section 7. Proofs are in the Appendix.",Information acquisition with heterogeneous valuations,https://www.sciencedirect.com/science/article/pii/S0022053120301484,12 November 2020,2020,Research Article,163.0
"Edmond Chris,Lu Yang K.","University of Melbourne, Australia,Hong Kong University of Science and Technology, Hong Kong","Received 14 February 2020, Revised 31 October 2020, Accepted 1 November 2020, Available online 12 November 2020, Version of Record 17 November 2020.",https://doi.org/10.1016/j.jet.2020.105145,Cited by (5),"We develop a model in which a politician seeks to prevent a group of citizens from making informed decisions. The politician can manipulate information at a cost. The citizens are rational and internalize the politician's incentives. In the unique equilibrium of the game, the citizens' beliefs are unbiased but endogenously noisy. We interpret the social media revolution as a shock that simultaneously (i) improves the underlying, intrinsic precision of the citizens' information, but also (ii) reduces the politician's costs of manipulation. We show that there is a critical threshold such that if the costs of manipulation fall enough, the social media revolution makes the citizens worse off despite the underlying improvement in their information.","Consider a politician who seeks to discredit information, to prevent people from becoming well-informed about an inconvenient truth. Can the politician achieve this goal even when people are rational and perfectly understand the politician's incentives? Should we be optimistic that new social media technologies will make it more difficult for the politician to discredit inconvenient reporting? Or will these new technologies make it easier for the politician to ====, frustrating people in their desire to be well-informed?====We develop a simple model to answer these questions. There is a collection of citizens each of whom seeks to form an accurate assessment of an underlying state using the sources of information available to them. An informed politician seeks to discredit the citizens' information, at a cost. The citizens are rational and internalize the politician's incentives.====We interpret the social media revolution as a shock that simultaneously: (i) increases the underlying, intrinsic ==== of the information available to the citizens, and (ii) decreases the ==== the politician incurs in manipulating information. We argue that these new technologies have led to new sources of information, both in the form of new media outlets and in the form of blogging and amateur journalism, thereby increasing the intrinsic precision of the information available to citizens, but that these new sources of information are not all subject to the same standards of accountability as traditional media and moreover are consumed in a feed that blurs distinctions between outlets and that makes it easier for all kinds of news, real and fake, to “go viral,” thereby reducing the costs of manipulation.====We find that the social media revolution can generate a “regime change” in the amount of manipulation: The net effect of the shock depends on whether the costs of manipulation can be kept above a critical threshold. If the intrinsic precision of information is high and the costs of manipulation fall below this critical threshold, the economy will enter a ====. In this high manipulation regime, the politician's manipulation prevents improvements in the intrinsic precision from passing through to citizens, making them worse off and the politician better off. But if the costs of manipulation can be kept above this critical threshold the economy will stay in a ====. In this low manipulation regime, the politician fails to prevent improvements in the intrinsic precision from passing through to the citizens, making the citizens better off and the politician worse off.====Section 2 outlines the model. There is a politician who knows the underlying state of the world. There is a continuum of citizens who share a common prior and receive idiosyncratic signals about the state. Each citizen wants to take an action that is appropriate for the state and the politician seeks to ==== them from doing so. Thus in contrast to standard political economy models, the citizens' and politician's interests are not even partially aligned. The politician has a technology that allows them to manipulate information by choosing the citizens' signal mean at a cost that is increasing in the distance between the true state and the signal mean. The citizens are rational and internalize the politician's incentives. To keep the model tractable, we assume quadratic preferences and normal priors and signal distributions. We study equilibria that are linear in the sense that the citizens' strategies are linear functions of their signals.====Section 3 solves the model and shows that there is a ==== (linear) equilibrium. In equilibrium, the citizens' signals are unbiased but are made endogenously noisier by the politician's manipulation. The equilibrium amount of manipulation can be very sensitive to parameters. If the costs of manipulation are high, increasing the intrinsic precision decreases the amount of manipulation and the citizens become more responsive to their signals than they would be if the politician could not manipulate at all. But if the costs of manipulation are low, increasing the intrinsic precision increases the amount of manipulation and citizens are less responsive to their signals than they would be in the absence of manipulation. Moreover we show that if the intrinsic precision of information is high there is a critical threshold for the costs of manipulation. At this threshold, a small change in the costs of manipulation causes a discontinuous ==== in the amount of manipulation, giving rise to the possibility of abrupt transitions between low manipulation and high manipulation regimes.====Section 4 contains our results on the welfare effects of a social media revolution, interpreted as a simultaneous increase in the intrinsic signal precision and decrease in the costs of manipulation. The net effect of the social media revolution depends crucially on the size of the reduction in the costs of manipulation. If the costs fall enough, the economy tips into the high manipulation regime, where no change in the intrinsic signal precision can compensate the citizens' for the welfare loss brought by the fall in the costs of manipulation. Indeed in the limit where the costs of manipulation become negligible, the politician's manipulation renders the citizens' signals completely uninformative even if the underlying, intrinsic precision of their signals is arbitrarily high. But if the costs of manipulation do not fall too much, the citizens eventually benefit from the increase in the intrinsic signal precision. In this sense, even small changes on the part of social media platforms that make it harder for misinformation to propagate may have large welfare effects.====Section 5 discusses two extensions of our benchmark model. First we outline a version of our model where citizens do not simply consume signals but rather consume media reports produced by journalists that have their own preferences that need not be perfectly aligned with the citizens. Each journalist cares both about reporting the truth and about how their report fits with other reports (their actions may be either strategic substitutes or complements). In this version of the model the politician is directly concerned with manipulating the journalists' information with the citizens affected as a byproduct. This setting gives rise to some new possibilities. For one, the politician's manipulation can ==== if there are sufficiently strong strategic interactions among the journalists, i.e., there are scenarios where the politician would value being able to ==== information. For another, we find that the citizens can benefit from the politician's manipulation if the journalists' actions are strong strategic substitutes and the intrinsic precision of their signals is sufficiently low. That said, regardless of the strength of strategic interactions among the journalists, the politician gains the most and the citizens lose the most when the costs of manipulation are low and the intrinsic precision of the signals is high, as in our benchmark model.====Second, we outline a version of the model where citizens have heterogeneous priors and where the politician can directly manipulate both the signal mean and the signal variance. We use this setting to analyze an alternative notion of ====, namely a reduction in prior dispersion. We again find that the politician's manipulation can prevent this better information from passing through to the citizens.==== Our model is intended to capture features of political messaging that used to be known by terms like “muddying the waters” but more recently has become known as the “politics of confusion”. Pomerantsev (2019) discusses such political messaging at length and explains how it has been used by authoritarian regimes around the world to consolidate power and to sow doubts about democratic institutions (see also Bennett and Livingston (2018) and Sunstein (2018) among others). In democracies, this form of political messaging has come under considerable scrutiny since the 2016 UK Brexit referendum and the 2016 US presidential election, as voters found themselves on the receiving end of a relentless deluge of spin and “alternative facts” especially as propagated via social media. More systematically, Bradshaw and Howard (2019) and Nyst and Monaco (2018) document evidence of organized social media manipulation by governments and political parties in 70 countries, covering both democracies and autocracies. According to these reports, the goal of the organized social media manipulation is to confuse the very notion of truth, sow seeds of distrust in the media, discredit criticism and oppositional voices, and drown out political dissent.====But there is also a competing, more benevolent, view of the role played by social media. After all it was not so long ago that the conventional wisdom was the other way round, arguing that social media is a force for transparency and democratic accountability (see e.g., Shafer (2010) on ====) and helping to bring about important social and political reforms (see e.g., Codrea-Rado (2017) and Rickford (2015) on ==== and ====).====Our model interprets these competing views in the following way. Absent manipulation, new social media technologies would allow people to make more informed decisions, making them better off. But in the presence of manipulation, these new technologies also reduce the politician's costs of manipulation, thereby creating a tension. Our results then provide a characterization of the net effects of this tension, allowing us to say when people will and will not be better off overall.==== Our model is a sender/receiver game with many imperfectly informed receivers.==== As in Crawford and Sobel (1982), the preferences of the sender and receivers are not aligned and the sender is informed. But as in Kartik (2009) we have ====, not cheap talk. By contrast with standard cheap talk models, our model with costly talk features a unique equilibrium. In the limit as the sender's distortion becomes ====, the unique equilibrium features a kind of ==== where the receivers ignore their signals. Our model with costly talk is related to Kartik et al. (2007) and Little (2017) but our receivers are not “credulous” or subject to confirmation bias.==== In equilibrium, our receivers have unbiased posterior expectations. Despite this, the sender still finds it optimal to send costly distorted messages. This is because of the effects of their messages on other features of the receivers' beliefs, as in the Bayesian persuasion literature following Kamenica and Gentzkow, 2011, Kamenica and Gentzkow, 2014. In particular, the sender can be made better off by the increase in the receivers' posterior variance resulting from the sender's messages. A crucial distinction however is that in Kamenica and Gentzkow (2011), the sender can ==== to an information structure and this commitment makes the model essentially nonstrategic in that their receiver only needs to solve a single-agent decision problem. Other approaches to information design, such as Bergemann and Morris (2016) also allow the sender to commit. By contrast, our sender ==== and chooses their message after becoming informed about the underlying state, as in Crawford and Sobel (1982).====Applications to political communication that follow the Bayesian persuasion approach in assuming the sender can commit include Hollyer et al. (2011), Gehlbach et al. (2014), Gehlbach and Simpser (2015) and Rozenas (2016). In terms of the sender not being able to commit, our model is more similar to Little, 2012, Little, 2015 and Shadmehr and Bernhardt (2015) but we differ from Little (2012) in that our sender is informed, and from Shadmehr and Bernhardt (2015) in that the sender uses a distinct information manipulation technology. Other related work includes Egorov et al. (2009), Edmond (2013), Lorentzen (2014), Huang (2015), Guriev and Treisman (2015), and Chen and Xu (2017). For overviews of this literature, see Svolik (2012) and Gehlbach et al. (2016).==== The media bias literature often assumes that receivers ==== distorted information==== — e.g., Mullainathan and Shleifer (2005), Baron (2006), Besley and Prat (2006), Gentzkow and Shapiro (2006), Bernhardt et al. (2008) and Martin and Yurukoglu (2017). Allcott and Gentzkow (2017) and Gentzkow et al. (2015) have used this kind of setup to explain how there can be a viable market for “fake news” that coincides with more informative, traditional media. To be clear, we view such behavioral biases as very important. Our point is that such biases are ==== for manipulation to be effective. In our model, the sender can still gain from sending costly distorted messages because of the endogenous noise that results from such messages.====Or to put things a bit differently, in our model no one is misled by the politician's “alternative facts” and yet the politician can benefit greatly from the ensuing babble and tumult.",Creating confusion,https://www.sciencedirect.com/science/article/pii/S0022053120301381,12 November 2020,2020,Research Article,164.0
Kopylov Igor,"Institute for Mathematical Behavioral Sciences, and Department of Economics, University of California, Irvine, CA 92697, United States of America","Received 31 January 2018, Revised 23 August 2020, Accepted 23 October 2020, Available online 4 November 2020, Version of Record 19 November 2020.",https://doi.org/10.1016/j.jet.2020.105132,Cited by (0)," observe ====: people are more ambiguity averse when they evaluate ambiguous and clear prospects jointly rather than in isolation. To accommodate such patterns, I relax the multiple priors model and allow subjective sets of priors to depend on ==== that are generated by feasible prospects. All sets of priors are derived uniquely on the corresponding partitions. This model violates transitivity, but preserves all other axioms of ====. A parsimonious special case identifies a ==== partition ==== such that all ====-measurable prospects are ranked via a set of priors ====, but all other comparisons rely on a distinct set ====. Another refinement derives partition-dependent probabilistic beliefs without imposing ambiguity aversion.","In the words of Fox and Tversky (1995, p. 588, henceforth FT), “ambiguity aversion will be present when subjects evaluate clear and vague prospects jointly, but it will greatly diminish or disappear when they evaluate each prospect in isolation.” Such persistent variations in ambiguity aversion are called ====. In one of FT's experiments, the average willingness to bet on Istanbul weather was 50% higher when such bets were evaluated in isolation rather than jointly with side bets on San Francisco weather. FT attribute this behavior to stronger feelings of ==== about the Istanbul weather in the joint evaluations. Similar patterns have been observed in other experimental settings (e.g. Fox and Weber, 2002; Chow and Sarin, 2001; Rubaltelli et al., 2010).====Comparative ignorance can potentially contribute to ====. For example, Iyengar et al. (2004) analyze 401(k) participation in various retirement plans managed by the Vanguard Group. They find that adding ten funds to the list of accessible investments is associated with 1.5 percent to 2 percent drop in participation rates. Such indecisiveness can be explained in part by comparative ignorance. It is plausible that contemplating many uncertain prospects jointly can make the DM feel less competent about each of them. Then she can reject all ambiguous investments in large menus in favor of some clear status quo, such as keeping money in the bank or using it for immediate consumption.====Comparative ignorance is incompatible with any utility function that evaluates each uncertain prospect independently of other feasible alternatives. Formally, variations in ambiguity aversion should produce ====. To illustrate, consider two urns with balls of two colors, red (====) or blue (====). The proportion of each color is unknown in the first urn, but is known to be precisely half in the second urn. Suppose that the DM is initially told about the first urn and reports a number ==== such that she is indifferent between a sure payment of $100 and receiving ==== contingent on ====,==== Then she is told about the second urn and reports a number ==== such that==== Finally, she compares the binary bets ==== and ====. If her contemplation of unambiguous bets on the second urn increases aversion to ambiguous bets on the first urn, then she should strictly prefer ==== to ==== and hence, exhibit an intransitive cycle,====To accommodate comparative ignorance, I relax the multiple priors model of Gilboa and Schmeidler (1989). I adapt the same primitives—preferences ⪰ over uncertain prospects with lottery payoffs contingent on a finite state space Ω. All axioms of Gilboa and Schmeidler are also preserved, except for transitivity.==== Weakening transitivity allows subjective sets of priors to vary together with the ==== ====—the coarsest partition of Ω that makes both feasible prospects ==== and ==== measurable.====My main result (Theorem 1) characterizes a ==== (PMEU) representation==== where ==== is an expected utility index, and each ==== is a closed and convex set of probability distributions on Ω. Each set ==== is derived uniquely on the corresponding partition ====. If transitivity holds, then (2) becomes the regular multiple priors model where ==== is invariant across all contexts ====.====I present several refinements of PMEU that do not assume transitivity. Consider a decision maker who first ranks ====-measurable prospects for some partition ====, and then makes all other comparisons. In the latter stage, she can feel less competent about events in ==== when she finds clearer events in the description of acts that are not ====-measurable. Theorem 2 characterizes the PMEU model where the function ==== has the form==== Here ==== is unique, ==== is unique on ====, and the ==== partition ==== is unique whenever ⪰ is not transitive. Theorem 3 obtains the set inclusion ==== that captures a monotonic form of comparative ignorance.====Variations in ambiguity aversion can result directly from different timing of choices rather than from contrasts between events. A common finding in psychology (e.g. Gilovich et al., 1993) is that individuals tend to lose confidence in their prospects as the realization of uncertainty approaches. In Section 4, I reinterpret PMEU in terms of temporal variations in ambiguity aversion.====Another general motivation for intransitive patterns is ==== in beliefs. In many empirical settings, events are perceived as more likely when partitioned into several arbitrary parts (Fischhoff et al., 1978; Fox and Birke, 2002). For example, Sonnemann et al. (2013) set up experimental markets for contingent assets ==== that pay $1 if some random variable (like the DAX index) belongs to an interval ==== of the real line. Their subjects first evaluate assets ==== in isolation and then trade ==== together with other contingent assets ==== and ==== such that ==== for disjoint ==== and ====. Sonnemann et al. observe the average evaluation of ==== to be substantially higher (by about 20-25 cents) in the market setting. Consider two contingent prospects ==== and ====. Then the preference ==== is plausible for people who are willing to pay more than the cash amount ==== for ==== in the market, but view ==== as less valuable than ==== in isolation. The ranking ==== is implied by monotonicity. Thus the intransitive cycle ==== is plausible.====Partition-dependent beliefs can be modeled via PMEU without assuming any ambiguity aversion. In Theorem 4, I adapt Dekel's (1986) Betweenness to characterize a special case of PMEU where each set ==== consists of a single probabilistic belief ====. The partition-dependent belief function ==== can be further refined with",Multiple priors and comparative ignorance,https://www.sciencedirect.com/science/article/pii/S0022053120301253,4 November 2020,2020,Research Article,165.0
Xiong Siyang,"Department of Economics, University of California Riverside, 900 University Avenue, Riverside, CA, 92521, United States","Received 17 September 2019, Revised 21 October 2020, Accepted 26 October 2020, Available online 4 November 2020, Version of Record 10 November 2020.",https://doi.org/10.1016/j.jet.2020.105133,Cited by (2),"Recently, a trend has developed around the world for referenda to be used to determine binary social decisions. In a couple of setups, we prove impossibility results of the following form: a binary social goal can be achieved via a referendum if and only if it is dictatorial. Hence, our results challenge the conventional wisdom in ==== theory that social decisions are permissive in two-outcome environments (May's Theorem).","Recently, a trend has developed around the world of referenda==== being used to determine social decisions, for example, the Scottish independence referendum in 2014, the Greek bailout referendum in 2015, the Italian constitutional referendum in 2016 and the UK Brexit referendum in 2016. Among these referenda, arguably, the most influential one has been the UK Brexit referendum, which was held on June 23, 2016, with those favoring an exit winning by 51.9% versus 48.1%.====Before the Brexit vote, a petition was set up by William Oliver Healey on the UK Parliament's website====: ====At the time when the Brexit result was revealed, 22 people had signed the petition. However, by June 25 (i.e., two days after the Brexit vote), more than 2.5 million people had signed the petition.====Suppose we were outside judges (such as the UK Parliament) who were called upon to assess the petition. What should the criterion be for us to make a judgment? The 2.5 million people proposed a new voting rule to replace the original rule because, presumably, they thought the original rule was bad–this naturally leads to the following questions: In what sense is the original voting rule bad? In what sense is the proposed new voting rule good? Does there even exist a good voting rule? In this paper, we aim to provide results which shed light on these questions.====We model a referendum as an implementation problem ==== Maskin (1999),==== which is described by ====. Specifically, ==== is a set of two social outcomes, of which one should be collectively chosen by all people in a society. Every agent in the society has her==== own strict preference on ==== and ====, that is, she strictly prefers either ==== to ==== or ==== to ====. A state is a preference profile that specifies the strict preference of every agent, and ==== denotes the set of all possible states. The function, ====, is called a ==== (hereafter, SCF), which specifies the socially desirable outcome for each possible state in ====. Thus, the goal of a mechanism designer (e.g., the government) is to select ==== whenever ==== is the true state. However, the mechanism designer may not be able to directly observe the true ====. Furthermore, agents are not obliged to reveal it truthfully. Hence, the mechanism designer has to design an appropriate (voting) mechanism so that incentive compatibility of agents under any true ==== would enforce the agents to take the targeted strategies in the game, and ==== equilibria of the game induce the desired outcome ====.====We first propose a general definition of “voting mechanism,” which covers almost all of the currently used voting rules. Our main result (Theorem 1) is a simple (albeit general) impossibility result: an SCF ==== is implementable in Nash equilibria if and only if ==== is dictatorial.==== To some extent, this impossibility result may not be too surprising (to some people): after all, it is easy to show that the majority voting rule cannot Nash-implement a non-dictatorial SCF. However, Theorem 1 has three merits. First, it says that, besides the majority voting rule, a large class of voting rules (called “voting mechanisms”) cannot Nash-implement a non-dictatorial SCF. Second, it identifies a property of voting rules, called voting monotonicity (Definition 1), that drives the impossibility result. Third, it can be generalized to much more general setups, e.g., incomplete information, other equilibrium notions (e.g., correlated equilibria) (see Xiong (2019)). In Section 4, we also discuss how the impossibility result adapts to setups in which some critical assumptions are relaxed (e.g., dominant-strategy implementation, stochastic mechanisms).====The remainder of the paper proceeds as follows: we describe the model in Section 2 and present the impossibility result in Section 3. We discuss several critical assumptions in Section 4, and review the literature in Section 5. We conclude in Section 6.",Designing referenda: An economist's pessimistic perspective,https://www.sciencedirect.com/science/article/pii/S0022053120301265,4 November 2020,2020,Research Article,166.0
"Roy Souvik,Sadhukhan Soumyarup","Economic Research Unit, Indian Statistical Institute, Kolkata, India","Received 2 July 2019, Revised 14 October 2020, Accepted 23 October 2020, Available online 1 November 2020, Version of Record 28 September 2021.",https://doi.org/10.1016/j.jet.2020.105131,Cited by (2),"We show that a large class of restricted domains such as single-peaked, single-crossing, single-dipped, tree-single-peaked with top-set along a path, Euclidean, multi-peaked, intermediate (====), etc., can be characterized by using betweenness property, and we present a unified characterization of unanimous and strategy-proof random rules on these domains. As corollaries of our result, we show that all the domains we consider in this paper satisfy tops-onlyness and deterministic extreme point property. Finally, we consider weak preferences and provide a class of unanimous and strategy-proof random rules on those domains.",None,A unified characterization of the randomized strategy-proof rules,https://www.sciencedirect.com/science/article/pii/S0022053120301241,1 November 2020,2020,Research Article,167.0
"Bommier Antoine,Schernberg Hélène","ETH Zurich, Switzerland","Received 19 October 2018, Revised 15 June 2020, Accepted 15 October 2020, Available online 28 October 2020, Version of Record 6 November 2020.",https://doi.org/10.1016/j.jet.2020.105126,Cited by (0),"We study the demand for retirement income of agents who gradually learn about their life expectancy. For a given expected budget, temporally risk-averse agents prefer that pension levels respond to incoming information about life expectancy rather than being fixed ex-ante. Indeed, this offers a hedging strategy that couples shorter lives with higher consumption levels, and longer lives with lower consumption levels. A calibrated life-cycle model provides an order of magnitude of the effects at play.","If you recently began to contribute to a (public or private) pension system, retirement is far away. Over the years, you will accumulate information regarding your health, your exposition to environmental or industrial risks, and any factor affecting the duration of your life. This will allow you to regularly update the prediction of your life expectancy.====Should your future pension benefits reflect this learning? If the financial consequences of uncertain survival rates can neither be shared among policyholders nor insured, then surely yes. Budget constraints would require that benefits decrease when life expectancy increases. What if pension providers could remove all idiosyncratic risks by pooling and insure the remaining aggregate risk? Then, they could offer fixed pensions that do not react to new information. Whether this is optimal is another story. The insurance literature reminds us that individuals may prefer insurance contracts whose state-contingent payments provide a hedge against the welfare impacts of the underlying risk. We illustrate how this basic insurance mechanism affects the optimal design of a pension system when uncertainty bears on one's future life expectancy.====We study an agent's demand for retirement income when she gradually learns how long she may live. Importantly, we consider flexible preferences that disentangle the elasticity of intertemporal substitution from risk aversion. With standard, time-separable preferences, the agent prefers pension benefits that are frozen ex ante. Such preferences, however, constrain risk aversion to equal the inverse of the elasticity of intertemporal substitution. We show that a more risk-averse agent disagrees: she prefers retirement benefits that respond to new information regarding her life expectancy. In fact, the optimal pension plan offers income profiles whose level and shape depend on her future mortality prospects. “Bad news”, that is a lower life expectancy than predicted, are compensated with higher retirement incomes that decline more steeply over time. Conversely, “good news” entail lower pensions that decrease more slowly. These flexible retirement benefits partially hedge the risk on life expectancy. This increases the individual's ex ante welfare.====Moving towards flexible pension systems raises some concerns: this could increase the poverty rate among the elderly, in particular for the lucky ones who expect longer lives. Our quantitative illustration suggests an order of magnitude of the effects at play. It uses realistic mortality rates and plausible preference parameters. Although perceptible, the adjustments remain moderate (a few percent): individuals with good survival prospects do not fall into poverty.====Flexible pension systems appear to be a relevant policy tool. They increase the welfare of pensioners without increasing the average pension payments. Moreover, they create a negative correlation between the number of years lived and the yearly benefits. Thus, flexible pension systems may generate a valuable reduction of the risk borne by pension providers. Our work also emphasizes that properly investigating optimal pension systems requires moving from the standard time-separable preferences towards more flexible frameworks that isolate the role of risk aversion.====  This paper relates to various lines of research. First, it borrows from the literature on intertemporal choice and life-cycle theory. More precisely, it draws from theoretical papers advocating flexible representations of preferences that disentangle risk aversion and intertemporal substitution. We refer to Kihlstrom and Mirman (1974), Epstein and Zin (1989), or Hansen and Sargent (1995) for seminal contributions. These models are now commonplace in finance and macro-economics, most often to describe the preferences of an infinitely-lived representative agent. Yet, they also provide new insights in other domains, such as the life-cycle theory, household finance and the economics of aging. Increasing risk aversion naturally enhances the impact of all risks, and mortality in particular. Properly accounting for risk aversion improves our understanding of saving behaviors, portfolio choices, the willingness to pay for mortality risk reductions and the impact of mortality changes (see Bommier 2006, 2013). In this paper, we investigate a particular aspect bearing on the demand for annuities (or retirement income) when life expectancy is uncertain.====Second, this paper relates to the literature on the demand for insurance with state-dependent utilities (see Cook and Graham, 1977, or Drèze and Rustichini, 2004). In a life-cycle model with uncertain lifetime, utility is fundamentally state-dependent. Indeed, the enjoyment of consumption depends on whether one is alive or dead. The seminal contribution of Cook and Graham (1977) clarifies that, in the state-dependent case, the demand for insurance differs from what state-independent preferences predict. In particular, the demand for insurance may decrease (and not increase) with risk aversion. This apparently counter-intuitive prediction is reflected, for example, in the demand for annuities (see Bommier and Le Grand, 2014). In this paper, comparing time-separable preferences with more flexible specifications shows that increasing risk aversion shifts the optimum from fixed to flexible pensions. Thus, more risk-averse agents opt for more uncertainty on their retirement income because this hedges the risk on their life expectancy.====This paper also indirectly relates to a literature discussing inequality in intertemporal settings with uncertainty (see Hammond, 1981, 1983 and Fleurbaey, 2010). Here, we do not discuss these redistributive aspects. Yet, we show that when risk aversion is properly accounted for, the optimal pension system provides higher pensions for those with low life expectancies, and the other way around. This reduces the welfare gap between long-lived and short-lived individuals, and reduces (ex post) inequalities. Moreover, the life expectancy gap typically correlates with a wealth gap: wealthier individuals are more likely to enjoy longer lives. For example life expectancy at birth in 2015 was below 60 years in some deprived neighborhoods of Chicago, but reached 90 in wealthy ones.==== Thus, with our flexible pension system, it is generally the wealthiest, who enjoy longer lives, whose pensions may decrease.====Finally, this paper complements the actuarial and financial literature that considers indexed annuities as solutions for the management of the aggregate longevity risk (see Richter and Weber, 2011, or Denuit, Haberman and Renshaw, 2011, or Milevsky, 2015). The current paper focuses on the demand side, assuming that the aggregate longevity risk can be either fully shared across agents or insured. This paper, however, indicates that even if the longevity risk can be fully insured at not cost, risk-averse individuals actually want to bear a share of it.",Would you prefer your retirement income to depend on your life expectancy?,https://www.sciencedirect.com/science/article/pii/S0022053120301198,28 October 2020,2020,Research Article,168.0
"Tsakas Nikolas,Xefteris Dimitrios","Department of Economics, University of Cyprus, P.O. Box 20537, Nicosia 1678, Cyprus","Received 23 July 2019, Revised 23 October 2020, Accepted 23 October 2020, Available online 28 October 2020, Version of Record 5 November 2020.",https://doi.org/10.1016/j.jet.2020.105130,Cited by (1),"A majority of truth-seeking voters wants to choose the alternative that better matches the state of the world, but voters may disagree on which alternative is the best match due to ====. When we have an arbitrary number of alternatives and sophisticated partisan voters exist in the electorate, electing the correct alternative is challenging. We show that multi-round runoff voting achieves asymptotically full-information equivalence. That is, when the society is large, it can lead to the election of the correct alternative under fairly general assumptions regarding the information structure and partisans' preferences.","Consider a group of like-minded voters who wish to make a correct decision, but who might disagree on what the correct decision is due to private information. These common value voters wish to efficiently aggregate their pieces of (possibly conflicting) information. But can they achieve their goal by the means of simple voting procedures?====Since Condorcet and his celebrated jury theorem, we know that when there are only two available alternatives, the society is composed exclusively of such common value voters (henceforth, referred to as ====), and voters vote sincerely –i.e. according to their private information–, then the plurality rule leads to the right outcome across a large range of cases. Unfortunately, as Austen-Smith and Banks (1996) pointed out, this kind of behavior is rarely an equilibrium. Fortunately, McLennan (1998) demonstrated that even when there is no sincere voting equilibrium, there is always an equilibrium that properly aggregates information. This intuition has been recently shown by Barelli et al. (2017) to also hold in environments with multiple alternatives and general information structures.====Alas, in real societies, truth-seeking voters are not alone: Groups of partisan voters –i.e. individuals who have their own private reasons for supporting certain candidates– also participate in the elections. This makes the task of the truth-seeking voters even more complicated. They need to solve not only the information aggregation problem that they face among them, but also to overcome the effect of partisan voters on the election's outcome. As it turns out, this is not possible through simple plurality voting even when the group of truth-seekers constitutes a majority: As long as there are multiple alternatives and a substantial fraction of partisan voters, there are information structures that lead to a divided majority and to the election of an alternative that is not the best match to the state of the world (see, for instance, Bouton and Castanheira, 2009).====This apparent deadlock has attracted the interest of economic theory. Recently, a series of papers identified approval voting as the most efficient among all scoring rules in terms of information aggregation (e.g., Goertz and Maniquet, 2011; Bouton and Castanheira, 2012; Ahn and Oliveros, 2016). Among other things, approval voting allows the majority voters to surpass the obstacles presented by the existence of partisan minorities and leads to full-information equivalence –i.e. to the implementation of the correct alternative– with a probability that converges to one as the society grows larger.==== While these studies present an elegant solution to the described information aggregation problem, they do so only for elections with three alternatives and rely on partisans behaving in a rather unambiguous manner.====In this paper, we turn our attention toward another class of applied electoral systems, runoff rules, and we investigate the information aggregation properties of runoff rules in general informational environments with an arbitrary number of alternatives and partisan groups. According to the system that we primarily focus on, voting takes place in multiple rounds, and in each round, the alternative with the least number of votes is eliminated until we have a unique winner.==== Given that this rule involves several stages, it should provide more opportunities to the truth-seeking majority to effectively coordinate and elect the correct alternative than truth-seeking voters have under a simple plurality rule (Piketty, 2000). Indeed, under very specific assumptions regarding the information structure, this has already been proven to be the case. In a divided majority framework with three alternatives, Martinelli (2002) demonstrated that when the information structure is precisely symmetric –and, as a consequence, sincere voting is enough to lead large societies to elect the correct alternative– sincere voting becomes asymptotically incentive compatible, and, hence, information is aggregated efficiently.====In a great variety of contexts, though, voting sincerely under a runoff rule does not lead to efficient decisions. Consider, for instance, an election among three candidates, A, B, and C, each being the correct choice in one of three ex-ante equally probable states of the world, ====, ==== and ==== and let the electorate be large. Moreover, candidate C is expected to be supported by several partisans (each voter is a C partisan with probability 4/10 and truth-seeking with probability 6/10), whereas truth-seeking voters can be of three types, with the likelihood of each type depending on the state====: When the state is ==== (resp. ====), each truth-seeking voter is assigned type ==== (resp. ====) with probability one, whereas if the state is ====, a truth-seeking voter is assigned type ==== with probability 4/10 and type ==== with probability 6/10. Therefore, a voter who is assigned type ====, where ====, believes that state ==== is more likely to be the correct state than any other.==== Hence, with sincere voting candidate B would not be elected in state ==== –in fact, she would be eliminated from the first round– since each voter who is assigned type ==== would vote for candidate A instead! So, we consider the question: Can runoff voting properly aggregate information in cases like this when truth-seeking voters are allowed to behave strategically?====This paper undertakes the task of providing a general answer and shows that, indeed, multi-round runoff voting can lead to full-information equivalence under general assumptions regarding the information environment. Our analysis is the first to establish that this rule is superior in terms of information aggregation compared to other applied rules, such as the plurality or the two-round runoff rules, in the presence of multiple partisan groups, and we achieve this by employing an alternative methodological approach that makes the study of relevant questions more efficient.====First, we study these questions by modifying the results of McLennan (1998) in a way that accommodates the existence of expressive partisan voters (i.e. voters who do not engage in strategic reasoning). McLennan's argument applies to games in which all players have the same utility over final outcomes and can be summarized in the following way: Any strategy profile that maximizes the players' common utility is, necessarily, an equilibrium profile as well (see, Ahn and Oliveros, 2016, for a recent application of this approach). Evidently, all models that consider information aggregation problems in the presence of partisan voters cannot directly employ this approach. Indeed, Bouton and Castanheira (2012) and Martinelli (2002) have followed alternative paths in order to produce relevant results. We demonstrate that McLennan's approach can be applied even when partisan voters exist by studying an “ex-ante” version of the model that we are interested in, and by proving that these two games are strategically equivalent. The “ex-ante” game is such that all voters share common preferences, and, after they select their strategies, nature moves and might alter their votes to each of the available alternatives with exogenously given probabilities. This equivalence allows us to argue that if a common strategy followed by all truth-seeking voters induces the election of the correct alternative, then a (Bayes-Nash) equilibrium with similar properties should also exist. Utilizing recent findings of Barelli et al. (2017), we tailor such a strategy for our setup and establish our first main result.====We then proceed by adding rational partisan voters with arbitrary policy preferences to our model. Extending the analysis in this direction is important since, to our knowledge, all existing approaches consider partisan voters whose actions are unaffected by the preferences of the rest of the voters –e.g., given partisan voters' preferences and the voting rule at hand, they have a unique dominant strategy (see, for instance, Goertz and Maniquet, 2011; Bouton and Castanheira, 2012). While this makes sense in some frameworks, it is, arguably, a limitation that we would like to break free from: partisan voters, even those less sophisticated than common value voters, may adjust their voting behavior in response to their expectations regarding the behavior of the rest of the voters. To accommodate the extra assumptions of all voters being rational (i.e. they best-respond to the beliefs that they hold) and of this rationality being common knowledge (i.e. each player believes that all other players are rational) we require that partisan voters use rationalizable strategies (in the spirit of Bernheim, 1984 and Pearce, 1984) and do not behave in a naïve sincere manner. Notice that rationalizable strategies are not best-responses to any (potentially unreasonable) beliefs, but they do involve a high level of sophisticated reasoning. Indeed, they require that the beliefs that players have are consistent with the assumption that the other players are also rational, and, hence, those players also best-respond to reasonable beliefs. For instance, if a group of voters is expressive or has a dominant strategy (e.g., if common value voters never use a particular action when they are assigned a certain type), then all other partisan voters take this information into account and properly adjust their strategies. In this richer and more complicated setup, we still find that the common value majority can reach efficient decisions, establishing our second result.====Our contribution is, hence, threefold. First, we demonstrate that multi-round runoff voting can lead to efficient outcomes in the presence of partisan voters under general assumptions regarding the space of alternatives and the information structure. Second, we propose a modeling approach, which can help us intuitively explore more questions and voting rules in this interesting environment. Finally, we introduce instrumental partisan voters, arguably complicating analysis while also making it more relevant to settings of applied interest.====Overall, these findings combine and strengthen the case for democratic decision making. Indeed, truth-seeking majorities may achieve full-information equivalence by the means of simple voting rules, even when: a) there are many alternatives, b) the information structure is general, and c) majority voters have to face sophisticated minorities with complex objectives. To our knowledge, this is the first paper that makes this claim, one we believe is an observation of independent and wider interest.====We conclude our formal analysis by comparing the information aggregation properties of our multi-round runoff system to other popular electoral rules –namely, the plurality rule and the two-round runoff system. As mentioned earlier, existing literature (e.g., Bouton and Castanheira, 2009; Martinelli, 2002) has already studied multicandidate elections under the plurality rule and has established that, in the presence of partisan voters, aggregation of information need not be achieved in equilibrium. Due to restrictions imposed by the Poisson voting model, though, these studies have restricted attention only to symmetric equilibria and up to three alternatives. We establish that when there are more than two alternatives, the plurality rule does not always admit an efficient equilibrium, whether symmetric or asymmetric, allowing us to conclusively address this issue. Moreover, we perform the same analysis for the two-round runoff system, which is arguably the most commonly applied runoff rule, and show that it can aggregate information efficiently as long as there are at most four alternatives.==== These additional findings establish the superiority of the multi-round runoff system with respect to both of these widely used rules.====In what follows, we first provide a motivating example (Section 2), and then we present our main formal analysis both with expressive and with instrumental partisan voters (Section 3). Finally, we compare these results in relation to other electoral rules (Section 4) and conclude (Section 5).",Information aggregation with runoff voting,https://www.sciencedirect.com/science/article/pii/S002205312030123X,28 October 2020,2020,Research Article,169.0
"Malik Komal,Mishra Debasis","Indian Statistical Institute, New Delhi, India","Received 14 March 2019, Revised 25 September 2020, Accepted 24 October 2020, Available online 28 October 2020, Version of Record 5 November 2020.",https://doi.org/10.1016/j.jet.2020.105128,Cited by (7),"We consider a combinatorial auction model where preferences of agents over bundles of objects and payments need not be quasilinear. However, we restrict the preferences of agents to be dichotomous. An agent with dichotomous preference partitions the set of bundles of objects as ==== and ====, and at the same payment level, she is indifferent between bundles in each class but strictly prefers acceptable to unacceptable bundles. We show that there is no Pareto efficient, dominant strategy incentive compatible (DSIC), individually rational (IR) mechanism satisfying no subsidy if the domain of preferences includes ==== dichotomous preferences. However, a generalization of the VCG mechanism is Pareto efficient, DSIC, IR and satisfies no subsidy if the domain of preferences contains only ==== dichotomous preferences. We show tightness of this result: adding any non-dichotomous preference (satisfying some natural properties) to the domain of quasilinear dichotomous preferences brings back the impossibility result.","The Vickrey-Clarke-Groves (VCG) mechanism (Vickrey, 1961; Clarke, 1971; Groves, 1973) occupies a central role in mechanism design theory (specially, with private values). It satisfies two fundamental desiderata: it is dominant strategy incentive compatible (DSIC) and Pareto efficient. We study a model of combinatorial auctions, where multiple objects are sold to agents simultaneously, who may buy any bundle of objects. For such combinatorial auction models, the VCG mechanism and its indirect implementations (like ascending price auctions) have been popular. The VCG mechanism is also individually rational (IR) and satisfies no subsidy (i.e., does not subsidize any agent) in these models.====Unfortunately, these desirable properties of the VCG mechanism critically rely on the fact that agents have quasilinear preferences. While analytically convenient and a good approximation of actual preferences when payments involved are low, quasilinearity is a debatable assumption in practice. For instance, consider an agent participating in a combinatorial auction for spectrum licenses, where agents often borrow from various investors at non-negligible interest rates. Such borrowing naturally leads to a preference which is not quasilinear. Further, income effects are ubiquitous in settings with non-negligible payments. For instance, a bidder in a spectrum auction often needs to invest in telecom infrastrastructure to realize the full value of spectrum. Higher payment in the auction will lead to lower investments in infrastructure, and hence, a lower value for the spectrum.====This has initiated a small literature in mechanism design theory (discussed later in this section and again in Section 4), where the quasilinearity assumption is relaxed to allow any ==== preference of the agent over consumption bundles: (bundle of objects, payment) pairs.==== The main research question addressed in this literature is the following:",Pareto efficient combinatorial auctions: Dichotomous preferences without quasilinearity,https://www.sciencedirect.com/science/article/pii/S0022053120301216,28 October 2020,2020,Research Article,170.0
"Rossi Stefano,Tinn Katrin","Bocconi, IGIER, Italy,CEPR, United Kingdom of Great Britain and Northern Ireland,ECGI, Belgium,Desaultels Faculty of Management, McGill University, Canada","Received 22 March 2018, Revised 8 October 2020, Accepted 15 October 2020, Available online 22 October 2020, Version of Record 3 November 2020.",https://doi.org/10.1016/j.jet.2020.105127,Cited by (3),"We present a model where quantitative trading − trading strategies based on the quantitative analysis of prices, volumes, and other asset and market characteristics − is systematically profitable for sophisticated traders whose only source of ==== is knowing better than other market participants how many fundamental traders, i.e., traders informed about fundamentals, are active in the market. In equilibrium, the direction of optimal quantitative trading depends on the number of fundamental traders and often switches sign when order flow increases: with few fundamental traders, optimal quantitative trading is trend-following (re. contrarian) after small (re. large) price changes; with many fundamental traders, the opposite holds: it is contrarian (re. trend-following) after small (re. large) price changes.","Financial market participants and academic research recognize that quantitative trading, that is, trading strategies based on the quantitative analysis of such variables as price, volume, and other asset and market characteristics, has become pervasive in financial markets.==== Narang (2013) classifies quantitative strategies commonly used in practice into three categories: (1) trend-following (e.g., buying after prices have gone up); (2) contrarian (e.g., buying after prices have gone down); (3) hybrid forms where the direction of price-contingent trading varies across time horizons, magnitudes of past price changes, instruments, and market structures.====The empirical evidence on the performance of quantitative trading documents a striking difference between retail investors and quantitative funds. In fact, the evidence shows individual and retail investors lose from pursuing trend-following or contrarian strategies.==== Consequently, a large literature argues that trend-following or contrarian trading must stem from behavioral biases, imperfect or bounded rationality, non-standard preferences, or institutional frictions (e.g., Barberis et al., 1998; Daniel et al., 1998; Hong and Stein, 1999; see Shleifer, 2000 and Barberis and Thaler, 2003 for surveys).====By contrast, institutional investors (e.g., hedge funds) appear to systematically profit from quantitative trading.==== As a result, the unprecedented growth of quantitative trading by sophisticated financial institutions (e.g., Osler, 2003; Hendershott et al., 2011) poses a challenge to a purely behavioral view of quantitative trading. The reason is that—unlike retail investors—these institutions appear to systematically profit from quantitative trading, whereas in the behavioral models reviewed above individual trend-following or contrarian traders incur systematic financial losses.====Our objective in this paper is to develop a micro-founded model of rational quantitative trading and to use this model to understand the mechanics and the profitability of quantitative trading by financial institutions. We build on Kyle's (1985) and Holden and Subrahmanyam's (1992) models, and we study a stylized setting with one risky asset and two trading dates, with strategic traders who have information about the fundamental, and noise traders. As in these papers, we impose that prices are set by the market maker (henceforth, the Market) to be equal to the expected value of the asset based on all public information. We relax some key assumptions in the model's information structure.====First, we relax the assumption that the Market has perfect information about how many informed traders are active and submitting orders. We then prove that “sophisticated” traders, traders whose only source of superior information is that they know (better than the Market) how much informed trading has occurred in the past, profit systematically from quantitative trading strategies in equilibrium. The reason is that they can assess, better than the Market, whether the past order flows were generated by noise trading or fundamental information. Our model thus suggests profitable quantitative trading by institutions (e.g., quantitative hedge funds) may stem from their specialization in acquiring superior information about market structure and other market participants' strategies, rather than from acquiring fundamental information directly.====Second, our model sheds light on the key forces that make optimal quantitative strategies trend-following, contrarian, or hybrid. We prove that a crucial determinant of the direction of optimal quantitative trading is the mere possibility that fundamental traders may have observed “confirmatory information,” that is, new evidence that the true asset value is close to the prior. As a result of observing such confirmatory information, fundamental traders optimally choose not to trade, which turns out to be crucial in determining the direction of optimal quantitative trading.====Specifically, we establish with few fundamental traders the optimal quantitative trading is usually trend-following (re. contrarian) after small (re. large) order flows. The reason is that the Market interprets too cautiously small order flows as reflecting confirmatory information, while the quantitative trader takes correctly into account the possibility that the fundamental traders did receive positive information but were too few to add up to a large order flow. As a result, the quantitative traders trade in the same direction as the price change. Conversely, at large order flows the quantitative trader better assesses the likelihood that a large noise trading shock just occurred and optimally trades against it. With many fundamental traders, the opposite holds, albeit for similar reasons: it is contrarian (re. trend-following) after small (re. large) price changes.====Therefore, our model suggests a theoretical rationale for the third type of quantitative trading mentioned above—hybrid strategies where the direction of trading (trend-following or contrarian) depends on the magnitude of past order flows (returns). Although closely related theoretical models predict either trend-following or contrarian strategies, to the best of our knowledge, no unifying framework exists that highlights the conditions under which one or the other opposite strategy prevails, and why the direction of trading may change non-monotonically depending on the magnitude of past order flows and returns.====We further analyze some variations of the model. In our main model, fundamental traders observe the fundamental and the number of other fundamental traders. We show that our key qualitative results also apply in a setting where quantitative traders are strategic, whereas fundamental traders follow a “rule of thumb” strategy, whereby they demand a fixed quantity of the asset whenever they observe a high (or low) asset value and trade zero when their information confirms the prior.==== Our key results also apply in a setting where fundamental traders observe the fundamental, but not the number of other fundamental traders.====We also present an extension that suggests a possible theoretical explanation for why trend-following quantitative strategies could be more prevalent at small order flows. Suppose a limited number of traders can observe fundamental information or not. All these traders can set up a quantitative trading rule before they learn fundamental information, which they can override if and when they do learn fundamental information. When fundamental information is easy to acquire, many (if not all) traders acquire this fundamental information and act as fundamental traders. When fundamental information is difficult to acquire, only some traders acquire it, whereas the rest implement their quantitative strategies. If the market does not know whether acquiring information was easy or difficult, uncertainty exists about the number of informed traders as in our baseline model. However, in this extension quantitative strategies will be overridden whenever fundamental information is easy to acquire, so that they will be implemented only when few fundamental traders are present, in which case our model predicts trend-following at small order flows and contrarian trading at large order flows. The only difference with our baseline model is that quantitative strategies are now more likely to reflect the knowledge that the number of informed fundamental traders was less than the market expected (these traders would be informed otherwise).====Our paper relates to the broad literature on asset pricing and learning in micro-founded financial markets surveyed in Brunnermeier (2001) and Vives (2008). More specifically, our work relates to the literature that studies trading in imperfectly competitive markets with asymmetric information (as in Kyle, 1985 and Glosten and Milgrom, 1985, among others).==== Our model has two key empirical implications that help distinguish our results from the extant literature. First, the order flow is predictable whereas returns are not. Second, quantitative traders make strictly positive profits in excess of the risk-free rate (which we normalize to zero) and of the risk premium (which is also zero because we model risk-neutral traders). Existing literature yields different predictions and relies on different mechanisms. For example, in the behavioral literature mentioned above, both returns and order flow are predictable from past public information, and trend-following and contrarian traders make losses; and in the rational expectations literature that considers competitive markets (see Grossman and Stiglitz, 1980; Brown and Jennings, 1989), neither returns nor order flows are predictable, and learning from prices and order flows does not give traders profits in excess of the risk-free rate and of the risk premium. On the other hand, in the rational literature based on Glosten and Milgrom (1985), where returns are unpredictable but the order flow is predictable because of the predictable arrival of noise traders or the predictable trades of strategic investors (e.g., see Hirshleifer et al., 1994), trend-following and contrarian traders earn zero profits on average.====Looking more closely at the economic mechanisms at play, we note that in many classical models of trading under imperfect competition (i.e., Kyle, 1985; Holden and Subrahmanyam, 1992; Glosten and Milgrom, 1985; and the vast literature building on these models), the number of informed traders is known to the Market and to strategic traders, which in turn leads to the Easley and O'Hara's (1991) result on the impossibility of uninformed traders earning returns in excess of the risk-free rate and of the risk premium.==== We are not the only ones to point out that relaxing the assumption about perfect knowledge of traders' types has important consequences on the equilibrium strategies and prices (see Avery and Zemsky, 1998; Chakraborty and Yilmaz, 2004a, 2004b, and 2008; Odders-White and Ready, 2008; Gao et al., 2013; Li, 2013; Banerjee and Green, 2015; Back et al., 2017). In general, these papers emphasize very different mechanisms. For example, Banerjee and Green (2015) study a dynamic competitive setting under rational expectations and uncertainty about the share of informed traders. In their model, informed traders are risk averse and react asymmetrically to positive and negative news, which drives both return and order-flow predictability. Closer to our focus, papers in this literature that analyze strategies of traders who have superior information about trader types, consider uncertainty about whether there has been any informed trading in the past, rather than how much informed trading there has been in the past. In our model we assume that the number of informed fundamental traders is either high of low, and the uncertainty is about the magnitude of informed trading. Our model also nests the special case in which the Market is uncertain about whether no informed traders or some informed traders are present, as studied in Chakraborty and Yilmaz (2008), Li (2013), Back et al. (2017). In such a special case, we show that only trend-following or contrarian quantitative trading can occur in the equilibrium. This special case, however, hides interesting effects that emerge under more general information structures where the Market considers that at least some informed trading always occurs, and where the probability that informed traders observe confirmatory fundamental information is non-negligible.==== Our results show that under these conditions, the optimal strategy of quantitative traders varies non-monotonically with the magnitude of past order flows, holding fixed the market structure.====Our paper also relates to the literature on stock price manipulation, that is, the idea that rational traders may have an incentive to trade against their private information. Provided manipulation is followed by some (exogenously assumed) price-contingent trading, short-run losses can be more than offset by long term gains (see Kyle and Viswanathan, 2008 for a review). Chakraborty and Yilmaz (2004a, 2004b) study the incentives of an informed trader when uncertainty exists about whether such a trader is informed, or is a noise trader instead. If this trader turns out to be informed, he may choose to disregard his information and trade randomly in early periods, in order to build a reputation as a noise trader. Unlike in our model, these traders are not strategic when not informed. Goldstein and Guembel (2008) show that if stock prices affect real activity, a form of trade-based manipulation such as short sales by uninformed speculators can be profitable insofar as it causes firms to cancel positive NPV projects, and justifies ex post the “gamble” for a lower firm value. In their setting, both uninformed trading and successful stock price manipulation stem from the feedback effect between stock prices and real activity. By contrast, in our paper, we present an equilibrium where quantitative trading but no manipulation occurs. Therefore, our results demonstrate that quantitative trading does not inevitably make uninformed investors the prey of (potentially informed) speculators.====More broadly, our paper relates to the literature on rational herding (see Avery and Zemsky, 1998; Park and Sabourian, 2011; and Chamley, 2004 for a review), because this literature also explores the possibility of seemingly behavioral trading in efficient markets. Note, however, that herding and quantitative trading are very different phenomena: herding refers to situations where traders disregard their private information and follow public information instead, whereas traders who follow quantitative strategies do not disregard private information but obtain private information about other market participants' strategies, which enables them to better interpret price and order-flow data.====Finally, by considering a setting where strategic traders either have or do not have fundamental information about a particular asset, our paper also relates to portfolio selection models with costly information acquisition (see, e.g., Van Nieuwerburgh and Veldkamp, 2010), where investors choose to specialize in learning fundamental information about one/a few assets and risk factors.",Rational quantitative trading in efficient markets,https://www.sciencedirect.com/science/article/pii/S0022053120301204,22 October 2020,2020,Research Article,171.0
Orzach Roi,"Massachusetts Institute of Technology, United States of America","Received 22 January 2020, Revised 10 June 2021, Accepted 11 June 2021, Available online 29 June 2021, Version of Record 16 August 2021.",https://doi.org/10.1016/j.jet.2021.105304,Cited by (0)," proposes a model of price dispersion with multiproduct firms. Theorem 7 in ==== incorrectly characterizes one class of equilibria in this setting, constant profits equilibria. I provide a counterexample to this theorem and show that two main features ==== emphasizes are still true: such equilibria can exist with more than two products but cannot exist if the number of products tends to infinity."," analyzes an equilibrium concept of a multiproduct consumer search model called a “constant profits equilibrium” (CPE). A CPE exists when there is a set of prices in which firms receive constant profits on each good when pricing within the set and receive weakly lower profits when pricing outside the set. Lemmas 4 and 5 in ==== give necessary and sufficient conditions for the existence of a CPE based on the size of this set. Upon adding an additional convexity assumption, ==== states Theorem 7, which gives a necessary and sufficient condition for such an equilibrium to exist.====In this corrigendum, I demonstrate that Theorem 7 in ==== is incorrect by showing that a CPE exists when the necessary condition in the theorem does not hold. Additionally, I show that a CPE cannot exist when the number of products is sufficiently large.",Corrigendum on “Multiproduct equilibrium price dispersion” [J. Econ. Theory 67 (1) (1995) 83–105],https://www.sciencedirect.com/science/article/pii/S0022053121001216,29 June 2021,2021,Research Article,175.0
Jindani Sam,"Department of Economics, University of Oxford, United Kingdom","Received 29 April 2018, Revised 16 November 2019, Accepted 23 November 2019, Available online 27 November 2019, Version of Record 4 December 2019.",https://doi.org/10.1016/j.jet.2019.104972,Cited by (0),"Can cooperation be sustained in large populations? This paper studies settings in which a large group of players is rematched at random each period. In such settings cooperation cannot be sustained by an equilibrium unless deviators are sanctioned by third parties. This is known as the problem of ====. Previous analyses have relied on strong assumptions about what information players have access to. This paper shows that when players are matched with multiple partners in each period, it is possible to limit the amount of information required to support cooperative outcomes. The results hold for general games and for equilibria that are robust to noise.","The folk theorem for repeated games can be seen as a formal solution to the problem of cooperation between individuals.==== For instance, it implies that players in a repeated prisoner's dilemma can cooperate, despite incentives to cheat one another, by threatening credible sanctions if one of them deviates. More generally, it implies that in many repeated games efficient outcomes can be enforced by an equilibrium if players are sufficiently patient. The canonical folk theorem applies to settings in which players interact with the same partners every period. However in many economic settings, agents interact with different partners at different times and interactions with any given partner may be too infrequent for the standard folk theorem to apply. Can cooperation still be sustained in such settings?====Following Kandori (1992) and Okuno-Fujiwara and Postlewaite (1995), I model this situation as a ====. Each period, players are randomly matched to play a stage game. Because any two players meet infrequently, sanctions against players who deviate must be carried out by third parties. This creates a problem of information transmission: If a player deviates, how will her partners in future periods know that she should be sanctioned? And if one of her future partners fails to sanction her, how will the partner's future partners know that she, in turn, should be sanctioned? And so on. This is what Kandori termed the problem of ====. In a standard repeated game, the problem does not arise, because players observe every action taken by every player; in a repeated game with random matching, this would be implausible. The question is how much information transmission is necessary to sustain cooperation.====Kandori's seminal paper presents two approaches. First, he shows that cooperation can be sustained in the prisoner's dilemma without information transmission, but at the price of instability: in his equilibrium, players cooperate as long everyone they encounter cooperates; but as soon as one of their opponents defects against them, they defect in all future periods. This is known as a ====, because defection spreads from player to player through the population. This type of equilibrium is only feasible in certain stage games such as the prisoner's dilemma, where sanctions correspond to a stage-game Nash equilibrium. Moreover, the equilibrium, as Kandori notes, is not robust to noise, in the sense that if a single player makes a mistake and defects, the whole population starts to defect and never returns to cooperation.====Kandori's second approach relies on strong assumptions about information transmission. Kandori assumes that there is an exogenous information clearinghouse that recognises deviations and assigns publicly observable ‘labels’ to players who deviate. Given this assumption, he is able to prove a robust folk theorem for (almost) general games and equilibria that are robust to noise. In his equilibrium, the labels denote the number of periods a player should be sanctioned for. If a player deviates, her label is updated to a positive integer. Players who are matched with her in the ensuing periods observe this label and know that they are supposed to sanction her; if they fail to do so their label is updated and they will be sanctioned in turn. In his conclusion, Kandori notes: ‘Perhaps the most important question which is unanswered by the present paper concerns the way in which the information transmission postulated in our model is implemented’ (Kandori 1992, p. 77). Okuno-Fujiwara and Postlewaite make a similar assumption in their paper (with ‘statuses’ instead of ‘labels’).====Most of the literature on repeated games with random matching has followed Kandori's first approach in assuming little or no information transmission between players. These results tend only to apply to games like the prisoner's dilemma or to be limited to non-robust or weak equilibria (see the literature review in section 2). I take the second approach. I show that it is possible to obtain a folk theorem for general games and for strict and robust equilibria under significantly weaker assumptions about information transmission than those made by Kandori.====The results are obtained using a simple construction: In a standard repeated game with random matching, players are paired with a single partner each period. I assume that players are matched with multiple partners in each period, and play the same stage game with each. This drastically changes the nature of the game, because it becomes easier to identify deviators. In a standard repeated matching game without multiple matching, players may require a large amount of information to determine whether a partner deviated in a previous period. For instance, in the prisoner's dilemma a player who defected in a given period may have deviated, or may have been sanctioning a player who deviated. To find out, one needs to determine whether the player she was matched with had deviated previously, which may require determining whether the players that player was matched with in previous periods deviated, and so on. In general, the amount of information required increases over time and is unbounded. In contrast, when players are matched with multiple partners, one can determine whether or not a player deviated in a given period just by looking at her actions and the actions of other players in that period – specifically, it suffices to check whether a player sanctioned someone when others did not, or failed to sanction someone when others did. That is, one can compare a player's actions with the corresponding ==== in that period. If they differ, then one can infer that the player deviated.====I assume that players can observe partners' actions in recent periods as well as the corresponding modal actions. I show that this is sufficient to obtain a folk theorem for general games. In the equilibrium, players sanction partners who were non-modal in the recent past. Then it is in each player's interest not to deviate on the equilibrium path, because a player who does so will be non-modal and will be sanctioned. It is also in each player's interest to sanction a player who deviates, because failing to do so would mean failing to sanction someone whom others sanction, which would trigger sanctions in the following periods. In this framework, the amount of information required by players is uniformly bounded from above across all possible histories. In particular, unlike in Kandori's framework, the amount of information required does not increase over time or in the size of the population; moreover, the result does not rely on an exogenous information clearinghouse.====The exact amount of information each player requires depends on how far in the past they have to look back, which in turn depends on how long deviators have to be sanctioned for. I argue that for relevant applications, this may not be very long at all. For instance, in a worked example using the prisoner's dilemma, I show that players only need to look back one period to sustain cooperation.====The model can be thought of as applying to the following setting: Consider a relatively large and tight-knit community such as a medium-sized town. Each townsperson interacts with a number of others on a regular basis. However, any given pair of townspeople may interact infrequently.==== The interactions between townspeople could be economic transactions, such as buying and selling goods and services, or purely social interactions, ranging from stopping to exchange pleasantries, to inviting someone to a social function, to providing assistance when necessary.====There may be a number of different social norms in place in the town. For instance, townspeople could be expected not to be rude or disrespectful to one another; or they might be expected to pay their debts and return favours. These norms are enforced by the community. Depending on the severity of the violation, the sanction may be more or less severe. For instance, suppose a townsperson is rude towards someone at a social function. Then people may stop inviting her to other events. If someone fails to pay a debt, people may refuse to do business with her. More severe violations could lead to complete ostracism. Crucially, deviations are sanctioned by the whole community. Being sanctioned just by the person who was harmed by an agent's deviation would be ineffective.====The problem of community enforcement is the specific mechanics of how these sanctions are carried out by members of the community. In particular, a key issue is how information about who should be sanctioned is transmitted to relevant agents. In this setting, there is no central clearinghouse that would label deviators as in Kandori's framework. It would also be unrealistic to assume all townspeople have full information about all other agents' past actions since the beginning of the game. For one thing, the population is too large; for another, it is unclear when the game even began.====The present paper offers an answer to this problem. In the setting described above, it is natural to assume that agents are matched with multiple other agents in each period. In that case, the paper shows that players' statuses as deviators or non-deviators can be tied to whether or not they were modal. This considerably simplifies the information transmission required. The only thing agents need to know is whether a partner was non-modal in the recent past; that is, the townspeople need to be able to find out if a person they interact with acted in a way that other townspeople did not. This kind of information is readily available to different members of the community and can be shared and verified easily – one can imagine the townspeople gossiping and sharing information about a particular agent's violation of a norm. Multiple matching provides one key feature: what an agent should have done in any given interaction is clear and easy to identify.====Like Kandori's labelling equilibrium, the proposed equilibrium is robust to noise. In a real-world setting, such as the one described above, noise is pervasive. Players might fail to sanction someone when they were meant to sanction them, or sanction someone they weren't meant to. They could observe their partners actions inaccurately, or they might simply be confused. Some players could even be irrational and have some other basis for picking actions than best-responding. Given this, robustness to noise is an important property of an equilibrium. I consider two different definitions of robustness and show that the proposed equilibrium is robust under both. First, Kandori defines a notion of robustness he calls ====, according to which players' continuation payoffs must converge to their equilibrium-path values from any history. In other words, the effect of any individual deviation must eventually die out. This is a relatively straightforward and intuitive definition of robustness. It is satisfied by Kandori's labelling equilibrium but not his contagion equilibrium. The equilibrium of the present paper is straightforwardly globally stable, since punishment phases are finite.====Second, Ellison (1994) argues that if noise occurs in every period, global stability may not be an appropriate definition of robustness. If noise is persistent, an equilibrium could be globally stable, yet if it takes too long for the impact of an error to die out players could have expected payoffs far from their values in the case without noise. Ellison models this setting by assuming that each period each player ==== with some small probability and plays an action other than the one she intended to play. He shows that contagion equilibria in the prisoner's dilemma can be made robust to noise in the sense that, as noise tends to zero, the strategies remain an equilibrium and the players' continuation payoffs converge to the equilibrium-path payoffs. Although he requires public randomisations to obtain global stability, this is not required for robustness to persistent noise.====In the spirit of Ellison, I show that the proposed equilibrium is robust to persistent noise. As in Ellison's analysis, I assume players tremble with some small probability. However, I also consider two additional types of noise: observation errors, whereby players observe their partners' past actions inaccurately, and noisy players, whereby some players always choose their actions at random. Given the importance of information transmission in random matching games, robustness to observation errors is a particularly desirable form of robustness. Robustness to noisy players is also significant, because, as Ellison notes, in a contagion equilibrium a single noisy player can cause defection throughout the population in every period. I show that the proposed equilibrium is robust to trembles and noisy players. The equilibrium is not robust to observation errors when they are the only source of noise. However it is robust when there is also another type of noise and the probability of an observation error is small relative to the other source of noise.====The rest of this paper is organised as follows: In section 2, I provide an overview of the literature on community enforcement. In section 3, I set up the model of repeated matching games with multiple partners. In section 4, I prove the main result and solve a worked example with the prisoner's dilemma. Finally, in section 5, I consider robustness to noise.",Community enforcement using modal actions,https://www.sciencedirect.com/science/article/pii/S0022053118301248,27 November 2019,2019,Research Article,182.0
"Eeckhoudt Louis R.,Laeven Roger J.A.,Schlesinger Harris","IESEG School of Management, CNRS-LEM, UMR 9221 and CORE, France,Amsterdam School of Economics, University of Amsterdam, EURANDOM and CentER, Netherlands,Department of Finance, University of Alabama, United States of America,University of Konstanz, Germany","Received 19 January 2018, Revised 3 September 2019, Accepted 16 November 2019, Available online 22 November 2019, Version of Record 29 November 2019.",https://doi.org/10.1016/j.jet.2019.104971,Cited by (10),"By specifying new model free preferences towards simple nested classes of lottery pairs, we develop the dual story to stand on equal footing with that of (primal) risk apportionment. The dual story provides an intuitive interpretation, and full characterization, of dual counterparts of such concepts as prudence and temperance. The direction of preference between these nested classes of lottery pairs is equivalent to signing the successive derivatives of the probability weighting function within ===='s (====) dual theory. We explore implications of our results for optimal portfolio choice and show that the sign of the third derivative of the probability weighting function may be naturally linked to a self-protection problem.","Although first received with some skepticism, the notions of prudence and temperance have now been widely accepted almost on par with the fundamental concept of risk aversion, at least in an expected utility (EU) framework.====The expanding use of these notions, sometimes termed “higher order risk attitudes”, can be explained by the fact that they were progressively given a more general interpretation. Consider prudence, for instance. This term was coined by Kimball (1990) in an influential paper in which he showed that precautionary savings as an optimizing type of behavior is characterized in an EU framework by a positive third derivative of the utility function (i.e., “====” or “prudence”). However, it is by now well-known that this positive sign of ==== can be interpreted more generally outside the specific decision problem of saving. This more primitive interpretation of prudence was initiated by Menezes et al. (1980), who used the term “downside risk aversion”, and it was further pursued in Eeckhoudt and Schlesinger (2006), who also showed how to proceed from prudence to higher order risk attitudes. These authors first state a “model free” preference, namely that decision makers (DM's) like to “combine good with bad” instead of having to face either everything good or everything bad. Next, this model free preference is shown to be translated into prudence (====) within the EU model, and from prudence—by defining a sequence of nested lotteries and always asserting the preference for combining good with bad—the higher order risk attitudes may be obtained similarly, starting with temperance (====) at the fourth order.====An advantage of the characterization of prudence and higher order risk attitudes in terms of simple primitive assumptions on preferences is that it allows evaluating these virtues in their own right instead of evaluating their implications in specific applications. It turns out besides that the simple primitive interpretation of prudence and higher order risk attitudes found in Eeckhoudt and Schlesinger (2006) lends itself easily to experimental verification. As a result, there is now an intensive experimental research activity around the concepts of prudence and temperance in an EU framework (e.g., Ebert and Wiesen, 2011, 2014; Deck and Schlesinger, 2010, 2014, and Noussair et al., 2014, to name a few).====The preference for “combining good with bad” has appeared under different names in the literature. It was called “risk apportionment” in Eeckhoudt and Schlesinger (2006). A little earlier, Chiu (2005) referred to a “precedence relation”, in which one stochastic dominant change precedes another. The phrase “combining good with bad” as a primitive trait first appeared in Eeckhoudt et al. (2009).====While prudence, temperance, and higher order risk attitudes can be presented initially as natural properties in a model free environment, their interpretation and implementation have been developed so far exclusively within an EU framework.==== The first result in this paper provides a characterization of these (primal) virtues within Yaari's (1987) dual theory (DT). This characterization reveals that a DT maximizer likes to combine good with bad, when formulated as a “location preference” of always being more willing to accept an extra zero-mean risk in a state where wealth is higher than in a state where wealth is lower as in Eeckhoudt and Schlesinger (2006), if and only if the probability weighting function of the DT model is the identity function; then DT simplifies to expected value maximization, which, of course, is also a special case of EU. This result illustrates that while (primal) prudence and higher order risk attitudes are often presented as being model free, and rightfully so, they may have, at the same time, no specific meaning outside EU.====Next, by specifying new model free preferences towards simple nested classes of lottery pairs, we develop the dual story to stand on equal footing with that of (primal) risk apportionment. The dual story provides an intuitive interpretation, and full characterization, of dual counterparts of such concepts as prudence, temperance, and other virtues. We show that the direction of preference between the nested classes of lottery pairs that we construct is equivalent to signing the ====th derivative of the probability weighting (or distortion) function within DT, with ==== an arbitrary positive integer.==== It turns out that this development requires a fundamental departure from the approach of Eeckhoudt and Schlesinger (2006), which is unable to deliver the desired implications within the DT framework. The dual story we develop retains generic features of the primal story—e.g., a precedence relation—but crucially departs from it in its construction and implementation—e.g., by reference to what we will refer to as “squeezing” and “anti-squeezing” and to the “dual moments”.====This paper thus represents a first step towards a more general interpretation of higher order risk attitudes within alternative non-EU decision models, such as rank-dependent utility and prospect theory (Quiggin, 1982; Schmeidler, 1986, 1989; Tversky and Kahneman, 1992), for which DT is a building block. Indeed, because DT is “orthogonal” to EU, our analysis not only reveals the differences with the primal story under EU, but it is also a prerequisite for a development of higher order risk attitudes as primitive traits of behavior compatible with the more general models of choice under risk provided by rank-dependent utility and prospect theory. These decision theories require their own “model free” preferences to characterize properties of the preference representations.====A positive sign of the third derivative of the probability weighting function is consistent with an “inverse S-shape”, exhibited by the popular probability weighting functions proposed by Tversky and Kahneman (1992) (see also Wu and Gonzalez, 1996, 1998) and Prelec (1998) under typical parameter sets implied by experiments.==== These inverse S-shaped probability weighting functions typically feature a positive sign for the odd derivatives and an alternating sign (first negative at low probability levels, then positive at high probability levels) for the even derivatives. Provided that the probability weighting function is first concave and then convex with second derivative equal to zero at the inflection point, a positive sign of the third derivative of the probability weighting function implies that the function becomes more concave when moving to the left of the inflection point and becomes more convex when moving to the right of the inflection point. Our simple characterizations may spark an interest in a direct analysis of the signs of the probability weighting function's derivatives.====As is well-known, primal and dual stochastic dominance coincide up to the second order and may diverge from the third order onwards. As a by-product, of interest in its own right, the model free story appropriate for DT will nicely make apparent the fundamental reason behind this divergence.====Our results about the shape of the probability weighting function are relevant for the analysis of well-known problems in the economics of risk, such as portfolio choice and the level of self-protection in the presence of a background risk. We first illustrate our results by deriving their implications for optimal portfolio choice with a risky asset, a risk-free asset and access to zero-mean financial derivative products on the risky asset. We show that, contrary to under EU (Gollier, 1995), an ====th order improvement of the risky asset's return, achieved by supplementing (hence, squeezing) the risky asset with an appropriate selection of derivative products (e.g., a straddle at the third order or a volatility spread at the fourth order), never reduces the demand for the risky asset under the DT model. Furthermore, we show that the third derivative of the probability weighting function naturally appears in a self-protection problem that trades off the risk of a loss and the effort of protecting against the loss, in the presence of an independent background risk. In particular, if the third derivative of the probability weighting function is positive (“dual prudence”), the background risk stimulates self-protection.====This paper is organized as follows. In Section 2, we fix the notation and setting, introduce some preliminaries for the DT decision model, and provide some basic intuition and a formal characterization motivating our results. In Section 3, we introduce dual higher order risk attitudes by developing new model free preferences. Section 4 contains the formal presentation of our general results. Section 5 first illustrates implications of our results for optimal portfolio choice and next shows that the sign of the third derivative of the probability weighting function is naturally linked to a self-protection problem. We conclude in Section 6 with a summary of the results and an indication of potential extensions. Proofs are relegated to Appendix A.",Risk apportionment: The dual story,https://www.sciencedirect.com/science/article/pii/S0022053119301218,22 November 2019,2019,Research Article,183.0
"Troyan Peter,Morrill Thayer","Department of Economics, University of Virginia, United States of America,Department of Economics, North Carolina State University, United States of America","Received 3 October 2018, Revised 1 November 2019, Accepted 5 November 2019, Available online 11 November 2019, Version of Record 15 November 2019.",https://doi.org/10.1016/j.jet.2019.104970,Cited by (26),"A mechanism is ==== if agents can never profitably manipulate it, in any state of the world; however, not all non-strategy-proof mechanisms are equally easy to manipulate - some are more “obviously” manipulable than others. We propose a formal definition of an ==== in which agents compare worst cases to worst cases and best cases to best cases. We show that a profitable manipulation is obvious if and only if it can be identified as profitable by a cognitively limited agent who is unable to engage in contingent reasoning, as in ====. Finally, we show that this system of categorization is both tractable and intuitively appealing by classifying common non-strategy-proof mechanisms as either ==== or ====.","When designing mechanisms for allocating resources, such as in auctions, matching, or other assignment problems, there is a long and rich literature studying strategy-proof direct mechanisms, which are often seen as desirable because an agent need not forecast what they expect others to do in order to determine their own optimal strategy.==== Indeed, strategy-proof direct mechanisms have played a large role in many practical market design applications, including auctions, school choice (Abdulkadiroğlu and Sönmez, 2003), medical residency matching (Roth and Peranson, 1999), and kidney exchange (Roth et al., 2004), among others. At the same time, imposing strategy-proofness can be costly, and allowing for non-strategy-proof (or, manipulable) mechanisms widens the space of possibilities. While some agents may benefit by lying in a manipulable mechanism, the ease of recognizing and enacting such manipulations may vary across mechanisms. The goal of this paper is to provide a simple and tractable method for determining when a mechanism is easy to manipulate.====To motivate our project, consider two widely-used, manipulable mechanisms. The first is the Boston mechanism for school choice. Under this mechanism, a student loses her priority at a school unless she ranks it first. Therefore, if a student has high priority at a school that is her true second choice, she may be better off by lying and ranking this school first. By doing so, she can guarantee being assigned to it, whereas if she told the truth, she risks losing it to others who ranked it higher, and may end up at her third (or worse) choice. Not only is the Boston mechanism manipulable in the formal sense of failing to be strategy-proof, but further, the relevant manipulations are also very easy to identify and enact. Indeed, this has been discovered and used by both parents and policymakers. For instance, Pathak and Sönmez (2008) report on a well-organized parent group in Boston advising their members as follows: ====On the other hand, consider labor markets for new physicians, which are often organized using a centralized clearinghouse. Comparing variations in mechanisms across regions in the UK and US, Roth (1991) shows that priority match mechanisms (which are closely related to the aforementioned Boston mechanism) tend to perform poorly in practice, while mechanisms based on Gale and Shapley's (1962) Deferred Acceptance (DA) algorithm perform well. The explanation proposed in Roth (1991) is that DA produces a stable outcome, while priority match mechanisms do not. This applies to the reported preferences, and it is also well-known that DA is not strategy-proof for both sides of the market.==== However, while it is possible for hospitals to manipulate their preferences under DA and obtain a better assignment in some states of the world, to do so successfully requires a detailed understanding of the mechanics of the mechanism and of the preferences of the other agents. Without such knowledge, it is very possible that attempting such a manipulation may backfire: the manipulating hospital may not be assigned a doctor it would be happy to employ. This is in stark contrast to the Boston mechanism, where a student can guarantee a spot at her second-choice school, and thereby surely avoid a potentially worse outcome from reporting truthfully.====These examples suggest that some mechanisms may provide opportunities for manipulation that are much easier for agents to recognize and execute successfully than others; in other words, some manipulations are more “obvious” than others. The main contribution of this paper is a formalization of the word “obvious”, which we then use to classify non-strategy-proof mechanisms as either ==== or ====.====For a given agent, a report ==== is a ==== if the agent ever does strictly better reporting ==== over reporting her true type, ====. In this case, truthful reporting cannot be a dominant strategy. We define ==== to be an ==== if either the best possible outcome under ==== is strictly better than the best possible outcome under ====, or the worst possible outcome under ==== is strictly better than the worst possible outcome under ====. Clearly an obvious manipulation is also a manipulation; however, we argue that an obvious manipulation is identifiable to agents in a way that non-obvious manipulations are not.====To formalize the idea that obvious manipulations are easier to identify, we consider an agent who is not fully informed (or does not fully understand) how a mechanism ==== is defined, but instead is only able to determine the set of possible outcomes from any given strategy; mathematically, she knows the range of ==== conditional on her own report, but not the full function itself, state-by-state. For example, in the context of school assignment, this could be a neighborhood parent group that does not fully understand (or has not been told) the assignment algorithm being run but has kept track of what preferences parents have submitted and what the resulting assignments were. Theorem 1 demonstrates that obvious manipulations are exactly the manipulations that can be identified by such an agent. This is our theoretical foundation of the term “obvious”: even an agent who does not fully know how the mechanism is defined can deduce that the mechanism can be manipulated.====Both our formal definition and our behavioral characterization are inspired by the influential paper of Li (2017) on ====. Li (2017) starts from the observation that real-world agents are often unable to engage in the intricate, contingent reasoning necessary to fully understand the implications of a given course of action on a state-by-state basis (mathematically, in our context this would be equivalent to knowing the entire function ====).==== Formally, Li (2017) also considers agents who know only the set of possible outcomes from any given strategy, which can be understood as either a lack of ability to contingently reason, or equivalently as agents who are given only a partial description of the mechanism. Obviously dominant strategies are then those that are recognizable as dominant by such agents. While robust when they exist, very few mechanisms will have obviously dominant strategies; indeed, almost no normal-form games will be obviously strategy-proof. Many real-world applications like those we are concerned with (school choice, NRMP) have tens of thousands of agents, making it impractical to run an extensive-form (OSP) mechanism,==== which motivates our restriction to direct mechanisms. Even in this context, strategy-proofness itself is limiting, and so, rather than strengthen it, our approach is to relax strategy-proofness and instead look for mechanisms that are not ==== manipulable.====After our behavioral characterization, we apply our definition to several canonical market design environments, starting with school choice. We first formalize the above discussion regarding the Boston mechanism and show it is indeed obviously manipulable (Proposition 1). The main alternative to the Boston mechanism, the (student-proposing) DA mechanism, is strategy-proof for the students, but may produce Pareto inefficient assignments. To correct this, many new mechanisms that Pareto improve on DA have been proposed. While it is known that any such mechanism is manipulable (Abdulkadiroğlu et al., 2009; Kesten, 2010; Alva and Manjunath, 2019), we show a striking result: while they may be manipulable, any mechanism that Pareto dominates DA is not ==== manipulable (Theorem 2). This has particularly important implications for the efficiency-adjusted deferred acceptance (EADA) mechanism of Kesten (2010), which has received renewed attention, as several recent papers have shown that EADA is the unique Pareto efficient mechanism that also satisfies natural fairness axioms (Dur et al., 2015; Ehlers and Morrill, 2017; Tang and Zhang, 2017; Troyan et al., 2018). The only shortcoming of the EADA assignment is its implementation: it is a manipulable mechanism. However, Theorem 2 implies that EADA is not obviously manipulable.====After presenting our results for school choice, we discuss several other canonical market design applications. For two-sided matching, we show that while DA is manipulable for the receiving side, it is not obviously so (Theorem 3). For multi-unit auctions, we show that first-price/pay-as-bid multi-unit auctions are obviously manipulable (Corollary 2), while the ====-price auction is not (Theorem 4).==== Finally, we consider the classic bilateral trade setting with one buyer and one seller. We first show directly that double auctions (Chatterjee and Samuelson, 1983) are obviously manipulable. We then ask whether there is any NOM mechanism that also satisfies other common desirable properties. Our last result is an impossibility result in the spirit of Myerson and Satterthwaite (1983): every efficient, individually rational and weakly budget balanced mechanism is obviously manipulable (Theorem 5).====We stress that in our model, agents have standard preferences over outcomes, and we make no assumptions about prior probability distributions over the types or reports of other agents; rather, we presume that the ability of agents to recognize certain deviations as profitable may vary across mechanisms. Thus, our approach is consistent with the Wilson doctrine (Wilson, 1987), in the sense that determining whether a mechanism is obviously manipulable requires no assumptions about common knowledge or agents' prior beliefs. For instance, in the bilateral trade setting, it is difficult for the buyer to determine her optimal bid in a double auction mechanism, because it is highly sensitive to his beliefs about the seller's ask (and vice-versa). Our definition captures this difficulty by classifying this mechanism as obviously manipulable.====A common alternative approach to relaxing strategy-proofness in market design (without moving all of the way to Bayesian incentive compatibility) relies on large markets. Immorlica and Mahdian (2005) and Kojima and Pathak (2009) show that the incentives to manipulate DA vanish as the size of the market approaches infinity. Azevedo and Budish (2019) define a related concept of strategy-proofness in the large (SPL). While similar in motivation, our approach is distinct in several respects. Most notably, we require no assumptions on how preferences are drawn or agent beliefs; further, our results hold for markets of any size, and not just in the limit.==== Another recent strand of literature tries to quantify a mechanism's manipulability using particular metrics. This includes Carroll (2011), who defines a mechanism's susceptibility to manipulation as the maximum cardinal utility any agent can gain from lying, and Pathak and Sönmez (2013), who use a profile-counting metric to define one mechanism as “more manipulable” than another if, for any preference profile where the latter is manipulable for some agent, the former is as well. We do not require any assumptions on cardinal preferences, nor do we attempt to rank mechanisms by their degree of manipulability, but instead want to eliminate all obvious manipulations.====As an incentive criterion, NOM is weaker than strategyproofness, and thus allows more flexibility in the choice of mechanism. Correspondingly, if agents report truthfully under a NOM mechanism, then NOM mechanisms will allow for implementation of a wider range of allocation rules compared to strategyproof mechanisms. One caveat to our approach is that depending on the context, it may or may not be reasonable to assume that agents will default to truth-telling in an NOM mechanism. Further, even if NOM mechanisms do result in more agents reporting truthfully, this may allow very sophisticated agents who do manipulate an NOM mechanism to benefit even more at the expense of the non-strategic agents. If this is the case, it may be normatively undesirable to use an NOM mechanism (for instance, it could exacerbate the inequity concerns raised by Pathak and Sönmez (2008)). How agents react to NOM mechanisms and whether NOM mechanisms actually improve outcomes in practice are interesting empirical questions to pursue in future research.",Obvious manipulations,https://www.sciencedirect.com/science/article/pii/S002205311830629X,11 November 2019,2019,Research Article,184.0
"Ushchev Philip,Zenou Yves","National Research University Higher School of Economics, Russia,Monash University, Australia,CEPR, United Kingdom","Received 25 March 2019, Revised 28 October 2019, Accepted 31 October 2019, Available online 7 November 2019, Version of Record 12 November 2019.",https://doi.org/10.1016/j.jet.2019.104969,Cited by (34)," of the linear-in-means model and investigate its properties. We show that individual outcomes may increase, decrease, or vary non-monotonically with the taste for conformity. Equilibria are usually inefficient and, to restore the first best, the planner needs to subsidize (tax) agents whose neighbors make efforts above (below) the social norms. Thus, giving more subsidies to more central agents is not necessarily efficient. We also discuss the policy implications of our model in terms of education and crime.","There is substantial empirical evidence showing that peer effects matter in education (Calvó-Armengol et al., 2009, Epple and Romano, 2011, Sacerdote, 2011), crime (Ludwig et al., 2001, Patacchini and Zenou, 2012, Damm and Dustmann, 2014), risky behavior (Clark and Loheac, 2007, Hsieh and Lin, 2017), performance in the workplace (Herbst and Mas, 2015), participation in extracurricular activities (Boucher, 2016), obesity (Christakis and Fowler, 2007), environmentally friendly behavior (Brekke et al., 2010, Czajkowski et al., 2017), and tax compliance and tax evasion (Fortin et al., 2007, Alm et al., 2017), among other outcomes. The standard model used in these studies is the so-called ====, which can be written as==== where ==== is the outcome of individual ==== belonging to group ====,==== ==== are the observable characteristics of individual ==== (e.g., age, race, and gender), ==== are the observed exogenous characteristics that are common to all individuals in the same group ====,==== ==== is the number of individuals in group ====, and ==== is an error term. Parameter ==== captures the “social interaction effect” of the average outcome of the reference group on an individual's own outcome; this is the key parameter of interest that is estimated to measure peer effects.====As noted by Blume et al. (2015), Boucher and Fortin (2016), and Kline and Tamer (2019), it is useful to interpret the linear-in-means model as corresponding to a perfect information game in which (1) is the best-reply function of individual ==== choosing action (outcome) ====. The corresponding utility function is such that individuals have a preference to conform to the ==== action of their neighbors in a social network. For this reason, this game is often referred to as the ====. Surprisingly, the theoretical properties of this model in terms of comparative statics, welfare, and policies have not been investigated. On the contrary, the literature on games on networks==== (Ballester et al., 2006, Bramoullé et al., 2014, Jackson and Zenou, 2015, Bramoullé and Kranton, 2016)==== studies the properties of another model, the ====, in which the sum (not the average) of actions (or outcomes) of neighbors affects own action.====Thus, there is a discrepancy between the theoretical analysis of the local-aggregate model and the empirical applications using the linear-in-means model or local-average model. In this study, we analyze the comparative statics, welfare properties, and policy implications of the local average model and show that these properties are very different from those of the local-aggregate model.==== Indeed, we show that the differences between the local aggregate and the local average, although seemingly minor, lead to substantial divergence in both positive and normative prescriptions. In other words, the local-aggregate model fails to approximate the local-average model in each of the following key dimensions: comparative statics, welfare properties, and policy recommendations.====Our main findings are summarized as follows. First, we characterize the Nash equilibrium in the local-average model and show that individual efforts, social norms,==== and aggregate effort are the weighted sums of productivity, whereby the weights are non-linear functions of the taste for conformity. To understand these results, we compare two extreme cases: ==== and ====. Under pure individualism, each agent's equilibrium effort is equal to her intrinsic productivity and is independent of her own social norm. By contrast, under total conformism, all agents choose the same level of effort, which is equal to the weighted mean of individual productivity, whereby the weights are proportional to the degree (numbers of links) of the agents in the network. Whether total effort is higher under pure individualism or total conformism depends on the correlation between the productivity distribution across individuals and the degree distribution of the social network.====Second, we provide comparative statics of individual and aggregate efforts with respect to the key parameters of the model. We focus especially on the taste for conformity. Endogenous social norms give rise to general-equilibrium effects. A complex interplay between these effects may result in a non-monotonic relationship between the taste for conformity and individual efforts. Whether an individual is above or below her social norm is key for understanding the shape of this relationship. Interestingly, in regular networks, aggregate effort remains neutral to changes in the taste for conformity and is always equal to aggregate productivity.====We also study the impact of adding a link on the equilibrium efforts of all agents in the network. ==== agents in the network increase their effort if and only if a link between two agents with sufficiently high productivities is added in the network. This result is driven by the following ====. When a link is formed between two very productive agents, their social norm increases, because the effort of the newly added agent is high. The best response for the agent for whom the social norm increases is to increase her effort. This, in turn, increases the effort of her neighbor, which increases her social norm, and so forth. Note that, when a link is created between a high-productive and a low-productive agent, then the low-productive agent increases her effort, because her social norm increases while the high-productive agent decreases her effort because her social norm is reduced. As a result, the impact of adding this link on the effort of all agents in the network is ambiguous. Using these results, we discuss the ====, whose aim is to determine the link between two agents which, once removed, reduces total crime the most. We show that, irrespective of the network structure, the planner should remove the link between the two most productive agents in the network.====Third, we provide a complete welfare analysis of the local-average model. We derive a necessary and sufficient condition for the equilibrium to be socially optimal. However, this condition is not likely to hold in most networks. Indeed, each agent exerts externalities on her neighbors, which she does not take into account when making effort. In particular, when the effort of agent ===='s neighbor (say, agent ====) is below her own social norm, then an increase in ===='s effort increases the social norm of ====, which has a negative impact on ===='s conformist utility, because ===='s effort is now further away from her own social norm. In this case, agent ==== exerts a negative externality on her neighbor ====. To restore the first best, the planner taxes agents who exert negative externalities on their neighbors. If the effort of agent ===='s neighbor (say, agent ====) is ==== her own social norm, then the reasoning is the same in reverse, so that to restore the first best, the planner ==== agents who exert ==== externalities on their neighbors. This is very different from the policy implications of the local-aggregate model, in which agents always exert positive externalities on their neighbors so that the planner always subsidizes agents and gives higher subsidies to more central agents. Here, if central agents have higher productivity, they are more likely to exert negative externalities on their neighbors, since the latter are more likely to have effort below their own social norms. For example, in a star-shaped network, if the central agent has, on average, higher productivity than that of the peripheral agents, in the local-aggregate model, to restore the first best, the planner gives the highest subsidy to the central agent. By contrast, in the local-average model, the planner taxes the central agent and subsidizes the peripheral agents.====We also consider different extensions of our benchmark model. First, we extend our utility function so that agents have different tastes for conformity. We show that all our results are robust to this extension. Second, we consider an ==== model in which agents benefit from deviating from the social norm of their friends. We show that if agents are not too anti-conformist, then our results hold even if some agents provide zero effort in equilibrium. However, when agents become more anti-conformist, then either no equilibrium exists or multiple equilibria prevail. We also consider a model in which agents may want to make effort above the average effort of their friends. In this model, contrary to our benchmark model in which agents either overinvest or underinvest in efforts compared to the first best, we show that they tend to mostly overinvest, because they always want to exert efforts above the social norm of their neighbors. Finally, we extend our model to directed and weighted networks and show that all our results are robust to this extension.====Next, we study the implications of our model for network formation. Specifically, we consider a two-stage model in which, in the first stage, agents form links, and in the second stage, they exert effort. We show that, in the local-aggregate model, the unique pairwise Nash equilibrium is the complete network. On the contrary, in the local-average model, the unique pairwise Nash equilibrium is the complete ==== network in which agents of the same type form a complete network but never create links with agents of the other type. In other words, the local-average model provides a simple explanation of homophilous behavior, whereas the local-aggregate model fails to do so.====Finally, we discuss the differences in policy implications of the local-average and the local-aggregate models. We show that, in the former model, group-based policies are more efficient while in the latter model, it is better to implement individual-based or key-player policies.====  Other researchers have studied the local-average (conformist) model in network games.==== Patacchini and Zenou (2012) and Liu et al. (2014) characterized Nash equilibrium and showed that it exists and is unique; Blume et al. (2015) and Golub and Morris (2017) introduced imperfect information====; Boucher (2016) embedded the local-average model into a network formation model, while Olcina et al. (2017) embedded it into a learning model.==== To the best of our knowledge, ours is the first study analyzing the comparative statics properties of the local-average model as well as its welfare and policy implications. Ours is also the first study to examine how adding or removing a link changes the effort of all agents in the network.====One may argue that many peer-effect empirical studies cannot distinguish between the local-average and the local-aggregate model because, in the usual case, the size of the reference group is constant in the sample. For example, if the reference group is the neighborhood, the class, or co-workers, then the network is the same for everyone, namely, a complete graph in which all the students in a class, residents of a neighborhood, or employees of a firm are interlinked. Fortunately, because of network data availability, many recent studies have precisely described the network of agents (see, e.g., Christakis and Fowler, 2007, Bramoullé et al., 2009, Calvó-Armengol et al., 2009, Banerjee et al., 2013; for overviews, see Breza, 2016, Jackson et al., 2017) and therefore, can easily distinguish between the two models. Thus, the results of the present study can be used to derive adequate policy recommendations for each model.====The rest of the paper unfolds as follows. In Section 2, we develop the local-average model and characterize the best response functions. In Section 3, we study the comparative statics properties of the model. In Section 4, we investigate the welfare properties of the local-average model. Section 5 considers different extensions of our model. In Section 6, we examine the policy implications of our results. Finally, Section 7 concludes. All proofs are in Online Appendix A. Online Appendix B provides a comparison between the local-average and the local-aggregate model. In Online Appendix C, we provide a probabilistic interpretation of our model. In Online Appendix D, we provide a simple example that shows how a mean-preserving spread of the productivity impacts own and aggregate outcome. In Online Appendix E, we provide additional results and examples on the comparative statics of the taste for conformity while in Online Appendix F, we compare equilibrium and first-best outcomes for specific networks. In Online Appendix G, we consider different extensions of our model.",Social norms in networks,https://www.sciencedirect.com/science/article/pii/S0022053119301206,7 November 2019,2019,Research Article,185.0
"Bochet Olivier,Tumennasan Norovsambuu","Division of Social Science, New York University Abu Dhabi, United Arab Emirates,Department of Economics, Dalhousie University, Canada","Received 2 August 2018, Revised 13 October 2019, Accepted 18 October 2019, Available online 28 October 2019, Version of Record 28 November 2019.",https://doi.org/10.1016/j.jet.2019.104952,Cited by (2),"Truthtelling is often viewed as focal in the direct mechanisms associated with strategy-proof decision rules. Yet many direct mechanisms also admit ==== whose outcomes differ from the one under truthtelling. We study a model that has been widely discussed in the mechanism design literature (====) and whose strategy-proof and efficient rules typically suffer from the aforementioned deficit. We show that when a rule in this class satisfies the mild additional requirement of replacement monotonicity, the set of Nash equilibrium allocations of its preference revelation game is a complete lattice with respect to the order of Pareto dominance. Furthermore, the supremum of the lattice is the one obtained under truthtelling. In other words, truthtelling Pareto dominates all other ====. For the rich subclass of weighted uniform rules, the Nash equilibrium allocations are, in addition, strictly Pareto ranked. We discuss the tightness of the result and some possible extensions.","In the mechanism design literature, the single-peaked preference domain has played a central role. Most importantly, it paved a way out of the many impossibility results on the design of prior-free mechanisms. The celebrated Gibbard and Satterthwaite theorem (see Gibbard (1973) and Satterthwaite (1975)) showed the impossibility of designing efficient and strategy-proof rules that would escape the dictatorship predicament under arbitrary preferences. In contrast, within the confine of the single-peaked domain, possibility results emerge. In a pathbreaking paper, Moulin (1980) characterizes the class of generalized median voting rules when the feasible set is made of all points on a line. On the private goods front, Sprumont (1991) studies the problem of allocating a divisible and nondisposable good.==== Sprumont (1991) characterizes a remarkable rule: the uniform rule which is uniquely characterized down by efficiency, strategy-proofness and a fairness requirement. The Sprumont model has received a great deal of attention in the mechanism design literature, from alternative characterizations of the uniform rule (see e.g. Ching (1994), Thomson, 1994a, Thomson, 1994b, Thomson, 1995, Thomson, 1997), to the exploration of different families of rules (Barberà et al. (1997), Moulin (1999)), or the extensions of the model and the preference domain (see e.g. Adachi (2010), Bochet et al. (2013), Massó and Neme (2004) among others).====In this paper, we show an unexpected property for a rich family of rules in the Sprumont model. We consider the largest class identified in the literature, the ====, characterized in Barberà et al. (1997) by the combination of ====, ==== and ====. Notice that each sequential allotment rule is fully implementable in dominant strategies by its direct revelation mechanism—this can be seen for instance following the results in Mizukami and Wakayama (2007). However, with the exception of dictatorship-type rules, the sequential allotment rules admit a plethora of Nash equilibria whose outcomes differ from the rule under truthtelling in their preference revelation games. Considering a rule as a direct revelation mechanism, our main result is as follows. We show that the set of Nash equilibrium allocations of any such rule is a complete lattice ====.==== Every complete lattice has a well-defined supremum and infimum. We show that the former is the allocation obtained under truthtelling, hence truthtelling Pareto dominates all the other Nash equilibrium outcomes. The infimum of the lattice may, on the other hand, be rule-specific. Nevertheless, we show that for any sequential allotment rule for which the agents' initial guaranteed levels are invariant to regime changes, the infimum of the lattice is the allocation formed by these initial guaranteed levels.==== For instance, in the case of the uniform rule the infimum of the lattice is the equal division allocation. Finally, for the special case of the weighted uniform rules (an extension of the uniform rule to a non-symmetric treatment of agents), we show that the Nash equilibrium allocations are in fact strictly Pareto ranked.====In the remaining section of the paper, we check the tightness of our results and discuss some variations of the model (or the preference domain) where the lattice result may or may not hold. We first investigate the role that replacement monotonicity plays for our result with two examples. In Example 4.1 we construct a rule that violates replacement monotonicity and whose set of Nash equilibrium allocations is not a lattice with respect to the order of Pareto dominance. This hints that replacement monotonicity is essential for the result—and it certainly is in our proof. In addition, the rule considered there is efficient and ====, demonstrating that our result cannot be proved if we just impose these two properties. Example 4.3 shows that replacement monotonicity is however not necessary for the lattice structure to hold. This suggests that replacement monotonicity can be replaced with weaker requirements in our main theorem. While this remains an open question at this stage, we show that for a rule that is efficient and strategy-proof, ==== is a necessary (but not sufficient) condition for the lattice structure to hold.====Next, we look at (i) a different preference domain for which the lattice result may hold, and (ii) a possible variant of the model. On the former, Massó and Neme (2004) show that there are efficient and strategy-proof rules in the Sprumont model for the set of (partially) single-plateaued preferences. We show that the lattice result does not hold for the extended uniform rule characterized on this domain. Regarding the latter, we consider the model of Moulin (1980). We show that for the target rules (Thomson, 1993)—the subclass of the generalized median rules that satisfy replacement monotonicity (in welfare)—the set of Nash equilibrium public good levels are strictly Pareto ranked. On the other hand, for the well-known median rule, the lattice result does not hold.====The paper is organized as follows. In Section 2 we introduce the model and the necessary definitions. In Section 3, we present our main results. In Section 4, we discuss our results and some extensions to variants of the model. We offer some concluding remarks in Section 5.",Dominance of truthtelling and the lattice structure of Nash equilibria,https://www.sciencedirect.com/science/article/pii/S0022053118304319,28 October 2019,2019,Research Article,186.0
"Burkett Justin,Woodward Kyle","Georgia Institute of Technology, United States of America,University of North Carolina at Chapel Hill, United States of America","Received 7 December 2018, Revised 23 July 2019, Accepted 17 October 2019, Available online 25 October 2019, Version of Record 30 October 2019.",https://doi.org/10.1016/j.jet.2019.104954,Cited by (10),"We model multi-unit auctions in which bidders' valuations are multidimensional ====. Under a natural constraint on aggregate demand we show that the ==== uniform-pricing rule admits a unique equilibrium with a simple characterization: bids are identical to those submitted in a single-unit first price auction. The form of equilibrium bids suggests that last accepted bid uniform-pricing is a generalization of single-unit first-pricing: in both auctions winners pay the highest market clearing price. Contrasting the separating equilibrium of the last accepted bid auction, we show that equilibrium bids in pay as bid and first rejected bid uniform price auctions must pool information. Thus other common multi-unit auction formats cannot generalize single-unit first-pricing, in which equilibria do not pool information. The existence of a unique equilibrium implies that price selection may be an additional tool for avoiding the zero-revenue equilibria which exist in the first rejected bid uniform price auction. Finally, we show that equilibrium bids in our ==== model are significantly flatter than in an analogous random supply model, suggesting that uniform price auction bids may not be as steep as commonly believed.","In a multi-unit uniform price auction, bidders submit demand functions to a seller who awards ==== homogeneous units to the highest ==== bids at a single clearing price. This per-unit price may be the last accepted bid, the first rejected bid, or any intermediate amount.==== These rules govern well-known large scale auctions, such as those run by the U.S. Treasury and the independent system operators in charge of electricity distribution. They are also used to model decentralized markets under the guise of competition in supply functions (Klemperer and Meyer, 1989)====Among common multi-unit auction formats, including pay-as-bid and Vickrey pricing, the uniform price rule is especially attractive because it awards homogeneous units at a homogeneous price.==== The uniform price auction is fair, in the sense that bidders never pay less than other bidders for the same number of units won. If a goal of the auction is price discovery, the uniform price rule is a natural choice. This pricing rule also aligns well with textbook descriptions of decentralized markets (see, e.g., Kyle (1989); Vives (2011)).====However, a number of studies emphasize unappealing properties of the equilibria that may arise in these auctions. First, when bidders have private information, their bidding problem can be complex. A bid for any unit — except possibly for the first unit — may determine the clearing price, and hence influence the total amount paid. Consequently, bidders strategically reduce their bids below their demand, complicating inferences about the distribution of opposing bids when bidders have private information (Vickrey, 1961; Ausubel et al., 2014). Second, there may be equilibria that generate little to no revenue for the seller, and reveal no private information (Back and Zender, 1993). In these low-revenue equilibria bidders tacitly agree on a division of the goods and submit demand curves that implement this division at the lowest possible price. A third, closely-related issue is that there may be multiple equilibria, each with different allocations. Finally, and certainly true of many of the low-revenue equilibria, it has been argued that the equilibria of these auctions are generally inefficient (Ausubel et al., 2014).====In this paper, we show that incentives in uniform price auctions — and the undesirable properties they imply — are closely tied to the common modeling assumption that the per-unit price is the equal to the first rejected bid, and equilibrium predictions are dramatically affected if a different clearing price is selected. In practice, market clearing prices are computed as the price which would clear a Walrasian market, taking reported bid curves to be actual demand curves. With discrete goods this typically leaves some discretion to the seller: when ==== units are available, markets will (weakly) clear as long as the price is between the ====th and ====th highest bids. Existing literature on multi-unit auctions has tended to focus on the selection of the lowest market clearing price, which we term the ==== (FRB).==== We focus instead on the ==== (LAB) pricing rule, in which the highest market clearing price is selected.====This apparently small change in the rules of the auction alters the alignment of bidding incentives and has dramatic effects on equilibrium behavior. In a first rejected bid auction the price is set by the bid for a supra-marginal unit, while in a last accepted bid auction the price is set by a bid for an exactly marginal unit. Then in a last accepted bid auction the probability a small increase in bid affects the resulting allocation is identical to the probability it affects the market clearing price, while in a first rejected bid auction these probabilities differ. While theoretical literature on uniform price auctions has noted the possibility of multiple pricing rules (see, e.g., McAdams (2003)), a natural reading suggests that the practical difference in price selection rules has been ignored due the large quantities typically sold in real-world auctions: when the number of units is large, there is typically not much difference between the level of the last accepted bid and the level of the first rejected bid.==== We show that the implicit assumption that the exact location of the market clearing price is unimportant elides the different incentives induced by selection of one price or another. In spite of the fact that per-unit incentives do not change much, the aggregation of small changes yields a clear distinction between the two pricing mechanisms.====Our model of multi-unit auctions employs a novel private value framework, in which bidders have multidimensional private information about their demand for any number of units of an indivisible good.==== Such values can arise if, for example, small agents with independent private values route orders through large institutional bidders. Our multidimensional private value model allows the flexible specification of each bidder's expected number of units demanded at each price, referred to as a bidder's mean demand curve, while imposing restrictions on the distribution of realized demand curves about this mean demand curve. We show that this model, applied to the last accepted bid uniform-pricing rule, is tractable and yields equilibrium behavior with several desirable properties, especially when compared to existing models.====We first show that, with two bidders, the equilibrium bids for ==== marginal unit take the form of bids in an asymmetric first price auction for a ==== unit. An immediate implication is that many of the results from the first price auction literature translate directly to our environment. For example, we are able to draw a connection between the relationship between two bidders' mean demand curves and how aggressively they bid in the auction. Extending the work of Maskin and Riley (2000), we classify bidders' mean demand curves as “strong” or “weak” and show that distributional weakness leads to aggressive bidding. If the two bidders' demands are symmetric, the bid curves can be solved analytically as in the first price auction. This analytical result extends cleanly to the case of ==== symmetric bidders, provided an algebraic condition on demand (market balance) is satisfied. Whereas the modern literature on multi-unit auctions emphasizes differences between equilibrium bidding strategies in multi-unit auctions and single-unit auctions, we find a close connection between our model of the uniform price auction and the standard model of the first price auction: the two auctions may have essentially identical equilibrium strategies.==== This is in stark comparison to the first rejected bid and pay as bid auctions, which we prove cannot satisfy this equivalence.====Our closed-form results rely on an algebraic assumption on demand which we term ====. In a balanced market with ==== bidders, each collection of ==== bidders exactly demands the ==== units available for auction. Alternatively, any bidder's opponents exactly cover market supply. Under market balance the tie between last accepted bid and first price auctions is intuitive. A bidder's bid for unit ==== is relevant when it sets the market clearing price. This occurs when the bidder has outbid exactly ==== of her opponents' bids. When participants' values are given by ordered draws from a common distribution, this is equivalent to the bid for unit ==== defeating ==== draws from a common distribution, as long as the mapping from values to bids is unit-independent. Conditional on beating exactly these ==== draws the bidder pays her bid for this unit ==== times over, generating exactly the same incentives as the first price auction. As we discuss in Section 3, this “====-vs-====” intuition neatly captures the inability of first rejected bid and pay as bid auctions to generalize first price auctions.====The extent to which a last accepted bid auction generalizes a first price auction depends on the features which should be replicated. In the order statistic value model with private values, market balance is sufficient to imply an equilibrium in the last accepted bid auction which is essentially identical to equilibrium in an equivalent first price auction. If market balance is not satisfied, we find that qualitative features of equilibrium, such as separation and uniqueness, are satisfied by both last accepted bid and first price auctions (see Appendix D). Whether or not market balance is satisfied, these features are not shared by the first rejected bid or pay as bid formats. Thus while our closed-form results depend on the relationship between demand and supply, the shared equilibrium properties of the last accepted bid and first price auctions suggest that the pricing rule itself is the source of the comparison.====We emphasize the close connection between single-unit first price auctions and multi-unit last accepted bid auctions by comparing the last accepted bid auction to the first rejected bid uniform price and pay as bid rules. The comparison to first rejected bid is natural: an auctioneer implementing a uniform price auction has a choice of pricing rules, and our results suggest that the last accepted bid rule is preferable. The comparison to pay as bid is also of economic interest. We show that it is impossible for a multi-unit pay as bid auction to generate equilibrium bids which are identical to single-unit first price equilibrium bids, giving further evidence for our claim that last accepted bid pricing generalizes single-unit first pricing. Since multi-unit auctions are typically implemented as uniform price or pay as bid, our comparison of payment rules provides a guide for avoiding some undesirable properties of the first rejected bid and pay as bid formats by implementing a last accepted bid auction instead.====In our comparison of auction formats, we first show that the first rejected bid and pay as bid auctions do not admit separating equilibria. Solving analytically for equilibrium in the first rejected bid and pay as bid auctions involves the determination of “pooled intervals” in the underlying type space, which imply regions over which the first order conditions cannot be naïvely applied. In the first rejected bid auction, pooling arises for relatively low valuations for which the marginal gain associated with an increase in winning probability is outweighed by the increased probability of setting the market price. In the pay as bid auction, pooling arises due to the constraint that bids be weakly decreasing while agents would sometimes prefer to submit nonmonotone bid schedules. In both cases this provides a clear contrast with the unique separating equilibrium we find in the last accepted bid auction. All equilibria in the first rejected bid and pay as bid auctions are inefficient, in comparison with the unique and efficient separating equilibrium we characterize in last accepted bid auctions with market balance.====Second, we show that the zero revenue, collusive-seeming equilibria of the first rejected bid auction cannot be sustained in the last accepted bid auction. In a first rejected bid auction, participants can tacitly agree on an allocation and bid aggressively for quantities they do receive, and nonaggressively for quantities they do not receive. Because the market price is set by the bid for a supra-marginal quantity, any deviation to obtain a greater quantity requires a dramatic increase in payment, which is not optimal. In the last accepted bid auction the market price is set by the bid for an exactly marginal quantity. Then if a bid profile guarantees a low clearing price there are strong incentives to increase bids, and there does not exist a generically low-revenue equilibrium. A subtle change in the uniform pricing rule can have beneficial effects on equilibrium selection, in line with other results on removing these equilibria with small adjustments to the uniform price model; we discuss related literature in Section 1.2.====Finally, we examine the role of private information in our model by analyzing the large-supply limit our unique equilibrium. This allows a clean comparison of uniform price auctions with multidimensional private information to much of the literature on uniform price auctions, which assumes common information but random supply (see Klemperer and Meyer (1989); Ausubel et al. (2014), and others): when market supply is large, private information in our model essentially disappears.==== Furthermore, since quantities are effectively divisible when aggregate supply is large, there is no difference between first rejected and last accepted bid pricing, holding fixed a profile of (continuous) demand curves.====In the large-supply limit, bids in our private information model are shallower than in common information models with random supply. The slope of equilibrium bids relates to per-unit incentives, and the question of who bidders are competing against. In existing models with full information, bids for high-value units compete against bids for high-value units and bids for low-value units compete against bids for low-value units, while in our model bids for high-value units compete against bids for low-value units, and vice-versa. Bidders with high values bid more aggressively against other high-value bidders than against low-value bidders, and bids are steeper in the former case than in the latter. This adds an important caveat to the common understanding that bids in uniform price auctions will be relatively steep: existing results have rested on the manner in which randomness enters the auction.====Taken as a whole, our results suggest that care be paid to the selection of salient features of auction models. For example, bidders report truthfully in the canonical equilibrium of a single-unit second price auction; it is known that the optimality of truthful reporting does not extend to multi-unit first-rejected bid pricing (see above, and also Back and Zender (1993), Engelbrecht-Wiggans and Kahn (1998), Wang and Zender (2002), and Ausubel et al. (2014) among many others). The optimality of truthful reporting in a single-unit second price auction derives from its equivalence to a single-unit Vickrey auction. With multiple units this equivalence disappears. If the defining characteristic of a single-unit second price auction is truthful reporting, the Vickrey format is the multi-unit generalization. In a similar way, it is also known that the intuitive behavior in a single-unit first price auction does not translate to multi-unit pay as bid auctions, in spite of bidders paying their bids in both settings (see, e.g., Woodward (2016)). Our results show strategic equivalence between single-unit first price auctions and multi-unit last accepted bid auctions, suggesting that the strategically salient feature of these auctions is the selection of the highest market clearing price. To our knowledge this has gone unaddressed in the literature.",Uniform price auctions with a last accepted bid pricing rule,https://www.sciencedirect.com/science/article/pii/S0022053119301048,25 October 2019,2019,Research Article,187.0
"Kane Robert F.,Peretto Pietro F.","International University of Japan, Japan,Duke University, United States of America","Received 24 January 2018, Revised 17 October 2019, Accepted 22 October 2019, Available online 25 October 2019, Version of Record 4 November 2019.",https://doi.org/10.1016/j.jet.2019.104964,Cited by (2),"We model distribution, the delivery of goods to customers, as an activity governed by its own technology and undertaken by firms subsequently to production. We then use the model to investigate how distribution shapes innovation-driven economic growth. We contrast two canonical specifications of distribution costs, iceberg vs. per-unit. The per-unit cost implies that factory-specific productivity improvements cannot sustain steady-state growth. Quality improvement, instead, raises the services that customers obtain from each unit of the good so that firms can increase the volume of services without increasing the volume of shipments. Unless technological advancements allow the distribution cost to fall to zero, quantity growth must cease and growth must be driven by quality improvement. More generally, the ratio of distribution to manufacturing unit costs must be constant in steady state. The iceberg cost delivers this property by assumption. The per-unit distribution cost, instead, yields an endogenous structure of the costs of serving the market.","Distribution, defined as the delivery of goods to customers, is an essential component of economic activity. This fact supports the conventional wisdom that infrastructure—in particular the construction, maintenance and improvement of transportation and communication networks—is important for economic performance. Consider the following quote: ====The empirical evidence on the role of infrastructure is, nevertheless, far from clear. Fogel (1964), for instance, estimates that the removal of all railroads in 1890 would have only reduced the GNP of the United State in the same year by 2.7%. Even though recent work (Donaldson and Hornbeck, 2016) increases this estimate to 3.22%, the effect remains small.==== One plausible reason for such small effects is that this literature assumes that railroads had no effect on the rate and direction of technical change. We argue that questioning this assumption and studying the relationship between distribution costs and innovation is essential to improve our understanding of the role that infrastructure, shipping, retail and so on, play in shaping economic growth.====To our knowledge, growth economics has not considered the topic. The closest literature that we could find is that on the role of public capital, pioneered by Barro (1990), which is consistent with the evidence that infrastructure raises the level of economic activity (Aschauer, 1989).==== This literature, however, models infrastructure as public capital that enters the aggregate production function and thus fails to disentangle the production of goods (manufacturing) from the delivery of goods to the customer (distribution). We argue that to make progress we need models that disentangle the two stages. Empirically, distribution is a large part of the economy. Burstein et al. (2003), for example, show that distribution costs account for over 40% of the retail price of a typical consumer good in the United States and over 60% in Argentina. Yet, although distribution has received substantial attention in international economics, it has been neglected in growth economics.====Our goal in this paper is to shed light on the relationship between growth dynamics and distribution. We build an R&D-driven growth model that incorporates distribution and features three types of innovation: cost reduction, quality improvement, and variety expansion. The framework allows us to make progress in two complementary dimensions.====First, we model distribution as a distinct economic activity and analyze its interaction with endogenous innovation. While an improvement in the distribution technology has no direct impact on the production process, it affects the incentives to engage in R&D because it reduces the cost of delivering goods to customers. We show that this disentanglement of production from distribution has important implications for how one ought to model distribution costs and for how to discipline the theory with data.====Second, in our framework distribution discriminates among the different types of innovation and thus has ==== effects on how the economy grows. For example, if the distribution technology contains a constant per-unit component, cost reduction must eventually cease and is thus not an engine of steady-state growth. The intuition is that as manufacturing productivity rises, the distribution cost becomes the dominant driver of the product's price. Consequently, cost reduction becomes less effective at increasing demand and eventually the rate of return to cost reduction becomes so low that it is unprofitable to engage in it. More generally, in our framework the steady-state rate of cost reduction cannot exceed the rate of technical progress in distribution. Quality innovation, on the other hand, is not subject to this mechanism and can thus be an engine of steady-state growth. Since this is a key insight of our analysis, it is worth reviewing it in some detail.====It is commonly held that cost reduction and quality improvement are isomorphic. The classic exposition is due to Spence: ====The difference between cost reducing R&D and quality improving R&D is related to a policy issue that predates endogenous growth theory. The President's Commission on Industrial Competitiveness once stated: “It does us little good to design state-of-the-art products, if within a short time our foreign competitors can manufacture them more cheaply” (The White House, 1985). Japanese manufacturers, who at the time were the main competitors to American firms, did engage in more cost reduction. Mansfield (1988) found that American firms devoted two-thirds of their R&D expenditures to product innovation while Japanese firms only devoted one-third, with the remaining R&D expenditures going to cost reduction. Our model provides an explanation for the differing R&D expenditure shares; the transitional dynamics are such that as economy's develop, they engage in relatively less cost reduction.====Our model is related to a literature, thus far focused only on international trade, that explores the differences between productivity and product quality. Sutton (2007a) shows that when there are internationally traded materials, there is a minimum level of quality that must be attained if a firm or country is to enter world export markets, but no minimum productivity. Hallak and Sivadasan (2013) also break the isomorphism. They do so by assuming that iceberg costs are a declining function of product quality. Because of this assumption, firms producing high quality goods are more likely to export than firms producing low quality goods. The purpose and mechanism of our paper is different from, though highly complementary to, the trade models of Sutton (2007a) and Hallak and Sivadasan (2013). Sutton (2007a) and Hallak and Sivadasan (2013) only allow for a “one time” increase in product quality; in both models firms cannot engage in R&D to improve manufacturing productivity. We allow firms to engage in both quality improvement and cost reduction and study the resulting long-run dynamics.====In addition to providing results on the difference between quantity and quality, our model sheds new light on the difference between iceberg and per-unit frictions. Indeed, the paper's title pays homage to the ====, which states that the introduction of a per-unit (tariff or shipping) cost reduces the relative price of expensive goods (Alchian and Allen, 1964). Building on that work, Sørensen (2014) and Irarrazabal et al. (2015) show that increases in per-unit frictions lead to higher welfare losses compared to increases in iceberg frictions in a heterogeneous trade model. We argue that the iceberg specification implicitly assumes that the technologies for manufacturing and distribution are identical, whereas the per-unit formulation breaks the linkage between the two.====The remainder of the paper is organized as follows. Section 2 presents the model. Section 3 solves for the rates of return to all R&D activities. Section 4 solves the model. Section 5 presents the transitional dynamics and analyzes the effects of changes to the distribution technology. Section 6 extends the model to include endogenous innovation in distribution. Finally, section 7 concludes.",More apples vs. better apples: Distribution and innovation-driven growth,https://www.sciencedirect.com/science/article/pii/S0022053119301140,25 October 2019,2019,Research Article,188.0
"Alger Ingela,Weibull Jörgen W.,Lehmann Laurent","Toulouse School of Economics, CNRS, University of Toulouse Capitole, Toulouse, France,Institute for Advanced Study in Toulouse, Toulouse, France,Stockholm School of Economics, Sweden,Department of Ecology and Evolution, University of Lausanne, Switzerland","Received 4 September 2018, Revised 25 September 2019, Accepted 18 October 2019, Available online 25 October 2019, Version of Record 5 November 2019.",https://doi.org/10.1016/j.jet.2019.104951,Cited by (17),"During human evolution, individuals interacted mostly within small groups that were connected by limited migration and sometimes by conflicts. Which preferences, if any, will prevail in such scenarios? Building on population biology models of spatially structured populations, and assuming individuals' preferences to be their ====, we characterize those preferences that, once established, cannot be displaced by alternative preferences. We represent such uninvadable preferences in terms of fitness and in terms of material payoffs. At the fitness level, individuals can be regarded to act as if driven by a mix of self-interest and a Kantian motive that evaluates own behavior in the light of the consequences for own fitness if others adopted this behavior. This Kantian motive is borne out from (genetic or cultural) kin selection. At the material-payoff level, individuals act as if driven in part by self-interest and a Kantian motive (in terms of material payoffs), but also in part by other-regarding preferences towards other group members. This latter motive is borne out of group resource constraints and the risk of conflict with other groups. We show how group size, the migration rate, the risk of group conflicts, and cultural loyalty shape the relative strengths of these motives.","Preferences are fundamental to economic theory.==== If preferences are transmitted across generations and if they affect the expected survival and reproduction—the fitness—of their bearer: which preferences are likely to be favored by evolution and which preferences are likely to disappear? Analysis of the long-term evolution of preference distributions can help understand the proximate drivers and motivation of human behavior in social and economic interactions (Hirshleifer, 1977, Bergstrom, 1996, Binmore, 1998, Robson, 2001, Newton, 2018, Alger and Weibull, 2019). Here we build on previous work on strategy evolution in structured populations (Lehmann et al., 2015) by studying preference evolution in such populations.====For more than a million years, our ancestors most likely lived in groups of hunter-gatherers (probably ranging from 5 to 150 grown-ups), extending beyond the nuclear family (Grueter et al., 2012, Malone et al., 2012, Van Schaik, 2016, Layton et al., 2012). This population structure, whose defining features are small group size and limited migration between groups (i.e., not all individuals migrate), is thus part of the environment of evolutionary adaptation of the human lineage (e.g., Van Schaik, 2016). Analysis of the long-term evolution of preferences should thus take such population structure into account. We here do exactly that, and we ask how such structural features as group size, migration rates between groups, and the risk of conflicts between groups, determine the qualitative nature of the preferences that evolution favors. Combining the economics paradigm of utility-maximizing behavior with methods from population genetics, we obtain predictions about the nature of individuals' preferences and motivations in the canonical model of evolution in structured populations, the so-called island model of migration originally due to Wright, 1931, Wright, 1943. The model allows us to examine both genetic and cultural transmission of preferences in such structured populations.====The island model is a textbook evolutionary biology model (see, e.g., Cavalli-Sforza and Bodmer, 1971, Frank, 1998, Rousset, 2004, Hartl and Clark, 2007), which formally captures in a tractable and stylized way the fact that all natural populations, human or otherwise, are structured into small groups (or bands, or villages; patches for plants) connected to each other by limited migration (or dispersal). Limited migration causes limited genetic and/or cultural mixing in the population, and this results in several individuals from the same group possibly having a recent common ancestor. For example, suppose that a genetically transmitted new trait suddenly appears in one individual. In the next generation, multiple carriers of the new trait may coexist in the same group. Hence, the immediate descendants of the initial mutant are more likely to interact with each other than are individuals sampled at random from the whole population. Such assortative matching, induced by limited migration, even when the mutant trait is rare in the population at large, tends to favor mutant behaviors that promote the survival and/or reproductive success of others in their group. The reason is that such behavior is more likely to benefit other mutants than it would be if all offspring always migrated and matching therefore would be uniformly random (Hamilton, 1964, Hamilton, 1971; Grafen, 1985, Frank, 1998, Rousset, 2004). This is the so-called mechanism of ==== in evolutionary biology (Maynard Smith, 1964).==== In the biology literature, assortative matching between pairs of individuals is usually quantified by the ====–which indicates the likelihood that interacting individuals share a common ancestor–a quantity that depends on such features of the population structure as group size and migration rates.====By the same token, however, individuals who share a local common ancestor are also more likely to expose each other to fitness externalities, than are randomly selected individuals from the overall population. Indeed, through the local interactions, which occur in islands of finite size, related individuals may harm or enhance each other's fitnesses (think of young siblings fighting over candy, or individuals teaming up to fight off a common enemy), and such externalities have an impact on selected traits (Hamilton, 1971, Schaffer, 1988, Frank, 1998, Rousset, 2004). As assortative matching and local fitness externalities can, in general, not be separated, their joint effects need to be taken into account in order to understand the evolutionary success of traits under limited dispersal, a question that has received much attention in the evolutionary biology literature (see e.g. Hamilton, 1967, and Taylor, 1992a, Taylor, 1992b, for pioneering and paradigmatic examples, and Frank, 1998, and Rousset, 2004, for general theoretical treatments).====While clearly relevant for understanding the evolution of traits relevant in human social interactions, the evolutionary biology literature is yet of limited direct value for economists, because in the bulk of these analyses: (a) the focus is on the evolution of strategies, not preferences, (b) predictions are derived at the level of basic fitness components, such as reproduction and survival, and not at the level of the material payoffs obtained in strategic interactions, (c) transmission is genetic instead of cultural, while cultural evolution is also relevant for the understanding of human behavior. Our model enriches the analysis in all of these dimensions.====We propose a framework in line with that of economists and game theorists, and model the following thought experiment that takes place in a large population over an infinite sequence of demographic time periods. The population is structured into a large number of groups or islands of equal size. Within each group or island, individuals engage in a strategic interaction in which all individuals' strategy choices may affect the material payoffs to all participants. The strategic interaction is modeled as a game in material payoffs, and the game may be arbitrarily complex and may take place over many stages within each demographic time period. By material payoff we mean a one-dimensional summary measure, like income (or calories). The expected material payoffs, realized in a demographic time period, in turn determine the fitness of each individual in the population in that demographic time period. An individual's fitness is defined as the expected number of individuals in the following time period who have acquired their trait from him (or her). If transmission is genetic, an individual's fitness is the number of his surviving offspring and the individual himself if he survives. If transmission is cultural, an individual's fitness is the number of individuals in the next time period who acquired their cultural trait from this individual. Offspring may migrate to other groups or islands, or stay in their natal group or island. Many different transmission scenarios are covered by this model framework. For instance, generations may or may not be overlapping, islands may wage wars against each other, traits may be transmitted culturally from parent to child or by imitation of materially successful individuals, etc.====In all our scenarios, genetic and cultural, the population is initially homogeneous; all individuals are ==== identical. Suddenly, a different, mutant heritable trait spontaneously appears in exactly one individual. The original, resident trait is uninvadable if there exists no mutant trait, such that the initial mutant produces enough descendants for its trait to be maintained in the population in the long run.====To study preference evolution, we let the heritable traits be continuous utility functions, defined over all strategy profiles that are possible in the material game that represent the interaction on each island. Together with the individual's (probabilistic) belief about other group members' strategy choices, an individual's utility function guides his or her choice of strategy in the local interaction. We evaluate a utility function's fitness consequences for its carriers in terms of the expected material payoffs that result in all (Bayesian) Nash equilibria under incomplete information, that is, when each individual's utility function is his or her private information, but individuals' beliefs about each others' strategies are consistent with some (Bayesian) Nash equilibrium. We ask if there exist utility functions that are uninvadable in the sense that any mutant utility function does worse, in terms of its carriers' fitnesses, than the residents, in ==== equilibria. Thus bridging the gap between economics and biology, we obtain links between preferences, material incentives, and population structure (including migration and potential group conflicts). The following four main results emerge from our analysis.====First, we obtain a necessary and sufficient condition for a utility function to be uninvadable. This characterization says that a utility function is uninvadable if and only if all strategies used in any Nash equilibrium among individuals with this utility function are, when viewed as heritable strategies, uninvadable by other strategies.====Second, we identify a class of utility functions that, for any given game in material payoffs, contains an uninvadable utility function. Each utility function in this class can be interpreted as a mix of self-interest and a Kantian concern, both expressed at the fitness level. Specifically, the Kantian concern, driven by kin selection, consists in evaluating one's behavior in the light of what one's own fitness would be if others in one's group were to behave in the same way. This concern vanishes under unlimited migration (that is, when all offspring always migrate) and when groups are very large.====Third, when material payoffs only have marginal effects on fitnesses (a property which arguably holds for many human interactions), uninvadable preferences generically involve a mix of self-interest, a Kantian concern, and also a concern for neighbors, all concerns being expressed at the level of material payoffs. The weight given to the Kantian motive is then proportional to the coefficient of relatedness, but it also depends on fitness externalities between neighbors. The weight on other group members' material payoffs may be negative (“spite”) or positive (“altruism”), and it depends on the ====, which measures the effect on own fitness that an individual obtains relative to his neighbors by diminishing or enhancing their material payoffs.====Finally, we provide sufficient conditions for the uninvadability of preferences of a particularly simple form, namely, a convex combination of own material payoff and the own material payoff that would arise should all others choose the same behavior. Under these specific conditions, the weight given to the second, Kantian, component is determined by the ====, a coefficient that combines the (standard) coefficient of relatedness with the coefficient of fitness interdependence. This weight allows to determine whether, on balance, equilibrium behaviors are pro-or anti-social, in the sense that equilibrium material payoffs are higher or lower than under selfishness. We show that an increased risk for group conflicts make preferences less anti-social, and, at a critical level of the risk of group conflict, preferences are neither anti- nor pro-social, while at higher risk levels, preferences turn pro-social. Hence, at this intermediate risk of conflict, preferences have only a self-interested and a Kantian component, while at lower (higher) risks, a third component appears, a component that expresses envy or spite if the risk is low, and empathy or altruism if the risk is high. We also show that cultural transmission of preferences may trigger anti-sociality because of local competition for proselytes.====Compared to the existing economics literature on preference evolution in social interactions (see Alger and Weibull, 2019, for a recent survey), our model makes two key innovations.==== First, it explicitly analyzes the effects of population structure and limited dispersal upon behavior and preferences. While Alger and Weibull, 2013, Alger and Weibull, 2016 investigated the evolutionary stability of preferences under incomplete information, they did so in an abstract model of assortative matching which did not explicitly account for the demographics and population dynamics.==== They found that preferences expressing a certain combination of self-interest and a Kantian concern are evolutionarily stable, and that preferences that are behaviorally distinct from these are evolutionarily unstable. They also showed how the weight given to the Kantian concern depends on the assortativity in group formation. While assortativity in those models is treated as an abstract primitive, it here arises explicitly and endogenously from the population structure; group size, rates of survival, migration, and conflicts together determine the probability that rare mutants get to interact with each other—i.e., relatedness. The present model thus contributes to this strand of literature by explicitly modeling the population structure and how it gives rise to assortativity.====Second, it establishes a clear distinction between preferences at the fitness level and preferences at the material payoff level. In the existing economics literature on preference evolution, these are taken to coincide. The model makes it clear that, when preferences are expressed at the level of material payoffs, relatedness must go hand in hand with local fitness interdependence, a force which does not appear in Alger and Weibull, 2013, Alger and Weibull, 2016. We here also show how relatedness and fitness interdependence can be formally traced back to group size and limited migration. While we already made this distinction in Lehmann et al. (2015), we then did not analyze preference evolution. Instead, we asked under what conditions, if any, evolving strategies can be interpreted as chosen by rational individuals endowed with specific utility functions (we examined three candidate utility functions, two of which are described above). The value added of the present paper is that we here analyze preference evolution, rather than strategy evolution, in group-structured populations. In addition, we (a) examine other utility functions than those used to establish the “as if” results in Lehmann et al. (2015), (b) obtain new results concerning fitness interdependence and scaled relatedness, and (c) analyze a wider class of evolutionary scenarios.====Apart from our previous work, the most closely related work is by Akçay and van Cleve (2012). They investigated the evolutionary stability of preferences parameterized by scalar traits (in the vein of Heifetz et al., 2007a, Heifetz et al., 2007b). In addition to focusing on complete rather than incomplete information, their model differs from ours in two broad respects. First, since they focus only on the effects of traits on reproduction under genetic transmission, they do not obtain results for preferences over strategy profiles or material payoffs distinct from fecundity. Second, they focus only on necessary first-order conditions. These conditions express how many offspring an individual is willing to forgo, at the margin, in order to marginally increase the number of offspring of other group members.====The paper is organized as follows. Section 2 describes the model and provides a characterization of an uninvadable trait. Section 3 presents the analysis. In Section 4 we illustrate our results in three canonical evolutionary scenarios, including genetic and cultural evolution, as well as potential “wars” between groups. Section 5 concludes. Mathematical proofs are provided in an appendix.","Evolution of preferences in structured populations: Genes, guns, and culture",https://www.sciencedirect.com/science/article/pii/S0022053118305337,25 October 2019,2019,Research Article,189.0
"Bierbrauer Felix,Winkelmann Justus","Center for Macroeconomic Research, University of Cologne, Germany,Bonn Graduate School of Economics, University of Bonn, Germany","Received 23 January 2019, Revised 26 September 2019, Accepted 18 October 2019, Available online 24 October 2019, Version of Record 4 November 2019.",https://doi.org/10.1016/j.jet.2019.104955,Cited by (3),"We study public goods provision subject to ex post incentive and participation constraints. We also impose a requirement of anonymity. Different public goods can be bundled if sufficient resources are available. The analysis focuses on the ====: Expand provision as much as is resource feasible if no one vetoes - otherwise stick to the status quo. We show that the probability of the all-outcome converges to one as the capacity becomes unbounded. For a given finite capacity, we provide conditions under which the all-or-nothing-mechanism is ex ante welfare-maximizing - even though, ex post, it involves an overprovision of public goods.","We study the following situation: There is a status quo with a limited provision of public goods. Moving towards more goods being provided requires both sufficient resources and sufficient political support. Our main result shows that an increase in capacity, i.e. in resources available to finance public goods, makes it possible to overcome all obstacles to increased public goods provision. It eliminates resistance by those who dislike certain public goods and it eliminates incentives to free-ride on the contributions of others. We also provide conditions under which this mechanism maximizes expected welfare.====The paper contributes to the literature that studies public goods provision from a mechanism design perspective. By and large, the existing literature, reviewed in more detail below, emphasizes the difficulties that are associated with incentive and participation constraints. The second-best mechanisms that respect these constraints typically involve an underprovision of public goods. By contrast, our setting – which, in addition, invokes a requirement of anonymity – gives rise to a second-best mechanism with an overprovision of public goods.====A seminal reference is Mailath and Postlewaite (1990) who show that, as the number of individuals gets large, the probability of public good provision goes to zero under any mechanism that respects incentive and participation constraints. A tempting conclusion therefore is that a requirement of unanimity in favor of increased public good provision makes it impossible to have significant expenditures on public goods. Such expenditures can then be reconciled only with a violation of participation constraints or, equivalently, a use of the government's coercive power to finance public goods, against the will of at least some of the people. Against this background, our analysis shows a possibility to have substantial public goods provision in the presence of participation constraints: An increasing capacity allows to bundle public goods in such a way that moving towards increased expenditures is in everyone's interest.==== There are ==== individuals and there is sufficient capacity to finance ==== additional public goods. Individuals have private information on their valuations of these goods. For anyone else, valuations are taken to be ==== random variables with a mean that exceeds the per capita provision cost and which take values lower than the cost with positive probability. Thus, it is a priori unclear which public goods should be provided.====A mechanism determines which goods are provided and also what individuals have to pay. Admissible mechanisms satisfy participation, incentive and budget constraints. We require that all these constraints hold ex post. Thus, whatever the state of the economy, ex post, no individual prefers the status quo over the outcome of the mechanism, nor does any one individual regret to have revealed her preferences. In addition, the money that is collected from individuals is exactly what is needed to cover the cost of provision. We also impose a condition of anonymity.====Mailath and Postlewaite (1990) have established an impossibility result for the case ====: With many individuals, the probability of public goods provision is close to zero under any admissible mechanism. Mailath and Postlewaite employ participation, incentive and resource constraints that are more permissive than ours. In their analysis, participation constraints are satisfied if all individuals' expected utility under the mechanism is higher than in the status quo. Incentive compatibility holds if a truthful revelation of preferences is a Bayes-Nash equilibrium, rather than an ex post or dominant strategy equilibrium. Our analysis shows that the impossibility of public goods provision can be overcome if many public goods are provided simultaneously. An impossibility result in mechanism design gets stronger with weaker constraints. A possibility result gets stronger with stronger constraints. Thus, while for the purposes of Mailath and Postlewaite, it was a natural choice to have constraints that need to hold only in expectation, for us, the natural choice is to have separate participation, incentive and budget constraints for each state of the economy.====The all-or-nothing-mechanism plays a decisive role in our analysis. This mechanism has only two outcomes: Either the status quo prevails, or the capacity for increased public goods provision is exhausted. Costs are shared equally among individuals. Exhausting the capacity requires a consensus. As soon as one individual opts for the status quo, the status quo stays in place. This mechanism is obviously admissible: The veto rights ensure that participation constraints are satisfied. If no one makes use of his veto power, then, whatever the preference profile, the mechanism stipulates the same outcome. This limited use of information on preferences ensures incentive compatibility.====Our first set of results shows that, under the all-or-nothing-mechanism, the probability of the “all-outcome” is an increasing function of the capacity ==== and converges to 1 as ==== becomes unbounded. This can be understood as a large numbers effect. The larger the bundle, the closer are individual preferences to the mean of the distribution from which preferences are drawn. As the mean exceeds the per capita cost, the larger the bundle the less likely is a veto. To relate our analysis to Mailath and Postlewaite (1990) we also consider the possibility that both the capacity ==== and the number of individuals ==== grow. If this process is such that the ratio ==== converges to a positive constant, the limit probability of the all-outcome is bounded away from zero.====A second set of results establishes conditions under which the all-or-nothing-mechanism is a second-best mechanism, i.e. a mechanism that maximizes the expected surplus over the set of admissible mechanisms. The all-or-nothing-mechanism may not appear as a natural candidate for an optimal mechanism: The all-outcome gives rise to an overprovision of public goods as it typically includes public goods with negative surplus. Since the mechanism offers only the alternatives “all” and “nothing”, there is no possibility to eliminate those goods from the bundle. When the nothing-outcome prevails there is an underprovision of public goods. For a given finite capacity, this happens with positive probability. By our first set of results, the probability of underprovision decreases and the probability of overprovision increases in the capacity for public goods provision.====The requirement of anonymity plays a prominent role in this part of our analysis. It is a requirement of equal treatment: A permutation of individual preferences must not affect public goods provision levels. Moreover, the payments of individuals with the same public goods preferences are equal. We show that – in the presence of incentive, participation and budget constraints – there is only one payment rule that is anonymous: equal cost sharing. This finding facilitates the analysis. It makes it possible to focus on the second-best provision rule for public goods – as opposed to having a joint analysis of payment and provision rules.====Our analysis invokes the famous impossibility result by Gibbard (1973) and Satterthwaite (1975). According to this result, with an unrestricted preference domain, any mechanism that is ex post incentive compatible and allows for more than two outcomes is dictatorial. We show that, under an ancillary assumption, this theorem applies to our setup. The implication is that the set of admissible mechanisms becomes small: There can be at most two outcomes. One of the two outcomes has to be the status quo. Otherwise, it would impossible to respect participation constraints. Thus, the only degree of freedom is the choice of the second outcome. The assumption that public goods provision is desirable in expectation, implies that it is desirable to exhaust the capacity to provide public goods. Thus, a second best mechanism gives a choice between two outcomes, “all” or “nothing”.====The ancillary assumption is that the different public projects are ordered. The project with index ==== can be implemented only if all projects with an index smaller than ==== have been implemented already. As we will discuss, this assumption makes the formal analysis tractable. It also has an empirical plausibility for specific types of infrastructure investment. The development of transportation networks (railroads, highways) frequently follows a sequential logic where the major centers are connected before the network is extended towards less important cities, and finally supplemented with branch lines. Economic historians have documented that developments of railway or highway networks in the 19th and early 20th century followed this pattern, see Fogel (1962), Voigtlaender and Voth (2014), Hornung (2015), or Donaldson (2018) for examples.==== The observation that bundling can alleviate inefficiencies due to incentive or participation constraints is due to Jackson and Sonnenschein (2007) and Fang and Norman (2006). Both papers focus on Bayes-Nash equilibria and on participation constraints that need to hold at the interim stage where individuals know their own type but still face uncertainty about the types of others and hence about the outcome of the mechanism. Moreover, both papers show that bundling a large number of decisions allows to approximate first-best outcomes. Our work differs in that we invoke ex post incentive and participation constraints. As a consequence, first-best outcomes cannot be reached. The second-best outcome is the all-or-nothing-mechanism that gives rise to an overprovision of public goods.====If bundling is not an option, second-best mechanisms give rise to an underprovision of public goods.==== More specifically, Güth and Hellwig (1986) show that the second-best mechanisms involve underprovision. Mailath and Postlewaite (1990) show that, under any admissible mechanism, the probability of public goods provision goes to zero as the number of individuals becomes unbounded. An important assumption is that the per capita cost of provision remains constant as additional individuals are added to the system. Hellwig (2003), by contrast, allows for scale economies. Welfare-maximizing provision levels then increase with the number of individuals. Still, these second-best provision levels may fall short of first-best levels. For excludable public goods, as shown by Norman (2004), second-best mechanisms involve use restrictions to mitigate the distortions from incentive and participation constraints, again with the implication that second-best provision levels are smaller than first-best levels.==== The following section introduces the formal framework. In Section 3, we show that, under the all-or-nothing-mechanism, public expenditures increase in the capacity to provide public goods. Section 4 shows that the all-or-nothing-mechanism is a second-best mechanism. The last section contains concluding remarks. Formal proofs are relegated to the Appendix.",All or nothing: State capacity and optimal public goods provision,https://www.sciencedirect.com/science/article/pii/S002205311930105X,24 October 2019,2019,Research Article,190.0
"Sandholm William H.,Izquierdo Segismundo S.,Izquierdo Luis R.","Department of Economics, University of Wisconsin, 1180 Observatory Drive, Madison, WI 53706, USA,BioEcoUva Research Institute on Bioeconomy, Department of Industrial Organization, Universidad de Valladolid, Paseo del Cauce 59, 47011 Valladolid, Spain,Department of Civil Engineering, Universidad de Burgos, Edificio la Milanera, Calle de Villadiego, 09001 Burgos, Spain","Received 25 January 2019, Revised 4 September 2019, Accepted 20 October 2019, Available online 24 October 2019, Version of Record 5 November 2019.",https://doi.org/10.1016/j.jet.2019.104957,Cited by (13),"We study a family of population game dynamics under which each revising agent randomly selects a set of strategies according to a given test-set rule; tests each strategy in this set a fixed number of times, with each play of each strategy being against a newly drawn opponent; and chooses the strategy whose total payoff was highest, breaking ties according to a given tie-breaking rule. These dynamics need not respect dominance and related properties except as the number of trials become large. Strict ==== are rest points but need not be stable. We provide a variety of sufficient conditions for stability and for instability, and illustrate their use through a range of applications from the literature.","By assuming that agents apply simple myopic rules to update their strategies, evolutionary game dynamics provide a counterpoint to traditional approaches to prediction in games based on equilibrium knowledge assumptions. To focus on dynamics that only impose mild informational demands on the agents, one can model explicitly how agents obtain information through individual random matches and use this information to decide which strategy to play. For instance, Helbing (1992) and Schlag (1998) show that if agents' decisions are based on single matches rather than complete matching, proportional imitation rules lead aggregate behavior to follow the classical replicator dynamic of Taylor and Jonker (1978).====Two distinct approaches based on optimization rather than imitation have also been analyzed. In the approach closer to traditional economic modeling, agents use information from samples of opponents play to form point estimates of the population distribution of actions, and then play a best response to this estimate (Sandholm (2001), Kosfeld et al. (2002), Kreindler and Young (2013), Oyama et al. (2015)). An approach using weaker assumptions about agents' capacities was pioneered by Osborne and Rubinstein (1998) and Sethi (2000). Here a revising agent tests each candidate strategy in random matches against distinct draws of opponents, and then selects the one that earned the highest total payoff during testing. This approach does not assume that agents know the payoffs of the game they are playing, or even that they know they are playing a game; only payoff experiences count.====This paper introduces a general formulation of these ==== (====) ====, allowing variation in which candidate strategies agents contemplate, in the number of trials of each such strategy, and in tie-breaking rules. While the resulting dynamics have complicated functional forms, we find that they are surprisingly susceptible to analysis.====We show that if revising agents do not consider all available strategies as candidates for revision, then the rest points of BEP dynamics can include not only strictly dominated strategies, as Osborne and Rubinstein (1998) observe, but even strategies that are guaranteed to perform worse than a given alternative strategy even if opponents choose different responses to each. We also show that strictly dominant strategies are globally asymptotically stable when the number of trials of each tested strategy is large enough. Surprisingly, the latter conclusion is not obvious, but instead requires precise estimates of the likely outcomes of samples at population states in the vicinity of the equilibrium.====The remainder of the paper concerns the instability and stability of strict equilibria. Under the assumption that revising agents know the current state, strict equilibria are stable under very weak assumptions (Sandholm (2014)). But Sethi (2000) shows that under dynamics based on testing each strategy exactly once, strict equilibria need not be stable. His sufficient condition for instability in two-player games requires that every nonequilibrium strategy ==== supports invasion of at least two nonequilibrium strategies ==== and ====, in that the presence of strategy ==== in a match makes both ==== and ==== outperform the equilibrium strategy.====Here we obtain instability results for a considerably more general class of dynamics, and in doing so we identify two qualitatively new sources of instability. First, we show that instability can be driven by the introduction of “spoiler” strategies, whose presence in matches causes the equilibrium strategy to earn a lower payoff than does the second-best response to the equilibrium. Second, we show that instability can be generated not only by the concerted action of all opposing strategies, but by any smaller group of nonequilibrium strategies that through both support and spoiling are mutually supportive of invasion. We complement these analyses with sufficient conditions for stability of strict equilibrium, and we illustrate the wide applicability of our conditions through a range of applications.====Our analyses of local stability all originate from a simple observation. While the general formulas for best experienced payoff dynamics are daunting (see Section 2.3), the behavior of the dynamics near strict equilibria is driven by terms of at most first order in the fractions playing nonequilibrium strategies. This greatly reduces the number of terms relevant to the stability analysis, allowing us to derive our sufficient conditions by direct manipulation and by applying basic results from linear algebra. Specifically, our main instability result is an application of Perron's theorem to the dynamics' Jacobian matrices, and our stability results rely on direct bounds on the flows of agents between strategies and on basic conditions for diagonalizability.====There are some questions about BEP dynamics—the computation of all rest points for a given instance of the dynamics in a given game, and the evaluation of stability of interior rest points—that are not susceptible to the approaches we follow here. In a companion paper, Sandholm et al. (2017), we show how Gröbner bases and other tools from computational algebra, along with approximation results from linear algebra, can be used to answer such questions. We combine these techniques with exact and numerical analyses to provide a complete account of the behavior of BEP dynamics in the Centipede game (Rosenthal (1981)).====As noted above, BEP dynamics have their origins in the work of Osborne and Rubinstein (1998) and Sethi (2000). Osborne and Rubinstein (1998) introduce the notion of ==== equilibrium to describe stationary behavior among “procedurally rational” agents. This equilibrium concept corresponds to the rest points of the BEP dynamic under which agents test all strategies, subject each to ==== trials, and break ties via uniform randomization. They present many examples, and show that the limits of ==== equilibria as ==== grows large are Nash equilibria. Building on this work, Sethi (2000) introduces the corresponding specification of BEP dynamics, focusing on the case in which strategies are tested once. Sethi (2000) shows that both dominant strategy equilibria and strict equilibria can be repellors under these dynamics, and that dominated strategies can be played in stable equilibria. He also provides a sufficient condition for repulsion from strict equilibria that includes one restriction on payoffs for each nonequilibrium strategy (see Section 5.1).====Our analysis generalizes the work of Sethi (2000) in various respects. First, by accounting for the effects of spoilers, our sufficient condition for repulsion applies to a larger class of games than that of Sethi (2000). Second, using an analysis of eigenvalues, we obtain conditions under which a strict equilibrium is unstable but not necessarily a repellor. Third, we provide new sufficient conditions for instability, including one that only requires mutual reinforcement by small sets of invading strategies. Finally, our results hold for the general class of BEP dynamics rather than just the basic instance considered by Sethi (2000).====Procedurally rational agents and their associated equilibria have been used in a variety of applications, including trust and delegation of control (Rowthorn and Sethi (2008)), market entry (Chmura and Güth (2011)), use of common-pool resources (Cárdenas et al. (2015)), contributions to public goods (Mantilla et al. (2019)), ultimatum bargaining (Miȩkisz and Ramsza (2013)), and the Traveler's Dilemma (Berkemer (2008)). As we will show, the general instability and stability criteria we develop here provide a simple and unified way of deriving many of the results that these papers derive individually, as well as several new results.====Our study of best experienced payoff dynamics also contributes to a literature on the aggregate consequences of decision rules that restrict attention to small numbers of alternative strategies. For instance, Berger and Hofbauer (2006) and Hofbauer and Sandholm (2011) show that strictly dominated strategies need not be eliminated when revising agents consider limited numbers of alternatives, as under the BNN (Brown and von Neumann (1950)) and Smith (1984) dynamics: a strictly dominated strategy may achieve the second-best payoff at many states, and so may survive when agents do not always evaluate every strategy. Our analysis of dominated strategies accords with these results. Zusai (2018) introduces a general class of optimizing dynamics that converge globally to Nash equilibrium in contractive games (Hofbauer and Sandholm (2009)), providing a general argument for convergence that allows the set of candidate strategies to be random and incomplete. In a similar spirit, our analysis shows that the instability of strict equilibria is partially robust to the incompleteness of the set of candidate strategies; however, we show that smaller consideration sets must be paired with stronger restrictions on payoffs for instability to be assured.====The remainder of the paper is organized as follows. Section 2 introduces the family of best experienced payoff dynamics. Section 3 evaluates properties of their rest points. Section 4 presents elimination and survival results connected to dominance and related notions. Section 5 presents sufficient conditions for the stability and instability of strict equilibria. Section 6 concludes. The appendix presents basic definitions from dynamical systems, provides proofs of most of the results in the paper, and analyzes examples that are not covered by our general results.",Stability for best experienced payoff dynamics,https://www.sciencedirect.com/science/article/pii/S0022053119301073,24 October 2019,2019,Research Article,191.0
"Cheremukhin Anton,Restrepo-Echavarria Paulina,Tutino Antonella","Federal Reserve Bank of Dallas, 2200 N Pearl St, Dallas TX 75201, United States of America,Federal Reserve Bank of St. Louis, One Federal Reserve Bank Plaza, Broadway and Locust St., St. Louis MO 63166, United States of America","Received 29 July 2017, Revised 20 September 2019, Accepted 18 October 2019, Available online 24 October 2019, Version of Record 30 October 2019.",https://doi.org/10.1016/j.jet.2019.104956,Cited by (10),"We propose a parsimonious matching model where a person's choice of whom to meet endogenizes the degree of randomness in matching. The analysis highlights the interaction between a productive motive, driven by the surplus attainable in a match, and a strategic motive, driven by reciprocity of interest of potential matches. We find that the interaction between these two motives differs with preferences—vertical versus horizontal—and that this interaction implies that preferences recovered using our model can look markedly different from those recovered using a model where the degree of randomness is not endogenous. We illustrate these results using data on the U.S. marriage market and show that the model can rationalize the finding of aspirational dating.","When searching for a match in the dating or marriage market, circumstances can influence whom you meet, but individuals' choices matter as well. As a result, the empirical literature has shown that matching is neither purely random nor perfectly assortative.==== In this paper, we propose a parsimonious way to model the choice of whom to meet that endogenizes the degree of randomness in the matching process and show that the model can be used to estimate underlying preferences. Furthermore, we show that the preferences recovered using our model can look markedly different from those recovered using a model where the degree of randomness is not endogenous.====Finally, while our model has a multitude of applications beyond the marriage market, we present one example for which our model is particularly empirically relevant. Bruch and Newman (2018) find that when searching for a match, individuals pursue partners who are “out of their league” in some characteristics, and we show that this is a behavior that derives naturally from our environment.====We blend the stochastic discrete choice literature with the frictionless matching environment of Becker (1973) with two-sided heterogeneity and assume that, on both sides of the market, individual types of agents are characterized by multidimensional attributes. Even though agents know the distribution and their preferences over types, they do not know where to find a particular type. To do so, they decide how much effort they want to exert to locate a particular partner by trading off the cost of search with the payoff they can achieve if successful in finding their desired match.====An agent chooses whom to contact in a probabilistic way, and the strategies chosen are discrete probability distributions over types. Each element of the distribution represents the probability with which an agent will target (i.e., contact) each potential match based on the agent's expected payoff. Exerting more search effort, which results in a higher search cost, allows agents to spot a particular type more accurately. Given the discrete nature of the probability distributions, we model the search cost as proportional to the distance between an uninformed—uniform—strategy, where every type has the same probability of being contacted and the distribution that is chosen by the agent.====The optimal probability distribution representing an agent's contacting strategy balances two motives: the productive and the strategic. The productive motive pushes the agent to pursue the potential match that gives the agent the highest payoff. The strategic motive pushes the agent to pursue the potential match that is more likely to reciprocate interest. Thus, people act strategically not only when deciding whether to form a match or wait for a better option (like in Eeckhout (1999)), but also when choosing whom to contact.====The interaction between the productive and the strategic motives determines the meeting rates in the model. The relative strength of the two motives depends on the search cost. When exerting effort to find the best partner is not very costly, it is easy for agents to locate their preferred types accurately, and reciprocity of interest is the paramount determinant of who meets whom: The strategic motive dominates.====When exerting search effort is costly, agents will not be able to locate their preferred types with accuracy, so the likelihood of contacting someone else increases. In this case, payoffs become the driving force behind who meets whom. In the unique equilibrium, every agent's strategy is to target the partner that would yield the highest payoff: The productive motive dominates the strategic motive.====The equilibrium is generally inefficient. Two externalities prevent the competitive equilibrium from achieving the social optimum. The first is a positive externality. If an agent increases her search effort, not only does she increase the probability of finding a match, but she also generates an incentive for others to increase their search effort, which can increase the quality of matches. Since the agent is not compensated for this additional effort, she fails to internalize this gain. The second externality is negative. When the productive motive drives optimal individual strategies, there is more congestion because competition increases for types targeted with higher intensity, undermining matching probability. Although the equilibrium is generally inefficient, it is possible for the two externalities to offset each other to achieve efficiency in the competitive equilibrium.====The model can be used for empirical identification of agents' preferences. In particular, it can identify whether agents' preferences are vertical—attraction is based on a commonly agreed upon ranking—or horizontal—people are attracted to agents with similar characteristics.==== The existing literature finds it hard to distinguish between these cases empirically.==== Both cases lead to identical assortative stable matching in the frictionless case. In contrast, the equilibrium matching rates predicted by our model differ markedly for these two cases. When preferences are horizontal, the strategic and productive motives pull agents in the same direction, as similar types both get the highest payoff from each other and their interests are also more likely to be mutual. In this case, a stochastic version of assortative matching is preserved in equilibrium and the shapes of the observed matching rate and the underlying payoff function are similar.====However, in the case of vertical preferences, there is common agreement on the ranking of agents and everybody would like to chase a particular type but there would be a lower probability of reciprocation. As a result, in this case the productive and the strategic motives pull in opposite directions. The productive motive will encourage agents to target their preferred type, and the strategic motive will encourage them to diversify to increase the chance of forming a match. This case gives rise to a novel equilibrium pattern that resembles neither positive nor negative assortative matching. We call it a mixing equilibrium.==== This type of equilibrium rationalizes the behavior observed by Bruch and Newman (2018) in online dating where so-called aspirational dating, or “reaching up the desirability ladder,” is documented. Such behavior—and sorting in equilibrium—implies that there can be a large difference between the shape of the observed matching rate and the underlying payoff function (preferences).====Our model is not the first one where the shape of the matching rate differs from the shape of the underlying payoff function. This difference arises in models that have a strategic motive (see Eeckhout (1999), Shimer and Smith (2000), Chade (2001), Adachi (2003), and Eeckhout and Kircher (2010) among many others). However, our model is the first that can rationalize specific patterns of strategic behavior such as reaching up the desirability ladder. What makes our model different is the endogenous source of randomness in the matching equilibrium, unlike in the existing literature where randomness is postulated.====We provide conditions for existence, uniqueness, and efficiency of equilibrium as well as a characterization of equilibrium sorting. Our results on equilibrium uniqueness allow us to develop a methodology for recovering agents' preferences using only data on aggregate matching rates.====We find that the model does a very good job rationalizing the observed aggregate matching rates in the U.S. marriage market based on education, race, income, and age. Furthermore, we show that due to the presence of the strategic motive—for which relative importance is determined endogenously—the degree of horizontality of the recovered payoff function can be different from that recovered using models in which there is no strategic component or the relative importance of this motive is built in ad hoc. Finally, the resulting strategies—who targets whom—are consistent with the desirability estimates of Bruch and Newman (2018) derived from online dating data.====The paper proceeds as follows: Section 2 describes the model and Section 3 derives the theoretical results. We develop an empirical methodology for recovering preferences and apply it to the U.S. marriage market data in Section 4. Section 5 states some final remarks.",Targeted search in matching markets,https://www.sciencedirect.com/science/article/pii/S0022053119301061,24 October 2019,2019,Research Article,192.0
"Korpela Ville,Lombardi Michele,Vartiainen Hannu","Turku School of Economics, University of Turku, Finland,Adam Smith Business School, University of Glasgow, United Kingdom,University of Helsinki and Helsinki Graduate School of Economics, Finland","Received 18 April 2019, Revised 21 August 2019, Accepted 17 October 2019, Available online 22 October 2019, Version of Record 28 October 2019.",https://doi.org/10.1016/j.jet.2019.104953,Cited by (9),"In this paper, we re-examine the classical questions of implementation theory under complete information in a setting where coalitions are fundamental behavioral units, and the outcomes of their interactions are predicted by applying the solution concept of the core. The planner's exercise includes designing a code of rights that specifies the collection of coalitions having the right to block one outcome by moving to another. A code of individual rights is a code of rights in which only unit coalitions may have blocking powers. We provide the necessary and sufficient conditions for implementation (under core equilibria) by codes of rights, as well as by codes of individual rights. We also show that these two modes of implementation are not equivalent. The results are robust and extend to alternative notions of core, such as an externally stable core. Therefore, coalitions are shown to bring value added to institutional design. The characterization results address the limitations that restrict the relevance of the existing implementation theory.","The challenge of implementation lies in designing a mechanism (i.e., game form) in which the equilibrium behavior of agents always coincides with the recommendations given by a ==== (SCR). If such a mechanism exists, the SCR is said to be implementable.====As such, the key question is how to design an implementing mechanism so that its outcomes are predicted through the application of game theoretic concepts. Most early studies on implementation focused on noncooperative solution concepts, such as the Nash equilibrium and its refinements. However, one of the difficulties with this approach is that canonical mechanisms are typically complex and difficult to explain in natural terms, as they rely on tail-chasing constructions, such as the integer game.====As demonstrated in the seminal paper of Koray and Yildiz (2018) [henceforth KY], an alternative to the noncooperative approach is to allow groups of agents to coordinate their behaviors in a mutually beneficial way. To move away from noncooperative modeling, the details of coalition formation are not modeled. Then, coalitions—not individuals—become basic decision-making units. Here, the role of the solution concept is to explain why, when, and which coalition forms and what it can achieve.====By using the notion of a ====, introduced by Sertel (2001), KY study implementation problems by rights structures.==== A rights structure Γ consists of a state space ====, an outcome function ==== that associates every state to an outcome, and a code of rights ====. A code of rights specifies, for each pair of distinct states ====, a collection of coalitions ==== effective in moving from ==== to ====.====As a coalitional solution, KY adopt a version of the ====, referred to as the Γ-====. A state ==== is an equilibrium state under a given rights structure and agents' preferences if no effective coalition can guarantee each of its members a utility level higher than the one they receive under ====. Then, the implementation problem consists of designing a rights structure Γ, with the property that the equilibrium outcomes always coincide with the recommendations of the given SCR. If such a rights structure exists, the SCR is said to be ====.====The implications of KY's approach are interesting. They show that a SCR that is implementable by any rights structure is implementable, in particular, by a rights structure that only uses singleton coalitions. A puzzling consequence of this result is that coalitions appear to have no value added to the implementation problem. For all purposes, it is sufficient to focus on individuals as actors alone. The question, then, is why should institutions be designed based on coalition formation, as is often the case? For example, under a typical democratic constitution, a bill can only be passed by consent from a majority of individuals.====To analyze this question, we focus on the simple and natural restriction on rights structures: the set of states is assumed to coincide with the set outcomes. This mode of implementation was dubbed by KY as ====.==== Our analysis complements KY, who characterize implementation by codes of rights by assuming universal domain of strict preferences. Assuming an unrestricted domain of preferences, our main results are as follows: we provide an alternative characterization of implementation via a code of rights (Theorem 2), we characterize implementation by a code of individual rights (Theorem 4), and we show that restricting the code of rights to be individual-based reduces the set of implementable SCR (Corollary 1).====Note that a code of rights captures the allocation of blocking powers in many real-life situations in a natural way, allowing us to formulate implementation problems in an everyday language as feasible options available to agents, or to groups of them, that only depend on the status quo outcome. Conversely, a mechanism that cannot be defined in this way must be conditioned on the history of the play. Many real life schemes lack this feature. For example, what is achievable to the agents or coalitions is often defined on the grounds of property rights or alike concepts. Marriage market is a case in point. Feasible changes in marriage relationships are dictated by who is married to who, and not by who has been married to who in the past. In political decision making, what a party can do depends on its popularity. In the house allocation problem, a coalition of individuals should be allowed to exchange their own houses. And so on.====We demonstrate that implementation by a code of rights is fundamentally dependent on nonsingleton coalitions. That is, when the effective coalitions can only depend on the status quo outcome, mechanisms cannot rely on individual behavior alone, i.e., coalitions matter. To show this, we identify two necessary and sufficient conditions for implementability: one is the unanimity condition and the other one is what we call ====, which is stronger than (Maskin) monotonicity (Maskin, 1999). When we restrict our attention to the full domain of linear orderings, our characterization result is equivalent to that provided by KY.====To understand the way our restriction on the state space affects implementable SCRs, we study implementation by codes of rights under which only singleton coalitions can induce new outcomes. We call this type of codes of rights ====. Under this setting, we identify the necessary condition for implementability, which we call ====. This condition is also sufficient when combined with unanimity.====Singleton strong monotonicity implies strong monotonicity but, as we will demonstrate, they are ==== equivalent. Therefore, the key insight for the implementation by rights structures—that coalitions do not matter—does ==== extend to the implementation by codes of rights. The underlying reason for this observation is that implementation by an individual rights structure requires a significant amount of information concerning the preferences of the agent permitted to move at a particular state. When the state space is coarsened, the needed information may no longer be conveyed by the underlying state. Since the preferences of a coalition are less volatile than those of an individual agent, coalitions may no longer be usefully replaceable by individuals. An example is the ====, which is implementable by a majority coalition-based code of rights but not an individual-based one.====The conclusion that coalitions make a difference is robust and can be extended to the implementation by codes of rights for alternative core definitions. Indeed, we add to the notion of core as the requirement that blocking must be achievable through outcomes that are themselves in the core, meaning we also consider implementation by codes of rights of what is often referred to as an ====. We call this type of implementation ====.==== KY were the first to study externally stable implementation by rights structures under the assumption of universal domain of linear orderings (KY, 2018; Proposition 2).====The remainder of this paper is divided into four sections. Section 2 sets out the theoretical framework and outlines the basic model. Section 3 provides a novel characterization of the class of SCRs implementable via codes of rights, whereas Section 4 fully characterizes the class of SCRs that are implementable by codes of individual rights. Section 5 concludes the paper. Appendix includes proofs not in the main body.",Do coalitions matter in designing institutions?,https://www.sciencedirect.com/science/article/pii/S0022053119301036,22 October 2019,2019,Research Article,193.0
"Koch Alexander K.,Nafziger Julia","Aarhus University, Denmark,CEPR, United Kingdom","Received 13 March 2018, Revised 20 September 2019, Accepted 5 October 2019, Available online 22 October 2019, Version of Record 28 October 2019.",https://doi.org/10.1016/j.jet.2019.104949,Cited by (7),"We study theoretically and experimentally how the bracketing of non-binding goals in a repeated task affects the level of goals that people set for themselves, the actual effort provided, and the pattern of effort over time. In our model, a sophisticated or partially naïve individual sets either daily or weekly goals to overcome a motivational problem caused by present-biased preferences. In an online, real-effort experiment, we randomly allocated subjects to treatments where they set either daily goals for how much to work over a one-week period or a single weekly goal. Consistent with the theoretical predictions, in the treatment with daily goals, the aggregate goal level for the week was higher and subjects provided more effort compared to the treatment with a weekly goal. The higher effort was driven by the higher aggregate goal level. Additional treatments complemented internal commitment through goals with an externally enforced minimum work requirement to get started working each day.","Every day people face motivational problems in repeated tasks such as working, studying, dieting, exercising, or saving. While decades of research in psychology document that goals play an important role in helping people to overcome their motivational problems (e.g. Locke and Latham 2015), it is still poorly understood how goals work in repeated tasks. In such settings, a person may focus on single instances of the task and evaluate tasks relative to narrowly bracketed goals or, instead, evaluate the aggregate performance over a longer time period relative to a broadly bracketed goal. How goals are bracketed can often be linked to the way feedback about performance is given (e.g. Asch, 1990; Cadena et al., 2011), the availability of salient reference points (e.g. Pope and Schweitzer, 2011; Allen et al., 2016), or explicit advice about how to set goals.==== But how does the bracketing of goals affect the level of goals that people set for themselves, the actual effort provided, and the pattern of effort over time? In this paper, we study these open questions theoretically and experimentally.====We develop a model where a sophisticated or partially naïve individual works repeatedly on a task. He sets non-binding goals to overcome the motivational problems caused by present-biased preferences. Goals can either be narrow (daily goals) or broad (a weekly goal). Our model predicts that the aggregated daily goals are higher than the weekly goal. As a consequence, individuals with daily goals work more than those with the weekly goal. The reason is the following. A weekly goal tempts individuals to put in low effort at the beginning of the week and to compensate with higher effort later (====). This asymmetric effort profile is suboptimal (from an ex ante perspective). Because effort costs are convex, the individual would prefer a constant effort pattern. Under plausible assumptions, a reduction in the weekly goal level leads to less variation in effort over time. Thus, taking into account evidence on the distribution of present bias, our model predicts that on average individuals adopt a lower weekly goal compared to the aggregated goal level that they would chose with daily goals.====We tested the predictions of the model using an online, real-effort experiment. Subjects were randomly assigned to set either non-binding daily goals for how much to work online in the following week or a non-binding weekly goal. The experiment mimicked a typical work-leisure self-control problem. Work was desirable (the piece-rate pay was generous) but involved unpleasant effort. Subjects faced the usual real-life temptations because our study neither required them to show up at a lab nor to obey a particular schedule.====By exogenously varying the goal bracket, we provide a clean test of the ==== hypothesis that narrowly bracketed goals help individuals to address their self-control problems (e.g. Read et al., 1999). It was first suggested as an explanation for why individuals who can choose their working hours, such as taxi drivers, often appear to have daily income targets (e.g. Camerer et al. 1997, Dupas and Robinson 2016). While there is much suggestive evidence for this hypothesis, clean evidence is missing.====We find support for the motivational bracketing hypothesis. Our data reveal that subjects with daily goals set a higher aggregated goal for the week than subjects with a weekly goal, and that they worked more than those with a weekly goal. The latter effect largely disappears when we control for the goal level. That is, in line with the theory, the higher effort with daily goals seems to be related to the higher goals that subjects set under the daily bracket and not to the daily bracket per se.====We extended our two baseline treatments in two directions. First, we conducted treatments that manipulated whether goals were framed as daily goals or a weekly goal, but that left the overall goal level unaffected by the framing.==== These treatments allow us to directly test the prediction that the higher effort with daily goals is driven by the higher aggregated goal level compared to a weekly goal and not the framing of the goal. We confirm the prediction. Further, we test and confirm the prediction that subjects with a weekly goal work less at the beginning of the week than subjects with daily goals who face the same aggregated goal level, and that they work more at the end of the week to make up for the shortfall.====Second, to examine whether the positive effect of daily goals stems from their ability to get people started working each day, we ran additional treatments which complemented goals with a minimum work requirement. To receive any payment, subjects had to complete at least one real-effort task per day, which took less than a minute. An innovative feature of the requirement is that it forced subjects to ‘get started’ working each day, but otherwise gave full flexibility of how to allocate work and how much to work. In the treatments with the work requirement, effort and goal levels no longer differed across treatments where subjects set daily goals or a weekly goal, and effort patterns were similar. This result is suggestive for the interpretation that forcing subjects to ‘get started’ each day mitigates problems of effort substitution with a weekly goal.====With these treatments we make another novel contribution by addressing the question of how externally enforced work requirements interact with internal commitment through goals. Surprisingly, subjects worked less if they were forced to ‘get started’ working each day (in addition to setting daily goals) than if they just set daily goals. This result can be explained by a large fraction of subjects dropping out when they were forced to work each day. Focusing only on those subjects who did not drop out, performance did not differ across treatments with and without the work requirement. This is consistent with the interpretation that daily goals on their own already get people started. The pattern is reversed for the treatments with a weekly goal. For subjects who did not drop out, effort was significantly higher with the requirement than without it - as one would expect if the requirement gets people started working. But due to an increase in dropout, the overall effort with the requirement was no different from that without the requirement.====The paper is structured as follows. After reviewing the related literature, we present the experimental design in Section 2. Section 3 presents the theoretical model and predictions. In Section 4, we empirically compare daily and weekly goals and effort under them. Section 5 presents findings from the two extensions to our baseline treatments. Section 6 discusses alternative mechanisms and possible extensions. Section 7 concludes. The online appendix contains proofs, robustness checks regarding the theoretical predictions and the empirical analysis, a description of control variables, further results, and the experimental instructions.====  The narrow bracketing literature goes back to Tversky and Kahneman (1981). Much of it considers simultaneous risky choices, where narrow bracketing is a choice error (e.g. Rabin and Weizsäcker, 2009). We contribute to the literature strand that considers narrow bracketing as a tool to overcome self-control problems (e.g. Shefrin and Thaler, 1988; Fudenberg and Levine, 2006). Our theoretical contribution is to provide the first model of how the bracketing of goals in repeated tasks affects the level of goals that people set for themselves, the actual effort provided, and the pattern of effort over time. Previous work gives conditions under which narrow bracketing is optimal with simultaneous tasks (Koch and Nafziger, 2016) or in a twice repeated optimal stopping problem (Hsiaw, 2018).==== The phenomenon of effort substitution was previously noted in a simple two-period model by Jain (2009) and for two tasks by Koch and Nafziger (2016). These studies provide many important insights, but they do not capture situations where a task has to be performed repeatedly over some time and where the decision is not about stopping, but about how much effort to provide.====Our theoretical predictions build on the premise that people have a present bias in the real-effort task. Augenblick et al. (2015) are the first to estimate present bias in effort. They find evidence of present biased preferences in the effort domain, but not in the money domain. Further, they find that the present bias relates to demand for commitment devices. Augenblick and Rabin (2018) enrich the former study by eliciting the beliefs of the individuals about their future effort. They find that most subjects are (partially) naïve about their present bias.====Our empirical contribution is to compare behavior for narrowly and broadly bracketed goals in repeated tasks. Two early studies from psychology (Bandura and Simon, 1977; Bandura and Schunk, 1981) suggest that narrow (‘proximal’) goals are better at motivating effort than broad (‘distal’) goals. Yet, they have several conceptual problems and small sample sizes.==== Other studies of goal setting in repeated tasks are distinct from our study because they either focus on (repeated) daily goals only or have a single goal for the entire time span. In Kaur et al. (2015), workers could set individual, daily work targets that were then externally enforced by the firm with a penalty for low output. Kaur et al. observed the effort and daily goals of the workers repeatedly over a period of 13 months, but did not vary the goal bracket. Setting goals was a dominated option for rational workers because goals penalized low output without any additional reward for high output. Nevertheless, in 36 percent of the cases workers set positive goals. They produced more and earned more than when not offered the opportunity to set goals. Uetake and Yang (2018) provide descriptive evidence from a weight loss app that daily goals can help achieve a long-run goal. The app suggests daily calorie targets for achieving a self-chosen, long-run weight loss target. Other studies consider the impact of a single, self-set goal on the outcome of a repeated task, such as weight loss (Toussaert, 2016), energy saving (Harding and Hsiaw, 2014), or studying (van Lent and Souverijn, 2017; van Lent, 2018; Clark et al., 2019; Himmler et al., 2019). Most of these studies find a positive effect of goals.====By comparing goals and effort under different goal frames, we also contribute to the literature in economics that studies how to optimally design and set goals. Suvorov and van de Ven (2008), Jain (2009), Koch and Nafziger (2011), and Hsiaw (2013) model non-binding personal goals in one-time tasks. Goals help to overcome self-control problems by serving as reference points that make substandard performance psychologically painful. Empirical work yields mixed results on the impact of asking subjects to set goals in one-time tasks. While Akina and Karagozoglub (2017) observe that goals have no effect on performance, Smithers (2015) and Goerg and Kube (2012) find that goal setting increases performance. Other studies consider non-binding goals in work environments. These goals can either be tied to monetary rewards (Dalton et al., 2015; Goerg and Kube, 2012; Kaur et al., 2015) or not (Brookins et al., 2017; Corgnet et al., 2015, Corgnet et al., 2018). The latter find that goals increase performance. Evidence on the former is mixed. While Dalton et al. (2015) find an overall null result, Goerg and Kube (2012) and Kaur et al. (2015) find positive effects. Overall, most studies point to goals having a positive impact on performance – specifically in the presence of monetary incentives. Several studies point to goals being more effective for men than for women (Smithers, 2015; Dalton et al., 2015; Clark et al., 2019) – a finding that we replicate (cf. Appendix J). Clark et al. (2019) find that effort goals outperform performance goals.====Finally, we contribute to the literature on externally enforced commitment devices (for an overview see Bryan et al., 2010). Ariely and Wertenbroch (2002) observe that imposing binding deadlines on students improves their academic performance. Bisin and Hyndman (2014) and Burger et al. (2011), however, find no such effects. The novel feature of our experiment is to study whether a flexible, externally enforced minimum work requirement can complement self-enforced goals.",Motivational goal bracketing: An experiment,https://www.sciencedirect.com/science/article/pii/S0022053119301000,22 October 2019,2019,Research Article,194.0
"Gorno Leandro,Iachan Felipe S.","FGV EPGE, Praia de Botafogo 190/1100, Rio de Janeiro, RJ 22250-900, Brazil","Received 9 February 2018, Revised 2 September 2019, Accepted 21 September 2019, Available online 27 September 2019, Version of Record 3 December 2019.",https://doi.org/10.1016/j.jet.2019.104945,Cited by (1),"We study a research and development race by extending the standard investment under uncertainty framework. Each firm observes the stochastic evolution of a new product's expected profitability and chooses the optimal time to release it. Firms are imperfectly informed about the state of their opponents, who could move first and capture the market. We characterize a family of priors for which the game admits a stationary equilibrium. In this case, the equilibrium is unique and can be explicitly constructed. Across games with priors in this family, there is a maximal intensity of competition that can be supported, which is a simple function of the environment's parameters. Away from this family, we offer sufficient conditions for convergence of a non-stationary equilibrium. When these hold, the intensity of competition tends to the maximal possible value. Furthermore, we develop methods that can be useful for other applications, including a modified Kolmogorov forward equation for tracking posterior beliefs and an algorithm for computing non-stationary equilibria.","Real options, such as the option to interrupt a product's development and schedule its release, lack clear contractual terms. For instance, they typically do not expire on a proper deadline, but lose a significant part of their value if a competitor moves first.====Consider a race between several firms to develop, produce, and market an autonomous car. The first marketed product gets the possibility to set-up a new industry standard, lock in key suppliers, and obtain significantly higher profits than any follower. Although technical knowledge can only accumulate and contribute to a better product, the same unambiguous evolution does not apply to expected profits. Prototyping often evidences problems in implementation. Marketing studies convey a combination of good and bad news about consumer perceptions. Suppliers might be lost and financing dry up. These issues can be addressed with additional expenses and further delay. But waiting is risky, as a competitor might move first.====The conditions of these competitors are typically only imperfectly known to each other. First, a firm does not observe the private technological achievements of opponents. Second, even for shocks that are publicly observed, as when new regulatory standards are applied to the industry, a given firm does not know how badly compromised the specific designs of competitors are. Moreover, the final decision to produce and market a product depends on several other financial assessments which are, at best, imperfectly anticipated by opponents, such as projections of the marginal impact of the new product on previous business lines.====We study this situation by extending the continuous-time real option framework. Our model features both competition and incomplete information. Each player is privately informed about the evolution of his or her expected payoffs. He or she also continuously faces the choice between exercising the option (entry) or delaying this decision. The benefit of delay originates from increments to expected profits, which involve some randomness.==== In addition to deferred revenues, the cost of delay includes the possibility that an opponent might enter the market first and wipe out the player's profit opportunities. Beliefs about the likelihood of an opponent's entry in the future are central determinants of optimal exercise strategies.====Our main results are the following. First, we characterize the class of prior beliefs for which a stationary equilibrium exists. For each prior within this class, we show that the associated stationary equilibrium is unique and explicitly construct it. Moreover, a particular, ==== leads to the stationary equilibrium with the highest sustainable intensity of competition. We provide an explicit formula for this maximal equilibrium intensity in terms of primitives, namely the drift and volatility of each opponent's expected payoff of entry.====Second, we track the evolution of beliefs about opponents' states for priors that lead to non-stationary equilibria and provide a partial analytical characterization of these equilibria. In particular, we give conditions for convergence toward the stationary equilibrium of the game associated with the canonical prior. The analytic methods we use to obtain these results are likely to be of interest beyond competitive real options.====Last, we compute non-stationary equilibria. The algorithm we develop for this purpose jointly iterates on the forward-looking differential equations that characterize value functions and a backward-looking integral equations for beliefs. This approach allows the study of asymmetric competition and comparative dynamics across different industries, but it can also be useful in other contexts. In our setting, we illustrate how meaningful changes in the competitive environment, such as providing a firm with an initial advantage, have both mechanical effects (that firm is closer to any exercise threshold) and strategic ones (opponents initially see stronger competition and respond more aggressively).====The strategic effects vary over time, often non-monotonically. The intuition is that if one's opponent is more aggressive in the initial months, one should respond more aggressively during that period because the risk of preemption is higher; however, once that initial phase passes without any entry, this constitutes evidence that the opponent was never in a particularly strong position. As such, competition weakens. Transitions can be extremely long-lived and have meaningful effects on firm value and optimal strategies. We conclude that accounting for the time varying nature of competition can be important for applied researchers and financial managers alike.====To introduce some of the main ideas in this paper, we start with an important particular case of the model. Two symmetric players compete in a race to develop a product and first enter a market. We seek to construct a symmetric stationary equilibrium. In the recursive formulation below, two objects are key for the equilibrium characterization: the value function and the beliefs about opponents' conditions.====For clarity, we look at the problem from the perspective of Player 1, who does not observe the actual level of development of Player 2 and only holds a prior ==== about it.====At the same time, Player 1 privately observes the evolution of his or her own expected profitability, summarized by a payoff state ====, and discounts the future at a rate ====. The cost of the product's introduction into the market is ====, so that ==== is the net payoff from exercise at time ====, for ====. If Player 2 enters the market first, the game ends and Player 1 obtains a payoff of zero. This winner-take-all feature of the game simplifies the exposition.====The state ==== follows==== where ==== for ==== are two standard independent Brownian motions. We assume that ====, focusing on the case in which longer product development processes generate, on average, higher profits. Actual increments to profitability, however, are random and can be negative, with ==== representing their volatility.====In a stationary equilibrium, Player 1 conjectures a constant defeat rate, ====. A simple extension of well-known results==== implies that the value function, ====, satisfies the following stationary Hamilton-Jacobi-Bellman (HJB) equation:====The maximization above is between continuation or immediate exercise, in this order. The evolution of the continuation value is the combination of the instantaneous deterministic product improvement, uncertain innovations to profitability, and the possible arrival of a defeat.====The solution features a constant threshold, ====, so that exercise is optimal if and only if ====.====Static net present value (NPV) maximization would lead to investment whenever ====. The optimal threshold ==== displays a positive wedge relative to this static criterion, due to the option value of delayed entry. The defeat and the discount rates play analogous roles: an increase in either decreases the wedge by the same amount. This is consistent with a literature devoted to investment practitioners that suggests the use of an increased discount rate to account for competition.==== By varying ==== from zero to infinity, one can span degrees of competition between monopoly and full profit dissipation. One of this paper's contributions is to offer a game-theoretic foundation for that rate. Another contribution is to show that optimal exercise thresholds, even non-stationary ones, are bounded by the monopolist's and zero-NPV policies.====In equilibrium, exercise thresholds and perceived defeat rates must be mutually consistent. In particular, in a stationary equilibrium, the belief distribution about Player 2's payoff state needs to satisfy==== with support in ==== and boundary conditions ==== and ====.====We derive this modified Kolmogorov forward equation for (stationary) conditional beliefs in Section 3.2 and offer for now only a preview of its intuition. The interpretation of the first two terms is standard: a positive drift makes it less likely that the state is below any given value as time passes, while the diffusion component leads to a smoothing of the distribution over time. The novelty lies in the last term, which originates from conditioning on the absence of defeat. As time passes and Player 2 is expected to cross the exercise threshold at a rate ====, the conditional probability of his or her state being below any ==== (given that defeat was not observed) increases proportionately at that same rate. Intuitively, the absence of defeat is good news for Player 1: had Player 2 been close to the threshold, he or she would have been relatively more likely to enter the market. In this game, survival is indicative of a relatively weaker opponent than previously thought.====We show that Equation (2) admits a single (prior) probability distribution as a solution for any ====, where ==== is the highest level of perceived competition that can occur in a stationary equilibrium. A key consequence is that, for each ====, the game in which the prior marginal distribution about the opponent's condition satisfies Equation (2) has a stationary equilibrium with the value function determined by Equation (1). Also, if the prior marginal distribution does not satisfy Equation (2) for any ====, no stationary equilibrium exists and a more general approach is required.====In the rest of the paper, we go beyond the stationary case and consider a flexible model, which allows for multiple asymmetric players and arbitrary priors.",Competitive real options under private information,https://www.sciencedirect.com/science/article/pii/S0022053119300973,27 September 2019,2019,Research Article,195.0
"Stachurski John,Toda Alexis Akira","Research School of Economics, Australian National University, Australia,Department of Economics, University of California San Diego, United States of America","Available online 28 April 2020, Version of Record 28 April 2020.",https://doi.org/10.1016/j.jet.2020.105066,Cited by (5),"This note corrects the proof of Proposition 5 in ====, which shows that the consumption function has an explicit linear lower bound and is used to prove their main result that wealth inherits the tail behavior of income in Bewley–Huggett–Aiyagari models.","It has been a ‘folk theorem’ in the quantitative macroeconomics literature that heterogeneous-agent models that feature infinitely-lived agents, constant discount factors, and risk-free asset returns have difficulty in explaining the empirically observed heavy-tailed behavior of the wealth distribution. ==== (henceforth ST) provide a theoretical explanation by proving that the wealth accumulation process in such models has an AR(1) upper bound (Proposition 6), which implies that the wealth inherits the tail behavior of income (Theorems 3 and 8). However, their proof of Proposition 5, which is central to their analysis, contains errors. This note provides a correct proof after slightly strengthening the assumptions.====Although the assumptions used here are stricter than those in ST, they do not exclude the applications that follow the main impossibility theorem (Theorem 8).",Corrigendum to “An impossibility theorem for wealth in heterogeneous-agent models with limited heterogeneity” [Journal of Economic Theory 182 (2019) 1–24],https://www.sciencedirect.com/science/article/pii/S0022053120300648,28 April 2020,2020,Research Article,200.0
Cai Zhifeng,"Rutgers University, United States of America","Received 14 August 2018, Revised 19 August 2019, Accepted 2 October 2019, Available online 15 October 2019, Version of Record 18 October 2019.",https://doi.org/10.1016/j.jet.2019.104947,Cited by (3),"This paper studies the role of information acquisition in propagating/stabilizing uncertainty shocks in a dynamic financial market. In a static world, uncertainty raises the value of information, which encourages more information acquisition. In a dynamic world, however, uncertainty can depress information acquisition through a dynamic complementarity channel: More uncertainty induces future investors to trade more cautiously. This renders future resale stock price less informative and reduces the value of information today. Due to the dynamic complementarity, transitory uncertainty shocks can have long-lasting impacts. Direct government purchases can stimulate information production, eliminate equilibrium multiplicity, and attenuate the impacts of uncertainty shocks by raising the effective risk-bearing capacity of the informed investors.","Fluctuations in economic uncertainty matter for asset-market fluctuations as well as business-cycle dynamics. Persistent uncertainty risks are shown to be a crucial driver of asset prices (e.g., Bansal and Yaron, 2004) and business cycle dynamics (Bloom, 2009). But why are uncertainty risks persistent in the first place? After all, information acquisition activities are ubiquitous in modern financial markets. Moreover, market participants' abilities to collect and analyze information have improved tremendously thanks to the adoption of new technologies and the availability of big data in recent decades. Can short-lived uncertainty shocks still generate long-lasting impacts on the financial market when information can be endogenously acquired?====The answer is yes. This paper presents a mechanism whereby heightened uncertainty deters, instead of encourages, information acquisition activities. Therefore, transitory uncertainty shocks can have long-lasting impacts even if an effective information acquisition technology is available. The core of the mechanism is ====,==== which arises naturally in dynamic financial markets in which assets are long-lived. In such a world, investors care not only about dividend payoffs but also about ====, and heightened uncertainty can depress information acquisition due to a dynamic feedback effect: Heightened uncertainty induces ==== informed investors to trade more cautiously. As a result, future resale stock prices become less sensitive to the fundamental, thus reducing the current value of information. Note that, in this world, investors remain uninformed not because of exogenous restrictions on their information choices, but because they endogenously choose not to do so, as there is excessive noise and very little information in the future resale stock price, rendering information acquisition unprofitable.====The mechanism is illustrated in two steps within a standard dynamic trading framework (as in Wang, 1993 and Watanabe, 2008), in which investors are allowed to endogenously acquire information a la Grossman and Stiglitz (1980). In the first step, I analytically characterize a special case in which the dynamic complementary channel is absent. In this case, I formally show that uncertainty shocks exhibit no persistence in the presence of endogenous information: Rising uncertainty will be fully offset by private information acquisition activities and have no impact on equilibrium price informativeness.====Two important assumptions underlie this result. First, there exists an effective information acquisition technology through which one can observe the ==== fundamental at a cost (Grossman and Stiglitz 1980). This condition is mild, in the sense that such a technology can be very costly and in equilibrium is only adopted by a negligible (but positive) share of the investor pool. Second, it is assumed that noisy stock supply shocks exhibit no persistence.==== This assumption leads to an irrelevance property: Current information does not affect future ==== with respect to fundamental. This property implies that the dynamic complementarity effect is absent and the incentive to acquire information closely resembles the one-shot financial market of Grossman and Stiglitz (1980), in which future resale stock prices are fixed to be zero.==== In sum, the “One-shot Theorem” says that if (1) there exists an effective (and perhaps costly) information acquisition technology and (2) there is no dynamic complementarity, then the information market can efficiently absorb exogenous uncertainty shocks.====In the second step, I explore the case where the dynamic complementarity channel is present: with persistent noisy supply shocks, past uncertainty affects future return sensitivity. Because of this, no easy analytical solution is available. A further complication is that due to the presence of equilibrium multiplicity, the standard shooting method cannot be applied here. By exploiting the recursive structure of the dynamic Kalman filter, I find that all past history of prices, dividends, and public signals can be summarized into two state variables: fundamental uncertainty and prior price informativeness. I thus numerically solve for the model focusing on a notion of recursive equilibrium in which pricing coefficients are time-invariant functions of these state variables. Applying the methodology to solve the model around the unique interior stable steady state, I find that the dynamic complementarity works to ==== future asset return sensitivity, and thus prolongs the impact of uncertainty shocks: Persistent supply shocks endogenously depress information acquisition and hence lead to persistent impacts of uncertainty shocks.====Why do more persistent stock supply shocks depress the value of information? When both fundamental shocks and noise supply shocks are very persistent, the current stock price serves as a fairly good summary statistic for the two shocks and by itself is a good predictor of future stock returns. Knowing separately the fundamental and noise information does not add much value on top of observing the current stock price. Intuitively, investors would like to identify stocks that (1) are underpriced today and (2) will be correctly priced in the future. When noise shocks are persistent, any currently underpriced stock will be persistently underpriced. This lowers the value of information acquisition, and hence prolongs the impact of uncertainty shocks.====The analysis calls for identification of a deep model parameter: the persistence of noisy supply shocks. Such persistence could arise for various reasons. For instance, it could be due to order-splitting, which is the common practice of splitting a parent order into a series of child orders and executing them over time (Choi et al., 2018). This is particularly relevant nowadays, given the prevalence of algorithmic trading and smart execution algorithms that facilitate institutional investors in trading large quantities of stock gradually over time (Hendershott et al., 2011). This persistence could also arise due to herding behavior, in which investors mimic each other in terms of their trading strategies, and in particular individual retail investors (Kumar and Lee, 2006; Dorn et al., 2008; Barber et al., 2009a, Barber et al., 2009b). Barber et al. (2009b) provide direct evidence that retail tradings are highly persistent in time and the persistence horizon extends beyond one year. These evidences suggest that uninformed demand shocks tend to be persistent in the data. This, combined with the theoretical analysis presented previously, indicates that transitory uncertainty shocks could indeed have long-lasting impacts on asset markets, even when information can be endogenously acquired.====Lastly I study the impacts of government interventions. They theory predicts that the impact of uncertainty shocks is intimately linked to market liquidity, i.e. the sensitivity of stock prices with respect to noisy supply shocks.==== In particular, more liquidity financial markets should observe less impacts from uncertainty shocks. Thus, policies that boost price informativeness and hence market liquidity would be particularly helpful in eliminating uncertainty shocks. Guided by this observation, I first consider public disclosure policies where the government varies the precision of the public signals. I find that disclosure has little impact, as it hardly alters the information/noise composition of the asset prices. This is because more public information tends to crowd out private information production, leaving equilibrium price informativeness and liquidity unchanged.====I then consider direct-trading policies whereby government are allowed to directly purchase security in financial markets. I show that direct purchase policies are more effective, because they resemble shocks to investors' risk appetite.==== As informed investors effectively become less risk averse and more aggressive in trading, more information gets incorporated into the asset prices. This effectively boosts price informativeness and thus reduces the supply sensitivity. This eliminates equilibrium multiplicity and renders uncertainty shocks less persistent. I thus conclude that direct intervention can be an effective form of regulation in the information markets.====The direct-trading policy can also be interpreted as high-frequency traders (HFT) looking to maximize profit by trading with permanent price changes and against transitory noises (Hendershott et al., 2011; Brogaard et al., 2014). The result of this paper thus uncovers another benefit of HFT, in addition to its role of, among others, liquidity provision and price discovery: it helps to eliminate information equilibrium fragility and renders financial markets more resilient to uncertainty shocks.==== This paper provides a theory of endogenous persistence of uncertainty risks in a dynamic financial market with information asymmetry and endogenous information acquisition. It is first related to the literature that studies strategic interactions in investors' information acquisition activities (e.g. Veldkamp, 2006a, Veldkamp, 2006b; Ganguli and Yang 2009; García and Strobl 2011; and Goldstein and Yang 2015). The classic insight (Grossman and Stiglitz, 1980; Hellwig, 1980) is that informations are static substitutes because asset prices leak useful information to uninformed investors, reducing their incentives to learn. In a dynamic environment with long-lived assets, however, informations are also dynamic complements: more information tomorrow implies that future resale stock price becomes more sensitive to fundamental and thus raises the value of information today. This dynamic complementarity has been shown to have important implications on arbitraging activities, price efficiency, and market fragility (equilibrium multiplicity) (Froot et al., 1992; Dow and Gorton, 1994; Chamley, 2007; Avdis, 2016).====The most closely related paper is Avdis (2016), who also studies information choices in a dynamic financial market. His focus is on equilibrium multiplicity, whereas I focus on the nonstationary dynamics and endogenous persistence of uncertainty shocks. The novel contribution of this paper is that the dynamic complementarity in information acquisition makes uncertainty shocks endogenously persistent, which has important asset-pricing (Bansal and Yaron, 2004) and business-cycle implications (Bloom, 2009). Related to this point, Mele and Sangiorgi (2015) also explores market reactions to changes in uncertainty in a static model in which investors are subject to Knightian uncertainty. They do not, however, explore dynamics upon uncertainty shocks.====This paper is also related to the literature that studies dynamic trading models with asymmetric information, pioneered by Wang, 1993, Wang, 1994 and Campbell and Kyle (1993). It is particularly related to models with overlapping generations of investors (Spiegel, 1998; Bacchetta and Van Wincoop, 2006; Watanabe, 2008; Biais et al., 2010; Albagli, 2015). This literature takes information choices as given and only focus on stationary equilibria where price functions are invariant over time. This paper's contributions here are that it: (a) analyzes endogenous information choice in dynamic noisy rational expectation models and (b) develops a methodology to study nonstationary equilibria in which steady-state multiplicity presents. This methodology (recursive equilibria) can be potentially applied to other similar frameworks, as equilibrium multiplicity is pervasive in overlapping-generation noisy-rational expectation models.====A recent literature explores the implications of dynamic information acquisition on long-run trends observed in (international) financial markets (Farboodi and Veldkamp 2017; Farboodi et al. 2018; and Valchev 2017). It is related to Brunnermeier et al. (2018), who demonstrate that active government intervention may harm information efficiency by leading investors to learn about noises introduced through the intervention. Although similar dynamic coordination forces exist in their frameworks, this paper instead focuses on the interactions between information markets and uncertainty shocks.",Dynamic information acquisition and time-varying uncertainty,https://www.sciencedirect.com/science/article/pii/S0022053118304721,15 October 2019,2019,Research Article,206.0
"Erdil Aytek,Kumano Taro","University of Cambridge, Faculty of Economics, UK,Yokohama National University, Department of Economics, Japan","Received 21 September 2017, Revised 26 September 2019, Accepted 5 October 2019, Available online 10 October 2019, Version of Record 14 October 2019.",https://doi.org/10.1016/j.jet.2019.104950,Cited by (13)," which not only nest important classes of priorities and preferences studied in the literature, but also allow us to formalize plausible priority structures not captured in previous literature. ==== is typically in conflict with ==== (i.e., ====), and therefore the natural welfare objective is ====. A generalization of the ==== process yields a stable assignment, but this outcome is not necessarily constrained efficient. We identify an easily verifiable sufficient condition for a stable assignment to be constrained efficient, which then leads to an algorithm to compute a constrained efficient assignment. Finally we illustrate practical applications of our framework and algorithm, including a widely studied matching problem with distributional constraints.","Assigning indivisible goods to unit-demand agents is a classical economic problem. In many realistic contexts, monetary transfers (as would be in an auction) are ruled out for a variety of reasons, yet discrete resources are assigned via mechanisms which take recipients' preferences into account. Typically, each agent submits an ordinal preference ranking over the available goods, whereas each object comes with its exogenously fixed ==== which form the basis on which over-demanded objects are rationed. A widely used interpretation of ==== is formalized by the ==== property which is central to two-sided matching markets. For example, for an over-demanded school, students who live closer to the school can be given higher priority for that school; a criterion which can be expressed by a ranking over students. Respecting such priorities requires that a seat at school ==== can be assigned to a student only when all students who have higher priority for ==== are assigned a school which they weakly prefer to ====. Note that this leads to a well-defined ==== for the school (say with ==== seats): given a set of applicants, those ==== students who have the highest priority among those applicants are admitted. Abdulkadiroğlu and Sönmez (2003) show that these ==== admission rules operate like college preferences in Gale and Shapley's (1962) two-sided matching framework. In particular, the deferred acceptance algorithm of Gale and Shapley returns the unique ==== assignment: the stable assignment which is preferred (weakly or strictly) by every student to all other stable assignments. However, not every plausible admission rule is a strict responsive rule. In particular there are two different and important extensions of the strict responsive framework. ====, first introduced by Roth (1984a), is an ordinal analogue of Kelso and Crawford's (1982) ==== condition in the absence of ties. In essence, it requires that whenever the set of applicants shrinks, those students who were admitted from the original set of applicants should continue to be admitted. Equivalently, whenever the set of applicants grows, those students rejected from the original set of applicants should still be rejected from the larger set of applicants. Strict responsive rules are substitutable, but as Echenique (2007) shows, the class of substitutable rules is much bigger than the class of responsive rules. Another direction to extend the domain of strict responsive rules is to allow for ties in the priority orders over students, while maintaining the responsiveness assumption. Commonly observed in school admission programs, such ties imply the admission rules are multivalued. An important consequence is that constrained efficient assignments are not unique any more, and the way ties are resolved in a deferred acceptance mechanism matters in terms of welfare as well as distribution of seats (Erdil and Ergin, 2008, Abdulkadiroğlu et al., 2009). In this paper, we allow for ties and introduce the appropriate generalization of substitutability for multivalued admission rules. As such, the class of rules we explore will nest all of the above and provide the proper ordinal analogue of Kelso and Crawford's gross substitutes preferences without having to rule out ties.====Given any set of applicants, an admission rule is a ==== that specifies which subsets of the applicants are ====. For example, suppose a school with two seats is facing six equally qualified candidates consisting of two Asian, two black and two white students. A natural admission rule which gives ‘diverse’ cohorts priority over homogeneous cohorts would identify each mixed pair as an ==== of students from among the six applicants. Hence this school's admission rule will typically involve ties between several, but not all, possible subsets of the applicants. We define an admission rule to be ==== if it is both ==== and ====. Whenever the set of applicants becomes smaller, AM requires that for every admissible subset ==== from the original set of applicants, there must be an admissible subset ==== from the smaller set of applicants such that all those students in ==== who are still among the applicants are in the admissible subset ====. And whenever the set of applicants becomes larger, RM means for every ==== (i.e., the complement of an admissible subset) from the original set of applicants, there must exist a rejectable subset ==== of the larger set of applicants such that ==== is contained in ====. In the absence of ties, AM and RM are equivalent and boil down to the standard substitutability condition. However, neither of them implies the other when we allow for ties. Together, these two conditions are critical in ensuring that our ==== process constructively establishes the existence of a stable assignment (Proposition 2).====Our framework allows modeling admission rules which do not necessarily fit into earlier formulations of priorities. Recall the example of a two-seat school facing six equally qualified applicants ====, where letters indicate the applicants' types (Asian, black, and white, respectively). Prioritizing diverse cohorts over homogeneous cohorts, but otherwise staying indifferent between sets can be summarized by a transitive order ≿ over pairs of students given by==== Note that this plausible and simple priority order ≿ cannot be captured within the well-studied ==== framework, because if ≿ were responsive,==== we would necessarily have==== and==== yielding a contradiction. In Section 5.1, we expand on this example to combine more general distributional constraints with priorities that can be expressed as an order over the set of students (such as those based on walk-zone, sibling, or test-score). By showing substitutability of such priorities, we can appeal to the MDA process to find a stable assignment (Proposition 4). Importantly, the outcome of MDA is typically ====, i.e., Pareto dominated by another stable assignment. We show (Proposition 3) that we can detect any such inefficiency by checking whether the assignment admits a ====: one of our key constructions. PSIC is similar in spirit to Erdil and Ergin's (2008) stable improvement cycles (SIC), but functions differently in a number of ways. Unlike a SIC, a PSIC allows multiple students from the same school to move in the same cyclic trade, and such simultaneous moves do not necessarily preserve stability. Still, in the context of assignment with distributional constraints, we circumvent this issue and go on to establish an algorithm based on PSIC which indeed finds a constrained efficient assignment in ==== (Corollary 1).====Another class of applications are directly motivated by admission rules in two real-life contexts: admissions at the University of Cambridge, and course allocation in various Japanese universities. Deferring the descriptions of the actual procedures to the Appendix, we model the essential features of these mechanisms as follows: multiple referees in an ==== take turns to admit students from a given pool of applicants. Each referee ==== has her own ranking ==== over the set of students. The referees can access the admissions files whenever they like, and until reaching a capacity constraint, admit as many students as they wish from the pool of remaining files according to their own ranking. If we were to fix the order in which the referees make decisions, and the numbers of students chosen in each decision, the admissions procedure would boil down to fixing a ==== in Kominers and Sönmez's (2016) model of matching with slot-specific priorities. The workings of the above admissions committee, however, refrain from fixing such an order, and therefore implies a multivalued admission rule. While it is fairly straightforward to establish substitutability of this rule (thanks to Kominers and Sönmez' results), finding a constrained efficient assignment can be a significant challenge. We show that our PSIC process solves this problem (Corollary 2).====In Section 2, we introduce the model including the definition of substitutable priorities with ties before we formalize what we mean by respecting priorities. In Section 3, we describe the modified deferred acceptance (MDA) process and show that it results in a stable assignment. In Section 4, we explain why constrained efficiency is a natural welfare benchmark, point out that the outcome of MDA is typically constrained inefficient, formally define a potentially-stable improvement cycle (PSIC), and establish its connection with constrained inefficiency. In Section 5, we study, in detail, two applications where constrained efficiency of an assignment is equivalent to lack of PSICs, and design a polynomial time algorithm which computes a constrained efficient assignment. Section 6 concludes.",Efficiency and stability under substitutable priorities with ties,https://www.sciencedirect.com/science/article/pii/S0022053119301012,10 October 2019,2019,Research Article,207.0
"Karakaya Mehmet,Klaus Bettina,Schlegel Jan Christoph","Department of Economics, İzmir Kâtip Çelebi University, Çiğli, 35620, İzmir, Turkey,Faculty of Business and Economics (HEC), University of Lausanne, Internef 538, CH-1015 Lausanne, Switzerland,Department of Economics, City, University of London, Northampton Square, London EC1V 0HB, United Kingdom","Received 19 September 2017, Revised 13 September 2019, Accepted 5 October 2019, Available online 10 October 2019, Version of Record 15 October 2019.",https://doi.org/10.1016/j.jet.2019.104948,Cited by (4),"We study the house allocation with existing tenants model (====) and consider rules that allocate houses based on priorities. We introduce a new acyclicity requirement and show that for house allocation with existing tenants a top trading cycles (====) rule is ==== if and only if its underlying priority structure satisfies our acyclicity condition. Next we give an alternative description of TTC rules based on ownership-adapted acyclic priorities in terms of two specific rules, ==== and ====, that are applied in two steps. Moreover, even if no priority structure is ==== given, we show that a rule is a top trading cycles rule based on ownership-adapted acyclic priorities if and only if it satisfies ====, ====, ====, ====, and either ==== or ====.","We consider the allocation of indivisible objects when agents have preferences over the objects and objects possibly have priorities for the agents. This problem occurs in many applications, e.g., for school choice, the allocation of social or university housing to tenants, or kidney exchange. In many applications, some of the agents additionally have existing claims for an object that have to be respected. When allocating social housing, there might be existing tenants that already occupy a housing unit, but may want to switch if they can obtain a unit that they prefer, while new applicants have to be newly assigned to a vacant unit. When assigning teachers to jobs (Combe et al., 2018), employed teachers may have the option to be reassigned to a new job, while new teachers have to be assigned to their first job. In many school choice districts students have a right to attend a neighborhood school that has to be respected in the assignment.====These situations are captured by the model of ==== which is introduced by Abdulkadiroğlu and Sönmez (1999): A finite set of houses has to be allocated to a finite set of agents without using monetary transfers. Each agent is either a tenant who occupies a house or an applicant, and each house is either occupied or vacant. Furthermore, each agent has strict preferences over all houses and the so-called null house (or outside option). An outcome for a house allocation problem with existing tenants is a matching that assigns to each agent either a real house or his outside option, such that no real house is assigned to more than one agent. A rule selects a matching for each house allocation problem with existing tenants.====The model of house allocation problems with existing tenants is a hybrid of two models, ==== and ====. A house allocation problem with existing tenants reduces to a ==== (Shapley and Scarf, 1974) if there are no applicants and no vacant houses, i.e., all agents are tenants and all houses are occupied. A house allocation problem with existing tenants reduces to a ==== (Hylland and Zeckhauser, 1979) if there are no tenants and no occupied houses, i.e., all agents are applicants and all houses are vacant.====Ideally, any rule used in practice to allocate indivisible objects, with or without existing claims, would be ====, ==== and ====. In the case of housing markets, ==== in the form of ====,==== ==== in the form of ====,==== and ==== in the limited sense that existing rights are respected, can be achieved by the top-trading cycles rule (Roth, 1982; Ma, 1994). However, for more general settings and for fairness in the stronger sense of ==== (or ====),==== no rule satisfying all three desiderata exists (Ergin, 2002). Since no ideal allocation rule exists, it is important to study the trade-offs of the various properties that represent ====, ====, and ====. This research agenda uses the ==== (see Thomson, 2001) to understand the various normative trade-offs when implementing certain types of rules instead of others. To place our paper within this research agenda, we would like to refer to the characterizations of deferred acceptance mechanisms (Kojima and Manea, 2010; Ehlers and Klaus, 2014, Ehlers and Klaus, 2016) and immediate acceptance mechanisms (Kojima and Ünver, 2014; Doğan and Klaus, 2018): in all those characterizations the priorities (or more generally, choice functions) are obtained together with the rule using a set of normative criteria that reflect our desiderata. Similar characterizations for top trading cycles rules, without exogenously fixed priorities, have not been established yet. We are interested in completing the research agenda of a full normative understanding of standard house allocation and school choice rules. To this end, in this paper, we study rules that allocate houses to agents for house allocation problems with existing tenants with a particular focus on top trading cycles (TTC) rules.====Next, we survey the state of the art of the axiomatic approach for housing market, house allocation, and house allocation with existing tenants problems. Then, we formulate the exact research question we tackle in this paper and explain our results against the background of the literature.","Top trading cycles, consistency, and acyclic priorities for house allocation with existing tenants",https://www.sciencedirect.com/science/article/pii/S0022053119300997,10 October 2019,2019,Research Article,208.0
"Esponda Ignacio,Pouzo Demian","UC Santa Barbara, United States of America,UC Berkeley, United States of America","Received 27 June 2019, Revised 22 September 2019, Accepted 25 September 2019, Available online 7 October 2019, Version of Record 17 October 2019.",https://doi.org/10.1016/j.jet.2019.104946,Cited by (1),"In developing the theory of long-run competitive equilibrium (LRCE), Marshall (====) used the notion of a representative firm. The identity of this firm, however, remained unclear. Subsequent theory either focused on the case where all firms are identical or else incorporated heterogeneity but disregarded the notion of a representative firm. Using Hopenhayn's (","The theory of long-run competitive equilibrium (LRCE), first developed by Marshall in his ==== (1890), has had a profound influence on our understanding of competitive markets. One distinguishing feature of Marshall's theory is his conceptualization of the (long-run) industry supply function. Pigou (1928), Viner (1953)[1931] and others subsequently formalized Marshall's notion of LRCE. The latter author, in particular, is credited for popularizing the typical diagram taught in introductory courses and reproduced in Fig. 1.====The figure represents an industry with fixed input prices where all firms are identical and characterized by the marginal (MC) and average (AC) cost functions depicted in the left panel. In an LRCE, price is at the minimum point of the AC function, ====, and aggregate quantity is given by the demand function evaluated at that price, ====. Suppose that there is a shift of the (inverse) demand function from ==== to ==== in Fig. 1. In the short run, the number of firms stays fixed, so price and quantity increase from the original LRCE at point ==== to the new short-run equilibrium at point ====, a movement occurring along the short-run supply function ====. But then firms make positive (economic) profits, and these profits attract additional firms into the market. In the long-run, the new LRCE is at point ====, where all firms make zero profits at price ==== and aggregate production increases to ====. Thus, the (long-run) industry supply function, ====, is horizontal at the minimum of the average cost function, ====.====A distinguishing characteristic of Marshall's analysis is the notion of a ====. While Marshall recognized that there are ==== firms in an industry, subsequent developments have focused on the case where all firms are identical in a long-run equilibrium. Viner (1953)[1931], pg. 222, justifies this view: ====Viner's argument may justify why firms do not make rents in the presence of markets that bid up the price of advantageous factors, such as exceptional managerial ability. But the argument does not imply that firms with different technologies or productivities cannot coexist in equilibrium. A realistic feature of an industry is that low-productivity firms can potentially become high-productivity firms and vice versa. This feature implies that equilibrium will be characterized both by coexistence of heterogeneous firms and turnover (entry and exit), and it does not seem appropriate to exclude these realistic features from a theory of LRCE.====Our objective in this paper is to go back to Marshall's original motivation and to extend the classical theory of LRCE to the case of heterogeneous firms. Fortunately, we don't have to formulate a new model, since Hopenhayn (1992) actually introduced and studied a model of competitive industry dynamics where firms' productivities evolve over time and exit and entry is an equilibrium phenomenon. We take the steady-state equilibrium in Hopenhayn's model as the natural extension of the theory of LRCE to the case with heterogeneous firms. Hopenhayn (1992), however, did not link his work to the early theory on LRCE, and our contribution is to fill-in this gap.====Our main result is that the (long-run) industry supply function with heterogeneous firms can indeed be characterized as the solution to the minimization of a representative average cost function, as Marshall originally envisioned. The standard textbook case, depicted in Fig. 1, is just a special case where there is no firm heterogeneity.====There are several reasons to care about this result. First, it formalizes Marshall's original motivation of a representative firm and of the industry supply function in the presence of heterogeneous firms. Second, it provides a connection between the early literature on LRCE and the modern literature on industry dynamics (to be reviewed below). Third, it makes the model of LRCE with heterogeneous firms accessible to a larger audience (in particular, the example in Section 2.2 conveys much of the intuition and can be taught in introductory courses). Finally, it permits a reinterpretation and extension of the classic textbook model as a reduced-form representation of a richer economy with heterogeneous firms and entry and exit.====Our paper links the classic theory of LRCE, which does not explicitly model dynamics, with the modern literature on ==== industry dynamics started by Lucas (1967). Lucas and Prescott (1971) developed the first theory of dynamic competitive equilibrium with stochastic demand, costly capital stock adjustments, and correct (i.e., “rational”) expectations about future prices, but firms are homogeneous and there is no entry and exit. Lucas (1978) studied a model where firms are heterogeneous, but there is still no entry and exit. Subsequent developments incorporated both firm heterogeneity and entry and exit, at the expense of no longer studying the dynamics of capital accumulation. Jovanovic (1982) developed the first of such models. Each period, a firm draws a productivity shock from a distribution that depends on an unknown productivity type. Firms have different productivity types and, as they learn their own type, more productive firms stay and less productive firms exit. The objective of these papers was to study the dynamic evolution of a competitive industry, not the steady state. Consequently, all of the interesting action happens outside the steady state and, indeed, there is no entry and exit in the steady state of these models.====Hopenhayn (1992) considers a model with both heterogeneous firms and entry and exit in the steady state. In contrast to Jovanovic's model, firms know their productivity types, but productivity types evolve randomly in such a way that firms that have a low productivity today can have a high productivity tomorrow and vice versa. As mentioned earlier, this is the model that we will use to formalize Marshall's idea that the LRCE of a competitive industry is characterized by the cost function of a representative firm.====To summarize, previous literature has followed one of two approaches. In the first approach, all firms are homogeneous, so the existence of a representative firm follows trivially, thus rendering a convenient and tractable framework. In the second approach, firm heterogeneity is explicitly introduced, but the notion of a representative firm is ignored. Our approach incorporates the best of both worlds: We explicitly introduce firm heterogeneity and show that, under certain conditions, a representative firm does exist.====For brevity, we focus on the case where input prices are fixed, which implies that the long-run industry supply function is horizontal. The extension to the case of input prices that increase with aggregate quantity was controversial in the early literature; see Opocher and Steedman (2008) for an insightful historical account. The initial approach, by Pigou (1928), Viner (1953)[1931], and others, considered a cost function that depends both on individual and aggregate quantity. Subsequent literature (e.g., Kaldor (1934), Allen (1938), and Hicks (1946)) criticized this reduced-form approach because of lack of microfoundations. For either approach, the extension of our result is straightforward: A given aggregate quantity leads to a given equilibrium input price and, fixing this input price, the LRCE price is still the minimum point on a representative average cost function. The long-run industry (inverse) supply function is simply the mapping from aggregate quantities to these minimum points. In particular, the aggregate supply function may be increasing if input prices increase with aggregate quantity.",The industry supply function and the long-run competitive equilibrium with heterogeneous firms,https://www.sciencedirect.com/science/article/pii/S0022053119300985,7 October 2019,2019,Research Article,209.0
"Gars Johan,Olovsson Conny","GEDB and the Beijer Institute, the Royal Swedish Academy of Sciences, SE-104 05 Stockholm, Sweden,Sveriges Riksbank, SE-103 37 Stockholm, Sweden","Received 29 November 2017, Revised 15 August 2019, Accepted 3 September 2019, Available online 27 September 2019, Version of Record 1 October 2019.",https://doi.org/10.1016/j.jet.2019.104941,Cited by (8),": fossil fuel and biofuel. With non-energy and energy goods being gross complements, and with higher costs for improving the energy efficiency for biofuel than for fossil fuel, there exist two balanced growth paths: one with low growth where energy is derived from biofuel and one with high growth where energy is sourced from fossil fuel. Heterogeneity in initial technology levels can generate the Great Divergence. The demand for fossil fuel in technologically advanced countries drives up its price, thereby reducing demand for fossil fuel in less advanced countries that instead choose the more stagnant energy input.","Sustained economic growth is a relatively recent phenomenon. Between year one and the 16th century, increases in income per capita were episodic and, at best, modest. Roughly 200 years ago, however, Western Europe and the Western offshoots (i.e., United States, Canada, Australia and New Zealand) began to emerge from this so called Malthusian trap and embark on a transition path to sustained economic growth.==== This process began in England around 1760 and is often referred to as the Industrial Revolution.====The transition to sustained growth has not, however, been universal. Many countries around the world have either remained stagnant or experienced only limited growth throughout history. Since some countries have reached a state of sustained growth while others remained stagnant, there are currently large and growing differences in income per capita across countries.==== Pomeranz (2000) refers to this phenomenon as the Great Divergence.====Even though several important contributions have significantly increased our understanding of the Industrial Revolution and the Great Divergence, these phenomena are still far from fully understood.==== The question of exactly which factors were crucial for the transition from stagnation to sustained growth remains at least partly unanswered. The same is true for the question of why the transition to sustained growth has not been universal. In a world where ideas can flow rapidly between countries, new ideas and machines that increase production in some countries should also be able to do that in other countries. Since the welfare effects of economic growth are so large, the question of exactly which factors can explain the sudden takeoff from stagnation to growth in some countries and the persistent stagnation in others is sometimes viewed as the most important question within the field of social science.====This paper analyzes whether accessibility to fossil fuels could be a potentially important factor simultaneously accounting for the pre-industrial period of stagnation, the post-industrial period of balanced growth, and the Great Divergence. Without making statements about causality, the focus on fossil fuel is motivated by the simple observation that income and economic growth tend to be high in regions and time periods in which a relatively large share of the total energy supply is derived from fossil fuel.====This observation is consistent with the Industrial Revolution. Prior to the Industrial Revolution, the standard of living was roughly constant, and economic growth was limited and temporary.==== The sources of energy in all pre-industrial civilizations were human and animal labor, water, wind, and biomass fuels (such as wood, crop residue, and dried dung). As shown in Fig. 1, the takeoff into sustained growth then occurs at the beginning of the 19th century. This is also the time period during which these energy inputs were gradually replaced by fossil fuels. Only a hundred years later, several European countries were almost completely energized by coal.====The observation also applies to the cross section, i.e., countries that have not embarked on any transition to sustained growth have also not chosen to substitute manual labor and biofuels with fossil fuel. This is shown in the first part of the paper, where we use data from the International Energy Agency on consumption of coal, oil, natural gas, nuclear power, and renewables for 134 countries over the time period 1960 – 2012 to derive two stylized facts. First, rich and poor countries use different energy inputs. Specifically, countries with higher income derive a larger share of their energy from fossil fuel, whereas poor countries, to a larger extent, use other energy sources. Second, countries that derive a large share of their energy from fossil fuel are growing at faster rates than countries that mainly use other energy inputs. Even though the fossil share and the growth rate are highly correlated, it is much harder to empirically make inferences about potential causality. The reason is that both variables are endogenous and are likely determined simultaneously.====To analyze the relationship between energy use and economic growth, we set up an endogenous growth model where final output is produced by combining a non-energy intermediate good that is produced with labor and capital, with two substitutable intermediate energy goods. One energy good is produced with fossil fuel, and the other is produced with what we broadly refer to as biofuel. We analyze the model analytically and show that it can account for the pre-industrial period of low or zero growth, the period after the Industrial Revolution with sustained balanced growth, the Great Divergence, and the two stylized facts.====Three assumptions are crucial for these results. First, energy and non-energy goods are assumed to be gross complements in the production of final goods. Second, while R&D investments make it possible to increase the efficiency of each intermediate good, it is more costly to improve the energy efficiency for biofuel than for fossil fuel, which we will argue is a historically realistic feature. Third, technological progress creates both advantages and disadvantages for countries that are less technologically advanced than those at the front. The advantage comes from technological progress spilling over across countries, which implies that backward countries benefit from technologically more advanced countries. As in Aghion et al. (2005), however, there is also a disadvantage of backwardness in that R&D becomes relatively more expensive the further away from the frontier a country is.====In this model, we derive three equilibria. First, abstracting from heterogeneity, we show that if the stock of fossil fuel in efficiency units is zero, there exists a Balanced Growth Path (BGP) that has low or zero growth and where final output is exclusively produced with biofuel. Intuitively, with energy and non-energy goods being gross complements, and with high costs for increasing the energy efficiency, overall growth will be low. This BGP is broadly consistent with the period before the Industrial Revolution.====Second, there exists a BGP where growth is constant and energy is asymptotically exclusively derived from fossil fuel. Positive investments are then continuously made to improve the efficiency of both labor and fossil fuel. This equilibrium is broadly consistent with the experience in the Western world in the period after the Industrial Revolution.====Third, introducing heterogeneity in the distribution of initial technology levels, we show that the model can produce the Great Divergence. Specifically, with sufficiently large differences in initial technology levels, the model features an equilibrium where a technologically advanced country experiences sustained growth and endogenously only produces with fossil fuel, whereas a technologically less advanced country instead endogenously chooses biofuel and growth is low or zero.====The main intuition for the Great Divergence in the model is that the demand for fossil fuel in the developing country depends on the (local) level of the fossil-energy efficiency in relation to the (global) fossil-fuel price. If the developed country is sufficiently more advanced, it drives up the fossil-fuel price, which lowers the demand for fossil fuel in the developing country. With a sufficiently small market for fossil fuel, and with the two energy inputs being gross substitutes, the market for improving the energy efficiency of fossil fuel becomes too small for R&D to be profitable in the developing country. Competition in the input market for fossil fuel thus creates a poverty trap by making the growth enhancing technology dominated by the technology with lower growth potential in the less developed country. Hence, if technology transfers require some R&D investments in the receiving country, the countries will diverge over time.==== In our benchmark specification, the positive effects of technological spillovers are exactly balanced by the negative effects of backwardness that come from higher R&D costs. As the negative effects are sufficiently reduced the range of parameter values for which the Great Divergence can be sustained are also reduced.====A loosely calibrated version of the model predicts that a less developed country will remain behind if technology levels in the developed world are just above five times higher than those in the less developed country. If the technology gap is smaller than this number, the less developed country will eventually catch up, but inequality can still, in fact, increase for 120 years before it actually starts to decrease.====We also analyze the transitional dynamics of the model and show that the existence of fossil fuel is not sufficient to initiate a transition to sustained economic growth. For this to be possible, the fossil-fuel stock (in efficiency units) must be sufficiently large. Otherwise, the market for improving the energy efficiency of fossil fuel is too small for R&D to be profitable. A numerical simulation of the model, however, shows that a realistically calibrated sequence of shocks to the stock of fossil fuel (replicating the growth rates for fossil fuels following the Industrial Revolution) can induce enough R&D in the fossil sector to initiate a transition from stagnation to sustained growth.====The model in this paper builds on several previous contributions. The presence of two different energy inputs bears some resemblance to Hansen and Prescott (2002), the endogenous multi-country growth model builds on Howitt (2000), and the features of directed technical change build on Acemoglu, 2002, Acemoglu, 2003. Finally, the role of energy transitions in long-run growth is analyzed in Tahvonen and Salo (2001) and Fröling (2011).====In focusing specifically on the role of fossil energy, we abstract from other important aspects such as human-capital accumulation, institutions, and demographic change. These issues have all been analyzed in great detail elsewhere.====This paper is structured as follows. Section 2 derives stylized facts about income, growth and energy use, Section 3 sets up the model, and Sections 4 – 5 present the results. The assumptions are discussed in Section 6, and Section 7 concludes.",Fuel for economic growth?,https://www.sciencedirect.com/science/article/pii/S002205311930095X,27 September 2019,2019,Research Article,210.0
"Name-Correa Alvaro J.,Yildirim Huseyin","Department of Economics, Universidad Carlos III, Calle Madrid 126, 28903, Getafe, Spain,Department of Economics, Duke University, Box 90097, Durham, NC 27708, United States of America","Received 10 August 2018, Revised 9 September 2019, Accepted 21 September 2019, Available online 25 September 2019, Version of Record 4 October 2019.",https://doi.org/10.1016/j.jet.2019.104943,Cited by (3),"We examine the consequences of vote transparency in committees whose members fear being blamed by interested observers for casting an unfavorable vote. We show that while individually undesirable, such social pressure can improve the collective decision by mitigating a voting externality. Hence, organizations may adopt public voting when the fear of blame is too little, and secret voting when the fear is too much. We also show that public voting is particularly desirable in committees with overly biased members or overly biased voting rules against the alternative. Anecdotal evidence supports our findings.","Many economic, social, and political issues are decided by committees. Notable examples include job promotions, jury trials, and legislative bills. Perhaps, the greatest advantage of a committee is to draw upon the diverse expertise of its members. Therefore, it seems plain and only democratic that committee members convey their expert opinions freely without feeling pressured by interested observers such as job candidates, defendants, and party leaders. Indeed, there is substantial evidence that decision-makers care about social pressure and would take costly measures to avoid it.==== In this paper, we argue that while it may be individually undesirable for a committee member, social pressure exerted by an interested observer can lead to better collective decisions by correcting for a voting externality. As a result, we find that organizations may institute more transparent voting to ==== social pressure on committee members when it is too low, and less transparent voting when it is too high. Our finding is supported by anecdotal evidence, which we discuss below.====Our base model features a group of experts who vote whether or not to accept a complex, multi-dimensional project by unanimous consent. For instance, job promotion in academia may involve research, teaching, and service aspects, while a healthcare bill may affect several states. Given his specialty, each expert privately evaluates the quality of only one dimension and is biased toward this dimension. We depart from the extant literature (see below) by assuming that besides the decision on the project, each expert is concerned about being blamed by an interested Bayesian observer (e.g., the job candidate, the defendant, or the party leader) for casting an unfavorable vote on the project so far as his vote can be inferred.==== Hence, social pressure that experts may feel depends crucially on the transparency of the vote, and we initially compare two such procedures: ==== vs. ==== voting, whereby either the entire vote profile or only the committee's decision is disclosed to the observer. We assume that the disclosure regime is chosen at an ==== stage by an uninformed planner who, perhaps having the greater society in mind, maximizes the average quality of projects without worrying directly about the potential blame faced by the committee.====Our first observation is that in a blame-free environment, each biased expert would be too demanding on his dimension of the project in that he would excessively vote down an otherwise high-quality project. Indeed, to correct for this negative “voting externality,” the planner would optimally dictate each expert to lower his acceptance standard. In practice, though, the planner cannot dictate experts' voting strategies because of their private knowledge about the issue, but she can influence them through social pressure. In general, anticipating blame for an unfavorable vote, each expert would relax his acceptance standard and do so more under public voting, since secrecy allows him to share the blame with the rest of the committee. We, therefore, find that if experts worry little about the blame, the planner ==== prefers public voting to amplify their worries “at the margin” whereas if their concern for blame is significant, the planner ==== prefers secret voting to diminish their concern.==== Hence, there is an optimal level of social pressure that the planner wants for committee members to mitigate the voting externality.====Given that the planner does not share the experts' dislike for blame, her optimal disclosure policy raises an obvious question: would the committee oppose to public voting? Interestingly, the answer is No. We show that if asked ====, before the arrival of any project, the committee members would strictly support public voting whenever it is the planner's choice. The reason is that by reducing the likelihood of an unfavorable vote, public voting also reduces the expected blame for a committee member due to his vote. Thus, any tension between the planner and the committee must be about decreasing transparency rather than increasing it.====To isolate social pressure as the unique source of strategic voting and easily compare the voting procedures, we have made four main assumptions in the base model: an all-or-nothing disclosure regime; experts with strong specialty biases; the unanimity rule for consensus; and a single observer. We relax each assumption in Section 5 and establish the robustness of our main results, though new insights emerge. We show that for a moderate level of blame, a ==== voting regime, whereby only the vote count is revealed to the outside observer, would strictly dominate the two extreme disclosure regimes. We then extend our analysis to less biased experts who also weigh other dimensions of the project, creating an additional source of strategic voting as in Moldovanu and Shi (2013). In particular, we find that with such interdependent preferences, experts lower their acceptance standards, the less biased they are toward their expertise, diminishing the planner's need for social pressure to correct for the voting externality mentioned above. As such, all else equal, we predict more transparent voting in committees that contain more biased experts. By considering general voting rules, we also confirm the optimality of public voting when, as with unanimity, the majority required for the project's approval is sufficiently strong, and the concern for blame is, again, too low. When the majority requirement is sufficiently weak, however, secret voting is always optimal, since the planner is primarily concerned about the over-acceptance of the project in this case.====We also obtain qualitatively similar results to the base model when there is a second observer who is against the project. We show that with two observers, what matters for an expert is the ==== blame. Hence, if, unlike in the base model, experts were blamed more for a favorable vote on the project, then secret voting would be optimal to prevent the over-rejection of the project. In some applications, it is, however, possible that the second observer is social-minded: he blames the committee members only when their votes do not match the ==== value of the project. Specifically, he blames an expert for an unfavorable vote on a socially desirable project as well as for a favorable vote on a socially undesirable project. We find that the presence of the social-minded observer points to secret voting. This is because, fearing ==== mistakes, experts are less willing to vote strategically, i.e., against their signals, under public voting, which does not serve the planner's objective to mitigate the voting externality mentioned above. Strategic voting is more pronounced under secret voting owing to the observer's need to infer the individual vote from the committee's decision in assigning blame. Nevertheless, even with secret voting, we conclude that the social observer is no substitute for the social planner who wants to minimize mistakes from an ==== perspective.==== There is strong anecdotal evidence that corroborates our results. In the United States, the meetings of organizations, clubs, legislative bodies, and other deliberative assemblies are often governed by ====, first introduced by Henry Martyn Robert in 1876.==== According to Robert's Rules, the usual method of voting is by the voice or show of hands; however, a secret ballot is recommended in the circumstances such as “the reception of members, elections, and trials of members and officers,” which apparently entail high peer pressure for the committee.==== For instance, the National Academy of Sciences and the American Kennel Club elect new members by a secret ballot. In this vein, T. Boone Pickens, an American business magnate, has recently lobbied for legislation to ensure confidential votes for shareholders. Pickens argues that shareholders would be more inclined to vote against the board of directors in a secret ballot (Ellerbach, 2018). The issue of vote transparency is also paramount in the judicial system. The courts typically release the identities of jurors both to the involved parties and to the news media, enabling post-verdict interviews. However, since the 1977 trial of ==== – the leader of a large drug trafficking network, – the courts have increasingly embraced fully anonymous juries in cases where the jurors' fear of retaliation by the defendants would be significant.==== In legislative bodies, all voting is a matter of public record. Although such transparency allows the party leaders to arm-twist their members, our investigation suggests that it may, nevertheless, be optimal so long as lawmakers do not care too much about an ==== unpopular vote. Last, but not least, in academia, colleges seem to leave the level of transparency to individual departments. For instance, the Stanford University policy “allows for secret or open ballots on promotions. Departments should adopt one system and apply it consistently in all cases.”==== Aside from the papers cited above, our paper is related to those on committee voting with “mixed motives”: besides the decision, each member may care about, e.g., being on the winning side (Callander, 2008), expressing his ideology (Morgan and Vardy, 2012), or ==== decision errors (Midjord et al., 2017). The non-instrumental motive in these papers is, however, purely internal, so transparency is a nonissue. As such, our work is more related to a large theoretical literature that examines the choice between public and secret voting. Most of this literature investigates vote transparency in the context of experts' reputational concerns – either reputation for precision/expertise; e.g., Ottaviani and Sørensen (2001), Levy (2007), Visser and Swank (2007, 2013), Meade and Stasavage (2008), Gersbach and Hahn, 2008, Gersbach and Hahn, 2012, Fehler and Hughes (2018) and Mattozzi and Nakaguma (2019), or reputation for preferences such as being social-minded; e.g., Gersbach and Hahn (2004), Stasavage (2007) and Ali and Lin (2013). Nevertheless, there are some studies where vote transparency affects a non-reputational motive. For instance, Malenko (2014) analyzes a common-value setting in which experts have a preference for conformity. She demonstrates that public voting may promote costly communication in the committee as it allows members to avoid being among the dissidents.==== In our model, such pre-voting communication would be less effective since committee members possess pure private values for the project. Gradwohl (2018) adds a direct preference for privacy to a Condorcet Jury model (with common values). He finds that public voting is socially optimal because by rendering privacy irrelevant, it induces nonstrategic voting, and that voter privacy may be best insured under partially public voting. Our model is different from his in two respects. First, voting is always strategic under blame avoidance, and depending on the level of blame, each disclosure regime can be socially optimal. Second, voting aggregates preferences – not information – in our model; as such, our setup is more in line with those of Albrecht et al. (2010) and Moldovanu and Shi (2013). These papers study a collective search problem with pure private (or more generally, interdependent) values for the project, which our static setup builds on. Last, but not least, Chemmanur and Fedaseyau (2017) examine a board's decision whether or not to fire the CEO (an outside observer), taking into account the possibility that each board member may face retaliation for dissent if the CEO is retained. They, however, do not explore transparency as an institution.====More broadly, the impact of social pressure in our model is reminiscent of the literature on vote buying. In particular, Dal Bo (2007) finds that the committee can be captured at virtually no cost if the voting is public while Seidmann (2011) and Name-Correa and Yildirim (2018) argue that secret voting and committee size can help deter capture. Unlike this literature, social pressure cannot be controlled by the interested observer in the current study,==== and vote transparency may improve the collective decision. Our paper is also reminiscent of the recent work on persuasion in voting. Specifically, Chan et al. (2019) establish that a self-interested sender can design the information structure so that each voter is persuaded to be pivotal in the passage of the alternative that entails a reputational (or political) cost. In contrast, the self-interested observer in our model would persuade each expert to be non-pivotal so that he would focus more on blame avoidance and vote affirmatively.====The remainder of the paper is organized as follows. In the next section, we lay out the base model. In Section 3, we characterize the optimal and equilibrium voting strategies under alternative disclosure regimes, followed by a comparison of them to establish the optimal disclosure in Section 4. In Section 5, we extend the analysis to partially public voting, interdependent valuations, general voting rules, and multiple observers. Section 6 concludes. Proofs of all formal results are relegated to an appendix.","Social pressure, transparency, and voting in committees",https://www.sciencedirect.com/science/article/pii/S0022053118304563,25 September 2019,2019,Research Article,211.0
"Nicolò Antonio,Sen Arunava,Yadav Sonal","University of Padua, Padua, Italy,University of Manchester, UK,Indian Statistical Institute, New Delhi, India,Umeå University, Umeå, Sweden","Received 24 May 2018, Revised 24 August 2019, Accepted 10 September 2019, Available online 23 September 2019, Version of Record 25 September 2019.",https://doi.org/10.1016/j.jet.2019.104942,Cited by (3),"We propose a model where agents are matched in pairs in order to undertake a project. Agents have preferences over both the partner and the project they are assigned to. These preferences over partners and projects are separable and dichotomous. Each agent partitions the set of partners into friends and outsiders, and the set of projects into good and bad ones. Friendship is mutual and transitive. In addition, preferences over projects among friends are correlated (homophily). We define a suitable notion of the weak core and propose an algorithm, the minimum demand priority algorithm (MDPA) that generates an assignment in the weak core. In general, the strong core does not exist but the MDPA assignment satisfies a limited version of the strong core property when only friends can be members of the blocking coalition. The MDPA is also strategy-proof. Finally we show that our assumptions on preferences are indispensable. We show that the weak core may fail to exist if any of the assumptions of homophily, separability and dichotomous preferences are relaxed.","In many situations agents are matched in teams in order to work on a project. Agents have preferences over the project they are asked to work on as well as over the partners they are assigned to work with. Consequently, forming stable teams is important - it ensures that agents do not have opportunities to abandon their assignments and do better for themselves.====A centralized authority matches agents in pairs and assigns them a project. We are interested in mechanisms that satisfy stability and provide incentives to agents to truthfully reveal their preferences. This problem shares some features with roommate matching models since agents have preferences over their potential partners. However it also has common features with one-sided matching models like the object allocation model and the house allocation problem because a project has to be assigned to each pair of agents. In this sense our model is a hybrid version of two classical models. We introduce appropriate notions of the weak and strong cores in our framework. We show that strong core allocations no longer exist in this setting. The contribution of our paper is twofold: we introduce a new model of matching and identify a suitable restriction on preferences such that weak core allocations exist. We propose an algorithm that generates weak core allocations and provides incentives to agents to truthfully reveal their preferences. Our paper can be thought of as an attempt to extend the matching literature to team formation focussing on the case of matching in pairs in a well-defined but restricted preference domain.====Our model is a variant of the roommate problem where agents have preferences over potential roommates ==== available rooms. A relevant application is the assignment of primary school teachers in Italy. In Italian primary schools around one third of the teachers are assigned in pairs to a class==== of students. The assignment of teachers to classes is done via a centralized mechanism at school level: the head of the school is responsible for matching the teachers in pairs and assigning each pair to a class of students. The two teachers work as a team, sharing the teaching load and doing some classroom activities jointly. It is therefore natural to assume that teachers have preferences over potential teaching partners and over the classes they teach.====Another example is the problem of assigning term papers to students. In several academic institutions, undergraduate or Master's students are required to take “Project” courses. Students have to undertake some independent (non-coursework) research in these courses. Due to the large number of students, they are often assigned in pairs (and sometimes in larger groups) to particular research topics. Students also benefit from learning from their peers. Grades in these courses depend in very large measure on a joint report prepared by a student team. As a result students have preferences over their assigned partner (often based on whether they “get on” with him/her and on their assessments of their abilities) and the project.====In our model there are a set of agents and a set of projects, and the number of projects is at least as large as the number of pairs of agents. We assume that it is not feasible for an agent to be unassigned. For instance, in the term paper assignment problem, every student must write a term paper for the course with another student. It is not an option for a student to not do a project. It would also be unfair for some students to work on their own while others work in pairs. Similarly dorm rooms are often built for sharing, and fairness would require all agents to share these rooms instead of some agents living on their own while others share. An alternative approach would be to assume a preference restriction: all agents strictly prefer to be assigned a partner and a project than to remain unassigned. We adopt the feasibility approach for convenience. Each agent has a preference ordering over partners and projects.====We investigate stable and strategy proof assignment rules in this setting. For stability, we employ standard notions of the core - the strong core and the weak core.==== A coalition can block an assignment using either an unassigned project or a project assigned to a pair of agents ====. An agent cannot unilaterally evict her partner from the assigned project and use this project with other members of the blocking coalition. A coalition can strongly block if all members of the blocking coalition are no worse-off and at least one member is strictly better off. It can weakly block if all members are strictly better off. Strong and weak core assignments are those that are immune to weak and strong blocking respectively. These concepts differ whenever indifferences occur in agent preferences (as they will in our model).====Neither the strong nor the weak cores exist in our model with an unrestricted domain of preferences. One of the contributions of our paper is to provide a plausible restriction on preferences under which the weak core and some variants of the strong core exist. We describe our preference domain below.====Preferences are ==== over partners and projects. Also preferences over each component are ==== i.e. alternatives in each component are partitioned into good and bad sets.==== The set of possible partners is partitioned into friends (good partners) and outsiders (bad partners), and the set of projects into good and bad projects. Therefore every partner, project pair can be placed into one of four indifference classes.==== Friendship is mutual and transitive, so that the set of agents can be partitioned into groups of friends. Finally, preferences of friends are correlated (====).==== Specifically, we assume a strong alignment in the preference for projects among friends: for any pair of friends, the set of good projects of one of them is weakly contained in the set of good projects of the other one. However, two friends need not have the same set of good projects. One agent can be fussier than the other and like only a subset of the projects liked by the other agent. Notice that homophily is satisfied when the projects can be ordered according to a criterion such as the level of complexity, degree of riskiness etc. and a threshold divides the projects into good and bad ones. Friends evaluate the projects using the same criterion but they may have different thresholds.====In our opinion, our model restrictions in the Italian primary teacher assignment application are believable. Teachers have to be matched in pairs to a “project” (a class) and cannot remain unmatched. Preferences over colleagues and classes are likely to be independent and the fact that teachers divide the set of colleagues and classes into those they like more and those they like less, seems plausible. Teachers typically have correlated preferences over classes==== and may have common criteria to evaluate them. For instance, teachers may agree that it is more difficult to teach a class in which there are many children from non-native language speaking families. However they are likely to disagree on the critical number of such students that make a class “hard” to teach.====Our main results are as follows. We show that the strong core may fail to exist in our domain. We propose an algorithm, the minimum demand priority algorithm (MDPA) that generates a weak core assignment. We also consider an intermediate core notion which we call the friendship core. In this notion, agents can weakly block but all members of the coalition must be friends. We show that the friendship core exists subject to certain cardinality restrictions.====We also investigate the incentive properties of the MDPA. Since friendship relationships are mutual, we assume that they are common knowledge. However agent preferences about good projects is private information. There is a conceptual difficulty arising from the fact that homophily is a restriction on preference profiles, rather than on individual preferences. Nevertheless we find a convenient way to deal with this issue. We assume that for every group of friends, there is a common ordering over projects in terms of acceptability. Each agent is characterized by a threshold level of acceptability - all projects “less” than the threshold project are good for the agent. A profile of thresholds generates a profile of preferences satisfying homophily and strategy-proofness can be then defined in the usual way. We show the MDPA algorithm is strategy-proof.====The key assumptions underlying our preference domain are separability, dichotomous preferences and homophily. We show the weak core fails to exist if any one of the three is dropped, while maintaining the other two. Our preference restrictions are indispensable in this sense.====Earlier in the introduction, we alluded to the fact that our model is a combination of the house allocation model and the roommate problem. We would like to emphasize that our model is distinct from both - in particular, none of our results follow from results in either model.",Matching with partners and projects,https://www.sciencedirect.com/science/article/pii/S002205311830214X,23 September 2019,2019,Research Article,212.0
"Ellman Matthew,Hurkens Sjaak","Institute for Economic Analysis (CSIC) and Barcelona GSE, Spain","Received 12 April 2017, Revised 12 July 2019, Accepted 27 August 2019, Available online 5 September 2019, Version of Record 11 September 2019.",https://doi.org/10.1016/j.jet.2019.104939,Cited by (53),"This paper characterizes profit- and welfare-maximizing reward-based crowdfunding, defined by an aggregate funding threshold for production. We disentangle crowdfunding's selling and funding roles, locating its key benefit in its market test role of adapting production to demand. Multiple prices prove necessary for effective learning and adaptation, even with relatively large crowds. Mechanism design proves general optimality in our baseline and shows the value of limiting reward quantities. Funding is not fundamental and crowdfunding may even complement traditional ====. We characterize welfare consequences, model price dynamics and identify platform designs and regulations that enhance innovation and social benefits.","Crowdfunding is a rapidly growing phenomenon with a major promise: to bring more socially beneficial projects to fruition. Online crowdfunding platforms have sharply reduced entrepreneurs' costs of pitching their projects to a wide range of potential funders before sinking the costs of production, ====. We study the prominent case of reward-based crowdfunding where funders are compensated with the project's product. So the funders are buyers. Each buyer chooses a bid after the entrepreneur sets a funding ====, ==== and a ====, ====. Production occurs in the “success” event where the aggregate funds or sum of bids reaches the threshold.==== That is, a simple aggregate fund threshold (AFT) fully determines production. The entrepreneur then receives these funds and has to sink ==== and deliver her product to all buyers who bid at least ====. Buyers can rest assured that (1) they pay nothing when funding fails and (2) they pay exactly their bids when it succeeds; so bids are prices. Together with crowdfunding's defining characteristic AFT, these reassuring properties explain why so many small funders are willing to participate.====We characterize optimal crowdfunding design with two buyer types and both for-profit and not-for-profit entrepreneurs. Using mechanism design, we find the general optimum (Proposition 1, Proposition 2 in Section 3) and then show how crowdfunding implements it (Proposition 3, Proposition 4 in Section 4). Our analysis disentangles crowdfunding's selling and funding roles. We locate its main benefit as a market test that adapts production to revealed demand: by only producing when the threshold is reached, crowdfunding avoids sinking ==== when demand is low. The threshold's implicit threat of non-production also improves rent-extraction. Proposition 5 derives the welfare implications.====Funding is not fundamental here: even an entrepreneur with no credit constraints uses crowdfunding for adapting to demand and extracting rent. With credit constraints, crowdfunding can, as its name suggests, substitute for traditional finance (Corollary 3), but we identify complementarities when entrepreneurs have not-for-profit motivations (Corollary 4) and when campaigns cannot reach all potential buyers (Proposition 12).====We now describe the main results in our baseline model with profit-maximization where a finite number of buyers have independent valuations that are either high (====) or low (====). Optimal crowdfunding design, like traditional selling, hinges on whether the high type frequency exceeds the ratio of low to high valuations. Above this critical value (high frequency), price is set at the high valuation, ====, which implies that low types are excluded (E), while below it (low frequency), ==== and low types are included (I). In the high frequency (exclusion) case, crowdfunding adapts production to demand via a threshold set at cost (====) to preclude losses. This induces production when the number of high types exceeds a cutoff which we call the pivot, ====. The low frequency (inclusion) case is more interesting. Crowdfunding again adapts production, but now sets ==== to extract rent from high types by making them sometimes pivotal for production. The optimal threshold trades off higher rents against lower success rates. Each high type pays less than with exclusion but thanks to the payments from low types under inclusion, the pivotal number of highs needed for production is lower (====). So production is more likely despite the higher threshold. Crowdfunding's adaptation of production to demand raises welfare in general, but excessively high thresholds in the low frequency (inclusion) case waste some production opportunities and can lower welfare.====Crowdfunding's profit gains are small in this illustration, because the relatively large crowd and ==== valuations restrict per capita demand uncertainty.==== Even so, the impacts on project success rates and social welfare are substantial, especially when motives extend beyond short-run profits, as we explain below. Notice that, despite the low pivotality motive that limits rent-extraction, the resulting multiple prices are crucial for profit and welfare gains. Imposing a single crowdfunding price in case (ii) would induce an exclusive strategy with a minimum price of 20, reducing project success rates to just 0.02% and decimating profits and welfare to 0.01. Sections 5.4 and 6 show that crowdfunding is important for pricing and profits in arbitrarily large crowds when buyers' valuations are correlated since demand uncertainty then remains substantial.====The not-for-profit solution is a little more involved. We model the idealized case where they maximize expected welfare under the constraint that expected profits cannot be negative.==== At low frequency, they maintain inclusion but reduce the threshold and corresponding production pivot as far as possible. In case (ii) of Illustration 1, Esther lowers her threshold to 2644.84, needing only 102 fans (paying 6.42 each). This lower production pivot more than doubles the success rate to 42.9% and welfare to 632.17.====At high frequency, welfare-maximizers still want to include low types, but deficit-avoidance may then require a production pivot above ====. In case (i) of Illustration 1, Esther can raise welfare to 1227.59 by setting minimum price 5 and threshold 2647.51, but this requires at least 149 fans (paying 5.99 each). Our general mechanism design solution indicates that such entrepreneurs should instead adopt a partially inclusive solution where low types get the good with a probability ==== less than one. We now describe two ways crowdfunding can implement the partially inclusive solution for case (i) by adding a modified reward. First, Esther could sell lottery tickets giving a ==== chance of winning a CD, at 3.70, as well as selling the CD at price 9. Combined with a threshold of 2554.90, this raises expected welfare from 1227.59 to 1589.91. Second, Esther could limit the set of low-priced rewards: setting threshold 2487 and offering an unlimited number of CDs at a price of 9 but only 258 at price 5 rations low types with probability ====.====Multiple rewards are an important feature of crowdfunding platforms and a recurring theme of our study. First, while our baseline treats a single product, the two prices in our inclusive solution are often coordinated via two rewards: the basic product and the product plus some token of appreciation to thank the high price bidder. Second, welfare-maximization may add stochastic or limited rewards for low types as just described. Third, in Section 5.2 we prove that product differentiation and crowdfunding are orthogonal tools for price discrimination (Proposition 8): crowdfunding does not change optimal differentiation, though differentiating optimally, by raising profitability, does weakly lower the optimal crowdfunding pivot.====Moving beyond the focus on entrepreneurs and buyers, Section 5 also analyzes the role of crowdfunding platforms, who create value by matching those two sides. One important platform decision is whether to ban or allow self-bidding. Entrepreneurs have incentives to bid on their own projects when funds exceed cost but fall short of the threshold. Self-bidding undermines “threshold-commitment”, the commitment not to produce when the threshold is not reached, and thus affects rent-extraction. We characterize optimal crowdfunding under no commitment (Proposition 6, Proposition 7). Platforms charge fees proportional to revenues and we characterize when platforms are biased towards promoting and attracting profit-maximizers or not-for-profits (Proposition 9).====Section 6 extends the model to allow for post-crowdfunding sales and uncertain project quality and derives profit-maximizing crowdfunding design (Proposition 11). Ex-post sales are a key source of complementarity between crowdfunding and traditional finance (Proposition 12). Project quality shocks generate correlation and imply substantial crowdfunding benefits for arbitrarily large markets (see also Proposition 10).==== This extension explains observed pricing dynamics and shows how reward rationing also raises post-crowdfunding price credibility.====Section 7 characterizes the general solution with multiple types and discusses how crowdfunding can and must be adjusted to implement it. Section 8 concludes.",Optimal crowdfunding design,https://www.sciencedirect.com/science/article/pii/S0022053119300870,5 September 2019,2019,Research Article,213.0
"Le Treust Maël,Tomala Tristan","ETIS UMR 8051, Université Paris Seine, Université Cergy-Pontoise, ENSEA, CNRS, F-95000, Cergy, France,HEC Paris and GREGHEC, 1 rue de la Libération, 78351 Jouy-en-Josas, France","Received 27 August 2018, Revised 15 August 2019, Accepted 1 September 2019, Available online 5 September 2019, Version of Record 16 September 2019.",https://doi.org/10.1016/j.jet.2019.104940,Cited by (74),"We consider a ==== persuasion problem where the persuader and the decision maker communicate through an imperfect channel that has a fixed and limited number of messages and is subject to exogenous noise. We provide an upper bound on the payoffs the persuader can secure by communicating through the channel. We also show that the bound is tight, i.e., if the persuasion problem consists of a large number of independent copies of the same base problem, then the persuader can achieve this bound arbitrarily closely by using strategies that tie all the problems together. We characterize this optimal payoff as a function of the information-theoretic capacity of the communication channel.","In modern internet societies, pieces of information are repeatedly and continuously disclosed to decision makers by informed agents. Information transmission is affected by at least two sources of friction. First, the sender and the receiver of a given message may have nonaligned incentives, in which case the sender might be unwilling to transmit truthful information. Second, communication between agents is often imperfect. The sender and the receiver may have time constraints to write or read messages, forcing the sender to summarize his arguments and making him unable to convey all the details. Further, there might be discrepancies between the informational content of a message that is intended by the sender and the one understood by the receiver. For instance, if the mother tongue of the sender and of the receiver are different, there are possible translation errors (see Blume et al., 2007). Additionally, messages traveling in a network of computers might be subject to random shocks, internal errors or protocol failures. Studying the effect of noise in communication channels is the starting point of information theory (Shannon, 1948).====Our paper aims to study the following questions. How does imperfect communication reduce the possibilities of persuasion in a sender-receiver interaction? When the sender communicates many pieces of information, to what extent does tying the pieces together help in overcoming the communication limitations?====We consider a sender and a receiver who communicate over an imperfect channel and are engaged in a series of ==== persuasion problems. The sender observes ==== independent and identically distributed pieces of information and sends ==== messages to the receiver. Messages are sent through a channel that consists of two finite sets ==== of respectively inputs and outputs messages and of a transition probability ==== from ==== to ==== such that when the sender chooses input message ====, the receiver receives output message ==== with probability ====. Upon receiving ==== output messages from the channel, the receiver chooses ==== actions, one for each problem. Payoffs are additively separable across persuasion problems. We assume that the sender is able to commit to a disclosure strategy that maps sequences of pieces of information to distributions of sequences of input messages.====We study the optimal average payoff secured by the sender by committing to a strategy. We give an upper bound on this optimal payoff and show that this bound is achieved asymptotically when the numbers ==== and ==== grow large. To prove this latter statement, we borrow techniques from information theory, namely, the coding and decoding schemes of Shannon, 1948, Shannon, 1959. This machinery allows to transmit a sequence of messages over a noisy channel with the property that the receiver recovers almost all messages correctly. The information theoretic literature typically considers an obedient receiver who calculates the decoded messages and takes them at face value. In the persuasion game framework, the receiver is strategic and may not follow any prescribed scheme. Rather, the receiver takes into account the strategy of the sender and the received outputs, calculates its Bayesian belief about the sequence of states, and chooses a sequence of actions that maximizes its payoff. Our technical contribution is to construct a strategy of the sender for which we are able to estimate and to control those Bayesian beliefs in order to ensure that the strategic receiver chooses a desired sequence of actions.====Our upper bound is the value of an ====, which represents the best payoff that the sender can achieve by sending a message, subject to the constraint that the mutual information between the state and the message is no more than the capacity of the channel. We show that this value is given by the concave closure of the payoff function of the sender, subject to a constraint on the entropy of posterior beliefs. This is also given by the concave closure of a modified payoff function, where the sender pays a cost proportional to the mutual information between the state and the message.",Persuasion with limited communication capacity,https://www.sciencedirect.com/science/article/pii/S0022053118305064,5 September 2019,2019,Research Article,214.0
Heumann Tibor,"Department of Economics, HEC–Montreal, Montreal QC, H3T 2A7, Canada","Received 13 March 2018, Revised 9 August 2019, Accepted 27 August 2019, Available online 2 September 2019, Version of Record 4 September 2019.",https://doi.org/10.1016/j.jet.2019.104938,Cited by (4),"This paper examines a single-unit ascending auction where agents observe two-dimensional Gaussian signals. The model combines the pure private-values model with the pure common-values model. The challenge is to characterize how the multi-dimensional signals observed by an agent are aggregated onto that agent's one-dimensional bid. The challenge is solved by projecting an agent's private signals onto a one-dimensional ====; the equilibrium bidding strategies are constructed as if each agent observed only his own equilibrium statistic. An agent's equilibrium statistic aggregates this agent's private signals while taking into account the additional information deduced from the other agents' bids. In contrast to one-dimensional environments, an ascending auction may have multiple symmetric equilibria that yield different social surpluses."," Auctions have been extensively studied in economics. It is an empirically relevant and a theoretically rich literature: auctions are commonly used to allocate goods across agents, and there is a rich class of models that allows the study of bidding in auctions. A critical assumption in most auction models is that agents observe one-dimensional signals. In this paper, the main objective is to characterize the equilibrium of an auction in which agents observe multi-dimensional signals. As a by-product, we provide predictions of auctions that arise only when agents observe multi-dimensional signals.====Our paper is motivated by the observation that, in many environments, agents' information is naturally a multi-dimensional object. Consider, for example, the auction of an oil field, and suppose that an agent's valuation of the oil field is determined by its size and by the agent-specific cost of extracting oil. Furthermore, assume that each agent privately observes her own cost and all agents observe conditionally independent noisy signals about the oil field's size. This environment is therefore one in which agents observe two-dimensional signals. In most auction environments, agents similarly observe multi-dimensional signals about their valuation of the good (e.g., timber, highway construction procurements, art, real estate).====There is an important conceptual difference between bidding in an auction with one-dimensional signals and in one with multi-dimensional signals. If agents observe one-dimensional signals, then observing agent ===='s bid is informationally equivalent to observing agent ===='s signal. Yet, in environments with multi-dimensional signals, observing agent ===='s bid is ==== informationally equivalent to observing all the signals observed by agent ====. It follows that a bid reflects only an aggregate statistic of an agent's signals. In the oil field example, agent ==== cannot tell whether agent ===='s low bid is due to a high cost of extracting oil or to agent ===='s belief that the oil reservoir is small. The extent to which agent ===='s bidding is driven by his private costs or his beliefs about the oil reservoir's size is critical for agent ==== to determine her own bidding strategy.==== After all, agent ===='s valuation of the oil field is independent of agent ===='s costs but is not independent of agent ===='s signal about the size of the oil reservoir. This distinction leads to a crucial difference in the equilibrium bidding. The conceptual challenge is to account for the feedback between agents' bids: the way agent ===='s signals are aggregated onto his bid depends on how agent ===='s signals are aggregated onto her bid.==== Our model consists of ==== agents bidding for an indivisible good in an ascending auction. The utility of an agent who wins the object is determined by a common shock and an idiosyncratic shock. Each agent privately observes his own idiosyncratic shock and, additionally, each agent observes a (noisy) signal about the common shock. The signals and the payoff shocks are normally distributed. We focus on symmetric environments and symmetric equilibria.====The two-dimensional signals contain elements of a pure–common values environment and a pure–private values environment; our only departure from classic models in the auction literature is the information structure's multi-dimensionality.==== This allows us to identify those elements of bidding strategies that arise only due to the multi-dimensional signals.====The focus on an ascending auction and on Gaussian signals is useful in fully characterizing a class of equilibria. The ascending auction is a frequently used auction format and the assumption of Gaussian signals has been used in the empirical auction literature (see, for example, Hong and Shum (2002)). Therefore, this is a natural model for the study of auctions with multi-dimensional signals.==== This paper's main result is to construct a class of equilibria in the ascending auction. In the class of equilibria we construct the drop-out time of an agent is determined by a linear combination of the signals he observes; we refer to this linear combination of signals as an ====. The equilibrium bidding strategies are the same as if each agent observed only his own equilibrium statistic. An agent's equilibrium statistic is a sufficient statistic — for this agent's two private signals— to compute his valuation at the end of the auction (i.e. after all drop-out times are observed); however, an agent's equilibrium statistic does not satisfy this sufficiency property before the auction ends.====The equilibrium characterization is tractable because the drop-out time of an agent is determined by a linear combination of the signals he observes: the equilibrium statistic. The linearity arises because expectations with Gaussian signals are linear. The ascending auction allows us to keep the Bayesian updating within the Gaussian family when we evaluate the equilibrium conditions. In the equilibria we characterize, an agent's drop-out time remains optimal even after observing the drop-out time of all other agents.==== Consequently, we evaluate the best response conditions using the ==== drop-out time of each agent (and not a lower bound for those times). This property of the equilibria in an ascending auction, in conjunction with the assumed Gaussian nature of signals, renders the problem tractable. For example, a first-price auction with Gaussian signals does not preserve the same tractability because in such an auction it is not possible to evaluate an agent's best-response conditions using the realized bids of other agents.====We show that, in contrast to one-dimensional environments, multi-dimensional environments may feature multiple symmetric equilibria of an ascending auction.==== The various equilibria yield distinct levels of revenue as well as a different social surplus.==== The multiplicity of equilibria is due to a strategic complementarity in the weight that agents place—in their bidding strategy—on their own idiosyncratic shock. This complementarity arises because signals must be aggregated onto an agent's bid.==== There is an extensive literature on auctions with one-dimensional signals. Much of this research is based on the seminal contribution of Milgrom and Weber (1982), who eloquently describe the assumption as follows: ====The literature on auctions with multi-dimensional signals has made progress in two ways. The first way is to make the appropriate assumptions about the distribution of signals such that an agent's bid is determined only by her interim expected valuation. The second way is to provide properties of an auction without having to characterize the equilibrium bids. The distinguishing features of our paper are that we do not impose any assumptions on how signals are correlated across agents and we fully characterize a class of equilibria. This allows us to study how signals are aggregated onto an agent's bid while taking into account the feedback effects between bids of different agents, which ultimately delivers new predictions about ascending auctions. We now discuss the literature on auctions with multi-dimensional signals and interdependent valuations.====Wilson (1998) studies an ascending auction with two-dimensional signals. Wilson (1998) assumes that the random variables are log-normally distributed and also drawn from a diffuse prior. Because of its tractability (which our model shares to a great extent), the model studied by Wilson (1998) is often used in empirical work.==== In Section 4.4, we explain how the model in Wilson (1998) can be obtained as a particular limit of our model.====Dasgupta and Maskin (2000) studies a generalized VCG mechanism. They show that, if agents' signals are independently distributed across agents, then an agent's interim expected valuation delivers a one-dimensional statistic that can be used to characterize the mechanism's Nash equilibria.==== The intuition is that an agent has no information about the signals observed by other agents and so the impact of the signals on the agent's payoff is a sufficient statistic of all the information this agent observes. Provided the signals are independently distributed, this approach can also be used in many other mechanisms—including a first-price auction and an ascending auction (see Goeree and Offerman (2003) or Levin et al. (2007)). We discuss the assumption of independent signals after we provide the information structure in our model (in Footnote 12).====Jackson (2009) provides an example of an ascending auction for which no equilibrium exists. The model he describes is similar to ours in that it features both a private and a common signal; however, the distribution of signals and payoff shocks in that model have finite support (so they are non-Gaussian). Jackson (2009) establishes that the existence of an equilibrium is not guaranteed in an auction model with multi-dimensional signals. The extent to which it is possible to construct equilibria with multi-dimensional non-Gaussian information structures is still an open question.====Pesendorfer and Swinkels (2000) study a sealed-bid uniform price auction in which there are ==== goods for sale, each agent has unit demand, and each agent observes two-dimensional signals. They study the limit in which the number of agents grows to infinity. Pesendorfer and Swinkels (2000) are able to provide asymptotic properties of any equilibrium without the need to characterize such an equilibrium or even prove its existence.====Finally, there is a literature that studies mechanism design with multi-dimensional signals. Jehiel and Moldovanu (2001) show that when agents observe multi-dimensional independently distributed signals there is no efficient mechanism. The impossibility result in Jehiel and Moldovanu (2001) does not apply to our model because in our model signals are not independently distributed (this is precisely what makes the construction of an equilibrium challenging); however, the Nash equilibrium in our model is still inefficient. Jehiel et al. (2006) show that when agents observe multi-dimensional signals then it is not possible to construct a mechanism that has an ex post equilibrium. The equilibria of the ascending auction when agents observe multi-dimensional signals is not an ex post equilibrium.====The rest of our paper proceeds as follows. Section 2 describes the model and Section 3 studies one-dimensional signals. Section 4 characterizes the equilibrium with two-dimensional signals. Section 5 illustrates the presence of multiple equilibria and Section 6 examines the effect of a public signal. We conclude in Section 7 with a discussion about our paper's main assumptions and possible extensions. Appendix I gives the proofs that are not included in the main text.",An ascending auction with multi-dimensional signals,https://www.sciencedirect.com/science/article/pii/S0022053119300869,2 September 2019,2019,Research Article,215.0
Lei Xiaowen,"Department of Economics, University of Oxford, Manor Road Building, Oxford, OX1 3UQ, United Kingdom","Received 27 June 2018, Revised 12 August 2019, Accepted 15 August 2019, Available online 26 August 2019, Version of Record 6 September 2019.",https://doi.org/10.1016/j.jet.2019.08.007,Cited by (12),This paper studies ,"We live in a so-called ‘information age’. We also live in an age of growing inequality. This paper shows these phenomena might be connected. It attempts to formalize and quantify an argument by Arrow (1987). Arrow noted that in financial markets the value of information is greater, the greater is the amount invested. At the same time, the cost is likely to be nearly independent of the amount invested. Consequently, wealthy individuals devote a higher fraction of their wealth to information. Arrow used a simple 2-period/1-agent example to argue that endogenous information acquisition amplifies inequality.====Although suggestive, Arrow (1987)'s example cannot address the quantitative significance of information acquisition in the dynamics of wealth inequality. This paper quantifies Arrow's example by incorporating learning and information acquisition into recent models of idiosyncratic investment risk. In contrast to models of idiosyncratic labor income risk (Aiyagari, 1994), investment risk models generate the sort of power laws that characterize observed wealth distributions.====To focus on the role of information, this paper abstracts from all other sources of heterogeneity that create inequality. Agents have identical life expectancy, are born with identical initial wealth, and have access to private investment projects with identical mean returns. This mean return is unknown, however, and agents must learn about it by witnessing the history of their own investment returns. In addition, agents can pay a cost to acquire an additional signal. More precise signals are more expensive. In this environment, inequality is initially created by luck. Relatively wealthy agents experience relatively high investment returns. With learning, luck plays two important roles. First, it makes agents relatively ‘optimistic’, in the sense that high returns produce relatively high estimates of the mean return, which encourages risk-taking and wealth accumulation. Second, higher wealth allows agents to buy more information, which makes them more ‘confident’, where confidence is defined by the precision of their estimate.====Although it is well known that random growth over a random length of time is enough to generate fat-tailed Pareto wealth distributions (Reed, 2001), recent work by Gabaix et al. (2016) (henceforth GLLM) shows that these random growth models cannot explain observed inequality quantitatively. In particular, they show their transition dynamics are far too slow. GLLM argues that it is important to allow for either ‘type dependence’ or ‘scale dependence’, which generate deviations from Gibrat's Law. This paper shows that endogenous information acquisition produces a form of scale dependence.==== The mean growth rate and volatility of an agent's wealth increase with wealth. This occurs for two reasons. First, relatively wealthy agents have higher savings rates. This is a widely documented feature of the data (Dynan et al., 2004). Second, relatively wealthy agents allocate a higher fraction of their portfolios to risky assets. Again, this is a widely documented feature of the data (Carroll, 2002). The key mechanism driving both these decisions is that wealthy agents buy proportionally more information. They do this because information is proportionally more valuable to them.====One downside of studying a scale-dependent growth model is that we can no longer solve the model analytically. The model consists of a set of partial differential equations, which cannot be solved in closed-form.==== In response, I employ a combination of Monte Carlo simulation techniques and classical perturbation approximations.====One might suspect that lowering the cost of information would exert an equalizing force on the distribution of wealth. Models based on asymmetric information and insider trading no doubt have this implication. However, here investment projects are agent-specific. One agent's information is of no value to anyone else.==== As far as information choice is concerned, each agent is a Robinson Crusoe. From the perspective of an individual agent, information is simply a source of increasing returns, since it encourages risk-taking, which encourages growth. However, when these agent specific scale effects are combined with heterogeneous, non-diversifiable shocks, a powerful force for inequality is ignited. Agents who get lucky early in life use their good fortune to acquire information about future investment returns. In this way, wealth begets wealth.====To study the quantitative implications of the model, I assume the US economy started from a stationary distribution in 1981, with information being relatively costly. I calibrate the initial value of the information cost parameter to match the initial top 1% wealth share. Since information costs are not easily measured, to evaluate the ability of the model to explain changes in observed inequality, I compare two alternative strategies. To start, I simply pick the information cost parameters to replicate the top 1% wealth shares in both 1981 and 2014, and then evaluate their plausibility by calculating the implied shares of wealth and income spent on information. This strategy suggests that information costs declined by a factor of 24 for the median household. Although the initial shares of wealth and income spent on information seem plausible, the implied 2014 shares seem far too high. However, this should not be too surprising, since this strategy assumes that information costs can explain the ==== increase in wealth inequality. In response, I then use fees from the hedge fund industry to directly measure the decline in information costs. These data suggest that, by 2014 costs had declined by a factor of 11 for the median household. Introducing this information cost reduction into the model, and assuming the economy had reached its steady state by 2014, I find that the top 1% wealth share increases from 24.4% to 32.7%. Since the actual share was 37.2%, hedge fund data suggest that the model can account for about two-thirds of the observed increase.====The analysis here is related to work by Peress (2004) and Kacperczyk et al. (2018). They too are motivated by Arrow (1987). However, there is an important difference between their work and mine. Endogenous information acquisition in these papers is about allocating attention to multiple assets for a given information capacity. Initially wealthy agents are ==== to have higher information capacity. These models focus on portfolio choice, and emphasize the general equilibrium effects on endogenous asset prices, with certain assets becoming less traded by unsophisticated investors due to strategic substitution. In my model, information capacity is endogenous, whereas asset returns are exogenous. By shutting down the general equilibrium channel and assuming a private investment technology, I am able to derive explicit expressions for the dynamic relationship between wealth and information. Note that in my model, agents do not initially differ in either wealth or information capacity. Lucky agents who are hit with a series of positive shocks ==== more sophisticated because information is more valuable for wealthy agents, which makes them even wealthier.====The remainder of the paper is organized as follows. Section 2 motivates the discussion by providing background information on wealth inequality, hedge fund growth, and household portfolios. Section 3 outlines the model. Section 4 derives first-order perturbation approximations of an agent's policy functions. Sections 5 studies aggregation and the cross-sectional distribution of wealth and beliefs. Section 6 presents numerical solutions to the stationary wealth distribution. Section 7 compares top wealth shares in several economies with alternative information structures. Section 8 reports a sensitivity analysis by showing how top wealth share changes when the benchmark parameter values change. Section 9 briefly discusses related literature, and Section 10 offers a few concluding remarks on policy implications and possible extensions. Proofs and derivations are contained in an online technical appendix.",Information and Inequality,https://www.sciencedirect.com/science/article/pii/S002205311830317X,26 August 2019,2019,Research Article,216.0
"Liu Tingjun,Bernhardt Dan","Faculty of Business and Economics, The University of Hong Kong, Hong Kong,University of Illinois, United States of America,University of Warwick, United Kingdom of Great Britain and Northern Ireland","Received 25 June 2018, Revised 10 May 2019, Accepted 29 June 2019, Available online 23 August 2019, Version of Record 11 September 2019.",https://doi.org/10.1016/j.jet.2019.06.009,Cited by (3),"We analyze the design and performance of equity auctions when bidder's valuations and opportunity costs are ====, distributed according to an arbitrary joint density that can differ across bidders. We identify, for any incentive compatible mechanism, an equivalent single-dimensional representation for uncertainty. We then characterize the revenue-maximizing and surplus-maximizing equity mechanisms, and compare revenues in optimal equity and cash auctions. Unlike in cash auctions, the adverse selection arising from bidders' two-dimensional types in equity auctions can lead to a global violation of the regularity condition, which represents a maximal mismatch between incentive compatibility and maximization of revenue or surplus. Such mismatch can lead a seller to exclude bidders and demand a bidder-specific stake from a non-excluded bidder, providing insights into when a firm should employ an auction and when it should just negotiate with a single bidder.","A central premise underlying the use of auctions is that bidders have private information that affects both their payoffs and a seller's revenue. In practice, bidder information is often multi-dimensional. Concretely, a bidder may have private information about both the gross revenues that it can generate with an auctioned asset and its opportunity cost. For instance, an acquiring firm in a corporate takeover may be privately informed about its synergy with the target and its own standalone value; and in project-rights auctions bidders are often privately informed about the opportunity costs associated with alternative projects to pursue.====In auctions where bidders pay with cash, if the asset for sale is indivisible, the sole determinant of a bidder's strategy and seller revenues is the bidder's net valuation, i.e., the difference between gross revenues and costs.==== However, when bidders pay with securities that tie payments to the revenues generated (Hansen, 1985, DeMarzo et al., 2005), because the winner retains only a share of revenues but incurs all opportunity costs, the revenue-cost composition affects bidding strategies. Moreover, bidders with different revenue-cost compositions may select the same bid, even though the bid's value to a seller can vary. Thus, the multi-dimensional informational structure affects seller revenue in security-bid auctions in non-trivial ways.====Illustrating the potential consequences, Che and Kim (2010) show that when a bidder's opportunity cost rises deterministically with the expected total cash flow sufficiently quickly, an extreme form of adverse selection arises in standard second-price security-bid auctions: bidders with higher NPVs bid less, resulting in low seller revenues. This suggests that securities auctions should be designed with care when opportunity costs are private information.====We derive the optimal equity auction design when bidders' values and opportunity costs are private information.==== We allow for an arbitrary joint density over the expected cash flows ==== and opportunity cost ==== of a bidder ====, requiring only independence across bidders, mild continuity conditions on the density, and a compact, connected support. Thus, the extent of adverse selection is arbitrary, and can vary across bidders.====We first find a transformation of a bidder's objective that simplifies the resulting envelope condition.==== We show that in any incentive-compatible mechanism, an agent's preferences are determined only by ====, the ratio of his opportunity cost to expected project revenue. This ratio represents the equity stake the agent needs to break even. While an agent's preferences are determined only by this one-dimensional summary of his two-dimensional type, it does not guarantee that two type pairs with the same ==== are pooled in equilibrium. Nonetheless, an application of the envelope theorem reduces the design problem to a one-dimensional problem. It follows that almost all types of a bidder with the same ==== have the same equilibrium probability of winning and the same expected equity share retained—even if project revenues net of opportunity costs are very different for these types.====These results yield a general characterization of any incentive-compatible equity mechanism that we use to analyze revenue-maximizing and surplus-maximizing mechanisms. When a seller seeks to maximize revenues, we use this one-dimensional representation to express a bidder's virtual valuation as a function of ==== only, denoted ====. Different bidder types with the same ==== have the same virtual valuation, which depends on the aggregate properties of all types with that ====. Expected seller revenues decompose into the sum of a component that reflects the virtual valuation of the winning bidder and a component that reflects the rents obtained by a bidder type that needs the highest equity share to break even.====These results yield a general characterization of equity mechanisms, where incentive-compatibility requires the winning probability of all bidders ==== to weakly decrease in ====. We use this characterization to analyze revenue-maximizing and surplus-maximizing mechanisms. When a seller seeks to maximize revenues, we show a bidder's virtual valuation—which represents the rents the seller can extract from the bidder—is a function of ==== only, denoted ====. Different bidder types ==== with the same ==== have the same virtual valuation, which depends on the aggregate properties of all types with that ====.====Under the regularity condition that ==== decreases in the break-even equity stake ====, incentive compatibility and virtual valuations move in the same direction. We show that the resulting optimal selling mechanism has the features that (1) a bidder with the highest virtual valuation wins whenever his virtual valuation exceeds the seller's valuation, and (2) a bidder type with the highest ==== extracts no rents. When the design problem is not regular, a concavification argument similar to that in Myerson (1981) is used to solve for the optimal mechanism. In the optimal design, the winning bidder has the highest adjusted virtual valuation, and a bidder type with the highest ==== still earns no rents.====With two-dimensional uncertainty, the regularity condition in equity auctions becomes demanding. Virtual valuations measure available rents, and thus tend to grow with a bidder's net valuation ====. In cash auctions, the relevant bidder type ==== this net valuation, so the regularity condition, which mandates that virtual valuations increase with bidder type, holds under mild assumptions—a monotone hazard condition on the distribution of types suffices. In contrast, in equity auctions with two-dimensional types, the relevant bidder type is ====. What determines regularity is the monotonicity of virtual valuation with respect to ====, but a lower ==== need not imply a higher net valuation. This is true even when ==== depends deterministically on ====, and more so, due to aggregation, when ==== and ==== are distributed on a two-dimensional space. Indeed, the regularity condition can be violated at ==== ====: virtual valuations can ==== in ====, going in the opposite direction of incentive compatibility over the entire domain. Global violations imply a maximal mismatch between revenue maximization and incentive compatibility. Revenue maximization requires the asset to be allocated to bidders with higher virtual valuations—who have higher ==== when the regularity condition is globally violated—but incentive compatibility demands that bidder types with lower ==== be weakly more likely to win.====If the regularity condition for a bidder is globally violated, the optimal design pools all types so that the winning probability does not vary with type. If such violation occurs for all bidders, it is optimal to identify the bidders with the highest (constant) adjusted virtual valuation and sell to one of them when that adjusted virtual valuation exceeds the asset's value to the seller, demanding the highest share that this bidder would cede regardless of his type. Hence, if bidders are ex-ante identical and the regularity condition is globally violated, then even with multiple bidders, the optimal mechanism is a take-it-or-leave-it offer—expected seller revenue in the optimal design is unaffected by the number of bidders.====This feature contrasts with optimal cash auctions. Consistent with this design feature, Boone and Mulherin (2007) find that mergers and acquisitions involving equity are twice as likely as pure cash acquisitions to have a single bidder.==== If, alternatively, the regularity condition for some, but not all, bidders is globally violated, one can implement the optimal mechanism by first conducting an auction among bidders for whom the regularity condition is not globally violated, setting a reserve price that reflects the value of selling to bidders for whom the regularity condition is globally violated. A seller only turns to bidders exhibiting extreme adverse selection when the virtual valuations of the other bidders are not high enough.====Our analysis provides insights into the forces affecting revenue comparisons of optimal equity and optimal cash auctions. When uncertainty solely concerns valuations, Hansen (1985) shows that equity auctions generate higher revenues because equity bids tie payments to bidder types. When bidders have two-dimensional private information, additional forces come into play. First, with severe adverse selection, where the regularity condition is largely violated, revenues in optimal equity auctions rise minimally with the number of bidders. This favors cash if there are enough bidders. Second, the distribution of net valuations in the one-dimensional representation for equity mechanisms second-order stochastically dominates that for the original two-dimensional distribution (and hence for cash auctions). With few bidders, this favors equity because it lets a seller set the reserve price more efficiently, reducing the risk of no sale. In contrast, with many bidders, what matters primarily is the upper tail of the distribution, especially with substantial two-dimensional uncertainty. Reflecting the combined effects, optimal equity auctions generate more revenues than cash auctions if there are few bidders or moderate levels of two-dimensional uncertainty, while cash auctions do better with extensive two-dimensional uncertainty and many bidders.====Our dimensionality reduction result extends to mechanisms that maximize expected social surplus, where we identify a ‘surplus valuation’ that drives the optimal design. This analysis reveals the inefficiencies of equity auctions relative to cash auctions when bidders have two-dimensional types. With cash, the Vickrey-Clarke-Groves mechanism ensures an efficient allocation, and any pooling in net valuations is suboptimal. In contrast, in equity auctions, the VCG mechanism breaks down due to the constraint that payments are non-cash whose value depends on bidder type. Our findings show that this constraint binds in the sense that sometimes surplus-maximizing mechanisms can feature pooling, failing to achieve the first-best welfare. Indeed, when adverse selection is severe, disregarding all bids and selling to any bidder with the highest adjusted surplus valuation maximizes surplus. This contrast with cash auctions reinforces how in equity auctions with severe adverse selection, incentive compatibility requires “inferior” types (in terms of either virtual or surplus valuations) to win with a weakly higher probability. Thus, a mechanism designer can do no better than randomize.==== The study of equity auctions begins with Hansen (1985). Like Hansen (1985), subsequent papers (e.g., DKS, Che and Kim (2010), Deb and Mishra (2014) or Liu (2016)) assume one-dimensional information for bidders.==== DKS analyze optimal selling mechanisms in standard auction formats when bidders have the same opportunity cost and select bids from a class of ordered securities. They show that steeper securities yield higher seller revenues. Che and Kim (2010) introduce bidders whose opportunity costs rise deterministically with their valuations and show that severe adverse selection can arise that causes steeper securities to generate less revenues in standard auctions. They also uncover how the monotonicity of break-even stakes affects the extent of adverse selection and hence equity auction designs.====In contrast, we consider two-dimensional private information for bidders. We identify a way to reduce the dimensionality that we use to characterize incentive-compatible equity mechanisms, and to identify revenue- and surplus-maximizing mechanisms. We can recover Che and Kim's (2010) framework when the distribution of types has a positive mass only on a one-dimensional subset of the two-dimensional space. However, one cannot just myopically use a one-dimensional formulation: not all two-dimensional distributions reduce to a setting in which ==== is a deterministic function of ====; and even if ==== is deterministically and monotonically related to ====, our dimensionality reduction approach is still needed when the relevant bidder type ==== evolves non-monotonically. Moreover, for a given two-dimensional distribution, equity auctions typically reduce to a different one-dimensional distribution than that for cash auctions. We derive a stochastic dominance relationship between the two distributions, and show how the implications for revenue comparisons of optimal equity and cash auctions depend on both the extent of two-dimensional uncertainty and the number of bidders.====Ekmekci et al. (2016) consider the problem of selling a firm to a ==== buyer who is privately informed about post-sale cash flows and the benefits of control. The offer consists of a menu of tuples of cash-equity mixes, and the bidder must obtain a minimum 50% stake to gain control. They provide sufficient conditions for the optimal mechanism to take the form of a take-it or leave-it offer for either 50% of the firm, or for all shares. The minimum stake introduces a discontinuity in buyer payoffs, which Ekmekci et al. address by characterizing incentive-compatible mechanisms via the exclusion boundary, separating types who obtain controlling stakes from those who do not. In contrast, we examine auctions in which bidders offer equities for full ownership and characterize the optimal auction for multiple buyers.====The study of mechanism design problems when agents have type-dependent opportunity costs (outside options) was initiated in Jullien (2000) and Rochet and Stole (2002), who show that an interior type can have zero surplus. In their models, the value of the opportunity cost is independent of the allocated quantity. In our single unit auction setting, however, this value depends on the allocated quantity: a bidder wins and incurs an opportunity cost, or loses and incurs no cost; and if allocations are probabilistic, the expected opportunity cost is proportional to the expected allocation. More fundamentally, equity complicates design, because the monetary value of payments depends on a bidder's private information about valuations, and private information about opportunity costs introduces further complications.====Lastly, Deneckere and Severinov (2017) consider an agent whose gross utility ==== depends on quantity ==== and privately-known type ====. His net utility in a direct mechanism is==== where ==== is the reported type, and ==== is the principal's revenue. They identify conditions on preferences such that if ==== is optimal for a type ====, and==== for any ==== with ====, then ===='s optimal choice is also ====, reducing the dimensionality. Their set-up relates to a cash-only version of our setting. To see why, let ==== be the bidder's gross valuation and opportunity cost, let ==== be the expected probability of winning, and let ==== be the expected cash payment (unconditional on winning). Then ====, and (2) becomes ====. That is, the relevant bidder type is the net valuation. Equity payments materially alter the framework: net utility becomes==== and seller revenue is ====, where ==== and ==== are the winning probability and the bidder's equity share. Equation (3) cannot be cast in the form of (1), and the form of seller revenue differs, hinging on both the bidder's reported and true (private) type.====These differences highlight how mechanism design involving equity payments with two-dimensional types differ from existing studies of mechanism design with cash transfers, underscoring that different approaches for dimensionality reduction are needed.",Optimal equity auctions with two-dimensional types,https://www.sciencedirect.com/science/article/pii/S0022053118303053,23 August 2019,2019,Research Article,217.0
"Kondor Péter,Zawadowski Adam","London School of Economics, United Kingdom of Great Britain and Northern Ireland,Central European University, Hungary","Received 18 September 2018, Revised 21 May 2019, Accepted 15 August 2019, Available online 23 August 2019, Version of Record 5 September 2019.",https://doi.org/10.1016/j.jet.2019.08.006,Cited by (5),"We present a novel entry-game with endogenous information acquisition to study the welfare effects of opacity and competition. Potential entrants to an opaque market are uncertain about their competitive advantage relative to other investors, i.e. their type. They construct optimal costly signals to learn about their types, where the marginal cost of learning captures the opacity of the market. In general, the individually optimal entry and learning decisions are socially suboptimal. Players over-invest in learning and more opaque markets are associated with more crowding. Nevertheless, more opaque markets might still lead to higher welfare by implying a better trade-off between the degree of crowding and the total cost of learning. Similarly, decreasing the share of smart investors in the market might also improve welfare. However, fierce competition is always detrimental to welfare as it leads to more wasteful learning without changing the level of crowding.","In our rapidly changing global economy, entrepreneurs and financial investors have to be nimble and well prepared to survive. Fleeting investment opportunities arise routinely in global financial markets. Technological developments create new market segments by the day and make existing ones obsolete. It is insufficient to simply recognize new opportunities: The key to success for the potential entrant is to tell whether she was sufficiently early in noticing the opportunity and whether her existing skill sets make her able to compete efficiently. In light of this, it is not surprising that market participants, ranging from venture capitalists, global banks to leading technological companies, invest vast sums into analyzing markets, developing know-how and technology to help them decide whether to undertake investment opportunities. Also, new opportunities are heterogeneous. Some opportunities are in the public eye, leading to a large mass of would-be entrants standing on the sideline considering whether to jump into the fray. Some opportunities are opaque making it costly or even impossible to predict whether the existing skill sets will lead to success or failure. Which kinds of new market segments or trading opportunities are subject to overcrowding? Are the vast resources invested in learning about opaque opportunities socially useful? Should regulators push for transparency? In general, what are the welfare implications of learning about such new investment opportunities in the face of competition and opaqueness?====In this paper, we present a unique, parsimonious framework to answer these questions. We analyze a novel entry-game with endogenous information acquisition to study the welfare effects of opacity and competition. Potential entrants (players hereon) to an opaque market are uncertain about their competitive advantage relative to others, i.e. their type. They construct optimal signals to learn about their types subject to an entropy cost. The opacity of the market is captured by the marginal cost of learning, while the extent of competition is modeled by the mass of players standing on the sidelines. In general, the individually optimal entry and learning decisions are socially suboptimal. Players always over-invest in learning and more opaque markets tend to be more crowded. Nevertheless, transparency is not always optimal as more opaqueness might still lead to higher welfare by discouraging costly learning without excessively increasing crowding. Opaqueness is more likely to be beneficial if competition is not too fierce, while transparency is preferred if competition is excessive. Fiercer competition leads to more wasteful learning leading to deteriorating welfare without affecting crowding.====In our game, players' type is distributed over the unit interval. Each player's pay-off upon entry is a linear function of the mass of entrants with better types, and the mass of entrants with worse types. We focus on the case for which an entrant's revenue is decreased by an additional better entrant and increased by a worse entrant such that players engage in a “rat race”. We also assume that the effect of an additional random entrant (if she is better or worse with equal probability) is also negative, leading to potential “crowding” in the market. Each player constructs an optimal signal structure to learn about her type. Building on Sims, 1998, Sims, 2003, the cost of any given signal structure is proportional to the implied reduction in entropy. Given the equilibrium strategies, a player's posterior on her type implies a posterior on the mass of better and worse entrants. Hence, each player learns about her competitive advantage on the given market relative to other entrants.====Our formalization results in a parsimonious structure. In equilibrium, the optimal information acquisition and entry strategies are reduced to a single function, mapping player's possible types into a probability of entry. For example, if the player were to decide to enter with a given probability independently of her type, we represent this choice with a constant function. This strategy does not require learning. In contrast, if the player were to decide to enter if and only if her type is better than a given threshold, this can be represented by a step function. However, for this, her signal has to be sufficiently precise to know with certainty whether her type is above this threshold. This strategy turns out to be very costly under our specification. In equilibrium, players typically choose an interior strategy represented by a smooth, monotonic function, implying higher entry probability for better types.====Our main result is that whether a regulator should aim for more transparency depends on the level of competition. Typically, full transparency is preferred only if there is a large mass of players aware of the opportunity and ready to enter, i.e. if competition is fierce. Otherwise either an interior level of opacity or full opacity maximizes welfare. The intuition relies on two effects. First, full transparency leads to insufficient entry because worse players do not internalize their beneficial effect on better entrants. On the other hand, full opacity leads to crowding which is more severe if competition is more fierce because players unaware of their types exert a negative externality on other entrants. Therefore, less than full transparency might push entry closer to its efficient level especially when competition is limited. Second, there is costly over-learning in our model because of the rat-race between the players. This leads to a more subtle benefit of increased opacity. Less than full transparency might help to reduce the overall cost of learning. While decreasing transparency, by definition, increases the marginal cost of learning, it might reduce the amount of learning sufficiently that the overall cost decreases. The benefit of this reduction in learning expenditure can more than offset the welfare loss of increased crowding due to more opacity. Thus increasing opacity is more likely to improve welfare if competition is limited because the welfare loss from crowding is less of a concern.====An additional result is that fiercer competition decreases welfare. First we show that unless the mass of players is so small that the entry decision is trivial, increasing competition does not change aggregate entry, which stabilizes at an inefficiently low or high level. To understand this result, note that players adjust their entry decisions along two main dimensions as competition increases. First, the marginal benefit of knowing your type more precisely before entering is increasing in competition because there are more players with a better type, increasing the “rat race” among players. Second, with more players, “crowding” becomes a bigger concern. We show that these two effects have exactly offsetting effects on aggregate entry. Nevertheless, as competition increases, welfare decreases. The key insight is that the “rat race” effect increases with competition, thus players choose to learn more. While ceteris paribus more learning can alleviate inefficient over-entry (as stated above), more learning due to increased competition does not change the amount of entry. Thus the higher learning cost implied by more competition is socially wasteful.====We analyze two extensions. First, we show that when better types find socially more valuable deals in the new market, then more competition often leads to an allocation that is “too efficient” compared to the planner's solution. The reason is similar to that in the baseline model, it is due to over-learning that decreases welfare. Second, we also extend our model to the case in which there is heterogeneity across players: some are more sophisticated and thus can learn at a lower cost than others. Keeping the mass of all players fixed but increasing the share of sophisticated players might also decrease welfare. Initially, increasing the fraction of sophisticated players increases welfare since it raises the average sophistication of players and this can alleviate over-entry. However, further increasing the fraction of sophisticated players beyond a certain threshold, less sophisticated players are afraid of being ripped off and exit the market. Once less sophisticated players exit, sophisticated players engage in a vicious “rat race” of learning which leads to decreasing welfare, similar to the baseline model. Thus like in opacity, in many cases there is an intermediate mix of sophisticated and unsophisticated players that maximizes welfare. Identifying the most sophisticated players as high-frequency traders connects this result to the policy debate on the social benefit of ultra-high frequency trading.====Our main contribution is to study the welfare effects of optimal learning in an entry game with uncertain competitive advantage. Our paper is connected to various branches of literature. First, there is a growing literature on the welfare effects of endogenous information acquisition, e.g. Myatt and Wallace (2012) and Colombo et al. (2014). While this literature focuses on a common-value learning, we analyze an environment when players learn about their relative advantage compared to the other entrants.====Second, from a methodological viewpoint we rely on the rational inattention approach pioneered by Sims, 1998, Sims, 2003. We follow the branch of the literature which allows for fully flexible information acquisition as Matějka and McKay (2015), but restrict ourselves to binary actions similarly to Woodford (2008), Yang, 2015, Yang, forthcoming.====Third, there is a literature analyzing entry/exit in financial markets in the presence of externalities induced by other investors. Stein (2009) introduces a simple model of crowded markets. More generally, there is a classic literature on socially inefficient entry, e.g. Tullock (1967), Krueger (1974), and Loury (1979). We contribute to this literature by introducing a flexible, but costly learning technology and studying its welfare effects. Relatedly, Abreu and Brunnermeier (2003) and Moinas and Pouget (2013) show that the inability to learn about one's relative position versus that of other investors' is a key ingredient in sustaining excessive investment in bubbles. This highlights our contribution in adding learning to a model of crowded markets with potential over-entry.====Finally, there are numerous papers showing excessive investment in learning or effort. There is a literature on the social value of private learning: e.g. in Hirshleifer (1971), private information can be detrimental as it changes ex ante incentives for insurance, in Glode et al. (2012), learning affects ex-post trading opportunities. These papers study welfare effects in markets with asymmetric private information, while in our framework information is imperfect but symmetric.====The rest of the paper is structured as follows. In Section 2 we present our model. In Section 3 we analyze the optimal choice of entry and learning and the effects of opaqueness and competition on crowding and welfare. In Section 4 we consider extensions of the payoff function and also allow for heterogeneity in player sophistication. Section 5 concludes. All proofs are relegated to Appendix A. Further analysis can be found in the online appendices: In Appendix B we analyze median entrants. In Appendix C we give a structural microfoundation for the reduced form model and analyze its economic implications. We analyze Gaussian signals instead of fully flexible learning in Appendix D.",Learning in crowded markets,https://www.sciencedirect.com/science/article/pii/S0022053118303995,23 August 2019,2019,Research Article,218.0
"Bolton Patrick,Wang Neng,Yang Jinqiang","Columbia University, NBER and CEPR, United States of America,Columbia Business School and NBER, United States of America,The School of Finance, Shanghai University of Finance and Economics (SUFE), and Shanghai Institute of International Finance and Economics, China","Received 13 May 2018, Revised 5 July 2018, Accepted 29 June 2019, Available online 12 August 2019, Version of Record 28 August 2019.",https://doi.org/10.1016/j.jet.2019.06.008,Cited by (32), but also ,"Real-options theory following McDonald and Siegel (1986) and Dixit and Pindyck (1994) assumes that firms operate in a Modigliani-Miller (MM) environment. This is for a good reason. The firm's investment decision can then be formulated as an American option-exercise and valuation problem that can be solved by using the classical option pricing tools of Black and Scholes (1973) and Merton (1973).====But, in reality firms operate under imperfect capital markets and face significant external financing costs that arise from informational asymmetry, moral hazard, transaction costs, and other frictions. In order to avoid incurring these costs too frequently firms optimally retain earnings and attempt to mostly internally finance their investments and cover their operating costs. Indeed, according to Chen, Karabarbounis and Neiman (2017), “nowadays nearly two-thirds of global investment is funded by corporate saving.” Also the surveys of chief financial officers (CFOs) by Graham and Harvey, 2001, Graham and Harvey, 2002 have revealed the great importance CFOs attach to maintaining financial flexibility by holding internal funds.====In practice, the value of ==== derived from the optimal timing of real options is intertwined with the value of ==== derived from the optimal management of retained earnings and the optimal external financing timing and issuance decisions. Both the effective discount rate used to determine the value of a growth opportunity and the cost of investment depend on the firm's marginal value of cash, which varies with both the size of the firm's cash holdings and the firm's earnings fundamentals. The reason is that the firm takes account not only of its current ==== of internal funds but also the information about its future cash ==== prospects contained in persistent earnings shocks. Through the firm's evolving marginal value of cash, the real option problem becomes a fundamentally two-dimensional problem, which entails a significant generalization of the classical one-dimensional problems of McDonald and Siegel (1986) and Dixit and Pindyck (1994).====We embark on such a generalization in this paper and derive how optimal investment-timing and abandonment-timing decisions are made, as well as how assets in place and growth options are valued, when firms face external financing costs. ==== By introducing external financing costs, we are, in effect, integrating two strands of literature, the classical real options literature following McDonald and Siegel, 1985, McDonald and Siegel, 1986 and Dixit and Pindyck (1994) with the corporate finance literature following Miller and Orr (1966). Our continuous-time model allows us to not only derive optimal investment, abandonment, and equity issue size and timing policies but also to carry out a systematic valuation analysis.====We model the firm's life-cycle and its evolving collection of assets in place and growth opportunities in the spirit of Myers (1977). Although we do not explicitly model adverse selection, one interpretation of costly external financing in our model is equity dilution cost (due to mis-pricing a la arguments in Myers and Majluf (1984).====In our model, the firm's investment, financing, payout and abandonment policies depend on both earnings fundamentals and the firm's financial slack. Therefore, our model can explain why following a recession low investment persists even though earnings fundamentals have recovered. The reason is that following a recession firms are generally in repair mode, seeking to rebuild their financial slack. Similarly, in our model firms have a preference for investments with front-loaded earnings. We show that, as a result, a start-up firm may choose to invest in a project sooner than predicted by the classical real-option theory in an effort to build future internal funding capacity. We also show that the firm's payout policy is fundamentally different depending on whether it is in the growth phase or in the mature phase. In the mature phase a more profitable firm pays out more, while in the growth phase it pays out less.====Our model also predicts that the value of growth and abandonment options effectively vanishes for firms with low internal financing ability. Firms with scarce internal funds are forced into inefficiently liquidating their valuable operating assets. We also find that the hurdle for investment in the growth phase is a non-monotonic function of the firm's internal funds. When the firm's savings are sufficient to entirely cover the investment cost, the firm's investment hurdle decreases with its internal funds. But when its internal funds cannot entirely cover the cost of investment, the hurdle is increasing with the firm's internal funds. The reason is that when the firm is approaching the point where it may be able to entirely fund its investment with retained earnings it has a stronger and stronger incentive to delay investment until it has sufficient funds to be able to avoid tapping costly external funds. An important implication of this result is that investment is not necessarily more likely when the firm has more cash. Investment could well be delayed further, as the firm's priority becomes avoiding costly external financing.====We also show that financially constrained firms prefer growth opportunities with front-loaded cash-flows. In the MM-based real options framework there is an equivalence between an option that pays cash flows over time and an option that pays immediately upon exercising a lump sum with the same present value. This is not true in our model for a financially constrained firm. Not only is the value of real options with more front-loaded cash-flows higher but also the firm may be induced to over-invest. More generally, a financially constrained firm cares not only about the present value of an investment project but also the project's payback period.====Another subtle prediction is that the amount of external equity financing is also non-monotonic in the profitability of the firm's operating assets. The intuition is as follows. For a firm whose investment option is sufficiently out of the money, financial flexibility has little value. On the other hand, a firm that is so profitable that its investment option is deep in the money can easily generate cash from operations. Therefore, funds obtained through an equity issue are most valuable for a firm whose profitability is in the medium range. In sum, for firms facing external financing costs the value of real options is not just tied to ==== but also to ====. Hugonnier, Malamud and Morellec (2015) consider a model of costly and uncertain external financing in which the amount of external financing may also be non-monotonic in profitability. Décamps, Gryglewicz, Morellec and Villeneuve (2017) obtain a similar prediction in a model with both permanent and transitory shocks.====A real options theory that includes external financing costs is obviously on a substantially stronger empirical footing than the classical theory that assumes perfect capital markets. This is evident with respect to the empirical evidence in Kim and Weisbach (2008) on global seasoned equity offerings (SEOs) and DeAngelo, DeAngelo and Stulz (2010) on SEOs of U.S. industrial corporations. The former paper finds that a large fraction of the SEO proceeds is saved inside the firm and savings are larger for growth firms. The latter finds that SEOs are first and foremost last-resort financing decisions for firms running out of cash, or in need of funds towards anticipated investment outlays. Although they find evidence in support of the market timing hypothesis of Loughran and Ritter, 1995, Loughran and Ritter, 1997 and Baker and Wurgler (2002) on the one hand, and the life-cycle hypothesis on the other,==== these hypotheses do not explain most of the SEOs that are observed. The main conclusion that DeAngelo, DeAngelo and Stulz (2010) draw is that “====” Our theory of SEOs is consistent with findings in both papers. In particular, it can explain why so many “old and young dogs” bark, as it accounts for both offerings for survival and offerings for growth.====A striking new prediction of our theory (from our simulation analysis) is that the equity stake given up to outside investors via an SEO is substantially smaller on average for firms that use the SEO proceeds to invest than for those that do not invest. Such a prediction cannot be obtained in models without persistent productivity shocks.====From a macro perspective, our model sheds light on a major challenge for the real-options based models that seek to explain the persistently low post-crisis investment despite the substantial increase in corporate cash holdings and the quick recovery of earnings and macroeconomic fundamentals. Several studies, in particular, Baker, Bloom and Davis (2016), and Bloom, Floetotto, Jaimovich, Saporta-Eksten and Terry (2016), have pointed out that, although investment timing optionality as in Bloom (2009) is highly relevant for understanding investment dynamics, the standard real options models predict only a short-lived pause in investment caused by higher uncertainty following the crisis.====By construction, the standard real options model also cannot explain the increase in corporate savings. Our model with external financing costs, however, produces both more persistent investment dynamics and cash build-ups following a crisis, consistent with the findings in Alfaro, Bloom and Lin (2016) and Chen (2017). Our model is also consistent with Campello, Graham and Harvey (2010) and Campello, Giambona, Graham and Harvey (2011) who find that the more financially constrained firms planned deeper cuts in investment, spending, burned more cash, drew more credit from banks, and also engaged in more asset sales and forced liquidations in the crisis.==== In sum, our real options model with financial constraints that ties ==== with ==== predicts significantly more plausible investment and cash dynamics.==== Following McDonald and Siegel (1986) the basic formulation of the classical real-options problem has been extended in many different directions. Dixit (1989) uses the real option approach to examine entry and exit from a productive activity. Titman (1985) and Williams (1991) analyze real estate development in a real options framework. Abel and Eberly (1994) analyze a unified framework of investment under uncertainty that integrates the ==== theory of investment with the real options approach.====Mauer and Triantis (1994) considers a real options problem for a levered firm, which otherwise does not face any external financing costs. Hackbarth and Mauer (2012) and Sundaresan, Wang and Yang (2015) study the joint investment and financing decisions by building on the capital structure model of Leland (1994) and integrating it into the real options framework with institutional features. Grenadier (1996), Lambrecht and Perraudin (2003) and others extend the real options decision problem to a game-theoretic environment. Grenadier and Wang (2005) incorporate informational asymmetries and agency problems into a real options framework. Morellec (2004) and Lambrecht and Myers, 2007, Lambrecht and Myers, 2008 consider managerial agency problems in the context of a real options framework. Kyle, Ou-Yang and Xiong (2006) introduce prospect theory and Grenadier and Wang (2007) introduce hyperbolic discounting into the classical real options framework.====In strategic dynamic contexts, Grenadier and Malenko (2011) study games in which the decision to exercise an option is a signal of private information to outsiders. Grenadier, Malenko and Malenko (2016) consider a problem where an uninformed principal makes a timing decision interacting with an informed but biased agent. Orlov, Skrzypacz and Zryumov (2019) study a Bayesian persuasion game in the context of real options. Miao and Wang (2007) analyze an incomplete-markets real-options problem.==== Grenadier and Malenko (2010) develop a model of real options with learning about the permanence of shocks.====None of these models discussed above allow for financial flexibility through the accumulation of corporate savings. More recently, two related articles have introduced financial flexibility into a real options problem, but they only consider one-dimensional problems by removing earnings fundamentals as a state variable. The first is by Décamps and Villeneuve (2007) and the second is by Hugonnier, Malamud and Morellec (2015). Both articles consider a financially constrained firm with an asset in place that generates cash-flows with (transitory) ==== shocks and that faces a growth option that can increase the drift of the cash-flow process. Put differently, in the MM version of these models there is no value of waiting and hence there is no real flexibility: the firm would exercise its growth option as soon as it is available.====Thus, our main contribution is to consider a two-dimensional problem that lends itself to a general analysis of the value of real and financial flexibility. Our paper is not the first to consider a two-dimensional real-options problem, but it is the first to offer a complete and rigorous analysis of this problem. The first article to consider a two-dimensional real options problem for a financially constrained firm is by Boyle and Guthrie (2003). They assume that the firm cannot invest at all unless it meets an exogenous borrowing constraint. Once the investment is made the firm is immediately liquidated at the market value of the asset. The main point of their analysis is to highlight an over-investment incentive, which is driven by the risk of (locally) unbounded large losses from the operating assets in place, and the firm's concern that it might be short of cash before it can invest. This concern is contrived as it arises from the ad hoc assumption that the firm is not allowed to voluntarily downsize its operating asset.==== That is, they do not account for the firm's abandonment option, which is an important part of our analysis. Unlike their setup, our model fully captures both real and financial flexibility by separating financing and investment option decisions. The firm's financing needs reflect the joint considerations for investment, continued operations, and survival. Moreover, the firm continues to face operating risk after investment and therefore also faces an abandonment option.====Anderson and Carverhill (2012) study a liquidity management problem for a firm operating over an exogenously fixed time horizon and that is subject to mean-reverting productivity shocks. Besides these different model specifications the other fundamental difference with our analysis is that their model has a one-time investment opportunity at an exogenously fixed time implying no investment-timing flexibility.====Three independent studies analyze a similar problem to ours: First, Copeland and Lyasoff (2013) consider a somewhat narrower framework to ours and do not allow for either abandonment or sequential growth options. Second, Boot and Vladimirov (2019) consider a financially constrained entrepreneurial firm with an asset in place that generates random cash flows following a geometric Brownian motion and a new investment opportunity. Third, Babenko and Tserlukevich (2013) consider the optimal hedging policy for a financially constrained firm with a decreasing returns to scale technology and growth opportunities.====Our paper is also related to Décamps, Mariotti, Rochet and Villeneuve (2011), who consider a financially constrained firm's optimal dynamic payout and SEO policies, and Bolton, Chen and Wang, 2011, Bolton, Chen and Wang, 2013, who develop a ====-theory of investment for financially constrained firms facing ==== shocks. One important difference with the generalized ====-theory of investment (with convex adjustment costs) is that the financially constrained firm issues equity only for survival and never for investment as it uses its internal savings to smooth investment, and hence cannot explain the above mentioned empirical evidence on SEOs. Bolton, Wang and Yang (2019) use the recursive optimal contracting method to develop a dynamic liquidity and risk management model also with real investment options for a firm run by an entrepreneur with inalienable human capital.",Investment under uncertainty with financial constraints,https://www.sciencedirect.com/science/article/pii/S002205311830173X,12 August 2019,2019,Research Article,219.0
"Laohakunakorn Krittanai,Levy Gilat,Razin Ronny","LSE, United Kingdom of Great Britain and Northern Ireland","Received 14 May 2018, Revised 23 April 2019, Accepted 2 August 2019, Available online 12 August 2019, Version of Record 28 August 2019.",https://doi.org/10.1016/j.jet.2019.08.002,Cited by (7),"We analyze auctions when individuals have ambiguity over the joint information structures generating the valuations and signals of players. We analyze how two standard auction effects interact with the ambiguity of bidders over correlation structures. First, a “competition effect” arises when different beliefs about the correlation between bidders' valuations imply different likelihoods of facing competitive bids. Second, a “winner's value effect” arises when different beliefs imply different inferences about the winner's value. In the private values case, only the first effect exists and this implies that the distribution of bids first order stochastically dominates the distribution of bids in the absence of ambiguity. In common value auctions both effects exist and we show that compared to the canonical model, both in the first-price and second-price auctions, these effects combine to imply that the seller's revenue decreases with ambiguity (in contrast with the private values case). We then characterise the optimal auction in both the private and common value cases. A novel feature that arises in the optimal mechanism in the common values case is that the seller only partially insures the high type against ambiguity.","In auctions, as in many other strategic situations, individuals often have a good understanding of their own private information but they might know less about others' information sources. For example, in auctions for drilling rights, a company can understand the test results that it conducted but might be worried that its results are correlated with those of other firms. Similarly the evaluation one gets about a piece of art on sale might be correlated in complex ways to the evaluations other bidders get.====In these situations bidders might worry about their lack of understanding of the correlation structure between their own information and that of other bidders.==== These considerations are important for their bidding behaviour. In private value auctions, beliefs about correlation influence the assessment of bidders about the competition they might face. In common value auctions, such beliefs also affect the bidders' valuation of the good, which implies an additional effect on the bidders' strategies.====In this paper we analyze private and common value auctions when individuals have ambiguity over the joint information structures generating valuations and signals. Specifically, we assume that individuals know the marginal information structure generating a value or a signal to each bidder, but that they are aware that their information sources might be correlated to a degree, and face ambiguity over the possible correlation scenarios. We propose a simple model to analyze ambiguity over correlation structures that is tailored to the comparison with the standard model.==== In particular, we use a single parameter, ====, to bound the degree of ==== of the information structures.==== When an individual receives a signal and contemplates what strategy to play, she faces ambiguity aversion (as in Gilboa and Schmeidler, 1989) about the set of potential joint information structures that are bounded by ====.====Specifically, we analyze two-bidder, binary-value and binary-signal auctions (in Section 5 we extend the analysis to models with continuous signals and with many bidders). In this model, the set of information structures that the bidders consider is centered around the canonical case of independent values and signals, and takes the following form. Consider the two bidders, 1 and 2, with values ==== each receiving a signal, ==== and ==== respectively. Let ==== denote a joint probability of the signal vector ==== conditional on the valuations for the good, and ==== and ==== denote the marginal probabilities of ==== and ==== conditional on ====. Similarly, let ==== denote the prior over ====, with ==== the marginal prior. We assume that individuals' ambiguity is over the set of joint information structures satisfying ==== for some finite parameter ====, for any ====.==== In other words, the (exponential) pointwise mutual information (ePMI) is bounded. The higher is ====, the larger is the ambiguity. When ==== we are back in the standard model with conditionally independent signals and no ambiguity. This framework allows us to analyze ambiguity in the private value auction (where we focus on ambiguity over ====), and in the common value auction (where we focus on ambiguity over ====).====In the model, ambiguity over information structures is exogenous but ambiguity over the valuations or the equilibrium bids of opponents is endogenous, and depends on the strategic interaction. An ambiguity averse bidder focuses attention on beliefs that minimise her expected utility. This will be reflected in beliefs that either put weight on more competition, believing the other bidders have similar valuations, or on a low value of the good. The former effect, the “competition effect”, is present in both private and common value auctions. The second effect, a “winner's value effect”, exists only in common value auctions.====We analyze the equilibria in the second-price and first-price auctions. We first focus on the competition effect. We show how this effect implies overbidding in the private values case; the distribution of bids in the first price auction in the face of ambiguity first-order stochastically dominates that in the standard model. This implies that ambiguity is beneficial for the seller as her revenue increases with ====. In contrast, the “winner's value effect” generally implies that bidders should shade their bids. We show that in common-value auctions, in which both effects are present, the “winner's value effect” is stronger and the seller's revenue decreases in ambiguity. In particular, the low type always shades her bids while the high type does so unless ambiguity is small and signals too imprecise. In Section 5 we show that this result (and the intuition behind it) also holds when we consider an environment with many bidders, as well as an environment with continuous signals.====We then turn to consider optimal auction design in the face of ambiguity over correlation structures. We first show that the above results about the seller's revenue also hold for the optimal auction; the seller's optimal revenue increases in ambiguity in the private values case but decreases in the common value case. In the private values case our results are consistent with Bose et al. (2006) who show that the optimal mechanism fully insures the bidders against ambiguity. In Bose et al. (2006) bidders believe that valuations are independently drawn, but face ambiguity regarding the particular distributions of valuations that others have. In contrast, in our model bidders know the marginal distribution of valuations but face ambiguity about the joint distribution of valuations. Nevertheless, the full insurance result holds in our setting when values are private.====The full insurance result implies that the competition effect does not arise in equilibrium in the optimal auction. However, there is a sense in which the seller exploits the competition effect to increase her revenue. Since under the worst case belief the high type believes that her opponent is likely to also be a high type, she is less willing to deviate than when there is no ambiguity, so that the seller is able to extract more rent from the high type. Thus, the seller's revenue increases in ambiguity.====In the common value case, insuring the buyers against ambiguity is not necessarily optimal. In the standard case without ambiguity, any optimal mechanism must allocate the good with probability 1 and fully extract rent. However, any such mechanism cannot be a full insurance mechanism. To prevent the high type from deviating, the seller has to conduct side bets with the low type. Under ambiguity, these side bets are costly since the expected payment to the seller is smaller under the seller's belief than under the worst case belief of the buyer. However, if ambiguity is small or if the signals are very precise, these side bets remain optimal and the low type is not insured against ambiguity.====Furthermore, even though there is no need to conduct side bets with the high type to satisfy any incentive constraint, it is also not necessarily optimal to fully insure the high type. This is because to fully insure the high type, the allocation rule needs to be independent of the type of her opponent. Otherwise, the winner's value effect implies that the high type will underestimate the value of the object. However, when the ambiguity is small or the signals are precise, the seller will allocate the good to the high type with higher probability when her opponent is a low type in order to slacken the incentive constraint. In this case, the seller provides partial insurance by also asking the high type to pay more when her opponent is low type. Under the worst case belief, the high type does not care about the type of her opponent, but undervalues the object conditional on winning. In other words, the seller insures the buyer against the competition effect but not the winner's value effect.====On the other hand if the signals are very imprecise and the ambiguity large, the seller finds it optimal to fully insure the buyers against the ambiguity, so that the allocation of the good does not depend on their signals. As a result, the high type earns positive rents in equilibrium. Finally, we show that the seller's revenue in the optimal mechanism is decreasing in the amount of ambiguity, as we found in both the first and second price auctions.====Our paper is related to a recent literature on ambiguity and auctions. As far as we know, our paper is the first to analyse ambiguity in common-value auctions. For private-value auctions, Salo and Weber (1995) and Chen et al. (2007) show how ambiguity aversion translates to higher bids as individuals underestimate their winning probabilities.==== We complement their analysis by considering the common-values case and the comparison with the private-values case, both positively and normatively. Bose et al. (2006) analyze optimal auction mechanisms for private-value auctions with ambiguity over other bidders' valuations. They show that the seller will fully insure the buyers against ambiguity. Again, our key contribution is to analyze the common-values case and compare it to the private-values case. Specifically, we show that in the common value auction sometimes only partial insurance arises. Lo (1998) shows that the first-price auction dominates the second-price auction in some environments. He uses a multiple priors approach and shows that equilibrium bids are simply determined as if all players hold the worst-case prior. In our analysis players with different signals focus on different beliefs so the model is not equivalent to one in which players start for example from a mis-specified model with wrong beliefs.====Bose and Renou (2014) study how principals can use ambiguous mechanisms to implement social welfare functions that are not attainable under unambiguous mechanisms. In particular, they construct ambiguous communication mechanisms between the agents and a moderator resulting with agents updating to sets of beliefs. Hanany et al. (2018) is a recent contribution to the study of incomplete information games and ambiguity. Finally, Bergemann et al. (2015) consider private values auctions and study the set of achievable utilities when considering, as modelers, the set of different feasible information structures. Our analysis is different as in our approach it is the economic agents, rather than the modeler, who span the possible information structures. In addition, we restrict the set of possible information structures using the notion of pointwise mutual information. We show how this shifts equilibrium behaviour in a non-trivial way.",Private and common value auctions with ambiguity over correlation,https://www.sciencedirect.com/science/article/pii/S0022053118301765,12 August 2019,2019,Research Article,220.0
"Jadbabaie Ali,Kakhbod Ali","Institute for Data, Systems, and Society (IDSS), MIT, Cambridge, MA, 02139, USA,Department of Economics, MIT, Cambridge, MA, 02142, USA","Received 3 July 2017, Revised 5 April 2019, Accepted 31 July 2019, Available online 9 August 2019, Version of Record 14 August 2019.",https://doi.org/10.1016/j.jet.2019.07.017,Cited by (18),"We study optimal contracting between a firm selling a divisible good that exhibits positive externality and a group of agents in a social network. The extent of externality that each agent receives from the consumption of neighboring agents is privately held and is unknown to the firm. By explicitly characterizing the optimal multilateral contract, we demonstrate how inefficiency in an agent's trade propagates through the network and creates unequal and network-dependent downward distortion in other agents' trades. Furthermore, we describe bilateral contracts (non-linear pricing schemes) and characterize their explicit dependence on the network structure. We show that the firm will benefit from uncertainty in an agent's valuation of other agents' externality. We describe the profit gap between multilateral and bilateral contracts and analyze the consequences of the explicit dependence of the contracts on network structure. When the network is balanced in terms of homogeneity of agents' influence, network structure has no impact on the firm's profit for bilateral contracts. On the other hand, when the influences are heterogeneous with high dispersion (as in core-periphery networks) the restriction to bilateral contracts can result in profit losses that grow unbounded with the size of networks.","Over the past decade, there has been a flurry of research on economics of social networks and the monetization of social data. From display advertising to viral marketing, firms have been trying to use consumers' social interactions to increase their sales revenue, leading to exponential growth in revenue for online social networking platforms. While most revenue generated from social media accrues through advertising, there has been a major push to devise intelligent strategies for pricing products that exhibit network effects. Information about goods and services often spreads in networks as “word of mouth”, while in other instances, the product itself has features that induce positive network effects. Examples include Nike+, the technology that tracks data from every run and connects runners around the world to online music services such as Apple Music and Spotify. In all these examples, firms can utilize a positive externality to sell more goods and services, since the choices of friends and acquaintances influence each consumer's decisions.====In many instances firms have data on the consumption of the products/services they sell, as well as the social network activity of their consumers. For example, online social-networking communities, such as Facebook, Twitter, Instagram, and Pinterest, allow firms to target users based on their social interactions. Some companies also provide services to firms based on information on aggregate network effects obtained by quantitative analyses of consumers' online behavior (e.g., Klout, Commun.it, Cloze.com and Kred). Firms expend major effort to exploit underlying network effects and forces in order to maximize profit. However, a major difficulty in using this information is the uncertainty as to how much agents ==== the externalities. An externality's value is often the private information of the agents. As an example, an agent may become aware of opinions or experiences of her friends about their usage of a specific good or service via her social interactions, but how much she cares (i.e., the extent of her attention) about their opinions is often her private information.====A monopoly firm selling a divisible good that demonstrates network effects naturally faces the following questions: How should firms incorporate knowledge of their customers' underlying social-network structure into their selling strategies when information on network effects is incomplete? Is uncertainty between agents about network effects beneficial to the firm? Does explicit knowledge of the network structure always matter in firms' profit-maximizing strategies?====To address these questions, we study optimal contracting (multilateral and bilateral) between a firm and a network of agents (consumers) with two distinguishing features: First, the positive externality that agents receive from their neighbors' consumption is captured by the underlying social structure that is often incomplete (i.e., not an all-to-all graph), with different agents in the network have varying degrees of influence.==== Second, and more importantly, the extent of network externality to each agent is privately held, unknown to the firm and other agents. Our aim is to understand how network structure affects optimal contracts and to investigate the nature of resulting distortions and inefficiencies in principal-agent(s) problems, when the above two features are both present.====The goal of this paper is to study optimal contracting in networks when there is incomplete information about the strength of network effects. A major twist in our model, and a point of departure from the existing literature, is that the impact of aggregate network effect (externalities) to each agent is her private information which we interpret as the agent's ====. A network's structure and the uncertainty in the strength of network effects are essential features of our analysis, and lead to the following implications.",Optimal contracting in networks,https://www.sciencedirect.com/science/article/pii/S002205311930081X,9 August 2019,2019,Research Article,221.0
"Aguiar Mark,Amador Manuel","Princeton University, USA,Federal Reserve Bank of Minneapolis and University of Minnesota, USA","Received 5 February 2019, Revised 24 July 2019, Accepted 2 August 2019, Available online 8 August 2019, Version of Record 13 August 2019.",https://doi.org/10.1016/j.jet.2019.08.005,Cited by (17),"Using a dual representation, we show that the Markov equilibria of the one-period-bond ==== incomplete markets sovereign debt model can be represented as a fixed point of a contraction mapping, providing a new proof of the uniqueness and existence of equilibrium in the benchmark sovereign debt model. The arguments can be extended to incorporate re-entry probabilities after default when the shock process is ====. Our representation of the equilibrium bears many similarities to an optimal contracting problem. We use this to argue that commitment to budget rules has no value to a benevolent government. We show how the introduction of long-term bonds breaks the link to the constrained planning problem.","This paper provides a compact characterization of the canonical Eaton and Gersovitz (1981) model of sovereign debt with one-period defaultable, but otherwise noncontingent, bonds. In particular, we show that the Markov equilibrium allocation can be characterized in a single Bellman equation that is the fixed point of a contraction mapping. This immediately yields existence and uniqueness, providing an alternative approach to the recent results of Auclert and Rognlie (2016).====The fact that the equilibrium is the solution to a Bellman equation also sheds light on the economics of the equilibrium. The dynamic programming problem shares many similarities with an optimal contracting problem between a principal and an agent that is risk averse and lacks commitment. In this sense, the equilibrium allocation has a number of efficiency properties. However, the contracting problem includes a constraint that reflects market incompleteness, highlighting the inefficiencies that arise due to the ad-hoc constraints on contracts imposed in the Eaton-Gersovitz tradition.====The argument has two crucial steps. The first is to consider a dual representation of the equilibrium. Specifically, we consider maximizing the value of debt subject to delivering a level of utility to the government. This yields a function ====, which specifies the value of debt when the exogenous state is ==== and the “promised utility” of the government is ====. The choice variables in this program are government consumption (or net payments to the lender) in the current state as well as continuation values in successor periods, ==== for all ==== in some state space ====. The optimization is subject to the government's lack of commitment.====At this point, the description is a textbook principal-agent problem with complete markets and one-sided limited commitment. Hence, the second step of the argument is to ensure that the optimal allocation is consistent with incomplete markets. In particular, the value of debt next period cannot vary across state realizations in which the government repays. This requires that ==== is invariant across all ==== in which the government repays. This is reflected in an additional constraint that features the representative lender's value function.====The equilibrium is then characterized by an operator that maps the space of possible debt values, ====, into itself. Differently from a primal approach, our dual representation allows us to show that the function ==== is the fixed point of a contraction operator. This immediately implies uniqueness of the equilibrium and can be used to show its existence. Uniqueness has recently been established by Auclert and Rognlie (2016), who provide a clever proof of uniqueness based on a replication argument similar to Bulow and Rogoff (1989b). One contribution of this paper is to establish uniqueness using standard recursive techniques that are taught in first-year graduate courses.====As a consequence of the dual-contracting approach, the analysis highlights certain efficiency properties of the equilibrium. The ability to commit to future debt issuances cannot improve upon equilibrium allocation, and thus fiscal rules have no value. In particular, aside from the default decision, there is no loss or gain if the government, as part of the contract, cedes future fiscal deficit choices to the lenders. There is also no coordination failure that can lead to a sub-optimal outcome, highlighting the difference between the Eaton-Gersovitz model and the closely related model of Cole and Kehoe (2000) that does feature self-fulfilling crises. The different timing assumptions in the Cole-Kehoe model are not consistent with our dual formulation.====The use of the dual-contracting approach and the efficiency properties of short-term bonds has a precedent in Aguiar et al. (2019b). That paper demonstrates how short-term financing is efficient in a sovereign debt model where uncertainty affects only the value of default (that is, the only shocks are those that affect the outside option of the government) and consumption is deterministic absent default. A contribution of the present paper is to extend this efficiency result to the standard incomplete markets environment where uncertainty affects the endowment and the equilibrium consumption process. This requires the introduction of a new constraint that restricts how continuation values can vary by state.====The efficiency of short-term bonds stands in contrast to models with long-term bonds.==== This distinction is made transparent in our dual formulation. The relevant objective for the representative lender is the ==== of debt. The relevant constraint from incomplete markets is that the ==== of debt is noncontingent. Conditional on nondefault, with one-period bonds, the market value equals the face value at the time of repayment; hence, the same function ==== characterizes both the value and the constraint set, and the program searches for a single object that satisfies the dual Bellman equation. With long-term bonds, this is no longer the case. This provides a stark reflection of the fact that long-term bonds are subject to dilution (that is, disagreement about fiscal policy between the lender and government) and can induce multiple equilibria in the Eaton-Gersovitz model (see, for example, Aguiar et al. (2018) and Stangebye (2018)).====The environment we study hews closely to the canonical one-period Eaton-Gersovitz models popular in the quantitative literature, such as Aguiar and Gopinath (2006) and Arellano (2008). Following the original Eaton-Gersovitz paper, we assume there is no reentry after default, but do allow for arbitrary additional punishments. This makes the deviation utilities primitives of the environment rather than equilibrium objects. The results can be extended to allow for reentry under ==== endowment shocks, but we do not have results for general Markov processes.====In addition to long-maturity bonds, other deviations from the standard one-period-bond Eaton-Gersovitz model can lead to multiplicity. Lorenzoni and Werning (2018) consider an environment in the spirit of Calvo (1988) where a government following a fiscal rule is vulnerable to self-fulfilling shifts in the interest rate of its debt. Ayres et al. (2018) consider an environment in which lenders offer the sovereign an interest rate before the sovereign commits to a borrowing amount and show how this auction protocol generates multiplicity with one-period bonds.==== This highlights the role played by the Eaton-Gersovitz assumption that the government is strategic in regard to the impact debt issuance has on prices.====Passadore and Xandri (2018) show that multiplicity can arise in the Eaton-Gersovitz model if the government is prevented from holding assets. We therefore place no bounds on the space of assets. To ensure that ==== is bounded, we restrict that consumption lies in an arbitrarily large but compact set; this places a de facto upper bound on the government's value achieved in any equilibrium and hence ensures our equilibrium operator maps bounded functions into bounded functions. As is standard, the government has time-consistent preferences. Therefore, the analysis does not extend to models in which the government has quasi-geometric preferences, such as Aguiar et al. (2011) or Alfaro and Kanczuk (2017).====The paper is organized as follows. Section 2 introduces the environment and provides a basic characterization of equilibria. Section 3 shows that the equilibrium is a fixed point of a contraction mapping. Section 4 discusses the efficiency of the unique equilibrium and why fiscal rules are not useful. We also discuss why introducing long-term bonds breaks the usefulness of our dual approach. Section 6 shows how the analysis can be extended to the case of reentry after default (as long as the shock process is ====). Section 7 concludes. Appendix A collects all of the remaining proofs not included in the main text.",A contraction for sovereign debt models,https://www.sciencedirect.com/science/article/pii/S0022053119300857,8 August 2019,2019,Research Article,222.0
"Gama Adriana,Rietzke David","Centro de Estudios Económicos, El Colegio de México, Mexico,Department of Economics, Lancaster University, United Kingdom","Received 13 June 2017, Revised 29 March 2019, Accepted 2 August 2019, Available online 7 August 2019, Version of Record 12 August 2019.",https://doi.org/10.1016/j.jet.2019.08.004,Cited by (2),"In this paper, we employ lattice-theoretic techniques to derive a number of comparative statics in a logit contest – a class of games for which best-replies are generically non-monotonic. Using the same approach, we obtain several comparative statics in a ","Thanks to works such as Topkis (1978), Vives, 1990, Vives, 1999, and Milgrom and Roberts (1990), lattice-theoretic techniques are used extensively in the economics literature. These tools allow researchers to obtain comparative statics results without the strong assumptions needed to apply the Implicit Function Theorem. Although these tools have proved extremely useful, their use in games has mainly been restricted to those with monotonic best-replies. Thus far, this has precluded their application in important classes of games – such as contests – which simply ==== be games of strategic complements/substitutes (Acemoglu and Jensen, 2013). Other games, including Cournot oligopoly, require assumptions on demand and/or costs that ensure globally monotonic best-replies (see Vives, 1999, for an overview). These assumptions rule out important classes of inverse demand curves frequently used in applications, such as hyperbolic demand.====In this paper, we show how lattice-theoretic tools (developed originally only for games of strategic complements/substitutes) are useful more generally. We first apply these tools in a (symmetric) contest under the logit contest success function (CSF). Contests are one important class of models that cannot yield monotone best-replies, and for which the existing literature (e.g. Dixit, 1987; Nti, 1997) has relied exclusively on the Implicit Function Theorem. We show that by focusing on subsets of the strategy space, the tools from supermodular games/games of strategic complementarities can be applied to obtain a strong regularity property on the best-response correspondence. This allows us to derive a number of comparative statics, which hold whenever a pure-strategy equilibrium exists. We then illustrate how our approach proves useful in other games that have non-monotonic best replies. To do so, we examine a Cournot oligopoly model. We obtain several comparative statics without relying on the Implicit Function Theorem or imposing the usual assumptions (e.g., log-concavity of inverse demand) that guarantee monotonic best-replies (see, e.g., Amir, 1996; Vives, 1999, Vives, 2005).====The logit CSF generalizes the model popularized by Tullock (1980), and is one of the most commonly used CSFs in the literature.==== The regularity property we establish on the best-response extends a finding from Dixit (1987), while our comparative statics extend/clarify results in Nti (1997), and complement Vives (2005), Acemoglu and Jensen (2013) and Jensen (2016). Dixit (1987) and Nti (1997) impose decreasing returns to scale on the contest technology, and invoke the Implicit Function Theorem. Contrasting Nti's (1997) results, when the contest technology exhibits increasing returns to scale, we show that equilibrium payoffs may be strictly decreasing in the value of the prize, and that equilibrium aggregate activity may be decreasing in the number of players. Vives (2005) studies a patent race model, which relates closely to the logit-contest. He shows that under very mild conditions, the game is strictly log-supermodular, thus yielding upward sloping best replies. The logit-contest, in contrast, generally has non-monotonic best-replies, and requires a different approach. Acemoglu and Jensen (2013) and Jensen (2016) allow asymmetries between players and exploit tools from aggregative games.====We also provide conditions that ensure existence of a pure-strategy equilibrium in the logit contest. For these results, we exploit an equivalence with the Cournot oligopoly model, and apply findings from Amir (1996) and Amir and Lambson (2000). These results complement the analyses of Szidarovszky and Okuguchi (1997), Cornes and Hartley (2005), Yamakazi (2008), Acemoglu and Jensen (2013) and Jensen (2016). These studies consider asymmetric contests, but impose stronger assumptions on the returns-to-scale of the contest technology.",Monotone comparative statics in games with non-monotonic best-replies: Contests and Cournot oligopoly,https://www.sciencedirect.com/science/article/pii/S0022053119300845,7 August 2019,2019,Research Article,223.0
Sotomayor Marilda,"EPGE Brazilian School of Economics and Finance, Brazil,Department of Economics of the University of São Paulo/SP, Brazil","Received 14 September 2018, Revised 12 July 2019, Accepted 29 July 2019, Available online 6 August 2019, Version of Record 14 August 2019.",https://doi.org/10.1016/j.jet.2019.07.013,Cited by (1),"We replicate the “Multiple-partners game” of ==== to yield a sequence with infinitely many terms. Each term, with more than one stage, is endowed with a structure of subgames given by the previous terms. The concepts of sequential stability and of perfect competitive equilibrium are introduced and characterized. We show that there is a subsequence, such that, for all its terms, these concepts, as well as the traditional concepts of stability and of competitive equilibrium, lead to the same set of allocations, which may be distinct of the core. This not always hold for the terms out of that subsequence.","This work introduces a general framework that fits well for the class of cooperative games, called ==== (multi-stage games, for short). Each stage configures a sub-game, where new participants are added, so that the whole set of players is only formed at the final stage. The new entrants have perfect information on the game-stage they are playing and can interact with the current players. Consequently, the coalitional interactions that take place in a given stage-game may be undone in a subsequent stage-game.====The novelty is that, instead of an equilibrium analysis supported on the usual static cooperative game structure, it is also taken into account the multi-stage game structure with which this model is endowed. The concept of the ==== then emerges, distinct and stronger than the traditional cooperative equilibrium concept:==== ==== ====.====This general model encompasses the class of the ====, which is the name given here to the multi-stage games that have a finite number of types of players and may have infinitely many players of each type.==== A sequential game can be regarded as one of the terms of some sequence ==== generated by some finite cooperative game ====. Any finite cooperative game ==== generates a sequence of sequential games. The first term ==== is the sequential game with one player of each type and the final term ====# is the sequential game with infinitely many players of each type. The ====-th term ==== of the sequence is obtained by replicating ==== times the ====; ====#, is the infinite union of all terms of the sequence. In this context, any sequential game ==== is also a ==== of the sequential game ====, for all ====; any finite sequential game is a ==== of the ====#, with ==== (====# is a subgame of itself and ==== is a subgame of itself, for any positive integer ====).====The idea behind the ==== for a sequential game is analogous to that behind a subgame perfect equilibrium of a non-cooperative dynamic game: ====.====Cooperative games, which are also endowed with a competitive market game structure, generate sequential games with a structure of competitive market game with multiples stages. For this structure, the idea of the “perfect” competitive equilibrium allocation emerges as the natural solution concept. Roughly speaking, if a competitive equilibrium allocation is not perfect for a given sequential game ====, then there is a subgame ==== of ====, and an agent, present in this subgame, who is not able to reach her current payoff allocation at the given prices, by adopting a price taker behavior in ====. In other words, a ====.====This paper intends to illustrate the use of the sequential games to represent two-sided continuous matching markets, whose traditional treatment, via a static game, seems to be unsatisfactory.====Two-sided continuous matching models have been used in the literature as acceptable approximations to buyer-seller markets (or labor markets of firms and workers). In these markets, cooperation is done through pairwise trades, between a buyer and a seller (or a firm and a worker), and the presence of other agents provides outside options for each trading pair, which presumably affect the terms of trade within the partnerships (Shapley and Shubik (1972), Crawford and Knoer (1981), Kelso and Crawford (1982), Demange and Gale (1985), Sotomayor, 1992, Sotomayor, 2002, Sotomayor, 2013, among others).====The sequential games, generated by a two-sided continuous matching market ====, are called here ==== (extended markets, for short). The present work is addressed to the study of the extended markets for the sequence ====(====), whose ==== is the Multiple-partners game, introduced in Sotomayor (1992). In this game, every player has a quota, representing the maximum number of partnerships he/she/it can enter and no one is allowed to form more than one partnership with the same partner. This model provides both approaches, the cooperative and the competitive. To fix ideas we will think of it as a buyer-seller market, where each seller has a finite set of identical indivisible goods to sell and each buyer can acquire a set of objects, from distinct sellers, of size less than or equal to her quota.==== The transactions are accomplished pairwisely and independently and the utility functions are additively separable. An allocation specifies a matching (a set of partnerships that respects the quotas of the players) and a payoff allocation (an array of individual payoffs for each player, resulting from the division, among the partners, of the income yielded within each partnership formed).====For the Multiple-partners game, we distinguish four distinct solution sets: the core, the set of stable allocations, the set of competitive equilibrium allocations and the set of non-discriminatory (the individual payoffs of each agent are the same in all transactions done) stable allocations. One set is a super-set of the other and these set inclusion relations may be strict. The first three sets are non-empty (Sotomayor, 1992, Sotomayor, 2007) and the last one may be empty, as it is proved in the text (Proposition 4.1). The cooperative equilibrium concept is captured by the notion of stability (Sotomayor, 2016), which, in this model, is identified with the pairwise-stability concept (Sotomayor, 1992). The competitive equilibrium allocations are characterized as ==== (====) (Sotomayor, 2007).====We are interested in exploring the main ideas of competition and cooperation among the agents. More specifically, we would like to know:====The definitions above show some relation between these concepts. Nevertheless, an equilibrium analysis that establishes nothing more than that multi-stage cooperative equilibria are cooperative equilibria and perfect competitive equilibria are competitive is not likely to tell us much. It is also desirable that it might be able to identify====To answer these questions, we provide a game theoretical analysis, based on several different solutional viewpoints, which involves six solution sets: the core, the set of stable allocations (which characterizes the set of cooperative equilibrium allocations of these games), the set of competitive equilibrium allocations, the set of non-discriminatory stable allocations, the set of perfect competitive equilibrium allocations and the set of sequentially stable allocations, defined below.====The sequential stability concept comes out as a refinement of the concept of stability, but distinct and stronger than that concept (Example 3.2, Example 3.3). In addition, it captures the intuitive idea of multi-stage cooperative equilibrium for the extended markets. Roughly speaking, ====.====Thus, at a sequentially stable allocation for a given term of the sequence of extended markets, the players who are present ====.====In the same vein, the perfect competitive equilibrium concept for the multi-stage extended markets provides a refinement of the concept of competitive equilibrium, which is distinct and stronger than this concept: ====.====We start our analysis by establishing the connection between the solution sets mentioned above. Our purpose is to analyze, not only, the market games with finitely and infinitely many agents, but also those markets which do not configure as a large market. For such purpose, the interesting case is when the extended markets are the terms of a subsequence ==== of ====(====), starting at the term ====, where ==== is the largest quota of the agents. Although the large extended markets are in ====, ==== is not necessarily a large market, so we may have extended markets in ==== that do not fit as large markets.====The answer to question A can be concluded from two equivalence theorems. The first one (Theorem 3.1) implies that ==== is ====, in the sense that this equivalence holds in ==== and it is preserved in all the subsequent terms. Therefore, all terms of the subsequence ==== are stable with respect to this equivalence.====Indeed, our result is more general and proves that, as population grows large, the sets of stable and competitive equilibrium allocations shrink to the set of non-discriminatory stable allocations and stay distinct of the core. In other words, the concepts of stability, of competitive equilibrium allocation and of non-discriminatory stability lead to the same set of allocations in all extended markets of ==== (this does not necessarily happen in the other extended markets), which may be a proper subset of the core (Proposition 3.1). Therefore, the competitive equilibrium allocations may not be the only corewise-stable allocations in these markets, and the core equivalence principle, which relates the core of a perfectly competitive market to its competitive equilibria, may not hold for some large extended markets of ====. This is in opposition to which is suggested by the result of Debreu and Scarf (1963) for exchange economies, and by that of Aumann (1964) for exchange economies with a continuum of agents, where in a large enough economy the only allocations that are immune to deviations by groups of agents are competitive equilibrium allocations (see some discussion on this subject in section 3).====The second equivalence theorem (Theorem 3.2) establishes the equivalence between the concepts of cooperative equilibrium and competitive equilibrium for the terms of the subsequence ====, viewed as sequential games: ==== ==== ==== (this is not necessarily true for the other extended markets). Thus, the competitive equilibrium allocation yielded in an extended market in ====, when the agents behave as price takers in every subgame, can also occur in a different scenery, without reference to the prices, where the agents adopt a cooperative optimal behavior in every subgame, and vice-versa.====The answer to question ==== is given by the following characterization of the sequentially stable allocations: ==== ==== ==== ==== (this property is not shared with all non-discriminatory stable payoffs of the extended markets of ====). In addition, ==== (Theorem 4.2).====Such characterization is obtained through three results. The first one is general and identifies ====. This result (Theorem 4.1) asserts that ==== (Theorem 4.1). That is, these payoff allocations extend to stable payoffs of any multi-stage extended market and they are the only feasible payoffs of ==== with such property. Therefore, in case the set of non-discriminatory stable allocations of ==== is empty, we must have that none of the stable payoffs of a multi-stage extended market is the extension of a corewise-stable allocation of the initial market. In particular, no competitive equilibrium allocation of a multi-stage extended market is the extension of a competitive equilibrium allocation of the initial market. In addition, none of the stable payoffs of an extended market (other than ====) results from the replications of a payoff allocation, compatible with a feasible matching for the initial market.====Clearly, the extension of any optimal matching of ==== to an extended market is compatible with the extension of any non-discriminatory stable payoff of ==== to that market (since every stable allocation for ==== is compatible with any optimal matching – Sotomayor (1999)). Thus, ==== (non-discriminatory stable payoff allocation plus a compatible matching) ==== ==== ====. However, the converse of this assertion is not always true. In fact, an example in the text (Example 3.2) presents a situation where the set of non-discriminatory stable payoffs of ==== is non-empty and some non-discriminatory stable payoff of the asymptotically extended market is the extension of an unfeasible payoff of ====; furthermore, that stable payoff is not compatible with the extension of any feasible matching of ====. Then, the corresponding stable allocation is not the extension of any feasible allocation of ====.====The key lemma (Lemma 4.1) for the proofs of the other two results asserts that ==== – all agents of the same type receive the same payoff vector. In particular, this lemma holds for the stable allocations of ==== ====. (An example illustrates that this property does not hold for every corewise-stable allocation, as it happens in the exchange economy defined in Debreu and Scarf (1963).) It follows straightforwardly from this lemma that ==== ==== ==== (====) ==== (Proposition 4.2). Such payoff allocation of the initial market is naturally defined from the restriction of the given stable payoff to the market ====. Consequently, since the sequentially stable payoffs of an extended market of ====, restricted to ====, are stable, by definition, and non-discriminatory, by the first equivalence result, these payoff allocations must be extensions of non-discriminatory stable payoffs of the market ==== (Proposition 4.3). The desired characterization is then derived from these three results together.====Some consequence of this characterization is that ==== ==== ====. Then, for the markets ====, in which the set of non-discriminatory stable payoffs is empty, the ==== (Theorem 5.1). As for the non-emptiness of the other solution sets for an extended market in ====, the set of stable allocations and, equivalently, the set of competitive equilibrium allocations are always non-empty, and so does the set of corewise-stable allocations (Theorem 5.2).====This paper is organized as follows. In section 2, we present the framework and preliminaries for the multiple-partners game and its extensions by replication. Section 3 is devoted to the solution sets and the equivalence results for the terms of ====. Section 4 has two subsections. Subsection 4.1 presents the characterization of the feasible allocations of the initial market that extend to stable allocations of the extended markets. Subsection 4.2 characterizes the set of sequentially stable allocations for the extended markets in ====. The emptiness and non-emptiness of the solution sets are discussed in section 5. Section 6 concludes the paper.",Competition and cooperation in a two-sided matching market with replication,https://www.sciencedirect.com/science/article/pii/S0022053118305726,6 August 2019,2019,Research Article,224.0
Bich Philippe,"Université Paris 1 Panthéon Sorbonne, UMR 8074 Centre d'Economie de la Sorbonne, Paris School of Economics, France","Received 21 October 2016, Revised 22 July 2019, Accepted 1 August 2019, Available online 6 August 2019, Version of Record 12 August 2019.",https://doi.org/10.1016/j.jet.2019.08.001,Cited by (2),"We introduce the new concept of prudent equilibrium to model strategic uncertainty, and prove it exists in large classes of discontinuous games. When the game is better-reply secure, we show that prudent equilibrium refines ====. In contrast with the current literature, we don't use probabilities to model players' strategies and beliefs about other players' strategies. We provide examples (first-price auctions, location game, Nash demand game, etc.) where prudent equilibrium concept removes most non-intuitive solutions of the game.","Consider a first-price sealed-bid auction with complete information between two bidders. The players are characterized by their valuation ==== and ==== of the item for sale, ====, and they are supposed to choose bids ==== and ==== in ====, ====. Assume that in case of ties, i.e. if ====, then the winner is the player with the highest value. An easy computation proves that for every ====, the strategy profile ==== is a Nash equilibrium of this strategic game. Yet, for ====, these equilibria represent fragile situations, because of ====: if player 2 does not respect his equilibrium strategy and decreases slightly his bid, then player 1 gets the item for a price ==== higher than his valuation ====. As a matter of fact, any strategy ==== is also a best-reply of player 1 if player 2 plays ====, but it is also immune to a small modification of player 2's strategy. Thus, if player 2 is supposed to play ====, and if he predicts that his opponent should play ====, then he could be tempted to lower his bid ==== in order to increase his payoff. Finally, playing ==== for player 1 seems definitely a bad choice, even if the other player is assumed to play the same strategy.====This example illustrates that Nash equilibrium concept has sometimes to be refined in order to keep some predictive power, and the same idea can be found in many other situations (e.g., Nash demand game, location game, Bertrand duopoly, etc. see Section 4). Note that the existence of a pure Nash equilibrium in the previous game can be obtained from Reny's theorem (Reny, 1999), which guarantees existence for the large class of better-reply secure games. This class encompasses many discontinuous==== economic games, in particular the first-price auction described above. Astonishingly, there is no refinement notion of pure strategy Nash equilibrium for discontinuous games which covers this example, or more generally the class of better-reply secure games. A possible reason is that most refinement notions - like perfect equilibrium of Selten (1975) - require the existence of equilibria of some auxiliary games, where the players' strategies are perturbed by random mistakes, and payoffs are expected payoffs. If the initial game is better-reply secure, such auxiliary games are in general neither better-reply secure nor quasiconcave, thus no general existence result in pure strategies can be applied to them.====In this paper, we introduce a new refinement of pure strategy Nash equilibrium in discontinuous games, called ====. We prove its existence (Theorem 15) for the class of p-robust games, which contains most discontinuous economic games. Roughly, a game is p-robust if for every strategy profile ====, no player, when anticipating the worst possible local modifications of other players' strategies, can effect a large change in his payoff by a small change in his strategy.====For example, the first-price auction above can be easily proved to be p-robust (see Example 7): at every strategy profile ==== with ====, if player 1 increases his strategy a little bit, player 2 can answer by the same modification, so that player 1 does not increase his payoff.====We now provide an informal definition of our main solution concept, prudent equilibrium. The main issue in the introductory example is strategic uncertainty, i.e. the uncertainty related to other players' strategies and rationality (see Brandenburger, 1993). A radical way to remove strategic uncertainty in games would be to consider extremely prudent players, who try to maximize ==== with respect to their strategy ====, ==== being the initial payoff function of player ====, and ==== the strategy sets of his opponents. A less extreme answer would be to assume that given player i's belief about the potential strategy profile ==== of his opponents, he has good reasons to think that their true strategies will stay in some set ==== (for example a small ball centered at ====). Then, a prudent player would choose his strategy ==== in order to maximize ====. This function can also be written as==== where====In the infimum above, note that only the strategies ==== for which ==== is equal to 0 are useful, and the other ones, for which ==== is infinite, can be removed. In short, ==== is a measure for player ==== of the unlikelihood of a potential deviation ==== from the expected strategy profile ====.====A natural generalization leads to the definition of the following auxiliary “prudent” game==== where ====, and ==== is some normalization coefficient. The function ==== could be interpreted as a (free) insurance paid to player ==== if the other players −==== play ==== instead of the expected strategy ====, and ==== would parametrize the insurance level. By analogy with the previous case, the function ==== is also a way to parametrize strategic uncertainty: ==== is the strategy expected by player ====, and ==== means that the possible deviation ==== has more payoff importance for player ==== than ==== has.====Interestingly, the prudent behavior described above has a smoothing effect on the initial game: in general, the prudent payoff functions ==== are more regular than the initial payoff functions ====. This smoothing effect implies that for every p-robust game ====, and for a large class of functions ==== (for example distances), there exists a Nash equilibrium of the prudent game associated to ==== (Theorem 14). This opens a route for refinement, and indeed, we prove that if the initial game ==== is better-reply secure, and if the level of insurance ==== tends to +∞, then any limit point of Nash equilibria of the prudent games is a Nash equilibrium of ==== (Theorem 15). We call such a limit point a prudent equilibrium: for example, in the first-price auction above, the only prudent equilibrium is the intuitive solution ==== (see Proposition 29).====Our definition of a prudent game should be compared to variational preferences, introduced by Maccheroni et al. (2006) to model uncertainty aversion in decision theory. Recall that variational preferences on the set of acts ==== are represented by==== where ==== is a utility function, ==== an act, Δ the set of priors over a state space ====, and ==== an index of uncertainty aversion. The interpretation by Maccheroni et al. is the following. When the decision maker contemplates choosing an act ====, the malevolent Nature tries to minimize its expected utility. Any prior ==== can be chosen, but Nature must pay a cost ==== to do so. In their setting, this cost is also interpreted as an ambiguity index.====Our model adapts==== some of these ideas to a strategic setting. In particular, ambiguity is turned into strategic uncertainty. But a major difference is that variational preferences are valid in a probabilistic setting, although we consider only a deterministic framework. Also, the cost ==== in our model depends on the potential strategy profile ==== of −====, which plays the role of ==== in variational preferences, but also on the strategy profile ==== expected by ====.====Many other papers have tried to model strategic uncertainty in games. In quantal-response equilibrium models, pioneered by McKelvey and Palfrey (1995), strategic uncertainty is represented by a probability distribution (some noise) added to the initial payoff of each player, which defines a perturbed game. For every mixed strategy profile ====, every player ==== acts optimally in the perturbed game against ====. This induces another probability distribution over the observed actions of the players. If this probability distribution is ====, it is, by definition, a quantal-response equilibrium. In a similar vein, Andersson et al. (2014) consider that players choose pure strategies, and strategic uncertainty is now represented through probabilistic subjective beliefs about the strategies of each player's opponents. As above, this defines an equilibrium notion in some auxiliary noisy game. Remark that both approaches are related to refinement literature (see Selten, 1975 or Myerson, 1978), and as a matter of fact, when the level of noise tends to zero, they provide refinement concepts.====Non-additive models are an alternative to model strategic uncertainty. If strategic uncertainty is represented by a set of priors, then the preferences of each players can be defined through Choquet expected utility model (see Mukerji, 1995, Marinacci, 2000, Ryan, 2002, or Eichberger and Kelsey, 2000 who also model optimism or pessimism in strategic games), or through Gilboa-Schmeidler maximin model (Klibanoff, 1996, Dow and Werlang, 1994, Lo, 1998 or De Marco and Romaniello, 2013). Most of the papers above differ in their definition of the support for the beliefs. Recently, Renou and Schlag (2010) have proposed a dual model based on minimax behavior: in their approach, regret guides players in forming probabilistic assessments and, ultimately, in making choices.====The main difference between our model and these models is that beliefs about the strategies of the other players are not represented by sets of priors, but by deterministic functions. It turns out to be a very tractable approach in many cases, even when the initial game is discontinuous (see Section 4), and it has several interpretations.====Last, it should be added that prudent equilibrium shares a common feature with most previous refinement notions: it does not pretend to select the reasonable outcome in all familiar games. Indeed, in general, it is always possible to find a strategic game for which a given refinement concept is ineffective. But we think that (1) our notion is complementary to the previous ones, (2) it helps to remove non intuitive solutions for many discontinuous or continuous games, as illustrated in the paper (see Section 4 for many examples, and also Section 3, which proves that dominated strategies - in some specific sense - cannot be played at a prudent equilibrium), and (3) it is not difficult to compute for many simple games. A specific feature of prudent equilibrium in pure strategies is that the strategic uncertainty is local (contrarily to trembling-hand equilibrium concept, which considers that every strategy could be played by mistake - i.e. out of equilibrium - with some probability). According to us, it could be an interesting feature, since it only requires that agents examine local mistakes. But clearly, it implies that we do not encompass previous refinement notions for every game. This is a price to pay to be able to get an existence result in pure strategies for large and simple classes of discontinuous games. Another important feature of prudent equilibrium is that it is possible (though not automatic) that the choice of the functions ==== that parametrize strategic uncertainty influences the prudent equilibrium.==== This is not surprising: indeed, ==== models strategic uncertainty in a local way, and it could influence the selection of the Nash equilibrium of the initial game. In general, the choice of ==== could depend on the context of the game. Anyway, we provide examples for which the prudent equilibrium does not depend on ==== (see Examples 26 or 46).====The paper is organized as follows. Section 2 introduces prudent games, prudent equilibrium, and finally p-robust games, the general class of (possibly discontinuous) games for which we can prove the existence of a prudent equilibrium. In Section 3 is stated the main existence result, and also the fact that prudent equilibrium refines Nash equilibrium in better-reply secure games. Then, the new concept of prudent-dominance is introduced, and it is proved that prudent equilibria rule out Nash equilibrium profiles such that the strategy of one player is prudent-dominated. Section 4 provides examples, Section 5 concludes the paper, and finally, some possible extensions (for example when the game possesses enough symmetry), together with some proofs, are proposed in the appendix.",Strategic uncertainty and equilibrium selection in discontinuous games,https://www.sciencedirect.com/science/article/pii/S0022053119300821,6 August 2019,2019,Research Article,225.0
"Chemla Gilles,Hennessy Christopher A.","Imperial College Business School, DRM/CNRS, and CEPR, United Kingdom,London Business School, CEPR, and ECGI, United Kingdom","Received 22 April 2018, Revised 29 June 2019, Accepted 30 July 2019, Available online 6 August 2019, Version of Record 11 September 2019.",https://doi.org/10.1016/j.jet.2019.07.016,Cited by (7),"We develop a formal model of placebo effects. If subjects in seemingly-ideal single-stage RCTs update beliefs about breakthroughs based upon personal physiological responses, mental effects differ across medications received, treatment versus control. Consequently, the average cross-arm health difference becomes a biased estimator. Constructively, we show: bias can be altered through choice of control; higher-efficacy controls mitigate upward bias; and efficacy states can be revealed through controls of intermediate efficacy or controls that mimic a subset of efficacy states. Consistent with experimental evidence, our theory implies outcomes within-arm and cross-arm differences can be non-monotone in treatment probability. Finally, we develop novel differences-in-differences and triangle equality tests to detect RCT bias.","A critical stated objective in medical trials is to produce an unbiased estimate of the non-placebo physiological effect of a treatment, also known in the medical literature as ==== or ====. Since Fisher (1935), the double-blind randomized controlled trial (RCT below) has been viewed as the gold standard in eliminating placebo effects and isolating non-placebo physiological effects. In describing the rise of RCTs in medicine in ====, Kaptchuk (1998) notes, “The greater the placebo's power, the more necessity there was for the masked RCT itself.” In the U.S., E.U. and Japan, the gold standard status of RCTs is codified under the International Conference on Harmonization of Technical Requirements for Registration of Pharmaceuticals for Human Use (ICH, 2000). The ICH writes, “Control groups have one major purpose: to allow discrimination of patient outcomes [...] caused by the test treatment from outcomes caused by other factors, such as the natural progression of the disease, observer or patient expectations, or other treatment.” In fact, the perceived reliability of the RCT has caused the methodology to be emulated in other disciplines. For example, in their influential textbook, ====, Angrist and Pischke (2009) hold up the RCT as the ideal for achieving unbiased estimates of causal effects.====Given its practical importance, as well as its contemporary methodological influence, it is worthwhile to revisit the logical foundations of the double-blind medical RCT. In the traditional statistical argument for the RCT, health quality is assumed to be the sum of a direct non-placebo physiological effect plus a brain-modulated physiological (“mental” or “placebo”) effect.==== Since subjects are randomly assigned to treatment and control groups, with blind assignment, the mental effects are assumed to be identically distributed random variables, having probability distributions independent of the assigned group. Under these assumptions, the difference between the average treatment and control group health quality yields an unbiased estimate of the expectation of the non-placebo physiological effect.====The medical literature's informal theoretical account of mental effects is known as ====. Stewart-Williams and Podd (2004) state, “On the expectancy account, the effects of such factors come through their influence on the placebo recipient's expectancies.” In this spirit, in perhaps the first known controlled medical trial, Haygarth (1801) wrote that his study, “clearly prove[d] what wonderful effects the passions of hope and faith, excited by mere imagination, can produce on disease.”====Departing from purely statistical treatments, as well as informal articulations of expectancy theory found in the medical and psychiatry literatures, we posit that agents manifest better present-day health quality in response to expectations of better health quality in future periods arising from a higher probability of a medical breakthrough. For example, expectation of better future health can reduce anxiety, improving outcomes today for subjects suffering from ulcers or hypercholesterolemia. Similarly, expectation of higher future survival rates can alleviate the severe anxiety associated with life-threatening diseases, with relaxation, rest and sleep improving measured health quality today. Similarly, expectation of better future mental health can mitigate feelings of hopelessness and anxiety, thus reducing depression today.====Indeed, the psychiatry literature, with its ====, has informally articulated the mechanism central in our model, without apparently understanding the implications for RCTs. MacLeod et al. (1991) argue that: “Worry is a cognitive phenomenon, it is concerned with future events where there is uncertainty about the outcome, the future being thought about is a negative one, and this is accompanied by feelings of anxiety.” In turn, anxiety reduction may represent one source of mental effects. For example, in the context of pain treatment, Turner et al. (1994) conjecture that “A patient's expectation that treatment will relieve symptoms may reduce anxiety and thus ameliorate symptoms.”====In a stylized way, our model mimics a seemingly-ideal medical RCT. Specifically, we consider the testing of a new medication with an unobservable efficacy state that is either high or low. Test subjects enter the trial holding subjective prior probability assessments which may or may not be equivalent to objective probabilities.==== Subjects are randomly assigned to take the new medication or a control. After taking their assigned medication, but before health quality is measured, subjects privately observe their respective direct physiological state and then revise their beliefs regarding the efficacy state using either Bayesian or subjective non-Bayesian updating as in Epstein (2006) and Epstein et al. (2008). Measured health quality at the end of the RCT is the sum of the direct physiological state plus a brain-modulated physiological effect which is a monotone increasing function of expected future health quality. In turn, expected future health quality is monotone increasing in the subject's posterior probability assessment of the high efficacy state.====As shown, in such environments, ====. In particular, beliefs about the efficacy state of a novel medical treatment will vary systematically with the objective probability distributions governing the direct physiological responses induced by the treatment and control medications. Unless the objective probability distributions of direct physiological responses are equal across the treatment and control medications, beliefs regarding efficacy will generally differ across groups leading to differences in hope-based mental effects (expectancy). Therefore, the difference in average health outcomes across treatment and control groups generally delivers a biased estimate of the conditional expectation of the non-placebo physiological effect.====We first characterize how bias varies with choice of control. We initially assume priors have the monotone likelihood-ratio property (MLRP) while objective probabilities satisfy first-order stochastic dominance (FOSD) conditions. It is shown that an unscrupulous drug manufacturer can create upward bias by using a stochastically dominated control. Conversely, a conservative regulator can ensure that bias is downward by using a stochastically dominating control. Controls of intermediate efficacy create upward (downward) bias in the high (low) efficacy state. Finally, despite existence of bias, a positive result obtained under these technical conditions is that the treatment-control difference is positive (negative) if the treatment generates higher (lower) mean health than the control. That is, here the treatment-control difference correctly ranks medications in terms of mean health outcomes.====We then relax the MLRP and FOSD assumptions. It is shown that here the treatment-control difference need not rank medications correctly: a positive (negative) treatment-control difference can occur even though the treatment is no better (no worse) than the control in terms of mean health effects. Nevertheless, other constructive results emerge here. For example, the unconditional expectation of bias is zero if the objective probability density for the control mimics the objective unconditional density of health outcomes under the treatment. Further, bias can be eliminated in one of the two efficacy states by using a control mimicking that state's objective probability density. Finally, with such a mimicking control, a non-zero treatment-control difference identifies the true efficacy state as being the non-mimicked state.====We next develop a novel differences-in-differences test for RCT bias and for our posited control medication effect. In particular, if RCTs are unbiased, then the treatment-control difference should fall one-for-one with each increase in the mean health outcome associated with different controls. In contrast, we predict that when more effective controls are used in RCTs, the treatment-control difference will fall more than one-for-one with the control's mean outcome. That is, the difference between RCT differences should exceed the difference between control medication effects. If the objective effects of the two control medications are not known, a third RCT comparing the two controls must be conducted. In the absence of RCT bias, the following triangle equality will hold: The difference in RCT outcomes between the novel treatment and control 1 is equal to the difference between the novel treatment and control 2 plus the difference between control 2 and control 1.====We also analyze the role played by treatment probability in altering the bias. To begin, we show bias approaches zero as treatment probability approaches zero.==== Of course, concern over standard errors and eliciting participation would rule out infinitesimal treatment probabilities in practice. We therefore derive technical conditions under which bias is increasing in treatment probability. These monotonicity results can be seen as supporting the notion that smaller treatment groups are bias-reducing. However, the conditions for bias-monotonicity are restrictive, and we show that bias magnitude can locally decrease with treatment probability.====Finally, we derive testable implications regarding the effect of varying treatment probability. Here our model has the potential to explain experimental evidence at odds with the “canonical theory” of placebo effects. The canonical theory predicts better mental effects arise from beliefs that one is receiving the treatment rather than the control. Stewart-Williams and Podd (2004) describe the “archetypal placebo event” as follows: “A physician gives a patient a pill that, unbeknownst to the patient, is merely a sugar pill... Presently, the patient's health improves, apparently because of the belief that the pill was a pharmacological agent, effective for the condition.” Consistent with the canonical theory, Malani (2006) writes, “patients in the higher-probability [of treatment] trial will expect better health outcomes from their clinical trial, all other things being equal.”====We contrast testable implications. First, both theories predict outcomes within each trial arm should vary with treatment probability. Second, in the canonical placebo theory, health outcomes within each trial arm are predicted to increase with treatment probability. In this spirit, Chassang et al. (2015) offer antidepressants as a motivating example, writing “participants treated with probability ==== (1/1 odds) will expect more social anxiety than participants treated with probability ==== (3/1 odds).”==== In contrast, our model formally predicts negative responses to higher treatment probabilities if low-efficacy medications are administered, since subjects then assess a lower probability of a breakthrough. Finally, the canonical theory predicts health outcomes across arms should increase at equal rates, with cross-arm mental effects canceling, implying zero bias. In contrast, our proposed theory predicts unequal response rates, implying RCT bias.====In a pioneering paper, Malani (2006) tests these three hypotheses in a sample of medical RCTs with varying treatment probabilities for subjects suffering from ulcers or hypercholesterolemia. He finds that health outcomes do indeed vary with treatment probabilities. However, he finds some evidence of negative treatment-arm responses to increases in treatment probability, an observation inconsistent with the canonical theory. Further, Malani documents that treatment and control arms exhibit unequal rates of response to changes in treatment probability, contradicting the canonical theory but supporting our central prediction of bias in medical RCTs.====Chassang et al. (2012) analyze RCTs in settings with hidden types or actions. They consider the normal form of an abstract signal structure. In contrast, we consider the extensive form of a specific signal technology and provide a detailed analytical treatment of how bias is affected by changes in control medication (and treatment probabilities). Relatedly, they show how mechanism-like extensions can improve on RCTs. In contrast, we attempt to improve on RCTs through bias-reducing or state-revealing control medications.====Philipson and Desimone (1997) also stress limitations on RCTs. In their model, bias arises via differential attrition in multi-stage trials, there is a single efficacy state for the novel drug, and the health signal is binary.==== Our framework shows the problem of RCT bias is much more severe than their analysis suggests. First, as we show, with placebo effects, bias can even occur in single-stage trials where attrition is impossible. Second, if there is more than one efficacy state for the novel drug, as is the case in our model, and as must be the case in practice if an RCT is actually resolving uncertainty, the RCT must be biased in at least one efficacy state. That is, at best, bias elimination is state-contingent. Finally, with binary health signals, the sufficient condition for no bias is that treatment and control have ====. In a more realistic environment in which health varies along a continuum, the analog sufficient condition for no-bias is much more-demanding: equality of treatment and control ====. In addition, with continuous health signals, differences in density functions can change ordinal rankings of medications in a way that depends upon the shape of mental effect functions, in the same way that changes in security payoff functions change preferences over payoff densities.====The mental effect in our model operates through subject beliefs regarding the promise of ==== health, which is positively correlated with the true efficacy state of the novel therapy being tested. We posit that less anxiety about future health leads to better health quality during the experiment. In this sense, the underlying causal channel in our model is related to the anticipatory expected utility framework formalized by Caplin and Leahy (2001). Specifically, the two models share the notion that “anxiety is anticipatory,” with our model going on to consider concomitant health quality feedback effects.====Our paper contributes to the medical literature on RCTs. Existing critical examinations of medical RCTs have emphasized practical difficulties in their implementation. Bothwell and Podolsky (2016) provide an historical account of RCTs. Rothwell, 2005, Rothwell, 2006 provides excellent critical surveys. There is also a large medical literature examining the role of expectations in influencing health outcomes. Regarding the potentially beneficial effects of greater hope, Di Blasi et al. (2001) perform a meta-analysis of context effects in ====. They note, “Three of these studies showed that enhancing patients' expectations through positive information about the treatment or the illness, while providing support or reassurance, significantly influenced health outcomes.” Consistent with the notion that anxiety reduction leads to better health outcomes, Thomas (1987) offers empirical evidence that physicians offering to patients a more positive prognosis, holding fixed the nature of treatment, leads to reductions in reported symptoms. In addition, Shapiro and Shapiro (1984) offer evidence that placebo effects are more powerful in more anxious patients.","Controls, belief updating, and bias in medical RCTs",https://www.sciencedirect.com/science/article/pii/S0022053119300808,6 August 2019,2019,Research Article,226.0
Prokopovych Pavlo,"Kyiv School of Economics, 92-94 Dmytrivska, Kyiv 01135, Ukraine,Department of Economics, Tippie College of Business, University of Iowa, Iowa City, IA 52242-1994, USA","Received 10 October 2018, Revised 21 May 2019, Accepted 29 July 2019, Available online 5 August 2019, Version of Record 28 August 2019.",https://doi.org/10.1016/j.jet.2019.07.012,Cited by (9),"This paper develops a new approach to investigating equilibrium existence in first-price auctions with many asymmetric bidders whose types are affiliated and valuations are interdependent and not necessarily strictly increasing in own type. We begin with studying a number of continuity-related properties of the model, which are used, in conjunction with tieless single crossing and ====-convexity, to establish the existence of monotone approximate interim equilibria. Then we provide two sets of sufficient conditions for the game to have a sequence of monotone approximate equilibria whose limit points are pure-strategy Bayesian-Nash equilibria.","In Bayesian games where the players' best-reply correspondences are nonempty-valued, upper semicontinuous, and contractible-valued, Eilenberg-Montgomery's (1946) fixed-point theorem can be of help in establishing the existence of monotone Bayesian-Nash equilibria (Reny, 2011). If the payoff functions are not continuous in actions, values of the best-reply correspondences might be empty, and one has to study either finite-action approximations of the game or its approximate best-reply correspondences.====Finite-action approximations of games have served as an important technique for establishing equilibrium existence for quite a while.==== In normal-form games, the interest in using such approximations has somewhat faded since the introduction of the notion of a better-reply secure game by Reny (1999).==== However, the technique has retained its relevance in the Bayesian game framework so far. Using finite-action approximations, Athey (2001) investigated the existence of a monotone Bayesian-Nash equilibrium in games with incomplete information. Within the framework of Athey's (2001) approach, Reny and Zamir (2004) carried out a detailed study of an asymmetric first-price auction with affiliated private information and interdependent values.====The approach we employ to examine equilibrium existence in first-price auctions relies on studying the existence of approximate equilibria. If no pure-strategy equilibrium exists in a Bayesian game, both Athey's (2001) finite-action technique and the lattice-theoretic approach (see, e.g., Vives, 1999, Vives, 2005; Amir, 2005, Van Zandt and Vives, 2007) are powerless. A possible way to handle such situations is to turn to approximate equilibria==== For example, one of the assumptions customarily made to ensure the existence of a pure-strategy Bayesian-Nash equilibrium in first-price auctions is that the bidders' valuations are strictly increasing in own type (see, e.g., Athey, 2001, Maskin and Riley, 2003; and Reny and Zamir, 2004). If it does not hold, a Bayesian-Nash equilibrium can fail to exist (see, e.g., Lebrun, 1996, p. 422). However, this assumption is not needed for the existence of approximate equilibria. Consequently, the problem of existence of a pure-strategy Bayesian-Nash equilibrium is reduced to developing sufficient conditions for the limit point of a convergent sequence of ====-equilibria, when ==== tends to 0, to be a Bayesian-Nash equilibrium.====This paper's results concerning approximate equilibrium existence rest on two cornerstones: on a number of continuity-related properties of the game and on the ====-convexity of each bidder's set of nondecreasing strategies. Among the continuity-related properties of the auction game studied in this paper are: (i) the transfer lower semicontinuity of each interim payoff function in variables different from own bids (interim payoff security, in other terminology); and (ii) the continuity of the interim value and ex-ante value functions. These properties are employed to establish that, in the class of monotone strategies, every approximate best-reply correspondence has the local intersection property; that is, it has a multivalued selection with open lower sections.====Since, in our setting, values of the approximate best-reply correspondences need be neither convex-valued, nor contractible-valued, nor even closed-valued, it might be problematic to use any Kakutani-type fixed-point theorem. Following Athey (2001), McAdams (2003), and Reny (2011), we employ a kind of generalized convexity as an alternative for convexity; namely, each bidder's set of nondecreasing strategies is interpreted as an ====-space, a space comprising contractible families of strategies. Then the approximate best-reply correspondences, whose values consist of nondecreasing approximate interim best replies, have ====-convex values. Consequently, as shown in Theorem 2, the existence of monotone approximate interim equilibria in the game follows from Horvath's (1987) fixed-point theorem, a generalization of Browder's (1968) fixed-point theorem.====If, in addition, the bidders' payoffs for every vector of types are aggregate upper semicontinuous in bids, then every limit point of a sequence of monotone interim ====-equilibria, with ==== tending to 0, is a Bayesian-Nash equilibrium (Theorem 3). Another condition guaranteeing the existence of a convergent sequence of ex-ante ====-equilibria whose limit point is a Bayesian-Nash equilibrium of the game is that each bidder's valuation is strictly increasing in own type (Theorem 4).====We illustrate the proposed equilibrium existence conditions with a number of examples. Example 1, Example 2 illustrate the fact that no pure-strategy Bayesian-Nash equilibrium might exist in a first-price auction if bidders' valuations are not strictly increasing in own type. In such cases, it is natural to turn to studying approximate Bayesian-Nash equilibria. Example 3 describes another problem that might appear when bidders' valuation are not strictly increasing in own type, namely, the absence of Bayesian-Nash equilibria in strictly increasing strategies, which itself can considerably complicate investigation of such games. Example 4, Example 5 are common-value first-price auctions in which the bidders' valuations are not strictly increasing in own type and the existence of a monotone pure-strategy Bayesian-Nash equilibrium follows from Theorem 3, but not from Athey's (2001) and Reny-Zamir's (2004) results. Example 6 explains some subtleties of choosing an appropriate sequence of monotone approximate equilibria when the bidders' valuations are strictly increasing in own type. Example 7 is a private-value first-price auction with subsidies.====The structure of the paper is as follows. Section 2 contains the model and some theoretical underpinnings necessary for studying equilibrium existence in the Bayesian game. A number of important continuity-related properties of the interim payoff and value functions are investigated in Section 3. In Section 4, we provide sufficient conditions for every approximate interim best-reply correspondence to have a nondecreasing single-valued selection. The existence of monotone interim ====-equilibria and Bayesian-Nash equilibria in the first-price auction is studied in Section 5. The section also contains examples illustrating the equilibrium existence conditions. A number of proofs are relegated to the Appendix.",On monotone approximate and exact equilibria of an asymmetric first-price auction with affiliated private information,https://www.sciencedirect.com/science/article/pii/S0022053118306501,5 August 2019,2019,Research Article,227.0
Beauchêne D.,"MAPP Economics, 2, Square de l'Opéra-Louis-Jouvet, 75009 Paris, France","Received 28 June 2017, Revised 7 May 2019, Accepted 29 July 2019, Available online 2 August 2019, Version of Record 22 August 2019.",https://doi.org/10.1016/j.jet.2019.07.015,Cited by (3),"Not necessarily. I consider two firms that compete both on the research market and on the product market. Assuming the product market outcome is ambiguous, ====, uncertainty is described by multiple distribution of outcomes, I show that an increase in firms' ==== aversion increases the ==== likelihood that they invest in R&D. An increase in their ","Microsoft and Sony consider including motion-sensing controllers with their gaming consoles.==== Apple and Samsung consider introducing a voice recognition digital assistant to their smartphones.==== Michelin and Continental consider equipping their tyres with computing chips to monitor inflation pressure and road conditions.====These research projects all have one thing in common: it is unclear how these innovations will translate into additional sales.====Specifying probabilities of commercial success is a difficult endeavour. Indeed, when one is concerned with the future success of an as-yet non-existent product, the expert is denied access to his favoured frequentist approach.==== In decision-theoretic terms: the commercial success of innovations is ====.====An ambiguous situation is a situation of uncertainty which is not and/or cannot be ==== measured by a probability distribution over outcomes. A roulette wheel on the other hand is best described as ====, as there are clear probabilities that dictate the outcomes. In a situation of risk then, decision makers, firms, can compute the expected utility they would derive from profits based on the objective probability. If the firm is risk averse, ====, the firm's marginal utility of profit is decreasing, then the firm would rather get 50$ for sure than 100$ if “heads” come out. In a situation of ambiguity, firms do not have access to a unique probability distribution. Instead, I assume, following Gajdos et al. (2008), that firms possess a set of distributions.==== These distributions correspond to the various estimates of various experts. An ambiguity averse decision maker would then compute its expected utility for each distribution and base his decision on the lowest value. Thus, ambiguity averse firms take their decisions on a worst-case scenario basis.====This paper presents a stylized model of investment in R&D. Two firms are present and compete both on the R&D market and the product market. Initially, both firms face an investment decision in a new R&D project. One firm at most (the patent winner) will then have access to the innovation and must decide whether to develop the innovation or not at a fixed cost. If it does, it will then produce a good of a higher quality than its competitor. Otherwise, or if no firm succeeds in the technical phase, both firms continue to compete with a good of the same quality in the market. The higher quality obtained is unknown at the investment stage and is assumed to be ambiguous. Quality in this sense does not represent an objective value such as horsepower of an engine or storage capacity of a hard drive but rather the subjective assessment of the consumers: how much more money are consumers willing to pay for that innovation? An iPhone with SIRI is better than an iPhone without SIRI, but how much better? When a firm increases the quality of its product, it raises its profits and lowers that of his competitor through a business-stealing effect. The model is presented in detail in Section 2.====Given uncertainty of the commercial success of innovations, one could assume that uncertainty averse firms would shy away from research. This paper shows that it depends on the kind of uncertainty aversion. Indeed, ambiguity averse firms tend to invest in more projects while risk averse firms invest in less projects. This is the main result of the paper and is presented in Section 3. The key intuition behind the result is that risk averse firms will tend to prefer a certain outcome, not developing an innovation, to an uncertain outcome, developing the innovation. Thus, risk averse firms would develop less innovations and therefore invest less often. On the other hand, an ambiguity averse firm will base its decision on the worst-case scenario. Given there is a business-stealing effect, the worst-case scenario when the competitor invests is that the innovation is very good. This leads firms to invest so long as it is possible, but not necessarily certain, that the project increases quality significantly. Subsection 1.1 presents an introductory example which illustrates the intuition.====I discuss in Section 4.1 how this result holds when product competition is best modelled by Cournot or Bertrand competition with vertical differentiation. Given multiple equilibria may co-exist, I show that, except for a corner case, whenever both an investment equilibrium and a no-investment equilibrium co-exist, the no-investment equilibrium is not Cournot-stable and there exists an investment equilibrium that is Cournot-stable in Section 4.2.====Finally, I show in section 4.3 that ambiguity aversion can either have a positive, negative or constant effect on the level of investment in equilibrium. I show that when the gains of innovation to the winner far exceed the losses to the loser, for example for more radical, game-changing projects, then an increase in ambiguity aversion can weakly decrease the level of investment. On the opposite side of the spectrum, incremental innovation in which the gains to the winner only moderately surpass the losses to the loser, then investment levels will increase with ambiguity aversion. Note this can also apply to radical innovations if the firms are also highly risk-averse.====The intuition behind this result stems from the fact that the level of investment in equilibrium increases as firms' beliefs regarding future quality becomes more optimistic. Thus, when potential gains far outpace potential losses, an ambiguity averse firm will tend to use a rather pessimistic belief in equilibrium. If the more pessimistic belief in the belief set is such that the project is strictly profitable, then any increase in ambiguity aversion will lead the firms to use an even more pessimistic belief and will therefore lower the level of investment. If the project was already barely profitable at said belief, then an increase in ambiguity aversion will not change much.====On the other end of the spectrum, consider the case where potential gains are not significantly higher than expected losses. In this case, noting that a more optimistic distribution regarding quality will lead to a riskier lottery (higher rewards if winning the patent but higher losses if not) than a more pessimistic one. Thus, if firms are risk averse, a more optimistic belief regarding quality could lead to a lower expected utility than a more pessimistic one. As a result, in those cases, the firms act as if their belief was the most optimistic one in their belief set. Any increase in ambiguity aversion will therefore raise this upper bound which leads to higher investment in equilibrium.====In conclusion, ambiguity aversion will tend to boost incremental projects, both the number of projects and the level of investment in them. Regarding radical projects, the effect of ambiguity is twofold. On one hand, some radical projects that have a low floor would not be invested in without high ambiguity aversion. On the other hand, ambiguity aversion will decrease the level of investment in some projects that would have been invested in regardless of ambiguity aversion. This is notably the case for projects which are profitable under any belief.",Is ambiguity aversion bad for innovation?,https://www.sciencedirect.com/science/article/pii/S0022053119300791,2 August 2019,2019,Research Article,228.0
Lang Ruitian,"The Australian National University, Australia","Received 21 October 2016, Revised 10 July 2019, Accepted 29 July 2019, Available online 2 August 2019, Version of Record 14 August 2019.",https://doi.org/10.1016/j.jet.2019.07.014,Cited by (7),"This paper develops a model of dynamic information acquisition where a buyer acquires information about a product, and a monopoly seller sets the price of her product anticipating the buyer's behavior. It finds that the buyer makes a purchase decision when his expected gain from trade hits one of two boundaries which are deterministic functions of time. Those boundaries are independent of the buyer's prior value. The seller's profit is increasing in the buyer's cost of information if and only if the buyer's prior purchase probability is above fifty percent. In analyzing the problem, a close connection between the information acquisition problem and the theory of American options is established and exploited.","Before making a decision, an agent often has opportunities to acquire information about his preference or the underlying state of the world. Even at the time he makes a decision, he still has the option to acquire more information. This leads to the problem of dynamic information acquisition, which differs from static information acquisition models where the agent chooses the quality of his signal once and for all and has to make a decision after observing the said signal. Economists have been using Bayesian updating models to describe the agent's decision-making; however, the strategic interaction between this information-acquiring agent and other agents has not been adequately explored in the literature. The paper develops a tractable model of dynamic information acquisition where a buyer observes signals taking the form of a diffusion process and studies the monopoly seller's price-setting strategy anticipating the buyer's dynamic Bayesian updating. More specifically, the seller first posts the price of her product, and then the buyer observes a costly flow of signals about his value of the product and makes his binary purchase decision upon stopping observing signals.====The practice of giving potential buyers a chance to learn about a product before purchase is widespread. For example, in the software industry, a commonly used practice is to offer a cheap student license of an expensive software so that students can explore its various features at a relatively low financial and opportunity cost during their school time, in the hope that they purchase the business license when they find jobs or start their own businesses. In this case, the information acquisition process comes to an end when the student stops exploring more features of the software, which might not coincide with the expiry of his cheap license.====Compared with the classical pricing problem, optimal pricing in the presence of dynamic information acquisition brings two technical challenges. First, the buyer's optimal information acquisition strategy takes the form of a stopping time, the time at which he stops the costly process of acquiring information and makes a decision. In contrast, the buyer's optimal strategy is usually a number measuring how “much” information he intends to acquire in a static model. In solving her pricing problem, the seller needs to worry about how the buyer's optimal stopping time, a more complex object than a number, responds to her price. Secondly, the probability that the buyer eventually buys the product can potentially depend on the seller's price in a complicated way. Without a careful choice of functional forms, even if the buyer's optimal stopping problem can be solved for every possible price chosen by the seller, it may be difficult to determine which price yields the highest expected profit for the seller.====This paper models the buyer's information acquisition process as a Bayesian updating process based on information that arrives in continuous time. Specifically, the information is a Brownian motion with a drift (or a “diffusion”): the drift equals the buyer's true value of the product, but the noise in the Brownian motion prevents the buyer from learning his value instantaneously. The buyer incurs a flow cost observing this information process and the amount of information he acquires is measured by the time ==== at which he stops observing the process and makes his decision. The analysis focuses on three aspects of the equilibrium: the condition under which the buyer makes his decision, the prior probability that the buyer eventually purchases the product, and the monopoly seller's optimal price. The prior purchase probability, as a function of the price, plays the role of a demand curve in the seller's pricing problem. As the model of information acquisition process is not specific to using the trial version of a software and the buyer's decision problem can be used as a module in other applications (or on its own), the paper also offers insights about dynamic information acquisition beyond the scope of monopoly pricing.====Through the analysis, several questions will be raised and answered. First, how much information is considered sufficient for the buyer to make a decision? Secondly, what the buyer's opinion about the product looks like when he makes a decision? Finally, would the seller have an incentive to make the buyer's information acquisition easier or harder?====One advantage of using our diffusion model of information acquisition is that the buyer's posterior opinion about the product at time ==== can be summarized by two variables: his expectation of the net value, ====, which is the difference between his value of the product and the price, and his uncertainty about the true net value, ====. Decision theorists are also interested in the probability ==== that he would regret his decision after learning the true value if he were forced to decide at ====. Both ==== and ==== are stochastic in that they depend on the realization of signals. Surprisingly, the value of ==== at the buyer's optimal decision time ==== is not so stochastic in two senses. First, conditional on ====, ==== and ==== are not random at all! If an econometrician sees a rational buyer make his decision at time, say, ====, she can tell precisely the absolute value of the buyer's expected net value at that time, the sign of which determines whether the decision is “to purchase” or “not to purchase”, and the probability that he would regret his decision. All the history that leads up to this decision becomes irrelevant for predicting how happy the buyer will be with his decision. Secondly, the deterministic values ==== and ==== are independent of the buyer's prior expectation of the product's value and the price of the product. In a static model, the buyer's posterior expected net value has a non-degenerate probability distribution (unless he only observes a binary signal) which depends on his initial opinion of the product; the same holds for his probability of regret.====There are many channels through which the seller may facilitate the information acquisition process, such as offering technical support for the trial version and making the trial version less limited in features. The model predicts that the seller has an incentive to do so only when the prior probability that the buyer purchases the (full version) product is lower than one half. When this is the case, facilitating information acquisition increases the buyer's purchase probability and thus boosts the seller's profit.====The model bears close connection with the optimal exercising problem of an American option. In fact, many results described above are derived using analytical tools borrowed from asset pricing. This connection also provides methods for numerical simulation. The model is simple enough so that the equilibrium behavior essentially only depends on one particular combination of parameters. This parameter is denoted by ==== and is related to the cost of information that reduces the buyer's uncertainty from its initial value to half of that.====The study of decision-making problems under sequential information acquisition dates back to Wald (1947) and Arrow et al. (1949). The early studies lay down the basic framework of the sequential sampling problem: focus on the decision maker's posterior belief and compute the regions in which he decides and the region in which he continues the search. Later developments include Bather (1962) and the monograph DeGroot (2004). The quantitative application of dynamic information acquisition model, though, requires a model that is simple enough to allow for easy computation and rich enough to fit various features of data. Since Ratcliff (1978), diffusion models have been used to describe decision-making processes and to fit laboratory data. Recent progresses include Ratcliff and McKoon (2008), Shadlen and Kiani (2013). There is a survey Fehr and Rangel (2011) on this development. Gabaix and Laibson (2005) use diffusion to model cognitive processes. Fudenberg et al. (2018) study decision-making under diffusion processes, which is very similar to the buyer' problem considered in this paper. All of these papers focus on the decision-making problem of a single agent, and leave the question of other agents' strategic response to information acquisition open.====Bolton and Harris (1999) consider information acquisition in games; Branco et al. (2012) and Ke et al. (2016) consider information acquisition in a buyer-seller setting. All of these papers use exogenously given processes to characterize the agent's payoff or posterior beliefs. The current paper adopts a Bayesian updating framework and shows that the constant decision boundaries used in Gabaix and Laibson (2005) and Branco et al. (2012) is only the lowest-order approximation when information is very difficult to acquire. Most results of the current paper hold regardless how difficult information is to acquire. Bar-Isaac (2003) considers a dynamic information flow model where revelation of information depends on the purchase decisions of generations of buyers. Board and Lu (2018) consider a model of information provision. Natenzon (2019) and Lu (2016) consider static information acquisition. Part of the current paper emphasizes qualitatively different predictions made by dynamic and static information acquisition models.====There are also models where one party learns about the other party's binary type through a diffusion. Daley and Green (2012), Kolb (2015) and Kolb (2019) consider settings where the second party knows her own type, while Pease (2018) considers a buyer-seller setting similar to ours. As common in these papers, the buyer's posterior is a Bernoulli distribution and thus his “uncertainty” (posterior variance) is a function of his posterior expectation and does not reflect the “amount” of information that he has acquired. This results in a Bayesian learning model where decision boundaries are constant over time. The current paper considers the Gaussian posterior distributions where the uncertainty and expectation are captured by two different parameters. This richer setting allows for discussion of the relationship between duration of information acquisition and quality of decision, as done in Section 3.3.====The classical mechanism design papers following Baron and Myerson (1982) and Laffont and Tirole (1986) assume that the agent is endowed with more information and cannot acquire further information. Cremer and Khalil (1992) and Cremer et al., 1998a, Cremer et al., 1998b consider binary information acquisition in a mechanism design setting. This paper considers a richer information acquisition process. Information acquisition is studied by different authors in the auction design literature; see Bergemann and Välimäki (2006) for a survey. In this literature, Persico (2000) and Shi (2012) study auctions assuming that each buyer makes a one-shot decision among different experiments that can be sorted by their informativeness. In this paper, the buyer's information acquisition strategies cannot be sorted in a meaningful way. Bergemann and Pesendorfer (2007) and Eso and Szentes (2007) study settings where the seller can control the information process available to the buyer and the optimal information structures from the seller's perspective are simpler than what the current paper considers. In a similar spirit, Roesler and Szentes (2017) consider the information structure optimal for the buyer, and Gul and Pesendorfer (2012) consider competition in information provision.====Bergemann and Välimäki (2002) study efficient mechanisms with information acquisition, Szalay (2009) studies information acquisition in a principal-agent problem, and Gershkov and Szentes (2009) study information acquisition in a voting problem. Compared with these papers, the current paper considers a very simple contract design problem in which the designer (seller) only chooses one number (the price) to allow for a tractable model with dynamic information acquisition. This simple setting is motivated by Lewis and Sappington (1994).====The paper borrows analytical tools from the theory of American options, for which Duffie (2001), Karatzas (1988), and Karatzas and Shreve (1998) are all consulted. The numerical simulation uses methods from Andersen and Piterbarg (2010). The paper is also related to empirical findings in the merger and acquisition literature, for which Jensen and Ruback (1983) is an early summary and recent contributions include Moeller et al. (2004) and Golubov et al. (2015).====The rest of the paper is organized as follows. Section 2 describes the model. Section 3 presents the main qualitative results and derives some comparative statics. Section 4 considers approximate solutions that relate our model to existing models in literature, and explores other aspects of the model through numerical simulation. Section 5 considers extensions of the baseline model and compares the baseline model with a static model. Section 6 concludes. All proofs are collected in Appendix A.",Try before you buy: A theory of dynamic information acquisition,https://www.sciencedirect.com/science/article/pii/S002205311930078X,2 August 2019,2019,Research Article,229.0
"Bloise G.,Citanna A.","Dept. of Economics, Yeshiva University, United States of America,Div. Soc. Sciences, Abu Dhabi, New York University, United Arab Emirates","Received 26 May 2017, Revised 22 July 2019, Accepted 23 July 2019, Available online 1 August 2019, Version of Record 14 August 2019.",https://doi.org/10.1016/j.jet.2019.07.011,Cited by (2),"We study the effect of asset shortages on liquidity in economies with limited enforcement of debt contracts. We establish that, under a suitable assumption in terms of gains from trade, liquidity does not dry up and trading volumes do not disappear as credit conditions worsen. The equilibrium outcome becomes arbitrarily close to a purely speculative equilibrium, and bubbles occur in the limit. The result applies to incomplete markets and, more generally, to equilibria where collateral constraints interfere with the additivity of the payoff pricing functional.","How much does liquidity dry up as a result of a deterioration of credit conditions, and with what effects on trading volumes? How much collateral is needed to reach credit volumes sustainable in a roll-over debt regime? What role do credit market conditions play in the emergence of bubbles? We explore these issues in a conventional economy with competitive markets under limited enforcement of debt contracts.====The extent of trade in financial markets is constrained by the value of outside assets and the amount of enforceable borrowing, a quantitative limit that is sometimes referred to as liquidity (see, ====, Brunnermeier and Pedersen, 2009 and Holmström and Tirole, 2011). Liquidity itself, though, is determined endogenously at equilibrium. Its level depends on the type of debt enforcement mechanism used to counter limited commitment, and on its severity. When debt enforcement becomes weaker, the conditions under which credit is supplied, from the lenders' viewpoint, deteriorate. This happens, ====, when agency issues become more severe, and the amount of pledgeable resources in the economy is reduced. Equivalently, it occurs when there is a scarcity of assets that can be used as collateral. Weaker reputational punishments also are an example of worsening credit conditions, leading to tighter debt limits. Models of debt constraints induced by limited commitment thus offer a natural theoretical ground to study asset shortages, weakening reputational punishments and their effects on liquidity and intertemporal trade.====We link the effects of asset shortages on liquidity to a suitable form of gains from trade that identifies directions of improvements feasible under the limited commitment restrictions. The effects of vanishing pledgeable resources, or of less stringent punishments for default, on liquidity and credit volume depend on whether or not individual endowments admit the identified gains from trade. When this happens, trade persists and speculative bubbles emerge in a broad class of economies with incomplete markets subject to a large variety of portfolio restrictions.==== This conflicts with a certain practice of interpreting autarky as an approximation of scarce assets and tight credit conditions. Our methodological innovation uncovers a general property of competitive equilibrium and identifies a mechanism for the occurrence of speculative bubbles.====As in Kocherlakota (2008), we concentrate on a reduced form of competitive equilibrium with solvency constraints in which outside assets, or a pledgeable part of the private endowment, can serve as collateral to secure debt and, in addition, debt limits define the amount of further (unsecured) credit an individual can receive. These limits can be induced by some punishment in terms of restricted market participation upon default. A few prominent existing models fit these assumptions and are thus embedded in our analysis (see Section 4 for a formal appraisal). For a more transparent analysis, we first focus on economies with a complete set of contingent claims. The advantage of this assumption is that limited commitment is effectively the only friction in the economy and this allows for a sharper characterization without further interferences.====Our basic insight is that, when long-term outside assets are traded, or else when debt enforcement is sufficiently severe, competitive equilibrium is efficient ==== on no default constraints. This notion of efficiency is admittedly an artifact, because default incentives are themselves endogenously determined at equilibrium. Thus, due to a residual pecuniary externality, the criterion is unsuitable for a proper welfare analysis. However, when outside assets vanish or debt enforcement deteriorates, the limit allocation retains this artificial property of conditional efficiency. On the other hand, when endowments are sufficiently dispersed, autarky is not conditionally efficient. Hence, no trade cannot be the outcome of a shortage of assets or a deterioration of credit conditions. Furthermore, when unsecured debt is unsustainable and the outside supply of long-term assets yielding dividends disappears in the limit, trade can only be supported by speculative bubbles which serve as store of value.====Our argument is inspired by Aiyagari and Peled's (1991) analysis of overlapping generations economies. They establish that a (stationary) monetary equilibrium exists if and only if autarky is (Pareto) inefficient.==== To ensure a positive value of money, they develop a perturbation argument. More precisely, they construct a sequence of (stationary) economies which converge to the given economy. Along the sequence, a long-term security yielding dividends is traded, and the original economy corresponds to the limit case when the dividends vanish. Each economy along the sequence admits an equilibrium that is efficient, because positive dividends ensure a finite value of the aggregate endowment and this in turn enforces efficiency when other frictions are absent. The limit of these perturbed equilibria is also efficient and, hence, cannot be autarky. Therefore, the given economy must have an efficient equilibrium with trade and this can only occur if money is valued. Their logic does not straightforwardly apply to our economy because equilibria are typically (Pareto) inefficient. In fact, our auxiliary concept of conditional efficiency serves as a substitute for the failure of (Pareto) efficiency.====How do speculative bubbles emerge in the limit? The mechanism for the occurrence of bubbles is better understood when all debts are secured by long-term outside assets, or by pledgeable resources. Without outside assets, debt is unsustainable and no trade can occur at a competitive equilibrium. Scarce outside assets, instead, still provide a relevant source of liquidity, as they circulate in the economy and can be used as a store of value for self-insurance purposes. As these assets become scarcer, the liquidity premium inflates their market value to compensate for the decline of dividends in the aggregate. In fact, outside assets are approximately valued as a sort of money in Bewley's (1980) equilibrium, though they accessorily yield negligible dividends. Further, as in Bewley's (1980) monetary equilibrium, a decline in the supply of outside assets only increases their market value with approximately no real effect on the allocation. In the limit, when dividends vanish, trade is sustained by an exact speculative bubble.====We then move to the study of economies with incomplete markets and where trade is subject to a large class of portfolio constraints that are compatible with two properties: (a) portfolios can always be scaled up and down; (b) holdings of outside assets can always be increased without violating any of the portfolio constraints. Since these further frictions introduce autonomous causes of inefficiency independent of limited commitment, conditional efficiency of competitive equilibrium fails. Our approach can nevertheless be adapted to show the persistence of trade under a shortage of assets. Indeed, we identify some directions of welfare improvement that are exhausted at equilibrium when outside assets are traded in competitive markets. In particular, at equilibrium, the planner cannot roll over transfers along the identified direction so as to increase utility of all individuals at ==== contingencies. When such gains from trade are instead available at autarky, no trade cannot be the limit outcome of vanishing outside assets.====The paper effectively provides sufficient conditions for bubbles to emerge in stochastic economies with limited commitment. In this sense, our analysis extends the original Bewley's (1980) results on the existence of a monetary equilibrium, and complements Kocherlakota, 1992, Kocherlakota, 2008 and Santos and Woodford (1997), who focus on necessary conditions. Harrison and Kreps (1978) provide a sharp characterization of self-enforcing debt equilibria and an equivalence with unbacked public debt equilibria, while referring to previous known studies for existence. We extend their insights, also including economies with incomplete markets and other market frictions derived from collateral requirements.====Our results align with the theory of bubbles as means for the market to create liquidity in response to a shortage of assets, as expressed, among others, by Caballero (2006), Caballero and Krishnamurty (2006) and, more recently, Farhi and Tirole (2012).==== In particular, in overlapping generations economies, Farhi and Tirole (2012) also find that bubbles may emerge when the supply of outside assets is scarce and the pledgeability of incomes is limited. They show the presence of another equilibrium, with bubble, when pledgeable resources are scarce. We show that, when appropriate gains from trade are present, the equilibrium continuously slides close to a bubble-like situation as credit conditions worsen, and produces a bubble when credit conditions are the worst (====, when lending standards deteriorate and credit is supplied against no real income). The bubble is a substitute, not a complement, of easy credit, that is, of the amount of pledgeable resources. Our continuity result suggests that bubbles may emerge without transitions across equilibria, or without abrupt changes in beliefs.====While collateral is surely not necessary to support intertemporal trade, our continuity result also shows that there is no substantial difference in terms of trade or asset pricing between Hellwig and Lorenzoni's (1978) equilibria with self-enforcing debt (as well as Bewley's, 1980 monetary equilibria when markets are incomplete) and equilibria with debt secured by scarce collateral.==== The continuity of equilibrium prices and allocations to worsening credit conditions further suggests that differences in asset prices behavior between a speculative bubble and a liquidity premium regime may be minimal. Positive tests of the presence of bubbles in empirical analyses (using integration as in Diba and Grossman, 1988, or transversality tests as in Giglio et al., 2016) must provide evidence of sufficient distance of consumption and prices from the patterns obtained at a speculative bubble equilibrium.====Our limit result reinforces the notion of ‘inessentiality of credit’ that appears in some of the new monetarist literature (namely, in Gu et al., 2015).==== A proper comparison is arduous because, in the tradition of Bewley (1980), we do not specify a preference for liquidity and money can only be demanded as a store a value in our framework. This being said, Gu et al. (2015) show that, when credit is tight, changes in credit conditions are neutral because real balances respond endogenously to keep total liquidity constant. This is true in a variety of settings, with secured and unsecured lending. We also establish that, when credit is tight, changes in credit conditions produce little effect on equilibrium and, in the limit, speculative bubbles (money) almost perfectly compensate for the liquidity shortage. When speculative bubbles occur in the limit, neutrality obtains as an application of Kocherlakota (2008)'s bubble equivalence theorem: a downward perturbation of solvency constraints is perfectly compensated by an equal reduction of the market value of speculative bubbles, that is, debt and money are perfect substitutes.====The paper is organized as follows. In Section 2, we present a simple example which provides the basic intuition. In Section 3, we introduce the general economic environment. In Section 4, we describe markets under dynamic completeness and with non-distortive portfolio restrictions. We define the notion of competitive equilibrium, showing how our model encompasses several institutional frameworks already present in the limited commitment literature. In Section 5, we define conditional efficiency. Asset shortages are studied in Section 6, while Section 7 presents the results under incomplete markets and portfolio frictions. Appendix A contains the proofs, Appendix B further explores results about trade in the limit, and Appendix C discusses gains from trade at autarky.","Asset shortages, liquidity and speculative bubbles",https://www.sciencedirect.com/science/article/pii/S0022053119300778,1 August 2019,2019,Research Article,230.0
Moreira Alan,"University of Rochester, United States of America","Received 26 July 2017, Revised 20 June 2019, Accepted 23 July 2019, Available online 31 July 2019, Version of Record 13 August 2019.",https://doi.org/10.1016/j.jet.2019.07.010,Cited by (7),"I build a model in which financial intermediation slows down capital flows. Investors optimally learn from intermediary performance to allocate capital toward profitable intermediaries. Intermediaries reach for yield—i.e., they invest in high-tail-risk assets—in an attempt to drive flows and reduce liquidation risk. Intermediaries with strong opportunities face a trade-off between choosing a portfolio that maximizes profitability, and choosing one that maximizes the speed at which capital flows. In equilibrium, reaching for yield is stronger among intermediaries with weak opportunities, resulting in a reduction in the informativeness of performance; investors thus take longer to learn, and capital flows become less responsive to performance. Capital becomes slow-moving because the reach for yield dampens learning. The model predicts capital immobility to be stronger when tail risk is high; when tail risk is under priced; and in asset classes with large cross-sectional variation in tail-risk exposures.","Despite the large and increasing share of wealth managed by financial intermediaries, a growing body of work documents that financial capital moves slowly (Duffie, 2010; Pedersen et al., 2007). At the same time, there is an emerging consensus that reaching for yield is widespread in the financial sector (Rajan, 2005, Rajan, 2008, Rajan, 2012; Stein, 2013). Financial intermediaries seem to over invest in high-yield high-tail-risk assets.==== These facts have important aggregate implications on their own, but they are also deeply intertwined. How can investors know where to allocate their capital, if intermediaries can manufacture “alpha” by loading on low-probability tail risks?====In this paper, I build a dynamic model of financial intermediation to study this connection. The model is centered on the interaction between intermediaries and investors' capital allocation decisions. The key result of this model is that the reach for yield by intermediaries leads to capital immobility. Capital is endogenously slow-moving as a result of yield chasing by some intermediaries. The basic mechanism works as follows: financial intermediaries have strong incentives to improve short-term performance by loading on tail risks. Better performance improves their track record, attracts capital, and reduces the risk of liquidation. Incentives are particularly strong for intermediaries without good investment opportunities. These “opportunistic” intermediaries reach for yield more, dampening the performance advantage of “skilled” intermediaries. Intermediary performance becomes less informative about the underlying quality of the intermediary. Investors take longer to learn, and capital misallocation persists for longer.====Thus, capital immobility is an endogenous result of investors' optimal response to the incentives intermediaries face. The key assumption is that investors cannot directly measure tail risks in the intermediary portfolio. As a result, loading on tail risks is particularly attractive to an intermediary. It boosts short-term performance without appearing in easy-to-measure forms of risk (like return volatility). The reach for yield delays the speed of learning to the extent that it is stronger among opportunistic intermediaries, which always happen in equilibrium. Intuitively, if all intermediaries were equally aggressive in their reach for yield, performance in a tail event would be completely uninformative. In this case, a more aggressive investment on high-tail-risk assets would enable the opportunistic intermediary to attract flows in the short term, without impacting flows once a tail event hits. Thus, in equilibrium, the opportunistic intermediary must reach for yield more aggressively, resulting in less learning in the short term and more learning during a tail event. Because the opportunistic intermediary optimally balances out these competing forces, the result is that the reach for yield by the opportunistic intermediary reduces the speed of learning and the speed of capital flows.====Reaching for yield by the skilled intermediary has the opposite effect. It increases the speed at which investors learn because it makes it harder for the opportunist to keep up with the performance of the skilled type. However, it generates inefficient variation in investment flows across assets, reducing the expected returns earned by fund investors. Intuitively, as the skilled intermediary concentrates investments in high-tail-risk assets, the portfolio becomes poorly diversified, resulting in a fall in the portfolio's Sharpe ratio and expected returns. Assets that are heavily exposed to tail risks are quickly inundated with capital as the intermediary attracts new capital flows. Assets that pay well during a tail event experience the opposite phenomenon. As investors pour capital into the intermediary, capital flows into these assets at a very low rate. Thus, when the skilled intermediary reaches for yield more aggressively, the future allocation of capital improves faster, but the present allocation of capital gets worse.====An example is useful to illustrate how the decisions of the skilled intermediary shape the allocation of capital. Suppose the maximum alpha of the skilled type is 5% and it is achieved with a zero tail exposure. Suppose further that any intermediary can manufacture 5% alpha by loading on tail risks. If only the opportunist reaches for yield, short-term performance differences across types disappear. As a result, learning stalls and capital ceases to move in the short term. Given the allocation of capital to each intermediary, the allocation of capital is the most efficient as it achieves the highest expected return. However, from a dynamic perspective, the allocation of capital is the worst as capital ceases to move to the intermediary that allocates capital better. Now consider the case in which the skilled type also reaches for yield. The fund alpha declines to 4% as her portfolio deviates from the maximum expected return portfolio, but the short-term performance increases to 7% as the portfolio becomes more concentrated on high-tail-risk assets that over perform in the short term. In this case, short-term performance is again informative and capital more mobile. Capital flows faster to the good capital allocator, even though expected returns are lower and the allocation of capital is worse. This trade-off between the present and the future allocation of capital is a key insight from the model.====A novel insight that emerges from the model is the feedback loop between the reach-for-yield behavior of both investors and intermediaries. The closer an intermediary is to liquidation, the stronger the incentives to reach for yield. The increase in incentives to reach for yield is particularly strong among good intermediaries who have the most to lose from liquidation. The relative change in incentives drives investors to expect larger short-term performance differences across intermediaries. Thus, after bad performance, investors naturally respond more aggressively to short-term performance as short-term performance becomes more informative. This response further amplifies the incentives to reach for yield. This reach-for-yield spiral implies that flow-performance sensitivity is a nonlinear function of past performance.====The model makes several predictions that apply to investment vehicles that raise demandable equity capital from arms-length investors—such as mutual funds, hedge funds, and some private equity funds—and that have flexibility in their investment mandate to choose their assets, i.e., the theory applies to active managers. The theory does not require that these intermediaries have the ability to directly make clean tail-risk bets using options, but simply requires that there is cross-sectional variation in asset tail risk that can be identified by the expert but not by her investors.====The first set of predictions relates capital immobility to fund characteristics. Funds with more flexible investment mandates, or with mandates to invest in assets with relatively greater cross-sectional variation in tail risk, exhibit a weaker and more concave relationship between flows and performance. Intuitively, an opportunity set with greater variation in tail risk provides the intermediary with more freedom to invest in high-tail-risk assets without changing observable measures of portfolio risk. There is evidence that these predictions hold in the data for asset classes in which the manager has more flexibility, such as private equity (Kaplan and Schoar, 2005) and hedge funds (Goetzmann et al., 2003), as well as for asset classes with a relatively large cross-sectional variation in tail risk, such as corporate bonds (Goldstein et al., 2015).====The model provides a novel explanation for the evidence in Kacperczyk and Schnabl (2012), who documented a large increase in reach for yield across money market fund managers and investors, and who attributed this increase to a lack of market discipline.==== In the model, investors' understanding of fund manager incentives amplifies the reach-for-yield behavior of both investors and managers. It is the market discipline imposed by investors—i.e., the threat of liquidation—that drives the rampant reach-for-yield behavior.====Perhaps even more strikingly, the model is consistent with the very persistent overpricing of senior tranches of collateralized debt obligations (CDOs) and the concomitant underpricing of junior tranches documented by Coval et al. (2009). Intuitively, the model implies capital flows faster toward the most senior tranches that have relatively higher tail exposure. This results in under investment in the junior tranches and over investment in the senior tranches.====The model has several implications for financial stability. First, tail risks tend to concentrate in the portfolios of financial intermediaries. Second, it suggests that capital reallocation is particularly slow when tail events are more likely. Third, tail risks are more likely to build up in relatively low-risk asset classes, where volatility is a particular poor proxy for tail risks.==== Fourth, the model predicts that reaching for yield is stronger in low-interest-rate environments, which is consistent with recent empirical evidence (Choi and Kronlund, 2014) and the view of leading policymakers (Rajan, 2005, Rajan, 2012; Stein, 2013).====The remainder of the paper is organized as follows. After a brief discussion of the literature, Section 2 presents the model setup and characterizes the model solution. I study two economies: a benchmark economy where tail risk is readily observable by investors, and an economy where tail risk cannot be directly measured. In Section 3, a numerical calibration is used to illustrate the implications of the model.==== This paper relates to a growing body of literature that focuses on implicit incentives that are induced by investor behavior. Chevalier and Ellison (1997); Basak et al. (2007); Chapman et al. (2009); and Basak and Makarov (2014) studied implications for manager portfolio choice, while Brennan and Li (1993), Shleifer and Vishny (1997), Vayanos (2004), Cuoco and Kaniel (2011), Basak and Pavlova (2013), and Kaniel and Kondor (2013) studied the implications of these implicit incentives for asset pricing. These authors took the behavior of investors as given and studied the implications for portfolio choice and equilibrium pricing.====A second strand of literature relevant to this paper studies learning in money management. Berk and Green (2004) showed that the behavior of fund flows and lack of persistence in fund performance could be explained by investors' use of fund performance to learn about their managers. Pastor and Stambaugh (2010) relied on a similar idea to explain the dynamics of the size of the money-management industry. Berk and Stanton (2007) built on these ideas to explain the closed-end fund discount, while Dangl et al. (2008) studied the effect of learning on the optimal replacement of a manager. More broadly, Kozlowski et al. (2016) showed that tail risk can slow down learning substantially. They used this connection to explain the slow recovery after the 2008 financial crises. My work is very different, in that it highlights how tail risks and intermediaries interact in a way that endogenously slows down learning.====This paper connects these two sides of the literature, and studies an environment where learning is endogenous to the manager's trading behavior. The previous work that emphasized the dynamics of investor learning has mostly abstracted from the interaction between portfolio choice and learning by either simplifying the investment opportunity set or arguing that the investment opportunity set was “sufficiently non-stationary,” which made this type of endogenous response by fund investors infeasible (Shleifer and Vishny, 1997). This paper contributes to the agency literature by showing that learning and manager incentives interact in powerful ways. A novel feedback loop between the reach-for-yield behavior of investors and managers emerges, producing amplification and time variation of reach-for-yield incentives.====My paper is more closely related to the literature that connects the agency and learning views, which is founded on the signal-jamming framework of Holmstrom (1999). Dasgupta and Prat (2008) and Dasgupta et al. (2011) studied the effects of fund managers' reputation concerns on asset pricing in a model where prices were determined by a market maker. Vayanos and Woolley (2013) showed that learning about manager efficiency had the potential to explain the momentum effect. Acharya et al. (2013) also developed a model in which reputation concerns slowed down the identification of good managers. The authors assumed that learning had to start again every time the manager switched to a new project, creating an incentive for managers to switch projects inefficiently to mitigate their reputation risk. This paper focuses instead on the choice of the payoff distribution and applies more directly to financial intermediaries. Makarov and Platin (2015) studied optimal contracting in the case of symmetric information between investors and managers. My paper emphasizes the role of asymmetric information and the link with the speed of capital flows, but takes the contractual environment as given.====The papers most related to my work are Guerrieri and Kondor (2012), Malliaris and Yan (2015), and Di Maggio (2014). These papers studied the impact of reputation concerns on the willingness of a fund manager to invest in strategies that paid well with a low probability. Guerrieri and Kondor (2012) emphasized excess volatility, i.e., how changes in reputation concerns can drive risk premia. Malliaris and Yan (2015) showed that reputation leads managers to avoid bets that pay poorly most of the time. Importantly, none of these showed how reach for yield lead to capital immobility, i.e., a reduction in the speed of capital reallocation to good capital allocators. My paper is the first to show a fundamental trade-off between static and dynamic capital allocation. Specifically, when skilled intermediaries over-allocate to high-yield assets, they reduce the quality of capital allocation in the short term, but they increase the speed at which capital allocation improves.",Capital immobility and the reach for yield,https://www.sciencedirect.com/science/article/pii/S0022053119300766,31 July 2019,2019,Research Article,231.0
"Battigalli P.,Francetich A.,Lanzani G.,Marinacci M.","Department of Decision Sciences and IGIER, Università Bocconi, Via Röntgen, 1, 20136 Milano, Italy,School of Business, University of Washington Bothell, 18115 Campus Way NE, Bothell, WA 98011, USA,Department of Economics, Massachusetts Institute of Technology, 50 Memorial Drive, Cambridge, MA 02139, USA","Received 29 March 2017, Revised 13 July 2019, Accepted 23 July 2019, Available online 30 July 2019, Version of Record 8 August 2019.",https://doi.org/10.1016/j.jet.2019.07.009,Cited by (17),"We consider an ambiguity averse, sophisticated decision maker facing a recurrent decision problem where information is generated endogenously. In this context, we study self-confirming actions as the outcome of a process of active experimentation. We provide inter alia a learning foundation for self-confirming equilibrium with model uncertainty (====), and we analyze the impact of changes in ambiguity attitudes on convergence to self-confirming equilibria. We identify conditions under which the set of self-confirming equilibrium actions is invariant to changes in ambiguity attitudes, and yet ambiguity aversion may affect the dynamics. Indeed, we argue that ambiguity aversion tends to stifle experimentation, increasing the likelihood that the decision maker gets stuck into suboptimal “certainty traps.”","We study the dynamic behavior of a decision maker (DM) who faces a recurrent decision problem in which the actions he selects depend on the information endogenously gathered through his past behavior as, for example, in multiarmed bandit problems (cf. Gittins, 1989). We diagram the flow of actions and information in Fig. 1.====Our DM is ambiguity averse, finitely patient, and uncertain about the stochastic process of states of nature. In this setting, there are three crucial elements of our analysis. First, the process of states is governed by an unknown objective probability model (e.g., the composition of an urn). Second, the uncertainty of the DM about the objective model is represented through a subjective probability measure, a belief, which is updated according to information feedback. Each period, the DM evaluates the possible actions (given his updated belief) according to a dynamic version of the smooth ambiguity criterion of Klibanoff et al. (2005), which separates ambiguity attitudes (a personal trait) from the evolving perception of ambiguity, and allows for a Bayesian analysis of learning. Third, the DM uses a rational strategy given his prior belief.====It is essential to understand the meaning of the term “rational” in our setting. An uncertainty averse DM may have ==== (cf. Example 5). While we allow for such reversal of preferences, we assume that the DM is sophisticated in the sense that he formulates a ====, that is, a strategy that satisfies the one-deviation (or intrapersonal equilibrium) property: There is no instance where the DM has an incentive to choose an action different from the one prescribed by the given strategy. In a finite-horizon model, this is equivalent to folding-back planning. But we cannot rely on folding back, because we focus on infinite-horizon models to study the limit properties of behavior and beliefs, and to exploit the ensuing stationarity of the dynamic decision problem.====We study how steady-state actions arise from an active experimentation process, providing a novel convergence result. Specifically, we show that the stochastic process of beliefs and actions converges with probability 1 to a random limit action-belief pair. This random limit pair satisfies almost surely the following ==== conditions: The limit action maximizes the one-period value given the limit belief, and the limit belief assigns probability 1 to the set of probability models that are observationally equivalent to the true one given the limit action.====Therefore, even if the DM cares about the future, the limit action-belief pair must be a self-confirming equilibrium of the one-period game repeatedly played against nature. Since the belief may only partially identify the true model (nature's “behavior strategy”), such limit behavior may be very different from the “Nash” (or “rational expectations”) equilibrium, in which the DM plays the objective best reply.====Since we assume that the state process is exogenous, that is, the DM's actions cannot influence the probabilities of states in future periods, our framework cannot model long-run interactions with a fixed set of co-players. However, our exogeneity assumption is justified within the scenario of large population games. Indeed, our setup can represent the point of view of a DM who plays a game recurrently with other agents independently drawn from large, statistically stable, populations. Hence, the DM recognizes to be unable to influence the evolution of the environment with his actions. The probability models describe the distribution of behaviors in the co-players' populations. Experimentation is valuable to the DM since a better understanding of the correct distribution may allow him to select a better strategy in the following periods. In particular, our setting is consistent with a steady-state learning environment ==== Fudenberg and He (2018), where individual agents learn through their life, but the population's statistics are constant.====Under this interpretation, we provide a learning foundation for self-confirming equilibrium with model uncertainty (Battigalli et al., 2015, henceforth BCMM). Specifically, the random limit pair corresponds to the “smooth” self-confirming equilibrium concept of BCMM since the limit action is a myopic best response, and the evidence generated by the limit action and the steady-state distribution of opponents' actions confirms the limit belief. BCMM prove that higher ambiguity aversion yields a larger set of self-confirming equilibrium actions. Intuitively, the reason is that a self-confirming equilibrium action is “tested,” hence it yields known risks (objective probabilities of consequences), whereas deviations yield unknown risks that are the less attractive the higher the aversion to ambiguity. Since we show that self-confirming equilibrium emerges as the long-run outcome of an active experimentation and learning process, the comparative statics result of BCMM implies that higher ambiguity aversion reduces the predictability of long-run behavior.====We provide special conditions under which the BCMM theorem holds as an invariance result: The set of self-confirming equilibrium actions does not depend on ambiguity attitudes. Nonetheless, ambiguity aversion may still affect the dynamics. Specifically, we argue that ambiguity aversion tends to stifle experimentation, increasing the likelihood that the DM gets stuck into suboptimal “certainty traps.” The intuition is as follows. Suppose that the DM can only learn from observing his realized payoffs. The actions perceived as ambiguous, that is, those with uncertain distributions of payoffs, are those from which the DM expects to learn. If instead an action is perceived as unambiguous, the DM expects to have the same belief before and after choosing it, i.e., he does not expect to learn from it. Hence, ambiguity aversion biases the DM toward “exploitation” and against “exploration.”====  We point out that there is a formal connection between the concept of SCE and the literature on active learning (or “stochastic control”), and in particular the seminal work by Easley and Kiefer (1988, henceforth EK). The working paper version==== provides a translation between our setup and the active learning setup. Our paper departs from EK in two fundamental aspects. First, we allow for non-neutral ambiguity attitudes and dynamically inconsistent preferences. Second, EK requires the DM to assign positive subjective probability to (every neighborhood of) the correct model, whereas our sufficient condition for convergence to an SCE allows for misspecified beliefs.====Our definition of self-confirming equilibrium is related to the notion of subjective equilibrium of Kalai and Lehrer, 1993, Kalai and Lehrer, 1995. Relatively minor details aside, there are two key differences between the two concepts. First, KL define and analyze subjective equilibrium as the rest point of a process of updated beliefs about the path of play in a supergame. Such beliefs can be interpreted either as “subjective averages” of probability models, or as subjective Dirac beliefs over probability models. Focusing on such beliefs is without loss of generality under subjective expected utility maximization, but not under non-neutral ambiguity attitudes (see Sections 2.1 and 4). Second, since in KL the set of interacting players is fixed once and for all, their analysis concerns the convergence to a steady state of beliefs about supergame behavior. Our analysis, instead, is consistent with steady-state learning in a population game scenario; thus, we obtain convergence to an equilibrium of the one-period game.====Our results on ambiguity aversion and experimentation are consistent with the findings in Li (2019) and Anderson (2012). Li (2019) characterizes the optimal experimentation strategy under ambiguity aversion in an independent K-armed bandit problem. Aside from focusing on this specific case, the key difference with our paper is that Li (2019) models ambiguity aversion following the two-stage multiple-prior model of Marinacci (2002), while we employ the smooth ambiguity criterion of Klibanoff et al. (2005). As a result, the comparative-statics analysis in Li (2019) considers the impact of changes in the perception of ambiguity, while ours studies the effect of changes in ambiguity attitudes. Moreover, Li (2019) uses a recursive version of the maxmin expected-utility criterion and is thus able to employ standard dynamic programming techniques. Such a recursive representation is precluded in our setting. Unlike Li (2019) and our paper, Anderson (2012) derives the predictions of his model under the implicit assumption that the decision maker can commit ex-ante to any strategy. However, the empirical evidence he presents is consistent with the theoretical predictions of our model.====  The paper is structured as follows. Section 2 presents the static and dynamic decision framework, as well as preliminary concepts. Section 3 describes the endogenous information process. Section 4 describes the DM's intertemporal preferences. Section 5 analyzes self-confirming equilibrium, rational strategies, and presents our results on convergence to SCE. Section 6 presents our comparative dynamics results with respect to changes in ambiguity attitudes. Finally, Section 7 briefly relates our analysis to the literature on learning in games and concludes. Proofs are relegated to the appendix. We refer to the working paper version for the complete derivation of the results presented in the examples.",Learning and self-confirming long-run biases,https://www.sciencedirect.com/science/article/pii/S0022053119300754,30 July 2019,2019,Research Article,232.0
Nisan Noam,"The Hebrew University of Jerusalem (Federmann Center for the Study of Rationality, Department of Economics, and Institute of Mathematics), Israel,The Hebrew University of Jerusalem (Federmann Center for the Study of Rationality, and School of Computer Science and Engineering), Israel,Microsoft Research, Israel","Received 6 November 2017, Revised 1 July 2019, Accepted 14 July 2019, Available online 29 July 2019, Version of Record 14 August 2019.",https://doi.org/10.1016/j.jet.2019.07.006,Cited by (34),"We consider the ==== of mechanisms as a measure of their complexity, and study how it relates to revenue extraction capabilities. Our setting has a single revenue-maximizing seller selling a number of goods to a single buyer whose private values for the goods are drawn from a possibly correlated known distribution, and whose valuation is additive over the goods. We show that when there are two (or more) goods, simple mechanisms of bounded menu size—such as selling the goods separately, or as a bundle, or deterministically—may yield only a negligible fraction of the optimal revenue. We show that the revenue increases at most linearly in menu size, and exhibit valuations for which it increases at least as a fixed fractional power of menu size. For deterministic mechanisms, their revenue is shown to be comparable to the revenue achievable by mechanisms with a similar menu size (which is exponential in the number of goods). Thus, it is the number of possible outcomes (i.e., the menu size) rather than restrictions on allocations (e.g., being deterministic) that stands out as the critical limitation for revenue extraction.","Are complex auctions better than simple ones? Myerson's (1981) classic result (see also Riley and Samuelson, 1981, and Riley and Zeckhauser, 1983) shows that if one is aiming to maximize revenue when selling a single good, then the answer is “no.” The optimal auction is very simple, allocating the good to the highest bidder (using either first or second price) as long as he bids above a single deterministically chosen reserve price.====However, when selling multiple goods the situation turns out to be much more complex. There has been significant work both in economics and in computer science==== showing that, for selling multiple goods, simple auctions are no longer optimal. Specifically, it is known that randomized auctions may yield more revenue than deterministic ones, and that bundling the goods may yield higher (or lower) revenue than selling each of the goods separately. This is true even in the very simple setting where there is a single buyer.====In this paper we consider such a simple setting: a single seller, who aims to maximize his expected revenue, sells two or more heterogeneous goods to a single buyer whose private values for the goods are drawn from an arbitrary (possibly correlated) but known prior distribution, and whose value for bundles is additive over the goods in the bundle. Since we are considering only a single seller, this work may alternatively be interpreted as dealing with the monopolistic pricing of multiple goods.====In our previous paper, Hart and Nisan (2012/2017),==== we considered the setup where the buyer's values for the different goods are ====, in which case we showed that simple mechanisms are ==== optimal: selling each good separately (deterministically) for its optimal price extracts a constant fraction of the optimal revenue. In this paper (see Hart and Nisan, 2013, for the original version), we show that the picture changes completely when the valuations of the goods are ====, in which case “complex” mechanisms can become arbitrarily better than “simple” ones.====The setup is that of ==== goods, whose valuation to the single buyer is given by a random variable ==== with values in ====; we emphasize that we allow for arbitrary dependence between the coordinates of ====. The buyer's valuation for a bundle of goods is additive over the goods; thus, for example, getting the first two goods is worth ==== to the buyer. We denote by ==== the optimal revenue achievable by any mechanism for selling ==== goods to an additive buyer with a random valuation ====.====Consider first the case of just two goods, i.e., ====. When the valuations of the two goods are independent (i.e., ==== and ==== are independent random variables), Hart and Nisan (2017) showed that selling the goods separately—each one at its optimal one-good price—is guaranteed to yield at least 50% of the optimal revenue, a bound that was later improved to 62% by Hart and Reny (2017).==== This can be stated in terms of the “Guaranteed Fraction of Optimal Revenue” (====)==== as====How does this fraction change when the two goods need not be independent? Our first result is that it drops all the way down to zero:==== Indeed, we show that==== ====This suggests considering the other one-dimensional mechanism, namely, that of selling the two goods as a bundle. That does not help: the guaranteed fraction of optimal revenue is still zero; i.e.,==== In fact, even the larger class of all “deterministic” mechanisms—in which the seller sets a price for each good separately as well as a price for the bundle—does not fare any better:====This immediately extends to any number of goods ==== (just add ==== goods with zero valuation):====While these results (all of which are special cases of Theorem A) are new in the case of ==== goods, they have already been established for ==== goods in the related model of a ==== (instead of additive) buyer—i.e., a buyer who wants to get ==== of the ==== goods—by Briest et al. (2010/2015); the case of two goods was left open, with some partial results indicating that ==== may be bounded away from zero for ====. While the unit-demand model and our additive model are different, they are closely related: the various revenues in the two models are within constant factors of one another (see Appendix A.3 for precise statements). On the one hand, this implies that our result (2) for ==== goods follows from the above-mentioned result of Briest et al. (2015); on the other hand, our result (1) solves their open problem for ====: there is an infinite gap between the deterministic revenue and the optimal revenue in the unit-demand model, already for two goods.====What these results say is that allowing for probabilistic outcomes, where the buyer gets some goods with probabilities that are strictly between 0 and 1, makes a huge difference in terms of revenue. But is it really the probabilistic vs. deterministic distinction that matters here? A deterministic mechanism for ==== goods consists of setting prices for nonempty subsets of goods and thus provides to the buyer at most ==== nonzero outcomes to choose from. Suppose we were to limit the seller to provide the same number, i.e., ====, of outcomes, but allow these outcomes to be probabilistic; would that significantly increase the revenue? The answer is that it would not! As we will see, the guaranteed fraction of optimal revenue remains zero for ==== fixed bound on the number of outcomes.====Formally, we define the ==== of a mechanism to be the number of possible outcomes of the mechanism, where an outcome (or “menu entry”) specifies for each good ==== the probability ==== that it is allocated to the buyer, together with the payment ==== that the buyer pays to the seller====; it turns out to be convenient not to count the “zero” outcome of getting nothing and paying nothing (this outcome is always available, as it corresponds to the individual rationality or participation constraint). It is easy to see, and well known, that in our setting any mechanism can be put into the normal form of offering a fixed menu and letting the buyer choose among these menu entries. Notice that while deterministic mechanisms for ==== goods can have a menu size of at most ==== (since each ==== must be 0 or 1), randomized mechanisms can have an arbitrarily large, even infinite, menu size. Let ==== denote the optimal revenue achievable by mechanisms whose menu size is at most ====. For a single good, ====, the characterization of optimal mechanisms of Myerson (1981) implies that ==== is already the same as the optimal ====, but this is no longer true for more than a single good: the revenue may strictly increase as we allow the menu size to increase.====Our general result—Theorem A—is that for any fixed ====, mechanisms that have at most ==== menu entries cannot guarantee any positive fraction of the optimal revenue:==== for any number of goods ==== and any menu size ====. Thus, having a large set of possible outcomes—a large menu from which the buyer chooses, according to his valuation (or type)—seems to be the crucial attribute of the high-revenue mechanisms: it enables the sophisticated screening between different buyer types that is required for high-revenue extraction. As stated above, taking ==== yields result (2), which suggests that (2) is not driven by the mechanisms being deterministic, but rather by their being limited in the number of outcomes that they can offer.====Result (3) says that it does not matter exactly how “simple” mechanisms are defined; as long as their menu size is bounded (which is natural, as unbounded menu size can hardly be considered simple====), we have ====At this point, all simple mechanisms look equally “bad” when compared to the optimal revenue-maximizing mechanism. It does not however preclude some mechanisms from being better than others in terms of their revenues. This leads us to compare mechanisms by taking as a benchmark the simplest ==== revenue (rather than the optimal revenue), which we take to be ====, the revenue that is achievable from ==== take-it-or-leave-it offer (i.e., a single menu entry); as we will see in Section 3.3, this basic revenue turns out to be nothing other than the revenue from selling the bundle of all goods at its optimal price, which we denote by==== ====. Thus, given a mechanism ==== we define the “Multiple of Basic revenue” of ====, or ==== for short, to be the maximum, over all (relevant) valuations ====, of the ratio of the revenue that ==== extracts from ==== to the basic revenue ==== from ====. Thus, ==== measures how many times better the revenue from ==== can be relative to the basic revenue. The definition of ==== is then extended to classes of mechanisms by taking, as usual, the maximum over the mechanisms in the class.====Besides its natural meaning, the ==== measure turns out to be quite a useful tool for the analysis; in fact, most of the results here are obtained using ====. First, ==== turns out to be given by a simple explicit formula (see Theorem 5.1 in Section 5). Second, ==== is equivalent to having mechanisms with arbitrarily large ==== (see Proposition 3.4(i)); we construct such mechanisms making use of the explicit formula for ==== (see Sections 6 and 7). And third, for any class ==== of mechanisms with a finite ====—such as deterministic mechanisms, or mechanisms with bounded menu size—the result that ==== follows immediately from ==== (see Proposition 3.4(ii)). All these together prove Theorem A.====In addition, we show that the relation between ==== and menu size is polynomial==== (see Theorem C); that the ==== of deterministic mechanisms is exponential in the number of goods (specifically, for many goods, i.e., large ====, the ==== of deterministic mechanisms is essentially the same as the ==== of mechanisms with the same menu size, i.e., ====; see Theorem D); and, finally, that the ==== of separate-selling mechanisms is linear in the number of goods (specifically, it equals the number of goods ====; see Theorem E).====To summarize the contribution of this paper: in the context of selling multiple goods, where finding the optimal, revenue-maximizing, mechanism is an extremely difficult problem, we study what can be achieved by simple mechanisms. First, we introduce the concept of menu size, which, although just a simple and crude measure of the complexity of mechanisms, nevertheless turns out to be strongly related to their revenue-extraction capabilities (see for instance Theorem C, Theorem D). Second, we show that mechanisms having an arbitrarily large, even infinite, number of menu items are needed when maximizing revenue for two or more correlated goods, whereas mechanisms with bounded menu size may yield only an arbitrarily low fraction of the optimal revenue (see Theorem A). And third, we compare mechanisms in terms of how high a multiple of the basic revenue they can achieve, a comparison tool that turns out to be very useful throughout the whole analysis.",Selling multiple correlated goods: Revenue maximization and menu-size complexity,https://www.sciencedirect.com/science/article/pii/S0022053119300717,29 July 2019,2019,Research Article,233.0
"Baruch Shmuel,Glosten Lawrence R.","David Eccles School of Business, University of Utah, Salt Lake City, UT 84112, United States of America,Columbia Business School, Columbia University, New York, NY 10027, United States of America","Received 12 May 2017, Revised 23 April 2019, Accepted 17 July 2019, Available online 23 July 2019, Version of Record 25 July 2019.",https://doi.org/10.1016/j.jet.2019.07.008,Cited by (18),Perfect competition in liquidity provision in limit order markets is characterized by a tail expectation condition (,"Limit order book markets have all but replaced traditional exchanges. Accordingly, this market structure is of great interest to traders, regulators, and academics. A workhorse for studying limit order book markets is the conditional tail expectation condition (Rock 1990, Glosten 1994); in a market in which quoters face informed trade, this tail condition is a zero-profit condition which states that a limit price equals the conditional expected value of the asset, given the execution of the limit order.====This paper makes two contributions to the literature. The first is a negative result. In a static model, we demonstrate that having infinitely many liquidity suppliers is not sufficient to invoke the tail condition. What underlies our result is that the equilibria we find (====) are not symmetric and (====) are zero rent. Specifically, in each of the equilibria, infinitely many liquidity suppliers find it suboptimal to supply liquidity, whereas those who do supply liquidity break even. There are no barriers to entry in our model, the liquidity suppliers are identical, and the equilibrium notion is a standard static Nash equilibrium in schedules.====Whereas having infinitely many liquidity suppliers is thus insufficient to assume the tail condition, the second contribution of this paper is a positive result. We prove the existence of a sequence of Nash equilibria for which the tail condition emerges at the limit. That is, the equilibrium random marginal price schedule gets arbitrarily close to the competitive marginal price schedule. Thus, the analytically tractable tail condition can nevertheless serve as an approximation for prices in limit order markets.====We are not the first to provide strategic foundations for the tail condition. Biais et al., 2000, Biais et al., 2013 and Back and Baruch (2013) solve models of strategic competition in liquidity provision in limit order book markets in which the convergence result holds. Our model complements these earlier models in that we study the environment in which the demand of uninformed traders is random and inelastic. Importantly, the type of equilibria we study is different: Here, the equilibria are in mixed strategies. Kyle (1989) and Vives (2011) also study competition in schedules with private information. However, these authors focus on a trading platform that uses a single price to clear the market. Although many financial markets use a single-price auction to open (and close) the trading day, we focus on a limit order market. More generally, our model adds to the literature that provides strategic foundations to the competitive equilibrium with rational expectations; see Milgrom (1981), Rustichini et al. (1994), Pesendorfer and Swinkels (1997), Kremer (2002), and Reny and Perry (2006).====Our model is closely related to that of Dennert (1993), who models a dealers' market in an environment similar to ours, and finds the equilibrium in mixed strategies. In Dennert's model, however, dealers compete in prices (quantities are exogenous). Notably, Dennert's equilibrium does not converge to the competitive outcome. Rather, it converges to something like monopoly pricing but with zero rents because an arbitrarily large size is offered. It was this result that partly motivated our handling of the question.====We note that the analysis of our equilibria suggests an empirical regularity and one that is different from that implied by the Dennert model. Other things equal, a high number of liquidity suppliers implies smaller orders and hence more transactions. In principle, empirical work might establish which is the most reasonable and, hence, whether or not it is appropriate to use the competitive solution as an approximation.====Our model is also related to the work on adverse selection with nonexclusivity, namely the papers by Attar et al. (2014) and, most relevantly, Attar et al. (2019), who analyze pure-strategy equilibria in a limit order book with a finite number of types (a type is identified with a valuation) and no noise traders. In contrast to Biais et al., 2000, Biais et al., 2013 and Back and Baruch (2013), who derive a pure-strategy equilibrium with transactions at many prices, Attar et al. (2019) conclude that only the informed trader with the most extreme valuation will transact at an extreme price.====Our model is an example of a game with discontinuous payoffs. Here, the discontinuity in payoffs is due to the discriminatory nature of limit order markets: Offering shares at a price marginally lower than the opponent's offering price is dramatically different from offering the shares at a price marginally higher, because the former implies a greater chance of trading with uninformed noise traders. Many games with discontinuous payoffs fail to possess equilibria in pure strategies. To see how an equilibrium in pure strategies can fail to exist in our model, consider a liquidation value that has unbounded support, and assume that the noise demand has the least upper bound ====. In a pure-strategy equilibrium, no more than ==== shares can be offered. A profitable deviation for anyone who offers shares is to offer those shares at infinity. This deviation generates unbounded expected profits. Clearly, this argument is an artifact of the assumption that noise traders are willing to pay any price. However, this is by far the most common setup in the literature. This is particularly true of the less quantitative, more policy-oriented literature in which the noise trader model is far easier to understand. This being the case, it is all the more important to verify that policy insights derived from the competitive model are right because the model is approximately right.====Interestingly, the equilibria we study here share similarities with those found in the Bertrand–Edgeworth literature. Specifically, in that literature, producers choose capacities in pure strategies and randomize the prices (Tirole 1988). Although in our model, competition is in schedules, in the equilibria we find, active liquidity suppliers choose deterministic quantities and randomize prices. Our equilibria are also similar to those in the Bertrand–Edgeworth literature in that the entire demand may not be satisfied at low prices.====An important goal of the Bertrand–Edgeworth literature is to provide a strategic foundation for competitive pricing. Specifically, Allen and Hellwig (1986a), Allen and Hellwig (1986b), and Vives (1986) demonstrate the convergence of mixed-strategy equilibria, as the number of suppliers becomes large, to the competitive deterministic price. Our paper extends this literature to a case in which competition is in schedules; the demand is random; suppliers face adverse selection; and the limit is a deterministic competitive schedule, rather than a single competitive price.====The rest of this paper is organized as follows. In Section 2, we define the tail condition, and show that in the economic environment considered in this paper, there exists a unique marginal price schedule that satisfies the tail condition. In Section 3, we define a strategic game in liquidity provision. In Sections 4 and 5, we demonstrate the existence of a sequence of Nash equilibria in mixed strategies in which a finite number of liquidity suppliers offer shares (quotes), and the quantities each quoter offers decrease with the number of quoters. In Section 6, we show that these quotes pile up just above the quotes implied by the tail condition, which is our convergence result. In Section 7, we provide a counterexample to our convergence result; the counterexample is based on Dennert (1993). In Section 8, we show how the model can be extended. In Section 9, we conclude.",Tail expectation and imperfect competition in limit order book markets,https://www.sciencedirect.com/science/article/pii/S0022053119300730,23 July 2019,2019,Research Article,234.0
"Straub Ludwig,Ulbricht Robert","Harvard University, Cambridge, MA, USA,National Bureau of Economic Research, Cambridge, MA, USA,Boston College, Newton, MA, USA,Toulouse School of Economics, University of Toulouse Capitole, Toulouse, France","Received 15 June 2016, Revised 22 June 2019, Accepted 14 July 2019, Available online 19 July 2019, Version of Record 24 July 2019.",https://doi.org/10.1016/j.jet.2019.07.007,Cited by (2),"We explore a mechanism by which second moments—such as cross-sectional dispersions, risk, volatility, or uncertainty—naturally and endogenously fluctuate over time as ","Many important statistics in macroeconomics and finance—such as cross-sectional dispersions, risk, volatility, or uncertainty—are ====. For example, dispersions can be measured by the cross-sectional variance, risk by the variance across future states with an objective probability measure, volatility by the variance of the realized path over time, and uncertainty by the variance across unknown states with respect to a possibly subjective probability measure. The recent financial crisis was a stark reminder that these second moments are nowhere near constant over the business cycle and allowing them to vary can help explain the economic fluctuations during the crisis.====In this paper, we explore a mechanism by which second moments can naturally and endogenously fluctuate across states or time, when economically interesting variables are ==== transformations of some fundamental. Specifically, we consider a general setup where there is a fundamental shock ==== whose realization is either different across economic regions or agents (dispersion), not realized yet (risk), varies over time (volatility), or unknown to agents (uncertainty). We distinguish between the variance of ====, which defines the second moment of the fundamental, and the variances of certain nonlinear transformations of ====, which characterize the second moments of interest to us.====In standard stochastic business cycle models, for instance, the variables of economic interest are commonly approximated as linear (or linearized) functions of the fundamentals. In order to use these models to explain movements in second moments of endogenous variables one must therefore rely on exogenous shocks to the second moments of fundamentals. Moreover, to capture the apparent cyclicality of the second moments of these variables, the exogenous shocks to second moments need to be correlated with the corresponding first-order shocks to fundamentals. Here, we go another route and consider a setting where some variable of interest, ====, is a convex or concave function of the fundamental ====.==== We provide general theorems that characterize the behavior of the variance of ==== as the distribution of ==== shifts up or down, ==== changing the fundamental variance of ====. Our framework hence provides a theoretical underpinning for a class of models where a single “first-moment” change in fundamentals causes fluctuations in first ==== second moments in endogenous variables.====  The usefulness of our results is illustrated in a series of four applications. The main application is a stylized business cycle model, in which we take ==== as the productivity of distinct economic units (e.g., firms, plants, or regions). We study how aggregate, ==== shifts of the distribution of ==== across these units (i.e., fluctuations in aggregate productivity) translate into endogenous fluctuations in the cross-sectional dispersion of key macroeconomic quantities, such as output, employment, investment, and Solow residuals. The novel feature in this application is the focus on non-unit elasticities between factor inputs at the firm level, which is key for generating a non-linear mapping from productivities to economic activity. In line with empirical cross-sectional patterns, the dispersions of output, employment and Solow residuals are shown to be countercyclical when employment and capital are gross complements. A simple calibration of the input elasticity to recent micro-data estimates suggests that this mechanism can account for a significant share of the empirical observed cyclical variation in various dispersion measures—without exogenous “volatility”-shocks.====The second application looks at a prominent proxy used by a recent literature to measure misallocation, the marginal revenue product of capital (MRPK), and explores how it changes with different shocks. We consider a simple model where firms face idiosyncratic borrowing constraints and are hit by idiosyncratic productivity and financial shocks. As expected, financial shocks lead to ====-cyclical fluctuations in the cross-sectional dispersion of MRPKs. In contrast, we show that productivity-driven fluctuations lead to ====-cyclical dispersions whenever the elasticity of borrowing limits to firm revenues is smaller than one.====The third application is a simple security market model where we explore how the comovement of a security's risk with the underlying fundamental depends on the security's payoff profile. In this context, we generalize the following two well-known results for a general class of underlying risk distributions: (i) concavely increasing securities (e.g., corporate debt) have return risk that is countercyclical to the underlying state of the corporation; and (ii) convexly decreasing securities (e.g., European Put options) have procyclical return risk.====Our final application illustrates how uncertainty in Bayesian inference problems can vary endogenously when signal structures have some degree of “non-linearity”. In particular, we study a setup where agents receive a signal about some non-linear transformation ==== of the fundamental ====. Such non-linear transformation arises, for instance, when agents learn from financially constrained firms about their business potential. In this setup, when ==== realizes in a range where ==== tends to be rather flat (e.g., firms are constrained), the signal endogenously loses some of its information content. This way, posterior uncertainty fluctuates with the realization of the signal and is thus determined by both, the fundamental ==== and the exogenous noise in the signal itself.====Together, these applications illustrate how our results can be used to provide a unified perspective to explain endogenous fluctuations in risk, dispersion, and uncertainty. While Application 3 is fairly standard, Applications 1, 2, and 4 describe new perspectives on recent results in their respective literatures. For example, Application 1 contributes to the recent literature on the cyclicality of the dispersion of firm- or plant-level statistics.==== In particular, it complements Ilut et al. (2016) who develop a model where asymmetries in hiring and firing based on ambiguity aversion microfound nonlinear responses of firms to changes in productivities. These nonlinearities play a similar role to the nonlinearities emerging in our application from a non-unit elasticity between factor inputs in that they introduce cyclical shifts in the dispersions of firm aggregates. Application 2 contributes to recent studies on the cyclicality of misallocation of capital (see, e.g., Kehrig 2015 and Gopinath et al. 2015), in that it discusses—based on a simple static framework with heterogeneous borrowing constraints—under what conditions the dispersion of the marginal revenue product of capital can be expected to be cyclical. Finally, Application 4 introduces a general result about the cyclicality of Bayesian uncertainty when signals are nonlinear functions of the economy's fundamental. Theories about the cyclicality of uncertainty have recently gained attention by a number of authors in the face of growing evidence and potential relevance of changes in uncertainty (see, e.g., van Nieuwerburgh and Veldkamp 2006; Orlik and Veldkamp 2014; Straub and Ulbricht, 2012, Straub and Ulbricht, 2014; Fajgelbaum et al. 2017).====  To be as broadly applicable as possible, the theory part of our paper is kept in a general and abstract form: Letting ==== and ==== be real-valued random variables with equal variance, we compare the variances of ==== and ====, where ==== is a monotone and convex or concave function.====Our results are easiest seen in the special cases where ==== is either strictly larger than ====—by which we mean the support of ==== exceeds the support of ==== without overlap—or when the distribution of ==== is simply a positive translation of the distribution of ====. Then as one would expect, the variance of ==== (weakly) exceeds the variance of ==== if ==== is convexly increasing or concavely decreasing. However, these cases are quite special and may not translate well to real world examples.====A natural question hence is whether and when these results carry over to the general case where the supports of ==== and ==== are allowed to overlap and where the distributions do not have the same parametric shape. As it turns out, the answer is less straightforward than one may think. For instance, it is possible to construct examples where ==== first-order stochastically dominates ====—that is, the cumulative distribution function of ==== is strictly below the one of ====—yet the variance of ==== is smaller than the one of ====, despite ==== being an increasing convex function (and ==== and ==== having equal variance).====The main theoretical result in the paper states that when ==== dominates ==== according to the monotone likelihood ratio property (MLRP)—that is, the ratio of the densities of ==== and ==== is increasing—but shares the same variance as ====, then the variance of ==== indeed exceeds the variance of ==== if ==== is convexly increasing or concavely decreasing.==== This gives us a precise notion of when positive shifts in an underlying fundamental, say ====, translate into positive shifts to the second moment of a transformed variable ====, namely when ==== is convexly increasing or concavely decreasing.====In addition to the non-parametric characterization in our main result, we provide a simple elementary proof for the above mentioned case where ==== is a translation of ====. Building on this proof, we further extend our results to the case where ==== and ==== are not necessarily similar in shape yet their transformations ==== and ==== are linked via an affine-linear relation==== with ====.====It is worth noting here that our results have nothing in common with Jensen's inequality.==== Instead, the correct analogy is to the known transformation properties of first moments. Specifically, it is well-known that two random variables ordered by MLRP have means ordered the same way, and this property is preserved under increasing transformations. In analogy to these known results on first moments, our results imply that for two MLRP-ordered random variables, the order of variances is preserved under increasing convex and decreasing concave transformations====In relation to the literature, these results substantially extend existing theoretical results on the transformation behavior of variances under monotone concave or convex transformations. To the best of our knowledge, the most general predecessor of our results is found in Bartoszewicz (1985). In the above notation, Bartoszewicz (1985, Theorem 1) proves that if ==== dominates ==== according to first order stochastic dominance (FOSD) and a convex stochastic order (Van Zwet, 1964),==== then a similar result to ours holds, namely that ==== for any convexly increasing function ====. Compared to Bartoszewicz (1985), our main result has the advantage that our stochastic orders (MLRP and ====) are in most applications easy to check while it can be hard to work with the convex stochastic order in Bartoszewicz (1985), especially in cases where ==== is of a different parametric shape than ==== (i.e., ==== is not a simple translation of ====).====  The layout of this paper is as follows: In Section 2 we first introduce the necessary mathematical setup and then show how it can be used to prove our theoretical results. Section 3 develops the main application on cross-sectional dispersions. Section 4 contains the remainder applications. Section 5 concludes. All proofs are contained in Appendix A, Appendix B, and Appendix C.","Endogenous second moments: A unified approach to fluctuations in risk, dispersion, and uncertainty",https://www.sciencedirect.com/science/article/pii/S0022053119300729,19 July 2019,2019,Research Article,235.0
Davoodalhosseini Seyed Mohammadreza,"Bank of Canada, 234 Wellington Street, Ottawa, Ontario, K1A 0G9, Canada","Received 8 February 2018, Revised 14 June 2019, Accepted 7 July 2019, Available online 12 July 2019, Version of Record 17 July 2019.",https://doi.org/10.1016/j.jet.2019.07.005,Cited by (6),This paper studies constrained efficiency in ,"Guerrieri et al. (2010), GSW hereafter, characterize the equilibrium of a model with adverse selection and directed (competitive) search. They present several examples in which they show that the equilibrium fails to achieve the first best.==== A natural and important question is whether it is possible to achieve “better” allocations than the market. To address this question, I study constrained efficiency in GSW's environment (albeit with transferable utility) using a mechanism design approach.====The model economy is populated by a fixed measure of sellers who have private information about their type. A large number of buyers can enter the market by incurring a cost. Buyers and sellers have transferable utility, match bilaterally and trade in different locations, called submarkets. In each submarket, there are search frictions in the sense that buyers and sellers on both sides are matched generally with probability less than one.====To study constrained efficiency, I introduce a planner who allocates resources subject to the frictions of the environment, but he can impose submarket-specific taxes and subsidies given a budget-balance condition (i.e., cross-subsidization). Cross-subsidization helps the planner to relax incentive constraints. In the first two theorems, I show that the planner can achieve ==== allocations than the GSW equilibrium using cross-subsidization. In the next two theorems, I characterize the ==== (i.e., welfare-maximizing) allocation that the planner can achieve.====I focus on the cases where adverse selection distorts the equilibrium allocation, i.e., where the GSW equilibrium does not achieve the first best. Then, I show in Theorem 1 that the planner achieves strictly higher welfare than the equilibrium, i.e., the equilibrium is not ex-ante constrained efficient. I show in Theorem 2 that if the measure of lower types is sufficiently small, then the planner can achieve an allocation that Pareto dominates the equilibrium, i.e., the equilibrium is not interim constrained efficient.====I next characterize the ex-ante–constrained–efficient allocation. In Theorem 3, I provide sufficient conditions for the planner to achieve the first best. Under these conditions, the planner can completely undo the effects of adverse selection by using an appropriate set of transfers. In Theorem 4, I provide necessary conditions for the ex-ante–constrained–efficient allocation. This result is useful for the environments in which the sufficient conditions of Theorem 3 do not hold, because it is usually easy in economic applications to pin down the ex-ante–constrained–efficient allocation from these necessary conditions.====The key to these results is cross-subsidization. However, finding a welfare-improving cross-subsidization scheme is challenging in this environment, because any cross-subsidization scheme affects many incentive compatibility (IC) constraints. To find such a scheme, I build on GSW's recursive method of constructing equilibrium. To characterize the equilibrium allocation for a given type, GSW show that it is sufficient to ensure that lower types do not gain by reporting a higher type (i.e., considering only upward IC constraints is sufficient). It follows that higher types ==== prefer to report their own type rather than a lower one (i.e., downward IC constraints are weakly satisfied). I show that higher types ==== prefer to report their own type rather than a lower one. That is, downward IC constraints are not binding in the equilibrium. As a result, if lower types are equally subsidized by a sufficiently small amount, then none of the IC constraints are violated.====To construct an allocation with higher welfare than the equilibrium allocation, take a type ==== whose payoff in the equilibrium is less than his payoff in the first-best allocation. The planner subsidizes types ==== by a small amount so that IC constraints are not violated. As a result, type ==== can create more surplus due to the relaxation of upward IC constraints of the lower types. To finance these subsidies, the planner needs to levy only a small lump sum tax on all types ====. Hence, the participation constraints of agents are not violated either. Therefore, this allocation respects all the constraints that the planner faces (budget-balance, participation and IC constraints). Welfare in this allocation is higher than welfare in the equilibrium allocation, as type ==== creates more surplus and other types continue to create the same amount of surplus.====To construct an allocation that Pareto dominates the equilibrium allocation, the same schedule of subsidies is used, but they are now financed by taxing only types ====. If the level of improvement in type ===='s payoff resulting from the relaxation of IC constraints is greater than the amount of tax that type ==== should pay, then this allocation Pareto dominates the equilibrium allocation. This happens if the aggregate measure of types ==== is sufficiently smaller than the measure of type ====. Indeed, type ==== sellers in this case collectively prefer to subsidize all lower types to reduce the cost of screening.====To illustrate my results in a special case of my model, I study constrained efficiency in a two-type asset market with lemons. Each seller has one indivisible asset. The high-type asset is more valuable to both buyers and sellers. In the unique equilibrium, which is separating, sellers with a high-type asset strictly prefer to trade in the submarket with a higher price but lower probability of finding a buyer. Sellers with a low-type asset, conversely, sell their assets in the submarket with a lower price but higher probability of finding a buyer. These low-type sellers are indeed indifferent between the two submarkets. The welfare-maximizing cross-subsidization scheme is to subsidize trade in the low-price submarket and tax trade in the high-price one so that low-type sellers ==== prefer the former to the latter. Since the upward IC constraint of low-type sellers is now relaxed, more buyers are willing to enter the high-price one, increasing the chances of high-type sellers to meet a buyer. Welfare increases as the volume of trade increases compared with the equilibrium allocation. Furthermore, if the fraction of low-type sellers in the population is sufficiently small, then the same cross-subsidization scheme can be used to construct an allocation that Pareto dominates the equilibrium.====My paper is related to the literature on directed search,==== specifically those studies using a mechanism design approach. As mentioned earlier, the closest paper is GSW's. They characterize the equilibrium with adverse selection and directed search and show existence and uniqueness (in terms of payoffs). I study constrained efficiency in GSW's environment and show that when adverse selection has a bite, the GSW equilibrium is ex-ante and sometimes interim constrained inefficient. While the existing literature acknowledges some of the inefficiencies of the GSW equilibrium, my paper is the first that formally establishes these inefficiencies. Golosov et al. (2013) study a model with directed search and moral hazard. The information asymmetry in their paper is not about gains from trade; rather, it is about workers' search decisions. Guerrieri (2008) and Moen and Rosén (2011) study constrained efficiency in environments with directed search and private information. Unlike in my paper, the agents who search (workers) in their paper do not have ex-ante private information; rather, their information is match-specific and realized only after they are matched with firms.====My paper is also related to the adverse selection literature, especially to Rothschild and Stiglitz (1976), Wilson (1977), Holmström and Myerson (1983) and Maskin and Tirole (1992). This literature makes extensive use of the idea that cross-subsidization helps to relax incentive constraints (e.g., Miyazaki (1977) and Spence (1978)). If one incorporates the matching probability into the payoff function of agents and treats it as another dimension of the contract space, my environment reduces to a standard model in this literature with no restrictions on the number of types or dimension of contract space or concavity of payoff functions. My contributions to this literature are to provide exact conditions under which the least-cost separating allocation (GSW equilibrium in this environment) is ex-ante constrained inefficient, and to characterize how the planner can improve efficiency relative to the separating allocation or achieve the first best. I am not aware of any paper that provides such a thorough efficiency analysis.==== My results also shed light on the way that cross-subsidization should be used; I specify who should be taxed and who should be subsidized.====The paper is organized as follows. In Section 2, I develop the environment of the model and define the planner's problem. I give the main results in Section 3 and characterize constrained efficient allocations in a two-type asset market application in Section 4. I use this application to illustrate why agents in the market economy fail to internalize the externalities that exist in this environment and discuss how the planner can allocate resources more efficiently than the market economy. In Section 5, I finish with concluding remarks. In Appendix A, I mention two points regarding GSW's environment and discuss their restrictions on off-the-equilibrium-path beliefs. In Appendix B, I include all the proofs not discussed in the text. In Appendix C, I study a labor market application. This is a version of the rat race originally studied by Akerlof (1976). In this application, the planner achieves the first best regardless of the relative size of low-type and high-type workers.",Constrained efficiency with adverse selection and directed search,https://www.sciencedirect.com/science/article/pii/S0022053119300705,12 July 2019,2019,Research Article,236.0
"Nosal Ed,Wong Yuet-Yee,Wright Randall","FRB Atlanta, United States of America,Binghamton University, United States of America,Zhejiang University, China,University of Wisconsin-Madison, United States of America","Received 15 August 2017, Revised 28 February 2019, Accepted 1 July 2019, Available online 11 July 2019, Version of Record 13 August 2019.",https://doi.org/10.1016/j.jet.2019.07.001,Cited by (5),"We analyze agents' decisions to act as producers or intermediaries using equilibrium search theory. Extending previous analyses in various ways, we ask when intermediation emerges and study its efficiency. In one version of the framework, meant to resemble retail, middlemen hold goods, which entails (storage) costs; that model always displays uniqueness and simple transition dynamics. In another version, middlemen hold assets, which entails negative costs, i.e., positive returns; that model can have multiple equilibria and complicated belief-based dynamics. These results are consistent with the venerable view that intermediation in financial markets is more prone to instability than in goods markets.","This paper studies intermediation in markets for goods and markets for assets, building on the search-and-bargaining framework of Rubinstein and Wolinsky (1987), hereafter RW. The analysis extends existing versions of RW on several dimensions, and, in particular, endogenizes market composition by letting agents choose to act as either middlemen or producers. A version of the model designed to resemble goods markets, in a simple but reasonable way, has a unique equilibrium under standard assumptions; a version designed to resemble asset markets, under similar assumptions, can have multiple equilibria that are ranked in terms of efficiency. It is shown that for this result we must have the sets of producers and middlemen endogenous – if they are exogenous uniqueness obtains in markets for assets as well as goods.====As background, first note that the original RW environment has no cost of production or search, equal numbers of producers and consumers, symmetric bargaining, and a fixed number of middlemen. In equilibrium middlemen participate in the market iff they have a better search technology than producers, as is efficient. In Nosal et al. (2015) we extended this to allow more general bargaining and costs, but that is relatively straightforward because, following RW, we maintained linear utility, only considered steady states, restricted inventories to ====, and kept the sets of producers and middlemen fixed. This paper relaxes all of those restrictions, with the key generalization being that agents decide to act as producers or middlemen, and that requires a rather different approach.====We then distinguish between markets for goods and markets for assets as follows: as in retail markets, holding inventories of goods entails a storage cost; holding inventories of assets instead entails a positive return, or a negative cost, as is potentially the case with houses, art, productive capital, etc. Now there can also be costs to safely storing assets, but it is worth considering the case where the net return is positive, because it turns out that is necessary (not sufficient) for generating multiple steady states and endogenous dynamics. Of course, it is well known that one can generate multiplicity and dynamics in search theory with a variety of other devices, e.g., increasing returns in matching or production technologies (Diamond, 1982, Mortensen, 1999). We eschew those devices to concentrate on something new, complementarities in decisions by middlemen to adopt either buy-and-hold or buy-and-sell strategies.====Our producers generate goods, or assets like capital, and trade them to end users. They may also trade them to middlemen, who may or may not trade them to end users. With goods as defined here (storage has a cost) the trading decision of a middleman meeting an end user is trivial, because buy-and-hold strategies cannot be optimal with negative returns. With assets as defined here (storage has a positive return) the decision is not trivial. To motivate why this is interesting, our asset market intermediaries can be interpreted in a stylized way as financial institutions, acquiring capital from originators and choosing when to pass it on. Our results thus provide support for the notion that financial institutions are less stable than other intermediaries, since we get multiplicity and belief-based volatility in intermediated markets for assets but not goods.====For the intuition, suppose first that middlemen pass capital inventories on to end users. Then lots of middlemen will be searching for new capital, leading to high producer profit and hence many producers. With more producers in the market it is easier for middlemen to get capital, thus rationalizing their decision to trade it away and making active intermediation an equilibrium for some parameters. Now suppose middlemen keep capital for themselves. Then they are more likely to already have capital, leading to lower profits and fewer producers. This makes it harder for middlemen to get capital, thus rationalizing their decision to not trade it away in another equilibrium for the same parameters. Moreover, the equilibria can be welfare ranked – having middlemen trade with end users is better. Again, this can only arise when the return on inventories is positive and when the composition of the market is endogenous.====The rest of the paper is organized as follows. Section 2 describes a general benchmark environment. Then we analyze markets for goods and markets for assets separately, the former in Sections 3-4, and the latter in Sections 5-7. Section 8 discusses robustness, including, in particular, an extension that allows general inventories, while the baseline model restricts inventories to ====.==== Section 9 contains concluding remarks. Extensions and proofs are in the Appendices.",Intermediation in markets for goods and markets for assets,https://www.sciencedirect.com/science/article/pii/S0022053119300663,11 July 2019,2019,Research Article,237.0
"Araujo Aloisio,Gama Juan Pablo,Novinski Rodrigo,Pascoa Mario R.","IMPA, Estrada dona Castorina, 110, Rio de Janeiro, 22460-320, Brazil,FGV EPGE Brazilian School of Economics and Finance, Praia de Botafogo, 190, Rio de Janeiro, 22250-900, Brazil,Department of Economics, Federal University of Minas Gerais, Av. Antônio Carlos, 6627, Belo Horizonte, 31270-901, Brazil,Faculdades Ibmec, Av. Presidente Wilson, 118, Rio de Janeiro, 20030-020, Brazil,School of Economics, University of Surrey, 388 Stag Hill, Guildford, GU2 7XH, United Kingdom","Received 11 July 2016, Revised 26 June 2019, Accepted 4 July 2019, Available online 10 July 2019, Version of Record 17 July 2019.",https://doi.org/10.1016/j.jet.2019.07.004,Cited by (1),"When the discount factors that infinite lived consumers use at each date are not predetermined but are instead chosen within some set, depending on what the consumption plan is, impatience might not hold. More precisely, if the utility is the infimum of discounted utilities over that set of discount factor sequences, then preferences may be just upper semi-impatient. Such lack of lower semi-impatience, which we refer to as ",None,"Endogenous discounting, wariness, and efficient capital taxation",https://www.sciencedirect.com/science/article/pii/S0022053119300699,10 July 2019,2019,Research Article,238.0
Gorno Leandro,"FGV EPGE, Praia de Botafogo 190/1119B, Rio de Janeiro, RJ 22250-900, Brazil","Received 4 July 2017, Revised 24 June 2019, Accepted 3 July 2019, Available online 10 July 2019, Version of Record 30 July 2019.",https://doi.org/10.1016/j.jet.2019.07.003,Cited by (3),"This paper studies preference identification in a general framework that allows for partial observability of optimal choices: Decision makers select some optimal alternatives, but not necessarily all of them. While partial observability is a methodologically appealing assumption for empirical applications, it makes recovering preferences much harder. The main result provides abstract conditions on classes of preferences and decision problems ensuring identification. The result is applied to several standard settings demonstrating the power of the method.","This paper examines the problem of recovering preferences from empirically observed choices when decision makers behave optimally. The main contribution is the formulation of reasonable theoretical restrictions that uniquely identify preferences in a general setting. Most significantly, identification is achieved without assumptions on how decision makers select among indifferent alternatives.====Identification is not always possible because observed decisions may be consistent with different preferences. For example, consider a couple trying to decide whether to move to Paris or stay in New York. Ann prefers to move, while Bob is indifferent. Suppose that either Ann or Bob makes the choice, but only the outcome is observed. Clearly, if the couple stays in New York, it would be safe to conclude that Bob made the decision, because Ann would have preferred to move. In this sense, staying in New York “reveals” that Bob's preference is the one that counted. However, if they move to Paris, the evidence available is insufficient to infer who determined the outcome.====The possibility of identifying preferences from observed behavior depends on the class of preferences being entertained. For example, suppose a voter wants to infer a politician's type by looking at his or her past policies. Assume first that the politician can only be “left-wing” or “right-wing.” There are two policies available: “liberal” and “conservative.” Left-wing politicians strictly prefer the liberal policy to the conservative policy, and vice versa. In this case, any past policy uniquely identifies the politician's type. Next, consider an additional type of politician, a “pragmatic,” who is indifferent between the liberal and the conservative policies. Including this third type makes potential inferences less conclusive. A liberal policy rules out a right-wing politician, but does not determine whether the politician is left-wing or pragmatic. A similar indeterminacy remains when multiple previous policy choices are observed: The only type of politician that can be learned from the data is the pragmatic (after observing a mixture of liberal and conservative policies).====As the aforementioned examples illustrate, indifference poses a problem to identification inasmuch as it enlarges the set of theoretical preferences that can be confounded for a given dataset. The traditional axiomatic approach to abstract revealed preference simplifies this issue by effectively assuming that ==== alternative the decision maker ==== have chosen is observed.==== However, this approach is unsatisfactory from a methodological viewpoint, because it transmutes what is supposed to be empirical data (i.e., the decision maker's observed choices) into a theoretical object (i.e., the set of preference-maximizing alternatives).====In contrast, this paper develops a general framework for revealed preference analysis that affords a cleaner separation between the theories being entertained and the data intended to evaluate them. This is attained by relaxing informational assumptions while keeping the basic logic of revealed preference intact: What the decision maker chooses cannot be worse than anything else she could have chosen; unchosen options, however, are not presumed to be suboptimal. Following Chambers et al. (2014), I refer to this approach as “partial observability.”====Once partial observability is adopted, the fundamental challenge is to obtain identification. This is because finding preferences consistent with the available evidence becomes relatively easy. In fact, if the only assumptions on preferences are completeness and transitivity, rationalization under partial observability is trivial: Any collection of observations is rationalized by universal indifference. At the same time, this lack of restrictions makes identification harder. Of course, it is possible to obtain restrictions on observable choice behavior, and even identification, by limiting admissible preferences.====The central contribution of this paper is its proposal of general theoretical assumptions that are sufficient for identification under partial observability. I then use this framework to study important classes of preferences including several types of ordinal continuous preferences, preferences satisfying the von Neumann–Morgenstern (vNM) axioms, and qualitative probabilities. Since indifference is easily disciplined within these classes, relatively weak additional conditions suffice to uniquely recover any preference driving behavior, provided that enough data is observed.====It should be stressed that departing from the axiomatic approach to relax full observability not only constitutes a methodological improvement but also has significant theoretical consequences. For example, under partial observability, choice correspondences are naturally interpreted as encoding empirical datasets. Under this interpretation, preference maximization on its own does not imply the celebrated axioms of revealed preference. Therefore, violations of these axioms are not necessarily evidence of irrational behavior: They might merely reflect the limits of the available data.====Overall, the message of this paper is somewhat comforting: It is possible to apply revealed preference analysis to interesting theories of behavior, while keeping the theoretical assumptions employed to obtain identification separate from the empirical data.",Revealed preference and identification,https://www.sciencedirect.com/science/article/pii/S0022053119300687,10 July 2019,2019,Research Article,239.0
"Dillenberger David,Raymond Collin","Department of Economics, University of Pennsylvania, USA,Department of Economics, Purdue University, USA","Received 22 December 2017, Revised 22 June 2019, Accepted 3 July 2019, Available online 10 July 2019, Version of Record 15 July 2019.",https://doi.org/10.1016/j.jet.2019.07.002,Cited by (0),"Individuals often tend to conform to the choices of others in group decisions, compared to choices made in isolation. We show that this behavior — which we term the consensus effect — is equivalent to a well-known violation of expected utility, namely strict quasi-convexity of preferences, which is shared by many popular non-expected utility models. In contrast to the equilibrium outcome when individuals are expected utility maximizers, quasi-convexity of preferences imply that group decisions may fail to properly aggregate preferences and strictly Pareto-dominated equilibria may arise.","Group decision-making is ubiquitous in social, economic, and political life. Empirical evidence suggests that individuals tend to make different choices depending on whether the outcome of interest is a result of their choice alone or also the choice of others in a group. In particular, the existing evidence largely supports the idea that these choice shifts in groups, which are prominent in a variety of contexts across fields, are predicted by the expected choice of the majority of individuals. The phenomena that have been documented include the bandwagon effect in political science (e.g., Goidel and Shields, 1994; Niemi and Bartels, 1984, and Bartels, 1988); risky and safe shifts studied by psychologists (e.g., Brown, 1986, Stoner, 1961, AF Stoner, 1968, Nordhöy, 1962; and Pruitt, 1971); and severity and leniency shifts in legal studies (Schkade et al., 2000, Sunstein et al., 2002, Sunstein, 2005). As an influential early article in sociology by Granovetter (1978) summarized it, “collective outcomes can seem paradoxical — that is intuitively inconsistent with the intentions of the individuals who generate them.”====Models of group decisions typically analyze either private-value or common-value settings. Because, as will be explained below, with expected utility preferences in a private-value setting we should not observe choice shifts, much of the literature exploring choice shifts has focused on the common-value setting. In this context, group decisions aggregate private information regarding the relative value of possible outcomes.==== In contrast, in this paper we maintain a private-value setting, but relax the assumption of expected utility. In particular, we show that well-known violations of expected utility can explain these commonly observed choice shifts, even in settings without private information.==== Thus, our paper joins a literature discussing how relaxations of the main assumptions of expected utility can have important implications for behavior in strategic situations, as in auctions (Karni and Safra, 1989, Neilson, 1994, Nakajima, 2011, Baisa, 2013, Eisenhuth, 2019), pricing by firms (Heidhues and Kőszegi, 2008, Heidhues and Kőszegi, 2014, Carbajal and Ely, 2016, Rosato, 2016), and incentives schemes (Herweg et al., 2010, Carbajal and Ely, 2012).====To see why a violation of expected utility may generate choice shifts in groups, note that an individual choice in a group decision matters only when that individual is pivotal, that is, when his vote actually changes the outcome. However, from an ex-ante perspective, when choosing for which option to vote, an individual does not know whether or not he will be pivotal. Thus, his choice is not a choice between receiving Option 1 or Option 2 for sure, but rather between ==== defined over these two options — where if the individual turned out to be pivotal his selected option will be implemented, and otherwise the probability of each alternative to win depends on the probability that the group chooses it conditional on him not being pivotal. Violations of the independence axiom of expected utility imply that an individual may prefer Option 1 to Option 2 in isolation, yet prefer the lottery induced in the group context by choosing Option 2 over the one induced by choosing Option 1, thus accounting for the aforementioned choice shift.====In Section 2 we formally link violations of expected utility with the phenomenon of choice shifts in groups. In doing so, we provide a relationship between two types of non-standard behavior, one observed at the individual level and one at the group level. Our first result states that individuals have preferences that are strictly ==== in probabilities if and only if they will systematically exhibit what we call a ==== — an individual who is indifferent between two options when choosing in isolation will actually strictly prefer to vote for the option that is sufficiently likely to be chosen by the group. As discussed, the consensus effect captures the stylized fact that in group contexts individuals want to exhibit preferences that match those of the group as a whole. Consistent with the predictions of our model, Agranov et al. (2017) find evidence that individuals are more likely to vote for an outcome if they perceive it as more likely to win. Quasi-convexity, on the other hand, is a well established preference pattern in decision making under risk, according to which individuals are averse toward randomization between equally good lotteries.==== Popular models of preferences over lotteries which can exhibit quasi-convexity include rank-dependent utility (Quiggin, 1982, hereafter RDU), quadratic utility (Chew et al., 1991), and Kőszegi and Rabin (2007)'s choice acclimating personal equilibrium model of reference-dependence. Moreover, as observed by Machina (1984), quasi-convexity occurs if, as in common in many applications such as insurance purchasing, before the lottery is resolved the individual is allowed to take an action that determines his final utility. As long as the optimal decision is affected by a change in the probabilities, the induced maximum expected utility will be convex in the probabilities, meaning that even if the underlying preferences are expected utility, induced preferences over the ‘optimal’ lotteries will be quasi-convex.====To gain some intuition for the link between quasi-convexity in probability mixtures and the consensus effect, consider Kőszegi and Rabin (2007)'s model. Suppose an individual is indifferent between either knowing for sure Option 1 is chosen, or knowing for sure Option 2 is chosen. For this to be true, each option has some benefits and some drawbacks relative to the other. However, if the individual expects that Option 1 will be chosen, and ends up with Option 2, the relative drawbacks will loom larger than the relative benefits (because of loss aversion relative to the reference point, which is Option 1). Thus, if a voter thinks Option 1 will often be chosen when they are not pivotal, they strictly prefer to ensure that it will also be chosen when they are pivotal, in order to align outcomes with expectations. We formalize this intuition in Sections 2.2 and 2.3, where we point to a deep connection between the notions of reference dependence, loss aversion, and the consensus effect.====To expand the applicability of our results, we further demonstrate how they extend to models of (i) globally quasi-concave preferences, where the opposite group behavior is predicted; (ii) preferences with both quasi-convex and quasi-concave regions, on which they can be applied locally; and (iii) behavior that may not be captured by maximizing a single preference relation.====In an earlier paper on choice shifts in groups, Eliaz et al. (2006, hereafter ERR) used the same model of group decision making but focused on group choices between particular pairs of options, safe and risky, where the former is a degenerate lottery that gives a certain outcome with probability one. They confined their attention to RDU preferences and established an equivalence between specific types of choice shifts and Allais paradox, one of the most documented violation of expected utility at the individual level. Since choice shifts in groups are observed in experiments even when all lotteries involved are non-degenerate, our results suggest that the choice shifts discussed in ERR are actually manifestations of the consensus effect. In Section 2.4 we relate our results to theirs. We extend their results for RDU preferences, but, more importantly, also demonstrate why the link to Allais paradox is restricted to that specific class of preferences. In particular, the consensus effect is in general consistent not only with Allais-type behavior but also with the opposite pattern of choice and, similarly, Allais-type behavior does not rule out the anti-consensus effect.====In Section 3 we analyze what type of equilibrium behavior results from quasi-convex preferences in conjunction with strategic considerations. We describe a majority voting game as a collection of individuals, each of whom has one vote to cast in favor of option ==== or option ==== (no abstentions are allowed). After observing their own preferences (which are drawn i.i.d. from some known distribution), but no other information, individuals vote. Whichever option receives the majority of the votes is implemented.====Since individuals with quasi-convex preferences do not like to randomize, voting games take on the properties of ====. These individuals benefit from coordinating their votes with others because it reduces the “randomness” in the election. They typically face a tradeoff between having the option they prefer selected and reducing the uncertainty regarding the identity of the chosen outcome.====We prove the existence of an equilibrium and describe the main properties of any possible equilibrium. When individuals exhibit the consensus effect, group decisions may fail to aggregate preferences properly because voters are willing to coordinate on either option, rather than voting for the option they prefer in isolation. Thus, strictly Pareto-dominated equilibria may result. This willingness to coordinate implies that our model features non-uniqueness of equilibrium not due to randomization by indifferent types (as in the expected utility case) but rather because of weak-preference reversals. We discuss conditions under which we would expect to see such preference reversals and how they relate to whether the equilibrium is unique or not. We further show that some individuals ==== exhibit strict preference reversal when the group becomes large.====In Section 4 we discuss how our model relates to, and can be distinguished from, alternative models in the literature on voting, including costly voting and common value settings. For example, individuals with quasi-convex preferences may be unwilling to pay the cost of voting even if they know they will be pivotal, but may be willing to do so when they have a smaller chance of being pivotal but their vote can help reduce the randomness of the election. The key conceptual difference from a common value setting is that there uninformed independent voters who want to choose the best candidate will tend to vote against — or to compensate for — a majority that is formed of partisans, while uninformed voters with quasi-convex preferences will tend to vote in accordance with this partisan majority.====Other approaches to conformity typically add an (additive) exogenous conformity benefits term to an otherwise standard model. One way to view our contribution is to take violations of expected utility in decision making under risk as descriptively valid and analyze — without tying our hands to any specific functional form — to what extent non-expected utility models can generate new predictions in the context of group decisions. Thus, our analysis provides a non-expected utility foundation for group choice anomalies. In particular, as we have discussed above, since the expected choice of the group serves as a reference point when the individual is deciding how to make his own choice, our intuitions are closely related to some of the recent models of expectation-based reference dependence (as in Kőszegi and Rabin, 2007).",On the consensus effect,https://www.sciencedirect.com/science/article/pii/S0022053119300675,10 July 2019,2019,Research Article,240.0
"Hayashi Takashi,Lombardi Michele","Adam Smith Business School, University of Glasgow, United Kingdom of Great Britain and Northern Ireland","Received 10 June 2018, Revised 14 May 2019, Accepted 29 June 2019, Available online 9 July 2019, Version of Record 17 July 2019.",https://doi.org/10.1016/j.jet.2019.06.007,Cited by (1),"Consider a society with two sectors (issues or objects) that faces a design problem. Suppose that the sector-2 dimension of the design problem is fixed and represented by a mechanism ====, and that the designer operates under this constraint for institutional reasons. A sector-1 mechanism ==== played by agents with those preferences always coincides with the recommendations made by ==== for that profile. If this mechanism design exercise could be accomplished, ==== would be constrained implementable. We show that ==== to make a socially optimal decision for sector 1, constrained monotonicity, combined with an auxiliary condition, is sufficient. This sufficiency result does ==== rule out any kind of complementarity between the two sectors.","The challenge of implementation lies in designing a mechanism (i.e., game form) where the equilibrium behavior of agents always coincides with the recommendations given by a ==== (SCR) ====. If such a mechanism exists, ==== is said to be implementable. The fundamental study on implementation in (pure strategies) Nash equilibrium is thanks to Maskin (1999; circulated since 1977).====Since this seminal work, the method used in the literature to understand how to solve an implementation problem is partial equilibrium analysis. This method isolates outcomes to be allocated as well as agents' preferences for those outcomes from the rest of the world, under a ceteris paribus (all else equal) assumption. When there is more than one decision problem and the practice dictated by the partial equilibrium analysis is taken as a given institutional constraint, Hayashi and Lombardi (2017) quantify the effect of the practice by showing that the scope of implementation has to be confined essentially to separable preferences. A centralized allocation mechanism may be better equipped to deal with issues arising from non-separability of preferences. However, this mechanism is not available or feasible in real life.====This leaves the question of whether there are ways to broaden the scope of implementation. The main objective of this study is to examine this question with only a minimal departure from the standard practice. The departure consists of two elements.====First, we consider a society with two sectors (issues or objects) that faces a design problem. We suppose that the sector-2 dimension of the design problem is fixed and represented by a mechanism ====, and that the designer operates under this constraint for institutional reasons.==== This departure is motivated by the observation that when, for example, a society faces the issue of whether to supply a public good, it must solve this problem by taking as given how private goods markets work. When a school authority faces the issue of allocating students to schools, the authority must solve it by taking as given how the housing market works.====Second, we introduce incomplete, yet not negligible, communication between the designer and other mechanisms, in the sense that the designer bases his allocation decision not only on information directly elicited from the agents but also on information elicited from them via ====. Suppose that ==== is a market mechanism for private goods and that a society faces the issue of sharing the cost of providing a public good under the constraint of ====. Then, the society needs to use the private information elicited from the agents via ==== if it wants to make a socially optimal decision. For the same reason, when ==== is a housing market and a school authority faces the issue of allocating students to schools under the constraint of ====, it needs to use the private information elicited from the agents via the housing market ====.====Note that this communication creates incentive problems. Indeed, given that agents are aware that the designer also bases his sector-1 decision on the information that they transmit to sector-2 mechanism ====, agents may have incentives to lie for manipulating not only outcomes determined by ==== but also sector-1 decisions. For example, once families know that the school authority bases its school allocation on information that they transmit to the housing market, families have incentives to use this information for manipulating not only the housing market outcome but also the school allocation outcome. Therefore, the change described above makes a significant departure from the standard literature as well as from the problem studied by Hayashi and Lombardi (2017). Recall that in Hayashi and Lombardi (2017), every sector-designer is under the assumption of partial equilibrium analysis, and so each allocation problem is isolated from others.====In this paper, the designer faces a constrained implementation problem. It consists of designing a mechanism for sector 1, ====, with the property that for any type of agents' preferences, the set of (pure) Nash equilibrium outcomes of the mechanism ==== coincides with the set of outcomes that ==== would select for those preferences. If this design exercise can be accomplished, the SCR is said to be ====.====Within this set-up, we investigate the theory of implementation pioneered by Maskin (1999) under the constraint ====. Our conclusion is that a small departure from the standard practice dramatically increases the scope for implementation, in the sense that our sufficiency results do not rely on any domain restriction of agents' preferences. Thus, unlike the negative result of Hayashi and Lombardi (2017), our sufficiency result does ==== rule out any kind of complementarity between the two sectors.====We also show that an SCR that can be constrained implemented satisfies an invariance condition, named ====. This condition is a strengthening of monotonicity. Monotonicity means that if an outcome ==== is recommended by the SCR ==== in state ==== but ==== does not recommend it when the state is changed to ====, then the outcome ==== must have fallen strictly in someone's ordering at the state ====. To introduce our condition, suppose that the sector-2 outcome ==== is supported by a profile of sector-2 strategy choices ====—that is, ====. Constrained monotonicity requires that if an outcome ==== is recommended by ==== in state ==== but the SCR ==== does not recommend it when the state is changed to ====, then, to break the Nash equilibrium via some deviation, there exists an agent ==== who can generate a sector-2 outcome by varying his own sector-2 strategy choice, ====, while keeping the other agents' sector-2 strategy choices fixed at ====, such that an outcome ====, which is less preferred than ==== at ====, is strictly preferred to ==== at ====.====Section 2 outlines the basic model. Section 3 defines constrained monotonicity and shows that it is necessary for constrained implementation. Section 4 provides our characterization result. Sections 5 provides an account of welfare implications of constrained implementability. Section 6 studies the relationships between constrained implementability and the standard unconstrained implementability. Section 7 concludes.",Constrained implementation,https://www.sciencedirect.com/science/article/pii/S0022053118302588,9 July 2019,2019,Research Article,241.0
"Scotchmer Suzanne,Shannon Chris","Department of Economics and Boalt School of Law, UC Berkeley and NBER, United States of America,UC Berkeley, United States of America","Received 17 May 2016, Revised 12 June 2019, Accepted 23 June 2019, Available online 27 June 2019, Version of Record 15 July 2019.",https://doi.org/10.1016/j.jet.2019.06.006,Cited by (4),"We consider group formation in markets with asymmetric information. Our model nests standard matching problems, including one-to-one, many-to-one, and many-to-many matching, as well as matching with salaries or contracts and matching with incomplete information. Prices for group positions and private goods as well as the groups that form are determined endogenously in equilibrium, as a result of demand and supply forces. The setup includes problems as diverse as moral hazard in teams, screening on ability, and mechanism design. Our analysis, including the definition of equilibrium and existence, revolves around the randomness in matching. Our main results characterize the limits on efficiency in such a ====, and show that a sufficiently rich set of group types can ensure the existence of an efficient equilibrium.","What determines the contracts, mechanisms, games, and other organizational forms that are used in an economy? What role does competition play in shaping incentives and institutional design? How does private information enter markets, and to what extent does competition mitigate or magnify the inefficiencies that arise from asymmetric information? This paper develops a model designed to address these questions.====Classical general equilibrium theory focuses on anonymous price-taking agents, typically ignoring any strategic effects or incentives. Modern theory of institutions, contracts, and mechanism design focuses on incentives and private information in isolation, typically ignoring market forces that might alter organizational design. As a consequence, neither can explain how incentives might influence markets or how competition might select among institutions.====To address such issues, this paper develops a model that melds key aspects of matching, contract theory, mechanism design and game theory with general equilibrium theory. Agents interact strategically in small groups, taking into account incentives and the effects of their actions on group outcomes, but trade anonymously in markets, taking prices as given. This allows us to study the interplay between market forces, private information, the provision of incentives, and the structure of institutions, and to assess the role of markets in limiting inefficiencies that stem from asymmetric information.====We take as a starting point models of group formation in markets developed in club theory. In these models, agents choose memberships in finite groups (“clubs”), and also trade private goods. Agents act as price takers in markets for memberships and goods. Market clearing determines prices and the types of groups that emerge. These models extend general equilibrium theory to include a vast array of economic and social interactions that take place in groups. In particular, as emphasized by Ellickson et al. (2005), club theory provides a natural model of firms. Prescott and Townsend (2006) and Zame (2007) expanded these ideas to incorporate more general contracting problems with private information.====We show that many standard matching models can be incorporated as examples of our group formation model. This includes the classic one-to-one, many-to-one, and many-to-many matching models, as well as matching with salaries, matching with contracts, and matching with incomplete information. Because the model allows for many different group types, the model allows for markets with one such matching problem in isolation, or for markets in which many such problems coexist simultaneously. The model provides a general framework and tools for studying matching in large markets. Our results cast matching problems directly within a market with a continuum of agents, and allow us to make use of familiar and powerful techniques from general equilibrium theory for large markets.====Our model extends the group formation model of Ellickson et al., 1999, Ellickson et al., 2005 (EGSZ below) to incorporate asymmetric information. Agents may have both verifiable and unverifiable characteristics. Unverifiable characteristics can be either hidden actions or hidden information. Thus they can be chosen, such as actions in games, learned, such as skills required for jobs, or innate, such as intelligence. We assume that unverifiable characteristics are observable ex post (after groups have formed) in the sense that they may affect the output and utility of other agents in the group. They are not verifiable ex ante. Thus, prices for memberships cannot depend on them, and they cannot be used for screening members. This framework includes problems as diverse as moral hazard in teams, signaling, screening, and mechanism design.====Because characteristics are unverifiable and groups form randomly, risk is a central feature of the model, both aggregate risk and idiosyncratic risk. Once agents have chosen memberships and strategies, a matching process determines who is matched with whom, and therefore determines the unverifiable characteristics or strategies played in each agent's groups. We model this matching process as random, and construct the associated stochastic processes so that the resulting distribution on possible matchings is uniform. Because the model has a continuum of agents, there are subtleties in making this precise. To do so, we adapt the construction of random matching in pairs in Duffie and Sun (2007) to the more general group setting. This construction has several important consequences. First, it leads to an exact law of large numbers. Second, it highlights the aggregate uncertainty that arises from matching: each possible matching is a random outcome that applies to the economy as a whole, and affects each agent's wealth and preferences for private goods.====In our model, aggregate risk is not ruled out by the law of large numbers. This contrasts with the approach of Prescott and Townsend (2006) and Zame (2007), who focus on purely idiosyncratic risk. For example, Zame (2007) argues that, due to the law of large numbers, aggregate demand and production are deterministic, and as a consequence, private-goods prices are deterministic. Since aggregate demand and production depend on prices, however, the law of large numbers does not suffice to draw this conclusion. Indeed, this is not true in our model. Agents' outcomes in the random matching are independent by construction, but individual demands may be correlated by prices. The law of large numbers can be applied in aggregating individual demands only after first assuming that prices are constant. Instead of assuming this, we show that constant prices materialize in equilibrium if a certain kind of insurance is offered in the market. With insurance, constant prices emerge as a conclusion, rather than an assumption. Insurance also provides efficiency gains. Absent insurance, equilibrium prices need not be constant, and trades in private goods can be inefficient even if prices are constant.====We use the matching process we construct to develop two equilibrium concepts, one in which agents are sophisticated enough to realize that their chosen groups might not form, and another in which they assume their demands for memberships are always met. The second equilibrium notion is close in spirit to that of Zame (2007), under the additional assumption that prices are constant across all matchings. We also develop a refinement that links the two equilibrium notions.====Our main results focus on the resulting efficiency in the trading of private goods and in the formation of groups. The mere fact that agents choose their groups optimally is a force toward efficiency; that is probably the main message of club theory. With symmetric information, for example, as central results of EGSZ (1999) show, equilibria are Pareto optimal and core equivalence holds. In our model with asymmetric information, however, equilibria can easily be inefficient. For example, the model includes simple finite player, finite actions games within groups; equilibria in such models can be robustly inefficient as a consequence. More importantly, we show that equilibria are typically incentive constrained inefficient as well. Moreover, we show that in the presence of asymmetric information, in contrast with EGSZ (1999), competition can create inefficiencies in large markets when key aspects of contracts or organizations are determined endogenously, in part because of endogenous coordination failures; see section 8 and Example 9. We provide several notions of incentive constrained efficiency for our model; examples throughout the paper show that equilibria need not be incentive constrained efficient, even in very simple settings.====We show that competition can nonetheless play an important welfare improving role with asymmetric information by selecting for more efficient institutions. First, we provide some conditions under which group equilibrium satisfies a restricted form of incentive constrained efficiency. Our main result then shows that efficiency can be achieved by introducing a sufficiently rich set of group types using reporting mechanisms and residual claimants in the spirit of Maskin (1999). Roughly, we show that if groups include appropriately designed mechanisms, there are equilibrium states that replicate those that would arise if all strategies were verifiable. These states are efficient, provided efficiency can be achieved in deterministic states of the economy.====In practical applications, our results are most useful in large, decentralized markets in which firms, sellers, or organizations are small and compete for agents. For such applications, our results provide conditions under which the orderly functioning of such competitive markets holds, despite the presence of multilateral asymmetric information. Our results also show that particular types of interventions could be welfare improving in some settings.====Our paper connects, and is connected to, several distinct literatures. One is the growing body of work on large matching markets. Much of this work shows that large markets can effectively eliminate welfare trade-offs and incentives for strategic manipulation that are prevalent, and have been the focus of significant research and policy attention, in small matching markets. For example, in a series of important papers, Roth and Peranson (1999), Immorlica and Mahdian (2010), Kojima and Pathak (2009), and Lee (2014) show that the fraction of agents with incentives to manipulate a stable matching mechanism goes to zero as the number of participants goes to infinity.==== All of this work uses powerful techniques from random graph theory by imposing structure on agents' preferences as the size of the market grows.==== Other work, including Legros and Newman (2007) and Shimer and Smith (2000), instead starts with specific structure on preferences, such as complementarities, and seeks conditions under which qualitative patterns of matching can be determined, like positive or negative assortativity.====Our work complements and contrasts with this line. We instead model large matching problems directly within a market with a continuum of agents. By leveraging powerful general equilibrium methods, we can incorporate many distinct but linked matching problems simultaneously, and allow for general interrelated preferences over outcomes in these matching problems as well as private goods. Thus our work yields a model of market design by markets, in which demand and supply forces determine both prices and details of matchings, including both who matches with whom and institutional details like contractual terms or mechanisms for sharing information and rents, endogenously in equilibrium.====Our paper also connects to the significant literature embedding private information, particularly contracts, in markets and general equilibrium. A number of papers have considered related themes in the context of particular applications. Examples include Cole et al. (2001), McAfee (1993), Peters, 1997, Peters, 2001, Bulow and Levin (2006), Magill and Quinzii (2008), Acemoglu and Simsek (2010), and Legros and Newman, 1996, Legros and Newman, 2008, Legros and Newman, 2013. In particular, Legros and Newman (1996) study a general equilibrium model of the determination of monitoring and incentive provision in firm formation. Using the specificity of their model, they determine a number of important relationships between the distribution of wealth and the pattern of organizational forms used in firms. Similarly to club theory, they view firms as finite groups of agents engaged in an activity. Their model differs from the clubs model of Ellickson et al., 1999, Ellickson et al., 2005 and from our model in that they adopt a cooperative, core-based equilibrium concept.====A number of other important papers focus instead on general competitive models incorporating asymmetric information. The pioneering work of Prescott and Townsend (1984) formulated the trading of contracts in general equilibrium by modeling incentive constraints as a restriction on contract trades. Due to the resulting nonconvexities, agents are modeled not as choosing a particular consumption plan or contract, but rather a lottery that is a distribution over consumption plans or contracts. They assume contracts are exclusive, meaning agents take a single contract ex post after lotteries are resolved and consume the bundle this contract specifies.==== This is the framework adapted by Cole and Prescott (1997) to clubs, and by Prescott and Townsend (2006), who extend the clubs model to accommodate unverifiable effort in firms. In many of these models, lotteries are offered by an intermediary or firm serving a continuum of agents (for simplicity, the whole economy). In these models some intermediaries or firms are thus required to be large – accommodating a continuum of agents – to implement the lotteries needed to convexify incentive constraints.==== We focus instead on settings in which agents choose deterministic consumption plans and contracts, all firms and other groups are small (finite), and all incentive constraints must be resolved within these small groups.==== Contracts need not be exclusive in our model: agents can take multiple different contracts and trade freely in private goods after contracts are resolved.====Similarly, the pioneering work of Dubey et al. (2005) introduces the central idea of pooling to study markets with asymmetric information. See also Bisin et al. (2011), Citanna and Siconolfi (2013), Minelli and Polemarchakis (2000), and Dubey and Geanakoplos (2002). In these models, sellers deliver to a pool, and buyers buy from this pool. When the goods delivered differ in quality, each buyer receives the average delivery or average quality from the pool, rather than being matched with a particular seller or receiving a particular good. In contrast, our model considers trade with unknown quality in finite trading groups, in which some members deliver goods, and other members consume them. In our model the precise matching between a buyer and a seller or good is central.====We adapt the clubs framework of Ellickson et al., 1999, Ellickson et al., 2005 instead of Cole and Prescott (1997), and therefore our model shares features with that of Zame (2007). The paper differs from Zame (2007) in three main ways. First, we model the random matching process that determines group formation, and prove the existence of such a process in which agents' matchings are iid random variables. This requires us to define several different group equilibrium notions, and establish results linking them. This is important because it allows us to break the circularity in conclusions regarding endogenous features of prices and absence of aggregate uncertainty present in other related work, including Zame (2007). Instead, we establish exogenous sufficient conditions under which prices are independent of matching outcomes. We then show that this result can be used to prove that there is no aggregate uncertainty under these exogenous assumptions. Second, the main result in Zame (2007) is the existence of equilibrium. In addition, Zame (2007) gives one result on efficiency of equilibrium in the absence of asymmetric information; in this case the model reduces to that in EGSZ (1999) and the efficiency result follows from theirs. We also establish results on existence of equilibrium, which are the starting point for any market equilibrium model. The primary focus of this paper, however, is instead on qualitative features of equilibrium and on their welfare properties. In particular, a main contribution of the paper is the notion of complete group types and the proof that efficient group equilibria exist when group types are complete. Finally, while Zame (2007) proposes a model of firm formation in general equilibrium, this paper develops a different set of group formation problems. Most importantly, we show in section 3 that our model nests most standard matching models, and show that our work provides a framework for studying matching in large markets.====In section 2 we lay out the model. In section 3, we first give a simple example to illustrate the model. We then show that standard matching models can be embedded in our model, and use these as leading examples throughout. In section 4, we formalize the notion of random group formation. In section 5 we define our basic equilibrium notion. In section 6 we define a second equilibrium notion with beliefs on membership characteristics, and explore the connection to our basic equilibrium notion by means of a refinement. In section 7, we introduce insurance markets that smooth the consumption of private goods, and establish a constrained version of the first welfare theorem. In section 8, we discuss incentive efficiency of group equilibrium. In section 9, we illuminate the role of residual claimants in achieving efficiency, arguing that group types with residual claimants will often drive out group types without residual claimants, and give our main efficiency theorem. In section 3 of Scotchmer and Shannon (2019), we show that randomization can be introduced as a choice variable through lotteries modeled as group types. Proofs and additional results are collected in the appendix.",Verifiability and group formation in markets,https://www.sciencedirect.com/science/article/pii/S0022053119300651,27 June 2019,2019,Research Article,242.0
"Baughman Garth,Rabinovich Stanislav","Federal Reserve Board, United States of America,University of North Carolina, Chapel Hill, United States of America","Received 23 July 2018, Revised 9 June 2019, Accepted 16 June 2019, Available online 19 June 2019, Version of Record 25 June 2019.",https://doi.org/10.1016/j.jet.2019.06.005,Cited by (3),In a ,"Under what conditions is there price dispersion for identical goods? A well-known result going back to Diamond (1971) is that simple search frictions do not alone suffice to generate price dispersion. In this paper, we identify a natural and intuitive source of price dispersion: monetary trade. In a frictional product market where buyers must carry money in order to make purchases, we show that price dispersion arises as an equilibrium outcome without recourse to costly simultaneous search, or any heterogeneity in production technology, preferences, or search opportunities. In this sense, price dispersion can be a purely monetary phenomenon.====We consider a random matching monetary model in which sellers post prices, and buyers simultaneously decide on their quantity of money balances. When choosing prices, a seller trades off revenue per sale against the chance of selling a discrete good. Similarly, when choosing money balances, a buyer trades off the cost of holding money against the probability of being able to afford a posted price. A non-degenerate distribution of money balances among buyers makes sellers indifferent across a range of prices; a non-degenerate distribution of prices, in turn, makes buyers indifferent across a range of money balances. Thus, as in any mixed strategy equilibrium, the distribution of money balances rationalizes the distribution of prices and vice versa. The key to generating this result is the strategic complementarity between buyer and seller choices, stemming from the rather natural and common assumptions that prices are posted and that trade requires money.====Due to the complementarity between price posting and money holdings, the model has many equilibria. We fully characterize the distribution of posted prices and money holdings in any equilibrium. Our main result is a necessary and sufficient condition for a set to be the support of an equilibrium price distribution. We derive this in three steps. First, we show that mass points in the distribution of money holdings imply gaps in the distribution of prices, and vice versa. Second, we provide closed-form expressions for the distributions on any interval. Third, these two results enable us to characterize the distributions on any closed set. We illustrate our results by describing two special cases: equilibria with a discrete support, i.e. exactly ==== prices, and equilibria whose support is an interval. We also derive expressions for average prices, real balances, and welfare.====There are four main lessons from our analysis. First, the set of equilibria is large. Consider the interval of prices that can obtain in a single-price equilibrium and yield sellers positive profits.==== Our results imply that any closed subset of this interval can be the support of the price distribution in some dispersed-price equilibrium. Importantly, this does not exhaust the possibilities for equilibria. Equilibria with price dispersion can feature prices higher than those observed in any single-price equilibrium. This is because, in a dispersed-price equilibrium, a chance of sometimes paying a low price compensates buyers for carrying high money balances. In other words, price dispersion allows for higher maximum prices, a result quite different from the predictions of other models of price dispersion, as we discuss below.====Second, despite the multiplicity of equilibria, the model makes very specific predictions about the shape of the price and money holdings distribution. In particular, on any interval in the support of the price distribution, the density of prices is increasing – the opposite results from the leading models of price dispersion, such as Burdett and Judd (1983). The reason for this difference is that as explained above, the equilibrium price distribution is pinned down the buyers' indifference condition. This is in contrast to Burdett and Judd (1983), where it is the sellers' indifference condition that pins down the equilibrium price distribution. This is discussed in more detail in Section 4.7. This difference in predictions provides a potential way of distinguishing among theories of residual price dispersion as more data becomes available.====Third, price dispersion reduces welfare, which depends in our model on the fraction of meetings that result in trade. In an equilibrium with price dispersion, there are meetings where the seller's posted price exceeds the buyer's money balances, and trade does not occur despite being mutually beneficial. Price dispersion reduces welfare by creating mismatch between posted prices and money balances.====Fourth, inflation reduces welfare by exacerbating the mismatch caused by price dispersion. This welfare cost of inflation is quite different from the standard inflation tax channel, whereby inflation lowers the demand for real balances. Instead, in our model, inflation exacerbates the welfare loss from mismatch by shifting the weight of the equilibrium price distribution onto higher prices. Importantly, this effect of inflation only exists in equilibria with price dispersion, and differs substantially from menu cost driven effects. Our analysis provides a new link between price dispersion and monetary theory.",Self-confirming price dispersion in monetary economies,https://www.sciencedirect.com/science/article/pii/S0022053118303922,19 June 2019,2019,Research Article,243.0
"Gallin Joshua,Verbrugge Randal J.","Board of Governors of the Federal Reserve System, Washington, DC 20551, United States of America,Federal Reserve Bank of Cleveland, 1455 E. 6th St., Cleveland, OH 44114, United States of America","Received 24 April 2017, Revised 6 June 2019, Accepted 8 June 2019, Available online 18 June 2019, Version of Record 15 July 2019.",https://doi.org/10.1016/j.jet.2019.06.003,Cited by (14),"This paper offers the first theoretical explanation of housing rent stickiness and differential stickiness by housing structure type, key features of rental markets worldwide. Previous theoretical work on the subject is sparse and explains why some landlords charge below-market rents, not stickiness. Apart from its interest for understanding housing markets and overall ====, rent stickiness is intriguing because it shows that stickiness can emerge in markets where the typical menu cost logic does not apply, and where market participants typically engage in repeated bargaining over contracts. Our model involves search and bargaining with incomplete information in a two-period game, and generates stickiness in a novel manner. The key decision is the landlord's choice of whether to renegotiate, or not. Crucially, landlords do not know tenant perceptions of unit quality. The key force generating stickiness is that offering the identical contract preempts tenant search, thus limiting tenants' outside options, and resolves uncertainty early. Risk aversion reinforces this mechanism and yields differential stickiness by landlord type. Renegotiation risk for large landlords disappears because the different outcomes—rent increase, rent decrease, and vacancy—“average out.” Landlords who manage only one unit loathe vacancy, and are highly motivated to avoid renegotiation.","Although price stickiness is a central issue in macroeconomics (for example, see Ball and Mankiw, 1994, or Boivin et al., 2009) there is little consensus about its deep sources. Most New Keynesian models—which are the workhorse models for most of the economics discipline, including central banks around the world—use tractable simplifications like Calvo pricing or state-dependent pricing, which posit arbitrary costs or constraints to changing prices. But the modeling details matter: The appropriate monetary policy response to shocks depends upon the precise source of price stickiness (see, e.g., Caballero and Engel, 1993 or Huang and Meng, 2014). Unsurprisingly then, explaining price stickiness remains an area of active theoretical and empirical research (Bils and Klenow, 2004; Golosov and Lucas, 2007; Nakamura and Steinsson, 2008; 2011; Gopinath and Itskhoki, 2011; Knotek, 2011; Head et al., 2012; Midrigan, 2011; Kehoe and Midrigan, 2012; L'Huillier and Zame, 2015).====Rent stickiness, in contrast, has garnered relatively little empirical attention and almost no theoretical attention. In terms of empirical work, Gallin and Verbrugge (2017) show that from 1999 to 2008, about half of all rents in the U.S. were unchanged after 12 months and about one-third of all rents were unchanged after two years. Moreover, they show that about two-thirds of single-unit units had no rent change over a year while only about one third of units in large buildings saw no rent change over a year. While very few previous studies of rent stickiness exist, they have broadly similar findings; these studies, which involve both U.S. and international data, are reviewed in the next section of the paper.====Extant theories aimed at explaining price stickiness are not appropriate for the housing rental market because rent setting is substantively different from price setting for things like cabbage, clothing, computers, catering, and even cars. Most transactions in these other goods and services involve search, but little or no bargaining. Moreover, even transactions that often involve bargaining—such as for cars and other durables—involve different customers on different days in different locations. Perhaps as a result, while there is a large literature of price setting involving search,==== most models of price setting abstract from bargaining (see Camera and Selcuk, 2009).====Theoretical work on rent stickiness itself is scant. The “good tenant” theory posits that landlords keep rents below market for good tenants—those who pay on time and cause little damage and headaches—in order to retain them (Barker, 2003). The “depreciation equals inflation theory” argues that landlords can raise real rent without changing nominal rent by allowing the quality of the housing unit to deteriorate.==== But these theories explain underpricing of rental units, or why nominal rent changes are often small, not stickiness of rents, that is, rent changes that are often exactly zero. Ours is the first model we are aware of that generates rent stickiness.====There are three main reasons for our interest in rent stickiness. First, it is a key feature of rental markets around the world. Second, rent stickiness is quite important for aggregate inflation because rent changes drive 30 percent of the US Consumer Price Index, 42 percent of the core CPI, and 19 percent of the core PCE price index (Verbrugge and Garciga, 2015). Third, rent stickiness is intriguing because it shows that stickiness can emerge even in markets where the typical logic of menu costs does not apply, and where market participants routinely engage in repeated bargaining over contracts.====Search and bargaining over match-specific surpluses with incomplete information play a central role in the rental housing market. Fixed costs, such as for searching, moving, and leaving a unit vacant, in conjunction with idiosyncratic and private valuations of particular rental units, produce a match-specific surplus that is potentially bargained over—particularly given repeat transactions over the same product, when tenants re-sign leases and renegotiation may occur. Landlords and tenants almost always enter into long-term contracts rather than negotiating terms continuously. But contracts expire; at the end of twelve months, a typical length in the U.S., we often see landlords and tenants agree to the identical contract, despite inflation in various costs such as maintenance and taxes, and inflation in the outside options of both parties.====Our model builds on these observations, and therefore offers a new explanation of rent stickiness that is subtly different from the common mechanism, where stickiness derives from a simple monetary or decision cost of adjusting prices. Those costs likely exist in the housing context, but are borne whether or not the rent changes. That is, landlords incur the cost of writing up a new contract either way.====The model has three time periods and exogenously determined initial rent; our focus is the stickiness of rents for renegotiated contracts rather than the determination of rent levels in equilibrium. Tenants, who are essentially identical, can search for units, accept or reject rent offers, and move to alternative housing; searching and moving is costly. We treat housing rental as an “experience” good (Nelson, 1970; Bils, 1989) as well as a search good: Tenants' valuations of units are identical ex ante, but match-specific ex post. Upon locating a unit, tenants get an initial estimate of match quality; but they only learn how suitable the unit really is, i.e. the true quality, after living in the unit for a while. The landlord never learns the match quality. Overall inflation is exogenous.====The crucial decision in the game for explaining rent stickiness is the landlord's choice of whether to renegotiate the rent contract or not. Either way, the landlord can lose a tenant. A tenant will leave if his valuation of the unit is low enough, even if the nominal rent is unchanged. However, renegotiation poses added risks to the landlord. Some tenants may engage in costly search, potentially providing them with attractive outside options. A landlord, uncertain about tenant-unit match quality, does not know how hard she can push in negotiations before prompting the tenant to seek out better options. Usually she is able to exploit moving and search costs, and increase the rent. But renegotiation can go badly for the landlord in two ways: The tenant might reject the offer or occasionally even bargain ==== the rent. Conversely, choosing not to renegotiate by offering the same contract preempts search, and resolves uncertainty at a sufficiently early stage, at the cost of a real rent reduction if inflation is positive. For certain parameter values, this real rent reduction is profit-maximizing.====Risk aversion amplifies this mechanism. In our model, there are two types of landlords, Multiple-unit, and Single-unit. Both types are risk-averse. They differ only in that Multiple-unit landlords manage a continuum of units and Single-unit landlords manage only one unit.==== But that difference is crucial for explaining differential rent stickiness. The risks of renegotiation and vacancy differ across the two landlord types, because landlords who manage many units effectively face less risk since the various unit-specific outcomes—getting a higher rent, accepting a lower rent, and suffering a vacancy—“average out.” Conversely, landlords who manage only one unit face substantial risk, and are therefore more motivated to avoid renegotiation. Risk aversion thus amplifies the relative benefit of stickiness for Single-unit landlords. In particular, we show that in the model equilibrium we present, risk management prompts Single-unit landlords to offer the same rent contract, while Multiple-unit landlords choose to accept the higher probability of vacancy associated with renegotiation.====Our theory is a dynamic model involving search and bargaining with incomplete information. In general, such models are difficult to solve analytically. Numerous papers presenting dynamic asymmetric information games such as Milgrom and Roberts (1982), Cho and Kreps (1987) and Geanakoplos (2010) examine a specific case of a more general model. We do the same, appealing to Gilboa et al. (2014).==== In particular, we demonstrate an equilibrium for a particular set of parameter values. However, other similar equilibria exist in an open ball around the parameter set we focus on.====Our model is highly stylized and focuses on the renegotiation decision in an existing tenant relationship, and thus abstracts from other aspects of the rental market. It is not a general equilibrium model; for instance, entry is modeled in a stark manner, exit is absent, the model does not attempt to explain vacancy rates, and it constrains pricing on vacant units.==== We do not deny the presence of other advantages that owners of multi-unit complexes enjoy.==== However, in this first attempt in the literature to explain rent stickiness, we seek to provide a simple model that focuses on one set of ideas, leaving extensions for future work.====The rest of the paper is organized as follows. Section 2 briefly presents a summary of some key empirical features of rent stickiness; Section 3 provides a description of the model; Section 4 defines the equilibrium and proves the existence of a perfect Bayesian equilibrium for specific parameter values; Section 5 presents a solution of the models for a specific set of parameters; Section 6 presents a discussion of the implications of some of key model assumptions and results, and Section 7 concludes.",A theory of sticky rents: Search and bargaining with incomplete information,https://www.sciencedirect.com/science/article/pii/S0022053119300638,18 June 2019,2019,Research Article,244.0
Meyer-Gohde Alexander,"Goethe-Universität Frankfurt, Chair of Financial Markets and Macroeconomics, Theodor-W.-Adorno-Platz 3, 60629 Frankfurt am Main, Germany,Institute for Monetary and Financial Stability, Theodor-W.-Adorno-Platz 3, 60629 Frankfurt am Main, Germany","Received 11 October 2017, Revised 9 May 2019, Accepted 10 June 2019, Available online 14 June 2019, Version of Record 28 June 2019.",https://doi.org/10.1016/j.jet.2019.06.004,Cited by (4),"I provide a model uncertainty foundation to the power certainty equivalent of Epstein-Zin-Weil risk sensitive preferences (EZ), enabling the analysis of these preferences using detection probabilities (DEPs) and worst case models. This completes the connection between these preferences and the model uncertainty of ==== (HS) that was previously limited to the special case of unit elasticity of intertemporal substitution. The connection between EZ and HS rests on a powerlike extension of entropy and its associated statistics from ==== and I show that the same additional margin of pessimism that implies this connection can close the gap to the empirical Sharpe ratio in a more general specification. For the specific cases of EZ and HS preferences, I find that calibrations that match detection error probabilities yield comparable asset pricing implications across models. Surprisingly, I find that the low levels of risk aversion with EZ preferences that match asset pricing facts are associated with a high level of model uncertainty in the long run risk environment of ====.","Tallarini (2000), Barillas et al. (2009), Ju and Miao (2012) and others have emphasized the close relationship between model uncertainty preferences==== and the risk-sensitive preferences of Epstein and Zin (1989) and Weil (1990) (henceforth EZ). This relationship, however, has only been shown formally under Hansen et al.'s (2007, p. 3975) “special set of assumptions” that align EZ recursive preferences with Hansen and Sargent's (2007) multiplier model uncertainty (henceforth HS) through a logarithmic transformation and a unit elasticity of intertemporal substitution.====Model uncertainty in macroeconomics (see Hansen and Sargent, 2001, Hansen and Sargent, 2010 and the detailed treatment in the monograph Hansen and Sargent (2007)) places agents in a decision environment riddled with unstructured, Knightian uncertainty that leads to agents forming their decision rules to be robust to a worst case (i.e., welfare minimizing) model. Barillas et al. (2009) evoke a critique from Robert E. Lucas, Jr., in their epigraph that although it would be nice to resolve the equity premium puzzle, we need to look past high values of risk aversion to do so and, as an alternative, offer a small amount of model uncertainty as a possible contributor to the resolution of this conflict. This is especially promising as Swanson (2016) shows that a large amount of risk aversion summarizes many of the puzzles in macro-finance literature. Yet Bansal and Yaron's (2004) popular long-run risk resolution of the equity premium, for example, requires that the intertemporal elasticity of substitution differ from one, exactly when the relationship between HS and EZ breaks down.====Specifically, this paper addresses the open question of Backus et al. (2005, p. 361) as to “whether there's a similar relationship between Kreps-Porteus preferences [(EZ)] with (say) a power certainty equivalent and a powerlike alternative to the entropy constraint [in HS]” by presenting ambiguity averse preferences that contain EZ preferences for arbitrary felicity functions. Furthermore it makes a first step towards assessing the plausibility of EZ preferences as interpreted from an ambiguity perspective using detection error probabilities (henceforth DEP) as proposed by Anderson et al. (2003) and Hansen and Sargent (2007) on their resulting worst case models.====I propose a generalization of the statistics of model uncertainty preferences beyond the logarithmic Bolzmann-Gibbs-Shannon measure of entropy to the measure introduced by Tsallis (1988) for nonextensive statistical mechanics in thermodynamics. This results in a generalized exponential certainty equivalent that encapsulates both the exponential and power certainty equivalents of HS and EZ. From the lens of model uncertainty, decreases in risk aversion in EZ's risk-sensitive preferences can be interpreted as a reduction in model uncertainty tempered by an increase in pessimism in the form of an overweighting the probability of pernicious distortions when formulating robust decision rules. This overweighting of events vis-a-vis objective probabilities relates to the choice-theoretic framework of Quiggin (1982) and results here from the generalized alternative entropy measure and its associated subadditivity of probabilities, the latter found also in Gilboa (1987) and Schmiedler (1989). Dow and Werlang (1992) emphasize that expectations formed under probabilities that do not sum to one (i.e., subadditive) reflect both agent's uncertainty and aversion thereto. My generalization and the introduction of an additional form of uncertainty, pessimism in the sense above, is not costless. While in the EZ case, there is only one free parameter to be calibrated using DEPs, the generalized uncertainty case has two parameters, which does not lead to a unique mapping from DEPs.====Applying these preferences to an endowment economy with long run risk following Bansal and Yaron (2004) and Bansal et al. (2016) and to an otherwise standard RBC production economy in the vein of Tallarini (2000),==== I find that both HS and the model uncertainty formulation for EZ behave comparably for a given DEP with respect to their maximum Sharpe ratios. In the endowment economy, I find that EZ preferences attribute a ==== amount of model uncertainty to a ==== amount of risk aversion in contrast to the results of Barillas et al. (2009) under HS preferences. The EZ specification of model uncertainty entails a degree of probability over/underweighting that involves a violation of the law of total probability – the equivalence with HS preferences necessitates an exclusion of this additional distortion on behalf of the econometrician, implying that agents over/underweight events made more/less likely under this distorted probability distribution.====Examining the worst case density associated with the different specifications, I find that agents with model uncertainty fear lower mean consumption/productivity growth. This is broadly consistent with other studies: Barillas et al. (2009), Bidder and Smith (2012), Ellison and Sargent (2015), Bidder and Dew-Becker (2016) find that the worst case is associated with lower mean growth. Under all three specifications, agents are concerned that mean of the volatility process in the endowment economy and that of output growth in the production economy might be higher than specified in the approximating model. In terms of the asset pricing implications, both EZ and HS preferences require a DEP of less than 5% to generate Sharpe ratios comparable to the empirical ratio, whereas the generalized model uncertainty can accomplish this with a conservative detection probability of 25%.====The remainder of the paper is as follows. I begin with the generalized measure of entropy in section 2. In section 3, I apply this measure to a general dynamic model, derive conditions that recover both EZ's as well as HS's original model uncertainty framework, assess atemporal risk aversion in all three frameworks, and examine the asset pricing implications of the generalized model uncertainty specification. I then apply the generalized model uncertainty to both an endowment economy and a production economy in section 4 and examine the asset pricing and macroeconomic performance of all three frameworks. Section 5 concludes.",Generalized entropy and model uncertainty,https://www.sciencedirect.com/science/article/pii/S002205311930064X,14 June 2019,2019,Research Article,245.0
"Attar Andrea,Campioni Eloisa,Piaser Gwenaël","Toulouse School of Economics, CNRS, University of Toulouse Capitole, Toulouse, France,DEF, Università degli Studi di Roma Tor Vergata, Roma, Italy,Ipag Business School, Paris, France","Received 18 August 2018, Revised 30 May 2019, Accepted 9 June 2019, Available online 12 June 2019, Version of Record 19 June 2019.",https://doi.org/10.1016/j.jet.2019.06.002,Cited by (2),"We study games in which principals simultaneously post mechanisms in the presence of several agents. We evaluate the role of principals' communication in these settings. As in ====, each principal may generate incomplete information among agents by sending them private signals. We show that this channel of communication, which has not been considered in standard approaches to competing mechanisms, has relevant strategic effects. Specifically, we construct an example of a complete information game in which (multiple) equilibria are sustained as in ==== and none of them survives in games in which all principals can send private signals to agents. The corresponding sets of equilibrium allocations are therefore disjoint. The role of private communication we document may hence call for extending the construction of ==== to incorporate this additional element.","We study competing mechanism games: principals compete through mechanisms in the presence of several agents. Such a strategic scenario has become a reference framework to model competition in a large number of market settings.====As first pointed out by McAfee (1993) and Peck (1997), the equilibrium allocations derived in these contexts crucially depend on the set of mechanisms that principals are allowed to post. Typically, letting agents communicate to principals additional information on top of their exogenous types supports additional allocations at equilibrium.==== This raises the issue of identifying a class of mechanisms inducing agents to reveal all their available information. In an important contribution, Epstein and Peters (1999) introduce a communication device that incorporates the market information generated by the competing mechanisms posted by principals. In their general construction, a mechanism for a principal requires each agent to send messages from a ==== type space. The corresponding set of equilibrium allocations may be very large: Yamashita (2010) has been the first to show that restricting attention to a subset of such mechanisms, i.e. the ====, is sufficient to derive a folk-theorem-like result. In a recommendation mechanism, a principal commits to post a certain direct mechanism if all but one agent recommend him to do so. Recommendation mechanisms hence allow to construct a flexible system of punishments: following a unilateral deviation of a given principal, agents can coordinate to select, amongst his opponents' decisions, those inducing the most severe punishment to the deviator. As a result, any incentive compatible allocation yielding each principal a payoff above a given threshold can be supported at equilibrium, if there are at least three agents.====The present work reconsiders the effect of communication between principals and agents on equilibrium allocations taking a more traditional mechanism design perspective. That is, we evaluate the strategic role of a principal privately communicating with agents in the spirit of the canonical construction of Myerson (1982). The above-mentioned approaches to competing mechanisms disregard this possibility. Indeed, they restrict principals to communicate by posting ==== mechanisms, which implement decisions contingent on the ==== messages received from agents. Yet, to the extent that he cannot directly contract on his opponents' mechanisms, a single principal may in principle gain by sending private signals to agents so to correlate their behaviors with the decisions of all principals. We show that this channel of communication has relevant strategic effects.====We establish our result in the simple framework in which principals compete to attract agents under ====, and each agent only takes an observable action. In such a scenario, we construct an example with two principals and three agents and explicitly characterize the set of equilibrium allocations supportable by recommendation mechanisms. In a next step, we show that ==== of the corresponding equilibria survives when all principals can send private signals to agents. By privately communicating with agents, a principal can make them differently informed of his final decisions. This uncertainty, which cannot be reproduced by standard stochastic mechanisms without signals, crucially affects the continuation game played by agents. We exploit this insight to construct a mechanism with private communication yielding a principal a payoff greater than any of those available without private communication. The result obtains despite the fact that his opponent also sends private signals and delegates to the agents the choice of the (worst) punishment against his mechanism. In the context of the example, this shows that the set of equilibrium allocations supportable by mechanisms with private signals for principals and the set of those supported by mechanisms which do not involve such private communication are disjoint. Finally, we characterize an equilibrium allocation supported by mechanisms with signals, which shows that this enlarged game admits an equilibrium. Yet, equilibrium allocations are typically not unique as we shortly discuss.====A direct implication of our main result is that the equilibria characterized by allowing only agents to privately communicate through possibly large message spaces, as in Epstein and Peters (1999), may not be robust against unilateral deviations towards mechanisms featuring principals' private communication. This in turn indicates that such signals may need to be included in any canonical system of communication, which calls for more theoretical work to identify a corresponding canonical set of equilibrium mechanisms.====To the extent that agents' observable actions can naturally be interpreted as participation decisions, the setting of the example is common to a large number of applications of competing mechanism models in which agents' participation decisions are strategic.==== Alternatively, our example can be reconciled with economic models of competing mechanisms under complete information, in which agents participate with all principals and principals post incentive schemes that assign a decision to each profile of agents' observable actions. This is, for instance, the approach followed by Prat and Rustichini (2003) to model the lobbying process in the presence of several policy makers. Under complete information, these incentive schemes are interpreted as direct mechanisms. As we discuss in Section 4, an implication of our analysis is that the restriction to such direct mechanisms is problematic once principals are allowed to design more sophisticated ones. This stands in contrast with the result of Han (2007), who establishes the robustness of equilibria supported by direct mechanisms against unilateral deviations to indirect ones in competing mechanism games of complete information. Yet, he only considers mechanisms, which allow agents to send private messages to principals but ==== allow principals to send them private signals, a restriction that we prove to be critical.====Our analysis can be cast in the framework of Yamashita (2010) once agents' actions are taken into account. An important limitation of Yamashita (2010) is the focus on deterministic behaviors. That is, agents play pure strategies in every continuation equilibrium, and principals cannot post random contracts. Szentes (2010) shows that the latter restriction is critical for the validity of Yamashita (2010)'s main result by exhibiting equilibrium allocations supported by deterministic mechanisms that yield a principal a payoff ==== Yamashita (2010)'s relevant threshold.==== We admit instead random contracts and mixed strategy equilibria in the agents' continuation game. In our complete information example, if principals do not privately communicate with agents, recommendation mechanisms allow to re-establish a folk-theorem result in the spirit of Yamashita (2010).====Several folk-theorem results have recently been established in the competing mechanism literature. Generalizing the approach of Yamashita (2010), Peters and Troncoso-Valverde (2013) construct an abstract framework in which all players have commitment power and (privately) communicate with each other. The equilibrium distributions over players' decisions can also be correlated, due to the presence of a public correlating device. Under complete information, they show that all the allocations characterized by Yamashita (2010) are supported at equilibrium, together with those arising due to (public) correlation. We consider, instead, the situation in which only a subset of players (the principals) is able to commit while the remaining ones (the agents) take actions given the mechanisms. In this context, we allow each principal to correlate his decisions to the signals he privately sends to each agent. This feature drastically affects equilibrium analysis, since none of the allocations characterized by recommendation mechanisms can now be supported at equilibrium.====A different strategy is followed by Kalai et al. (2010), Peters and Szentes (2012), Peters (2015), and Szentes (2015) who provide attempts at modeling contractible contracts. These works show that by posting contracts that ==== refer to each other, a principal may successfully deter his opponents' deviations. A folk theorem may hence obtain even if no communication takes place after mechanisms are posted, which limits the strategic role of agents and the power of the private communication we exploit.====The feature that principals can send private signals to agents is also key in the literature on information design with multiple senders in which signals affect agents' posterior probabilities over an unknown state of the world. Kamenica and Gentzkow, 2017a, Kamenica and Gentzkow, 2017b consider a Bayesian persuasion game with a single receiver in which each sender's set of signals is sufficiently large to include signals that are effectively correlated with those of the other senders. Koessler et al. (2018) extend this approach in several directions, including the presence of multiple receivers, and focus on uncorrelated signals. We take a more traditional mechanism design perspective in which principals do not hold any private information and send signals to affect agents' beliefs over their realized decisions, which induces correlated outcomes at equilibrium. Our results hold for arbitrarily rich sets of signals available to principals.====This paper is organized as follows: Section 2 introduces a general competing mechanism model, Section 3 presents our example, Section 4 provides a discussion, and Section 5 concludes.",Private communication in competing mechanism games,https://www.sciencedirect.com/science/article/pii/S0022053118304800,12 June 2019,2019,Research Article,246.0
"Ordoñez Guillermo,Perez-Reyna David,Yogo Motohiro","University of Pennsylvania, Department of Economics, 133 South 36th Street, Philadelphia, PA 19104, United States of America,National Bureau of Economic Research, United States of America,Universidad de los Andes, Department of Economics, Calle 19A No 1-37 Este, Bloque W, Bogotá, Colombia,Princeton University, Department of Economics, Julis Romo Rabinowitz Building, Princeton, NJ 08544, United States of America","Received 26 April 2018, Revised 29 May 2019, Accepted 3 June 2019, Available online 12 June 2019, Version of Record 17 June 2019.",https://doi.org/10.1016/j.jet.2019.06.001,Cited by (4)," because bad borrowers could default, and asymmetric information is not always resolved.","How can leverage signal credit quality? Ross (1977) and Leland and Pyle (1977) develop static models of credit markets with asymmetric information about borrowers' profitability and deadweight costs of default. Because good borrowers have a lower probability of default for a given loan amount, they can signal their type through higher leverage in a separating equilibrium. A large empirical literature that followed finds that leverage is negatively related to profitability in the cross-section of firms (Titman and Wessels, 1988; Rajan and Zingales, 1995), which is opposite of the simple theoretical prediction. In a comprehensive review of the literature, Schmid Klein et al. (2002) conclude that there must be other confounding factors that cause the empirical correlation to be opposite of the theoretical prediction.====This paper considers an alternative possibility, that the world is dynamic. A lender can potentially infer a borrower's credit quality based on the entire repayment history, not just the current loan balance. Indeed, repayment history is the most important of five factors that determine the FICO score (FICO, 2015), a leading measure of consumer credit quality, and the only factor that determines the PAYDEX score (Dun and Bradstreet, 2017), a leading measure of credit quality for small businesses.====We study how the lender's perception of credit quality evolves based on the history of borrowing, repayment, and default in a dynamic model of credit markets with asymmetric information. Under what conditions can credit history resolve asymmetric information at no cost? Under what conditions does information revelation involve costly default? Under what conditions does information revelation not even happen? These questions are important insofar as informational frictions could reduce efficiency in credit markets and cause misallocation of real resources to borrowers with worse investment opportunities. We show that the borrower's ability to signal credit quality through debt and repayment and the cost of such signaling strategies depend critically on the degree of uncertainty in collateral value.====We develop a three-period model of credit markets in which borrowers do not have immediate investment needs and borrow purely for signaling reasons, following Ross (1977). There are two types of borrowers, good and bad. Both types of borrowers have a pledgeable asset that can be used as collateral and generates observable income, whose value is subject to uncertainty. Only good borrowers have a non-pledgeable asset that cannot be used as collateral and generates unobservable income. Borrowers maximize net worth, as perceived by the lender or outside investors, which is increasing in ==== (i.e., the perceived probability that the borrower is good).==== The lender is risk neutral and prices debt to break even, conditional on reputation. Reputation is updated through Bayes' rule, based on repayment versus default and rollover choices (i.e., the new loan amount conditional on repayment).====Good borrowers have an incentive to signal through a strategic path of debt and repayment that reveals the presence of unobservable income. Bad borrowers have an incentive to mimic the path of debt, if possible, to delay or prevent information revelation. When uncertainty in collateral value is low, good borrowers fully separate by borrowing a sufficiently high amount and subsequently repaying with unobservable income. Bad borrowers, who do not have unobservable income, must roll over more debt to repay. Therefore, the ability to deleverage signals that the borrower is good. Importantly, signaling through deleveraging is costless because it does not involve default in equilibrium.====The effectiveness of costless separation through deleveraging depends critically on the degree of uncertainty in collateral value. When uncertainty in collateral value is higher, full separation is no longer possible through deleveraging alone. A loan amount that is necessary for separation through deleveraging when the collateral value rises forces bad borrowers to default when the collateral value falls. Although good borrowers do not default, they bear an ex-ante cost of adverse selection through a higher interest rate that reflects the possibility that bad borrowers default. Because we assume no deadweight costs of default in contrast to Ross (1977), the higher interest rate arises from adverse selection only. Bad borrowers benefit at the cost of good borrowers through pure cross-subsidization. In choosing the optimal loan amount, good borrowers must trade off the ex-post benefit of separation against the ex-ante cost of paying a higher interest rate. This tradeoff depends on uncertainty in collateral value because the benefit of separation is constant, while the interest rate for a given loan amount increases with uncertainty as default becomes more likely.====For intermediate uncertainty in collateral value, the benefit of separation outweighs the higher interest cost, so there is ==== in equilibrium. Good borrowers borrow a sufficiently high amount to fully separate by deleveraging if the collateral value subsequently rises. However, if the collateral value falls instead, bad borrowers do not have sufficient collateral and are forced to default. Thus, there is full information revelation with bad borrowers rolling over more debt in good states and defaulting in bad states.====For high uncertainty in collateral value, the higher interest cost outweighs the benefit of separation, so full separation is no longer optimal. Good borrowers borrow a relatively low amount such that, if the collateral value subsequently rises, even bad borrowers repay by rolling over a low amount, so there is no information revelation. If the collateral value falls instead, good borrowers fully separate by deleveraging. Thus, there is ==== in equilibrium, only in bad states.====In summary, the cost of asymmetric information rises with uncertainty in collateral value. When uncertainty in collateral value is low, good borrowers can costlessly signal their type by deleveraging, and asymmetric information is fully resolved. When uncertainty in collateral value is intermediate, the equilibrium entails bad borrowers defaulting in bad states. Although asymmetric information is fully resolved, good borrowers must bear an adverse selection cost through a higher interest rate. When uncertainty in collateral value is high, good borrowers borrow a conservative amount such that asymmetric information is not always resolved. In this case, there are realized paths of collateral value such that both types repay or default.====An important implication of our results is that credit history is a less precise signal of credit quality in environments with high uncertainty. For example, uncertainty could be higher in recessions or for collateral that is difficult to value. In such environments, asymmetric information becomes a more relevant friction that credit history cannot fully resolve, which leads to a higher probability of default and a higher interest rate.====Because the intended purpose of our model is to provide general insights about leverage dynamics and credit quality, it is not tailored to any particular credit market. However, we motivate our model with two examples of short-term credit markets with private information and uncertainty in collateral value. In consumer credit markets, a new borrower without a credit history may put a balance on a credit card to establish a credit history. A successful history of balances followed by repayment would improve the FICO score, which may be useful for a future durable-good purchase or capital investment. Although credit cards are a type of unsecured debt, creditors have “collateral” in the form of expected recovery value. The pledgeable asset in our model corresponds to the part of future income allocated to repayment of unsecured creditors in a chapter 13 bankruptcy.====In a banking context, asset-backed commercial paper (Acharya et al., 2013) and repurchase agreements (Gorton and Metrick, 2012) are examples of short-term collateralized debt. Banks borrow extensively through these short-term (sometimes overnight) contracts that are collateralized by asset-backed securities. These contracts must be rolled over frequently because of their short maturity, which provides frequent signaling opportunities through changes in debt and repayment. Investors' beliefs about the bank's credit risk is priced into contract terms such as repo rates and haircuts on collateral. In extreme scenarios, a bank may not be able to roll over contracts if its perceived credit risk is too high.",Leverage dynamics and credit quality,https://www.sciencedirect.com/science/article/pii/S0022053118301182,12 June 2019,2019,Research Article,247.0
Guo Huiyi,"Department of Economics, Texas A&M University, United States of America","Received 19 April 2017, Revised 21 April 2019, Accepted 27 May 2019, Available online 5 June 2019, Version of Record 11 June 2019.",https://doi.org/10.1016/j.jet.2019.05.009,Cited by (9),"This paper introduces ambiguous transfers to the problems of full surplus extraction and implementation in finite dimensional naive type spaces. The mechanism designer commits to one transfer rule but informs agents of a set of potential ones. Without knowing the adopted transfer rule, agents are assumed to make decisions based on the worst-case expected payoffs. A key condition in this paper is the Beliefs Determine Preferences (BDP) property, which requires an agent to hold distinct beliefs about others' information under different types. We show that full surplus extraction can be guaranteed via ambiguous transfers if and only if the BDP property is satisfied by all agents. When agents' beliefs can be generated by a common prior, all efficient allocations are implementable via individually rational and budget-balanced mechanisms with ambiguous transfers if and only if the BDP property holds for all agents. This necessary and sufficient condition is weaker than those for full surplus extraction and implementation via ","Many transaction mechanisms have uncertain rules. For instance, Priceline Express Deals offer travelers a known price for a hotel stay, but the exact name and location of the hotel remain unknown until the completion of payment. Alternatively, some stores run scratch-and-save promotions. Consumers receive scratch cards during check-out, which reveal discounts of uncertain levels, and thus the costs of their purchases remain unknown at the time they decide to buy. As a third example, eBay allows sellers of auction-style listings to set hidden reserve prices.====In all the above mechanisms, the mechanism designer introduces uncertainty about the allocation and/or transfer rule without telling agents the underlying probability distribution. The subjective expected utility model can be adopted to describe agents' decision making without an objective probability. However, since Ellsberg (1961), many studies have challenged this model, arguing that decision makers tend to be ambiguity-averse. Therefore, it is important to understand if and how a mechanism designer can benefit from agents' ambiguity aversion. More specifically, we would like to know whether engineering ambiguity on rules of mechanisms can help the designer achieve the first-best outcome.====This paper introduces ambiguous transfers to study two problems. One is full surplus extraction. The other is interim individually rational and ex-post budget-balanced implementation of any ex-post efficient allocation rule. There is one mechanism designer (assumed to be female) and at least two agents (assumed to be male). The analysis is based on finite dimensional naive type spaces, i.e., those in which each agent's type is his payoff-relevant private information. The problem of full surplus extraction aims to design a mechanism in which agents transfer the entire surplus to the designer. The efficient implementation problem constructs an interim incentive compatible, interim individually rational, and ex-post budget-balanced mechanism such that the socially optimal outcome emerges as an equilibrium. In our model, the mechanism designer informs agents of the exact allocation rule. She also commits to one transfer rule, but the communication is ambiguous so that agents only know a set of potential transfer rules. Without knowing the adopted transfer rule, agents are assumed to be ambiguity-averse. More specifically, agents are maxmin expected utility maximizers who make decisions based on the worst-case scenario.====Our main result shows that the necessary and sufficient condition to ensure the existence of first-best mechanisms with ambiguous transfers is the Beliefs Determine Preferences (BDP) property.==== In particular, we show that full surplus extraction can be guaranteed via ambiguous transfers if and only if the BDP property is satisfied by all agents. In addition, when agents' beliefs can be generated by a common prior, every efficient allocation rule is implementable via an interim individually rational and ex-post budget-balanced mechanism with ambiguous transfers if and only if the BDP property holds for all agents. By confining the analysis to private value common prior environments, we further show that efficient implementation can be guaranteed if and only if the BDP property fails for at most one agent. As an extension, we establish necessary and slightly stronger sufficient conditions for efficient implementation when beliefs may not be generated by a common prior. Lastly, we discuss the robustness of our sufficiency results under alternative models of ambiguity aversion.====The BDP property is weaker than Crémer and McLean (1988)'s Convex Independence condition, which is necessary and sufficient to guarantee full surplus extraction via Bayesian mechanisms. Convex Independence, together with the Identifiability condition established by Kosenok and Severinov (2008), is necessary and sufficient for implementing any efficient allocation rule via an interim individually rational and ex-post budget-balanced Bayesian mechanism. In both the full surplus extraction and the implementation problems, the ambiguous mechanism design approach requires a strictly weaker condition to obtain the first-best outcome than the Bayesian approach. As a result, when the conditions of Crémer and McLean (1988) or Kosenok and Severinov (2008) fail, engineering ambiguity deliberately may allow the designer to achieve first-best outcomes that are impossible under Bayesian mechanisms. Intuitively, this is because when ambiguous transfers are introduced, we do not need to construct one transfer rule satisfying all incentive compatibility constraints simultaneously. Instead, different transfer rules can be adopted to guarantee different incentive compatibility constraints, which may make mechanism design problems easier.====In applications, it is of interest to study some cases where Crémer and McLean (1988) or Kosenok and Severinov (2008)'s necessary and sufficient conditions fail, although a few works that we discuss in Section 1.1 show that these conditions are weak in some sense. In particular, the BDP property imposes weaker restrictions on the cardinality of the finite type space than Convex Independence and Identifiability. For example, when one agent has more types than the number of type profiles of all other agents, Convex Independence fails for this agent with positive probability, in which case the mechanism designer cannot always extract the full surplus. As another instance, Kosenok and Severinov (2008)'s necessary and sufficient conditions can never hold simultaneously for any common prior with only two agents, indicating an impossibility result on two-agent implementation problems. However, the BDP property holds for all agents in finite dimensional naive type spaces generically regardless of the number of agents and the cardinality of the type space. Hence, allowing for ambiguous transfers may help the mechanism designer to resolve these impossibility results.====In this paper, the mechanism designer announces an efficient allocation rule and introduces ambiguity in transfer rules only. As the ex-post efficient allocation rule is often unique in a finite-type framework, the mechanism designer may not have multiple allocation rules to choose from. In a related paper, Di Tillio et al. (2017) study how second-best revenue in an independent private value auction can be improved if the seller introduces ambiguity in both allocation and transfer rules. We discuss more on the relationship with this paper in Section 1.1.====The paper proceeds as follows. We review the literature in Section 1.1 and introduce the environment in Section 2. Our main result is presented in Section 3. Section 4 extends our main result along two directions. Section 5 concludes. The Appendix collects all omitted proofs from Section 3. Omitted proofs and examples from Section 4 are relegated to the Online Appendices.",Mechanism design with ambiguous transfers: An analysis in finite dimensional naive type spaces,https://www.sciencedirect.com/science/article/pii/S0022053119300559,5 June 2019,2019,Research Article,248.0
"Nehring Klaus,Pivato Marcus","Department of Economics, UC Davis, CA, USA,THEMA, Université de Cergy-Pontoise, France","Received 15 February 2018, Revised 11 May 2019, Accepted 19 May 2019, Available online 4 June 2019, Version of Record 17 June 2019.",https://doi.org/10.1016/j.jet.2019.05.006,Cited by (8),"Which is the best, impartially most plausible consensus view to serve as the basis of democratic group decision when voters disagree? Assuming that the judgment aggregation problem can be framed as a matter of judging a set of binary propositions (“issues”), we develop a multi-issue majoritarian approach based on the criterion of ==== (SME). SME reflects the idea that smaller supermajorities must yield to larger supermajorities so as to obtain better supported, more plausible group judgments. As it is based on a partial ordering, SME delivers unique outcomes only in special cases. In general, one needs to make cardinal, not just ordinal, tradeoffs between different supermajorities. Hence we axiomatically characterize the class of ====, whose (generically unique) outcome can be interpreted as the “on balance most plausible” consensus judgment.","The dominant paradigm of decision-making in groups by voting is preference aggregation. In preference aggregation, inputs (votes) are understood to express what is best for each voter personally. The crux of preference aggregation is a conflict of interest. The normative question is thus how to best resolve those conflicts of interest in a fair and efficient manner.====An alternative paradigm of decision-making in groups by voting is ====. In judgement aggregation, inputs (votes) concern judgements on the best course of action for the group as a collective agent. Inputs are views about what the group should do, about which decision is right. These judgements may be about the best choice directly, or may concern the basis for such choice in views about what is right as a matter of, say justice or law, or in beliefs about matters of fact. The crux of judgement aggregation is ====; the output (group choice) aims at a ==== by means of a consensus. A good consensus is one that has most merit in light of the input judgements. The normative standard is thus, broadly speaking, “epistemic” rather than “ethical”.====In this paper, we adopt this less common, and, in our view, still underdeveloped judgement aggregation perspective. Our aim is to develop a broadly applicable “====” approach whose intellectual ancestry can be tracked back to Condorcet (1785) via Guilbaud (1952). While much of the judgement aggregation literature has focused on impossibility results, we offer a ==== result by providing a general axiomatic characterization of a (novel) class of aggregation rules called ==== (AMRs); these include the widely studied ====. While we do not pin down a unique rule, we obtain a parsimoniously characterized class.====The judgement aggregation perspective often arises naturally from the institutional context of the voting problem. For example, ==== are often asked to come up with a decision that is effective or binding for a larger polity. Such committees include corporate boards, central banks, juries of various kinds, and multi-member courts, including supreme courts. “Aggregation” here is about making best use of the committee members' judgemental inputs, rather than about figuring out how best to satisfy their personal preferences.====But the judgement aggregation perspective is pertinent not only to committees with delegated decision power, but also to ==== with sovereign decision power. Regarding decisions for public office, for example, there is an important distinction between voting on personal preference and voting on a more impersonal view of the merit or quality of the candidate. Indeed, many pioneering contributions to social choice theory such as Lull (1299), Cusanus (1434) and Condorcet (1785) adopted the merit-oriented perspective; it has been forcefully developed more recently by Balinski and Laraki (2010). In political referenda, in some situations the preference aggregation perspective is the more natural one, as for instance in decisions on local public goods. But others are more focused on the perceived social merit of a decision, as for instance in referenda on same-sex marriage, to give an example of current interest. Frequently, such as in decisions on taxation and redistribution, both perspectives will be pertinent and illuminating.====The two perspectives enjoy different relative prominence in different disciplines. In economics, the preference aggregation perspective dominates almost entirely. By contrast, a broader view is often taken in political science and political philosophy, and the judgement aggregation perspective is much more prominent there, as exemplified by the work of authors such as Waldron (1999), Christiano (2008), Estlund (2008), List and Pettit (2011), and Schwartzberg (2015).====Our axiomatic approach aims at normatively optimal or at least sound aggregation rules within a multi-issue majoritarian approach. A strength of that approach is its great versatility and applicability to a wide range of applications. In special settings, there exist serious normative competitors to majoritarian aggregation rules. These are often of an averaging type. For example, in the literature on aggregating subjective probabilities, averaging-type rules predominate. In aggregating ====, the Borda rule (which averages ranks) is a prominent alternative, while in aggregating ==== (see Example 3.1 below), the average grade is an obvious alternative to the median grade. Yet even if it is found that, in a particular setting, the “ideal” consensus view takes a non-majoritarian, say averaging, form, there will often still be “second-best” considerations that favor majoritarian aggregation rules, and in particular, additive majority rules, as sounder and more reliable.====Broadly speaking, majoritarian aggregation rules are likely to work better than averaging rules when there is concern with the quality of judgmental inputs, and/or with underlying incentives. Firstly, averaging is highly sensitive to individual judgements. As Galton (1907) observed, the average “gives voting power to cranks in proportion to their crankiness”; Galton thus championed the median, as the least exposed to the influence of unreasonable or erratic opinions. (Note that robustness to cranks is a concern distinctive of judgement aggregation; by contrast, from a preference aggregation perspective, there is no reason to discount unusual preferences.) Secondly, averaging provides strong and clear-cut incentives for any voter (judge) to submit a more extreme judgement. Additive majority rules, by contrast, appear to be significantly more robust against strategic manipulation. In particular, on judgement spaces where fully strategy-proof anonymous and neutral aggregation rules exist at all, these rules take the form of issue-by-issue majority voting, and are thus AMRs.==== AMRs enjoy also “partial strategy-proofness” properties.====While AMRs are new in the literature, special cases are not. The median rule, in particular, has been studied extensively. In the context of the ranking problem, the median rule is also known as the ==== (1959), and has been axiomatized in the classic contribution by Young and Levenglick (1978). The median rule has been studied quite extensively for other special classes of judgement spaces as well (Barthélémy and Monjardet, 1981, Barthélémy and Monjardet, 1988; Barthélémy and Janowitz, 1991; McMorris et al., 2000). In addition, two “limiting” cases of AMRs have been studied in the special context of the ranking problem. These are the ==== (1961), and the ==== rule proposed by Tideman (1987). While these are not themselves AMRs, they possess refinements that ==== AMRs, as described in Section 4.====A technically interesting feature of our analysis is the need for representations ranging over the ==== numbers rather than just real numbers. For example, in order to satisfy the key axiom of ====, the aforementioned refinements of the Ranked Pairs and Slater rule require hyperreal-valued representations. We hope that our techniques may be useful in other applications to social choice and decision theory. At the same time, there is little loss to the larger picture if the reader neglects the distinction between reals and hyperreals.====The remainder of the paper is organized as follows. Section 2 sets the stage: Subsection 2.1 outlines our argument for a ==== approach to judgement aggregation; Subsection 2.2 introduces ==== as a framework for studying judgement aggregation (with some key examples); and Subsection 2.3 generalizes judgement spaces to ====, in which different issues can have different weights, and the input and output spaces can differ. Section 3 introduces our key axiom, ====, which formalizes the majoritarian philosophy articulated in Section 2.====Section 4 formally defines the class of ====, and discuss some properties and key examples of this class. Section 5 introduces our second axiom, ====. Section 6 contains our main results: the axiomatic characterization of additive majority rules. Finally, Section 7 considers some caveats and discusses some possible extensions.====Appendix A contains more general versions of the main results from Section 6. Appendix B is a brief formal introduction to hyperreal numbers. Appendix C exposits some convenient properties of additive majority rules. Appendices D and E contain the proofs of the main results stated in Section 6 and Appendix A.",Majority rule in the absence of a majority,https://www.sciencedirect.com/science/article/pii/S0022053119300535,4 June 2019,2019,Research Article,249.0
Kolb Aaron M.,"Indiana University, Kelley School of Business, United States of America","Received 8 June 2018, Revised 20 May 2019, Accepted 23 May 2019, Available online 31 May 2019, Version of Record 2 July 2019.",https://doi.org/10.1016/j.jet.2019.05.008,Cited by (15),"I study hidden investment in quality in a dynamic persuasion game. A seller, such as a project manager or startup company, controls an asset and wants to convince a potential buyer, such as an acquiring company or a regulator, that it has high quality. The buyer observes exogenous news with quality-dependent drift and holds a call option on the asset; the seller is privately informed about quality and can wait (at a flow cost), upgrade (at a fixed cost) or exit the market.====For low upgrade costs, multiple equilibria exist and exhibit novel reputational dynamics — resetting barriers and skew Brownian motion, a generalization of a reflected stochastic process. In one type of equilibrium, a seller with low quality waits until his reputation hits “rock bottom” and then upgrades with some probability, inducing upward reputational jumps in equilibrium; the seller may exit at an intermediate reputation. In a second type, the low quality seller mixes over exiting at low reputations and upgrading in an intermediate “sweet spot” region. The results suggest a social benefit to setting high expectations for effort when agents start with low reputations and help explain why agents respond to adversity in drastically different ways across environments.","In many environments, a sender wants to persuade a receiver to take some favorable action — for example, technology startup companies seek acquisition by large firms, software developers attempt to convince potential corporate clients to adopt their products or services, and pharmaceutical companies must obtain FDA approval in order to sell new drugs. In such interactions, the receiver often obtains information over time regarding the quality of the underlying asset and chooses when and how to respond. However, in many applications, the sender is also an active participant and can influence the receiver's problem. In particular, the sender often has some control over both the availability and quality of the asset. When should the sender admit defeat and cut its losses, and when should the sender sink further costs to improve quality?====To illustrate, consider a technology startup company which is developing some new software with the goal of being bought out by a large technology firm. The startup wants to convince the potential buyer that its asset — either the software or the startup's personnel — is of high quality. The buyer does not know the true asset quality initially but the buyer learns about this quality gradually as bits of information become available through various channels such as product testing and user reviews. The buyer thus faces the real option problem of when to acquire the startup. The startup naturally has more information about the quality, and accordingly whether the information that the buyer obtains will tend to be good or bad on average. Continuing the development involves opportunity costs for the startup, and if it is unlikely to attract the buyer soon, it may shut down. Additionally, the startup can exert effort to improve or upgrade the quality of the software. For example, the startup can fix design flaws in order to improve the software's security or stability, or hire better programmers for the team. In many cases, such improvements are not directly observable to those outside the startup, either because the actions themselves are difficult to monitor, or because understanding these actions requires significant expertise.====To capture some key features of situations like the one above, I model a game between two long-lived players, a privately informed seller and a potential buyer. The seller has a binary type, high or low, according to the asset's quality and can remain in the market at a flow cost, exit, or take a costly and hidden upgrade action to improve the quality. To focus on reputational dynamics, I model the buyer as simply as possible: she chooses when to buy (“adopt”) at a fixed price.==== The buyer learns about the seller's quality from exogenous news, which I model using Brownian motion to capture that both good and bad news arrive over time. Given this exogenous news and the conjectured seller strategy, the buyer forms a belief or reputation process tracking the probability that the seller is a high type.====In the model described above, there are multiple actions available to the seller; these capture two distinct channels the seller might use to persuade a potential buyer that it has high quality. The first channel is to fundamentally improve that quality, which is unobserved in the short term but gradually learned in the long term. The second channel is to signal that quality by remaining in the market despite an unfavorable reputation; this is a converse of the first channel in the sense that the choice to remain is directly observed but has no effect on quality. This paper addresses the question of which channel a seller uses and when; does a seller put more effort into quality when its reputation improves or when it falls? I demonstrate that this environment gives rise to new reputational dynamics in equilibrium, and I present novel methods for characterizing equilibria.====Before describing the new reputational dynamics and the methodology, I point out that the game I consider falls into a broader class of related persuasion games.==== With slight alterations to the model, the methods presented in this paper could be used to tractably analyze the role of hidden investment in various other applications.====: In Daley and Green (2012), extending the canonical lemons model of Akerlof (1970), a seller receives offers continuously from a market of competitive buyers who learn over time about the seller's (asset's) quality. Rejecting low offers in their model is analogous to remaining in the market in my model, and either serves as a favorable signal of the seller's quality. In a lemons model, a seller might have an incentive to upgrade the quality of its asset in order to accelerate favorable news and offers.====: Henry and Ottaviani (2017) and McClellan (2017) study a persuasion game between an agent and a regulatory authority. The agent provides costly information to the authority who chooses whether to approve the agent's project. In this regulatory approval setting, upgrades could correspond to hidden investments in the safety or efficiency of an innovation or product. In related work, Orlov et al. (2018) study dynamic information design by an agent attempting to influence the timing of action by a principal such as a regulator.====: While my model involves a seller who desires to hasten a positive outcome, a mirror image would be a model in which the seller seeks to forestall a negative outcome such as project termination. For instance, the seller could be a supplier who wants to sustain an existing relationship with a client, and the client observes the noisy signals of the quality of that supply and can choose when to terminate that relationship. In some cases, the supplier might have an incentive to make a direct, hidden investment in the quality of its supply to sustain the relationship. In related work, Madsen (2018) studies a dynamic contracting environment in which an expert privately observes the (random but exogenous) date at which a project ceases to be profitable and must be given incentives to report the truth.====: Market incumbents who are privately informed about their marginal costs often engage in limit pricing to persuade a potential competitor not to enter the market; see, for example, Goolsbee and Syverson (2008). In Gryglewicz and Kolb (2018), the market size is a diffusion process and the weak incumbent persuades the entrant by maintaining low prices even when the market becomes unfavorable to entry. In limit pricing settings, an upgrade by an incumbent could be an investment in cost-reducing technology.====The key feature of my model, and the main innovation relative to the existing literature, is the upgrade capability of the seller, which has two effects. The direct effect is that when upgrading actually occurs, the seller's type improves and the drift of the news (and thus reputation) process increases accordingly. The indirect effect is that when the buyer believes that behind-the-scene upgrading might have occurred, she revises her belief upward according to Bayes' rule. Due to the indirect effect, the seller does not upgrade with probability one in any equilibrium; if this were the case, the buyer would become certain that the quality is high and adopt immediately, but then the seller would prefer not to perform the costly upgrade. On the other hand, when upgrades are sufficiently cheap, upgrading must be a part of the seller's strategy because otherwise the benefit of the direct effect alone would outweigh the seller's cost of upgrading at some reputation levels.====When the upgrade cost ==== exceeds a threshold ====, a unique equilibrium exists subject to mild selection criteria.==== In this equilibrium, the low quality seller never upgrades but randomizes over exit at a lower threshold such that the equilibrium reputation process conditional on no exit is a reflected process, as in the existing literature.==== On the other hand, when upgrading is sufficiently cheap, multiple equilibria exist since the buyer's expectations and the seller's actions may be coordinated in various ways. The reputational dynamics in these equilibria feature two novel elements — resetting barriers and skew Brownian motion, which generalizes the notion of a reflected process.====I characterize two qualitatively different families of equilibria, one in which the seller is endogenously ==== and the other ====. In all the equilibria, the buyer plays a threshold strategy, but the form of the seller's strategy varies. When motivated by failure, the seller upgrades only when his reputation hits “rock bottom.” At that point, he upgrades with a large probability, and the buyer immediately resets her belief to a higher level, despite not seeing whether any upgrading took place. In entrepreneurship, this belief resetting could manifest as a ====, whereby a firm or startup drastically changes its business or marketing strategy. While the pivot is publicly observed, the change in reputation is based on the market's conjecture about unobserved, fundamental improvement.==== In this kind of equilibrium, the seller is worse off at intermediate reputations, where he is neither good enough for the buyer to adopt nor bad enough to be motivated to upgrade. In a business setting, a seller with a falling reputation would eventually have an incentive to acknowledge or even exaggerate its faults in order to make more credible any claims about efforts to improve, allowing its reputation to quickly bounce back. For example, in early 2014, General Motors announced the creation of a new “global product integrity” division in response to scrutiny over widespread ignition switch failures. The motivation by failure equilibrium in my model is consistent with such announcements, and my results also predict that firms will often ==== make significant changes following such announcements.====A unique implication of the reputational jumps under motivation by failure is that, when the cost of upgrading is below ==== but above a lower threshold ====, the seller may even exit the market with an intermediate reputation rather than a low one. As the seller teeters on the verge of exiting, his reputation is continuously regulated upward conditional on not exiting at this point. Technically, this phenomenon manifests as a process known as ====, discussed below. As the upgrade cost increases, the rate of exit increases until it creates a reflecting barrier, trapping the reputation process and crowding out upgrades.====On the other hand, if the buyer expects the seller to upgrade prior to hitting rock bottom, then the seller may instead be motivated by success in equilibrium. In this case, the seller is too discouraged to upgrade at very low reputations, since the benefit of being adopted sooner is heavily discounted, and instead randomizes over exit. The seller must wait until his reputation reaches an intermediate “sweet spot” before being motivated to upgrade. In equilibrium, the buyer correctly accounts for the hidden upgrading at this sweet spot and revises her beliefs upward, and again the resulting reputation process is skew Brownian motion. If the cost of upgrading is below a threshold ====, this sweet spot region expands to an interval.====The main results of the paper are thus characterizations of five equilibria, at most two of which coexist (one with motivation by failure and one with motivation by success) for a given set of input parameter values; Fig. 1 summarizes. One can view motivation by failure and motivation by success as two distinct “paths” of equilibria which both converge to a common, essentially unique equilibrium as the upgrade cost increases.====A synthesis of these results is the following. When upgrading is sufficiently expensive, no upgrading occurs in equilibrium, and some exit is a necessary feature of equilibrium. In contrast, when upgrading is sufficiently cheap, all equilibria involve upgrading in some form. In this case, there exists an equilibrium (under motivation by failure) in which exit does not occur, but there also exists an equilibrium (under motivation by success) in which exit does occur. For low starting reputations, the latter represents a sort of “bad coordination” because there is exit as opposed to value-creating upgrades. Indeed, if the seller begins with a sufficiently low reputation, then motivation by failure strictly Pareto dominates motivation by success (all players and types are better off under the former).====A contribution of this paper is the methodology used to characterize the reputational dynamics. In both instances above, skew Brownian motion arises when the seller's incentive for a particular action, exit or upgrade, is nonmonotonic. In the “motivation by success” equilibria, the resetting of low reputations creates a U-shaped value function for the low type; in the “motivation by failure” equilibria, there is a hump-shaped ==== between value functions for low and high types due to two opposing forces: discounting (which reduces the incentive to upgrade at low reputations) and volatility (which reduces the incentive to upgrade at high reputations). Skew Brownian motion was first developed rigorously by Harrison and Shepp (1981) and has since been used in diverse fields.==== To my best knowledge, skew Brownian motion is new to the economics literature, but its appealing feature is its tractability. The special behavior of skew Brownian motion is characterized by a single intensity parameter which can be interpreted in two ways. In discrete time, one can construct skew Brownian motion by a sequence of random walks whose “up” probabilities at the critical point are a constant other than 1/2, depending on the intensity. In continuous time, the process is regulated upward in proportion its local time at the critical point, and is thus the solution to a stochastic differential equation with an additional term for the process's local time.====An implication of the asymmetry of skew Brownian motion is that players' value functions form a kink at the point of skew. Strulovici and Szydlowski (2015) show that in a large class of problems optimal value functions must be continuously differentiable, but the main reason their result does not apply here is that the equation for skew Brownian motion involves the additional local time term. However, it is precisely this nondifferentiability which makes skew Brownian motion tractable; the intensity parameter is related to the ratio of left- and right-derivatives of value functions at the kink (when these derivatives are not both zero) and can thus be identified through a boundary condition.====Resetting barriers are also new to the reputations literature. Dixit (1993) discusses resetting barriers applied exogenously to a stochastic process and gives necessary boundary conditions for value functions. Dumas (1991) shows that the optimal regulation of a stochastic process involves resetting if regulation has a fixed cost component. However, this result does not extend to my analysis since the upgrade action is hidden and the reputation process must obey Bayes' rule. Resetting barriers have since been studied as exogenous policy instruments in both biostatistics (Grigg and Farewell, 2004) and finance (Sircar and Xiong, 2007).====There is a long-standing literature on real options in which the object of investment is nonstrategic (McDonald and Siegel, 1986; Dixit and Pindyck, 1994). Décamps et al. (2005) introduce uncertainty about the drift of the object's value, and Orlov et al. (2018) add dynamic Bayesian persuasion of the option holder. Another strand of literature examines exit-like behavior in reputation games. A unifying result in this literature is that at low reputations, weak types are indifferent between mimicking strong types or revealing themselves by giving up (Daley and Green, 2012; Bar-Isaac, 2003; Gul and Pesendorfer, 2012; Kolb, 2015). By contrast, I show that the seller's upgrade ability can eliminate exit altogether, or lead to exit at intermediate reputations.====Several recent papers analyze endogenous seller quality and buyer learning in dynamic settings. Board and Meyer-ter Vehn, 2013, Board and Meyer-ter Vehn, 2014 model hidden investment using stochastic technology shocks; the latter of these allows for market exit, but equilibrium existence is established with a fixed point theorem, whereas my simpler upgrade technology allows explicit constructions. The model of Dilmé (2019) features instantaneous upgrades and downgrades in quality. In those papers, learning is Poisson and there are no reputational jumps apart from Poisson news events. In Atkeson et al. (2014), sellers can freely enter and exit, but hidden investment is modeled as a one-shot opportunity at the beginning of the game.====The information structure follows Daley and Green (2012) and also appears in recent work on approval mechanisms (Henry and Ottaviani, 2017; McClellan, 2017). Other papers relate with respect to learning about project quality (Bergemann and Hege, 1998, Bergemann and Hege, 2005), optimal adoption (Frick and Ishii, 2015), and endogenous types (Cisternas, 2018). Finally, the current paper relates to Heinsalu (2017) where dynamic interactions also give rise to multiplicity of equilibria.====The paper is organized as follows. Section 2 presents the model and equilibrium concept. Section 3 analyzes motivation by failure and gives the uniqueness result for high upgrade costs. Section 4 analyzes motivation by success. Section 5 discusses the welfare properties of equilibria, the role of news quality and the possibility of pre-game communication, and Section 6 concludes. All proofs are contained in the appendix or supplementary appendix.",Strategic real options,https://www.sciencedirect.com/science/article/pii/S0022053118302576,31 May 2019,2019,Research Article,250.0
"Dutta Prajit K.,Siconolfi Paolo","Columbia University, United States of America","Received 17 October 2018, Revised 21 February 2019, Accepted 12 May 2019, Available online 30 May 2019, Version of Record 11 June 2019.",https://doi.org/10.1016/j.jet.2019.05.005,Cited by (1),"This paper studies asynchronous dynamic games with one period ahead transfers. There is a unique equilibrium that coincides with the Utilitarian Pareto Optimum whenever the horizon is finite. With an infinite horizon, the same result holds when action history dependence is allowed but not history dependence on transfers. The result is restored with a finite but costly memory of transfers as well as with continuous transfer strategies. Multiplicity can arise from strategies that have an infinite memory of transfers. Finally, we provide a full characterization of equilibrium payoffs when players become infinitely patient.","This paper studies a class of dynamic games over a finite and infinite horizon. The canonical model is the repeated game model in which players move simultaneously every period and there are no side-payments and the typical result is a Folk Theorem - multiple, indeed infinite number of, equilibria that are, generically, inoptimal.==== We show that if, instead, players ==== moving and can commit to a ====, then, under a variety of circumstances, there is a ==== Subgame Perfect Equilibrium and it is ====.====Consider the two italicized assumptions: ==== and ====. Individually, both assumptions are standard. Although simultaneous moves is the more common timing assumption, many authors have argued that it should not be the ==== benchmark. Two seminal papers with alternating moves are Rubinstein (1982) and Maskin and Tirole, 1988a, Maskin and Tirole, 1988b. In the bargaining model of Rubinstein (1982), players take turns making proposals. Maskin and Tirole, 1988a, Maskin and Tirole, 1988b assume that firms are committed to an action for a period due to exogenous reasons; installed capital, production lags, short-term binding contracts, etc. Alternating moves follow if a competitor is able to react before the commitment expires. In the above papers, alternating moves lead to interesting equilibrium consequences that are absent with simultaneous moves.==== Yet these results are special in that there are many other equilibria as well. Alternating move games are examples of stochastic games (with the fixed actions in a period being the “state”). Hence, by Dutta (1995), there is a Folk Theorem in these games.====Similarly, transfers are routinely studied in (static) Mechanism Design and Auction Theory and, in some settings, induce social optimality (via the Vickrey-Clarke-Groves mechanism). Yet, in general, when transfers are present and can be made contingent on players' own-actions, even a static game can have multiple equilibria and none of these equilibria need be efficient. This point has been forcefully made by Jackson and Wilkie (2005). Hence, when the stage game (with transfers) is repeated, yet again a Folk Theorem emerges (because of the multiplicity in stage game Nash equilibria).====Hence, having either alternating moves or transfers by itself still implies a Folk Theorem. This paper shows that if the two assumptions are combined, we have a ====. The statement is always true in the finite horizon model. In the infinite horizon model, the statement holds if transfers can depend on past actions but not on past transfers. The equilibrium has two distinctive properties. First, after every history, there is utilitarian optimality - the unique SPE has values whose sum coincides with what a “planner” would get if she were to maximize the sum of players' payoffs over the game horizon.====Second, there is maximal ==== of the sum; after every history, all surplus accrues to one of the players and the other is held down to a “reservation value”.====We contextualize our result by discussing the relevant literature in Section 9 but we should emphasize two aspects. First, the result holds for ==== discount factors, i.e., the efficiency property is not just an asymptotic result like a Folk Theorem. Second, it holds for ==== stage games, i.e., the uniqueness property is not driven by particular stage game features like dominant strategies or symmetry.====Instead, the result is driven by two consequences of having alternating moves and transfers. Transfers are a simple “smooth” way through which the transfer-giver can modulate the lifetime payoffs of the recipient. In particular, she can “induce” the recipient to take any action by an appropriate action-contingent transfer, much as a Principal can induce the Agent in an agency model. Not only that, the transfer-giver would induce an action by paying a minimal transfer, i.e., would make the recipient indifferent between the action induced and an alternative that the recipient would prefer absent transfers. Whenever one player is indifferent between actions, the other player extracts the full surplus, i.e., acts as a “planner” and hence takes the same transfer (and induced action) decision as a planner would. Hence, efficiency. If transfers were simultaneously decided, then, of course, both players would have this incentive and it is unclear that either “Principal” would be able to achieve their goal. That is where alternating moves helps by removing this “coordination problem.”====The description here should not be taken to mean that our model works the same way as a standard Principal-Agent model. For three reasons. First, unlike in an agency model, payoffs are ongoing, i.e., generated every period. So, unlike a Principal-Agent (Stackelberg like) model where the Principal gets no payoff while picking the transfer schedule (and only gets a payoff once the Agent acts in the “next period”), here incentives are more complicated. Second, there are no fixed roles of Principal and Agent. The same player is a transfer-giver in one period and a transfer-recipient in the next, i.e., the roles of Principal and Agent are fluidly ever changing. Finally, unlike many agency models, the transfer-giver cannot commit to future transfers beyond the current choice.====One way of thinking about our result is that we propose a simple decentralized mechanism to deliver uniqueness and efficiency. Hence, the result and model may have general implications for Repeated Agency, Dynamic Bargaining and other ongoing strategic interactions.==== The paper analyzes two-player==== games ==== played over either a finite or infinite horizon.==== There is a fixed stage game. In each period, only one player - she - moves. A move has two components; first, choice of an action from the stage game. Second, choice of a non-negative transfer schedule according to which she pays her opponent next period (the actual payment depending on the opponent's action). Period payoffs for the mover are the sum of stage-game payoffs and the transfer from her opponent while for the non-mover are stage-game payoffs less transfer. To emphasize the point made above: in this model, payoffs are ongoing, like they are in the canonical simultaneous move Repeated Game with the one difference that there is a fixed action every period.==== In the canonical simultaneous moves model with no transfers, one either gets uniqueness or efficiency but not typically both. If the stage-game has a unique stage Nash equilibrium (NE) - as in the Prisoners Dilemma - repeated play of that stage NE is the unique Subgame Perfect Equilibrium (SPE). Else, if there are multiple stage NE - as in Battle of the Sexes - then Benoit and Krishna (1985) prove a Folk Theorem.====In our model, one gets both; there is a unique pair of SPE value for every horizon whose sum coincides with the Utilitarian Solution. Since the planner's problem has a Markovian solution, the SPE actions and transfers are easily computable and the latter are bounded (independently of horizon length).==== In Section 4 we derive the exact results for the Prisoners Dilemma stage game. The intuition driving the result is the one described a few paragraphs back: that the transfer-giver holds the recipient to a binding no-transfer value and consequently extracts full surplus. In equilibrium, that happens every period and for every transfer-chooser.==== When we analyze the infinite horizon problem, we cannot lean on backward induction to generate SPE continuation values. Hence, in principle, there can be history-dependent SPE with strategies that depend on ==== past actions and on all past transfers.====On the other hand, the Utilitarian solution remains Markovian (as the solution to a Dynamic Programming problem). Consequently, it is simple and easy to compute; the action in the current period is chosen based solely on the action chosen in the immediate past period. Our first result shows that the Utilitarian solution is always an equilibrium for an appropriately chosen transfer (and this holds for all discount factors ====). Note that this result is very different from a Folk Theorem. There the Utilitarian solution is an equilibrium ==== for high ====.====The proof is very different as well. Unlike Folk Theorem proofs which are constructive, ours is an analytical proof in which we characterize a fixed point to (a modification of) the Abreu-Pearce-Stachetti (APS) operator. The domain of strategies on which the operator is applied is actually more general than Markovian strategies. We consider strategies that condition on ==== past actions, i.e., both the action and the transfer at period ==== can depend on actions in all periods before ====. In addition, the best response action at ==== can and will (typically) condition on the (payoff relevant) transfer chosen by the opponent at ==== and the best response transfer will condition on the simultaneously chosen action at ====. The only conditioning that we do not allow for is for the action and transfer at ==== to depend on (payoff irrelevant) transfers before ====.====We call such equilibria ==== (APE). Note that Markov Perfect Equilibria (MPE) are a subset of APE. Our fixed point analysis proves that the set of APE is unique and coincides in value with the Utilitarian solution, for all ====.====The problematical case is when strategies depend on (payoff irrelevant) past transfers. For example, if strategies (and hence continuation values) depend on a transfer chosen two periods back, we can have difficulty computing the minimal transfer ==== (chosen in period ==== to induce an action in ====). So far that has been determined by setting the ==== mover's lifetime payoffs to be the same across his actions where these lifetime payoffs include the continuation payoffs from ==== onwards. An implicit assumption was that - when indifferent - the mover would indeed pick the transfer-giver's preferred action because at ====, he would strictly prefer to do so, for any ====. But now, if the continuation payoffs depend on the transfer picked two periods back, then knowing that the mover is indifferent at ==== does not tell us anything about continuation values at ====. Bhaskar and Vega-Redondo (2002) have proposed a clever method to break such ties in asynchronous games - making memory costly. In that case, players do not carry “extra” memory in best response and hence do not play such history-dependent strategies in equilibrium. In that spirit, we show that if ==== then all SPE are in fact APE (and, hence, there is only a single optimal SPE which is also a MPE).====Another way in which we can have difficulty in computing the minimal transfer is through discontinuities in continuation payoffs. Simply knowing that an action is preferred at ====, for any ==== does not prove that it is also a best response at ====; hence there might not be a minimal transfer inducing an action. Our third result rules this out by restricting attention to strategies that condition on the immediate past transfer in a continuous fashion. Again, there is a unique SPE and it is utilitarian optimal.====Finally, we show in an example, that a Folk Theorem emerges if there is infinite memory of transfers.====In simultaneous move Repeated Games - modulo a dimensionality requirement - the infinite horizon model always has a Folk Theorem but the finite horizon model has one only with additional assumptions on multiplicity of stage Nash Equilibria.==== Here, with alternating moves and transfers, it is the infinite horizon model that is more demanding.====In the “real-world” there are many instances of non-simultaneity in moves: collective bargaining,==== time-stamping/encryption,==== announcements of national policy==== etc. Transfers are also widespread. Private companies routinely pay royalties, licensing fees,==== copyright and trademark fees, etc. That players can commit to an action and a transfer schedule for one period is not restrictive if there are escrow accounts or a legal system that enforces contracts.====One key assumption we make is the ability to commit to transfers in the ====: if a player says he will pay ==== upon seeing action ==== next period, he will indeed do so. It seems to us that, even in the spot market, we implicitly make this assumption. After all, when a buyer agrees to pay ==== as the “price” for getting good ====, it is rarely the case that ==== and ==== are simultaneously exchanged. Typically, either ==== arrives first and the seller is “committed” to making delivery upon payment or ==== arrives first and the buyer is “committed” to making payment upon delivery.====At any rate, there is a manner in which we can “weaken” the assumption. There are two ways of interpreting an action chosen by the mover in period ====. One can a) think of that action as having already been taken when the donor (possibly) reneges or b) one can think of that action as being an “announced plan” that is yet to be implemented.====Consider the finite horizon model. Under the first interpretation, in the very last period, the donor ==== renege. Knowing this, the last period's mover will always choose a myopic best response. In turn, that implies that the penultimate period's transfer schedule will be reneged on as well. Evidently, then the SPE of such a game will merely coincide with the SPE of the asynchronous game without transfers.====In the latter case b), though, our results remain unchanged. The reasoning is as follows. If the donor reneges in the last period, but the action mover has yet to move, then the recipient picks her best response ====. This is, of course, an option that the donor could also have implemented by choosing a zero transfer schedule - and yet chose not to do so. Hence, reneging is not the best response for the donor. Treating reneging as being equivalent to picking the zero transfer schedule, the logic works in every period, i.e., our results are completely unchanged.====The analysis in the infinite horizon model is more complicated. Even in case a) above, it is unclear that the efficient outcome fails to be an equilibrium. For instance, a standard reciprocity argument involving the “worst punishment” might sustain efficiency on path. We don't know whether other equilibria will also exist. A full investigation can be the subject of further research.====Note that the transfer commitment is short-term, i.e., is only for one period. It is therefore coterminous with the action commitment.==== We are not allowing a player to commit to a life-time of transfers. That would turn our model into a one-period model and surely no one would be surprised at the efficiency result. The power of the result derives precisely from the fact that a minimal period commitment delivers a maximal period efficiency.====In Section 2 we detail the model and in Section 3 prove the general theorem for the finite horizon. Section 4 computes equilibria in the finite horizon Prisoners Dilemma game. In Section 5 we prove the APE theorem for the infinite horizon and in Section 6 we generalize that result to all SPE. That section also contains a counter-example when conditioning is allowed on infinite transfer histories. Section 7 proves the uniform boundedness of transfers while Section 8 fully characterizes limiting equilibrium values as the discount factor converges to 1. Brief comments on general asynchronicity and the literature review are in Section 9.",Asynchronous games with transfers: Uniqueness and optimality,https://www.sciencedirect.com/science/article/pii/S0022053119300523,30 May 2019,2019,Research Article,251.0
"Minardi Stefania,Savochkin Andrei","Economics and Decision Sciences Department, HEC Paris, France,New Economic School, Russia","Received 25 July 2017, Revised 11 April 2019, Accepted 19 May 2019, Available online 27 May 2019, Version of Record 5 June 2019.",https://doi.org/10.1016/j.jet.2019.05.007,Cited by (6),"We depart from ==== and ====, as well as to optimal contract design.",None,Subjective contingencies and limited Bayesian updating,https://www.sciencedirect.com/science/article/pii/S0022053119300547,27 May 2019,2019,Research Article,252.0
"Arifovic Jasmina,Hommes Cars,Salle Isabelle","Simon Fraser University, Burnaby, BC, Canada,CeNDEF, Amsterdam School of Economics, University of Amsterdam, Netherlands,Tinbergen Institute, Netherlands,Bank of Canada, Ottawa, ON, Canada","Received 28 July 2017, Revised 14 November 2018, Accepted 10 May 2019, Available online 23 May 2019, Version of Record 13 June 2019.",https://doi.org/10.1016/j.jet.2019.05.001,Cited by (9),"We set up a laboratory experiment to empirically investigate equilibrium selection in a complex economic environment. We use the overlapping-generation model of ====, which displays multiple perfect-foresight equilibria, including periodic and chaotic dynamics. The equilibrium selection problem is not solved under learning, as each outcome is predicted by at least one existing learning theory. We find that subjects in the lab systematically coordinate on an equilibrium despite the complexity of the environment. Coordination only happens on simple equilibria, in this case the steady state or the period-two cycle, a result which is predicted only if the subjects follow simple learning rules. This suggests that relevant perfect-foresight equilibria should be robust to the use of simple rules.","From a theoretical standpoint, the self-fulfilling nature of expectations exposes dynamic general equilibrium models to indeterminacy. When multiple equilibria are possible, the selected one depends not only upon the economic structure, but on the beliefs that agents use to forecast prices (Benhabib and Farmer, 1999). Indeterminacy therefore poses a number of challenges for working with this class of models.====Conceptually, no combination of environmental structure and agent preferences alone can pin down expectations about the future. As a consequence, indeterminacy undermines the concept of rational expectations equilibrium and the predictive power of the model. When multiple equilibria exist, some may be suboptimal. For instance, some equilibria may imply high volatility in real variables that arises from random coordination devices or initial conditions, which is undesirable from the point of view of policy makers aiming to stabilize aggregate fluctuations. This multiplicity further creates both practical issues for comparative static analysis and additional considerations for robust policy design. Being able to predict equilibrium selection in this setting is therefore a critical issue for modeling and policy analysis.====Learning theory has been frequently advocated as a theoretical equilibrium selection device. The main idea is that only rational expectation equilibria that emerge as a long-run outcome of an adaptive learning process should be regarded as relevant (see Evans and Honkapohja (2001) for a comprehensive discussion). A problem with learning, however, is that ‘anything goes’: any equilibrium can be selected if the adaptive rule is suitably designed. For example, in an OLG economy with infinitely many periodic equilibria, any equilibrium cycle can be learned provided that the adaptive rule of agents is consistent with the periodicity of the cycle (Grandmont, 1985; Guesnerie and Woodford, 1991; Evans and Honkapohja, 1995). In a similar set-up, Woodford (1990)'s ==== shows that a suitable adaptive learning rule may lead to convergence to a sunspot equilibrium with probability one.====A theorist is then left with the crucial yet loosely-defined task of designing the belief formation process of the agents, with little guidance from theory and yet with major consequences for the model's conclusions. Unsurprisingly, this challenge is accentuated if the model exhibits non-linear or even complex dynamics. In addition, allowing for heterogeneity of beliefs introduces further difficulty, as the process of coordination among agents has to be modeled in turn. Similarly, an empirical economist can pick an interesting equilibrium and fit the dynamics to the data, but what constitutes an interesting or relevant equilibrium remains a non-trivial question.====How agents learn to form and coordinate beliefs, and which equilibria are consequently selected and regarded as plausible, ultimately remain empirical questions. Collecting empirical evidence about agents' processes of expectation formation and equilibrium selection can undoubtedly provide guidance when designing models of learning. Since this is difficult to do with most available data, economists have taken it to the lab. Lucas (1986) was the first to stress the experimental approach in studying expectations and stability of equilibria under learning: ====The goal of our paper, similarly, is to present laboratory experiments which can test different theories of learning and the resulting equilibrium selection in a ==== environment. To achieve this, we conduct an experimental study within an OLG economy ==== Grandmont (1985). There are many reasons for choosing this environment.====Importantly, this is a general-equilibrium environment with a pervasive multiplicity of equilibria, that possesses infinitely many long-run equilibria, including a steady state, cycles of all periods, and even chaotic dynamics. Additionally, since the model is fully deterministic, exogenous shocks play no role in cycle formation; complicated dynamics and multiple equilibria arise under perfect-foresight as soon as there is a strong conflict between the substitution and wealth effects of a change in the return on savings. By simply varying a single parameter, namely the risk aversion parameter in agents' utility functions, the complexity of the model can be tuned in order to produce various treatments with distinct multiplicities of equilibria. This feature considerably simplifies the lab implementation of the model, which undoubtedly represents a strong argument for the choice of the Grandmont OLG environment given the challenge of bringing general-equilibrium economies into lab experiments. What is more, this environment serves as an excellent and classical example of economic complexity, which has not yet been convincingly investigated in a laboratory setting.====A further advantage of the Grandmont model is that it has been extensively studied in the literature since the seminal work by Lucas (1972), and a wide range of learning predictions has been established to guide the construction of hypotheses. These learning theories, however, fail to deliver a clear prediction of equilibrium selection because ‘anything goes’: all equilibria can be selected under learning provided that agents use a suitable rule. As theory does not resolve equilibrium selection within this complex model, empirical insights gained through the present study may prove informative with respect to agents' coordinating behavior in the (considerably more complex) real world. Finally, the model has been designed so as to incorporate heterogeneous beliefs in a micro-founded general-equilibrium setting.====We use a learning-to-forecast experiment (LtFE), where the only degree of freedom is the belief formation process of the subjects; all other components are deterministic. This design allows the experimentalist to isolate the effects of expectations on the model dynamics (Marimon et al., 1993), and appears as the most natural way to implement a parsimonious, yet complex, lab-based model in which equilibrium selection depends only on self-fulfilling beliefs.====Within this framework, we design several experimental treatments and formulate two main hypotheses. Our first hypothesis relates to the possibility of coordination, amongst a group of individuals holding heterogeneous beliefs, precipitated entirely by repeated market interactions in such a complex environment. As the results will show, we always observe coordination on one of the existing perfect-foresight equilibria of the model, irrespective of the complexity of the underlying model. This is already a remarkable result: our experiment is the first to document systematic coordination of beliefs in a chaotic environment, and the first in which ==== coordination on a 2-cycle equilibrium arises, even if the 2-cycle is unstable under learning. This means that, unlike in previous studies, subjects were able to reach a periodic equilibrium in the absence of exogenous fluctuations (see the discussion of related work by Marimon et al. (1993) below). Neither of these outcomes are obvious given the complexity of the underlying model, the heterogeneity in beliefs and the imperfect information that subjects have.====Our second hypothesis is that coordination is more likely to emerge on simple equilibria (such as a steady state) than on more complicated equilibria (i.e., higher-order cycles). This rather intuitive prediction is based on theories of learning and on existing empirical evidence, both of which suggest that subjects tend not to make use of information from more than a few periods prior. Even after considering various treatments with increasingly complicated dynamics, we consistently find aggregate convergence of prices and individual forecasts to the steady state or the 2-cycle, possibly after a long transition. Accordingly, we never observe selection of any higher-order cycles or more complicated equilibrium dynamics.====None of the learning theories predict entirely our experimental results, which underlines the relevance of empirical investigation through lab experiments. A necessary condition for equilibrium selection in the experiment is the weak E-stability criterion that predicts that only if the forecasting rule of the agents is ==== consistent with a steady state or a 2-cycle can these outcomes be achieved under adaptive learning. However, this criterion does not eliminate completely the multiplicity issue, and its prediction is not robust to misspecification or overparametrisation of the forecasting rule. Accordingly, we find that the subjects in the experiment only select for simple, weakly E-stable equilibria. This finding shows that, despite the complexity of the environment, subjects adopt simple rules (based on information from last period), but do not use higher order rules.====After the two main hypotheses have been addressed, we assess the robustness of our results to the nature of the experimental task by implementing a learning-to-optimize experiment (LtOE), where subjects explicitly make quantity decisions. This exercise is motivated by previous experimental results which showed that coordination is more challenging in a LtOE than in a LtFE. In the LtOE, we almost systematically find coordination on the monetary steady state. Those sessions also allow us to highlight possible explanations for the absence of coordination on the 2-cycle in the LtOE, namely strategic uncertainty, as subjects may prefer an allocation for which the payoff is constant over one for which it fluctuates, and a higher cognitive load. Finally, two additional sets of experimental sessions show that our results are robust to alternative designs of the experiment.==== A large number of experimental studies have explored the question of equilibrium selection in static or repeated games; see, e.g., Camerer (2003) for a survey. We discuss here two contributions that are closely related to our experimental study, but still differ in key ways. In the first of these, Van Huyck et al. (1994) employ an experimental coordination game with two efficient Nash equilibria in order to investigate the problem of equilibrium selection. The myopic best-response dynamics coincide with the chaotic quadratic map, while the interior equilibrium is stable under adaptive learning. In all their experimental sessions, subjects coordinate on the interior solution, in line with the prediction of adaptive learning.====An important difference between this experiment and our own is that our set-up has infinitely many perfect-foresight periodic cycles that arise as equilibrium outcomes of the model. These occur without the need to impose (a priori) any expectation rules, and can be stable under adaptive learning. As such, we aim to find out which of these periodic equilibria, if any, subjects may coordinate on. By contrast, in Van Huyck et al. (1994), the chaotic dynamics are not an equilibrium of the coordination game, but result from the assumption of myopic best-response behavior. Furthermore, the authors do not address the question of whether or how subjects may coordinate on the best response, which seems especially difficult given the complicated dynamics at work.====The second closely related contribution is the work by Marimon et al. (1993), who were the first researchers to observe a form of coordination on 2-cycle dynamics in a laboratory experiment.==== They use a design similar to our LtFE, but there are nevertheless several major differences worth noting. First, they consider an OLG environment in which only a steady state, a two-period cycle and two-state sunspot equilibria exist, while our model involves infinitely many periodic and chaotic equilibria, making our equilibrium selection problem more complex. Second, they employ a three-population design, in which participants are randomly drawn from the pool to re-enter the market and form the new generation in each period. We use a single-population design, so that the resulting course of events in the experiment is the same as in the learning literature, especially the seminal contribution of Grandmont (1985), and we later show that our results are robust to an alternative design that introduces the overlapping generation friction.====Most importantly, Marimon et al. impose real shocks to the OLG economy by cyclically varying the number of subjects in each generation between a high and a low number in phase with the color of a blinking square on subjects' computer screens. This generates temporary ‘attenuated’ 2-cycle oscillations driven by these exogenous shocks. However, these oscillations dampen out once the exogenous shocks to generation size are removed.==== Hence, these authors do not find evidence of 2-cycles arising spontaneously, a phenomenon which characterizes our own experimental results.====The remainder of this paper is organized as follows. Section 2 introduces the underlying OLG model of the experiment and discusses its properties and learning dynamics. This section is quite technical and may be skipped over, as Section 3 then motivates and details the experimental design within the OLG model and our hypotheses based on learning predictions. Section 4 presents the experimental results, Section 5 provides estimates of individual forecasting rules, and Section 6 presents two additional sets of experimental sessions designed for robustness. Section 7 concludes. Appendices A-K contain details about all experimental sessions and treatments, their analyses and instructions.",Learning to believe in simple equilibria in a complex OLG economy - evidence from the lab,https://www.sciencedirect.com/science/article/pii/S0022053119300481,23 May 2019,2019,Research Article,253.0
"Lou Youcheng,Parsa Sahar,Ray Debraj,Li Duan,Wang Shouyang","MDIS, Academy of Mathematics and Systems Science, Chinese Academy of Sciences, China,Department of Economics, Tufts University, United States of America,Department of Economics, New York University, United States of America,Department of Economics, University of Warwick, United Kingdom,School of Data Science, City University of Hong Kong, Hong Kong,Center for Forecasting Science, Chinese Academy of Sciences, China,School of Economics and Management, University of Chinese Academy of Sciences, China","Received 29 October 2018, Revised 25 March 2019, Accepted 13 May 2019, Available online 22 May 2019, Version of Record 23 July 2019.",https://doi.org/10.1016/j.jet.2019.05.004,Cited by (9),"We study a financial market with asymmetric, multidimensional trader signals that have general correlation structure. Each of a continuum of traders belongs to one of finitely many “information groups.” There is a multidimensional aggregate signal for each group. Each trader observes an idiosyncratic signal about the fundamental, built from this group signal. Correlations across group signals are arbitrary. Several existing models serve as special cases, and new applications become possible. We establish existence and regularity of linear equilibrium, and demonstrate that the equilibrium price aggregates information perfectly as noise trade vanishes.","Consider an economy in which a single risky asset is traded, with unknown fundamental of common value to all of a continuum of individuals. Each trader belongs to one of a finite number of “information groups,” which could vary in size. There is a multidimensional aggregate signal for every information group. Each individual receives an idiosyncratic signal built from her group signal plus iid noise. The setting is multivariate normal, with arbitrary correlation structure across fundamental and signals. In addition, each trader also observes the asset price and can make inferences about the fundamental using that price. As in the seminal contribution of Hellwig (1980), traders retain the incentive to use their private signals in the presence of noise trade. We will allow noise trade to converge to zero to obtain our information aggregation result in the limit.====We are therefore in a classical rational expectations equilibrium (REE) world, but with significantly added generality in information structure and dimensionality. Moreover, we allow traders to differ not only in their access to information, but also in their attitudes to risk. Indeed, we permit risk heterogeneity both within and across information groups. Of course, we can nest several existing REE models — among them Grossman (1976) and the finite-agent model of Hellwig (1980) — by properly selecting the mass and the risk-aversion coefficient of each trader type. But well beyond that, the multidimensional signal structure we work with can facilitate other investigations. Except for normality, we do not impose restrictions on aggregate signals and allow these to exhibit any degree of asymmetry, or heterogeneity in correlation structure. This generality is important. For instance, given different locations, risk attitudes or informational capacities, traders might have access to diverse sources (newsletters, advisory services etc.) for their private information, which leads to an asymmetric correlation structure not handled by the classical models. And multidimensionality acquires particular salience when traders share their private signals with their neighbors via a social network. Then the effective signals of traders are essentially many-dimensional, because two signals cannot be aggregated ex-ante without knowledge of the full equilibrium structure generated by the price system. Therefore information-sharing over a network cannot be handled by models with one-dimensional signals.====Apart from conceptual generality, new analytical issues arise when these considerations — asymmetry, multidimensionality, as well as arbitrary cross-signal correlation — are studied. Even the seemingly intuitive properties of equilibrium that are immediate in the Grossman-Hellwig setup must now be proved at a non-trivial level when the information structure is general. Among these properties is the ==== of any linear equilibrium: an increase in demand implies also an increase in price. More importantly, and owing to the generality of our signal structure, the existence arguments given in Hellwig (1980) cannot be applied to solve our model. We resort to a non-standard argument involving sequences of fixed points to establish the existence of a linear equilibrium price function.====An accompanying complication concerns the impact of signals on the equilibrium price. When those signals are independently generated — and positively related to fundamental value — they exert a positive influence on prices, as in Hellwig (1980). Because our model admits general correlation across signals, no parallel assertion is available here: the corresponding coefficients of signals in the price function are generally ambiguous in sign. They depend on the correlation pattern, the sizes of information groups, as well as the distribution of risk attitudes, though we can pin down the signs of the weights for some special cases. Nevertheless, it is in this general context that we are able to revisit a solution to the Grossman-Stiglitz paradox first investigated by Hellwig (1980).==== If an imaginary “super-agent” were to observe every one of the signals, she would possess a best prediction of the fundamental, which is a linear function of the signal vector. Any price function which is the same linear function (up to an intercept term) would aggregate information perfectly. But an agent observing such a price function would entirely ignore her own signals. Indeed, she would ==== prefer ==== to use any of her information, whether or not it is freely available. Even the redundancy that tolerates some degree of mixing and allows information to seep in via indifference, is not to be had. But then: how can the market imitate the super-agent?====Certainly, with the existence of noisy movements in trades, prices lose their ability to perfectly aggregate information, contaminated as they must be by stochastic demand shocks.==== Then traders ==== use their own information at least to some degree, which therefore enters the price. Specifically, we show that the equilibrium price is positively correlated with the value of the fundamental, assuming, of course, that at least one of the observed signals is correlated with that value. But the more subtle question remains: as noise trade vanishes, must the price function converge to the ==== aggregator? Our answer to this question is in the affirmative: as the variance of noise demand converges to zero, the equilibrium price aggregates information perfectly, fully capturing a linear relationship, including weights and correlation patterns, across the fundamental and aggregate signals. This is consistent with (and substantially generalizes) the observations in Grossman (1976) and Hellwig (1980).====Section 2 introduces the model. Section 3 characterizes linear equilibria. Section 4 states and discusses the information aggregation result. Section 5 discusses the weights of aggregate signals on prices. Section 6 discusses related literature. Section 7 concludes. All proofs are in the Appendix.",Information aggregation in a financial market with general signal structure,https://www.sciencedirect.com/science/article/pii/S0022053119300511,22 May 2019,2019,Research Article,254.0
"Myatt David P.,Wallace Chris","London Business School, United Kingdom,University of Manchester, United Kingdom","Received 20 December 2017, Revised 30 April 2019, Accepted 11 May 2019, Available online 17 May 2019, Version of Record 22 May 2019.",https://doi.org/10.1016/j.jet.2019.05.002,Cited by (9),"In an asymmetric coordination (or anti-coordination) game, players acquire and use signals about a payoff-relevant fundamental from multiple costly information sources. Some sources have greater clarity than others, and generate signals that are more correlated and so more public. Players wish to take actions close to the fundamental but also close to (or far away from) others' actions. This paper studies how asymmetries in players' coordination motives, represented as the weights that link players to neighbors on a network, affect how they use and acquire information. Relatively centrally located players (in the sense of Bonacich, when applied to the dependence of players' payoffs upon the actions of others) acquire fewer signals from relatively clear information sources; they acquire less information in total; and they place more emphasis on relatively public signals.",None,Information acquisition and use by networked players,https://www.sciencedirect.com/science/article/pii/S0022053119300493,17 May 2019,2019,Research Article,255.0
Li Yunan,"Department of Economics and Finance, City University of Hong Kong, Tat Chee Avenue, Hong Kong","Received 23 June 2017, Revised 22 April 2019, Accepted 29 April 2019, Available online 7 May 2019, Version of Record 9 May 2019.",https://doi.org/10.1016/j.jet.2019.04.010,Cited by (6),"This paper studies the design of ex ante efficient mechanisms in environments where a single object is for sale, and agents have positively interdependent values and can covertly acquire information at some cost before participating in a mechanism. We find that ex ante efficient mechanisms discourage agents from acquiring excessive information by pooling or randomization. The optimal pooling regions are those where the semi-elasticity of information acquisition is large. There exists an ex ante efficient mechanism that can be implemented by standard auctions with restrictions on the set of allowable bids. In special cases, this implementation is simple and appealing: standard auctions with discrete bids.","In many important applications, the information possessed by agents is not exogenous. For example, in auctions for offshore oil and gas leases in the U.S., companies use seismic surveys to collect information about the tracts offered for sale before participating in the auctions. Another example is the sale of financial or business assets, in which buyers perform due diligence to investigate the quality and compatibility of the assets before submitting offers. In these settings, the information possessed by agents is not only endogenous but also costly to acquire. In the example of U.S. auctions for offshore oil and gas leases (see Haile et al. (2010)), companies can choose to conduct two-dimensional (2-D) or three-dimensional (3-D) seismic surveys. Because 3-D surveys can produce more accurate information, they were used in 80% of wells drilled in the Gulf of Mexico by 1996. However, this number was only 5% in 1989, when 3-D surveys were more expensive than 2-D surveys.==== Similarly, the legal and accounting costs of performing due diligence often amount to millions of dollars in the sale of a business asset (see Quint and Hendricks (2013) and Bergemann et al. (2009)).====When valuations are private, agents' incentives to acquire information coincide with social incentives, and ex ante efficient information acquisition is achieved (see Maskin (1992) and Bergemann and Välimäki (2002)). However, if valuations are interdependent, as in auctions for oil and gas leases, an ex post efficient mechanism results in socially suboptimal information acquisition. In fact, Bergemann et al. (2009) give an instance where the equilibrium level of information acquisition in an ex post efficient auction is excessive. In summary, when valuations are interdependent, there is a conflict between the provision of ex ante efficient incentives to acquire information and the ex post efficient use of information. This paper addresses the question of how to design an ex ante efficient mechanism to balance the two trade-offs in the sale of a single object when agents' valuations are positively interdependent.====Initially, the true value of the object to each agent is unknown. Before participating in a mechanism, agents can simultaneously and independently decide how much information to acquire. They acquire information by increasing the accuracy of the signals they receive. To acquire a more accurate signal, an agent must incur a higher cost. As in Szalay (2009) and Shi (2012), both the accuracy and the realization of his signal are an agent's private information, and the signals are independent across agents. We assume that the accuracy of the signals is ====. This notion of information order was first introduced into the literature by Ganuza and Penalva (2010) and later used by Shi (2012) when studying revenue-maximizing mechanisms in the independent private value setting with endogenous information.====In most parts of the paper, we focus on symmetric mechanisms and symmetric equilibria in which all agents acquire the same amount of information before participating in a mechanism. There are three main results.====To discourage wasteful information acquisition, we have to distort the allocation away from the ex post efficient one. This can be done by either withholding the object with some probability, or by pooling or randomization. The first main result of the paper is that the object is never withheld in an ex ante efficient mechanism. Intuitively, whenever the social planner withholds the object, she can also allocate it randomly. By doing so, the allocative efficiency increases while an agent's ex ante incentive to acquire information remains unaffected. In fact, this property holds more generally even if we consider asymmetric mechanisms and asymmetric equilibria in which agents acquire different amounts of information. Though intuitive, the proof of this result is nontrivial because of the presence of the nonstandard information acquisition constraint. This result is also important technically as it facilitates the analysis by allowing us to work with interim allocation rules directly.====Second, for any given information choice, we fully characterize all mechanisms that maximize the expected social surplus subject to implementing this choice. Define the semi-elasticity of information acquisition at a signal as the percentage change in the signal's accuracy if an additional piece of information is acquired. An optimal mechanism randomizes in regions where the semi-elasticity of information acquisition is above an optimally chosen threshold. Furthermore, there exists an ex ante efficient mechanism that can be implemented by standard auctions with restrictions on the set of allowable bids. In the model's environment, standard auctions such as first-price or second-price auctions are ex post efficient. By restricting the set of allowable bids, we bunch some nearby signals together, which clearly reduces agents' marginal benefits from acquiring information. We also provide conditions under which bunching occurs only at the bottom or top of the type distribution.====If the semi-elasticity of information acquisition is constant in signal, we say that the information structures are ====. In this special case, there exists an ex ante efficient mechanism that has a simple and appealing implementation: standard auctions with discrete bids. Restricting bids to discrete levels is not uncommon in practice. For example, eBay auctions require that the next bid exceeds the current price plus a bid increment, and FCC spectrum auctions adopt a minimum clock price increment. Most existing auction theories predict that discrete bids lead to inefficiency, and they are mainly used in practice to simplify communication processes and speed up auctions (see Ausubel and Cramton (2004)). Our results suggest an alternative justification for the prevalence of discrete bids. That is, when agents have positively interdependent values and can covertly acquire information at some cost, the use of discrete bids can improve ex ante efficiency.====Finally, we briefly discuss general ex ante efficient mechanisms without restricting our attention to symmetric mechanisms or symmetric equilibria. First, as mentioned above, a robust property of ex ante efficient mechanisms is that the object is never withheld. To obtain further results, we restrict our attention to the case of uniformly supermodular ordered information structures. In this special case, we show that when interdependency is high, the socially optimal information choices are the same for all agents and a symmetric ex ante efficient mechanism exists. When interdependency is low, an ex post efficient mechanism that induces an asymmetric equilibrium is ex ante efficient. The intuition behind this result is as follows. As in Bergemann et al. (2009), agents' information choices are strategic substitutes. Therefore, it could be socially optimal to discourage other agents from acquiring information by encouraging one agent to do so. Furthermore, distorting allocations ex post is costlier if interdependency is lower; therefore, when interdependency is sufficiently low, an ex post efficient mechanism that induces an asymmetric equilibrium is optimal.====The problem considered in this paper is technically challenging for two reasons. First, we want to work directly with interim rather than ex post allocation rules, which the literature has proven to be a very useful method.==== However, when valuations are interdependent, it is hard to write the social planner's objective function in terms of interim allocation rules. We overcome this difficulty by proving that the object is never withheld in an ex ante efficient mechanism. The second challenge arises because of the nonstandard information acquisition constraint. To overcome this difficulty, we use an approach first proposed by Reid (1968) and later introduced into the mechanism design literature by Mierendorff (2009). The proof, however, is not a straightforward modification of Mierendorff (2009). In Mierendorff (2009), the interim allocation rule is discontinuous at one known point. In this paper, the interim allocation rule could be discontinuous at most countably many times at unknown points.====The rest of the paper is organized as follows. Section 1.1 discusses related work. Section 2 presents the model. Section 3 provides the main results under the symmetry assumption. Section 4 examines ex ante efficient mechanisms without imposing the symmetry restriction. Section 5 concludes. All omitted proofs are relegated to the appendix.",Efficient mechanisms with information acquisition,https://www.sciencedirect.com/science/article/pii/S002205311930047X,7 May 2019,2019,Research Article,256.0
Tomoeda Kentaro,"University of Technology Sydney, UTS Business School, PO Box 123, Broadway, NSW 2007, Australia","Received 14 August 2017, Revised 18 April 2019, Accepted 26 April 2019, Available online 3 May 2019, Version of Record 7 May 2019.",https://doi.org/10.1016/j.jet.2019.04.009,Cited by (4)," and ==== investments. In our problem, the social planner aims to achieve efficiency in every equilibrium of a dynamic game in which agents strategically make investments before and after playing the mechanism. Our main theorem shows that a novel condition ==== investments is crucial in our model: there is no social choice rule that is efficient and implementable in subgame-perfect equilibria without ==== investments. We also show that our positive result continues to hold in the incomplete information setting.","The literature on implementation theory has identified which social choice rules can be fully implemented under various solution concepts and informational assumptions. Here, full implementation means that the set of all equilibrium outcomes of the mechanism coincides with the set of outcomes specified by the social choice rule. Although this is a strong requirement, the literature has shown rather permissive results: for example, under complete information and quasi-linear utility, any social choice rule is implementable in subgame-perfect equilibria (Moore and Repullo, 1988; Maskin and Tirole, 1999).==== In the implementation problem, however, the problem of investment incentives has not been fully examined. In many real-life applications such as auctions and the provision of public goods, there are opportunities for agents to invest in the outcomes of the mechanism (Tan, 1992; Bag, 1997; Arozamena and Cantillon, 2004). When agents strategically invest before participating in a mechanism, the positive results implied by implementation theory may be threatened. That is, although the mechanism implements efficient allocations at the market clearing stage, it may not necessarily induce efficient ==== investments in equilibrium. In particular, even when there is an efficient investment equilibrium, we may not be able to rule out other inefficient equilibria, which is a concern of the full implementation problem.====The goal of this paper is to provide a condition for an “efficient” social choice rule to be implementable when we take into account both investment and allocative efficiency.==== To do so, we extend the standard implementation problem to include endogenous investments. First, we consider a rich set of types that are defined by the agents' costs of investment rather than their valuations of alternatives. Given realized cost types, agents endogenously form their valuations of the alternatives by investing before and after participating in the mechanism, which we call ==== and ==== investments.==== Here, we explicitly model ==== investments because in many applications, agents make further investments after the market clearing stage to maximize the value of the outcome (McAfee and McMillan, 1986; Laffont and Tirole, 1986, Laffont and Tirole, 1987). A social choice rule ==== is defined as a correspondence from the set of cost types to the set of alternatives, transfers and investments. This social choice rule, however, is not standard because investments are non-contractible and they are only chosen by the agents as part of their strategies. We assume that the social planner can design a mechanism that specifies an alternative and a transfer vector, but that he cannot intervene in the structure of the investment opportunities. Therefore, the planner aims to achieve efficiency for every profile of cost types through a dynamic game in which agents strategically make ==== and ==== investments in addition to playing the mechanism itself. In the main part of the paper, we assume that the agents have complete information.====In this setting, our main theorem shows that the sufficient and necessary condition for an efficient social choice rule ==== to be implementable in subgame-perfect equilibria is the ==== of the associated allocation and transfer rules ==== at the mechanism stage (Theorem 1). The associated ==== represents the choice of ==== over contractible outcomes given the investments: these are the functions that specify the same alternative and transfer vector as ==== for each profile of agents' valuation functions at the mechanism stage. Establishing a condition on ==== is useful for the social planner because ==== can be interpreted as a standard social choice function where investments are exogenously given.====To provide the intuition for Theorem 1, let us consider an example where a city decides on a public project to utilize a vacant lot.==== Suppose that there are several potential projects and that each citizen supports one of them. The goal of the city is to maximize social welfare from the lot taking into account the costs of the citizens' potential investments to utilize it. However, the investments and cost structures are neither observable to the city nor contractible. Suppose that some inefficient supporters have made a huge costly and irreversible investment in their favorite project prior to participating in the mechanism. Then, since the city does not know the cost of their ==== investments and the cost is sunk, the city would simply choose their project as long as it is the most efficient one at the mechanism stage. However, there may be another project with more efficient supporters, which would require lower investment costs and hence achieve higher social welfare. This problem would not happen if none of the citizens had made any ==== investments. Indeed, when the investment cost does not increase over time, the social efficiency can be achieved by making everyone invest only after the project is selected.====How can the city prevent citizens from investing ==== without directly intervening in their investment activities? Our solution is to design allocation and transfer rules ==== carefully so that they satisfy ====. The commitment-proofness condition is interpreted in the following abstract way: suppose that (i) each citizen ==== is assigned a “default” valuation ==== for her favorite project, and (ii) ==== could increase ==== to ==== ==== through a certain commitment device that costs ====. Even with this costly commitment device, a citizen may want to commit to a high valuation ==== if her favorite project is selected only when she has ====. The commitment-proofness of ==== requires that under ====, none of the citizens should have an incentive to change their valuation from any default ==== to another ==== through this commitment device. Going back to the investment problem in the example, for each citizen, the choice between no ==== investment and making a costly investment corresponds to the choice between the default ==== and another ==== in the commitment-proofness condition. From this correspondence, none of the citizens has an incentive to invest ==== under this condition, and the city succeeds in achieving efficiency. Commitment-proofness is a relatively weak requirement: it is implied by strategy-proofness, and moreover, we can find ==== that is commitment-proof, efficient and budget-balanced (Proposition 3).====The difficulty and novelty of our implementation problem stem from the combination of the following assumptions: (i) the investments are not contractible, (ii) the agents' cost types are not known to the planner, and (iii) the investments are irreversible. First, if the investments were contractible, they could just be part of the outcome of mechanisms and our problem reduces to the standard implementation problem. However, investment activities are usually difficult to describe; they are multidimensional and they involve the expenditure of time and effort as well as the expenditure of money (Hart, 1995). These non-contractible investments have also been a central concern in the literature on the hold-up problem (Klein et al., 1978; Williamson, 1979, Williamson, 1983; Grossman and Hart, 1986; Hart and Moore, 1988, Hart and Moore, 1990). Second, if the planner knew the agents' cost types, he would be able to identify the first-best alternative in our model. Since the investments do not have externalities in our model, the efficient level of investment would be chosen by each agent if the planner just selects the first-best alternative. Finally, if the investments were reversible, ==== investments would not affect the valuations of the alternatives at the mechanism stage. Therefore, the planner would not be bothered by ==== investments, and the efficient choice of an alternative at the mechanism stage could simply achieve investment efficiency.====Our characterization result (Theorem 1) relies on the assumption that ==== investments are possible under the same cost functions. However, in many papers such as Rogerson (1992) and Hatfield et al. (2018), ==== investments are not explicitly considered. Hatfield et al. (2018) showed a result that is seemingly contradictory to our theorem: for efficient mechanisms, strategy-proofness is sufficient and ==== for the existence of an ==== efficient investment equilibrium. The difference stems from the availability of ==== investments and more precisely, how it interacts with the implication of the rich cost types. In our model, since every agent takes into account an optimal ==== investment, there is a natural restriction to the set of possible valuations at the mechanism stage and their costs. On the other hand, in the necessity result of Hatfield et al. (2018), the richness of the cost types implies that any valuations could be associated with any costs. Therefore, the availability of ==== investments allows us to restrict the set of valuations and their costs at the mechanism stage in a natural way and obtain a more positive result. In Section 5, we examine our full implementation problem when ==== investments are not possible. We obtain an impossibility result in this setting: there does not exist a social choice rule that is efficient and implementable in subgame-perfect equilibria without ==== investments (Proposition 2).====We also extend our main model to the incomplete information setting where agents are unsure about the cost types of other agents. In this environment, we show that an efficient social choice rule is implementable in PBE if its associated allocation and transfer rules are strategy-proof (Proposition 4).",Efficient investments in the implementation problem,https://www.sciencedirect.com/science/article/pii/S0022053119300468,3 May 2019,2019,Research Article,257.0
Babichenko Yakov,"Faculty of Industrial Engineering and Management, Technion—Israel Institute of Technology, Israel","Received 28 May 2018, Revised 10 March 2019, Accepted 24 April 2019, Available online 30 April 2019, Version of Record 3 May 2019.",https://doi.org/10.1016/j.jet.2019.04.008,Cited by (78)," signal to every receiver. The utility of the sender is a function of the subset of adopters and the realized state. We first consider a setting with a binary state space and no payoff externalities among receivers. We characterize an optimal signaling policy and the maximal revenue to the sender for two different types of utility functions: supermodular, and anonymous submodular. In particular, for supermodular utilities we show that the optimal policy correlates positive recommendation to adopt the product as much as possible. We generalize these results to the case of a nonbinary state space. The result for supermodular utilities is generalized to the case where receivers have payoff externalities. We also provide a necessary and sufficient condition under which public and conditionally independent signaling policies are optimal.","Bayesian persuasion is a model of information disclosure where an informed sender is endowed with commitment abilities when disclosing information to the receiver. As demonstrated by Kamenica and Gentzkow (2011), understanding the optimal persuasion mechanism is of substantial importance and can play a significant role in economic situations such as advertising, legal disputes, and financial disclosure, among many others.====Building upon the classic work by Aumann and Maschler (1995), Kamenica and Gentzkow (2011) provide a full characterization of the receiver's optimal utility for the case of a single receiver, which can be easily extended to the case where there are multiple receivers that share no payoff externalities and the sender can reveal only public information. In some setups, however, it is natural to consider the case where private communication with the receivers is allowed. The main goal of this paper is to extend and solve the Bayesian persuasion problem in a product adoption setting with multiple receivers where the sender is allowed to reveal information privately to the receivers.====The economic importance of this question is primarily due to the advent of social networks, which have fundamentally changed the landscape of advertising (Shapiro and Varian, 1998, Peres et al., 2010, Tucker, 2014, Wood et al., 2005). As a result firms have new ways to reach out to customers and can target their advertisements differently as a function of specific characteristics (such as age, gender, economic status, etc.). Under ==== (also known as ====) firms leverage data analysis to target individualized advertisements and product offerings to current or prospective customers.====In the words of Shapiro and Varian (1998): “====” The following two features are inherent in personalized advertising. First, the firm is able to communicate privately with a consumer via a personalized ad. Second, the consumers in the network do ==== share information (i.e., ads) with each other through the network. These features motivate the study of ==== information disclosure in multi-receiver persuasion problems. In this paper we examine this flexibility, in a simple Bayesian persuasion model of product adoption, and study how information should be revealed (namely which ad should be sent to each user) optimally by the firm in order to maximize its revenue as a function of its utility and the utilities of the consumers.====As a leading example, consider a monopolist (henceforth sender) who sells a product to potential consumers (henceforth receivers). The sender can commit in advance to an information disclosure policy that discloses information (signals) to the receivers regarding the quality of the product. The utility of the sender is a function of the group of receivers who have decided to buy (adopt) the product. If the signals can be observed by all of the receivers, i.e., they are public, this example can be analyzed using the methodology of Kamenica and Gentzkow (2011). If the sender can send private signals, the receivers share no payoff externalities, and the utility of the sender is additive (i.e., a linear production cost), then it is easy to show that a disclosure policy with conditionally independent signals that is separately optimal for any receiver is also globally optimal (see Theorem 4 in Section 5.2).====However, reasonable economic circumstances may include the following key features. First, the utility of the sender may not be additive, e.g., if the monopolist's production cost is nonlinear. Second, payoff externalities among the receivers may arise in situations where receivers care not only about the quality of the product but also about who among their peers have adopted the product. Our main goal is to understand how these attributes affect the optimal information revelation policy for the sender. More concretely, we address the following questions: What is the structure of the optimal policy in (nonadditive) classes of utilities for the sender? When is a public policy optimal (among all possible private policies)? How do externalities among receivers affect the optimal policy? Understanding these questions can shed light on the connection between market structure and optimal information disclosure.",Private Bayesian persuasion,https://www.sciencedirect.com/science/article/pii/S0022053118302217,30 April 2019,2019,Research Article,258.0
"Kocherlakota Narayana R.,Song Yangwei","Department of Economics, University of Rochester, Rochester, NY 14627, United States,NBER, United States,Institute for Economic Theory 1, Humboldt University Berlin, Spandauer Str. 1, 10178 Berlin, Germany","Received 6 September 2018, Revised 17 February 2019, Accepted 20 April 2019, Available online 30 April 2019, Version of Record 3 May 2019.",https://doi.org/10.1016/j.jet.2019.04.007,Cited by (6),We consider a canonical problem in economics: the financing and provision of a public good. ,"In this paper, we consider a canonical problem==== in economics: the financing and provision of a public good. Mailath and Postlewaite (1990) (MP) analyze this problem under two natural assumptions: each agent's valuation of the public good is private information to her and each agent has the ability to veto its provision. The ability to veto means that society cannot collect much in the form of taxes to finance the public good from agents with low valuations. But the privacy of information means that agents with high valuations can mimic those with low valuations. The two frictions combine to impose tight limits on how much society can collect to pay for a public good. Indeed, MP prove the remarkable result that, under weak auxiliary conditions, the probability of provision falls to zero as the population size converges to infinity even if provision of the public good is efficient.====MP assume that an agent's prior belief over other agents' private values is equal to an ex-ante distribution from which those values are actually drawn. In this paper, we explore the consequences of relaxing this assumption. We instead assume that each agent exhibits ambiguity, in the sense that she has a set of priors over the other agents' valuations. The set is anchored by objective reality in the sense that it equals all distributions within a distance ==== (measured using the sup-norm) of the true ex-ante cumulative distribution function from which the valuations are drawn. As originally axiomatized by Gilboa and Schmeidler (1989), agents deal with this ambiguity by making choices in a max-min fashion—that is, for a given action, they calculate the expected utility of an action for each possible prior, and then evaluate the action according to the worst possible expected utility.====We retain the private information and no-veto assumptions used by MP. We assume that the (true) expected net benefit of the public good is positive. We construct a class of ==== mechanisms that we will describe shortly. We prove that, as long as there is enough ambiguity, a society can use a consolation prize mechanism to finance the provision of the public good in economies with sufficiently many agents. More specifically, in an economy with ==== agents, we let ==== parameterize the difference between the per-capita cost of the public good and the smallest mean in the set of beliefs over other agents' valuations. We assume ==== is positive (so that there is some prior with a mean value that is less than the per-capita cost of the public good) for all ====, but allow for the possibility that ==== shrinks to zero as ==== grows to infinity. We show using Hoeffding's inequality that if this rate of shrinkage to zero is strictly slower than ====, then agents always consider it possible in large societies that the provision of the public good is inefficient with probability one, even though provision is efficient under the true distribution. It is this pessimism on the part of agents about how others value the good that allows for public good provision in large economies.====The role of pessimism is most readily explained through the structure of the consolation prize mechanisms that we use. A consolation prize mechanism asks each agent to report her valuation of the public good. If those reports lead to a determination that the public good should be provided, each agent is expected to pay a tax equal to her report. This tax, in and of itself, is not incentive compatible because it leaves an agent with an incentive to lie downward about her valuation. To restore incentive compatibility, consolation prize mechanisms also give a positive reward to each agent that is an increasing function of her report.====The resource cost of the rewards delivered by consolation prize mechanisms depends critically on agents' beliefs about the provision of the public good. If an agent sees the public good as highly likely to be provided, then that agent has a large incentive to claim a low value so as to generate a lower tax burden. It is only possible to undo this large tax incentive with a correspondingly large reward, and so the consolation prize mechanism cannot solve the incentive problem in a budget-feasible way.====In contrast, suppose an agent makes choices under the belief that the public good is highly unlikely to be provided. That agent has correspondingly little incentive to lie downward to reduce taxes. Conversely, it is then possible to use small rewards to motivate the agent to truthfully reveal large valuations. In this way, sufficient pessimism about the valuations of others—of the sort that is naturally generated by the Gilboa-Schmeidler approach to ambiguity—makes it possible to finance the public good.====The above analysis presumes that it is commonly known that all agents' valuations are drawn from a single distribution. In Section 4, we perturb this baseline by instead assuming that there are two possible distributions of valuations with commonly known ex-ante positive probabilities. Under the first of these distributions, it is efficient to provide the public good while provision is inefficient under the second distribution. In this setting, we only need to require that ambiguity is positive regardless of population size. Under this restriction, we construct a mechanism that satisfies ex-ante budget balance and provides the public good when it is efficient to do so. The key to this result is again pessimism: we show that when agents update (on a prior-by-prior basis) after seeing their own valuations, they entertain a posterior that assigns probability one to the second (low valuation) aggregate state.====We view our approach as pointing to a new way to think about the financing and provisioning of public goods through the use of consolation prize mechanisms that utilize a tax that is paid only if the public good is provided and a reward that is received even if the good is not provided. Our possibility result works because these non-contingent rewards are a cheap way to elicit truthful reports about values if agents are pessimistic about the cross-sectional distribution of others' valuations of the public good.====Our work is connected to a growing literature on mechanism design with ambiguity.==== In particular, our consolation prize mechanisms are closely related to the mechanisms used in the work of Bose et al. (2006) and Song (2018). Bose et al. (2006), Bose and Daripa (2009), and Bodoh-Creed (2012) study revenue maximization problem with ambiguity averse buyers. Carroll (2017) studies a multidimensional screening problem in which the seller is ambiguous about the joint distribution of the buyer's types. Those papers address the issue of maximizing revenue, whereas we are mainly concerned with implementing the efficient allocation rule. Song (2018) studies efficient implementation in a single object allocation problem without budget balance constraint.====Wolitzky (2016) studies efficient implementation in the bilateral trade problem of Myerson and Satterthwaite (1983). The consolation prize mechanisms in this paper differ from the mechanisms described by him. This difference is traceable to the differing specifications of the sets of priors. Wolitzky assumes that the expected values of the agents are common knowledge and each agent's set of priors (essentially) is the set of all probability measures with those known expected values, including the Dirac and two-point measures. He shows that, under this specification, there may exist mechanisms in which shading a buyer's report leads to a first-order cost in terms of the foregone gains from trade that offsets the (familiar) first-order benefit in terms of trading price. In our paper, since we assume that the set of priors is a ball around the true distribution, the Dirac and two-point measures in general are not contained in the set of priors. Our result instead relies on the agents' ambiguity about expected values.====Di Tillio et al. (2017) and Guo (2017) study the effects of introducing ambiguity in mechanisms. In particular, Di Tillio et al. (2017) consider the sale of an object to an ambiguity averse buyer and show that the seller can increase his revenue by using ambiguous allocation rules and transfer rules. In contrast, Guo (2017) introduces ambiguity only in transfer rules. She shows that if different types hold different beliefs about the other agents' types, then the seller can extract the full surplus.====Our paper is also connected to a smaller literature on public good provision. Eichberger and Kelsey (2002) and Bailey et al. (2005) study the interesting but distinct issue of how ==== ambiguity affects the private provision of public goods. In a more related contribution, Maldonado and Rodrigues-Neto (2016) point out how higher degrees of pessimism about the provision of a public good can induce larger contributions to those goods.====We treat ambiguity as exogenous. In contrast, Bose and Renou (2014) study situations in which a mechanism designer can create ambiguity deliberately through an ambiguous communication device. In Appendix E, we illustrate in an example how their approach can be applied to this public goods setting. More generally, we view our results as complementary to theirs: we are essentially providing a guide to the Bose-Renou mechanism designer about what kind of ambiguity should be generated in this particular setting.",Public goods with ambiguity in large economies,https://www.sciencedirect.com/science/article/pii/S0022053118305374,30 April 2019,2019,Research Article,259.0
"Gizatulina Alia,Hellman Ziv","Department of Economics, University of St. Gallen, Switzerland,Department of Economics, Bar-Ilan University, Israel","Received 28 July 2018, Revised 5 March 2019, Accepted 20 April 2019, Available online 26 April 2019, Version of Record 3 May 2019.",https://doi.org/10.1016/j.jet.2019.04.006,Cited by (5),"We show that contrary to currently widely-held misperceptions, the classical no trade theorem obtains even under heterogeneous priors. That is, when priors are not common, speculative trade is still impossible under common knowledge of rationality. However, trade becomes mutually acceptable if at least one party to the trade puts at least some slight probability on the other party being irrational. We also derive bounds on disagreements in the case of heterogeneous priors and common ====-beliefs.","One commonly hears it said that ‘if there is a common prior among agents, then no (speculative) trade is possible between them’; this is indeed true. It is, however, also not any less common to hear the inverse to that statement, that is, a claim that ‘if priors are not common, then any unbounded volume of trade may ensue’. This last statement is erroneous.====The goal of this paper is two-fold: first we show that even if priors are not common (and there are disagreements amongst agents about the expected value of a random variable), a corresponding version of the No Trade theorem obtains. In other words, whether or not priors are common, no speculative trade is possible if there is common knowledge of rationality.====Second, we extend to the case of heterogeneous priors a result due to Neeman (1996b) that shows the necessity of the existence of slight irrationality of each agent for trade to occur. Specifically, we demonstrate that when agents do not share a common prior, for trade to occur it is sufficient that each agent ==== believes that with some arbitrarily small probability ==== are irrational and play suboptimal strategies, while agent ==== ascribes probability one that himself he is perfectly lucid and plays the best-reply to others' suboptimal strategy.====The No Trade Theorem, as developed by Milgrom and Stokey (1982) and others (such as Rubinstein, 1975; Tirole, 1982; Dow et al., 1990), is considered one of the most surprising of the fundamental insights in the theoretical literature in recent decades. It states that under the assumption of common priors and ex-ante Pareto efficient allocations the arrival of private information will not induce further trade if the acceptability of a proposed trade is common knowledge. Under its usual interpretation as a ‘no speculation’ result, it has been especially perplexing because it stands in stark contrast to the immense volume of trade observed daily in security markets.====The No Trade theorem is usually presented as building upon two other surprising results, the No Disagreement theorem and the No Betting theorem. These theorems essentially state that if agents have common priors, they will never take opposite sides of any commonly acceptable proposed bet even after receiving private information. Moreover, the no betting result is bi-directional, i.e., a common prior precludes betting while heterogeneous priors imply the existence of agreeable bets (i.e. that both agents would ==== about the expected value of a random variable). It is this that leads to the common but erroneous argument that the No Trade theorem is also dependent on the common prior assumption. Indeed, the counter-intuitive quality of the No Trade theorem contrasted with observed trade volumes is sometimes adduced as an argument against assuming common priors in ‘the real world’. In this view, dropping the common prior assumption puts the theory back in harmony with the empirical existence of speculative trade.====We show here that, in fact, the No Trade result is independent of whether or not priors are common; it follows solely from the combination of ==== Pareto efficiency, common knowledge of rationality, and common knowledge of an agreed trade. The proof of this is, somewhat surprisingly, almost entirely a reprise of the proof of the original No Disagreements Theorem of Aumann (1976).====The impact is that attempting to explain the existence of large volumes of speculative trade by supposing that traders do not start from common priors is misguided. To restore the possibility of mutually agreed trading the alternative that remains is to weaken the assumption of common knowledge of rationality.",No trade and yes trade theorems for heterogeneous priors,https://www.sciencedirect.com/science/article/pii/S0022053118304071,26 April 2019,2019,Research Article,260.0
Sihvonen Markus,"Bank of Finland, Research Unit, Snellmaninaukio, P.O. Box 160, 00101 Helsinki, Finland","Received 1 February 2018, Revised 15 April 2019, Accepted 16 April 2019, Available online 23 April 2019, Version of Record 24 April 2019.",https://doi.org/10.1016/j.jet.2019.04.005,Cited by (4),", but idiosyncratic risks are not priced. Simulations confirm that the limiting results are relevant when the population of irrational agents is large.","Models in financial economics commonly assume that agents maximize expected utility under correct beliefs. Alchian (1950) and Friedman (1953) offer an evolutionary justification for such rationality. They argue that wealth dynamics will ultimately drive irrational agents out from the market, in the long run only rational ones will prosper.====This view is countered by De Long et al. (1990). They explain how the unpredictability of irrational noise traders' beliefs makes it risky for rational arbitragers to trade against them. Moreover, due to more aggressive risk-taking, the noise traders may earn higher expected returns than rational ones. In related work, De Long et al. (1991) note that in the long run irrational agents may actually come to dominate the market.====De Long et al. (1991) make an interesting additional point. Disagreement over idiosyncratic uncertainties reduces the survival probabilities of individual irrational agents but not of irrational agents as a whole. Even though the survival probabilities of each individual irrational agent become vanishingly small, irrational agents can still survive as a group. This is because a large population of irrational agents can feature a small group of very lucky types.====The seminal paper of De Long et al. (1991) has two main deficiencies. First, the analysis is partial equilibrium: irrational agents have no impact on prices even when they come to dominate the economy. This assumption is likely to affect the survival results. At the same time the paper cannot address the validity of the standard rational expectations paradigm in asset pricing. Second, the analysis is based on various approximations that may limit its value (Blume and Easley, 2006).====The later market selection literature analyzes the survival of irrational agents mainly using complete market general equilibrium frameworks. This includes Sandroni (2000), Blume and Easley (2006), Kogan et al. (2006), Yan (2008), Kogan et al. (2017) and Borovicka (2018). These models do not feature idiosyncratic uncertainty and, with the exception of a recent contribution by Massari (2018), the papers make no distinction between individual and group level survival.====This paper makes two key contributions. First, I consider the role of idiosyncratic uncertainty using an extension of a standard general equilibrium framework employed by the modern market selection literature. Here I obtain results similar to those of De Long et al. (1991) using an economically and mathematically improved model. I also shed new light on the economic mechanisms behind the results. Second, I am the first to provide a full analysis of price distortions caused by disagreement over idiosyncratic uncertainties.====The classic convergence results for an individual irrational agent, derived by Blume and Easley (2006), depend only weakly on preferences. In contrast I find that the evolution of the total consumption of irrational agents depends crucially on the elasticity of intertemporal substitution (EIS) parameter. Namely the total consumption of irrational agents tends to be increasing in time when EIS is above one. I explain that EIS affects the results mainly through a savings channel: a high EIS increases the savings rate of irrational agents. I further argue that in many cases the results depend only on the EIS of the irrational but not on the EIS of the rational agents.====A key contribution of the paper is to analyze the price impact of irrational agents given disagreement over idiosyncratic uncertainties. Here I show that such disagreements only distort the equilibrium interest rates but not market prices of risk. Idiosyncratic risks are not priced because there are enough agents with correct views about each such uncertainty. Interest rates are distorted as disagreement over idiosyncratic risks alters equilibrium saving decisions.====The closest recent paper to this one is Massari (2018). He studies a model with a continuum of agents and similarly to this paper and De Long et al. (1991) finds a separation between individual and group level survival. However, Massari (2018) does not consider disagreement over idiosyncratic uncertainties. Rather he shows that similar results can be derived in some cases without idiosyncratic uncertainty when a continuum of agents possesses sufficiently heterogeneous beliefs over the (path of the) state of the economy. Overall his results nicely complement those in this paper.====The structure of this paper is the following. I first lay down a simple general equilibrium model with idiosyncratic uncertainties and demonstrate that the results for individual and group survival are different. I then analyze equilibrium asset prices and show that disagreement over idiosyncratic uncertainties distorts interest rates. Finally, I argue using simulations that the results are relevant for large finite populations as well as discuss various generalizations of the results.",Market selection with idiosyncratic uncertainty,https://www.sciencedirect.com/science/article/pii/S0022053119300444,23 April 2019,2019,Research Article,261.0
Altermatt Lukas,"University of Wisconsin-Madison, United States of America,University of Basel, Switzerland","Received 8 May 2018, Revised 21 March 2019, Accepted 16 April 2019, Available online 18 April 2019, Version of Record 17 May 2019.",https://doi.org/10.1016/j.jet.2019.04.004,Cited by (5), shocks.,"Asset markets are a central part of today's economies, and an important driver for business cycles. Arguably, the last two recessions in the U.S. have been caused by downturns on asset markets. Thus, it is clear that they are also a major factor to consider for monetary policy. However, it remains unclear how exactly monetary policy should take them into account, and how it should react to changes in asset returns. While some economists think that monetary policy should basically ignore asset markets,==== others like Taylor (2014) claim that the wrong stance of monetary policy regarding asset markets caused the financial crisis of 2007-2009.====One reason why economists disagree on how monetary policy should take asset markets into account is that the (mainstream) literature on monetary policy has mostly ignored financial markets, especially before the financial crisis. In the aftermath of the financial crisis, some parts of the literature started to incorporate financial aspects, but often in an ad-hoc way. The New Monetarist literature following Lagos and Wright (2005) has taken financial assets more seriously and started to integrate them into its models, but focused mostly on their liquidity properties. While liquidity is certainly an important factor to consider, savings properties of financial assets, and life-cycle considerations in general, also appear to be central to their pricing. So to properly study the effects of monetary policy on financial markets, I think one needs a model where both liquidity and life-cycle considerations are essential.====This paper aims to better integrate the role financial markets play for savings into the New Monetarist literature. More precisely, this paper has three goals: (1) I want to build a tractable model that takes both liquidity and life-cycle considerations seriously; (2) I want to use this model to explain some stylized facts about the correlation of bond and equity returns, in order to ensure that the model is able to capture the relevant mechanisms regarding the substitutability of different assets; and (3) I want to study optimal monetary and fiscal policy within this model, in order to determine whether, and under which circumstances, monetary policy should react to asset return shocks.====To reach the first goal, I combine two standard frameworks. To study the role of savings and make life-cycle considerations essential, I use the overlapping-generations (OLG) model based on Wallace (1980), as it is the most natural framework. While there are many OLG papers in which money plays a role, money is used only as a savings instrument in most, if not all, of them. Since I want to build a model where money can be used not only as a savings instrument, but also as a medium of exchange instrument for transactions, I combine the OLG structure with the Lagos and Wright (2005) (LW) framework, as money is essential for transactions in this class of models.====Regarding the second goal, I analyze U.S. data on bond and equity returns. The data shows that since at least 1982, the dividend-price ratios of equity and real interest rates were negatively correlated in the United States. Around the beginning of the financial crisis however, this correlation ceased to exist. The regime change occurred more or less at the same time as the zero lower bound was hit. With the help of the model, I want to understand why different regimes regarding the correlations exist, and how an economy can move from one regime to the other. Being able to explain these stylized facts is important for two reasons: First, the question itself is relevant, as understanding why such regime changes occur can help policymakers and investors to make decisions. Second, ensuring that the model matches these stylized facts increases the credibility of the model, and makes the policy analysis, and thus the third goal of the paper, more relevant.====Combining the OLG and LW frameworks allows me to create a model in which prices for government bonds and risky assets are determined endogenously. In the model, agents can use government bonds (nominal, safe assets), equity (real, risky assets), or fiat money to save. Fiat money is essential for intra-period trade, but it is typically dominated in terms of the rate of return by the other assets, so it is not used for savings in equilibrium, except at the zero lower bound (ZLB). For simplicity, I assume that bonds and equity cannot be used for intra-period trade (I relax this assumption in Appendix A). There are two types of agents in the model, called buyers and sellers. Buyers have a finite life-cycle and cannot work in the last period of their life. This makes savings essential for them, because it is the only way to acquire consumption in the last period. Sellers on the other hand have an infinite lifespan, which means that their investment decisions are solely driven by arbitrage considerations. Although bonds have no liquidity properties, a ZLB (i.e., a situation where the interest rate on bonds is equal to zero) can occur. This is because buyers demand a positive amount of bonds for any nonnegative interest rate. If their demand is higher than the supply of bonds at positive interest rates, the market can only clear at the ZLB, and buyers use both bonds and money to save. In other words, the ZLB is hit due to a scarcity of savings instruments that leads to an increase in the stochastic discount factor (SDF).====If the supply of bonds is plentiful in this economy, the risk-averse buyers don't want to hold risky assets. In this case, the price of risky assets has to equal their discounted expected payoff, because sellers are indifferent about holding them at this price. If the bond supply is scarce, interest rates are low, and buyers are willing to hold some amount of risky assets in order to increase the expected return of their savings portfolio. The lower the bond interest rate, the more risky assets buyers are willing to hold - this is a portfolio balance effect. As long as the marginal investor in risky assets is a seller, this portfolio balance effect does not affect the price of risky assets. In this regime, there is no correlation between bond and equity returns, because a decrease in bond interest rates leads to an increase in demand for risky assets, but no changes in their price. However, once the demand for risky assets by buyers becomes sufficiently high, the price of risky assets has to increase to clear the market. In this regime, there is a positive correlation between the return on bonds and equity. In general, one can say that the returns are positively correlated if all assets are scarce, while there is no correlation if only bonds (or safe assets in general) are scarce, and the supply of risky assets is plentiful.====In the data, one can observe that the U.S. moved from a regime with positive correlation away from the zero lower bound to a regime at the zero lower bound with no correlation. There are two reasons in the model why an economy can move from a regime with negative correlation away from the zero lower bound to a regime without correlation at the zero lower bound: (1) a decrease in inflation, or (2) a simultaneous decrease in the supply of safe assets accompanied by an increase in the supply of risky assets. I argue that both of these situations occurred during and after the financial crisis. Inflation decreased in the United States as well as in most other developed economies, and many assets that were considered safe before the financial crisis, such as mortgage-backed securities or sovereign bonds from southern European countries, turned out to be risky at that time. Thus, the model is able to suggest plausible explanations for the transition from a world where dividend-price ratios and real interest rates were strongly correlated to one where they are uncorrelated.====The results I find for optimal monetary and fiscal policy are novel and interesting: As it is standard in this type of models, the Friedman rule (i.e., setting the opportunity cost of holding money to zero) allows the first-best to be achieved. Away from the Friedman rule, the fiscal authority can increase welfare by issuing a sufficient number of government bonds. However, in reality it may not be feasible politically to run the Friedman rule or to issue a large amount of public debt. If that is the case, then the monetary authority can use an optimal stabilization policy which applies different inflation rates after equity market shocks in order to increase welfare. The optimal policy is procyclical, i.e., inflation should be set higher when stock market returns are high and lower when stock market returns are low. Such a policy is able to minimize or even completely eliminate the risk that savers face by creating a negative correlation between the returns of nominal government bonds and real assets.====Besides this main result, there are two other interesting findings regarding monetary policy: The model shows that (1), quantitative easing reduces the welfare of future generations, because it reduces the returns on all savings instruments; and (2), increasing the inflation target reduces the risk of hitting the zero lower bound, but still decreases welfare overall.","Savings, asset scarcity, and monetary policy",https://www.sciencedirect.com/science/article/pii/S002205311830156X,18 April 2019,2019,Research Article,262.0
"Stachurski John,Toda Alexis Akira","Research School of Economics, Australian National University,Department of Economics, University of California San Diego","Received 25 January 2019, Accepted 5 April 2019, Available online 12 April 2019, Version of Record 12 April 2019.",https://doi.org/10.1016/j.jet.2019.04.001,Cited by (34),It has been conjectured that canonical Bewley–Huggett–Aiyagari heterogeneous-agent models cannot explain the joint distribution of income and ,"When studying wealth inequality, one empirical feature stands out as striking and persistent over time and space: the wealth distribution exhibits a power law tail. This fact was first discovered by Pareto, 1896, Pareto, 1897 and has since been confirmed by many studies.==== A closely related observation is that the income distribution is also heavy-tailed, although its Pareto exponent is significantly larger, implying a heavier tail for wealth than income.====It is well known in the quantitative macroeconomics literature that canonical Bewley, 1977, Bewley, 1983, Bewley, 1986–Huggett (1993)–Aiyagari (1994) models have difficulty in explaining the joint distribution of income and wealth. For example, Aiyagari (1994) documents that the wealth Gini coefficient is 0.32 in the model, while it is 0.8 in the data. Huggett (1996) notes that the model-implied top 1% wealth share is half of that in the data. Krueger et al. (2016) argue that idiosyncratic unemployment risk and incomplete financial markets alone cannot generate a sufficiently dispersed wealth distribution,==== even though such dispersion is crucial for the study of aggregate fluctuations. More specifically, Benhabib et al. (2017) show that, in a setting where income has a Pareto tail and agents use a linear consumption rule, the Pareto exponent of wealth is either entirely determined by the distribution of returns on wealth, or equal to the Pareto exponent of income. They argue that similar results must obtain with rational agents with constant relative risk aversion (CRRA) preferences because, in such settings, the policy rules are asymptotically linear.====In this paper we confirm and significantly extend this conjecture by showing that, for canonical Bewley–Huggett–Aiyagari models, all attempts to explain the large skewness of the wealth distribution are bound to fail. By the qualification “canonical”, we mean models in which (i) agents are infinitely-lived, (ii) saving is risk-free, and (iii) agents have constant discount factors. In our main result (Theorem 8), we prove that the equilibrium wealth distribution inherits the tail behavior of income shocks in ==== such model. This is an impossibility theorem in the following sense: the tail thickness of the model output (wealth) cannot exceed that of the input (income). If income is light-tailed (e.g., bounded, Gaussian, exponential, etc.), so is wealth. If income is heavy-tailed, so is wealth, but with the same Pareto exponent, contradicting the empirical relationship between the income and wealth distributions stated above. Thus, one cannot produce a model consistent with the data without relaxing at least one of the assumptions (i)–(iii).====Our findings can be understood via the following intuition. In infinite-horizon dynamic general equilibrium models, the discount factor ==== and the gross risk-free rate ==== must satisfy the “impatience” condition ====, for otherwise individual consumption diverges to infinity according to results in Chamberlain and Wilson (2000), which violates market clearing. But under this impatience condition, we show that rational agents consume more than what is implied by the permanent income hypothesis ==== (the interest income), and the accumulation equation for wealth ==== becomes a “contraction” in the sense that==== for large enough ====, where ==== is income and ==== is some positive constant strictly less than 1. This inequality implies that the income shocks die out in the long run, and hence the wealth distribution inherits the tail behavior of income shocks. To obtain (1), we use the results from Li and Stachurski (2014), who show the validity of policy function iteration for solving income fluctuation problems. With the bound (1) in hand, we characterize the tail behavior of wealth using the properties of the moment generating function and applying several inequalities such as Markov, Hölder, and Minkowski.====Relative to the work of Benhabib et al. (2017) discussed above, our contributions are as follows: First, we provide a complete proof of the impossibility result stated above in the context of an equilibrium model with rational, optimizing agents, thereby confirming their conjecture on optimizing households with CRRA utility in a general equilibrium setting. Second, our results are established in a class of models where relative risk aversion need not be constant. We require only that relative risk aversion is asymptotically bounded. This means that minor deviations from standard utility functions cannot reverse our results. Similarly, our income process is required only to have a finite mean. Third, we provide a complete analytical framework on tail thickness that accommodates both light-tailed and heavy-tailed distributions, and connect it to the joint distribution of income and wealth. In the sense that we handle all classes of shocks and allow for nonstationary additive processes, our proofs extend what is contained in the related mathematical literature, such as the work of Grey (1994) on Pareto tails.==== Moreover, our proofs are almost completely self contained, and hence can be readily adapted to subsequent research on income and wealth distributions that tackles extensions to our framework.====To tie up loose ends, we also show that the conditions of the impossibility theorem are tight. In Section 4, we show through examples that relaxing any of the three assumptions behind our main theorem (agents are infinitely-lived, saving is risk-free, and the discount factor is constant) can generate Pareto-tailed wealth distributions. In doing so, we draw on existing literature as it pertains to this topic and also provide a simple, exactly solved model that features heterogeneous discount factors and a Pareto-tailed wealth distribution.",An impossibility theorem for wealth in heterogeneous-agent models with limited heterogeneity,https://www.sciencedirect.com/science/article/pii/S0022053119300353,12 April 2019,2019,Research Article,263.0
"Dizdar Deniz,Moldovanu Benny,Szech Nora","Department of Economics, University of Montréal, C.P. 6128 succursale Centre-Ville, Québec H3C 3J7, Canada,Department of Economics, University of Bonn, Lennéstr. 37, 53113 Bonn, Germany,Department of Economics and Management, Karlsruhe Institute of Technology, Kaiserstr. 12, 76131 Karlsruhe, Germany","Received 19 October 2018, Accepted 7 April 2019, Available online 12 April 2019, Version of Record 15 April 2019.",https://doi.org/10.1016/j.jet.2019.04.002,Cited by (8),"Agents in a finite two-sided market are matched assortatively, based on costly investments. Besides signaling privately known, complementary types, the investments also directly benefit the match partner. The bilateral external benefits induce a complex feedback cycle that amplifies the agents' signaling investments. Our main results quantify how the feedback effect depends on the numbers of competitors on both sides of the market. This yields detailed insights into the equilibria of two-sided matching contests with incomplete information, in particular for markets of small or intermediate size. It also allows us to shed some new light on the relationship between finite and continuum models of pre-match investment.","Signals play an important role whenever agents need to form matches. In virtually every real-life matching situation, be it in the labor, the marriage, or the education market, agents use signals in order to transmit information about their own quality. In addition to their signal value, such investments also yield direct utility to the matched partner. Flashy facilities built by universities or firms are signals of quality that also create direct benefits for future students or employees, while individuals' investments in education and training signal underlying ability and are also valued directly by universities or prospective employers.====We study two-sided matching contests with arbitrary numbers of participants and with an NTU (nontransferable utility) matching market. The investments used by agents to signal information about privately known, complementary productive types also benefit the match partner.====The bilateral external benefits induce a feedback cycle that may cause agents to invest much more than they would if signals were completely wasteful. Increased signaling on one side of the market (more precisely, larger differences between the investments of different types) intensifies the competition among agents on the other side, causing them to invest more, which in turn intensifies again the competition on the first side, and so on...====In this paper, we shed light on how the feedback cycle works in finite markets with incomplete information, under the standard assumption that agents on each side of the market are ex-ante symmetric. In particular, we quantify how the feedback cycle depends on the numbers of agents on both sides of market, and we examine how it affects agents' equilibrium behavior and interim expected utilities.====Our focus is on finite markets since many real life examples, be it the marriage market in a rural area, or the labor market for specialized workers, are best modeled by assuming a moderate market size. Also standard market experiments in the laboratory focus on markets of intermediate size.====Both functions of pre-match investments have been emphasized in the literature. Hoppe et al. (2009, henceforth HMS) analyze signaling behavior when the signals are ==== as in Spence (1973). In this case, the investment feedback effect does not exist. On the other hand, in the important papers by Peters and Siow (2002), Peters (2007) and Bhaskar and Hopkins (2016), an agent's pre-match investment benefits his or her partner directly (so that feedback effects are present) but it has no signaling effect. Arguably, most pre-match investments observed in reality have both a signaling and a productive function (Hopkins, 2012), which motivates our setting.====Our two-sided ==== model combines the signaling model of HMS and the investment model of Peters (2007), and has the following main features:====In a market with finitely many participants, agents face uncertainty about the types of competitors. They are also uncertain about the actual types and investments of potential partners. These uncertainties differentiate our model from standard pre-match investment models with a continuum of heterogeneous agents, in which each agent knows exactly where he or she is ranked in the competition, and what he or she will get in return for any particular investment. Relative to a continuum economy, the finiteness of the market and the resulting uncertainties dampen the investment feedback effect. We quantify how the strength of the feedback effect depends on the numbers of men and women, and we identify the largest eigenvalue of a particular matrix as the key measure for the dampening of the feedback loop. More precisely, we derive sharp conditions for the existence of side-symmetric, strictly separating equilibria that only depend on this feedback coefficient (which is smaller than 1 in any finite market, while it is equal to 1 in a continuum economy) and on the parameters describing the marginal external benefits and the marginal costs of the investments. If the product of marginal external benefits is too high compared to the product of marginal costs and if competition is too intense, the feedback process can become self-perpetuating and push investments beyond any bounded multiple of the pure signaling investments.==== For the case in which the external benefits to partners are linear functions of the investments, we also find the unique side-symmetric, separating equilibrium in closed form. For most practical cases existence is not an issue, but the increases in investment needed to signal small quality differences may be very large even in markets of moderate size. A related phenomenon currently seems to arise in the case of US colleges. The New York Times speaks of a ‘paradox’ which in our model, however, occurs in equilibrium: ====Investments into students' amenities (and students' fees) thus steeply increase across competitors in order to signal (probably much smaller) differences in quality.====Computing the feedback coefficient explicitly (in closed form for balanced or slightly unbalanced markets, numerically for all other markets) allows us to obtain detailed qualitative and quantitative insights into how the strength of the feedback cycle depends on market size. For example, entry of additional agents on the short side of the market or simultaneous entry on both sides intensify the feedback cycle while, somewhat surprisingly, entry on the long side only generally weakens it. In particular, this implies that some of the main entry-related comparative statics results in HMS do not extend to the case of ==== signals.====Our results also produce some interesting bounds on under-investment for environments in which investments are “truly” ==== in the sense that all Pareto efficient and individually rational investments for a given pair of agents are strictly positive (i.e., exceed the privately optimal investments). These bounds provide quantitative information about the extent to which competition can rule out extreme under-investment in small markets with productive investments.====For matching contests in which investments are ====, we identify the exact asymptotic behavior of equilibrium utilities as the numbers of men and women go to infinity. In this case, equilibrium utilities converge to those in the unique equilibrium of a continuum model, for which the return to any possible investment is certain. In particular, this shows that, even though investments are only partially wasteful, the entire difference between aggregate match surplus and aggregate information rents gets dissipated through competition.====If the marginal benefit from investment is constant and equal to the marginal cost (called below the ==== or ==== investment case), the continuum model does not admit a side-symmetric strictly separating equilibrium: the intense competition together with the certainty of returns drive investments to infinity. However, such an equilibrium exists in any finite market with the same characteristics and we are able to characterize the limit behavior of equilibrium utilities: in large, balanced markets, the difference between aggregate match surplus and aggregate information rents is always shared fifty-fifty between men and women, irrespectively of other economic aspects such as the shares governing the division of physical surplus in each matched pair and the distributions of types.====  Considering one side of the market only, our agents are in a contest situation (see e.g. the survey of Konrad, 2007): they compete by means of sunk investments for heterogeneous “prizes,” which correspond to matches with the various potential partners. Recognizing this analogy, a sizeable literature has studied pre-match investment problems as matching contests, where agents on both sides of a two-sided market make observable investments and are then matched positive assortatively on the basis of these investments. In these papers, positively assortative matching based on investments is typically assumed, but it also corresponds to the stable outcome of a frictionless matching market (post-investment) with nontransferable utility. This is the case for the complete information models of Peters and Siow (2002), Peters (2007), and Bhaskar and Hopkins (2016), in which an agent who invests more generates higher benefits for partners, and also (in equilibrium) for the signaling model of HMS.====The challenges of analyzing non-cooperative equilibria of two-sided matching contests with a ==== number of participants when external benefits generate feedback effects are succinctly described in Peters (2007).==== With a few important exceptions (Peters, 2007, Peters, 2011; Bhaskar and Hopkins, 2016; Cole et al., 2001b; Felli and Roberts, 2016), the literature on pre-match investment problems has circumvented this difficulty by focusing on continuum models in which agents behave competitively (e.g., Cole et al., 2001a; Peters and Siow, 2002, Nöldeke and Samuelson, 2015, Dizdar, 2018).====The work of Peters, 2007, Peters, 2011 demonstrates that non-cooperative equilibrium investments in very large (but finite) two-sided matching contests can be quite different from the investments predicted by a continuum model with competitive agents. More precisely, for models without signaling concerns and with productive investments, Peters shows that equilibrium investments in unbalanced matching contests generally do not converge to competitive (or hedonic) equilibrium investments as the numbers of men and women go to infinity. In particular, agents at the bottom of the distributions generally over-invest. Our results and techniques do not allow new insights about equilibria in very large markets with truly productive investments, but the arguments in Peters (2011) imply that qualitative results similar to those in his paper, with additional over-investment due to signaling, must hold in our model for this case (compare the discussion in Section 4). We focus instead on a much more detailed analysis of the feedback cycle due to external benefits in a model where investments also serve as signals.====Bhaskar and Hopkins (2016) study a model with an NTU matching market and ==== investments, building on the tournament model of Lazear and Rosen (1981) rather than on the literature on all-pay contests. Moreover, they assume complete information and that agents on either side of the market are ex-ante symmetric. They prove the existence of a unique equilibrium and show that agents over-invest unless the two sides of the market are completely symmetric. While their main focus is on the analysis of a continuum model, they also show (under certain conditions) that the corresponding, unique equilibrium is the limit of the non-cooperative equilibria for a finite model.====Olszewski and Siegel (2016) characterize asymptotic bidding behavior in one-sided all-pay contests with many agents and many prizes. Their general results allow for complete or incomplete information and for ex-ante asymmetric agents, but because the prize structure is given exogenously these findings cannot be applied to characterize equilibrium behavior in environments with bilateral investments and with external benefits. Moreover, their results only hold for very large contests where particular approximation techniques can be applied.====  The paper is organized as follows. In Section 2, we introduce the model and define various pieces of notation. Section 3 presents the basic equilibrium characterization and our main results about the investment feedback effect, including the closed form solution for the case of linear external benefits and several illustrations. Section 4 contains the results for large markets with partially wasteful or TU investments. All proofs are in an Appendix.",The feedback effect in two-sided markets with bilateral investments,https://www.sciencedirect.com/science/article/pii/S0022053119300365,12 April 2019,2019,Research Article,264.0
"Chatterji Shurojit,Zeng Huaxia","School of Economics, Singapore Management University, Singapore,School of Economics, Shanghai University of Finance and Economics, Shanghai 200433, China,Key Laboratory of Mathematical Economics (SUFE), Ministry of Education, Shanghai 200433, China","Received 4 November 2017, Revised 12 March 2019, Accepted 6 April 2019, Available online 11 April 2019, Version of Record 15 April 2019.",https://doi.org/10.1016/j.jet.2019.04.003,Cited by (8),"We study random mechanism design in an environment where the set of alternatives has a Cartesian product structure. We first show that all generalized random dictatorships are sd-strategy-proof on a minimally rich domain if and only if all preferences are ====. We call a domain satisfying top-separability a ====, and furthermore generalize the notion of connectedness (====) to a broad class of multidimensional domains: ==== ====. We show that in the class of minimally rich and connected==== domains, the ==== restriction is necessary and sufficient for the design of a ====).","Multidimensional models arise very naturally in economic environments as it is often the case that the object of choice consists of several attributes or components (commodities in consumer theory, positions in political economy, different levels of provision of public goods, etc), with no dependence across choices in different components.==== The set of alternatives thus has the structure of a Cartesian product set, i.e., ====, where ==== collects all components, ==== is a component, and ==== is the corresponding component set.==== The underlying Cartesian product structure on the set of alternatives allows for a richer description of available alternatives and introduces furthermore the possibility of defining domains of restricted preferences which take cognizance of the multidimensional structure, and allow positive results for aggregation and economic design. We explore the theoretical underpinnings of such multidimensional preference domains from the perspective of mechanism design. We first identify that a particular condition, ==== (introduced by Le Breton and Weymark, 1999), is fundamental in formulating multidimensional preferences that admit new possibilities for mechanism design.==== Our principal finding is that within the class of top-separable preferences, ==== (introduced by Barberà et al., 1993), a particular generalization of single-peakedness to a multidimensional setting, emerge as the unique preference domains that allow for the design of attractive random mechanisms. Thus the notion of single-peakedness, which is well-studied and prominent in aggregation theory, voting theory and political economy, turns out to be a particularly distinguished one in the context of multidimensional random mechanism design.====We focus on probabilistic mechanisms in multidimensional settings in the absence of monetary transfers where the set of alternatives is assumed to be finite.==== We impose a strong version of the incentive compatibility requirement by requiring that truth-telling first-order stochastically dominate every possible manipulation of preferences. We thus study Random Social Choice Functions (RSCFs) that satisfy the ordinal version of strategy-proofness formulated by Gibbard (1977), which we henceforth term ====.==== We also impose the mild condition that the RSCFs satisfy unanimity, which says that if an alternative is top ranked for every agent at a preference profile, then it receives probability one under the RSCFs at that profile.====An important class of RSCFs is the class of ====. These are defined by fixing a probability distribution over agents; the probability assigned to an alternative at a preference profile is then the sum of the weights of the agents who have this particular alternative as their top ranked alternative. Random dictatorships are sd-strategy-proof and ex-post efficient (a strengthening of unanimity), and allow for a equitable distribution of power among agents which is precluded by a deterministic dictatorship. These are however not entirely satisfactory from the design point of view as they ====; indeed ==== alternative that is not top ranked for some agent at the profile in question can never get strictly positive probability. In particular, such an alternative may be second ranked for all agents in a profile where agents disagree on peaks; we refer to such an alternative as a ====, and suggest that it is desirable to design RSCFs that have the flexibility to give positive probability to such an alternative.====Under a Cartesian product structure, random dictatorships can be naturally generalized to accord with the multidimensional setting in the following way. Instead of fixing a probability distribution over agents, we fix a probability on each ====, which is an ====-tuple of agents, and associates each component with an agent who can be viewed as the dictator of that component (note that one agent can be associated to multiple components). At a preference profile, according to one voter sequence, we can assemble a unique alternative whose ====th component is the ====th component of the corresponding component dictator's preference peak. The probability assigned to an alternative at a preference profile is then the sum of the weights of the voter sequences which can assemble this alternative. These random mechanisms are called ====, and were introduced by Chatterji et al. (2012). Generalized random dictatorships recognize the Cartesian product structure and allow for greater flexibility than do random dictatorships as at some preference profiles: Some non-peak alternatives can be assembled and receive strictly positive probability. In contrast to random dictatorships, certain preference restrictions must however be imposed to ensure sd-strategy-proofness of a generalized random dictatorship. We show in Proposition 1 that top-separability is necessary and sufficient for sd-strategy-proofness of all generalized random dictatorships. However, due to the somewhat limited assembling capability of voter sequences, generalized random dictatorships sometimes ignore compromise alternatives.====This paper examines restricted domains of multidimensional preferences that allow us to construct sd-strategy-proof RSCFs which are flexible in that they systematically admit compromise. The preference domains we study satisfy a particular “richness” property that is based on the idea of connectedness initially proposed by Grandmont (1978) and Monjardet (2009), and has been recently adopted to explore various issues which include the equivalence of local sd-strategy-proofness and sd-strategy-proofness (e.g., Carroll, 2012; Sato, 2013; Cho, 2016; Mishra, 2016), the extent to which RSCFs can depend on agents' preferences (Chatterji and Zeng, 2018), and the characterization of preference restrictions that allow one to design attractive RSCFs (Chatterji et al., 2016). The notion of connectedness requires that one be able to reconcile the differences between two preferences via a sequence of preferences in the domain where each successive pair involves one “local switch” of two contiguously ranked alternatives. This richness condition restricts the probabilities received by alternatives that do not switch across two successive preferences, and plays a fundamental methodological role in deriving the results mentioned above.====This notion of connectedness however does not apply to domains of multidimensional preferences, e.g., the top-separable domain, as it is often the case that multiple pairs of alternatives have to be switched simultaneously across two successive preferences. We introduce a new notion of a connectedness which permits the requisite simultaneous local switches, and allows us to investigate systematically domains of multidimensional preferences that permit the design of nice sd-strategy-proof RSCFs. The domains we consider are termed ==== ====; these are subsets of the top-separable domain, and include the well studied instances of separable preferences (Barberà et al., 1991; Le Breton and Sen, 1999), multidimensional single-peaked preferences (Barberà et al., 1993), and their intersection and unions. Connected==== domains also possess the requisite generality and structure that would in principle allow one to investigate other issues being studied in the literature (like the equivalence of local sd-strategy-proofness and sd-strategy-proofness, etc, alluded to above), and can presumably be exploited beyond this paper.====In the class of connected==== domains, multidimensional single-peaked domains are an important and well studied class. These are a particular generalization of the idea of single-peaked preferences to a multidimensional setting using the Cartesian product structure and the city block metric. Our first theorem characterizes multidimensional single-peaked domains as the unique domains that permit the design of sd-strategy-proof and unanimous RSCFs departing from random dictatorships/generalized random dictatorships systematically, in that they admit compromises, wherein the compromise alternatives necessarily receive strictly positive probabilities whenever they appear (see Theorem 1). Our version of multidimensional single-peaked domains allows elements of each component set to be arranged on a tree which is a generalization of multidimensional single-peakedness initiated by Barberà et al. (1993).==== In the special case where the connected==== domain contains two complete reversal preferences, we refine the domain characterization to the more familiar formulation of Barberà et al. (1993). We next provide a characterization result for multidimensional single-peaked domains using deterministic social choice functions (see Theorem 2). We do so by replacing the compromise property by the familiar axiom of anonymity.====We finally turn to the setup of voting under constraints originally proposed by Barberà et al. (1997). Here, not all alternatives in the underlying Cartesian product structure are feasible. We investigate what structure on the set of feasible alternatives and preferences (applicable now only to the restriction of the original preferences to the feasible alternatives) would allow us to define RSCFs which satisfy our requirements of unanimity, sd-strategy-proofness and compromise on connected==== domains. We deduce that the set of feasible alternatives must be factorizable as a Cartesian product of trees, and the preferences must satisfy a particular version of multidimensional single-peakedness w.r.t. the feasible alternatives (see Theorem 3). Our results are therefore robust to voting under constraints.====The rest of the paper is organized as follows. The remainder of the Introduction explains in greater detail the relation of this paper to the literature. Section 2 describes the model, introduces generalized random dictatorships, establishes the domain richness condition, and specifies the formal notion of compromise. Section 3 presents the domain characterization results for multidimensional single-peaked preferences, while Section 4 concludes. The Appendix gathers proofs, examples and verifications that are not included in the main text.",Random mechanism design on multidimensional domains,https://www.sciencedirect.com/science/article/pii/S0022053119300377,11 April 2019,2019,Research Article,265.0
Dilmé Francesc,"University of Bonn, Germany","Received 10 March 2016, Revised 8 March 2019, Accepted 26 March 2019, Available online 2 April 2019, Version of Record 5 April 2019.",https://doi.org/10.1016/j.jet.2019.03.010,Cited by (4),"This paper analyzes quality and reputation management in the presence of adjustment costs. A firm produces and sells ==== over time. The firm can choose the product quality at any time, but changing it requires a costly investment. Customers learn about the product quality through a quality-dependent Poisson process. We characterize reputational dynamics, showing that the arrival of news may generate jumps in both the firm's quality choice and its reputation. We characterize the set of equilibria as a function of the adjustment costs, and show that the firm can benefit from increased adjustment costs as such costs enhance the credibility of its decisions.","In many markets for experience goods, product quality depends mostly on the production inputs chosen by firms. For example, managers of restaurants can determine the quality of the service by hiring or firing employees, like attendants and chefs, with different levels of human capital. As the cost of changing the production inputs is high, quality choices tend be persistent.==== Such persistence may help firms keep their reputation for producing high-quality products.==== The managers' temptation to fire a good chef to save salary costs is diminished not only by the revenue decline due to the resulting reputation loss, but also by the future costs of hiring and training another chef to regain reputation.====This paper characterizes equilibrium reputation dynamics in a setting with adjustment costs. A monopolistic firm sells an experience good. It chooses the product quality at each instant, which can be high or low. It incurs a downgrading (upgrading) cost whenever it downgrades (upgrades) the quality. Customers do not observe the quality and, instead, learn over time as news arrives following a public quality-dependent Poisson process. The price is set equal to the firm's reputation, which is the customers' belief that the product quality is high. We characterize the Markov equilibria, and show that they can be ranked in terms of the profits obtained by the firm. We identify how adjustment costs affect the firm's ability and incentives to build reputation for producing high-quality products.====The main novelty of this paper is the introduction of adjustment costs in a reputation model where the firm has full control over the product quality. The reputation of a firm stems from both the “belief” that customers have on the firm's previous quality choices, and the “trust” that they have in the current quality choice. Previous quality choices are the payoff-relevant private information of the firm (sometimes referred to as its “type”), as they determine the costs of the firm's unconstrained quality choice. The model permits studying the optimal timing of the quality change, e.g., ==== a restaurant decides to fire or hire a chef. Our analysis complements the analysis in Board and Meyer-ter-Vehn (2013, henceforth BMtV), who study the restaurant's choice of the product quality ==== the chef exogenously quits (see a more detailed discussion below). We determine how the timing of the quality change and the reputation dynamics depend on the costs of upgrading or downgrading the product quality. We obtain that increasing the upgrading cost may raise reputation by acting as a commitment device. Conversely, there may be no equilibrium with positive reputation if the upgrading cost is low enough.====To illustrate the influence of the signal process on reputational dynamics, we focus our analysis on Poisson news processes characterized by either (infrequent) good news or (infrequent) bad news. The equilibrium reputational dynamics are similar to those in BMtV: cyclical in the good-news case, and divergent in the bad-news case. The reason is that, in both models, the magnitude of the increase in the firm's future revenue from upgrading the product quality mostly depends on the news structure. Unlike in BMtV, however, the firm has full control of the quality choice in our model. This implies that its reputation changes fast (or jumps) when the incentive to change the product quality is strong, while there are long periods of time when the firm neither upgrades nor downgrades the product quality and reputation changes slowly. This makes the firm's reputation fragile: a trivial equilibrium in which the firm produces low-quality products at all times always exists.====In the good-news case, reputation dynamics are characterized by two cutoffs. If the reputation is above the upper cutoff, the firm downgrades the product quality immediately (with a probability mass point), and the firm upgrades the product quality when its reputation is below the lower cutoff. When the reputation is intermediate, it has a strict incentive to maintain the product quality. We obtain the effect of an increase of the upgrading cost on the equilibrium value of reputation is non-monotone. On the one hand, upgrading the product quality is suboptimal when the required upgrading cost is too high. On the other hand, when the upgrading cost is too low, there is no equilibrium in which the firm produces high-quality products. Intuitively, in an equilibrium in which high-quality products are produced on the path of play, the reputational value of doing so cannot be low. Such value has to compensate for the forgone cost savings from downgrading the product quality. Nevertheless, when the upgrading cost is low, the reputational value of producing high-quality products has to be low as well. Otherwise, the firm would have a strict incentive to upgrade the product quality and its reputation would stay high even in the absence of the arrival of news, in which case the firm would not have an incentive to produce high-quality products. Thus, an increase in the upgrading cost may increase the equilibrium value of reputation, and may make the firm better off even if the initial product quality is low. We analyze the welfare effects of policies affecting production costs, such as regulations determining severance pay, wages, or who should pay for employee training.====In the bad-news case, the reputation dynamics depend on the value of the upgrading cost. When the upgrading cost is low, the firm always produces high-quality goods to avoid the arrival of bad news. If bad news arrives off the path of play, the firm loses its reputation forever. On the contrary, when the upgrading cost is high, the firm never upgrades the product quality. In this case, building reputation is a slow process. Hence, the firm immediately lowers the product quality when its reputation is below a given cutoff. If the firm's reputation is above the cutoff, the firm keeps the product quality constant, and slowly gains reputation over time.",Reputation building through costly adjustment,https://www.sciencedirect.com/science/article/pii/S0022053119300341,May 2019,2019,Research Article,266.0
"Lahkar Ratul,Mukherjee Saptarshi","Economics Area, Indian Institute of Management Udaipur, Udaipur, Rajasthan, 313001, India,Department of Humanities and Social Sciences, Indian Institute of Technology Delhi, Hauz Khas, New Delhi, India","Received 28 June 2018, Revised 14 March 2019, Accepted 23 March 2019, Available online 27 March 2019, Version of Record 2 April 2019.",https://doi.org/10.1016/j.jet.2019.03.009,Cited by (23),"We consider implementation of the efficient state in a large population public goods game. Due to positive externalities, the efficient state is different from the ","Implementing the efficient outcome in the problem of public goods provision has remained an important area of research. The most well known solution to this problem is Pigouvian taxation (Pigou (1920)). In an environment where agents are of multiple types, the imposition of such taxes may be complicated by the social planner's lack of information about agents' types. The classical Vickrey–Clarke–Groves (VCG) mechanism provides a solution to this problem (Vickrey (1961), Clarke (1971), Groves (1973)). A VCG mechanism is a direct mechanism that renders truthful revelation of type the dominant strategy for each agent. The revelation of types is important in VCG mechanisms because the transfer scheme that internalizes the externality created by each agent in such mechanisms depends upon the type of the agent.====We consider the implementation of the efficient state in a large population public goods game. In principle, a VCG mechanism can be applied even in this setting. However, as noted by several authors, there may be practical difficulties in applying such a mechanism, particularly when the number of agents are large. For example, Sandholm (2005) remarks on the difficulty the planner may face in administering any standard mechanism including VCG mechanism–it requires collecting information about types from a very large number of agents and computing assignments based on the reported types. Phelps et al. (2010) note the potential cost of inducing type revelation in the context of real world electronic exchanges with a large number of participants. Rothkopf et al. (1990) and Rothkopf (2007) point out other practical problems with VCG mechanisms such as concern about revealing confidential information about types, possibility of cheating by the bid taker and competing bidders etc. Hyafil and Boutilier (2007) discuss the difficulty of eliciting type information with more complex outcome spaces. Indeed, in view of such problems that may arise when direct mechanisms are applied, Phelps et al. (2010) argue that it may be more realistic to view the emergence of optimal strategies in a mechanism design problem as a gradual evolutionary process instead of instantaneous coordination on the efficient solution.====We approach the implementation problem in the large population public goods game from a similar perspective. Hence, instead of a VCG mechanism, we apply the method of evolutionary implementation introduced by Sandholm, 2002, Sandholm, 2005, Sandholm, 2007 to arrive at efficient outcomes in large population settings. As in a VCG mechanism, evolutionary implementation will involve the planner designing a transfer mechanism designed to internalize externalities. Unlike the VCG mechanism, this mechanism will not rely on revelation of types. Instead, under this mechanism, the behavior of agents will evolve towards the efficient state of the public goods game. We will also address one point of criticism about the classical VCG mechanism. It is that the VCG mechanism lacks budget balance (Moulin (2009)). We will show that this is not a serious problem in our mechanism.====We consider a society of agents who may be of multiple types (or populations), each type characterized by a specific payoff function in the public goods game. All types have a common strategy set, which we assume is continuous. The benefit to each agent is a function of the aggregate strategy level while the cost is a function of individual strategy.==== Since the payoff to an agent depends upon the aggregate strategy level and his own strategy, the public goods game satisfies the definition of an aggregative game (Corchón (1994)). The Nash equilibrium of this game is a state that involves each agent playing the lowest possible strategy. Our interest is in implementing the efficient state of the game, which is the state that maximizes aggregate payoff in the society.====The difference between the Nash equilibrium and the efficient state arises because agents do not take into consideration the positive externality they generate in the public goods game. Hence, in order to implement the efficient state, we allow a planner to introduce a variable externality price scheme (Sandholm (2002)). This price scheme is a transfer that varies according to the social state and is added to the payoffs in the original game. Since the public goods game is one of positive externalities, the transfer takes the form of a subsidy which equals the externality generated by agents. We call such a subsidy that equals externality a pure subsidy. The transfer scheme constitutes the mechanism under which we will seek implementation of the efficient state.====The idea of adding a transfer to payoffs to internalize externalities is, of course, key to the classical VCG mechanism. There is, however, one crucial difference between our transfer scheme and that in a VCG mechanism. In a VCG mechanism, the transfer to an agent depends upon the type of the agent due to which, such mechanisms rely on the planner inducing truthful revelation of types. In contrast, the externality generated and, therefore, the subsidy granted in our model, is independent of the type of the agent. Therefore, issues like truthful revelation of type and, hence, the revelation principle, are not relevant in our analysis. Another difference from a VCG mechanism is that we do not need quasi–linearity of payoffs.====Sandholm (2002) shows that a population game adjusted for externalities through a variable externality price scheme is a potential game (Monderer and Shapley (1996), Sandholm (2001)). Extending this methodology to our continuous strategy setting, we show that the externality adjusted public goods game has a unique Nash equilibrium, which is the efficient state of the original public goods game. Thus, as with a VCG mechanism, this particular transfer mechanism also succeeds in implementing the efficient state in equilibrium, but without relying on revelation of type.====In evolutionary implementation, however, players would not coordinate on the efficient state immediately. Instead, once the planner has introduced the transfer mechanism, agents should converge, preferably globally, to the efficient state under plausible evolutionary dynamics. The fact that the externality adjusted public goods game is a potential game ensures that this is indeed the case. We first establish this result for the best response (BR) dynamic. As we discuss further later in this section, this dynamic has so far not been analyzed in the continuous–strategy setting. However, we are able to introduce this dynamic for aggregative games such as ours which have a continuous strategy set and which generate a unique best response at every social state. We then use existing results to extend our conclusion to the other well known evolutionary dynamics that have been studied for games with continuous strategy sets.====However, just as with the VCG mechanism, our transfer mechanism also suffers from lack of budget balance. In fact, because the transfer is a subsidy, the planner faces a budget deficit. This is arguably a more serious problem that a budget surplus because it requires the planner to infuse resources from outside the society to implement the transfer.==== But a simple modification of the pure subsidy scheme that subtracts the total subsidy at the efficient state restores budget balance at that state and also evolutionarily implements efficiency. Further, we also identify conditions such that in the transition from the inefficient Nash equilibrium to the efficient state, the modified transfer mechanism always creates a budget surplus for the planner. Thus, even though the modified mechanism does not ensure budget balance in the transition, it at least prevents the more serious problem of a budget deficit. Moreover, even the surplus is transitory. It goes to zero as the society converges to the efficient state. In this sense, the issue of budget balance is not a serious problem in our approach.====The fundamental idea of adding a variable externality price scheme and analyzing the resulting game as a potential game derives from the existing literature on evolutionary implementation (Sandholm, 2002, Sandholm, 2005, Sandholm, 2007).==== There are, however, significant differences between these papers and the present paper. First, the application to a public goods game is new. Sandholm's papers mostly focus on congestion games. The public goods game is one of the most widely studied problem in mechanism design (see, for example, Groves (1973), Groves and Ledyard (1977), Falkinger (1996)). Hence, applying evolutionary implementation to this game is a worthwhile exercise and provides certain interesting insights. For example, we learn that unlike in the classical VCG mechanism, the transfer to an agent does not depend on the type of the agent in our model. A related technical difference between our model and Sandholm's models is in the nature of payoff functions considered. Sandholm's payoff functions are of a specific form; they are decomposable into a common part and an idiosyncratic part with externalities arising only from the common part. Our payoff function is not so decomposable and is, hence, more general in this particular sense. We do need to assume that the planner is able to calculate externalities to implement the transfer mechanism. This may be done either by assuming that the planner knows the type distribution, an assumption that is standard in game theory (Harsanyi (1968)), or by allowing the planner to use well–known empirical methods to calculate such externalities. We discuss this point further in Section 4.====Second, our analysis is in the setting of a continuous strategy set while Sandholm, 2002, Sandholm, 2005, Sandholm, 2007 considers finite strategy models. Our choice of setting is motivated by the public goods model in which the number of strategies is intrinsically large. Hence, as we elaborate in Section 4.1, it is computationally simpler to characterize efficiency and equilibrium by directly using a continuous strategy set. Indeed, starting with Oechssler and Riedel, 2001, Oechssler and Riedel, 2002, this computational simplicity has been the main reason why the literature on evolutionary game theory has been generalized to a continuous–strategy setting to model games with a large strategy set. This includes important economic applications like oligopoly games, bargaining games and public goods games. Our approach contributes to this literature by extending the theory of evolutionary implementation to games with continuous strategy sets. As far as we know, this is the first paper that analyzes a game generated by a variable externality price scheme as a potential game in the continuous–strategy framework.====Our approach does raise certain technical complications like defining externalities and potential games in a measure–theoretic format.==== We could have avoided these complications by considering a finite–strategy approximation of the public goods game and applying Sandholm, 2002, Sandholm, 2005 methodology to the problem. Indeed, had we simply wished to illustrate the possibility of evolutionarily implementing efficiency in the public goods problem, this approach would have sufficed. But we also seek a tractable way of characterizing efficiency, which is much easier in the continuous–strategy approach. In our view, this makes it worthwhile bearing the measure–theoretic complications in the set–up of the model. Nor does the continuous–strategy approach sacrifice realism. The solution it provides is a good approximation of the finite–strategy solution when the number of strategies is large.====Besides extending the idea of evolutionary implementation, we also contribute to the literature on continuous–strategy evolutionary dynamics through our analysis of the best response (BR) dynamic. As mentioned in footnote 2, a number of evolutionary dynamics have by now been extended from finite–strategy to continuous–strategy games. The BR dynamic has been an exception. This dynamic is well known for finite–strategy games (Gilboa and Matsui (1991), Hofbauer (1995)) but has so far not been analyzed in the continuous–strategy case. The main difficulty is that in such games, the best response may not even exist at some states, which makes a general definition of this dynamic in this context unfeasible. However, in the aggregative games we consider, there is a unique best response at every state. The public goods problem, therefore, presents an interesting application to introduce this dynamic for the class of continuous–strategy aggregative games that generate a unique best response at every social state. Due to the novelty of this dynamic, we present our evolutionary implementation result in some detail for this dynamic. We have already stated that our result extends to the other important evolutionary dynamics mentioned in footnote 2. Our analysis of the BR dynamic highlights the point that evolutionary implementation in our model also holds for the canonical behavioral norm of best response that these other dynamics do not account for.====The earlier literature on evolutionary implementation has not addressed the issue of budget balance. One reason could be that this literature has mostly focused on models with negative externalities (Sandholm, 2002, Sandholm, 2005). In such models, the transfer mechanism is a tax which leads to a budget surplus, which is a less serious problem than the one of budget deficit in our model of positive externality. Our solution to the problem is parsimonious in the sense that it simply involves adjusting the pure subsidy scheme by a suitable lump sum amount. The fact that the problem of budget deficit, which is a serious complication in the classical VCG mechanism, can be resolved so conveniently is, in our view, a strength of the evolutionary implementation approach to the public goods game.====To summarize, the main contribution of this paper is to show the evolutionary implementation of efficiency in the public goods game under fairly general conditions. In showing this, we extend the method of evolutionary implementation to the continuous strategy case and introduce the BR dynamic for aggregative games with a continuous strategy set. We also provide a solution to the budget deficit problem that the classical VCG mechanism faces in the public goods model. It is also noteworthy that despite externalities being positive, we are able to use a deterministic dynamic like the BR dynamic to globally implement the efficient state in our model. This may not be possible under all models of positive externality, which would then require the use of stochastic evolutionary game theory (Sandholm (2007)). This is of importance because convergence under the deterministic approach is much faster than under the stochastic approach.==== We discuss this and the related issue of the concavity of the potential function in Sections 4.1 and 5.2.====Approaches that, like evolutionary implementation, rely on dynamic convergence to the optimal solution have also been applied in the artificial intelligence literature on market design with a large number of agents. For example, Luo et al. (2016) propose an all–pay auction in a crowdsourcing model. Unlike a standard auction where the reward to a bidder is fixed ex ante, the reward in this model is adaptive. It is updated over time as a function of the winner's contribution. Like our model, this model also does not require agents to report type. Instead, winner selection is based on contributions. Balkan et al. (2005) apply an algorithmic approach based on machine learning theory to design revenue-maximizing incentive-compatible mechanisms. There is also a literature on “reinforcement mechanism design”, in which an artificial intelligence seeks to maximize revenue through reinforcement learning. Shen et al. (2017), for example, analyze such a learning based mechanism to maximize revenue by selling online advertisement through sponsored search auctions. Tang (2017) discusses an algorithmic framework where a mechanism designer dynamically improves the mechanism using the data generated in the process of running the algorithm. Cai et al. (2018) apply reinforcement learning to tackle the problem of sellers faking historical transactions to receive favorable treatment from e–commerce websites. Phelps et al. (2010) provide a review of such evolutionary and learning approaches to the engineering of market mechanisms. We note that apart from broad similarity of convergence instead of instantaneous coordination on the optimal outcome, our model is completely different from this engineering literature on evolutionary mechanism design. For example, most of these papers are based on experiments and algorithms. In contrast, ours is based on the analytical method of potential games.====The rest of the paper is as follows. Section 2 defines the public goods game. Section 3 characterizes Nash equilibrium, efficient state and externalities in this game. In Section 4, we introduce the pure transfer mechanism and show that it implements the efficient state in equilibrium. The analysis in this section, which is central to the paper, is based on the theory of potential games. Section 5 introduces the BR dynamic for aggregative games, establishes evolutionary implementation under this dynamic and extends the result to other dynamics. Section 6 considers budget balance. Section 7 concludes. Most proofs are in the Appendix.",Evolutionary implementation in a public goods game,https://www.sciencedirect.com/science/article/pii/S0022053118303181,May 2019,2019,Research Article,267.0
Bezin Emeline,"Paris School of Economics, France,Centre National de la Recherche Scientifique (CNRS), France","Received 29 May 2017, Revised 6 March 2019, Accepted 8 March 2019, Available online 21 March 2019, Version of Record 2 April 2019.",https://doi.org/10.1016/j.jet.2019.03.005,Cited by (14),"A model which formalizes the interplay between green consumer culture and sustainable technology is used to revisit the trade-off between economic growth and environmental preservation. The theory includes (i) green preferences formed through cultural transmission which involves rational socialization actions, (ii) innovation endogenously directed to sustainable or unsustainable sectors depending on culture through market size effects. The model captures an important feature of sustainable innovation processes which is the existence of path dependency. The approach allows to examine implications for both market-based instruments (i.e., environmental taxes) and non-monetary interventions (i.e., environmental education). The two types of policies are either complements or substitutes depending on the substitutability between clean and dirty goods. Finally, an important disregarded issue is examined: the political sustainability of environmental taxes.","The attainment of serious environmental objectives along with sustained economic development has become a major challenge of current policymakers. Economists have emphasized that the long-term environmental impact of economic activity is profoundly affected by the rate and direction of technological change (see Nordhaus, 2002, Popp, 2004, Acemoglu et al., 2012). Another presumably significant channel in the trade-off between economic growth and environmental preservation is the change in preferences. For instance, some studies suggest that a shift in dietary preferences could allow to achieve significant greenhouses gases emissions reductions.==== Survey research on national samples has provided evidence of an aggregate pro-environmental attitudes change since the early 70's (see Kanagy et al., 1994, Inglehart, 2008, Capstick et al., 2015). Moreover, recent experiments with environmental policies suggest that environmental preferences change in response to both economic incentives and non-monetary instruments (see D'Haultfœuille et al., 2016, Thaler and Sustein, 2008, Ferraro and Price, 2013).====There is an increasing need to investigate how the response of preferences to economic changes and policies alter predictions about the cost of environmental preservation as well as to examine the potential for non-monetary instruments in reducing the environmental impact of economic growth. This paper contributes to this research agenda by developing a theory which formalizes dynamic interactions between innovation in sustainable technologies, changes in “green consumer culture” and pollution accumulation.====Empirical evidence indicates that sustainable innovation is sensitive to changing consumers' attitudes. For instance a European poll reveals that 88% of firms surveyed mention increasing market demand for green products as an important driver of innovation (Flash Eurobarometer 315, 2011). Furthermore, Popp et al. (2011) use patents data to examine the determinants of the dramatic rise in chlorine free paper technologies which occurred during the 1990s. Their study indicates that innovation was mostly due to changes in consumers concern over chlorine in paper.====In addition, there is evidence that preferences respond to technological change (Alesina et al., 2013, Talhelm et al., 2014, Galor and Özak, 2016). For instance, Galor and Özak exploit a natural experiment associated with the expansion of suitable crops for cultivation during the Columbian Exchange. They find that pre-industrial agro-climatic characteristics associated with higher crop yield, have had a persistent effect on the distribution of time preference across societies. Other works more precisely suggest that changes in production technology and prices significantly alter preferences for green consumption. Teisl et al. (2002) provide market-based evidence that the US dolphin-safe label has shifted consumers valuation of canned tuna. Moreover, D'Haultfœuille et al. (2016) examine the impact of the French bonus-malus on consumers valuation of vehicle CO2 emissions.==== Their results show that the policy triggered a substantial change in preferences towards low-emitting cars (it accounts for 40% of the overall decrease in average CO2 emissions of new cars in the period considered).====The interplay between green culture and sustainable technologies is also consistent with cross-country analysis revealing a positive correlation between green consumers' attitudes and clean technologies. Fig. 1 displays a scatterplot illustrating the cross-country relationship between the share of people who embed an environmental dimension in their consumption decisions and the development of organic farming (on the left) and another scatterplot revealing the link between the fraction of people who agree with the idea of buying goods for environmental reasons and the share of EU firms who had introduced one eco-innovative product or service during the last two years (on the right). Both graphs highlight a positive correlation between green consumers attitudes and sustainable technologies. The correlation is significant and robust to the inclusion of several control variables (see Appendix A).====This paper proposes a theory which formalizes the interplay between the formation of green consumer preferences, the direction of technological change and the accumulation of pollution. The theoretical set up builds on the three following blocks. First, some agents attach higher value to the consumption of non-polluting goods (i.e., goods which are less detrimental to environmental quality). This assumption is supported by various empirical evidence. For instance, a European survey reveals that 8 over 10 EU citizens felt that a product's impact on the environment is a critical element when deciding which product to buy (Flash Eurobarometer 256, 2009). Studies on actual behaviors using market data confirm the existence of a willingness to pay for “cleaner” products (see Teisl et al., 2002, for the willingness to pay for dolphin-safe labeled canned tuna, Bjørner et al., 2004 for the willingness to pay for the Swan-labeled paper). In particular, Bjørner et al. find that the willingness to pay for the Swan-labeled paper of Danish consumers ranges between 13% and 18% of the price.====Second, consumers' preferences are transmitted intergenerationally through role modeling and family socialization actions following the lines of Bisin and Verdier (2001). Works on environmentally friendly attitudes suggest that these attitudes are acquired during childhood (see Inglehart, 1995, Inglehart and Baker, 2000) and that family and peers matter in the socialization process (see Chawla, 1998, Villacorta et al., 2003, Litina et al., 2016). In particular, Litina et al. base on the epidemiological approach proposed by Fernandez (2007) using variation associated with international migration flows. They find that the environmental preferences of migrants within Europe are significantly affected by environmental preferences in the origin country suggesting a transmission within family. Also, some sociological studies have revealed the existence of socialization actions (see Cairns et al., 2013, for socialization to organic food consumption practices, Maurer, 2010, Boyle, 2011 for socialization to vegetarianism).====Finally, I assume that the direction of technological change, which depends on decisions by profit motivated agents, is influenced by green consumer culture through market size effects. This assumption is supported by evidence already detailed above such as the study by Popp et al. (2011).====The key insight is that innovation in clean sectors and green cultural change are co-determined. A first important result is that the dynamics generally exhibits path dependency. In this case, the economy can converge to two distinct long-term outcomes: a ==== equilibrium where both the fraction of green consumers and the relative productivity in the clean sector are high and a ==== equilibrium where the fraction of green consumers is low and dirty technologies prevail. Path dependency in eco-innovation is in line with empirical evidence (see Newell et al., 1999, Popp, 2002, Aghion et al., 2016). Compared to the theoretical literature (e.g., Acemoglu et al., 2012), the present result holds true (i) whatever the value of the elasticity of substitution between clean and dirty goods and in particular when this elasticity is low, (ii) when there is decreasing returns at the firm level.==== Hence, the theory proposed in this paper provides an alternative and complementary explanation for path dependency in eco-innovation.====The model involves new forces to complementarity in the dynamics of innovation. These forces arise from the cross effects which capture interactions between green culture and environmentally friendly technology. The first complementarity force is the ====. The higher the fraction of green agents, the higher aggregate consumption of clean goods and the higher the incentives to innovate in the clean sector. The second force is the ====. When productivity in the clean sector rises, clean goods become cheaper which makes green children better-off and increases incentives for altruistic parents to educate their child to green preferences. The strength of these cross effects reduces with the cost of green parenting and increases with the willingness to pay for clean goods, the elasticity of production to technological change and knowledge spillovers. Whenever both complementarity forces are sufficiently high, the dynamics exhibits multiple history-dependent equilibria.====In this set-up, some public intervention is welfare-improving since at the ====, the current generation does not internalize the negative impact of its own consumption choices on future generations through the reduction in environmental quality.==== I examine different types of public intervention and study how classical economic instruments (here pollution taxes) interact with alternative non-monetary policies (i.e., environmental education which is comprehended as a decrease in the cost of green parenting). I show that pollution taxes and the non-monetary policy are either substitutes or complements depending on the elasticity of substitution between clean and dirty goods. Pollution taxes correct the market failure by giving a price to the negative production externality while environmental education promotes the green culture consumption. When the substitutability between clean and dirty goods is high, the cultural change due to environmental education entails a substantial reduction in dirty goods consumption. The resulting decrease in the size of dirty goods market implies a drop in profits in the dirty sector which redirects all innovation toward the clean sector. Hence, environmental education allows to stop the rise of dirty production. In such a case, both environmental education and pollution taxes allow to correct the market failure due to dirty goods production growth: they are ====. When the substitutability between clean and dirty goods is low, whatever the cultural change, the resulting decrease in consumption of dirty goods is limited. Profits in the dirty sector remain sufficiently high to encourage some innovation in the dirty sector. Dirty goods production grows and so does pollution. Only pollution taxes allow to regulate the growth of emissions. However, environmental education plays a key role. Because the substitutability between clean and dirty goods is low, the tax has a limited impact on the market of dirty goods. As a result, this tax must be initially high and continuously increasing over time (this result is in line with the literature, e.g., Acemoglu et al., 2012, Popp, 2004). Environmental education causes a qualitative change in the dynamics and promotes the rise of the green culture consumption. This major cultural change counteracts the market rigidity to economic policies. As long as the share of green consumers rises, dirty goods consumption decreases so that pollution emissions are reduced and the tax can be decreasing. Hence, environmental education modifies the shape of the efficient tax rate: it implies a substantial reduction in the cost of the market-based instrument. Pollution taxes and environmental education are ====.====Finally, as a serious impediment to market-based instruments is their political acceptability, I study the political sustainability of environmental taxes. The question I ask is as follows: in which conditions will market-based instruments be politically feasible in the long-run? An important outcome is that non-monetary policies may be critical for the political sustainability of economic instruments. When the tax is implemented in the short-run, it positively impacts incentives to educate children to green preferences and incentives to innovate in the clean sector. Whether the tax is sufficient to change the long-run equilibrium depends on the cost of green parenting which is affected by non-monetary policies such as environmental education. When the cost of green parenting is high, the impact of the tax on green parenting is weak and so is the market size effect of cultural change. In this case, the tax is not sufficient to change the long-run equilibrium of the economy which converges to the brown equilibrium where the fraction of green agents is too low to support the economic policy. Environmental education, which reduces the cost of green parenting, strengthens the positive impact of the tax on green culture which in turn increases the market size effect of cultural change. Whenever the tax is implemented in the short-run, then it is sufficient to shift the long-run outcome. The fraction of green agents converges to the green equilibrium in which the tax is supported by the majority.====  This paper relates to four lines of work.====First, several theoretical papers investigate the interplay between specific cultural traits and some technology and show how the mechanism at play gives rise to path dependency and persistence (Bénabou et al., 2016, Galor and Özak, 2016, Maystre et al., 2014). For instance, Bénabou et al. analyze the co-evolution of religious beliefs and innovation in a framework where religious beliefs affect innovation through the political economy channel (i.e., a government which controls technological change) and where beliefs are eroded by innovation but can also be influenced by a religious institution. They show that the tension between scientific knowledge and religion gives rise to three distinct long-term outcomes regarding religiosity and scientific progress.==== A more closely related paper is that of Maystre et al. (2014) who build a framework interacting the cultural transmission of preferences for some differentiated consumption goods (in particular they focus on cultural goods such as movies and books) and the supply of these goods to analyze the impact of trade integration. Apart from the fact that they do not consider any negative environmental externality and implications for pollution accumulation, their framework differs from the one proposed in the present paper in several important ways. First, in their paper, any change on the production side is captured by increasing product variety. This modeling feature makes this framework unsuitable to discuss the role of the elasticity of substitution between two consumption goods (here clean and dirty ones) for economic outcomes. It also prevents the study of the impact of a tax on final good consumption such as the tax on polluting goods. Another important distinction is that, in their model, technology is not a state variable implying that there is no economic growth which is the focus of the present paper.====Second, the paper also contributes to the literature on growth and the environment with directed technological change (Smulders and De Nooij, 2003, Acemoglu et al., 2012). A major difference with the models proposed so far is that preferences are endogenous. This is a crucial feature as (i) I show that it can be the source of path dependency in sustainable innovation processes, (ii) it provides novel implications for public policies.====A third theoretical literature has accounted for endogenous environmental preferences when studying long-run environmental issues (Bezin, 2015, Schumacher, 2015). These works do not include endogenous technological progress. They neither study implications for sustainable innovation processes nor examine the trade-off between long-run economic growth and environmental preservation.====Fourth, the paper relates to a smaller literature analyzing the political sustainability of public good provision policy. Cremer et al. (2004) emphasize the critical role of the refunding rule (i.e., the allocation of tax proceeds) for political support toward tax on polluting goods. Second, Bisin and Verdier (2000) examine the sustainability of a public good provision policy when preferences for that good are transmitted through socialization actions and role modeling. They highlight the importance of the initial distribution of preferences for the political sustainability of the policy. In their paper, this result is due to self-fulfilling expectations. In the present framework, the result lies on the existence of dynamic complementarities between culture and technology.====The rest of the paper unfolds as follows. Section 2 presents the model. Section 3 is devoted to the analysis of the dynamics and to the characterization of steady state equilibria. Section 4 examines efficient and sustainable environmental policies. In Section 5, I propose alternative modeling assumptions (one of which is developed in Online Appendix D). Section 6 focuses on the political sustainability of environmental taxes. Section 7 concludes. Appendix A provides more details about the cross-country correlation between attitudes and sustainable production methods. Appendix B contains the proofs of the key results, while Appendix C, which is available online, contains the remaining proofs.","The economics of green consumption, cultural transmission and sustainable technological change",https://www.sciencedirect.com/science/article/pii/S0022053119300298,May 2019,2019,Research Article,268.0
"Dew-Becker Ian,Nathanson Charles G.","Northwestern University, United States of America,NBER, United States of America","Received 5 March 2018, Revised 1 March 2019, Accepted 7 March 2019, Available online 19 March 2019, Version of Record 2 April 2019.",https://doi.org/10.1016/j.jet.2019.03.004,Cited by (4),"This paper examines the implications of learning for the effects of ambiguity aversion. The key result is that since agents naturally choose to learn about the sources of uncertainty that reduce utility the most, information acquisition attenuates the most severe effects of ambiguity aversion. The specific setting we study is the canonical consumption/savings problem. Agents endogenously learn most about income dynamics at the very lowest frequencies. While ambiguity aversion typically implies in this setting excessive extrapolation of income shocks, that effect is eliminated here. Furthermore, deviations of consumption from the full-information benchmark are largest at high frequencies, so the model naturally generates overreaction of consumption to predictable short-run income variation.",None,Directed attention and nonparametric learning,https://www.sciencedirect.com/science/article/pii/S0022053119300286,May 2019,2019,Research Article,269.0
Schottmüller Christoph,"University of Cologne, Germany","Received 12 September 2016, Revised 25 February 2019, Accepted 11 March 2019, Available online 18 March 2019, Version of Record 20 March 2019.",https://doi.org/10.1016/j.jet.2019.03.006,Cited by (3),"A decision maker repeatedly asks an adviser for advice. The adviser is either competent or incompetent and knows his type privately. His preferences are not perfectly aligned with the decision maker's preferences. Over time, the decision maker learns about the adviser's type and will fire him if the adviser is likely to be incompetent. If the adviser's reputation for competence improves, he is less likely to be fired for incompetence but this makes pushing his own agenda more attractive to him. Consequently, very competent advisers are also fired with positive probability because they are tempted to pursue their own goals. The quality of advice can be highest if the adviser's competence is uncertain.","As specialization is one of the cornerstones of the modern knowledge society, it is not surprising that advice provided by specialized experts is important in so many domains of life. Savers have financial advisers to help them manage their wealth, consumers turn to sales personnel to make good choices, politicians and managers depend on their advisers to find the right policies, patients need their physicians' advice and internet users rely on search engines.====In most instances, the adviser's incentives are not necessarily aligned with the advice seeker's preferences. Financial advisers, sales personnel and search engine operators can win bonuses if their customers buy certain products, while politicians and managers might wonder whether their advisers have an agenda of their own and patients might be worried that their physician's enthusiasm for a certain drug stems from successful lobbying efforts on the part of its producer. Even ex post, it is hard to detect whether such concerns are justified, because advice in these contexts is complex and even the best possible advice can turn out to be wrong once in a while.====Another common feature of these examples is the repeated nature of the advice. Most people tend to receive advice from the same adviser several times and switch advisers only from time to time. It clearly makes sense to switch advisers if the advice seeker concludes that his adviser is not competent. However, long term advisers who are viewed as very competent are also occasionally fired. For example, in 2003, financial analyst Jack Grubman was banned by the Security and Exchange Commission from the financial industry for life and fined $15 million for misconduct. Grubman had used his good reputation to pursue his personal goals when he gave a public buy recommendation for AT&T as part of a complex plan to enable his children to be admitted to the prestigious 92nd Street YM-YWHA's preschool program (as he explained in a private email that later went public).==== By the time the ban was announced, market participants had, of course, already stopped listening to Grubman's advice but this reaction was not a response to perceived incompetence. When Grubman was hired by Distinctive Devices as consultant a year later, the company's stock price went up. The problem was that Grubman had apparently (ab-)used his good reputation: he had misrepresented his information and thereby had manipulated his followers for his own personal benefit.====History provides an ample supply of examples of kings who dismissed or even killed their most prominent advisers when those advisers were too competent and perceived as a threat to the throne. Most infamous in this respect was the Ottoman Sultan Suleiman the Magnificent, who killed not only his Grand Vizier and childhood friend Pargali Ibrahim Pasha but also his own son and designated heir Mustafa for this reason (after a successful military campaign on his father's behalf during which Mustafa committed the mistake of not stopping his soldiers from referring to him as “Sultan”).====In these examples, a competent adviser was mistrusted and fired after engaging in activities that made the decision maker doubt whether the adviser acted in the decision maker's best interest or whether he was instead (ab-)using his power to push his own agenda. This paper argues that instances such as these are typical. More specifically, advisers can be fired not ==== having a reputation for being competent but ==== of it. That is, they might have given better advice and kept their positions if their competence had been in doubt.====What is the logic behind this result? In this paper, I consider a setting in which the competence of an adviser is not perfectly known by the decision maker. An adviser whose competence is in doubt faces the danger of being dismissed for incompetence if his advice turns out to be bad (thus reinforcing the decision maker's initial doubts). Therefore, the adviser has a strong incentive to act in the decision maker's best interest in order to keep his position. An adviser who is believed – with high probability – to be competent, however, has more latitude because the risk of him being fired ==== in the near future is negligible. In other words, even if his advice turns out to be poor a few times, this is not immediately a sign of incompetence as it could simply be due to bad luck. The adviser is therefore free to pursue his own goals, which are usually not in line with the decision maker's goals. But in this case, the best response of the decision maker is to fire the adviser whose advice serves principally his own interests rather than those of the decision maker.====A competent adviser can therefore lose his job for two distinct reasons. If the belief that the adviser is competent is too low, the decision maker will fire the adviser because the information that the adviser is competent is hidden. If the belief that the decision maker is competent is high, the decision maker will fire the adviser not because of hidden information but because of moral hazard: the adviser does not act in the interests of the decision maker but pushes his own agenda.====The model is a repeated game in which the adviser recommends one of two options to the decision maker in every period until the decision maker ends the advice relationship. One of the available options fits the decision maker's needs and one fits the adviser's needs – e.g. he receives a bonus when recommending that option. The two might accidentally coincide from time to time but often they do not. The decision maker has a uniform prior concerning which option fits his needs and also concerning which option fits the adviser's needs. The adviser receives a noisy signal indicating which option fits the decision maker's needs and he knows perfectly which option will give him a bonus. The decision maker discovers whether the recommended option has fitted his needs only after he has followed the recommendation. The adviser has one of two types: either he is competent – i.e. his noisy signal is informative – or not.====In this model, no meaningful advice can be obtained in a static setting because the adviser will always recommend his bonus option if he does not face the threat of losing future bonus payments. The same is true in a finitely repeated game: as in the static setting, the adviser is unable to give meaningful advice in the last period and as a consequence, he will always be fired before the last period. Given this, the adviser is unable to give meaningful advice in the second to last period and the game unravels, meaning that the adviser is never consulted in equilibrium. Some informative advice is, however, possible in an infinitely repeated game setting. Unsurprisingly, the adviser is fired for sure if the decision maker's belief that the adviser is competent is very low. If this belief is sufficiently high, then the adviser is also fired with positive probability – at least in the event that he recommends an option that does not fit the decision maker's needs. For these high beliefs, equilibrium strategies are usually mixed. The decision maker is indifferent between firing and retaining the adviser and the threat of firing is just high enough to ensure that the adviser finds a strategy optimal that keeps the decision maker indifferent between these two options.====The expected length of the game, i.e. the number of periods before the adviser is fired, is uniformly bounded from above for any belief; that is, the bound is independent of the decision maker's belief regarding the adviser's competence. This illustrates that even an arbitrarily competent adviser will almost surely be fired within a finite amount of time. These results hold for all equilibria of the game, i.e. they are not affected by multiplicity of equilibria. The adviser suffers in many equilibria from a severe commitment problem: if he was able to commit to truthfully revealing his signal in every period, then he and the decision maker would both obtain strictly higher payoffs than in equilibrium.====The outline of the paper is as follows. The next section introduces the model. Section 3 presents the results holding in all perfect Bayesian equilibria and in Markov equilibria in particular (section 3.1). Section 3.2 discusses several generalizations that are pursued in greater detail in the supplementary material to this paper. The model is extended to allow for monetary transfers and competition among several advisers in section 4. Section 5 discusses the related literature and section 6 concludes. Proofs as well as some technical discussions are relegated to the appendix.",Too good to be truthful: Why competent advisers are fired,https://www.sciencedirect.com/science/article/pii/S0022053119300304,May 2019,2019,Research Article,270.0
"Gabrovski Miroslav,Ortego-Marti Victor","Department of Economics, University of Hawaii Manoa, 2424 Maile Way, Saunders Hall 516, Honolulu, HI 96822, USA,Department of Economics, University of California Riverside, Sproul Hall 3132, Riverside, CA 92521, USA","Received 6 March 2018, Revised 1 February 2019, Accepted 7 March 2019, Available online 14 March 2019, Version of Record 20 March 2019.",https://doi.org/10.1016/j.jet.2019.03.003,Cited by (9),"This paper develops a business cycle model of the housing market with search frictions and entry of both buyers and sellers. The housing market exhibits a well-established cyclical component, which features three stylized facts: prices move in the same direction as sales and the number of houses for sale, but opposite to the time it takes to sell a house. These stylized facts imply that in the data housing vacancies and the number of buyers are positively correlated, i.e. that the Beveridge Curve is upward sloping. A baseline search and matching model of the housing market is unable to match these stylized facts because it inherently generates a downward sloping Beveridge Curve. With free entry of both buyers and sellers, our model reproduces the positive correlation between prices, sales and vacancies, and matches the stylized facts qualitatively and quantitatively.","One salient feature of the housing market is that, similar to the labor and marriage markets, it is subject to search frictions. It takes time for households to find a suitable house and for sellers to find a buyer for their vacancy. Both sellers and buyers must spend costly search effort before they find a trading partner and a transaction takes place. The housing market also has well-established business cycle fluctuations. The observed large volatility in the time-to-sell is a clear indication that search frictions are a better description of the housing market than the Walrasian auctioneer paradigm.====This paper studies a business cycle model of the housing market with search frictions and entry of both buyers and sellers to explain housing market dynamics. We focus on the following stylized facts, which are based on the empirical findings in Diaz and Jerez (2013). First, house prices are positively correlated with both sales and vacancies, and negatively correlated with time-to-sell. Second, sales, time-to-sell and vacancies are much more volatile than prices in the data. More specifically, the stylized facts are: (1) the elasticity of prices with respect to sales is 0.14; (2) the elasticity of prices with respect to time-to-sell is −0.12; (3) the elasticity of prices with respect to vacancies is 0.06.====We focus on these stylized facts because, combined, these elasticities determine the dynamics of vacancies, buyers, prices and market tightness—where market tightness is defined as the ratio of buyers to vacancies. These are the key variables in a search model of the housing market and are the equivalent of job vacancies, unemployment, market tightness and wages in the labor market, which are the central variables in search models of the labor market, see for example Pissarides (2000).====We show that the stylized facts imply that buyers and vacancies are positively correlated.==== In other words, the empirical Beveridge Curve between buyers and vacancies is upward sloping in the housing market. By contrast, any search model à la Diamond-Mortensen-Pissarides (DMP) generates a downward sloping Beveridge Curve between vacancies and buyers. When vacancies become abundant, buyers find houses more quickly and the number of buyers drops, i.e. vacancies and buyers are negatively correlated. In the labor market, the same mechanism generates a downward sloping Beveridge Curve between vacancies and unemployment, which is consistent with labor market data, see Beveridge (1944) and Pissarides (2000).====To our knowledge the literature has not noted this important stylized fact. Given that the Beveridge Curve is downward sloping in baseline search models of the housing market à la DMP, these models are unable to match the sign of the cyclical behavior of vacancies, buyers, market tightness and prices, which are the key variables in a search model of the housing market—see for example Caplin and Leahy (2011), Diaz and Jerez (2013), Ngai and Sheedy (2017) and Novy-Marx (2009).====The assumption of both endogenous buyer and seller entry is what most distinguishes our paper from the existing literature on search and matching in the housing market. When sellers post more vacancies in the market, they make it easier for buyers to find a home. This raises the returns to search and incentivizes buyers to enter the market, which increases the number of buyers. This entry mechanism leads to an upward sloping Beveridge Curve between vacancies and buyers. As a result, the model is able to match the stylized facts qualitatively. We simulate the model with fluctuations in housing construction costs and the utility of being a homeowner.==== We show that the model is a substantial improvement compared to models that feature no endogenous entry of buyers. Beyond matching the signs of the correlations between the key variables, the model matches the elasticity of prices with respect to sales and time-to-sell almost perfectly. The model also predicts an elasticity of prices with respect to vacancies that is relatively close to its empirical counterpart. Overall, the model performs well quantitatively and accounts for the stylized facts.==== The first paper to develop a model of the housing market with search frictions to study the relationship between prices and vacancy duration is the seminal work in Wheaton (1990).==== Subsequently, a number of papers have used a search and matching framework to study the housing market, which include Anenberg (2016), Burnside et al. (2016), Caplin and Leahy (2011), Diaz and Jerez (2013), Genesove and Han (2012), Head et al., 2014, Head et al., 2016, Kashiwagi, 2014a, Kashiwagi, 2014b, Krainer (2001), Ngai and Tenreyro (2014), Ngai and Sheedy, 2015, Ngai and Sheedy, 2017, Novy-Marx (2009), Piazzesi and Schneider (2009) and Smith (2015). With the exception of Diaz and Jerez (2013), these papers focus either on the steady state, predictable cycles such as hot and cold seasons, or long-run trends. Diaz and Jerez (2013) is the first paper to study a business cycle model of the housing market with search frictions.====Our paper complements this previous work in two ways. First, we establish that the data imply a positive relationship between vacancies and buyers, i.e. that the Beveridge Curve in the housing market is upward sloping. Second, we propose an endogenous mechanism that can reproduce the positive correlation between buyers and vacancies. Given that the Beveridge Curve is downward sloping in standard DMP search models, it is not possible to match the sign of the co-movement of all the key variables, namely vacancies, buyers, market tightness and prices.==== Unlike papers in this literature, we assume an endogenous entry of both buyers and vacancies in a model with business cycle fluctuations. We show that this endogenous double entry is essential to generate an upward sloping Beveridge Curve. Finally, we show that the model is able to match the stylized facts qualitatively and quantitatively.====We begin by studying the steady state to illustrate the mechanism in the model. Using comparative statics we explain why the model generates a positive correlation between buyers and vacancies, unlike baseline models. Next, we solve the model with business cycle fluctuations and simulate it. The final section reports the results and shows that the model can match the stylized facts qualitatively and quantitatively.",The cyclical behavior of the Beveridge Curve in the housing market,https://www.sciencedirect.com/science/article/pii/S0022053119300274,May 2019,2019,Research Article,271.0
Carroll Gabriel,"Department of Economics, Stanford University, 579 Serra Mall, Stanford, CA 94305, United States","Received 18 June 2017, Revised 8 February 2019, Accepted 3 March 2019, Available online 8 March 2019, Version of Record 20 March 2019.",https://doi.org/10.1016/j.jet.2019.03.001,Cited by (23),"A principal needs to make a decision, and contracts with an expert, who can obtain information relevant to the decision by exerting costly effort. The principal can incentivize effort by paying a reward based on the expert's reported information and on the true state of nature, which is revealed ex post. Both parties are financially risk-neutral, and payments are constrained by limited liability. The principal is uncertain about the expert's information acquisition technology: she knows some actions (experiments) that he can take to obtain information, but there may also be other experiments available. The principal seeks robustness to this uncertainty, and so evaluates any incentive contract using a worst-case criterion. Under quite general conditions, we show that the optimal contract is a ====, in which the expert chooses from a subset of the decisions available to the principal, and is then rewarded proportionally to the value of his designated decision in the realized state.","How should one pay for information that requires effort to produce? An extensive theoretical literature on proper scoring rules (e.g. Savage, 1971, Gneiting and Raftery, 2007) studies how one can incentivize agents to truthfully report pre-existing knowledge, such as beliefs about the probability of some event, under very general conditions. By contrast, the question of how best to give incentives to discover this information has not received correspondingly general attention. Yet there are plenty of situations where people are employed precisely to produce information: market researchers trying to forecast demand for new products; stock analysts, meteorologists, policy advisors, and so forth. We examine here the question of what shape and size of incentives are best for generating information, trading off the benefits of inducing effort against the fact that providing strong incentives can be expensive.====We adopt an agency model. A principal needs to make a decision that depends on an unknown state of nature, and so hires an expert, who can privately obtain information about the state by exerting effort. The expert has no intrinsic preferences over the decision being made, but the principal can incentivize him to exert effort by making his payment depend on how well the information he reports corresponds to the true state, which is publicly revealed ex post. We assume both parties are risk-neutral, and we impose limited liability — payments to the expert can never be less than zero.====We build on the work of Zermeño, 2011, Zermeño, 2012, who gave an extremely general formulation of the principal-expert problem, in which the expert's technology for acquiring information may take an arbitrary form. However, we depart from that work, and from most of the existing agency literature, by assuming that this technology is not common knowledge. Instead, as in this author's previous work (Carroll, 2015), we take a robust-contracting approach: The principal knows some actions (here termed ====) that the expert can perform to acquire information, but there may be other, unknown experiments available. The principal does not have a probabilistic belief about which experiments are and are not available. Rather, she wishes for a contract that can assure her a high expected net payoff without requiring any further knowledge about the available experiments. Accordingly, she evaluates incentive contracts based on their worst-case performance over all possible experiments the expert may have access to.====The above-mentioned previous work (Carroll, 2015) took this worst-case approach to a standard moral hazard problem, in which an agent's action directly produces output for the principal, and the agent can be paid based on the observed output. In that simpler setting, the concern for robustness results in linear contracts — in which the agent is paid a constant share of output — being optimal. Briefly, the intuition is as follows: the principal evaluates contracts based on the worst-case guarantee they provide on her expected payoff, and her limited knowledge provides only a worst-case guarantee on the agent's expected payoff (via the actions she knows he has available); linearity provides a tight connection between these two.====In the present problem, a linear contract could be defined as follows: the expert recommends a decision, and is rewarded with some fixed fraction of the resulting payoff. The same forces leading to linearity in the simpler model apply here too. But the problem here is more complex, and it turns out the optimal contract is a variant which we call a ====. Roughly, instead of paying the expert a constant share of the principal's own realized payoff, she allows him to choose an “investment” from a certain subset of her possible decisions, and pays him based on a fraction of the payoff that ==== have accrued (in the realized state) if she had made the decision thus designated by the expert. By excluding extreme decisions from the allowed investments, the principal can make the limited liability constraint slack, allowing her to pay less to the expert, while preserving enough of the linear connection between the expert's payoff and her own to still provide the robust guarantee.====This paper aims to make two main points. One is descriptive: we give a recipe that can be used to write contracts in a particular agency setting, and the resulting contracts are optimally robust in a precise sense. As with most economic models, the model is not meant to be taken completely literally, but rather to deliver qualitative insights — the robustness of linear incentives, together with the benefits of excluding extreme decisions from the payment scheme — that seem relevant more generally. (The conclusion contains some further discussion on interpretation of the model.)====The other, broader point is methodological: we show how using a maxmin objective leads to a tidy and tractable model. By contrast, a traditional Bayesian approach, where the principal knows the expert's information acquisition technology (or has a probabilistic belief about it), is unlikely to be tractable without much more specific functional form assumptions, e.g. binary states and one-dimensional effort choice by the expert. (Some discussion of previous related work appears below.) Here, we can give results on the shape of the optimal contract that are quite general, with essentially ==== structural assumptions needed on the set of known experiments.====Beyond the worst-case criterion, the other key assumption in our model is that the state is fully revealed ex post, regardless of the decision made. Thus, the appropriate applications for our model are situations where the expert is predicting some event that will be publicly verifiable and is exogenous to the principal's decision. For example, the expert may be a political analyst, hired by a firm to predict whether a government will enact some regulation that is currently under consideration; the firm's business decisions depend on the predicted outcome, and afterwards it can be clearly observed whether the regulation was in fact enacted. Or the expert is a sports analyst, trying to predict a team's chances of winning a game in order to help a betting house set the correct odds. Or, the expert is a public health researcher, predicting whether some new epidemic is likely to strike a particular country, in order to help the government decide how much to stock up on medicines; this fits the model as long as the disease's arrival in the country is not causally affected by the government's decision.====The full revelation assumption allows us to separate the problem of choosing the decision from that of providing incentives to the expert. This distinguishes our work from some prior literature on incentives for experts, including Zermeño (2011) as well as the computer science literature on decision markets (Othman and Sandholm, 2010, Chen et al., 2011), in which optimal provision of incentives involves distorting decisions so as to reveal more information about the state. In the model here, the principal's decision plays no role in the incentives faced by the expert. Nonetheless, we model the decision problem explicitly because it determines how information is valued by the principal.====In the next section, we present the formal model, and also present a stylized application that will serve as a running example. We then give a more detailed discussion of the intuition behind restricted-investment contracts, before proceeding to the formal analysis showing that such contracts are optimal. The main proof is essentially an application of the same linear separation techniques used in Carroll (2015). In Section 3, we discuss a number of extensions and variations. In particular, we discuss when an ====restricted-investment contract — which, up to a normalization, is just a linear contract — is or is not optimal; this generally can happen when the principal's optimal decision is not too sensitive to the posterior, and in particular always holds if the decision problem is binary. The conclusion contains some additional discussion of interpretation and of the relation to realistic incentive schemes.====There is a variety of previous work on agency problems that considered incentives to acquire information, besides the work of Zermeño, 2011, Zermeño, 2012 cited above. Early forerunners include Demski and Sappington (1987) (who introduced the term “expert”) and Malcolmson (2009); a recent variant by Lindbeck and Weibull (2017) is based on rational inattention modeling. Unlike the present work, these models assume that the decision is delegated to the expert, and that only the realized payoff is observed, not the entire state. More recent years have seen much interest in dynamic contracts to incentivize experimentation (Bergemann and Hege, 1998, Manso, 2011, Gerardi and Maestri, 2012, Hörner and Samuelson, 2013, Halac et al., 2016). These works typically impose a very stylized structure (e.g. two states, binary signals, binary action choice for the agent), and study the dynamics of the optimal contract. Here, we consider one-shot information acquisition, but allow a rich state space and set of experiments, and impose no structural assumptions at all on the set of ==== experiments. Chassang (2013) considered a quite general model of information acquisition in a dynamic setting, also with very few structural assumptions, but with a different focus: the emphasis there was on showing that limited liability becomes approximately non-binding with a long time horizon, whereas here we focus on exactly optimal contracts in a one-shot problem. Finally, the scoring rules literature has also recognized at least informally that higher stakes give more incentives to acquire information (e.g. Karni, 2009), but without actually formulating the optimal contracting problem.====In addition to these strands of literature, the present paper also contributes to the growing literature on mechanism design using a maxmin objective in uncertain environments (e.g. Bergemann and Morris, 2005, Chung and Ely, 2007, Frankel, 2014, Garrett, 2014, Brooks and Du, 2019). The earlier paper (Carroll, 2015) contains more discussion of this literature, and of the interpretation of the worst-case objective.====There is other, less related literature on expert advice in economics, which focuses on other aspects of the problem. Most notably, a large literature beginning with Crawford and Sobel (1982) studies the incentive problems in reporting when the expert has intrinsic preferences over the decision being chosen; this issue is not part of our model.",Robust incentives for information acquisition,https://www.sciencedirect.com/science/article/pii/S0022053119300250,May 2019,2019,Research Article,272.0
Wang Fan,"HEC, Paris, France,Tel-Aviv University, Israel","Received 18 December 2017, Revised 25 December 2018, Accepted 16 February 2019, Available online 27 February 2019, Version of Record 4 March 2019.",https://doi.org/10.1016/j.jet.2019.02.009,Cited by (2),"Decision makers often stick to a status quo without explicitly reconsidering it. Yet, their observed choices might be compatible with a fully rational model. We ask when such a rationalization is possible. We assume as observable only the choice of sticking to the status quo vs. changing it, as a function of a database of cases. We state conditions on the set of databases that would make the decision maker change the status quo, which are equivalent to the following representation: the decision maker entertains a set of theories, of which one is that her current choice is the best; she is inert as long as that theory beats any alternative theory according to a maximum likelihood criterion.","Some decisions are taken, and some just happen. Consider, for example, Mary, who wakes up in the morning and goes to work. She could have quit her job on this particular day. The fact that she didn't would typically be modeled as a ==== to stick to her job. If asked, Mary might say that no explicit, conscious decision process was involved; the thought simply hasn't crossed her mind that morning. Indeed, one might conjecture that most people, on most mornings, do not explicitly consider some of their weightier life choices, including the jobs they have, the partners they live with, and the countries they reside in. Still, while the observed choices to stick to the status quo are not explicitly or even consciously made, we tend to model them as decisions.====The fact that people do not consciously consider all their choices obviously does not mean that they behave irrationally. As economists, we are used to thinking of observed choice ==== it were a result of conscious deliberation. Should Mary write down her options, list outcomes and possible scenarios, estimate utilities and probabilities, and calculate expected utility, she would probably find that it does not make sense to quit her job. Often she would figure out that the decision problem she faces has not changed dramatically since she last deliberated it. Thus, Mary can economize on the cognitive and emotional costs of decision making. A vague sense that the decision problem has not changed very much might be sufficient to make an implicit decision not to engage in explicit decision making. Further, it is possible that this implicit decision does not require awareness, or, differently put, that consciousness is summoned only when a sufficiently strong external impetus seems to require it.====Economists who espouse the rational choice paradigm can therefore explain the status quo bias as a rational decision to stick to an alternative that seems to be a best choice, until the arrival of information that convinces the decision maker that it isn't. Further, a wide range of behavior patterns can be rationalized if one takes into account sufficiently rich sources of information. Consider the famous studies on pension contributions (401(K)). Choi et al. (2002) and Carroll et al. (2009) showed the dramatic effect of changing a default decision, indicating that people may make sub-optimal decisions due to insufficient consideration of the options available. This phenomenon is justifiably considered to be an example in which behavioral economics had important policy implications. Moreover, there is no doubt that economists have not been sensitive to such phenomena before Tversky and Kahneman (1981) raised the issue of framing effects. At the same time, economists might argue that changing one's decision as a result of a change in the default option isn't irrational in any way. Economists would not deny that some decisions are implicit, automatic, or even unconscious, and that the rational models used in economics are not satisfactory descriptions of actual mental processes of decision. However, they would only claim that these models can serve as sufficiently good approximation of economic behavior. In this context, one would point out that the way information is imparted to the decision maker is often informative in and of itself.==== Indeed, rational players in a game are assumed to derive conclusions from the fact that they received certain signals and not others, from the incentives of those who chose which signals to send, and so forth.==== Along similar lines, one might speculate that forms are generally designed by benevolent administration in such a way that the default option is the modal one. With this working assumption, an employee who does not have sufficient information about her optimal pension contribution might do wisely in imitating the modal choice. This form of social learning, or “cognitive free-riding” may or may not lead to an optimal decision. In particular, it can lead to information cascades and sub-optimal herd behavior. Yet, the very fact that a certain option was chosen as the default might carry information, and it might be “more rational” to consider this information than to ignore it.====In this paper we ask which status quo decisions can be rationalized along similar lines. We assume as observable the decision maker's choice between sticking to the status quo (S) and changing it (D), as a function of a ====, which is a collection of ==== (or observations). In the pension contribution problem, we would conceptualize the default as a proxy for many cases in which that choice was selected by others. Thus, we would interpret the choice of the default option as a result of social learning, and expect it to be made given databases that include relatively many such choices. Specifically, a database is modeled as a “counter vector”, attaching an integer ==== to each “case type” ====, signifying the number of times cases of this type have been encountered.==== The set of all conceivable databases is divided into two – the set in which the decision maker sticks to the status quo, ====, and its complement, ====, in which a different decision is made. We state conditions on ==== and ====, which are equivalent to the following representation: the decision maker entertains a set of “theories”, ====, each of which is a probability distribution over the case types. That is, for ====, ==== is the probability theory ==== assigns to a case of type ====. A database ==== is in ==== if an only if==== for all ====.====In this representation, theory ==== can be interpreted as stating “the status quo choice is the best one available to me” whereas other theories ==== suggest that alternative courses of action are more promising. The decision maker is assumed to select a theory by the maximum likelihood criterion, and use it for decision making. Should (1) hold, theory ==== can be selected as the most likely theory, and if it suggests the status quo decision, it makes sense to stick to the status quo.==== In the default choice example discussed above, ==== could be thought of as “choice ==== is optimal”, while cases involve choices made by other, similar individuals. Assume that these individuals made the choices that were best for them, and that they have done so independently. Thus, the accumulation of many cases in which ==== was chosen by others would bolster the hypothesis that ==== is indeed optimal. If we further assume that the default choice is the modal choice in a group of similar individuals, one can rationalize the default effect as a form of social learning. This would be a rational form of learning if the observed choices by other individuals are indeed independent, as would be the case with film critics or other public figures whose choices are observed by the rest of the population. It can also be a model of an individual who implicitly assumes that the choices other made were independent, failing to take into account the effects of herding behavior.====Cases in our model are abstract entities that may correspond to various pieces of information. In the job search example, cases may involve information about the decision maker, such as positive or negative reviews she received; about others, such as the fact that another person changed her job; or about no one in particular, such as the news that there is higher demand for certain skills in certain markets. In an emigration decision example, a case might be the decision by another person to emigrate; a hate crime in one's country; a statement by a politician; and so forth. Thus, cases may or may not be accounts of similar decision problems (faced by the self or by others), and if they are, they may or may not specify outcomes and payoffs.====The representation (1) clearly implies that, for every ====, ==== if and only if ==== for all ====. Thus, rationalization of status quo decisions requires that the status quo decision be followed for a database ==== depending only on the relative frequencies of cases therein. Does it rule out anything else? Because in (1) the status quo is rationalized by a single theory ====, it follows that for two databases ==== we have to have also ====. This condition suggests another test of a rational status quo: if each of two databases suggests that the current option is the best one, we cannot observe a deviation from the status quo given their union. Our main result is that these two conditions basically suffice for the representation we seek. Theorem 1 proves that, coupled with so-called “technical” assumptions, status quo decisions can be rationalized in our sense if and only if they satisfy these two conditions.====Thus, Theorem 1 suggests conditions on data that allow a rationalization of status quo choices, and characterizes those patterns of choice that are incompatible with optimal decision making. For example, assume that we observe people's decision to emigrate from their home country as a result of the arrival of information – about others emigrating, about political developments in the home country, and so forth. For this application it makes sense to assume that only the binary decision to emigrate or not is observable, as data on final destinations may be unavailable (and perhaps only loosely related to the immigrants' plans). Are people who reside in a country just subject to a status quo bias, or do they rationally choose to live where they do? When we see a wave of emigration out of a country, is this a result of “dormant” decision maker who finally “wake up” and start thinking about their lives, or is it no more than rational information processing? Our result suggests an answer.====Observe that, as stated, our result is a bit simplistic as it does not allow for learning with noisy information. In the immigration example, suppose that database ==== contains a single case of a hate crime against an ethnic group. A single such case could be dismissed as an outlier. However, for a large ====, ==== would suggest that the country isn't safe. Thus, we could see ==== but ====. While this is incompatible with the representation (1), it is hardly irrational. We therefore provide a more general result (Theorem 2) that allows for such patterns of behavior. It characterizes the sets ==== that can be represented by a set of theories as above, where each theory has a coefficient ==== such that ==== is in ==== if and only if==== for all ====. This representation allows more freedom, and can capture situations in which a rational decision maker decides to switch from the status quo decision only with the accumulation of sufficient evidence. At the same time, as Theorem 2 shows, the representation is not vacuous. In particular, if a collection of news induces an immigration decision, it cannot happen that replicating these news would lead the decision maker back to the status quo decision.====To relate these results to the discussion above, assume that a behavioral economist says, “People are highly irrational. Some 50% of them fall prey to framing effects in their most important economic decisions. The 401(K) example shows that half of the employees make default-dependent choices.” A more classically-oriented economist might respond, “This might be a wild overestimate. Some of the people you talk about might have been reacting very rationally to the information encapsulated in the definition of the default.” The former might wonder is there anything that isn't compatible with such an account, and, relatedly, whether the rational approach has any empirical content. Possible answers are given by our characterization theorems. As Theorem 2 shows, even the more general representation (2) has refutable predictions. For example, if a database ==== denotes a person's original information, and if she changes her behavior as a result of new information ==== (so that ====), it is impossible that she would change her decision back as a result of additional information of the same type (that is, it is impossible that ==== for ====). Thus, the classical economist can challenge the behavioral one to find such violations of the representation (2). We do not attempt to design such experiments, let alone to speculate about the scope of deviations from the rational choice paradigm. Our goal here is only to sharpen the debate, and provide testable conditions that are not easily explained by the rational information processing account.====Our model can also be interpreted as a decision whether to make a decision. In this interpretation, the set ==== stands for the databases for which the decision maker makes an implicit decision not to make an explicit one, whereas in ==== she makes an explicit, conscious, decision. The latter may end up being identical to the status quo. In that case it might be hard to observe whether the decision maker sticks to the status quo without reconsidering it, or whether she deliberated the problem and decided not to change her choice. However, when applied to organization decisions, the decision whether to decide will typically be observable.==== Consider, for example, an academic department that is running business as usual until a proposal for a new policy is made. In order to adopt the new policy the proposal will have to be discussed in a department meeting. In this case the decision whether to decide is a conscious, rational decision – but of a single individual, rather than the organization itself: the department chair has to decide whether to convene a meeting. Our question will then be, under which circumstances will a meeting be convened, and can we rationalize the decision to call for a meeting?====The rest of this paper is organized as follows. The next sub-section is devoted to a discussion of related literature. Section 2 presents the model and the main result. Some extensions are presented in Section 3, while Section 4 contains a discussion.",Rational status quo,https://www.sciencedirect.com/science/article/pii/S0022053119300249,May 2019,2019,Research Article,273.0
"Ashkenazi-Golan Galit,Lehrer Ehud","Tel Aviv University, Tel Aviv 69978, Israel,INSEAD, Bd. de Constance, 77305 Fontainebleau Cedex, France","Received 5 September 2017, Revised 8 January 2019, Accepted 16 February 2019, Available online 27 February 2019, Version of Record 28 February 2019.",https://doi.org/10.1016/j.jet.2019.02.007,Cited by (0),"We consider two-player repeated games, where the players observe their own payoffs with a positive probability. Typically, a player observes neither the other's actions nor her own payoffs. We show that when costly communication is available to the players and when they are patient enough, being aware of her own payoffs suffices to provide the players with any strictly efficient payoff by sequential equilibrium.","The main result of the theory of repeated games is the Folk Theorem (Aumann and Shapley, 1994; Rubinstein, 1994; Fudenberg and Maskin, 1986). It states that in infinitely repeated games, when the players are sufficiently patient every feasible and individually rational payoff can be sustained by a subgame-perfect equilibrium. In particular, all efficient and individually rational payoffs can be obtained as equilibrium payoffs. This classical result relies on the assumption that the players have perfect monitoring: each player's actions are perfectly observable by his opponents. The extent to which the players can cooperate when they do not observe the opponents' actions, but rather receive noisy signals that depend on the actions taken, is still mostly unknown.====In this paper, we analyze two-player repeated games in which the players cannot fully monitor each other's actions. Rather, players receive noisy signals that reveal with a positive probability their own payoffs. This model encompasses in particular the cases of full monitoring and the case where the players always observe their own payoffs, as discussed in Lehrer (1992c). When a player observes only his own payoffs, he cannot fully monitor but he can obtain partial information about the other player's actions. A natural question arises as to whether this information is sufficiently rich to enable the players to sustain efficient payoffs in equilibrium. The main contribution of the current paper is showing that any strictly Pareto efficient payoff can be supported by a sequential equilibrium, if costly communication is available and the players are sufficiently patient. A simple consequence of this result is that any combination of a strictly Pareto efficient payoff and Nash equilibrium payoff of the one-shot game, can be also supported by a sequential equilibrium.====In general, repeated games with imperfect monitoring may be divided into two types. The first type consists of games where players obtain private signals and their strategies may depend on these signals. In a series of papers,==== Lehrer discusses this subject and fully characterizes the set of equilibrium payoffs in certain families of undiscounted games. Our model is closest to the one explored in Lehrer (1992c). In undiscounted repeated games any action that a player takes seldom enough has no effect on his total payoff. Indeed, in Lehrer (1992c) the equilibrium construction relies on statistical tests that become rare as the game unfolds, and therefore have no impact on the payoffs. None of the techniques employed to sustain equilibrium payoffs in undiscounted games could be employed in the current paper.====The second type of repeated games with imperfect monitoring consists of games with public monitoring. In these games, after each period, all players observe the same signal which determines, along with the players' own actions, their payoffs. The solution concept typically employed in such games is public equilibrium, where players cannot use their private information; the strategies may depend only on the public signals (see, Abreu et al., 1986, Abreu et al., 1990 and Fudenberg et al., 1994). This paper belongs to the first type.====Characterizing the set of equilibrium payoffs for infinitely discounted repeated games with private monitoring was described by Kandori (2002) as ‘a simple hard open question’. More than ten years later, this question still remains difficult and open both for discounted and for undiscounted games. The difficulties in analyzing repeated games with private signals might stem from different sources: (i) identifying the power of the correlation between the players that might be internally generated by private monitoring; (ii) detecting profitable deviations from the equilibrium path; (iii) establishing punishments and continuation payoffs for the cases when a deviation is detected; and (iv) specifying a system of beliefs to accompany the strategies of the players in order to create sequential equilibrium. For a comprehensive discussion of these issues the reader is referred to Mailath and Samuelson (2006). In what follows we elaborate on these difficulties and specify which ones are particularly relevant to our model.====The first difficulty in studying equilibrium payoffs in repeated games with imperfect monitoring, is that private monitoring may actually serve as a correlation mechanism among the players. In Lehrer (1991) this effect is called ====. The extent to which private signals may serve as an internal correlation device is still open. However, in the current context, internal correlation does not play a role because no correlation is needed to sustain efficient payoffs.====The common pattern of equilibrium strategies in repeated games is that players follow a play path unless a deviation occurs. Play paths are designed in a way that makes profitable deviations detectable. The second difficulty to establish an equilibrium with imperfect monitoring, is that profitable deviations might go unnoticed and therefore undetectable. However, when the payoffs are observable with a positive probability, as in this paper, all profitable deviations from efficient payoffs are detectable with positive probability. Thus, this difficulty too does not arise in our model.====The two main contributions of this paper lie in the methods it offers for handling difficulties (iii) and (iv). The punishment phase is designed using a new tool that we call the ==== and the beliefs are devised using a combination of communication and a specification of off-equilibrium path beliefs.==== When a deviation is detected during the play path, the players switch to a temporary punishment mode. While detecting profitable deviations from strategies leading to efficient payoffs is easy, dealing with the punishment phase and establishing proper continuation payoffs is not trivial. In some cases though, as when the desired equilibrium payoff strictly Pareto dominates a one-period Nash equilibrium, a punishment can be easily designed: when a deviation occurs, the players switch to playing the dominated one-period Nash equilibrium. In such cases (see Fudenberg and Levine, 2007, for example) a Nash-threat folk theorem is established. However, when the target equilibrium payoff does not strictly Pareto dominate a one-period Nash equilibrium, when players wish to effectively punish the deviator, they typically do it by playing mixed (minmax) actions. These actions are often not stage-game best responses. In order to provide incentives for the players to nevertheless use these mixed actions, the equilibrium strategies should specify continuation payoffs that would make the punishing player indifferent to all pure actions used. This method was developed by Fudenberg and Maskin (1986). When the monitoring is full, designing continuation payoffs that would incentivize the players to follow the punishment scheme is not difficult. However, when players cannot fully monitor each other, coordinating the continuation payoffs becomes difficult. The challenge is to design an effective punishment scheme in which players can only observe their own payoffs (with positive probability) and not others'.====In order to better explain this challenge, imagine that Player 2 wishes to punish Player 1. The problem is that Player 1 observes only his own payoff; he does not know Player 2's actions nor her payoffs, and he does not know what she knows about him. Without proper (future) continuation payoffs (to be given after the punishment phase is over) Player 2 would have no incentive to abide by the punishment instructions; she could profitably deviate without being detected. How then can we make sure that Player 2 follows her strategy and keeps punishing Player 1? The way to do it is to increase her future payoff when, during the punishment, she uses a low-paying action, and to reduce her future payoff when she uses a high-paying action. This way we make her indifferent between all actions used. But how can the players agree on continuation payoffs if they do not share the same information?====One of the two main contributions of this paper is to develop a method that enables a construction of adequate continuation payoffs. Despite the fact that the punished player cannot observe the punishing player's payoffs, we design a scheme that enables the players to coordinate the continuation payoffs, a scheme which in turn renders the strategies incentive compatible. This means, in particular, that the information embedded in the punished player's own payoff, as reflected in the information matrix, is sufficiently rich to sustain efficient payoffs in equilibrium by designing an effective punishment phase. The method by which we design the punishment scheme is constructed by translating the private information available to the punished player to a ==== matrix, called the ====. This information matrix enables us to find proper continuation payoffs after every history of punishment.==== The fourth difficulty is rooted in the definition of sequential equilibrium (Kreps and Wilson, 1982). The definition requires that an elaborate system of beliefs accompanies the strategies and that players would always best-reply to these beliefs. The beliefs should be defined after any history following any number of deviations from the equilibrium path, regardless of the number of deviating players, their identities, and the actions played. However, when players do not share common information about histories, keeping track of all their possible beliefs, regardless of how far from the equilibrium path they had gone, is a demanding task.====There are three ways to prevent players' beliefs from drifting too far apart: introducing communication, introducing a mediator and assuming substantial assumptions on the signal structure, such as that the signals are highly correlated (‘almost public’) or almost accurate (‘almost perfect’). These are discussed below.==== When a communication device that generates public signals is present, its signals can be used to coordinate the beliefs of the players. However, the presence of a communication device introduces yet a new challenge: how to provide the players with proper incentives to signal honestly. Compte (1998) and Kandori and Matsushima (1998) analyze games with communication. In these papers the general results involve at least three players. The players have incentives for honest signaling regarding a deviation because a deviation of a player is detected by a subset of players whose members are not affected by the punishment of the deviator. Therefore, the players that detect a deviation are not hurt when triggering a punishment. When signals are highly correlated, Kandori and Matsushima (1998) provide conditions to guarantee strict truth-telling incentives. In addition, following Abreu et al. (1991) that investigated the effect of delayed revelation of public signals, Compte (1998) and Kandori and Matsushima (1998) employ a delayed communication for their two-players' models. The communication is conducted every ==== periods, and ==== increases as the players become more patient. The delay aims to increase efficiency by accumulating more information through communication (and thus making statistics-based decisions more accurate). Obara (2009) develops the ideas of delayed communication in games with more than two players when monitoring is almost public.====Fudenberg and Levine (2007) take a different approach, which does not require delayed communication. They obtain conditions under which a perfect public equilibrium is robust to small perturbations. They use a one-period Nash equilibrium for punishment, and thus the resulting set of payoffs supported in equilibrium is restricted to those payoffs that Pareto-dominate a one-period Nash equilibrium payoff.==== Lehrer (1992a) added a mediator to two-player repeated games with private deterministic signals, and characterized the set of correlated equilibria payoffs. Hillas and Min (2016) generalized Lehrer's result to a model with stochastic signals. Recall that in games with imperfect monitoring, the histories may serve as a (internal) correlation device. However, in the presence of a mediator, this internal correlation plays no role. The correlation is already provided by the external mediation device. Renault and Tomala (2004) generalize Lehrer's result to an arbitrary number of players by adding the assumption that the players can also communicate with the mediator, thus using the solution concept of communication equilibrium (see, Forges, 1986).====In a recent work, Sugaya (2017) characterized a limit set of the communication equilibrium payoffs when the payoffs are random and one's own payoffs are observed by each player. Sugaya uses the randomization produced by a mediator in order to obtain signals with full support. The advantage of the full support is that it enables one to bypass the need to deal with beliefs off-equilibrium. The downside, however, is that due to this randomization, efficient payoffs may be only approximated in equilibrium, while in our model, they are obtained accurately.==== Results that assume no form of communication or mediation assume strong assumptions regarding the monitoring structure. For example, Mailath and Morris (2002) obtain conditions for public perfect equilibrium to be robust under small perturbations in monitoring (perturbations that make the monitoring private). They obtain conditions for folk theorem when monitoring is almost perfect and almost public. Hörner and Olszewski (2006) obtain a more general result related to monitoring that is almost public. They obtain a Folk Theorem under a standard dimensionality condition.====Another type of monitoring assumptions is made by Sugaya (2015). He obtains a Folk Theorem for two-player games with no communication while assuming that the signals have a full-support, and that any deviation of a player against any pure action of the opponent changes the distribution of the opponent's signals. In our model, in contrast to this assumption, when a player receives the same payoff upon playing against two different actions of the opponent, the signals he receives might be the same as well.====The monitoring structure examined in this paper is neither almost-public nor almost-perfect. Yet, we show that the payoffs provide information that is detailed enough to allow for supporting all efficient payoffs and having a minmax scheme for cases in which deviations are detected. This is the first main contribution of this paper.==== Players form beliefs regarding their opponents' private history with or without communication. Coordinating these beliefs is especially complicated when the private signals indicate that the game is off the equilibrium path. The equilibrium strategies that are common in the private monitoring literature have two ways with which to avoid dealing with off-equilibrium path beliefs. There are strategies, called ====, that each player plays optimally following every private history, independently of his beliefs regarding the opponents' private histories (see, for example, Piccione, 2002; Ely and Välimäki, 2002 and Ely et al., 2005). Thus, there is no need to specify the beliefs of the players. Typically, the set of equilibrium payoffs that use belief-free strategies is limited, and frequently does not even contain all individually rational efficient payoffs. In fact, Kandori (2011) demonstrates that even strategies conditioned on beliefs regarding the last action alone (strategies called ‘weakly belief-free’) may improve efficiency over belief-free strategies. When playing ==== strategies, on the other hand, each player plays a best reply to his beliefs. For example, in Bhaskar and Obara (2002) the prisoner's dilemma is analyzed, and the strategies induce two possible states for each player, ‘cooperating’ and ‘deviating’. The entire belief system, in this case, boils down to beliefs about the state of the players. An initial randomization device chooses between the two states, while both states enjoy positive probability along the play. Therefore, any realization of private signals is obtained with a positive probability on the equilibrium path, and there are no off-equilibrium path beliefs. In this case, the efficient payoff, induced by pure strategies, can only be approximated. Other papers assume full-support of private signals following any pure-actions profile or, when full-support of signals is not assumed, as in Sugaya and Wolitzky (2016), the players are instructed to mix their actions in order to retain full-support of private signals. Here, as in results using belief-free strategy, the need to randomize often prevents exact efficient payoffs from being achieved.====Our model assumes a minimal form of communication: a single costly private signal is available to each player. This minimal communication is used only off-equilibrium path. In other words, in equilibrium, the communication channel will never be used. Communication takes place only when a player observes a signal that bluntly reveals a deviation. Such a situation is impossible when the signals always have full support. Our paper adds to the current literature the insight that the private information available to the players is rich enough to enable cooperation when (a) players observe their own payoffs with some positive probability, and this event becomes common knowledge; and (b) a costly communication channel is available. A special case of our model is when own payoffs are observed with probability 1. In this case, both conditions hold, in particular, any strictly efficient strictly individually rational payoff can be obtained as sequential equilibrium payoff.====In order to circumvent dealing with off-equilibrium path beliefs, most of the private monitoring literature either assumes or forces (through proper mixing) full support of the private signals observed, leaving no signal off-equilibrium path. Full-support strategies typically cannot precisely support efficient payoffs, but rather approximate them. In contrast, efficient payoffs in our model are obtained as equilibrium payoffs. Our model involves signals that are off-equilibrium path and the strategies instruct the players what to do after observing them. This is why we cannot shy away from specifying the beliefs held by players off-equilibrium path, following any private history. Moreover, we must show that the players indeed best reply to these beliefs. The challenge becomes even more complicated when simultaneous deviations of both players are considered. In equilibrium deviations are unprofitable and mutual deviations occur indeed with probability zero. However, the notion of sequential equilibrium still requires the existence of a consistent system of beliefs following every history, including after simultaneous deviations. Furthermore, the actions prescribed by each player's strategy must best reply to his respective beliefs. Finding such a system is a difficulty that never rises when signals have full support. Since the information structure explored in this paper typically does not provide a full support of signals, we have to explicitly construct proper beliefs after all histories, no matter how far from the equilibrium path these histories are. This is the second main contribution of this paper.==== The main concern of the paper is the ability to sustain cooperation through the ability to effectively sanction an opponent when deviating from this cooperation. Specifically, it is concerned with the ability to differentiate between effective sanctions (minmaxing actions) that might damage the sanctioning player as well as the sanctioned one, and non-effective sanctions (actions that reduce the sanctioned player's payoffs, but are not minmax, meaning, the sanctioned player may recover at least some of his losses when reacting accordingly).==== Section 2 presents motivating economic situations for the paper's topic. Section 3 presents an example that demonstrates some of the challenges, and ideas for coping with these challenges. Section 4 details the model and introduces the main result. Section 5 presents the information matrix, a key technical tool employed in the proof process. The proof of the main result appears in Section 6, including the description of the beliefs held by the players. Section 7 concludes with some final comments. All proofs appear in Appendix A, and a formal detailed treatment of off equilibrium path beliefs in Appendix B.",What you get is what you see: Cooperation in repeated games with observable payoffs,https://www.sciencedirect.com/science/article/pii/S0022053119300225,May 2019,2019,Research Article,274.0
Yang Jingni,"Erasmus School of Economics, Erasmus University Rotterdam, Rotterdam, the Netherlands,Research School of Economics, Australian National University, Canberra, Australia","Received 14 January 2018, Revised 14 February 2019, Accepted 16 February 2019, Available online 21 February 2019, Version of Record 26 February 2019.",https://doi.org/10.1016/j.jet.2019.02.008,Cited by (8),"This paper shows that convexity of preference has stronger implications for weighted utility models than had been known hitherto, both for utility and for weighting functions. Our main theorem derives concave utility from convexity of preference on the two-dimensional comonotonic cone, without presupposing continuity. We then show that this, seemingly marginal, result provides the strongest tool presently available for obtaining concave/convex utility or weighting functions. We revisit many classical results in the literature and show that we can generalize and improve them.","Convexity of preference is a standard condition in many fields (De Giorgi and Mahmoud, 2016, Debreu, 1959, Mas-Colell et al., 1995 p. 44). We examine it for weighted utility models, where its potential has not yet been fully recognized. Our first theorem shows its equivalence to concave utility on the two-dimensional comonotonic cone. This generalizes existing results by not presupposing continuity and by providing flexibility of domain. With this seemingly thin and marginal result we can in one blow generalize virtually all existing theorems on convex or concave utility or weighting functions, and make them more appealing.====The aforementioned theorems concern: (1a) risk aversion for expected utility not only for risk (von Neumann and Morgenstern, 1944) but also for (1b) uncertainty (Savage, 1954); (1c) Yaari's (1969) comparative risk aversion generalized by allowing for different beliefs; (2) concave/convex utility and weighting functions (2a) for Gilboa's (1987) and Schmeidler's (1989) rank-dependent utility for ambiguity, and (2b) for Tversky and Kahneman's (1992) prospect theory for ambiguity; (3) corresponding results for Ghirardato and Marinacci's (2001) biseparable utility for ambiguity====; (4) smooth ambiguity aversion (Klibanoff et al., 2005). Wakker and Yang (2019) show how our main theorem can be applied to decision under risk,==== providing results on: (1) concave/convex utility and probability weighting for Quiggin's (1982) rank-dependent utility for risk and Tversky and Kahneman's (1992) prospect theory for risk; (2) corresponding results for Miyamoto's (1988) biseparable utility for risk====; (3) loss aversion in Köszegi and Rabin's (2006) reference dependent model; (4) inequality aversion for welfare theory (Ebert, 2004).====The main contribution of this paper is not to generalize some theorems, which would constitute a marginal contribution, but to provide a general technique to obtain convex/concave utility or weighting functions in a more general and appealing manner than done before. As corollaries, we can generalize and improve virtually all existing results on this topic in the literature. To limit the size of this paper, we focus on uncertainty henceforth. Our theorems can readily be applied, though, not only to risk (Wakker and Yang, 2019), but also to discounted utility for intertemporal choice with aversion to variation in outcomes, utilitarian welfare models with aversion to inequality, and other weighted utility models.====The outline of the paper is as follows. Section 2 presents elementary definitions and our main result. To show its usefulness, the following sections apply our main result to a number of well-known classical results in the literature, generalizing and making them more appealing. These applications demonstrate that we have provided a general tool for analyzing concave/convex utility and weighting. Section 3 presents implications for uncertainty focusing on classical expected utility. Sections 4 and 5 turn to ambiguity models, followed by a concluding section and an appendix with proofs. In each proof, we first find a substructure isomorphic to our main theorem, and then extend the desired result to the whole domain considered.",A powerful tool for analyzing concave/convex utility and weighting functions,https://www.sciencedirect.com/science/article/pii/S0022053119300237,May 2019,2019,Research Article,275.0
"Alva Samson,Manjunath Vikram","The University of Texas at San Antonio, United States of America,University of Ottawa, Canada","Received 30 November 2017, Revised 17 September 2018, Accepted 27 January 2019, Available online 18 February 2019, Version of Record 21 February 2019.",https://doi.org/10.1016/j.jet.2019.01.004,Cited by (29),"We consider a model where each agent has an outside option of privately known value. At a given allocation, we call the set of agents who do not exercise their outside options the “participants.” We show that one strategy-proof and individually rational mechanism weakly Pareto-improves another if and only if, at each preference profile, it weakly expands (in terms of set inclusion) the set of participants. Corollaries include: a sufficient condition for a mechanism to be on the Pareto-efficient frontier of strategy-proof mechanisms; uniqueness of strategy-proof Pareto-improvements under true preferences over certain normatively meaningful benchmark allocation rules; and a characterization of the pivotal mechanism.","We consider mechanisms that choose an allocation as a function of agents' preferences. In our model, each allocation is associated with a fixed set of participants.==== A non-participant at an allocation consumes his outside option. A mechanism is strategy-proof if, at each profile of preferences, no agent is able to beneficially misreport his preferences. It is individually rational if, at each profile of preferences, no participant prefers his outside option to the chosen allocation.====We show the equivalence of two binary relations over strategy-proof and individually rational mechanisms. The first is the Pareto-improvement relation: mechanism ==== (weakly) Pareto-improves mechanism ==== if, at each profile of preferences, each agent finds the choice of ==== at least as desirable as the choice of ====. The other is the participation-expansion relation: ==== (weakly) participation-expands ==== if, at each profile of preferences, each participant at the choice of ==== is a participant at the choice of ====. Therefore, the requirements of strategy-proofness and individual rationality contain enough information about preferences to ensure that a comparison that makes no reference to preferences (participation-expansion) is equivalent to one that does (Pareto-improvement).====We make three assumptions on preferences. Firstly, we rule out externalities on non-participants. So, if an agent participates at neither allocation ==== nor at allocation ====, then he is indifferent between ==== and ====. Secondly, only to show that participation-expansion implies Pareto-improvement, we assume the range of values that an agent's outside option may take are on the order of what the mechanism may offer him as a participant. We call this second assumption “richness of the outside option.” It ensures that the (upward) movement of an agent's outside option in his preference is essentially unrestricted. Since how an agent compares his outside option to the various alternatives as a participant is often private information, this is natural in many applications. Finally, only to show that Pareto-improvement implies participation-expansion, we assume that no agent is indifferent between his outside option and any allocation at which he participates. We call this final assumption “no indifference with the outside option.”====We point out three useful corollaries of our main result:====The rest of the paper is organized as follows. We introduce our model in Section 2. We define properties of allocations and mechanisms in Section 3. We state and prove our results in Section 4. We discuss applications in Section 5. We defer detailed discussion of related literature to Sections 4 and 5.",Strategy-proof Pareto-improvement,https://www.sciencedirect.com/science/article/pii/S0022053119300080,May 2019,2019,Research Article,276.0
"He Simin,Offerman Theo,van de Ven Jeroen","School of Economics and Key Laboratory of Mathematical Economics, Shanghai University of Finance and Economics, 111 Wuchuan Rd, 200433 Shanghai, China,CREED, University of Amsterdam, Roetersstraat 11, 1018 WB Amsterdam, Netherlands,Tinbergen Institute, 1082 MS Amsterdam, Netherlands,Amsterdam School of Economics, University of Amsterdam, Roetersstraat 11, 1018 WB Amsterdam, Netherlands","Received 31 October 2017, Revised 14 January 2019, Accepted 8 February 2019, Available online 18 February 2019, Version of Record 1 March 2019.",https://doi.org/10.1016/j.jet.2019.02.004,Cited by (2),"We study theoretically and experimentally the extent to which communication can solve coordination problems when there is some conflict of interest. We investigate various communication protocols, including one in which players chat sequentially and free-format. We develop a model based on the ‘feigned-ignorance principle’, according to which players ignore any communication unless they reach an agreement in which both players are (weakly) better off. With standard preferences, the model predicts that communication is effective in Battle-of-the-Sexes but futile in Chicken. A remarkable implication is that increasing players' payoffs can make them worse off, by making communication futile. Our experimental findings provide strong support for these and some other predictions.","Humans have achieved astonishing successes in creating ideas and developing technologies. Most of these accomplishments required some form of coordination between people. Communication is undoubtedly at the heart of such successful coordination. Yet, exactly how and under what kind of conditions people manage to coordinate effectively is still largely an open question, both theoretically and empirically. A main obstacle is that people's objectives are usually not fully aligned. There will often be disagreement over what to coordinate on, even if there are potential benefits of coordination and people can communicate with each other. While it has since long been recognized that communication can help (e.g., Farrell, 1987, Farrell, 1988; Farrell and Rabin 1996), existing theories of communication are focused on restrictive and somewhat unnatural communication forms, and therefore give little guidance over the outcomes we can expect to occur in this class of settings, and whether those outcomes will be efficient.====Our contribution is to develop a theoretical model in which players can alternatingly send messages to each other before making their decisions. A main way in which our model differs from most of the existing literature is the way in which players send messages. Whereas in most models players send messages simultaneously or only one of the players can send a message,==== in our setting players alternate in sending messages. As discussed in the concluding section of Rabin (1994), there are several advantages to our approach. Simultaneous communication is at variance with how people normally communicate. It also introduces another coordination problem since messages can be conflicting. One-way communication gives too much power to the player that can send a message. Especially in coordination games with partially conflicting objectives, it seems reasonable to allow both players to express their agreement or disagreement. The experimental evidence shows that these concerns are potentially very relevant (see later). Our model also differs from many other models in that we assume communication is costly and players can choose when to end the communication process. This captures the opportunity cost of time spent communicating, and prevents players from talking forever.====To study how people will communicate, we need to make some behavioral assumptions about the link between communication and actions. The main assumption we introduce is the ‘====’.==== According to this principle, an agreement reached in the communication stage is only effective if following the agreement is a Pareto-improvement compared to the expected payoffs without communication. If at least one player would be better off by ignoring (or pretending to ignore) the conversation, both players will play according to the outcome that is focal without communication. Which point is focal may depend on the structure of the game, and can be empirically determined.====Our model identifies conditions under which communication will help. With standard preferences, the main prediction of the model is that communication will result in successful coordination in a Battle-of-the-Sexes game (like in Fig. 1A), but will be futile in a Chicken game (like in Fig. 1B). The reason is that the pure strategy equilibria in a Battle-of-the-Sexes game yield higher expected payoffs to both players compared to the mixed strategy equilibrium, which in this game we expect to be the focal equilibrium without communication (see Farrell 1995). By contrast, in a game of Chicken, the payoff of a player's least preferred pure strategy equilibrium is worse than the expected payoff of the mixed strategy equilibrium. Thus, in a Battle-of-the-Sexes game, players will want to listen to each other, but in a game of Chicken at least one of the players will want to ignore the conversation. We also examine the case in which players are lying-averse (e.g., Vanberg 2008). In that case, we show that players in a game of Chicken may agree on playing the “both chicken strategy” (both playing strategy “L” in Fig. 1B), but they will not always conform to the agreement.====We put our theoretical predictions to an experimental test. The results support the main predictions. In a Battle-of-the-Sexes game, communication is very effective and helps players to coordinate. Typically, players coordinate immediately on the first sender's preferred equilibrium. This result resonates well with the existing experimental evidence using different communication formats (Cooper et al. 1989 and Duffy and Feltovich 2002). More surprising and novel is our finding that communication is largely ineffective in the game of chicken. Subjects also appear to anticipate the ineffectiveness of sending messages, and frequently forgo the option to communicate at all. As a consequence, and consistent with the theoretical predictions, higher values of the payoffs associated with ==== can make subjects worse off by making communication futile.====As subjects in our experiments can send free-form messages, we are also able to analyze their contents in more detail.==== The analysis tells us that first-senders frequently express an intention to play ==== in the Battle-of-the-Sexes, and this happens much less often in the game of Chicken. This is consistent with the comparative statics predictions of our model. With higher ==== payoffs, we find that subjects frequently reach an agreement to both play ====. In agreement with the equilibrium that allows for lying aversion, we find that subjects play ==== more often after agreeing on ====, but the effect is small and subjects still often choose ====. The data also support the prediction that more players conform to the agreement when the payoffs of ==== are larger.====In a follow-up experiment, we implement two alternative communication formats: one-sided costless communication and free-form costless chat. For one-sided communication, we can derive theoretical predictions under the ‘feigned-ignorance’ principle, i.e., assuming that players will ignore any messages that, if followed, do not yield a Pareto-improvement over the focal strategies without communication. Also for this case the model predicts that with standard preferences communication is very effective in a Battle-of-the-Sexes game but not in a Chicken game. The experimental results are in line with these predictions. Communication is also very effective in a Battle-of-the-Sexes game if players can chat free-format. This shows that the effectiveness of sequential communication is not driven by the imposed asymmetry that results from our assignment of the first sender. We also find that free-format chat is somewhat effective in a Chicken game. With free-format chat, participants communicate more often and the conversations are more lengthy.====The importance of our results go beyond a better understanding of how and when communication works. When faced with multiple Nash equilibria, many theorists focus on the set of efficient equilibria. The rationale is that communication would help players to coordinate on an efficient equilibrium (cf. Rabin 1994), even though the communication stage is often not explicitly modeled. Our results provide support for this approach.====The feigned-ignorance principle provides an alternative to the approach taken in, e.g., Farrell and Rabin (1996). They argue that if people talk about intentions, messages that are both self-signaling and self-committing seem especially credible.==== On the other hand, experimental evidence shows that communication is effective even when the messages are not self-signaling (Charness 2000; Clark et al. 2001; Blume and Ortmann 2007; Brandts and Cooper 2007; Avoyan and Ramos 2016), so that it is an open question when messages need to be self-signaling.====Our theoretical predictions are also quite different from those of the theory developed in Ellingsen and Östling (2010). They study the effect of communication in both the Battle-of-the-Sexes game and the Chicken game by using a level-==== model. They predict that one-way communication will powerfully resolve the coordination problem in such coordination games if players have some depth of thinking, even in games like Chicken where our approach predicts communication to be ineffective unless lying aversion plays a sufficient role.====In terms of communication structure, Santos (2000) is closest to our approach. He provides a model of finite sequential cheap talk communication in coordination games. In his game, the two players alternate making costless announcements that may be accepted or followed up by a counterproposal before they make their choices in the coordination stage. With a commonly known final round of communication, all the negotiation power is essentially given to the player who can make the last announcement. In this sense, this model is quite similar to a model of unilateral communication where only one player can make an announcement. In most actual cases, it is not commonly known on beforehand who will have the possibility to say the last word. Our model seems a better approximation of such conversations.====In terms of experimental work, our results shed light on the existing literature that shows the effectiveness of communication in Batte-of-the-Sexes games with different communication formats (see e.g., Cooper et al. 1989). Cooper et al. (1989) show that one-way communication increases the coordination rate dramatically from 0.48 to 0.95. Two-sided communication is much less effective though. One round of two-way communication raises the coordination rate only to 0.55, and three rounds yields a coordination rate of 0.63. Our findings suggest that the earlier mentioned concerns with the previous communication forms are valid. Our more natural form of communication increases the coordination rate from 0.43 to 0.80. Thus, one way-communication may overstate the effect of communication while two-sided communication underestimates its effect. In the context of Chicken games, the only work we are aware of is that by Duffy and Feltovich (2002), who investigate how one-way pre-coded cheap talk and observations of previous play affects behavior. They find that observations of previous play are more effective than cheap talk to increase coordination in the Chicken game. Duffy and Feltovich (2006) extend the analysis by investigating how results change when subjects' messages can contradict previous actions.====The remainder of the paper is organized in the following way. Section 2 describes the game and the theory. Section 3 presents the experimental design of the sequential communication protocol. Section 4 discusses the experimental results for sequential communication. Section 5 presents the design and results for other communication protocols. Section 6 provides discussion and Section 7 concludes.",The power and limits of sequential communication in coordination games,https://www.sciencedirect.com/science/article/pii/S0022053119300201,May 2019,2019,Research Article,277.0
"Al-Najjar Nabil I.,Shmaya Eran","Department of Managerial Economics and Decision Sciences, Kellogg School of Management, Northwestern University, Evanston, IL 60208, United States of America","Received 18 June 2018, Revised 26 November 2018, Accepted 12 February 2019, Available online 15 February 2019, Version of Record 4 March 2019.",https://doi.org/10.1016/j.jet.2019.02.005,Cited by (0),"We explore how the Epstein–Zin utility captures an agent's sensitivity to parameter uncertainty. Our main result is a closed-form representation of the Epstein–Zin utility for an i.i.d. consumption process with unknown parameter, as the discount factor approaches 1. Using this representation, and under the usual assumption about the relationship between risk aversion and the attitude towards time smoothing, we show that the agent is averse to parameter uncertainty.","In their seminal paper, Epstein and Zin (1989) proposed a functional form for intertemporal preferences that has become a building block in many asset pricing and macroeconomic models. The key advantage of the Epstein–Zin model relative to the standard additive utility is that risk aversion and the elasticity of intertemporal substitution can be separated.====To motivate the Epstein–Zin utility and illustrate our contribution, consider the following example. In each period ====, consumption is random with realization ====, with ====. Consider two stochastic consumption processes:====This feature of additive utility is counter-intuitive for two reasons. First, additive utility is empirically questionable as it ties an agent's preference for smoothing consumption over time to that agent's attitude towards risk. Second, additive utility is insensitive to uncertainty about the long-run fundamentals of the process—or ====, for short. In the example above, the agent is indifferent between process ====, where long-run average consumption is known with certainty, and ====, where that average is unknown.====Epstein–Zin utility captures, in a very subtle way, these phenomena in a recursive power utility form. Their model evaluates a stochastic consumption process using a discount factor, an elasticity of intertemporal substitution, and a coefficient of relative risk aversion. While this makes it clear how the Epstein–Zin utility separates risk aversion from attitude towards time smoothing, it is less obvious whether, and how, this model captures the sensitivity to parameter uncertainty. Part of the difficulty is that the Epstein–Zin utility is a fixed point of an operator that often lacks a closed form solution, making for a non-trivial dependence on the structure of the consumption process.====This paper considers Epstein–Zin utility for conditionally i.i.d. consumption processes and establishes a simple closed-form representation of utility as the discount factor approaches 1. Using this closed form representation, it is easy to see how the intertemporal substitution and risk aversion parameters characterize the impact of parameter uncertainty. To make this precise, we assume that the set of possible consumption levels ==== is finite, and that consumption is i.i.d. with an unknown distribution ==== over ====. Let ==== be the distribution over ==== representing uncertainty about this parameter. Our main result states that, as ====, the Epstein–Zin utility approaches:==== where ==== is the coefficient of relative risk aversion, and ==== is the elasticity of intertemporal substitution.====Intuitively, a patient agent with Epstein–Zin utility evaluates a consumption process in two steps. First, once ==== is known, the agent will almost surely face a sequence where each consumption level ==== occurs with a known frequency ====. Although the time periods in which these consumption levels occur remains uncertain, this uncertainty is irrelevant to a patient agent who will evaluate such sequence according to ====. Note that risk aversion plays no role in this expression since the agent is concerned only with consumption smoothing. Risk aversion is relevant only with respect to the uncertainty about how the utility from long-run consumption ==== varies with ====.====To illustrate this, consider the two processes introduced earlier. Since consumption takes only two values, 0 or 1, we have ==== and the above expression simplifies to:==== For process ====, ==== and this expression reduces to ====. For Process ====, by contrast, long-run consumption is uncertain, and the limiting utility is given by (2) with ==== being the uniform distribution. For example, if ====, so the agent cares only about the long-run consumption, the Epstein–Zin utility of a patient agent from ==== is just the long-run consumption ====, while the utility from ==== for a risk averse agent (====) is ====. Uncertainty about long-run consumption is “penalized” via the parameter ====, reducing overall utility.====Since the Epstein–Zin utility is defined as a fixed point of an operator, the closed form we obtain is useful and non-trivial. Finding the utility for Process ==== is not too difficult, as it can be obtained from solving one equation in one unknown. However, even for a simple process like ====, a closed form expression for a fixed ==== is not known and utility must be computed via simulations. The Epstein–Zin utility of a process like ==== is computed recursively as a function of the future utilities, which are indexed by the posterior beliefs about the parameter that can arise from Bayesian updating.====The assumptions of conditionally i.i.d. consumption processes and that ==== play an essential role in our analysis. Intuitively, after an initial learning period, the agent's posterior becomes concentrated around the true i.i.d. component of the process. At that point, the Epstein–Zin utility can be directly computed, while patience implies that the initial period of learning has a vanishing impact on utility. It is reasonable to expect this intuition to generalize to, for instance, settings where agents are uncertain about the ergodic component of a stationary process. In fact, we argued in Al-Najjar and Shmaya (2015) that stationary processes provide a natural context to study parameter uncertainty and showed, in Al-Najjar and Shmaya (2018), that the ergodic components of such processes are learnable. This seemingly natural extension to stationary environments is far from obvious, however: our proofs rely on special properties of conditionally i.i.d. processes, such as the uniform consistency of the Bayesian estimator (Section 3.3.2) that are not known to hold for more general environments—at least not without additional assumptions. Finally, generalizations that go beyond stationarity seem unlikely: it is easy to construct non-stationary deterministic consumption sequences for which the discounted sum ==== does not even converge as ====.====An interesting feature of our result is that the limiting Epstein–Zin utility does ==== depend on the timing of resolution of uncertainty. This is in contrast with the case of a fixed discount factor. Consider, again, Process ==== with a fixed discount factor. The Epstein–Zin utility in this case will be different under: (1) the standard information structure, where only past consumptions are observed, and (2) the finest information structure, where the entire future path of consumptions is revealed in period 1. This feature of the Epstein–Zin utility was highlighted by Epstein et al. (2014), who considered a number of calibrated macro-finance models. Under the usual assumption that ====, the agent will display a preference for early resolution of uncertainty. They find that, although information has no instrumental value, agents would be willing to give up a substantial portion of their future income to have the uncertainty resolved early. Our result implies that this does not occur under a conditionally i.i.d. process and an increasingly patient agent.====The Epstein–Zin utility belongs to the class of models developed by Kreps and Porteus (1978). Although this utility is extensively used in empirical work, the literature on its theoretical properties is relatively small. Within this literature, we make use of the results of Marinacci and Montrucchio (2010) on the existence, uniqueness, and continuity of the Epstein–Zin utility. Bommier et al. (2017), who characterize ordinal monotonicity for recursive utilities, show that the Epstein–Zin utility is not ordinally monotone. We further discuss the connection with their work below. Finally, the present paper is motivated by our earlier work, Al-Najjar and Shmaya (2015), where we characterize the implications of disagreement and parameter uncertainty in dynamic stochastic models. In the language of that paper, the parameter ==== in (1) above represents the attitude to structural uncertainty.",Recursive utility and parameter uncertainty,https://www.sciencedirect.com/science/article/pii/S0022053118302874,May 2019,2019,Research Article,278.0
Herrenbrueck Lucas,"Simon Fraser University, Canada","Received 3 January 2017, Revised 2 February 2019, Accepted 6 February 2019, Available online 14 February 2019, Version of Record 18 February 2019.",https://doi.org/10.1016/j.jet.2019.02.003,Cited by (18),.,"Monetary policies all over the world tend to be implemented via open-market operations in financial markets. Yet, most of monetary theory has focused on ‘helicopter drops’ where money is given directly to households – or, if open-market operations were considered, these took place in frictionless markets with instantaneous effects and no distributional consequences. But no real-world market is perfectly frictionless, and for several notable recent examples of asset purchases, such as the quantitative easing rounds in Japan, the United States, and the Eurozone, buying up illiquid assets was the point. So in order to obtain a more robust understanding of the effects of monetary policies, I construct a model where: (i) money can be introduced either via helicopter drops or via open-market purchases; (ii) such purchases take place in frictional markets for illiquid assets; and (iii) due to market frictions and household heterogeneity, money takes time to percolate through the economy, resulting in both short-run and long-run real effects.====Specifically, I model households as transitioning between ‘saver’ and ‘spender’ states at random times; only spender households can purchase goods with money. Bonds are illiquid in the sense that they cannot be used as media of exchange, and trade in financial markets is subject to frictions. Still, bonds have a positive yield while money does not; thus, savers value bonds more than money and spenders do the opposite. If they could, they would meet up and trade until spenders hold all the money and savers hold all the bonds. But due to the trading frictions in the asset market, this cannot be done quickly enough.====The main result of the paper is therefore that the distribution of real balances matters, and not just in the short run after an intervention. Since money held by savers is idle and does not get spent on goods (which is how the prices of goods are determined), the overall ==== of real balances is affected by their ====. This has consequences. For one, money injections are not neutral in the short run; for another, their effects depend on ==== the money is injected. On average, spenders hold more money than savers (as they should); thus, a helicopter drop that gives the same amount of cash to everybody shifts purchasing power away from spenders and towards savers. The result is a drop in production (due to lower demand) and a rise in asset prices (due to higher demand). This is exactly the opposite of the short-run effects of an open-market purchase in the frictional asset market: since households with a spending opportunity are the natural sellers of assets, open-market purchases make it easier for them to obtain the cash that they want. Thus, they stimulate the demand for goods at the expense of the demand for assets (net of the purchases themselves) along the transition to the new steady state.====Thus, while helicopter drops and open-market purchases are both “money injections” that increase the long-run prices of goods and assets, they are fundamentally different in the short term. In the long term, their effects point in the same direction, and their magnitude depends only on the ultimate ratio of bonds to money achieved by the intervention. Because the bonds perform a useful service in this economy – they help direct money into the hands of those who need it the most – lowering their supply reduces output.====While a purchase of illiquid bonds generally has the expected effects – higher bond prices due to scarcity, and at least temporary inflation due to more money in circulation – there is an important exception. If bonds are so scarce that their marginal buyer is rationed (a liquidity trap), and if fiscal policies are fixed in real terms, then the purchase may result in ==== in the long run.==== The reason for this surprising result is that the velocity of money is endogenous and an upward-sloping function of the bond supply inside the liquidity trap. Removing bonds from the market results in potential bond buyers holding on to their money for longer, a textbook increase in “money demand” that defeats the intent of the expansion of money supply. In contrast to almost all formal models of liquidity traps, here interest rates are positive throughout since the bonds are long-term.====My model is a hybrid of a New Monetarist model in the tradition of Lagos and Wright (2005) (surveyed by Nosal and Rocheteau, 2011, and Lagos et al., 2017) and a model of frictional asset markets in the tradition of Duffie et al. (2005) (DGP, henceforth). The most closely related New Monetarist model is the one of Rocheteau et al. (2015) (RWW, henceforth); from it, I adopt the monetary environment and the structure of goods and labor markets. I introduce two innovations: first, households are heterogeneous in how soon they expect to need money, and second, there are financial assets in addition to money which can only be traded in frictional asset markets. The structure of these asset markets is adapted from DGP. Compared to that paper, there are three innovations: first, assets are traded for money rather than transferable utility; second, random liquidity needs provide a microfoundation of DGP's exogenous asset valuation shocks (as originally suggested by DGP); and third, all assets are perfectly divisible.====The literature which applies insights from monetary theory to asset pricing is extensive and falls into two broad groups. In the first, financial assets directly compete with money as the medium of exchange (Geromichalos et al., 2007; Lagos, 2010, Lagos, 2011; Rocheteau and Wright, 2012); hence, this can be called the “direct liquidity” approach. Here, however, I study the pricing of an asset that cannot be used in exchange but has endogenous liquidity properties because it can be traded for money in an asset market. Thus, it inherits ‘moneyness’ from the fact that people who anticipate needing money in the future know that they can ==== their other asset when the need arises. This “indirect liquidity” approach unites the second group of papers, including Geromichalos and Herrenbrueck (2016a), Lagos and Zhang (2015), Berentsen et al. (2014), Mattesini and Nosal (2016), Herrenbrueck and Geromichalos (2017), and Huber and Kim (2017) (with Berentsen et al., 2007, and Berentsen and Waller, 2011, as precursors).====As is well-known, open-market operations are neutral when conducted by swapping liquid money for illiquid bonds in frictionless markets and fiscal implications are sterilized (Sargent and Smith, 1987). But they can have real effects on the economy when any of these conditions is not satisfied; indeed, in this paper, I study interventions in frictional markets for imperfectly liquid bonds throughout, and allow fiscal implications for a sub-section of my analysis. Previous New Monetarist work has also studied open-market operations in environments where one or two of these conditions was relaxed, but not all three. Williamson (2012) and Rocheteau et al. (2018) also study purchases of partially liquid bonds but model them as happening in a frictionless market. Neither paper looks at short-run dynamics. Andolfatto and Williamson (2015) and Geromichalos and Herrenbrueck (2017) do also study short-run dynamics, and they add an extra wrinkle: they identify the yield on the partially liquid bond as the main monetary policy instrument, so that the monetary authority “sets” this yield and conducts open-market operations to achieve it in the background.====The argument that frictions in portfolio management are the source of monetary non-neutrality and make intervention effective has a long tradition in monetary theory (Baumol, 1952; Tobin, 1956), continued by Alvarez and Lippi, 2009, Alvarez and Lippi, 2013. This approach evolved into the “limited participation” literature, in which not all agents participate in asset markets, and some agents face spending or borrowing constraints (Fuerst, 1992; Alvarez et al., 2002; Williamson, 2006). One closely related paper is by Alvarez and Lippi (2014), who also study the interaction of asset market frictions, portfolio heterogeneity, and monetary policy, and derive some similar results (e.g. the short-term liquidity effect of money on interest rates). There are two main differences. First, in Alvarez and Lippi (2014), assets enter the utility function (and key results are derived from the shape of that function) whereas in my paper, money has value because it is the medium of exchange (and key results are derived from the properties of the asset markets). Second, Alvarez and Lippi (2014) analyze an endowment economy and focus on stationary correlations between financial variables, whereas I model a production economy and focus on the effects of a one-time intervention.====Finally, in its emphasis on household heterogeneity as a driver of dispersed portfolios and ongoing asset trade, this paper also continues a tradition going back to Bewley, 1980, Bewley, 1983, Scheinkman and Weiss (1986), and Kehoe et al. (1992) (all of which are single-asset models where the only policies considered are lump-sum interventions). A recent contribution by Cúrdia and Woodford (2011) has in common with my paper that households are heterogeneous and differ in their demand for liquidity; they model it as patience shocks that make some households want to borrow, whereas I model it as random differences in how soon households expect to need money.====The paper is organized as follows. Section 2 develops the model; Section 3 solves for and characterizes its equilibria. In Section 4, the model is used to analyze monetary neutrality, the long-run and short-run liquidity channel, and the liquidity trap. Section 5 concludes. The Appendix A contains proofs and further details.",Frictional asset markets and the liquidity channel of monetary policy,https://www.sciencedirect.com/science/article/pii/S0022053119300134,May 2019,2019,Research Article,279.0
"Robson Arthur J.,Samuelson Larry","Department of Economics, Simon Fraser University, Burnaby, BC, V5A 1S6, Canada,Department of Economics, Yale University, New Haven, CT 06520, USA","Received 26 February 2018, Revised 27 December 2018, Accepted 27 January 2019, Available online 13 February 2019, Version of Record 18 February 2019.",https://doi.org/10.1016/j.jet.2019.01.006,Cited by (11),"We examine the evolutionary foundations of risk attitudes in age-structured populations. The effect of idiosyncratic risk concerning fertility or mortality rates is captured by the corresponding mean fertility or mortality rate. The effect of aggregate risk, relative to the mean, varies with the type of risk and age. We establish conditions under which aggregate risk in ","A fruitful approach to sharpening our understanding of preferences is to consider their evolutionary foundations (see Robson and Samuelson, 2011 for a survey). We consider in this paper how aggregate and idiosyncratic risks may have distinct implications for risk aversion.",Evolved attitudes to idiosyncratic and aggregate risk in age-structured populations,https://www.sciencedirect.com/science/article/pii/S0022053119300109,May 2019,2019,Research Article,280.0
Dindo Pietro,"Dipartimento di Economia, Università Ca' Foscari Venezia, Italy","Received 10 March 2017, Revised 20 January 2019, Accepted 1 February 2019, Available online 7 February 2019, Version of Record 13 February 2019.",https://doi.org/10.1016/j.jet.2019.02.002,Cited by (17),"In this paper, I consider an ==== with complete markets where agents have heterogeneous beliefs and, possibly, preferences, and investigate the Market Selection Hypothesis that speculation rewards the agent with the most accurate beliefs. First, on the methodological level, I derive the relative consumption dynamics as a function of agents' effective discount factors, related to consumption decisions across time, and agents' effective beliefs, related to consumption decisions across states. Sufficient conditions for agents' survival, either in isolation or in a group, depend on the relative size of effective discount factors and on the relative accuracy of effective beliefs. Then, I show that in economies where agents maximize an Epstein–Zin utility the Market Selection Hypothesis fails: there exist parametrizations where the agent with correct beliefs vanishes and parametrizations where beliefs heterogeneity persists in the long run. Results are robust to local changes of beliefs, risk preferences, and the aggregate endowment process. These failures are shown not to occur when agents' Epstein–Zin utility has a subjective expected utility representation due to an interdependence of effective discount factors and effective beliefs.","The dominant academic view of financial markets is that they facilitate hedging and risk diversification. A complementary view is that trade also occurs due to investors' disagreement about the return distribution of assets. Indeed, also in standard models of financial markets such as Lucas' model or the CAPM, agents' beliefs heterogeneity makes them willing to hold risky positions different from those they would have held under pure hedging. These positions are speculative, in that they include a bet on future realizations of assets' fundamentals.====Although speculative motives are certainly present, a widespread position of financial economists is that speculation can be only a transient characteristic of markets. The Market Selection Hypothesis (MSH) of Friedman (1953) applied to financial markets presumes that investors with accurate beliefs can earn high returns by taking positions against investors with inaccurate beliefs. These speculative positions should thus allow accurate traders to dominate and bring asset prices at the fundamental value implied by their beliefs. Indeed, when markets are complete, each agent can freely trade to allocate her future consumption on the paths which she believes more likely. In equilibrium, everything else being equal, the agent with the most accurate beliefs assigns the highest likelihood to paths that are actually realized and, thus, should hold everything in the long run. In bounded economies with time-separable Subjective Expected Utility (SEU) maximizers, the argument is rigorously established by Sandroni (2000) and Blume and Easley (2006). However, despite the importance of the result, the exact nature of the role of risk and time preferences in its validity is still unclear. Moreover, a number of contributions show that outside the SEU framework with bounded aggregate endowment the MSH may fail.====In this paper, I investigate the effect of speculation on agents' relative consumption dynamics by identifying the separate roles of optimal consumption decisions across time and states. The contribution is twofold. Firstly, on the methodological side, I derive the relative consumption dynamics as a function of agents' effective discount factors, related to consumption decisions across time, and agents' effective beliefs, related to consumption decisions across states. Sufficient conditions for agents' survival, either in isolation or in a group, depend on the relative size of a survival index that takes into account the size of the effective discount factor and the accuracy of effective beliefs. As the intuition above suggests, the agents with the most correct beliefs are still those who, assigning the highest likelihood to paths that are actually realized, do hold everything in the long run. However, the beliefs that matter are the effective ones, and those are endogenous, in that, depending on risk preferences, they also incorporate equilibrium prices.====Secondly, and related to the MSH, I show that when, for at least one agent, consumption decisions across time and states are not interdependent, e.g. when there is at least an agent who maximizes a recursive Epstein–Zin utility, the MSH fails: there exist economies where agents with heterogeneous beliefs survive in the long run, or others where the agent with correct beliefs vanishes on a set of paths with full measure, even when risk and time preferences are homogeneous. Path dependency, with an agent dominating on some paths and another on others is also a possible long-run outcome. Results are robust to local changes of beliefs, risk preferences, and the aggregate endowment process. These failures are shown not to occur when the Epstein–Zin utility has a subjective expected utility representation, due to the interdependence of consumption decisions across time (effective discount factors) and states (effective beliefs).====After having introduced the general set-up (Section 2), the methodological contribution is developed in Section 3 where I focus on 2-agent economies. The full analysis of ====-agent economies proceeds along the same lines but, being slightly heavier on the formal side, is postponed to Section 5.====In Section 3, I show that the dynamics of consumption shares depends on two key quantities: the ratio between the current value of next period consumption and current consumption, which accounts for optimality across time, and the ratio between the current value of next period consumption in each state and the current value of next period total consumption, which accounts for optimality across states. For an SEU maximizer with log Bernoulli utility, the first ratio (time dimension) is equal to her discount factor while the second ratio (states dimension) is equal to her beliefs. More generally, these ratios, which can be derived from the Euler equations, depend also on equilibrium state prices and time/risk preferences. Yet, they can be interpreted as an effective discount factor (for the time dimension) and effective beliefs (for the states dimension). In fact, they lead to the same consumption decision of an SEU-log agent who uses them, respectively, as discount factor and beliefs.====Sufficient conditions for an agent to survive, dominate, or vanish can be written in terms of the log of effective discount factors and accuracy of effective beliefs. I name the sum of these two terms the generalized survival index. The size of the effective discount factor matters for survival because it is related to saving and depends, other than on the discount factor, on the intertemporal elasticity of substitution (IES) and on the perceived mispricing. A unitary IES implies that perceived mispricing has no effect on saving, since income and substitution effect compensate each other. Larger (smaller) IES encourages (depresses) saving in the presence of perceived mispricing. The larger the mispricing, the larger the effect.====The accuracy of effective beliefs matters as well because it is related to the agent portfolio expected log-returns and is shown to depend both on the accuracy of beliefs and on an endogenous component that I name Non-Log-Optimality (NLO) term. Unless for agents with unitary relative risk aversion (RRA), who hold log-optimal portfolios, the NLO term is endogenous (it depends, through risk preferences, on state prices) and measures the relative accuracy of effective beliefs and beliefs. An agent with correct beliefs and non-unitary RRA has always a negative NLO term, because effective beliefs are less accurate than beliefs. An agent with non-correct beliefs may have instead a positive (negative) NLO term when beliefs and preferences imply effective beliefs that are closer (further away) to the truth than beliefs. A typical case of a positive NLO term is when optimism and risk aversion compensate each-other. Failures of the MSH depend on the interplay of this NLO contribution with the belief dependent component of the effective discount factor.====Examples of failure of the MSH are presented in Section 4, where I consider specific parametrizations of Epstein–Zin recursive utility. The examples provided are aimed to shed light on the separate role of effective discount factors (decision across time, saving) and effective beliefs (decision across states, portfolios) about MSH failures.====A parametrization which is particularly useful to clarify why the MSH can fail has all agents with unitary IES and equal discount factors, so that effective discount factors are homogeneous, but with different effective beliefs. In economies where all agents have RRA higher than 1 and there is aggregate risk, optimism may compensate risk aversion and the agent with non-correct beliefs may dominate due a high NLO term (Section 4.1). However, if optimism is too strong the outcome is long-run heterogeneity because each agent has a high NLO term, and also more accurate effective beliefs, at the state prices determined by the other agent. I provide examples of long-run heterogeneity in 2-agent economies in Section 4.2. The intuition for 2-agent economies works also for ====-agent economies as shown in the examples of Appendix A.3.====Path dependency occurs instead when both agents have an RRA lower than 1 and the non correct agent is a pessimist, because each agent typically has a negative NLO term, and also less accurate effective beliefs, at the prices determined by the other agent. I provide examples of path dependency in 2-agent economies in Section 4.3.====Finally, in Section 4.4, I consider examples where all agents maximize an SEU-CRRA utility. As established by the incumbent literature, provided preferences are homogeneous, the agent with correct beliefs dominates. My contribution is to show that the result holds because, the IES coefficient being the inverse of the RRA coefficient, the endogeneity of effective beliefs (portfolios) and of effective discount factors (saving) compensate each other and what matters is only the accuracy of beliefs and the size of discount factors. In these economies, it is enough to slightly change the preferences of one agent, so that her RRA is not equal to the inverse of her IES, to obtain MSH failures such as long-run heterogeneity.====Section 5 extends the methodological contributions to ====-agent economies. Appendix A presents further examples showing how failures are robust to local changes of beliefs, risk preferences, and the aggregate endowment process. Appendices B–D collect the proofs. In Appendix E, I compute effective discount factors and beliefs for general time-separable utility and, possibly, a non-linear probability weighting function. In the next section, I discuss the relation between my results and the incumbent literature.",Survival in speculative markets,https://www.sciencedirect.com/science/article/pii/S0022053119300122,May 2019,2019,Research Article,281.0
"Ke T. Tony,Villas-Boas J. Miguel","MIT, United States of America,University of California, Berkeley, United States of America","Received 1 July 2017, Revised 21 January 2019, Accepted 26 January 2019, Available online 5 February 2019, Version of Record 6 February 2019.",https://doi.org/10.1016/j.jet.2019.01.005,Cited by (50),". We show that the decision maker considers an alternative for learning or adoption if and only if the expected payoff of the alternative is above a threshold. Given both alternatives in the decision maker's consideration set, we find that if the outside option is weak and the decision maker's beliefs about both alternatives are relatively low, it is optimal for the decision maker to learn information from the alternative that has a lower expected payoff and less uncertainty, given all other characteristics of the two alternatives being the same. If the decision maker subsequently receives enough positive informative signals, the decision maker will switch to learning the better alternative; otherwise the decision maker will rule out this alternative from consideration and adopt the currently more preferred alternative. We find that this strategy works because it minimizes the decision maker's learning efforts. We also characterize the optimal learning policy when the outside option is relatively high, and discuss several extensions.","In many circumstances, agents have the opportunity to learn sequentially about multiple different alternatives before making a choice. Specifically, consider a decision maker who is deciding among several alternatives with uncertain payoffs and an outside option with a known payoff. Before deciding which one to adopt, he can gather some information on each alternative. After learning more about an alternative, the decision maker gains more precise knowledge about its payoff, with initial learning being more informative than later learning. It is costly to gather and process information. Therefore, at some point, the decision maker will decide to stop learning and make a choice on which alternative to adopt or to take the outside option.====For example, consider a consumer in the market for a car. The consumer could first learn information about a certain model A, then choose to get information about model B, then go back to get more information on model A again, and so on, until the consumer decides to either purchase model A, model B, or some other model, or not to purchase a car for now. With the recent development of information technologies, it becomes more and more important to understand this information gathering behavior of consumers.====Information gathering is not unique to the consumers' purchase process. In fact, many other important economic activities involve similar costly gradual information acquisition: companies allocating resources to R&D, individuals looking for jobs, firms recruiting job candidates, politicians evaluating better public policies, manufacturers considering alternative suppliers, etc.====We study a decision maker's optimal information gathering problem under a Bayesian framework. We consider a setting in which the payoff of each alternative follows a two-point distribution, being either high or low. The decision maker has a prior belief on the probability that an alternative is of high payoff. Each time he learns some information about an alternative, he incurs a cost and gets a noisy signal on its payoff. This signal does not reveal the payoff of this alternative completely; rather, it updates the decision maker's belief of the distribution of the payoff. The decision maker could buy another signal on the same alternative if he would like to learn more about it or, alternatively, he could buy signals on other alternatives. In this setting the precision of the decision maker's belief is a function of his belief. Therefore, we only need to account for one state variable per alternative during the learning process—the probability that each alternative is of high payoff. Despite its simplicity, this setup captures the ideas that the decision maker gains more precise information during the learning process, and that the marginal information per learning effort decreases with the cumulative information gathered so far on an alternative. This two-point distribution assumption also implies that the agent becomes more certain about the payoff of an alternative when the payoff is either relatively high or low. The decision maker solves the problem of optimal dynamic allocation of learning efforts as well as optimal stopping of the learning process, so as to maximize the expected payoff.====We consider a two-alternative continuous-time model with infinite time horizon, which enables a time-stationary characterization of the optimal learning strategy when the outside option takes relatively large or small values.==== The result is a partition of the belief space into regions, within which it is optimal to learn each alternative, adopt each alternative, or take the outside option.====We find that if the belief on an alternative is below a threshold, the decision maker will never consider that alternative—he will neither learn nor adopt that alternative. This then provides an endogenous formation of a consideration set, given the decision maker's belief on the payoff distribution, his learning costs, the value of the outside option, and the noisiness of the signals received. If two alternatives have the beliefs above that threshold, then the decision maker chooses to stop learning and adopt one alternative if the expected payoff of that alternative is sufficiently higher than that of the other alternative. That is, when the expected payoffs from the alternatives are not too different, the decision maker will continue to learn.====We also investigate the effect of the value of the outside option on optimal learning behavior. We find that when the value of the outside option is relatively high, the decision maker always chooses to learn about the alternative with the higher expected payoff first. More interestingly, we find that this is not necessarily the case when the value of the outside option is sufficiently low. In this case it may be optimal to first learn about the alternative that has a lower expected payoff, all else being equal among alternatives, with the purpose of possibly ruling it out early. If the decision maker receives sufficiently poor signals on this alternative, he will stop learning and immediately adopt the other alternative. We find this strategy works because it saves learning costs.====It is interesting to understand the intuition about why a decision maker's optimal learning strategy depends on the value of the outside option. When the outside option has a relatively high value, it will be relevant for the decision maker's ultimate choice at the end of the learning process. In this case, by learning the alternative with a higher expected payoff, the decision maker keeps both the outside option and the other alternative as reservation options. Therefore, the decision maker prefers to learn the alternative with higher expected payoff. On the other hand, when the outside option has a relatively low value, it is no longer as relevant for the decision maker's ultimate choice. In this case, the decision maker is basically deciding between the two uncertain alternatives, and it may be better to learn about the alternative with a lower expected payoff first to potentially rule it out early, so as to make a clear distinction between the two uncertain alternatives with minimum learning effort.====We also compute the decision maker's likelihood to adopt each alternative given his current belief, when he follows the optimal learning strategy. We find that the probability of choosing either one of the alternatives can fall as there are more alternatives available. This is because having more alternatives makes it harder for the decision maker to make a choice, and thus leads to more learning, which can ultimately result in no adoption of any of the alternatives. Specifically, information is ex ante neutral. It is possible that with more learning, the decision maker gets positive information on either alternative, in which case he will, at best, adopt one of the alternatives. On the other hand, it is also possible that with more learning, the decision maker gets negative information, in which case he may choose to take the outside option. We also compute the decision maker's expected probability of being correct ex post given his current belief, when he follows the optimal learning strategy. Finally, we consider the optimal learning problem when alternatives have heterogeneous learning costs or payoff distributions, when the number of alternatives is greater than two, and when there is time discounting instead of learning costs.====The problem considered here is related to the multi-armed bandit problem (e.g., Rothschild 1974a; Gittins 1979; Whittle 1980; Bergemann and Välimäki 1996; Felli and Harris 1996; Bolton and Harris 1999), where a decision maker learns about different options by trying them, one in each period, while earning some stochastic rewards along the way. That problem has an elegant result that the optimal policy is to choose the arm with the highest Gittins index, which for each arm only depends on what the decision maker knows about that arm until then. However, the problem considered here is different from the bandit problem in one major aspect. In our setting, a decision maker optimally decides when to stop learning and make an adoption. Therefore, the decision horizon is endogenous, and optimally determined by the decision maker. In contrast, multi-arm bandit problems generally presume an exogenously given decision horizon, which could be either finite or infinite. In fact, our problem belongs to a general class of the ====, first introduced by Glazebrook (1979), which generalizes simple bandit processes to allow for two arms per bandit. By adding a second “stopping arm” to each bandit, extra payoff can be generated when the stopping arm is pulled. In general, index policies are not optimal for this type of problems (Gittins et al. 2011, Chapter 4). Glazebrook (1979) shows that an index policy can be optimal under certain conditions. However, as shown below, Glazebrook's sufficient conditions are not satisfied in our setting, and the index policy is sub-optimal. We will compare our optimal policy with that of Glazebrook (1979) below. Forand (2015) considers a revised multi-armed bandit problem where the decision maker must incur maintenance costs to keep an alternative available for choice and any positive signal is fully revealing. Forand finds that it may be optimal to first try the worst alternative to rule it out, and then continue learning the other alternative. In contrast, we consider a decision maker that receives gradual signals on an alternative, either positive or negative. If he receives enough negative signals on the worst alternative, the decision maker chooses the other alternative immediately, without learning more about it.====The literature on search theory is also related to the results presented here. Although the problem considered here is central to choices in a market environment, it is quite under-researched when all of its dimensions are included. For the simpler case in which all learnable information about an alternative can be learned in one search occasion, there is a large literature on optimal search and some of its market implications (e.g., McCall 1970; Diamond 1971; Weitzman 1979; Doval 2014). This literature, however, does not consider the possibility of gradual revelation of information throughout the search process. There is also some literature on gradual learning when a single alternative is considered or information is gathered to uncover a single uncertain value (e.g., Roberts and Weitzman 1981; Moscarini and Smith 2001; Branco et al. 2012; Fudenberg et al. 2015),==== and the choice there is between adopting the alternative or not. In the face of more than one uncertain alternative (as is the case considered in this paper) the problem becomes more complicated. This is because opting for one alternative in a choice set means giving up potential high payoffs from other alternatives about which the decision maker has yet to learn more information. This paper can then be seen as combining these two literatures, with gradual search for information on multiple alternatives. Kuksov and Villas-Boas (2010) and Doval (2014) consider a search problem where everything that can be learned about one alternative is learned in one search occasion, but the decision maker can choose an alternative without search. This feature is also present here by including an outside option and allowing for adoption of an alternative without gaining full information about it. Doval, in particular, finds that in this case the decision maker may be indifferent to search an alternative that does not have the highest reservation price, which can be seen as related to the case in this paper of choosing to learn an alternative other than the best one under some conditions. Different from Doval, this paper considers gradual learning of each alternative, and we find that it could be strictly optimal to learn an alternative that has both lower expected payoff and lower uncertainty.====Ke et al. (2016) considers a stationary problem of gradual search for information with multiple products, where the precision of beliefs does not increase with search, and where earlier learning is not more informative than later learning. One important result in this paper is that it can be optimal to learn about an inferior alternative, which is never optimal in Ke et al. (2016). Another important difference is that the possible payoffs are bounded here, while they are unbounded in Ke et al. (2016).==== The problem considered here also relates to the literature studying search while learning the payoff distribution (e.g., Rothschild 1974b; Adam 2001), but there what can be learned about each alternative is learned in one search occasion, so there is no gradual learning on each alternative.====The remainder of the paper is organized as follows. Section 2 presents the optimal learning problem. Section 3 solves the problem and presents the optimal learning strategy, and Section 4 looks at the implications of the results for adoption likelihood and probability of being correct. Section 5 presents several extensions of the main model, including asymmetric alternatives, the effect of the number of alternatives, and time discounting. Section 6 compares our optimal learning strategy with Weitzman (1979) and Glazebrook (1979), and presents concluding remarks. All technical proofs are presented in the Appendix.",Optimal learning before choice,https://www.sciencedirect.com/science/article/pii/S0022053119300092,March 2019,2019,Research Article,282.0
"Hu Tai-Wei,Shmaya Eran","University of Bristol, United Kingdom of Great Britain and Northern Ireland,Northwestern University, United States of America","Received 3 February 2018, Revised 15 January 2019, Accepted 23 January 2019, Available online 29 January 2019, Version of Record 1 February 2019.",https://doi.org/10.1016/j.jet.2019.01.003,Cited by (4),". The government increases the money supply at a constant growth rate that induces ==== in a stationary monetary equilibrium. We identify the necessary and sufficient condition for a stationary monetary equilibrium (where money has a positive value and the aggregate real balance is constant over time) to exist, and, when it exists, we show that it is unique. The argument for uniqueness is based on a new monotonicity result for the average ====.","The Bewley–Aiyagari model (Bewley, 1986, Aiyagari, 1994, and Huggett, 1993) has become the standard environment to study heterogenous agents and endogenous distribution of asset holdings of various kinds. One use of such models is to study the welfare cost of inflation (see, for example, Imrohoroglu, 1992), and this is typically done within the class of stationary equilibrium with a constant real balance. It has been demonstrated (Green and Zhou, 2005 and Wallace, 2014) that the distributional aspect of monetary policy in these models is essential in determination of the optimal inflation rate. However, this endogenous distribution of money holdings also causes difficulty in deriving equilibrium existence and uniqueness. While equilibrium existence has been established, there are no general theoretical results for uniqueness of a stationary equilibrium in this type of model.====In this paper we prove equilibrium existence and uniqueness in a pure currency economy under the Bewley–Aiyagari environment. In this model, there is a single perishable consumption good and a population of infinitely lived agents who face idiosyncratic endowment shocks at each period but may trade their endowments for money. The government increases the money supply at a constant growth rate in a lump-sum manner. We focus on stationary equilibria. In contrast to similar models in which the (rental) price of capital is determined by current aggregate capital holdings, in our economy the price of money also depends on its future prices and hence can only be determined endogenously. An equilibrium is ==== if money is valued in equilibrium. Stationarity requires two endogenous variables to be constant over time: first, the total real value of money (and, hence, when money supply grows at a positive rate, there is inflation); second, the distribution of money holdings. We give the necessary and sufficient condition for a monetary equilibrium to exist. More importantly, we show that when it exists, it is also unique.====In this environment, an agent's optimal money holding depends on his endowment shock and his previous money holding, and hence follows a first-order Markov process. Under a constant money supply, equilibrium is determined by the unique invariant distribution of this Markov process. When money supply grows, however, the real value of the lump-sum transfer is also an equilibrium object, and market clearing gives an equilibrium condition that relates the average consumption under the equilibrium distribution and the real transfer. Uniqueness of equilibrium would follow if the average consumption is monotonic in the real transfer. The standard technique is to show that optimal consumption increases with the real transfer in the individual dynamic programming problem, which does not hold in general, however. Instead, we develop a new technique that directly demonstrates that average consumption increases with the real transfer. Our arguments rely on the ergodic theorem, which relates the long-run average of consumptions from an “typical” individual realization of endowment shocks to the average consumption of the stationary cross-section distribution of consumption. While similar existence results have been established in earlier papers, our uniqueness result is new.====Our uniqueness result implies real determinacy among stationary monetary equilibria. This is in contrast with the indeterminacy result in Green and Zhou (1998), which finds a continuum of stationary monetary equilibria in the context of a Kiyotaki and Wright (1989) model but with divisible money holdings and indivisible goods, and under double auctions. Moreover, since the unique equilibrium is upper hemi-continuous in inflation rate, the equilibrium ==== is also continuous. This implies that an optimal inflation rate exists, and the literature has provided examples in which such a rate is strictly positive, such as Green and Zhou (2005).==== In contrast with the Lagos and Wright (2005) model, in which the unique stationary monetary equilibrium features a degenerate distribution and inflation through lump-sum transfers is never optimal, in our model a monetary equilibrium always features a non-degenerate distribution and inflation can be optimal.====More broadly speaking, our paper contributes to a recent literature on the existence and uniqueness of equilibria in Bewley–Aiyagari models. Açikg̈oz (2016) proves the existence of a stationary equilibrium in such a model with capital accumulation, but, in contrast to our pure-currency economy, he also demonstrates that there can be multiple stationary equilibria. As pointed out there, one reason for this multiplicity is the income effects of a higher rate-of-returns on assets, and Lehrer and Light (2016) give a sufficient condition on the underlying utility function for the substitution effect to dominate the income effect; Light (2017) uses that condition to obtain uniqueness.====Finally, our uniqueness result allows for unambiguous comparative statics, and one can apply existing comparative-statics results to our setup. In particular, although our setup is not a special case of that considered in Acemoglu and Jensen (2015), we can readily translate their results in our setup. For example, given their results, it is easy to show that an increase in the discount factor will lead to an increase in the equilibrium real balances in our setup.",Unique monetary equilibrium with inflation in a stationary Bewley–Aiyagari model,https://www.sciencedirect.com/science/article/pii/S0022053119300079,March 2019,2019,Research Article,283.0
"Āzacis Helmuts,Vida Péter","Cardiff University, Cardiff Business School, Aberconway Building, Colum Drive, Cardiff CF10 3EU, Wales, UK,THEMA (UMR CNRS 8184), Université de Cergy-Pontoise, UFR d'Economie et Gestion, 33, boulevard du Port, 95011 Cergy-Pontoise Cedex, France","Received 15 December 2016, Revised 5 January 2019, Accepted 20 January 2019, Available online 25 January 2019, Version of Record 28 January 2019.",https://doi.org/10.1016/j.jet.2019.01.002,Cited by (1),We characterize the ,"Implementation theory studies objectives that society can achieve when its members behave strategically. Most of the literature in this field assumes that the implementation of an objective is a one-off event. (The seminal paper here is Maskin (1999).) However, the majority of interactions within society are repeated, with the objectives often remaining the same over time. The repeated nature of interactions can drastically change what can be implemented: objectives that are one-shot implementable might not be repeatedly implementable, and vice versa (see, for example, Examples 1 and 2 in Mezzetti and Renou (2017)). Our goal is to study what can be repeatedly implementable.====Specifically, we consider an infinite horizon problem when a new state of the world is realised in each period (i.i.d.). A social designer wants to select an outcome in each period, which depends on that period's state. However, the realised states are only observed by agents and never by the designer. Therefore, the designer must construct a sequence of mechanisms, referred to as a regime, that would elicit the state of the world from the agents and, at the same time, implement the desired outcome in each period.====Our objective is to characterize the social choice functions, that is, mappings from states of the world into outcomes, that are repeatedly implementable in Nash equilibrium. A function is repeatedly implementable if there exists a regime such that the set of Nash equilibria of the repeated game is non-empty and, for any sequence of realized states, the sequence of outcomes in any Nash equilibrium is such that in each period, the outcome coincides with the socially desired one.====This problem has been recently studied by Lee and Sabourian (2011) and Mezzetti and Renou (2017).==== Lee and Sabourian (2011) show in their Theorem 1 that if a social choice function is not weakly efficient in its range, then the function is not repeatedly implementable for sufficiently high discount factors. They also show in their Theorem 2 that strict efficiency in the range, together with some additional assumptions on the preferences (their Assumption ==== and Condition ====), are sufficient for outcome implementation from period 2 onwards. In turn, Mezzetti and Renou (2017) show in their Theorem 1 that a complex set of dynamic incentive constraints, called dynamic monotonicity (DM), must hold if the function is repeatedly implementable. Furthermore, in their Theorem 2, they show that dynamic monotonicity plus no-veto power (or their Assumption ====) are sufficient for repeated implementation both in finite and infinite horizon problems, irrespective of the magnitude of the discount factor.====While it is easy to check the efficiency of a function, there are many functions that are repeatedly implementable for a fixed discount factor, but are not efficient, and vice versa. On the other hand, due to its inherent complexity, checking DM can be a daunting task. Moreover, there are important functions, which do not satisfy no-veto power but are repeatedly implementable. To summarize, up to now we do not have a complete characterization of the repeatedly implementable functions. Furthermore, we want a practical and systematic way to verify any necessary and sufficient conditions for repeated implementation.====We now describe our main contributions. First, in Theorem 1, we provide an alternative characterization to the necessary DM condition.==== Namely, given the implementation environment, we construct an associated repeated game and show that a social choice function is DM if and only if: (a) it satisfies a simpler dynamic monotonicity condition, which we call Maskin monotonicity* (MM*); and (b) the associated repeated game has a ==== equilibrium payoff, which is equal to the value of the function. In some respects, this result provides a connection between the results of Lee and Sabourian (2011) and Mezzetti and Renou (2017), as we show that an appropriately defined efficiency of social choice function is a necessary condition for repeated implementation for all values of the discount factor.====MM* requires that whenever agents jointly lie about the state of the world in a certain period only, and then from the next period on they report honestly, there exists an agent who has incentives to deviate. Hence, MM* is described by only finitely many incentive constraints. It is also implied by both Maskin monotonicity and DM. At the same time, the equilibrium payoff set of the associated repeated game can be calculated using the dynamic programming technique that has been developed by Abreu et al. (1990). Therefore, the characterization of DM in Theorem 1 offers a practical and systematic way to test DM.====Second, we prove in Theorem 2 that DM is not only necessary but also sufficient for repeated implementation when there are at least three agents.==== When proving sufficiency, we face the following problem: we want an agent to report when others lie about the state of the world, but we do not want the resulting outcome to be an equilibrium. To ensure this, we need to slightly perturb the outcomes that this agent can induce with his claim. However, because DM involves the infinity of incentive constraints (as the lie can take place over many periods), it is not obvious that such a perturbation is always possible without violating some of these incentive constraints. To show that it is always possible, we use the characterization of DM in Theorem 1, and the fact that the equilibrium payoff correspondence of the associated repeated game is upper-semicontinuous. Intuitively, a small change in the outcomes that the dissenting agent can induce, does not affect the unique efficient equilibrium payoff of the repeated game.====Third, Theorem 1, Theorem 2 have an immediate implication for the social choice functions that are efficient in the sense of Lee and Sabourian (2011). They imply that an efficient function is repeatedly implementable if and only if it is MM* (see our Proposition 1). This result improves on Theorem 2 of Lee and Sabourian (2011) since we replace their unnecessary domain restrictions with the necessary MM* condition.==== We illustrate the practical importance of this result by applying it to utilitarian social choice functions, which are efficient, in the well-known economic environment of Laffont and Maskin (1982). These functions need not be Maskin monotonic, however we prove in Proposition 2 that they are MM* because the continuation payoffs effectively play the role of side-payments, which are usually needed for implementation in static setups. Furthermore, if all but one agent can have identical preferences, a utilitarian social choice function will typically not satisfy no-veto power. Consequently, one cannot apply the result of Mezzetti and Renou (2017) where no-veto power is assumed. This provides an example when closing the gap, between necessary and sufficient conditions, matters in practice.====Finally, we consider several extensions to Theorem 2. It has been established by assuming that the number of agents is at least three, that agents have strict preferences over alternatives, that the solution concept is pure-strategy Nash equilibrium and, most importantly, that the designer can use random stage mechanisms. We provide additional conditions under which our results extend to the case of two agents and improve upon the corresponding results of Lee and Sabourian (2011) and Mezzetti and Renou (2017). We discuss the case of weak preferences and also argue that our results extend to both mixed-strategy Nash implementation and pure-strategy subgame perfect implementation. We conjecture, however, that the necessary and sufficient condition for mixed-strategy subgame perfect implementation requires more than DM if the designer can only use public communication channels. We briefly discuss how the possibility of private communication can help to implement any function that is DM in mixed-strategy perfect Bayesian equilibrium. Finally, we show that DM is not sufficient for repeated implementation when the stage mechanisms are required to be deterministic, and we introduce an additional condition that together with DM is both necessary and sufficient for repeated implementation in this case. This additional condition is reminiscent of part (ii) of Condition ==== in Moore and Repullo (1990).====The rest of the paper is organized as follows. In Section 2, we introduce the model and basic notation. In Section 3, we provide the definitions of MM* and DM. In Section 4, we introduce the associated repeated game, state Theorem 1, and demonstrate through an elaborate example how the DM of a function can be checked numerically. In Section 5, we state Theorem 2 and provide a regime that implements any social choice function, which is DM. In Section 6, we relate our results to Lee and Sabourian (2011) and show that efficiency in the range and the necessary condition of MM* are sufficient for implementation. As an application, we next prove that the utilitarian social choice functions are repeatedly implementable in the environment of Laffont and Maskin (1982). In Section 7, we discuss various extensions to Theorem 2. The proofs of Theorem 1, Theorem 2 can be found, respectively, in Appendices A and B. In Appendix C, we study in detail the case when only deterministic stage mechanisms are allowed. Appendix D contains the regime for repeated implementation in mixed-strategies. Finally, the detailed analysis of the two agent case, as well as the analysis of repeated implementation of efficient functions from the second period onwards, can be found in the Supplementary Material.",Repeated implementation: A practical characterization,https://www.sciencedirect.com/science/article/pii/S0022053119300067,March 2019,2019,Research Article,284.0
"Skowron Piotr,Faliszewski Piotr,Slinko Arkadii","University of Warsaw Warsaw, Poland,AGH University Krakow, Poland,University of Auckland Auckland, New Zealand","Received 11 August 2017, Revised 23 September 2018, Accepted 30 December 2018, Available online 9 January 2019, Version of Record 11 January 2019.",https://doi.org/10.1016/j.jet.2018.12.011,Cited by (16),"Committee scoring rules form a rich class of aggregators of voters' preferences for the purpose of selecting subsets of candidates of a given size. We provide an axiomatic characterization of committee scoring rules in the spirit of celebrated Young's characterization of single-winner scoring rules. We show that committee scoring rules are characterized by the set of four standard axioms: symmetry, consistency, continuity and Pareto dominance. In the course of our proof, we introduce and axiomatically characterize multiwinner decision scoring rules, a class of rules that generalizes the well-known majority relation.","We extend Young's celebrated characterization of single-winner scoring rules (Young, 1974) to the multiwinner setting. Specifically, we show that committee scoring rules, introduced by Elkind et al. (2017), are exactly those multiwinner rules that are symmetric, consistent, continuous, and satisfy Pareto dominance property.====In our model of ====, we are given a set of ==== candidates, a collection of voters with preferences over those candidates, and an integer ====. A multiwinner voting rule is based on an algorithm that allows us to compare any two subsets of candidates of size ====—referred to as ====—on the basis of preferences of the voters; we call such an algorithm a ====.==== A transitive decision rule is called a ====. In other words, given a profile of preferences, a multiwinner voting rule produces a weak linear order over the committees (and, in particular, identifies the best committees).====Multiwinner elections have recently been attracting attention due to a wide range of their applications. For example, they can be used for choosing a country's parliament and, indeed, the multiwinner voting rules of Chamberlin and Courant (1983) and Monroe (1995) were introduced exactly for this purpose, whereas Brill et al. (2017) have shown that various apportionment methods can be expressed in the language of multiwinner voting. However, multiwinner elections also have other applications, ranging from selecting finalists of competitions (Elkind et al., 2017), through solving certain resource allocation problems (Lu and Boutilier, 2011, Lu and Boutilier, 2015, Skowron et al., 2016a), to making internal decisions within combinatorial optimization algorithms (Faliszewski et al., 2016a, Pourghanbar et al., 2015). Committee scoring rules form a large and versatile class of multiwinner voting rules that can be fine-tuned to suit many different applications.====This paper is organized as follows. First, in Section 2, we provide formal background regarding multiwinner elections and committee scoring rules. In Section 3 we formally describe the axioms that we use and present our characterization results. Section 4 contains further discussions of our model. The proofs of our results are delegated to the appendix.",Axiomatic characterization of committee scoring rules,https://www.sciencedirect.com/science/article/pii/S0022053119300043,March 2019,2019,Research Article,285.0
"Giovannoni Francesco,Xiong Siyang","Department of Economics, University of Bristol, United Kingdom of Great Britain and Northern Ireland","Received 12 December 2017, Revised 6 November 2018, Accepted 30 December 2018, Available online 8 January 2019, Version of Record 18 January 2019.",https://doi.org/10.1016/j.jet.2018.12.009,Cited by (9),"We study the welfare effect of language barriers in communication. Specifically, we compare the equilibrium welfare in a game with language barriers to that in the equivalent game without language barriers. We show how and why language barriers may (weakly) improve welfare by providing two positive results. First, in a game with ==== language barriers, we prove that if we allow for ====-dimensional communication, ==== equilibrium outcome of the equivalent game without language barriers can be replicated. Second, for ==== payoff primitive, we provide a welfare ranking for several noisy-communication devices, including language barriers, that generalizes the results in ====. In particular, our results imply that there always exist ==== language barriers whose maximal equilibrium welfare (always weakly and sometimes strictly) dominates ==== noisy-talk equilibrium (and hence also any cheap-talk equilibrium) under no language barriers.","Communication is often about transmission of information, so that a natural question to ask is “what” information is actually transmitted and this has been the focus of the literature on strategic communication, or “cheap talk.” This literature, however, has typically ignored the issue of “how” information is transmitted. Yet, everyday experience suggests that how information is transmitted may both hinder or help communication. For instance, it is notoriously hard to convey humor or any other emotion in modern electronic communication, and emoticons were developed as a response to the problem (Curran and Casey, 2006). Similarly, there is a concern that patients may not be able to understand medical jargon, so that the common recommendation is not to release to patients their medical records or at least avoid jargon when this is likely to cause misunderstandings (see Ross and Lin, 2003 for a survey of the medical literature on this issue).==== In this paper, we take the “how” issue seriously, and study a model with “language types,” as introduced by Blume and Board (2013), which allows us to model the possibility of “language barriers” within a strategic communication setting.====In the canonical Crawford and Sobel (1982) sender-receiver setting, it is implicitly assumed that all participants have perfect language ability. In contrast to this standard framework, Blume and Board (2013) introduce “language types” for both sender and receiver which describe, respectively, the sets of messages that can be sent and that can be understood.==== This provides a parsimonious way to study a fundamental question: do language barriers improve or harm welfare or, equivalently, is equilibrium welfare under language barriers greater or smaller than that under no language barriers? We pursue this fundamental question in two directions. We first ask: is there a communication protocol that can guarantee that ==== language barriers won't impact negatively on welfare? We then ask: for ==== payoff primitive, can we find ==== language barriers that (weakly) improve equilibrium welfare, even if we stick to the canonical communication protocol? We provide positive answers to both questions.====Our first main result is inspired by a phenomenon that we observe in real-life communication, which is that messages are formed by combining basic units to make complex structures that convey meaning. Thus, it seems restrictive to assume a fixed number of messages in modeling communication, each message with a predetermined level of complexity, rather than assuming that such messages can always be used as building blocks capable of forming more sophisticated structures. The communication protocol in Blume and Board (2013) ==== forbids forming more complex structures than the 1-dimensional messages in a set ====, so we relax this assumption in the simplest way possible by assuming that the set of available messages extends to ==== (for some integer ====). Our first main result is that, under some minimal assumptions, any equilibrium which would obtain in a game with 1-dimensional communication and no language barriers can be replicated by an equilibrium of the same game if we add any language barriers (independent of payoff states) but allow for ====-dimensional communication (for sufficiently large ====), i.e., language barriers do not harm welfare under multi-dimensional communication.====To achieve this result, we need to overcome three technical difficulties under language barriers: (1) the sender may not know the receiver's language type; (2) the receiver may not know the sender's language type; (3) there may not be enough common messages (between sender and receiver) to transmit useful information. It is straightforward to see that multi-dimensional communication overcomes the third difficulty, because the set of common messages expands as we increase the dimension of messages. The novelty of our equilibrium construction comes from how it overcomes the first two difficulties, although it does so in a way that resembles real-life solutions to similar problems. In particular, in our equilibrium construction, a sender partitions her ====-dimensional message into several blocks of sub-messages, with each block intended for a specific language type of the receiver. We show that this overcomes the first difficulty just as businesses – which sell the same product in different countries – solve the problem of communicating with customers by producing an instruction manual with the same instructions written in all the relevant languages. In addition, in our construction, each block of sub-messages from the sender is further divided into two parts, with one part voluntarily revealing the sender's language type, and the other part transmitting payoff-relevant information (using type-specific common messages). The part where the sender's language type is revealed overcomes the second difficulty, just as people sometimes add a “smiley face” emoticon in an email message to ensure the content is not taken too seriously.====In the second part of the paper, we tackle the second question in the context of 1-dimensional communication. In particular, we compare welfare across several protocols for cheap-talk communication. Our main result is a linear ranking of the maximal welfare achieved in these different protocols:==== where ====, ====, ====, ==== are the maximal equilibrium welfare achieved in a generic sender-receiver game under language barriers, mediation, language barriers with the restriction that language types are distributed independently of payoff states (we refer to these as independent language barriers from now on), and noisy talk, respectively. Both Goltsman et al. (2009) and Blume and Board (2010) ask a similar question assuming quadratic preferences and the uniform payoff distribution, and (together) establish the welfare equivalence result, ====. Instead, we show that equilibria with language barriers, mediation, independent language barriers and noisy talk correspond to a series of increasingly restrictive incentive compatibility conditions (in that order), which generate the welfare order described above. Thus, our results go beyond the environment with quadratic preferences and the uniform payoff distribution and indeed hold for ==== general preference and distributional assumptions. This greater generality allows us to show, through an example (Example 2A available in Giovannoni and Xiong, 2018), that in some environments ====, thus breaking the welfare equivalence established by Goltsman et al. (2009) and Blume and Board (2010).====The remainder of the paper proceeds as follows: Section 2 describes the model; Section 3 studies ====-dimensional communication; Section 4 focuses on 1-dimensional communication; Section 5 reviews the related literature; Section 6 concludes.",Communication under language barriers,https://www.sciencedirect.com/science/article/pii/S002205311930002X,March 2019,2019,Research Article,286.0
"Mukherjee Saptarshi,Muto Nozomu,Ramaekers Eve,Sen Arunava","Department of Humanities and Social Sciences, Indian Institute of Technology-Delhi, New Delhi, India,Department of Economics, Yokohama National University, Yokohama 240-8501, Japan,Université Catholique de Louvain, CORE, B-1348 Louvain-la-Neuve, Belgium,Indian Statistical Institute, New Delhi 110016, India","Received 11 November 2017, Revised 26 November 2018, Accepted 30 December 2018, Available online 7 January 2019, Version of Record 9 January 2019.",https://doi.org/10.1016/j.jet.2018.12.010,Cited by (6),We show that the Pareto correspondence can be implemented in weakly undominated strategies by bounded mechanisms. This resolves a question raised in ,"There are two basic models in mechanism design theory. The first is the model of ==== where agents' preferences are common knowledge for all agents but unknown to the designer. The well-known result of Maskin (1999) provides an almost complete answer to the question of what goals of the designer can be achieved in such situations. The second model is that of ==== where all preference information is private and known only to the concerned agents.====The standard solution concepts in incomplete information models are dominant strategy implementation and Bayes–Nash implementation.==== The Revelation Principle applies in both cases, so that attention can be restricted to direct revelation games. In the former case, a social choice function is implementable if truth-telling is a weakly dominant strategy for each agent of each type. In the latter case, a social choice function is implementable if truth-telling is optimal in expectation. This expectation is computed assuming that other agents are telling the truth and with respect to prior beliefs of agent types. The strengths and weaknesses of both approaches are well-known. The dominant strategy requirement has robust decision-theoretic foundations. However it may be difficult to satisfy — for instance, according to the Gibbard–Satterthwaite Theorem (Gibbard, 1973, Gibbard, 1977; Satterthwaite, 1975), only trivial social choice functions are implementable in the general voting environment. The Bayes–Nash approach is more permissive but is subject to the criticism that it is dependent on prior beliefs. Small errors in these beliefs may lead to the failure of implementation.====An alternative approach is to consider implementation in undominated strategies. After the realization of types, each agent eliminates all messages that are weakly dominated for her type. All outcomes that result from agents playing weakly undominated messages at a type profile must be socially optimal outcomes at that profile.====It is clear that implementation in undominated strategies is based on the same informational requirements as dominant strategy implementation. Agents are not required to have any information other than knowledge of their type. Nor do the beliefs of agents play any role in the identification of equilibrium messages. However, implementation in this sense has an inescapable consequence when compared with dominant strategy implementation: what will be implemented will typically be a ==== rather than a ====. We believe that allowing for correspondences rather than insisting on functions can be justified. As we have noted earlier, dominant strategy implementation leads to impossibility results in many problems. In such situations, consideration of social choice correspondences is natural. Indeed, the outcomes of an implementable social choice correspondence at a profile may be preferred by the designer to all outcomes of implementable social choice functions at that profile.====A fundamental and surprising result regarding implementation in undominated strategies was proved in Jackson (1992). The paper showed that in domains with strict preferences ==== social choice function could be implemented in undominated strategies. Unlike dominant strategy or Bayes–Nash implementation, no incentive-compatibility condition on the social choice function is required. However this permissivity is achieved at a cost. The implementing mechanism involves an infinite number of messages. Moreover, there are infinite chains of messages each message being weakly dominated by its successor for some agent types. This feature of the mechanism is clearly unsatisfactory — for some subsets of the message space, a best-response (in terms of the solution concept used), does not exist. Consequently, the paper proposed the following refinement of the implementation concept: if a message is weakly dominated by another message for an agent type, it must be weakly dominated by a message that is itself weakly undominated. We shall refer to this notion of implementation as implementation in undominated strategies via bounded mechanisms or USBM implementation.====Jackson (1992) demonstrated that a condition called strategy-resistance was necessary for USBM implementation. The condition was further generalised in Yamashita, 2012, Yamashita, 2015. Neither of these conditions is sufficient.==== In fact, no non-trivial sufficient condition (for instance, applicable to social choice functions that are also dominant-strategy implementable) of any generality is available at present and the problem appears to be a difficult one. One of the reasons for this is that the Revelation Principle does not apply for USBM implementation. Consequently, the direct revelation mechanism is not canonical for the solution concept.====Our contribution in this paper is to show that a particular class of social choice correspondences are USBM implementable. The most salient member of this class is the Pareto correspondence. The issue of the implementation of the Pareto correspondence is of independent interest and was raised in Börgers (1991). We note that while Börgers (1991) considered finite mechanisms, the implications of his results carry over to the case with general bounded mechanisms. Börgers (1991) pointed out that it in addition to dictatorial social choice functions, other sub-correspondences of the Pareto correspondence are implementable. One such sub-correspondence is the “top-ranked alternatives” correspondence that selects the top-ranked alternatives of all agents at a preference profile.==== An implementing mechanism is the “pseudo-random dictatorship” or “modulo” mechanism. Each agent announces an alternative and an integer between one and ==== (the number of agents). At any integer announcement profile, compute the residue of the sum of the integer announcement mod ====. The “winner” at every integer announcement profile is the agent whose index is one greater than this residue.==== The outcome of the mechanism at this announcement profile is the winner's announced alternative. Fix an agent and an arbitrary preference ordering for the agent. A message consisting of a non-truthful maximal alternative and an integer will be weakly dominated by the announcement of the truthful top alternative and the same integer. Moreover announcing the true top alternative and an arbitrary integer is also an undominated message. Hence the mechanism USBM implements the top-ranked alternatives correspondence.====Börgers (1991) pointed out an obvious drawback of the “top-ranked alternatives” correspondence. There are typically alternatives that are Pareto-efficient and yet are not first-ranked by any agent. These alternatives called “compromises” have the attractive ethical property of representing compromise between the conflicting interests of the agents; however they are excluded from the top-ranked alternatives correspondence. Börgers and Smith (2012) formulate this discussion by introducing a criterion of ranking mechanisms. The mechanism designer weakly prefers a mechanism to another if at each state, she weakly prefers any alternative implemented by the former mechanism to any of those implemented by the latter. The mechanism designer strictly prefers a mechanism to another if she weakly prefers the former to the latter, and she does not weakly prefers the latter to the former. According to this criterion, if she prefers compromises to dictatorial solutions (for example when she has a “Rawlsian” welfare function), the mechanism designer strictly prefers the Pareto correspondence, which contains compromises, to the top-ranked alternatives correspondence.====Börgers (1991) raised the question of whether it is possible to implement a sub-correspondence of the Pareto-correspondence that does not exclude compromises. It showed that this was possible in the limited case of three alternatives by a version of the approval voting rule but failed if there were at least four alternatives. For the general case the paper noted “...if agents play undominated strategies, it is not obvious how to ensure Pareto efficient collective decisions for all preference profiles without excluding compromises”.==== Our paper resolves this issue by showing that the entire Pareto correspondence is implementable.====Our result goes further than showing the implementability of the Pareto correspondence. We show that the same mechanism with obvious modifications can implement any social choice correspondence satisfying two properties. The first of these is tops-inclusivity which requires top-ranked alternatives of all agents to be social optimal at all profiles. The second is a relatively weak property that we refer to as the seconds property. The property says the following: if all agents except some agent ==== rank alternatives ==== and ==== first and second respectively, and ==== ranks ==== above ====, then ==== must be socially optimal at the profile. The Pareto correspondence obviously satisfies this property but so do many others including sub-correspondences of the Pareto correspondence.====Our mechanism is an augmentation of the modulo game. Our idea is to embed the modulo game in a larger game where extra messages are added for each agent. For each preference ordering and each player, there are extra messages that result in compromise outcomes when all agents send these messages. There are two steps that are central to the construction. The first is to use the modulo game “dictator” messages for an agent to weakly dominate all messages that involve misreporting the agent's preference. The second is to ensure that these dictator messages do not weakly dominate the ones that lead to compromise outcomes. We show that both objectives can be simultaneously achieved for the case of the Pareto correspondence and indeed for any social choice correspondence satisfying the tops inclusivity and seconds properties.====There are only a few papers on USBM implementation other than those already mentioned. Carroll (2014) proved the following complexity result: for any positive integer ====, there exists an environment and a social choice correspondence that can be implemented in undominated strategies and the implementing mechanism requires at least ==== messages. Moreover, the environment is extremely simple requiring only two agents and two types of each agent. This result clearly indicates the difficulties involved in proving general sufficiency results – the search for implementing mechanisms cannot be limited to those of a predetermined size (in contrast to dominant strategy and Bayes–Nash implementation). The implementing mechanism in our result requires an infinite number of messages for each agent even though the set of types is finite.====Yamashita (2015) considers second-best mechanism design problems for bounded mechanisms where agents play undominated messages. In specific auction and bilateral trade settings, the paper compares the optimal value of the mechanism designer's objective function for the undominated implementation case with that in the dominant strategy implementation case.====Mukherjee et al. (2017) provide a characterization result for USBM implementation when agents satisfy the additional behavioural assumption of partial honesty. Partial honesty was introduced in mechanism design by Dutta and Sen (2012) and Matsushima (2008). In the mechanism considered in Mukherjee et al. (2017), a message involves an infinite string of type announcements. Suppose two such messages lead to the same outcome for a given message profile for the other agents. Partial honesty then requires a message that involves a greater number of truthful type declarations to be strictly preferred to the other.====The paper is organised as follows. Section 2 describes the model, Section 3 contains the result for the Pareto correspondence while Section 4 considers extensions of the result to general social correspondences. Section 5 concludes.",Implementation in undominated strategies by bounded mechanisms: The Pareto correspondence and a generalization,https://www.sciencedirect.com/science/article/pii/S0022053119300031,March 2019,2019,Research Article,287.0
"Chan Jimmy,Gupta Seher,Li Fei,Wang Yun","Department of Economics, Chinese University of Hong Kong, China,Department of Economics, New York University, United States of America,Department of Economics, University of North Carolina Chapel Hill, United States of America,Wang Yanan Institute for Studies in Economics, Xiamen University, China","Received 7 September 2016, Revised 22 December 2018, Accepted 27 December 2018, Available online 4 January 2019, Version of Record 8 January 2019.",https://doi.org/10.1016/j.jet.2018.12.008,Cited by (40),"A sender seeks to persuade a group of heterogeneous voters to adopt an action. We analyze the sender's information-design problem when the collective decision is made through a majority vote and voting for the action is personally costly. We show that the sender can exploit the heterogeneity in voting costs by privately communicating with the voters. Under the optimal information structure, voters with lower costs are more likely to vote for the sender's preferred action when it is the wrong choice than those with higher costs. The sender's preferred action is therefore adopted with a higher probability when private communication is allowed than when it is not. Nevertheless, the sender's preferred action cannot be adopted with probability one if no voter, as a dictator, is willing to vote for it without being persuaded.","This paper considers an information-design problem of a sender who seeks to persuade a group of voters to support an action when voting for this action is costly for the voters. For example, consider a legislative body that consists of members from two political parties: D and R. Since party R does not have enough votes, it needs some support from party D to pass a bill. A party D legislator will pay a political cost if he votes in favor of the policy proposed by party R.==== To succeed, the leader of party R, therefore, must convince enough legislators from party D that the value of the bill is sufficiently large that it is worthwhile to pay the political cost. Other examples of this type of group persuasion include a candidate urging his or her supporters to turn out to vote, or a CEO trying to convince a board of directors to support a project. In some countries, supporters of opposition candidates may be subject to intimidation at the polls, and a director who supports a CEO's pet project may suffer a loss in reputation.====In our model, a group of ==== voters must decide between two alternatives: ==== and ====. Action ==== is the default choice; action ==== is chosen only if it receives at least ==== votes. There are two states, labeled ==== and ====. Action ==== is the right choice in state ====. Specifically, all voters prefer ==== in state ==== but are indifferent between ==== and ==== in state ====. While action ==== is a weakly better choice, a voter must incur an extra cost to vote for ====. The cost must be paid regardless of the outcome of the vote, and it is heterogeneous across voters. A sender tries to lobby the voters to vote for ====. We model the lobbying process as a persuasion game (Kamenica and Gentzkow, 2011). First, the sender commits to a signal structure that maps each state to a probability distribution of private signal profiles. The chosen signal structure is publicly observed by all voters. Then, a vector of private signals is drawn according to the signal structure. Finally, voting takes place after each voter observes her own private signal. Following the literature, we focus on the sender's favorite equilibrium.====If the voters knew the state, our model would become a standard threshold public-good game (Palfrey and Rosenthal, 1984). Voters would vote against ==== in state ====, and there would be multiple equilibria in state ====. In the best equilibrium for the sender, exactly ==== voters would vote for ====, while the rest would free ride. When the voters do not know the state, the sender can increase the probability of ==== being chosen by designing a signal structure judiciously. On one hand, the sender wants the signals to be noisy, such that the voters would vote for ==== in state ==== with some probability. On the other hand, the signals must be sufficiently accurate. Because of the incentives to free ride, the sender must convince a voter not only that voting for ==== is the right thing to do (i.e., the state is ====) but also that her vote is needed.====As a benchmark, we consider the case where the sender is restricted to public communication. In our model, this amounts to requiring voters' signals to be perfectly correlated. In this case, the best the sender can do is to target the voter with the ====-th lowest voting cost. If this voter is convinced by a public signal to vote for ====, then the ==== voters with lower voting costs will also be convinced to vote for ====. Hence, when restricted to public communication, our model is equivalent to one where the voter with the ====-th lowest voting cost is a dictator.==== The solution to the latter problem is well known since Kamenica and Gentzkow (2011). The optimal signal should obfuscate the states so that, conditional on voting for ====, the voter with the ====-th lowest voting cost is indifferent between voting for ==== and voting for ====.====In general, there is no reason why the sender must conduct all communication in public. In fact, interest groups often lobby legislators in private. In our model, the sender can use private communication to his advantage. Under the optimal information structure, every vote for ==== is pivotal in equilibrium. That is, conditional on receiving a signal that will lead her to vote for ====, a voter knows that exactly ==== other voters also receive signals that will lead them to vote for ====. But, because multiple groups of ==== voters may vote for ==== in equilibrium, a voter who knows that her vote is pivotal for ==== will not be able to infer which group of ==== other voters are voting for ====. This allows the sender to fully exploit the heterogeneity in voting costs. Under the optimal information structure, a voter with a lower voting cost will have a strictly higher probability of voting for ==== in state ==== than a voter with a higher voting cost. In comparison, when communication is public, the ==== voters with the lowest voting costs will all vote for ==== with the same probability in state ====. Thus, the sender can get ==== chosen with a higher probability when private communication is allowed than when it is not. However, the sender's ability to manipulate the outcome is limited. In particular, the sender cannot get ==== adopted with probability one if no voter, as a dictator, is willing to vote for ==== without any extra information. Our results sharply contrast with the case where voters' payoffs depend only on the collective decision and not on their individual votes. In such a model, a voter is indifferent about how she votes whenever her vote is not pivotal, and the sender can exploit this property to get ==== selected with a probability arbitrarily close to one, so long as the voters strictly prefer ==== in state ====. From the perspective of institutional design, the lesson is that a voting body is less susceptible to manipulation when voters must pay a price for their individual votes.====Throughout the paper, we follow the literature and assume that the sender can commit to any information structure. This assumption enables us to highlight key intuitions. In reality, the set of feasible information structures may be limited. From the perspective of the voters, our exercise can be viewed as a “robust approach” to evaluate the manipulability of majority voting rules.==== Our work belongs to the growing literature about information design and Bayes correlated equilibria (BCE) developed by Bergemann and Morris, 2013, Bergemann and Morris, 2016a, Bergemann and Morris, 2016b, Bergemann and Morris, 2019 and Taneva (2014). As in the aforementioned papers, we use a linear programming approach to characterize the sender's optimal implementable information structures. Our contribution is to analyze the benefit of private persuasion in a strategic voting environment.====Our paper is also related to the Bayesian persuasion literature, which uses a belief-based approach to analyze information-design problems. In a one-sender-one-receiver model, Kamenica and Gentzkow (2011) show that the sender's problem is simply to split the receiver's belief about the state subject to the standard Bayes' rule (or Bayes plausibility condition).==== Mathevet et al. (2017) propose a belief-based and epistemic approach to information design in a multiple-agent setting. Alonso and Câmara (2016b) consider public persuasion in a Bayesian persuasion game between one sender and multiple voters. They show that, when there are more than two payoff-dependent states, the optimal public signal may give rise to multiple distinct winning coalitions that adopt the sender's preferred action. Schnakenberg (2015) obtains a similar result in a cheap-talk model. In our model, because there are only two payoff-dependent states, and voters have common preference in each state, the optimal public signal always targets the ==== voters who are the easiest to persuade, and it is strictly dominated by private signals. Wang (2015) considers private persuasion with independent and identically distributed (i.i.d.) signals and shows that public persuasion performs as well as private persuasion. We allow the sender to use any correlated signals and highlight its contribution to the advantage of private persuasion. Bardhi and Guo (2018) investigate a persuasion game under unanimous rule when voters' preferences are correlated.====The comparison between private and public persuasion has also been studied in non-voting environments. Inostroza and Pavan (2018) study information design in a global game where a policy maker designs a stress test to minimize the chance of regime change. They also show that private persuasion dominates public persuasion as in our paper.==== In their model, private persuasion dominates as it makes it harder for the agents to coordinate on a successful attack. In Arieli and Babichenko (2016), a sender promotes a product to a group of heterogeneous consumers, each of whom makes his own adoption decision. In contrast to our setting, there is no strategic interaction among consumers. Also see Farrell and Gibbons (1989) and Goltsman and Pavlov (2011) for comparison between public and private communication in multiple-audience cheap-talk models.====There is a large body of literature on information aggregation through strategic voting since the seminal contributions of Austen-Smith and Banks (1996), Feddersen and Pesendorfer, 1997, Feddersen and Pesendorfer, 1998, and Li et al. (2001).==== The key insight of this literature is that, despite voting simultaneously, a rational voter votes as if she knows that her vote is pivotal. In our model, under the optimal information structure, every vote for ==== is pivotal. Nevertheless, the sender can still increase the influence of voters whose preferences are closer to his own.====A set of papers study the impact of voting cost on turnout and electoral outcomes; see Palfrey and Rosenthal (1985), Borgers (2004), Feddersen and Sandroni (2006), Taylor and Yildirim, 2010a, Taylor and Yildirim, 2010b, and Krishna and Morgan (2012). Unlike these models, ours assumes that the voters' voting cost depends on how they vote instead of whether they vote. Downs (1957) and Fiorina (1976) are the first to study voting models where voters are motivated by non-instrumental considerations. Morgan and Várdy (2012) consider a model where voters have both instrumental and expressive motives. They show that even a weak expressive motive may significantly affect equilibrium voting behavior and the optimal size of a voting body. In our model, voters pay a fixed cost for voting for the sender's preferred action. Levy (2007) analyzes how the reputational cost for voting for an action may depend on whether individual votes are revealed to the public.====Finally, we show that in the presence of voting cost, the sender finds it optimal to induce minimum winning coalitions to get his preferred action adopted. This result is consistent with the prediction of a large number of papers on coalition formation and vote buying. Examples include Baron and Ferejohn (1989), Koehler (1975) and Shepsle (1974).====The rest of the paper is organized as follows. We present the model in section 2; section 3 formalizes the sender's information-design problem and provides some preliminary results; section 4 characterizes an optimal information structure; section 5 discusses the robustness of our results; section 6 concludes; and omitted proofs are in the appendix.",Pivotal persuasion,https://www.sciencedirect.com/science/article/pii/S0022053119300018,March 2019,2019,Research Article,288.0
"Nimark Kristoffer P.,Sundaresan Savitar","Economics Department, Cornell University, United States of America,Imperial College London Business School, United Kingdom of Great Britain and Northern Ireland","Received 21 July 2018, Revised 19 November 2018, Accepted 26 December 2018, Available online 3 January 2019, Version of Record 9 January 2019.",https://doi.org/10.1016/j.jet.2018.12.007,Cited by (11)," and the latter the ====. Taken together, the two effects imply that the beliefs of ex ante identical agents over time can cluster in two distinct groups at opposite ends of the belief space. The complacency effect holds uniformly when information cost is proportional to channel capacity, but not when cost is proportional to reduction in entropy.","Many countries have experienced increases in political polarization and in disagreement about objective facts. For example, disagreement in the US about whether climate change is real and caused by human activities has increased and the views on what is essentially an empirical scientific question is well predicted by party affiliation.==== Political polarization and disagreement about facts clearly has many causes and politically motivated disinformation, is likely to be one of them. Stating a particular belief about a given fact may for many people also be more an expression of group belonging, rather than an expression of a sincerely held belief about the true nature of the world. However, what we show in this paper is that even ex ante identical, rational agents may self-sort into different informational bubbles, where agents within one group permanently hold beliefs about a fact that are the opposite of the beliefs of the members of the other group.====This result is driven by two effects, both of which are consequences of agents' endogenous information choices. The first effect, which we call the ====, causes agents to choose to observe signals that are more precise in states they believe to be more likely. Those signals are therefore more likely to confirm their prior beliefs. If two agents initially observe different realizations of signals drawn from the same distribution, the confirmation effect then makes it less likely that agents' beliefs will converge over time. The second effect, which we call the ====, causes agents to choose less precise channels as their uncertainty decreases. For sufficiently precise beliefs, this effect causes agents to choose completely uninformative signals. Combined, the two effects imply that the beliefs of ex ante identical agents over time will cluster in two distinct groups on opposite ends of the belief space.====A basic premise of this paper is that there exists an objective reality. However, agents cannot observe this reality directly and instead have to choose a noisy information channel. Less noisy channels are more costly and agents can choose channels that have different precisions in different states of the world. One way to interpret this setup is that agents choose the medium through which facts about the world are channeled. For example, we may all be trying to verify whether global warming exists. Informative sources require a lot of attention, while uninformative sources require very little: it may be harder to extract a signal from an article in ==== than from a sound bite on cable television. Some channels may also be more accurate in one state of the world, while other channels may be more accurate in other states. If precision is costly, we show that agents allocate more precision to those states they find a priori more likely. That means that agents endogenously choose channels that are less likely to prove their priors wrong. The endogenous allocation of precision may then perpetuate differences in beliefs and lead to permanent disagreement about the true state of the world.====The early literature on costly information acquisition treated information as a scarce resource, e.g. Grossman and Stiglitz (1980). The key conceptual shift introduced by Sims (1998) and Sims (2003) was that information may be plentiful, but people's ==== is a scarce resource. Beyond this basic change of perspective, the rational inattention literature inspired by Sims early work makes specific assumptions about how to model the cost of attention. The most common formulations either put a constraint on the reduction in entropy that agents experience by observing a signal, or a utility or pecuniary cost that is increasing in the reduction in entropy, e.g. Sims (1998), Sims (2003), Maćkowiak and Wiederholt (2009), Woodford (2009), Matějka (2015) and Matějka and McKay (2015).====In the analysis below, we study the optimal information choices under two different cost functions for information acquisition. An alternative to the standard approach to model information cost as proportional to the expected reduction in entropy, is to model information cost as proportional to channel capacity. Like mutual information, channel capacity is based on Shannon (1948)'s entropy concept and it can be interpreted as the maximum possible entropy reduction that any agent, could achieve by observing a given signal structure. Woodford (2012) was the first paper to propose using channel capacity as a measure of information cost and used it to explain several choice anomalies. Here, we show that the modeling information cost as mutual information or channel capacity lead to substantively different predictions about how agents choose signal structures as functions of their prior beliefs.====Woodford (2012) argued that specifying information cost as proportional to channel capacity rather than to entropy reduction allows for better explanations of experimental evidence on visual perception. We interpret the different specifications of information costs as reflecting differences in terms of how much control an agent has over the information generating process. If an agent can control the information generating process so that he only spends resources on acquiring new information, it is reasonable to model the cost of information as being related to how much he learns, i.e. how large the reduction in entropy is between his prior beliefs and his posterior. Examples of such behavior are surveys of market participants, direct measurements of some aspect of reality, or experiments designed to answer a particular question.====On the other hand, consider an agent who does not have complete control over how the information he observes is generated, but instead uses secondary sources such as newspapers, radio broadcasts or TV shows to acquire more information. It is then more natural to think of the cost of information in terms of time spent reading, listening or watching, and it is less clear that the cost of information should be measured relative to the prior knowledge of the agent. For instance, a longer broadcast that contains more information is more costly to watch, but also potentially more informative. It is possible that some information in a broadcast is already known to a relatively informed agent, but unless the agent knows exactly at which points in the broadcast already known information will be revealed, it is as costly for the well-informed as for the uninformed to watch the broadcast. In such settings it may be more natural to think of the cost of information as being determined by the maximum any person could learn from a text or broadcast and being independent of that agent's prior knowledge. This latter scenario corresponds to modeling the cost of information in terms of channel capacity.====Using mutual information as a measure of informativeness may thus be more reasonable in settings where agents can design their signal structure to completely avoid repetition of already known information. However, using channel capacity may be more reasonable in settings where agents can choose the precision of information but cannot avoid repetition of already known information. Both approaches have appealing features and we will not argue that one is always a better description of human behavior than the other. Instead, our focus is on demonstrating the different predictions that the two specifications imply about how agents form beliefs and how these evolve over time. In particular, we demonstrate that modeling information cost in terms of channel capacity implies that when agents' priors are sufficiently precise, they are more likely to choose completely uninformative signals and thus stop updating their beliefs.====To understand why the two cost functions imply different behavior, consider the decision problem of an agent who wants to decide whether a given signal is worth paying attention to or not. When the precision of the agent's prior is higher, the marginal value of observing an additional signal decreases since the agent is already pretty sure about the state of the world. The expected reduction in uncertainty from observing the signal is then small. But if the cost of the signal is measured in terms of entropy reduction, a given signal also becomes cheaper as the prior precision of an agent's beliefs increases. In the limit with perfectly precise priors, any signal, regardless of its precision, can be observed for free. On the other hand, if the cost of paying attention to a given signal depends only on the precision of the signal, agents demand less and less precise signals as the precision of their beliefs increase, since the marginal usefulness of the signal then decreases. This implies that when an agent's beliefs are precise enough, he will choose to observe completely uninformative signals and not update his beliefs further. Importantly, an agent may stop updating his beliefs before they become degenerate. In fact, even an agent that attaches a higher probability to the incorrect state than to the correct state of the world may stop updating his beliefs. The beliefs of different agents may then cluster permanently in two distinct groups, where one group is almost certain that one state has occurred and the other group is almost certain that it has not.====Our paper is not the first to propose a theory that can explain persistent disagreement among agents. The starting point of many studies is a well-known result by Savage (1954). He argued that repeated observation of signals will lead a Bayesian agent to assign probability 1 to a true event almost surely as his length of experience increases. Even if different agents start off with different priors, their beliefs should then converge over time. Underlying this result is an assumption that the true state of nature is assigned a positive probability a priori. Blackwell and Dubins (1962) built on Savage's result and showed that as long as the two agents' priors are absolutely continuous, then their beliefs will converge over time.====One way to break the result that beliefs converge over time is to assume that agents assign a zero probability to some states that may in fact occur. For instance, Freedman (1963) and Miller and Sanchirico (1999) show that relaxing the assumption of absolute continuity between the subjective and the true distribution weakens the result. In related work, Berk (1966) shows that if an agent uses an incorrect model, his beliefs may not converge to a single point. Our setup generates persistent disagreement without such restrictions. Our agents are rational Bayesians and the support of their prior beliefs contain the true state of nature, which conforms to the assumptions of Blackwell and Dubins (1962).====The agents in our model solve a dynamic information choice problem in a simple binary state setting and our paper contributes to a growing literature studying the optimal allocation of attention in dynamic settings. Examples include Steiner et al. (2017) who propose a framework to study richer discrete state models of inattention, Maćkowiak et al. (2018) who propose analytical methods to study dynamic attention problems in linear Gaussian settings and Afrouzi and Yang (2016) who study how inflation dynamics and forward guidance are affected by rational inattention. Sundaresan (2017) shows that inattention to ex-ante low-probability events can generate endogenously persistent increases in uncertainty. Ilut and Valchev (2017) propose a dynamic framework in which agents can pay attention to learn about a policy function, rather than about an exogenous random variable.====One way to interpret the choice of channels in our model is to think of it as a choice about which sources to get information from, or which news papers to read or what TV channels to watch. Gentzkow and Shapiro (2006) and Besley and Prat (2006) show that competition among information providers makes it difficult to hide information, and Hong and Kacperczyk (2010) shows it can decrease reporting bias. Mullainathan and Shleifer (2005) show that if behavioral agents prefer news that favors their beliefs, media sources will tend to be biased. This paper can deliver the same demand story while using rational agents. Additionally, this paper provides two dimensions over which channels are graded: overall informativeness and asymmetric precision across states.====Gentzkow and Shapiro (2006) present a model that, like our model, is populated with agents that are rational Bayesians. They posit that if newspapers are rewarded for the perceived accuracy of their reporting, they will bias their reporting to conform to agents' (possibly incorrect) priors. The mechanism behind their result is that the perceived accuracy of an information source is decreasing in the distance between signals and priors. Rational agents then perceive information sources that confirm their priors to be more accurate. In our model, agents know how precise their information is, and yet, they still choose channels that are more likely to confirm their prior beliefs.====Perego and Yuksel (2017) study ideological slant in news media markets where agents have heterogenous preferences over both what the political agenda should be and how issues should be addressed. In their model, increased competition leads to news outlets to provide more specialized content, making agents disagree more about the desirability of a given policy. However, the agents that disagree the most about the desirability of a policy, agree the most about the state, i.e. the consequences of the policy. In contrast, the agents in our model disagree about objective reality.====The confirmation effect that makes agents choose channels that are more likely to confirm their priors is related to other forms of confirmation biases that have been studied in the literature. Nickerson (1998) defines confirmation bias as “the unwitting selectivity in the acquisition and use of evidence”. Models of confirmation bias include Suen (2004), Cukierman and Tommasi (1998), Rabin and Schrag (1999), Koszegi and Rabin (2006). Baliga et al. (2013) show that divergence cannot occur in a Bayesian updating framework. Fryer et al. (2013) assume that agents receive ambiguous signals which they interpret as signals in favor of their prior, and keep only this interpretation in memory. In the present paper, the selectivity is not unwitting but intentional, and thus provides a theory to explain the observed behavior without relying on behavioral biases.====Maćkowiak and Wiederholt (2018) studies a model in which agents can choose how much information to acquire about their optimal actions in different states of the world. They show that agents will allocate more attention to learn about optimal behavior in states that are ex ante more likely to occur, and that the expected loss in a given state is inversely proportional to how likely that state is. As in our model, corner solutions may occur where agents may choose to not learn anything about the optimal behavior in one state and instead allocate all their attention to learn about what to do in the other state. However, their model differs from ours in that agents do not allocate attention to learn about which state they are in. In Maćkowiak and Wiederholt (2018), agents do not allocate attention to learn about objective reality, but to prepare for different contingencies that once they occur, are known with certainty.====Some papers such as Lord et al. (1979), Baumeister and Newman (1994), show that people pay less attention to information confirming their prior and evaluate “disconfirming evidence” more thoroughly. This paper's model finds that if agents receive disconfirming signals, they update their beliefs more strongly, which is consistent with this result. Benoît and Dubra (2017) propose a model that can explain why rational agents may interpret the same evidence differently.====Our model, as well as the rational inattention literature more broadly, presumes that agents' information gathering behavior responds to incentives. A parallel experimental literature provides supporting evidence for this assumption, e.g. Ambuehl (2016) and Bartoš et al. (2016).====The next two sections present the basic set up and describe an agent's optimal channel choice. There, we formally derive the confirmation and the complacency effects and show that for sufficiently precise priors, agents will choose a completely uninformative channel. We then generalize these results to a dynamic setting where agents choose the precision of the current channel while taking into account that more precise information today will also increase future utility. Using the dynamic model, we demonstrate that the combination of the confirmation and complacency effects endogenously generate permanent disagreement in a population of ex ante identical agents. We also extend some of the results to allow for asymmetric preferences across states and irreducible noise.",Inattention and belief polarization,https://www.sciencedirect.com/science/article/pii/S0022053118303867,March 2019,2019,Research Article,289.0
Zhang Jun,"Institute for Social and Economic Research, Nanjing Audit University, Nanjing, China, 211815","Received 25 July 2017, Revised 18 December 2018, Accepted 22 December 2018, Available online 29 December 2018, Version of Record 4 January 2019.",https://doi.org/10.1016/j.jet.2018.12.005,Cited by (6),"We study the assignment of indivisible objects to agents without using monetary transfers. We prove that, regardless of whether the number of objects is sufficient, a mechanism that satisfies ex-post ====, equal treatment of equals, equal total assignment (ETA), and uniform-head fairness (UHF) must be strongly manipulable by a group of agents. In other words, by misreporting their preferences, all group members can obtain lotteries that strictly first-order stochastically dominate the lotteries they would have obtained by reporting their true preferences. ETA requires that agents obtain equal total probability shares of objects. UHF requires that if all agents have equal preferences over a subset of objects and prefer the objects in that subset to the remaining objects, then they obtain equal probability shares of the objects in that subset. The random priority and probabilistic serial mechanisms satisfy our axioms.","The purpose of this study is to demonstrate the inherent difficulty in designing random assignment mechanisms. We study the object allocation problem without monetary transfers. This problem includes at least three agents and three objects. The agents have strict preferences over objects, and each demands only one object. The objects have no priority ranking over the agents, and there is only one copy of each object. The three primary goals of designing a mechanism to solve this problem are efficiency, fairness, and incentive compatibility. Because the objects are indivisible, fairness is often achieved by randomization. The main result of this study demonstrates the strong tension between the three goals of efficiency, fairness, and group incentive compatibility.====Before explaining our result, it is helpful to examine two simple and well-studied mechanisms: the random priority (RP; Abdulkadiroğlu and Sönmez, 1998) and probabilistic serial (PS; Bogomolnaia and Moulin, 2001) mechanisms. RP draws an ordering of the agents from the uniform distribution and then lets the agents make choices sequentially. PS regards the objects as divisible and lets the agents consume the objects in order from their favorite to least favorite at equal rates. Both mechanisms are fair and efficient.==== However, both mechanisms are strongly manipulable by a group of agents. For example, suppose that there are three agents ==== and three objects ====, and consider the following two preference profiles:====For each of the two preference profiles, RP and PS lead to the equal assignments that are shown below. The equal lottery obtained by ==== at ====, that is, ====, strictly first-order stochastically dominates the equal lottery ==== that they obtain at ====, regardless of whether their true preferences are ==== or ====. Thus, if the true preference profile is ====, ==== are unambiguously better off by reporting ====. In this sense, we say that ==== can strongly group manipulate the two mechanisms at ==== by reporting ====.====This study presents a general result regarding the group manipulation described in the above example. We prove that if a mechanism satisfies ==== (ExPE), ==== (ETE), ==== (ETA), and ==== (UHF), then, in some preference profile, a group of agents must be able to strongly manipulate the mechanism. In other words, through misreporting preferences, each member of the group can obtain a lottery that strictly first-order stochastically dominates the lottery he would obtain if all members of the group reported their true preferences. In this sense, we say that the mechanism is strongly group manipulable. RP and PS satisfy the above axioms.====ExPE and ETE are standard efficiency and fairness axioms in the literature. ETA and UHF are new fairness axioms introduced by this study. ETA requires that all agents obtain equal total probability shares of objects. In other words, all agents obtain nothing with equal probability. This requirement is arguably fair since agents have equal demands and report preference lists of equal length. It is especially relevant in problems, such as refugee matching (Andersson et al., 2016; Delacrétaz et al., 2016), in which agents may care more about getting any assignment than about getting a better assignment. When the number of objects is weakly greater than that of agents, ExPE implies ETA since every agent must obtain an object ex post. Thus, ETA plays a role in our result only when there are fewer objects than agents.====UHF requires a form of unanimity in assignments when agents' preferences have unanimous upper contour sets at some object and their preferences over the objects in the upper contour set are unanimous. Specifically, if all agents prefer a subset of objects to the remaining objects and have unanimous preferences over the objects in that subset, then UHF requires that all agents obtain equal probability shares of each object in that subset. When the number of objects is weakly greater than that of agents, we can obtain the same result by replacing UHF with another new fairness axiom, which we call ==== (ETF). ETF requires that if any two agents most prefer the same object, then they obtain equal probability shares of the object. All of the above fairness axioms are independent of each other, and they are all weaker than envy-freeness (EF). Thus, we can obtain a corollary that a mechanism satisfying ExPE and EF must be strongly group manipulable.====Our result stands in the tradition of the impossibility results on fair, efficient, and non-manipulable mechanisms for matching problems. Bade (2016) is the first to demonstrate the tension between ExPE, ETE, and group strategy-proofness.==== Group strategy-proofness is one of the strongest non-manipulability requirements. In this paper, we use ExPE and more fairness axioms to demonstrate a stronger conflict between fairness and group incentive compatibility. We choose ExPE to emphasize the difficulty caused by fairness, since fairness motivates the usage of random assignments.==== Our result could be more applicable than Bade's. For example, our result implies that RP and PS are strongly group manipulable since they satisfy our axioms. By comparison, Bade's result implies that they are not group strategy-proof, which is not a novel result in the case of PS, since Bogomolnaia and Moulin (2001) have shown that PS is not strategy-proof.====Aziz and Kasajima (2017) is another related study of the random assignment problem when agents may have multi-unit demands. Their motivation and results are very different from ours. A special case of their third result states that when there are at least four agents, all of whom have unit demands, any mechanism that satisfies ordinal efficiency (OE), anonymity, and neutrality is strongly group manipulable.==== OE implies ExPE, anonymity implies ETE, and neutrality is independent of UHF. The proof of Aziz and Kasajima (2017) hinges on their strong efficiency and fairness axioms and the assumption that the number of agents is equal to the number of objects. Their result cannot be applied to mechanisms that do not satisfy OE (such as RP). We present a mechanism in the case of three agents and three objects that is not strongly group manipulable and that satisfies OE, anonymity, and neutrality. Thus, their result cannot be extended to an environment with fewer agents and objects.====Most other related studies focus on individual manipulation. Zhou (1990) proves that, in the case of at least three agents, any ex-ante efficient and symmetric mechanism is not strategy-proof. Bogomolnaia and Moulin (2001) prove that, in the case of at least four agents, any mechanism that satisfies OE and ETE is not strategy-proof. Martini (2016) strengthens Bogomolnaia and Moulin's result by replacing OE with non-wastefulness. Nesterov (2017) proves that, in the case of at least three agents, any mechanism that satisfies ExPE and EF is not strategy-proof. Zhou, Bogomolnaia and Moulin, and Nesterov assume a sufficient number of objects, whereas Martini assumes the existence of outside options. These assumptions allow the authors to easily extend their results from smaller to larger problems.==== In this study, the existence of outside options is a special case of sufficient objects. However, our result holds regardless of whether objects are sufficient.",Efficient and fair assignment mechanisms are strongly group manipulable,https://www.sciencedirect.com/science/article/pii/S0022053118306999,March 2019,2019,Research Article,290.0
Zheng Charles Z.,"Department of Economics, University of Western Ontario, Social Science Ct., Rm 4071, London, ON, N6A 5C2, Canada","Received 15 August 2017, Revised 16 November 2018, Accepted 22 December 2018, Available online 29 December 2018, Version of Record 4 January 2019.",https://doi.org/10.1016/j.jet.2018.12.006,Cited by (7),"This paper investigates the conditions for full preemption of conflicts in the form of all-pay auctions. I define two notions of conflict preemption: to ==== peace on path with commonly expected continuation plays should one veto a peace proposal, or to ==== that each player accepts a peace proposal no matter what continuation play he might expect to occur should he veto it. For each notion I prove a necessary and sufficient condition in terms of the primitives. The conditions imply that peace cannot be secured when the infimum of a player's type support is sufficiently low, regardless of the distribution functions of the players' types. The conditions also imply that peace can be implemented even when each player forecasts that should he veto peace the cost he incurs in the ensuing conflict is infinitesimal. The findings are obtained through a distributional method on two-player all-pay auctions that unifies the methods previously separated by discrete versus continuous distributions.","Under what conditions can two rivals avoid conflicts through a mediated settlement? While the conditions must be that the outcome of conflict be sufficiently unattractive to each player, the question is What does such unattractiveness mean in terms of the primitives? The answer implied by the conflict mediation literature, such as Bester and Wärneryd (2006), Compte and Jehiel (2009), Fey and Ramsay (2011), Hörner et al. (2015) and Spier (1994), is that each player has a sufficiently bad exogenous outside option as the alternative to peace. This exogenous outside option is either the player's nonparticipation payoff as his type, or his expected payoff from an exogenous lottery determined by the two players' types. Thus, peace is guaranteed if each player's nonparticipation-payoff type is distributed on a sufficiently low support, or if each player's lottery-winning type is so stochastically dominant that the opponent's expected payoff from triggering the conflict lottery is sufficiently small.====The answer is more complicated, and different to a large extent, than the above if the outcome of the conflict is determined not by an exogenous payoff or lottery but rather by an endogenous continuation play during the conflict. That means a player's assessment of his outside option depends on how he thinks his opponent would do should conflict ensue. Specifically, this paper considers conflict as an all-pay auction for the contested prize, with each player's type equal to the reciprocal of his marginal cost of bids, interpreted as his strength level in the conflict. To see how our answer may differ from the above, suppose that player 1's type is drawn from a distribution supported by ====. Consider his decision, given some type ====, on whether to veto (unilaterally reject) a peaceful split of the prize proposed by the mediator. Suppose, for the moment, that player 1 thinks that, should he veto peace, his opponent would believe that player 1's type is zero. Driven by this belief, player 2 would bid arbitrarily close to zero; then player 1 would think that he can easily win the auction by bidding slightly above zero. Consequently, in contrast to the above literature, player 1 would reject any peace proposal that offers him less than the full prize, no matter how small the supremum ==== of his type support is, and no matter how stochastically dominant his opponent's type distribution is.====Thus, the prospect of conflict preemption depends on what each player forecasts as the continuation play should conflict ensue. With conflict off path in any equilibrium that fully preempts conflict, such forecasts are arbitrary, not subject to Bayes's rule. That leads to different notions of conflict preemption, depending on the degree to which the mediator can coordinate the players into having the same forecast about continuation plays in off-path events. For a mediator with such coordination power is the notion ==== of peace, meaning that a peace proposal admits a perfect Bayesian equilibrium (PBE) on the path of which conflict occurs with zero probability. By contrast, for a mediator without such coordination power is the notion ==== of peace, meaning that every type of each player is willing to accept the peace proposal no matter what continuation play he forecasts to occur in the event where he vetoes the proposal. For each of the two notions, this paper delivers a necessary and sufficient condition, in terms of the prior distributions of the players' types, for full preemption of conflict (Theorem 1).====The condition for peace security has an unprecedented implication: When the infimum of a player's type support is sufficiently lower than its supremum, peace is not securable, regardless of any other aspect of the players' type distributions (Corollary 1, Corollary 2.b and Theorem 3). Even if there is a conflict-preempting PBE, a player may forecast a different off-path continuation play than what the PBE prescribes, so he would find it profitable to deviate. Such disagreements in forecasting off-path plays have been justified by the self-confirming equilibrium literature such as Fudenberg and Levine (1993). In particular, the player may predict that, in the off-path event where he vetoes peace, his opponent will be complacent in the conflict because of her posterior belief that his type is the infimum; then he would rather engage her in the conflict and take advantage of her complacency, however stochastically dominant is her type distribution. This result does not require enlarging the spread or riskiness of a player's type distribution. In the previous example, the result obtains even when the support ==== of player 1's type is arbitrarily narrow. Here is a discontinuity of security between ====-likelihoods and zero-likelihoods: Start with a case where peace is secured and slightly perturb each player ===='s type distribution from its support ==== so that the type belongs to ==== with probability ==== and otherwise belongs to ====, then Theorem 3 implies that peace is not securable, however small is the positive ====.====The condition for peace implementability, however, has precedents in the conflict mediation literature, but there is an important difference. The similarity is in the implication that the prospect of satisfying the condition is improved when a player's type distribution becomes more stochastically dominant than before (Theorem 2). An important difference is that to implement peace the literature also relies on an assumption that conflict reduces the value of the contested prize by a sufficiently large exogenous cost, whereas this paper assumes no exogenous cost of conflict. Furthermore, in the PBE that this paper constructs to implement peace, all but one type of each player expects to incur zero or arbitrarily small cost in the off-path event that he vetoes peace (Theorem 7.a.ii). Thus, from the perspective that the cost of conflicts is endogenous, peace is implementable even when each player expects that conflict is not costly should he trigger it. This finding indicates that, regarding the question Why peace is implementable, the exogenous cost assumption would overstate the importance of the destructiveness of conflict, though the assumption may be justifiable with respect to the question Why conflict happens despite its costs, which concerned much of the conflict mediation literature.====To obtain a condition both necessary and sufficient for all types of each player to accept a peace proposal, one would need to characterize the entire set of endogenous outside options for all of his types, including the deviating types that are not expected in, and may (due to possibility of ties) have no best response to, the continuation play in the conflict. This task becomes tractable because the supremum among a player's expected payoffs in responding to a continuation play—when his bid ranges in ====—is monotone and continuous in his type (Theorem 5) and hence it suffices to characterize the set of all outside options only for the strongest type of each player. This set corresponds to all the Bayesian Nash equilibria (BNE) of the all-pay auction in the off-path event where the player vetoes peace, with each BNE rationalized by an off-path posterior belief about the vetoer. Since off-path posteriors are arbitrary, we need to characterize the BNEs of the all-pay auction given arbitrary type distributions, allowing for gaps and atoms.====In solving the all-pay auction game with arbitrary type distributions, this paper develops a distributional method generalized from Vickrey (1961, Section II) and Milgrom and Weber (1985). The method, encapsulated by Eqs. (9) and (19), unifies the previously separate approaches to two-player all-pay auctions in the literature, one based on discrete or degenerate distributions, and the other, continuous, strictly increasing, and often identical distributions. The first approach is not conducive to a general formula for equilibria, which we need in order to compare their performances; the second one provides general formulas but it relies on the pure strategy of an equilibrium and the invertibility thereof to map one's bid to the other's type submitting the same bid, whereas we need to handle mixed and non-invertible strategies due to type distributions with atoms and gaps. Given such general settings, my method obtains new properties of the bid-to-type correspondence despite its possible discontinuities (Sections 5 and B.5) and characterizes the equilibrium in terms of its distributions of bids (Appendix B.6). The result generalizes the second approach (e.g., Amann and Leininger, 1996 and Kirkegaard, 2008) and includes all cases handled by the first one, except when types are correlated across bidders (Krishna and Morgan, 1997, Siegel, 2014 and Lu and Parreiras, 2017).====The all-pay auction game solved given arbitrary type distributions, this paper finds for each player's strongest type the posterior belief that rationalizes the best BNE, and the posterior belief that rationalizes the worst BNE, among the BNEs in the off-path event where he vetoes peace (Theorem 7). What the best BNE provides for this type of the player is the minimum payoff that a peace proposal needs to offer the player in order to secure his acceptance whichever off-path BNE he might anticipate; what the worst BNE provides is the minimum payoff to make his acceptance a best response to some off-path BNE. These minimum peaceful payoffs are derived from the parameters explicitly (Eqs. (7) and (8)). Thus come the necessary and sufficient condition for peace to be securable, and that for peace to be implementable (Theorem 1).====Balzer and Schneider (2018) have independently considered conflict mediation with endogenous conflict. They provide characterization of conflict-probability-minimizing mechanisms in terms of the on-path posterior belief system in the associated equilibria given the assumption that both players are drawn from an identical discrete distribution and that conflict cannot be fully preempted. Celik and Peters (2011) have considered endogenous outside options in an oligopoly environment of cartel formation. Their focus is the possible loss of generality due to the full participation condition on mechanisms.====After presenting the primitives, Section 2 defines the two notions of conflict preemption. The conditions for conflict preemption according to these notions are derived in Section 3, which assumes a minimum set of properties of the conflict stage so that the derivation could shed a light on conflicts that are not necessarily all-pay auctions. The two conditions for conflict preemption, implementability versus security, are contrasted in Section 4, as well as in the examples after Theorem 1. Then Section 5 presents a general, distributional approach to two-player all-pay auctions that delivers the properties of the conflict stage assumed in Section 3. All formally stated claims are proved in the Appendix, in their order of appearance.",Necessary and sufficient conditions for peace: Implementability versus security,https://www.sciencedirect.com/science/article/pii/S0022053118307002,March 2019,2019,Research Article,291.0
"Naef Michael,Sontuoso Alessandro","Department of Economics, University of California, Santa Barbara, CA, 93106-9210, United States of America,Department of Economics, Royal Holloway, University of London, Egham, Surrey, TW20 0EX, United Kingdom of Great Britain and Northern Ireland,Philosophy, Politics and Economics, University of Pennsylvania, 249 South 36th St., Philadelphia, PA, 19104, United States of America","Received 23 November 2016, Revised 12 June 2018, Accepted 12 December 2018, Available online 27 December 2018, Version of Record 4 January 2019.",https://doi.org/10.1016/j.jet.2018.12.003,Cited by (21),"We study strategic interactions that may be affected by belief-dependent, conformist preferences. Specifically, we propose that beliefs about the behavior of individuals in the same role (i.e., beliefs about “peer behavior”) directly affect a player's utility. In examining conformism we propose an experimental design that verifies the presence of the relevant causality direction. Our data reveal “opportunistically conformist” behavior, as subjects are more likely to follow the purported majority if doing so implies an increase in expected material payoff. We provide a general framework that accounts for such a pattern.","Conformism is an element of major importance for economic outcomes, as information about peer behavior has been shown to influence a diverse range of choices, including employees' retirement savings decisions and executives' decisions (Beshears et al., 2015, Shue, 2013).==== The economic theory of conformism has generally fit into two broad research streams. One such stream presumes that peers' behavior is copied as it reflects private information relevant to the individual's own decision (e.g., Banerjee, 1992, Bikhchandani et al., 1992). The second stream aims to capture the individual's inherent tendency to identify with a certain class of people (e.g., Bernheim, 1994, Akerlof, 1980). The explanatory power of these frameworks is usually limited to situations with no direct strategic interdependencies between the agents' decisions. In fact, ==== imply that the predecessor's observed choice influences one's action via a belief revision, but the decision problem entails that each individual who chooses the right option will receive a fixed payoff in any case, regardless of others (Banerjee, 1992). Similarly, ==== assume that the individual cares about the perception of others regarding her own status, but the actions of these others do not typically enter each individual's utility function (Bernheim, 1994).====In this paper we set out to study strategic interactions that may be affected by belief-dependent, conformist preferences. Specifically, we propose that beliefs about the behavior of individuals in the same role (i.e., beliefs about “peer behavior”) directly affect a player's utility. In examining conformism we introduce an experimental design that verifies the presence of the relevant causality direction. We do so by exogenously varying beliefs about peer behavior in sequential trust games (Berg et al., 1995).====In particular, we investigate the social-psychology notion of conformism, that is, one's tendency to follow the modal behavior and beliefs of one's peers (Cialdini and Trost, 1998); this attitude is often characterized as driven by a desire to fit in or “band together” with others in a similar role (Cialdini and Goldstein, 2004). In order to pin down some behavioral predictions that are informed by this notion, we operationalize such a tendency by assuming that the utility function of a conformist player is the sum of a material payoff and a “psychological bonus”: the latter captures the player's intrinsic ====, and varies with the player's beliefs about peer behavior. For the purpose of generating additional predictions against which to analyze our experimental data, we survey alternative models that may entail a relationship between one's behavior and one's beliefs about peer behavior. We find that the best explanation of the data is consistent with our specification of conformism.====It should be stressed that the identification of endogenous peer influences in non-controlled environments has traditionally presented some challenges (Manski, 1993). Moreover, it has been observed that the agents' (first- and higher-order) beliefs about their peers' actions often match what the agents themselves end up doing in that same situation: this correlation between own actions and beliefs about others in the same role could be explained by two mechanisms. ==== A tendency to adjust one's behavior in order to fit in with the group (“social conformity”); that is, from the agent's viewpoint, ==== (Cialdini and Goldstein, 2004).==== ==== A tendency to overestimate the extent to which others are like oneself, and hence to project one's own action onto others (“false consensus effect”); that is, ==== (Ross et al., 1977).====In this connection, we note that belief-dependent motivations are generally inferred from experimental evidence of a belief-behavior correlation. (An exception is provided by Costa-Gomes et al., 2014, who create an artificial instrumental variable to estimate the causal effect of first-order beliefs; see also Andreoni and Sanchez, 2014.) In the context of trust games, a correlation between a particular class of beliefs and actions has been interpreted as “guilt aversion” (Dufwenberg and Gneezy, 2000, Charness and Dufwenberg, 2006) or as “trust responsiveness” (Guerra and Zizzo, 2004, Bacharach et al., 2007). That is, an individual ==== adapts her behavior to the beliefs (about her behavior) held by her matched participant ====, in order to avoid guilt from letting down ===='s expectations (Battigalli and Dufwenberg, 2007). Specifically, the presence of guilt aversion has been inferred from a correlation between own behavior and second-order beliefs (about own behavior), where such beliefs are elicited by asking subjects what they think their “opponents” (i.e., people in the other role) expect from them. However, it has been suggested that any such observed correlation may be due to consensus effects, which involve the opposite causal direction: people who are inclined to cooperate might infer from their own inclination that people in general are cooperative (Ellingsen et al., 2010).====Here we enter the debate on causality by examining a class of motivations – conformist preferences – that involve the same direction of causality as with guilt aversion (in the sense that beliefs cause behavior). More precisely, we test the hypothesis that beliefs about the ==== directly affect the player's utility, in such a way to reflect a desire to fit in with the purported majority of peers. Our experimental design involves a standard two-player binary trust game in which we inform each subject of the behavior that other same-role participants expect of same-role participants. To that end, we first elicit each subject's belief about the behavior of participants in the same role (i.e., one's first-order belief about peer behavior); then, some of these beliefs are averaged and transmitted to other participants in the same role, providing subjects with an “induced” second-order belief about peer behavior.====Our data show that Trustees holding a first-order belief of ==== (on the part of other Trustees) were significantly affected by the inducement of a second-order belief of predominant non-cooperation. Conversely, Trustees with a first-order belief of predominant non-cooperation were unaffected by the inducement of a second-order belief of predominant cooperation. The effect of the exogenous information on Trustors' behavior is symmetrical, that is, an increase in the transmitted belief has a significant positive effect only on the group of Trustors who held a first-order belief of ==== (on the part of other Trustors). Such data patterns suggest “opportunistic” conformism, since subjects are more likely to follow the purported majority if doing so implies an increase in expected material payoff.====We stress that the above establishes the presence of the causality implied by conformist preferences for two reasons. Firstly, by informing each subject about others' first-order beliefs, we bring about an ==== second-order belief: thus, any correlation between such beliefs and behavior cannot be attributed to consensus effects. Secondly, those exogenously-generated beliefs involve the behavior of participants in the ====; that is, in contrast to previous studies we focus on expectations about the behavior of one's peers, so that one's payoff is not directly affected by those beliefs. To the best of our knowledge, we provide the first evidence for the causal effect of such beliefs on behavior: whereas previous research has shown evidence of an effect of manipulating first-order beliefs about peer behavior,==== our study crucially accounts for the effect of second-order beliefs (about peer behavior) on one's behavior.====In a nutshell, we examine strategic interactions that may be affected by belief-dependent, conformist preferences. We operationalize such preferences by assuming that a player's utility is the sum of her material payoff and a psychological bonus. The latter varies with the player's beliefs about peer behavior, in such a way that a player gains a higher psychological utility from following a ==== (i.e., purportedly frequent) behavior. We further assume that the extent to which a player derives such intrinsic utility from popular strategies varies with her ==== (i.e., an individual-specific constant).====When peers' actual behavior is unobservable, as in our experiment, our framework suggests that individuals use the induced second-order beliefs to revise their first-order beliefs about peer behavior (a predictor of a strategy's popularity). Given that, our framework implies that – all else equal – one is likely to switch to the modal strategy indicated by the exogenous information especially ==== doing so increases one's expected material payoff as well.====The remainder of the article is organized in this manner: section 2 introduces the experimental design; section 3 discusses the notion of conformism and other relevant theories; sections 4 and 5 present the experimental results, and section 6 concludes. Appendix A lays out a general model.",Opportunistic conformism,https://www.sciencedirect.com/science/article/pii/S0022053118306975,March 2019,2019,Research Article,292.0
Bade Sophie,"Royal Holloway, University of London, United Kingdom of Great Britain and Northern Ireland,Max Planck Institute for Research on Collective Goods, Bonn, Germany","Received 8 June 2017, Revised 13 November 2018, Accepted 12 December 2018, Available online 21 December 2018, Version of Record 31 December 2018.",https://doi.org/10.1016/j.jet.2018.12.004,Cited by (27),"The crawler is a new efficient, strategyproof, and individually rational mechanism for housing markets with single-peaked preferences. In a housing market each agent is endowed with exactly one house. These houses are ordered – by their size for example – and all agents preferences are single-peaked with respect to that order. The crawler screens agents in order of their houses' sizes, starting with the smallest. The first agent who does not want to move to a larger house is matched with his most preferred house. Agents who currently occupy houses sized between this agent's original and chosen houses “crawl” to the next largest unmatched house. This process is repeated until all agents are matched. The crawler is easier to understand than Gale's top trading cycles and can be extended to allow for indifferences.","Consider a housing market where each agent ==== in a set ==== is endowed with a house, also called ====. Suppose there is some objective linear order on all houses. Houses could be ordered by their location, so that ==== means that house ==== lies to the south of house ====. Alternatively houses could be ordered by their sizes, their energy efficiency, etc. All agents preferences are single-peaked with respect to the objective order on houses. If preferences are single-peaked with respect to the north-south ordering then agents hope to live as close as possible to their preferred latitude. If size is the relevant objective order, then each agent has an ideal house size. Such an agent prefers a house that is a bit smaller (larger) than his ideal house to any other house that is yet smaller (larger). For ease of presentation I assume throughout that preferences are single-peaked with respect to house sizes.====A mechanism maps each profile of all agents' preferences to a matching. A matching, in turn, is a one-to-one function between agents and houses. The crawler, a new matching mechanism for the single-peaked domain, determines matchings by screening all houses in order of their size, starting with the smallest. Once a house whose current occupant ==== wants to either stay put or move to a smaller house is found, the crawler matches this agent ==== with his most preferred house. If this house is not the house that agent ==== occupied at the beginning of this step, then each occupant of a house sized between these two “crawls” to the next largest house.==== This process is repeated until all agents are matched.====A mechanism is strategyproof if no agent can ever benefit from misrepresenting his preferences. It is efficient if it maps each profile of preferences to a matching for which there does not exist an alternative matching weakly preferred by all and strictly by some. It is individually rational if no agent is ever matched with a house he deems worse than the one he was endowed with. Theorem 1 shows that the crawler is efficient, strategyproof, and individually rational.====Without the assumption of single peakedness, exactly one mechanism satisfies these three criteria: when all linear orders are permitted as preferences, then Gale's top trading cycles is the unique efficient, strategyproof, and individually rational matching mechanism. In Gale's top trading cycles each agent points to the owner of his most preferred house. Any agent in a pointing cycle is matched with the house he points to. The procedure is repeated with all unmatched agents and the restriction of their preferences to the unmatched houses and the algorithm terminates once a matching is reached.====While Shapley and Scarf's (1974) and Roth's (1982) results that Gale's top trading cycles is efficient, strategyproof, and individually rational also apply to the domain of single-peaked preferences, Ma's (1995) result that Gale's top trading cycles is the only such mechanism, does not. I provide a new proof of Ma's (1995) result to show how this result depends on richness of the domain of all linear preferences. Proposition 1 then indeed shows that the crawler differs from Gale's top trading cycles.====On the domain of single-peaked preferences the crawler has an advantage over Gale's top trading cycles. It has an extensive form implementation that is – in a well-defined sense – easier to understand than any extensive form implementation of Gale's top trading cycles. To define mechanisms that are more or less easy to understand, consider a strategy for some agent ==== in an extensive form mechanism. Arbitrarily fix a history where agent ==== moves and that can be reached if ==== plays the given strategy. This strategy is obviously dominant following Li (2017) if ==== (weakly) prefers the worst outcome associated with the continuation of his strategy to the best outcome following a deviation at the current history (and all later histories). To calculate the relevant worst (best) payoff the agent considers the most harmful (favorable) choices by all other agents at all histories following the current one. Li (2017) argues that even cognitively impaired agents or agents who suspect the designer of fraud never see a reason to deviate from an obviously dominant strategy. Theorem 3 shows that the crawler can be implemented in obviously dominant strategies. Conversely, I show that even on the restricted domain of single-peaked preferences Gale's top trading cycles cannot be implemented in obviously dominant strategies.====In Section 6 I define a variant of the crawler that can be used on a larger domain of single-peaked preferences where agents may be indifferent between some houses. Theorem 4 shows that this variant inherits the three crucial properties of the crawler: it is efficient, individually rational and implementable in obviously dominant strategies.====While, the assumption of single-peaked preferences has a long pedigree in the social choice literature (see for example Moulin, 1991), it is a novel assumption in the matching context. Independently of the present paper Damamme et al. (2015) also study single-peaked preferences in the context of a housing market and find that sequences of individually rational bilateral swaps always reach the Pareto frontier in such housing markets. Without the restriction to single-peaked preferences the Pareto frontier can generally only be reached if larger groups of agents exchange their endowments. So Damamme et al. (2015) and the present paper propose two different criteria according to which Gale's top trading cycles is not the best mechanism on the domain of single-peaked preferences: some mechanisms require less centralization (Damamme et al., 2015) others satisfy more stringent incentive properties (Theorem 3).",Matching with single-peaked preferences,https://www.sciencedirect.com/science/article/pii/S0022053118306987,March 2019,2019,Research Article,293.0
Jeong Daeyoung,"Pohang University of Science and Technology, Republic of Korea","Received 4 May 2017, Revised 15 October 2018, Accepted 10 December 2018, Available online 12 December 2018, Version of Record 14 December 2018.",https://doi.org/10.1016/j.jet.2018.12.002,Cited by (5),"We develop a model of strategic information transmission from an expert with informational superiority to decision makers who vote on a proposal. We show that an expert's simple cheap talk strategy can be surprisingly effective in persuading decision makers by polarizing or unifying their opinions. After observing the expert's cheap talk message, decision makers may ignore their ","In many economic situations, multiple individuals make collective decisions that determine the outcomes for the whole of society. Such decisions are often made by voting on a proposal or issue. For example, Supreme Court justices decide whether a bill is constitutional, and a board of directors or a group of policy makers decide together whether to approve a proposal. Because decision makers may not have all of the necessary information regarding the proposal at hand, they commonly hear advice from an expert who is potentially better informed about the proposal. This better informed agent, the expert, does not necessarily have the same preferences or incentives for the outcomes as the deciding body. Hence, the advice given has to be interpreted strategically.====In this study, we explore the transmission of information from an expert (female) with informational superiority to a group of decision makers (male) with heterogeneous interests and private information. As far as we know, this paper is the first to examine a cheap talk model with collective decision makers who have heterogeneous preferences and private information. More specifically, an expert who observes the qualities of a proposal in a two-dimensional state space sends a public cheap talk message, which is costless, non-binding for any agent, and unverifiable by a third party (Crawford and Sobel, 1982; Farrell, 1987), to a group of decision makers with limited information. This model is inspired by the voting example from Chakraborty and Harbaugh (2010). In their example, there are two groups of uninformed voters, within which the voters have identical preferences, whereas we introduce partially informed decision makers with various preferences and check if an outside expert's advice could divide them into groups with opposing opinions or unify them into one.====We consider two different models based on the type of expert: one model featuring a highly biased expert who always wants a proposal to be defeated (e.g., a radical interest group); the other, featuring a surplus maximizing expert who tries to maximize the total surplus of the group of decision makers (e.g., a bipartisan policy group). We show that, in both models, a simple cheap talk strategy used by an expert with informational superiority can be surprisingly effective in persuading collective decision makers by polarizing or unifying their opinions and voting decisions.====Consider the following example. Several legislators are voting on a pipeline construction bill. They may ask two questions: How good is the project for the environment, and how many jobs will it create? Each legislator may put different weight on these two aspects; one may care only about either the environment or the creation of jobs, another may care about both aspects equally. On the other side is an external lobbying firm, which gives advice to the legislators by sending a public message. The firm knows exactly how the project will turn out, but each legislator only has partial information, which is not perfectly accurate and only implies whether or not the project is good for him.====In a cheap talk equilibrium, the highly biased expert polarizes decision makers' opinions by sending a message that asserts the proposal is stronger in one dimension than it is in another. Given the expert's message, deemed as “comparative cheap talk,” a decision maker who cares more about the former dimension may be more favorable toward the proposal, but a decision maker who cares more about the latter dimension may be less favorable toward it.==== In the pipeline example, the lobbying firm, which always wants a bill to be defeated, reveals a simple truth about the quality of the pipeline project by saying “the project is better for jobs than it is for the environment” (or vice versa). This message polarizes the legislators into groups with opposing opinions: the legislators who care more about jobs believe the bill is good enough for them, and the legislators who care more about the environment believe it is not.====These polarized opinions of the decision makers help the expert to achieve her goal by creating polarized voting behavior. We show that under any super majority rule, when there is a significant informational gap between the expert and decision makers, the expert's manipulative cheap talk influences the decision makers to focus only on the expert's message and vote in her interest, even though they know the expert has her own bias. The equilibrium voting outcome would not depend on the decision makers' private information, so the voting procedure would fail in aggregating it. We further show that, under a certain condition, the expert's cheap talk strategy specified above is better for the expert than any other strategy, even when the decision makers' private information is arbitrarily accurate. To sum up, by exploiting the heterogeneity in decision makers' preferences in a multidimensional state space, the expert's comparative cheap talk polarizes the decision makers' opinions and persuades them to vote in her favor.====The surplus maximizing expert, on the other hand, unifies decision makers' opinions by sending a message that reveals the overall quality of the proposal. For instance, a bipartisan policy group in the pipeline example may say, “The overall benefit of the project along the two aspects would be sufficiently high.” Given the message, all voters may believe the proposal is good enough to support. So, the expert is capable of unifying the decision makers' opinions on the proposal. Similar to the previous case with the highly biased expert, this cheap talk message would also restrict the aggregation of information among the decision makers, and help the expert to achieve her goal.====Since our basic model with the highly biased expert examines a pure persuasion, in which an expert has a state-independent preference, it is natural to ask how much our main results extend to Bayesian persuasion à la Kamenica and Gentzkow (2011).==== We show that the highly biased expert can guarantee the rejection of the proposal by designing the Bayesian experiment, mimicking the comparative cheap talk, if, and only if, a rejection equilibrium exists in the cheap talk model. That is, a Bayesian persuader does not have any further advantage over the cheap talker in securing rejection at all times. However, we claim that when there is no rejection equilibrium in the cheap talk model, the expert in a model of Bayesian persuasion can design an experiment that gives herself a higher ex-ante payoff than the comparative cheap talk does.",Using cheap talk to polarize or unify a group of decision makers,https://www.sciencedirect.com/science/article/pii/S0022053118306835,March 2019,2019,Research Article,294.0
Pram Kym,"University of Nevada, Reno, United States of America","Received 26 July 2016, Revised 28 November 2018, Accepted 4 December 2018, Available online 7 December 2018, Version of Record 13 December 2018.",https://doi.org/10.1016/j.jet.2018.12.001,Cited by (5),A target equilibrium in a game of complete information is called robust to incomplete information when all nearby games of incomplete information have equilibria that generate similar ex-ante distributions over actions to the distribution generated by the target equilibrium. Robustness to canonical elaborations considers only nearby games with a special structure. I show that robustness to incomplete information and robustness to canonical elaborations are equivalent when the equilibrium concept in the nearby incomplete information games is agent normal form ====.,"Equilibrium predictions in games can be highly sensitive to the fine details of players' beliefs. A classic example is Rubinstein's email game, which shows that an equilibrium under complete information may not be close to an equilibrium when payoffs are almost common knowledge but uncertainty remains in the higher-order beliefs (Rubinstein, 1989). Since players' beliefs are often unobservable to the modeler, we would like to know when our predictions do not depend too much on the fine details of beliefs: this is the question of robustness to incomplete information (henceforth, robustness).====Robustness can be formalized in various ways. One approach is the framework of Kajii and Morris (1997b). In this approach, we fix a game of complete information and consider nearby games, called elaborations, in which, with high prior probability, all players know that their payoffs are given by the payoffs in the original game. An equilibrium is called robust if for every sequence of elaborations in which this probability approaches one, there is an associated sequence of equilibria which approach the target equilibrium.====Whether a given equilibrium is robust can be difficult to analyze because the space of possible elaborations is very large. Although some results are known, it is in principle simpler to consider a special class of elaborations: canonical elaborations. A canonical elaboration is one in which each player-type either has payoffs given by the payoffs in the original game, or has a strictly dominant action (Kajii and Morris, 1997a). I show that, in fact, robustness and robustness to canonical elaborations are equivalent, when the solution concept used in the elaborations is agent normal form correlated equilibrium.====This result is of interest for several reasons: First, it allows us to better characterize which equilibria are robust to incomplete information. Besides the observation that canonical elaborations are simpler, some specific results are currently known only for robustness to canonical elaborations: in particular, Ui (2001) shows that a unique potential maximizer in a potential game is robust to canonical elaborations. Morris and Ui (2005) introduce the (set-valued) notion of generalized potential functions, and show that a set of actions that maximizes a generalized potential function is robust to canonical elaborations. My result shows that these sufficient conditions also hold for general elaborations when correlated equilibria are used in the elaborations.====For example, consider a congestion game, à la Rosenthal (1973): players choose from among different routes between an origin and destination, with the cost of traveling along a route increasing in the number of other players using that route. At least one pure strategy of this game is a potential maximizer (Monderer and Shapley, 1993), therefore if it is a unique potential maximizer, it is robust to canonical elaborations. The result of this paper will imply that the equilibrium is also robust more generally, so that an analyst making this equilibrium prediction need not be concerned that having slightly misspecified the players' beliefs will greatly affect the prediction.====Secondly, the connection between canonical and general elaborations is of interest in other contexts. In information design, consider an information designer who selects an information structure for multiple receivers and wants to maximize the probability of a certain outcome in the ==== equilibrium. Techniques from the literature on canonical robustness can be used to give bounds on what the designer can achieve when the underlying game has the structure of a canonical elaboration (see Bergemann and Morris, forthcoming). In implementation theory, constructions similar to canonical elaborations can be used to virtually implement social choice functions in incomplete information environments where mechanisms known in the literature are insufficient.==== In both contexts the use of correlated equilibrium is very natural, as a designer can induce a correlated strategy profile through the use of payoff-irrelevant recommendations. Understanding the connection between canonical and general elaborations in the limit is a step towards generalizing these results.====Finally, as a conceptual point, the result establishes that behavior in (correlated) equilibria of nearby games is essentially captured by higher order beliefs over ==== without the need to consider higher order beliefs over more complex ====, an observation that may be of wider significance.====Recently, Takahashi (2018) has shown by example that for a set-valued notion of robustness (where a set of actions is called robust if nearby elaborations have equilibria generating behavior close to an element of the set), robustness and robustness to canonical elaborations are not equivalent when Bayes Nash equilibrium is used in the elaborations. The question of whether robustness and robustness to canonical elaborations are equivalent for singleton equilibria in the complete information game when Bayes Nash equilibrium is used in the elaborations remains open.====In addition to the work discussed above, there is a broader literature on robustness: among others, Tercieux (2006) and Oyama and Tercieux (2009) give further sufficient conditions for robustness in certain classes of games. As yet, no general characterization of robustness is known. The global games literature (following Carlsson and van Damme, 1993) has similarities to robustness to canonical elaborations, in the sense that the approach is driven by players putting probability in the higher order beliefs on opponents having a dominant action.====The organization of the paper is as follows: the model and the definitions of equilibrium, elaborations and robustness are given in Section 2. In Section 3, I give a heuristic outline of the proof followed by a formal proof via a series of lemmas. Detailed proofs of the lemmas are given in the online appendix.",On the equivalence of robustness to canonical and general elaborations,https://www.sciencedirect.com/science/article/pii/S0022053118306823,March 2019,2019,Research Article,295.0
He Wei,"Department of Economics, The Chinese University of Hong Kong, Shatin N.T., Hong Kong,Department of Economics and Risk Management Institute, National University of Singapore, 1 Arts Link, 117570, Singapore","Received 11 May 2018, Revised 8 November 2018, Accepted 30 November 2018, Available online 3 December 2018, Version of Record 13 December 2018.",https://doi.org/10.1016/j.jet.2018.11.007,Cited by (8),A general condition called “coarser inter-player information” is introduced and shown to be necessary and sufficient for the validity of several fundamental properties on pure-strategy equilibria in ,"Bayesian games in the static setting model the interaction of multiple players who make decisions simultaneously with only partial information about the payoffs of the other players. Harsanyi (1967–1968) formulated a general Bayesian game by introducing a type model, where a player knows her own type but not others' types. The payoff of a player depends on the type and action profiles. A player chooses an action among multiple choices, based on her observed type. Thus, a strategy of a player is a complete plan of actions to be taken contingent on her types. Nature randomly chooses a type for each player. A player then evaluates her expected payoff based on her belief about other players' types. Such a model for incomplete information has become a basic component of game theory and a standard tool with widespread applications in many areas.====Harsanyi's fundamental work and many follow-up contributions on the general theory of Bayesian games have been based on the concept of behavioral strategy. Besides Nature's randomization for the choice of types, a behavioral strategy of a player involves one more level of randomization. After knowing her own type, the player needs to choose some randomization device to select an action. As noted in Radner and Rosenthal (1982) and Milgrom and Weber (1985), this kind of further randomization has been criticized for its limited appeal in many practical situations.==== Indeed, various economic applications of Bayesian games have focused on pure strategies without the additional randomization.====Given the central importance of Bayesian games and the associated notion of pure-strategy equilibrium, a basic theoretical question arises: can one find a general condition to guarantee several fundamental properties of the pure-strategy equilibrium in Bayesian games?==== The first property is about the existence of pure-strategy equilibria. The second is on the purification from behavioral strategies, which is a useful method to relate behavioral strategies to pure strategies preserving the equilibrium property. The third concerns the sensitivity of equilibrium outcomes to modeling assumptions; namely, whether the equilibrium property remains under small perturbations on the game structure (i.e., the closed graph property of equilibrium). In this paper, we introduce the condition of “coarser inter-player information,” and show its sufficiency for all the three properties to hold. What is surprising is that this condition is also necessary for the validity of any of the properties. As a result, we have provided a definitive answer to the question.====In the special case of a Bayesian game with interdependent payoffs and independent types, the intuition behind the condition of coarser inter-player information is that each player's type can influence her own payoff fully but other players' payoffs partially; more precisely, it means that the inter-player information of each player is always less informative than her total private information, given any nontrivial event in her private information.==== When one allows the types to be independent conditioned on some common states, the condition says that the same kind of full/partial influence is satisfied for each common state. The condition can also be applied to the general information structure, where players' types can be correlated and the common prior on the joint type space has a density function with respect to the product distribution of players' individual type distributions.==== Such an information structure is normally adopted in applications of Bayesian games.====We first consider the existence of pure-strategy equilibria. Theorem 1 characterizes this existence property by the condition of coarser inter-player information for each player. Next, we introduce a new principle of conditional purification (Lemma 2) which says that any randomized decision rule can be purified to yield the same expected payoffs and distributions conditioned on some given information. In the setting of a Bayesian game, when a player has less information influencing the other players' payoffs than her own, this principle allows us to obtain in Theorem 2 a purification from a behavioral-strategy profile with the same expected payoffs and distributions conditioned on her inter-player information.==== In addition, we show that the condition of coarser inter-player information for each player is also necessary for the purification property in Bayesian games. The closed graph property of equilibrium means that any sequence of pure-strategy equilibria, for a corresponding sequence of Bayesian games converging to a limit game, has a subsequence converging to a pure-strategy equilibrium of the limit game. Thus, if the primitives that determine a Bayesian game vary continuously, then the set of pure-strategy equilibria should also vary upper hemicontinuously. The condition of coarser inter-player information is used again in Theorem 3 to characterize this closed graph property.====The proofs of the sufficiency results in Theorem 1, Theorem 2, Theorem 3 are provided via establishing a new connection between Bayesian games and conditional distribution of correspondences. For the necessity results in Theorem 1, Theorem 2, a sequence of Bayesian games is carefully constructed so that the equilibrium existence or purification for these games implies, for each player, the existence of many independent events (in her private information) beyond her inter-player information, which leads to the condition of coarser inter-player information. Furthermore, the proof of the necessity result in Theorem 3 involves a novel construction of a double sequence of Bayesian games.====To illustrate how the condition of coarser inter-player information could be used in specific economic environments, we present two examples in Section 4, which cannot be covered by the previous literature. We first consider a class of general auction games with externalities and risk-neutral bidders, where the bidders have interdependent values, and independent types conditioned on some common signals.==== The second example is a Bertrand pricing game in which firms with private costs face unobservable demand shocks and have interdependent payoffs, where the firms receive private signals independently, conditioned on the realized shock.==== It is easy to show that the condition of coarser inter-player information is satisfied by the Bayesian games in both examples. Thus, the existence of pure-strategy equilibria in these two examples follows from our general result in Theorem 1 for Bayesian games with finite actions.====There has been an active literature for finding conditions to guarantee the existence of pure-strategy equilibria in Bayesian games. Radner and Rosenthal (1982) worked with the conditions of independent atomless types==== and private values. Milgrom and Weber (1985) and Fu et al. (2007) allowed for payoffs with private values and correlations among the players by working with conditionally independent types.==== Barelli and Duggan (2015) considered Bayesian games with product structures. Our sufficiency results in Theorem 1, Theorem 2 cover the existence and purification results on pure-strategy equilibria in those papers;==== see Section 5 for the detailed discussions. As mentioned above, this paper also shows the condition of coarser inter-player information to be necessary for the relevant properties of pure-strategy equilibria to hold. There is another stream of literature on Bayesian games with additional order structures on the payoffs and action sets. Vives (1990) showed the existence of (a largest and a smallest) pure-strategy equilibria.==== It was further shown in Van Zandt and Vives (2007) that in the class of monotone supermodular games the extremal pure-strategy equilibria are monotone in types. They also provided a natural algorithm to compute those equilibria. Athey (2001) first established sufficient conditions for the existence of a monotone pure-strategy equilibrium, which was generalized by McAdams (2003) to settings with multidimensional actions and multidimensional types. Reny (2011) discovered contractibility to be automatically satisfied given any nonempty monotone best responses. A more powerful fixed-point theorem was adopted in Reny (2011) than those employed in Athey (2001) and McAdams (2003) (based on contractibility rather than convexity) to establish the general existence of a monotone pure-strategy equilibrium.====The paper is organized as follows. Section 2 introduces the model of Bayesian games with coarser inter-player information. The main results are presented in Section 3. Two applications to auctions and oligopoly pricing games are provided in Section 4. Section 5 discusses how our sufficiency results cover various earlier results as special cases. The proofs of Theorem 1, Theorem 2, Theorem 3 are given in Appendix A. The proofs of all the claims in Sections 2, 4 and 5 are left in Appendix B.",Pure-strategy equilibria in Bayesian games,https://www.sciencedirect.com/science/article/pii/S0022053118301716,March 2019,2019,Research Article,296.0
Basu Pathikrit,"California Institute of Technology, United States of America","Received 25 February 2018, Revised 19 November 2018, Accepted 22 November 2018, Available online 26 November 2018, Version of Record 30 November 2018.",https://doi.org/10.1016/j.jet.2018.11.005,Cited by (9),"We interpret the problem of updating beliefs as a choice problem (selecting a posterior from a set of admissible posteriors) with a reference point (prior). We use AGM belief revision to define the support of admissible posteriors after arrival of information, which applies also to zero probability events. We study two classes of updating rules for probabilities: 1) “lexicographic” updating rules where posteriors are given by a lexicographic probability system 2) “minimum distance” updating rules which select the posterior closest to the prior by some metric. We show that an updating rule is lexicographic if and only if it is ====, AGM-consistent and satisfies a weak form of path independence. While not all lexicographic updating rules have a minimum distance representation, we study a sub-class of lexicographic rules, which we call “support-dependent” rules, which admit a minimum distance representation. Finally, we apply our approach to the problem of updating preferences.","Updating beliefs in light of newly acquired information is a problem that is relevant and occurs in many situations in economic theory. In most environments, an agent's belief is represented by a probability measure over a state space, elements of which are payoff relevant. As a result, actions of the agent depend crucially on his opinion or belief over the state variable and consequently also on how he chooses to update his belief upon learning an event. A dominant principle used to update probabilities in most models is Bayesian updating. Starting with a prior ==== and observing a positive probability event ====, Bayesian updating suggests the posterior ====. However, in the event of a surprise i.e. observing a zero probability event, Bayesian updating remains silent and is not well-defined. Such situations arise in extensive-form games of imperfect information where a player's strategy must specify which action to choose in an information set that is reached with zero probability and the updating problem is one of assigning probabilities to nodes in the information set. The solution concepts of sequential equilibrium and trembling-hand perfect equilibrium (see Kreps and Wilson 1982b and Selten 1975) both place restrictions on admissible beliefs on information sets which lie off path (using Bayesian updating otherwise). These put restrictions on off-path beliefs by requiring them to be the limit of on-path beliefs corresponding to a sequence of perturbations of the equilibrium strategy profile. In perfect information extensive-form games as well, due to strategic uncertainty, a player may revise their beliefs after observing a deviation, about their opponent's strategies and hence, their future actions (see Battigalli and Siniscalchi 2002). Abstracting away from the game-theoretic scenario, we ask whether in the probabilistic model itself, there exists a systematic way to extend Bayesian updating to zero probability events.====In this paper, we interpret the updating problem as a choice problem with a reference point (see Rubinstein and Zhou 1999). The reference point here is a prior ==== on the state space and given an event ====, the choice problem is one of choosing a posterior from an admissible set of posteriors which have a common support in ====. This raises the question of how the admissible set of posteriors should be selected for a given updating problem ====. Since we wish to ==== Bayesian updating, if ==== we would want this set to be all probability measures whose support is the intersection of the support of the prior and the observed positive probability event and choice would be the Bayesian posterior. The intersection would include all states consistent both with the information ==== and the prior belief ====. But what happens if ==== has zero probability? Is there a consistent way to select the set of admissible posteriors for all events ====? This would allow us to define the support of the posterior belief after both positive and zero probability events.====The question of finding the support of the posterior can be posed as a problem of “theory change” as is known in the non-probabilistic theory of belief revision from propositional logic, namely, AGM belief revision (see Alchourron et al. 1985). In that setting, an agent's primitive is a belief set, which is a set of propositions or events that the agent believes to be true (in the present context this would be all probability one events according to the prior). The belief revision problem is one of revising the belief set to a new belief set, based on the information that event ==== has occurred. The desirable feature of applying the AGM procedure is that it provides solutions to the belief revision problem even when information obtained is inconsistent with prior beliefs. This property of the theory is, as we shall see, intimately linked with the problem of updating over zero probability events. If the prior is ==== and ==== is its associated belief set, then we want the posterior belief ==== to be such that the now revised belief set, ====, is derived from ==== and ==== via AGM belief revision.====The consistency requirement here means that we want the solution to our probabilistic updating problem to be consistent with the underlying change in the associated belief sets. If we believe that this latter change should satisfy certain postulates and properties (which is a central concern of the literature on belief revision, see Costa and Pedersen 2011), then we obtain a set of restrictions on the posteriors to be considered admissible. The advantage of this approach is that it is well defined in specifying the support of posteriors after all events, including zero-probability events. It also has the desirable property that all admissible posteriors have common support in the event observed and hence, updating satisfies consequentialism. When an updating rule abides by the AGM procedure, we say that it is ====.====We focus on two classes of updating rules. The first class of updating rules we study are ==== updating rules where the posteriors are defined by a lexicographic probability system (see Blume et al. 1991, Halpern 2010 and Hammond 1994). We show that an updating rule is lexicographic if and only if it is Bayesian, AGM-consistent and satisfies a weak form of path independence (order in which information arrives does not matter). The weakening is in the sense that order independence is satisfied only for certain pairs of events. It also turns out that this weakening is crucial. There exist no updating rules which are Bayesian and satisfy strong path independence (order-independence on all consistent pairs of events). Extending this approach to the problem of updating preferences, we also obtain a similar characterisation of lexicographic updating in that setting.====The second class of updating rules, which we call ====, picks the posterior closest to the prior according to a metric defined on the space of probability measures.==== We study the relationship between lexicographic and minimum distance updating rules. While not all lexicographic updating rules have a minimum distance representation, we show that a sub-class of lexicographic rules, which we call ==== updating rules, indeed admit minimum distance representations. In these rules, the alternative hypotheses that are used to update over events of zero probability, depend only on the support of the prior. Hence, they depend only on the set of states which have ex-ante zero probability.====For an exposition of AGM theory, Costa and Pedersen (2011) and Huber (2013) provide excellent surveys. The problem of choosing supports of posteriors in a manner consistent with AGM belief revision has been studied by Bonanno (2009)), where a choice correspondence given an event, chooses as a subset of the event, the support of the posterior. The framework is non-probabilistic and it is shown that rationalisability of the choice correspondence is equivalent to AGM-consistency. In the present framework, we derive a counterpart of this result and it provides a very useful characterisation of AGM-consistency of an updating rule (see also Grove 1988). The relationship between lexicographic probability systems and AGM consistency has also been discussed in Shoham and Leyton-Brown (2009) in terms of the belief operator for revising belief sets, again, in a non-probabilistic framework. We derive and build on that observation in our framework and also establish a complete characterisation of lexicographic updating rules.====In the theory of decision under uncertainty, in addition to Blume et al. (1991), who consider lexicographic probabilities, there has a been some attention devoted to alternative approaches to dealing with zero probability events. Myerson (1986) provides axiomatic foundations for conditional probability systems. Ortoleva (2012) studies an alternative approach where once a zero probability occurs, the agent uses a belief over beliefs and the maximum likelihood rule to obtain posteriors. In the present work, such updating rules may violate AGM consistency, which is central to the analysis considered here. Karni and Vierø (2013) study a framework where the set of states itself can expand due to growing awareness and they consider the phenomenon of reverse Bayesianism. This requires that whenever the state space grows and the support of the prior belief is contained in that of the posterior, the prior can be obtained by applying Bayes rules to the posterior. The issue of updating ambiguous beliefs as defined in Schmeidler (1989) and Gilboa and Schmeidler (1989) has been discussed in Gilboa and Schmeidler (1993). They consider the problem of updating convex non-additive probabilities and establish the equivalence of the Dempster–Shafer rule for conditioning and maximum likelihood updating. Though in the present work we do not discuss ambiguous beliefs, our approach may be used to define updating rules for it. This extension could potentially provide us with a connection between ambiguity aversion and agents' attitudes to zero probability events. Finally, our extension involving updating preferences is also related to the above papers and Hanany and Klibanoff (2007), who study updating rules for preferences with multiple priors.====The outline of this paper is as follows: In section 2, we introduce the framework and provide a brief summary of AGM belief revision. In section 3, we study lexicographic updating rules and minimum distance updating rules are studied in Section 4. Finally, we study preference updating in Section 5. Some proofs are in the appendix.",Bayesian updating rules and AGM belief revision,https://www.sciencedirect.com/science/article/pii/S002205311830680X,January 2019,2019,Research Article,297.0
"Delacrétaz David,Loertscher Simon,Marx Leslie M.,Wilkening Tom","Nuffield College, University of Oxford, New Road, Oxford, OX1 1NF, United Kingdom,Department of Economics, Level 4, FBE Building, University of Melbourne, 111 Barry St, Victoria 3010, Australia,The Fuqua School of Business, Duke University, 100 Fuqua Drive, Durham, NC 27708, USA,Department of Economics, Level 3, FBE Building, University of Melbourne, 111 Barry St, Victoria 3010, Australia","Received 2 December 2016, Revised 30 October 2018, Accepted 21 November 2018, Available online 23 November 2018, Version of Record 29 November 2018.",https://doi.org/10.1016/j.jet.2018.11.004,Cited by (10),) result that buyers and sellers are complements. We introduce a general family of payoff functions that ensures decomposability and thus impossibility.,"The Coase Theorem provides the essential insight that any efficiency rationale for government intervention must be based on transaction costs. Absent such costs, the allocation that results from mutually beneficial trade will be efficient irrespective of initial ownership.====The mechanism design literature has shown that private information may constitute such a transaction cost. In particular, the path-breaking work by Vickrey (1961) and Myerson and Satterthwaite (1983) provides conditions under which efficient trade between privately informed buyers and sellers is impossible without running a deficit. While subsequent work by Gresik and Satterthwaite (1989), McAfee (1992), and Williams (1999) has expanded our understanding of these conditions, the literature has largely focused on homogeneous goods with single-unit demands and supplies.==== Given the role that the Coase Theorem plays in economics, understanding the conditions under which secondary markets cannot be expected to operate efficiently is of fundamental relevance.====In this paper, we derive conditions under which the impossibility of efficient trade extends to rich environments in which buyers and sellers demand and supply packages of heterogeneous objects and in which both buyers and sellers can trade with multiple agents on the other side of the market. In line with the strand of literature on the impossibility of efficient trade mentioned above, we assume private values throughout the paper. However, in contrast to most of this literature, we allow for multi-dimensional types.====Our approach is both general and intuitive. We begin by synthesizing the literature and by showing a surprising connection between the known impossibility theorems and a result due to Shapley (1962). Shapley's result implies that when trading is one-to-one, any buyer and any seller are complements under surplus maximization—the buyer's marginal product is larger when the seller is present than when he is not and, analogously, the seller's marginal product is larger when the buyer is present than when he is not. This complementarity turns out to have important consequences when the values of buyers and the costs of sellers are private information and there exist least efficient types who should never trade: a key insight from the Vickrey–Clarke–Groves (VCG) mechanism and the literature on dominant strategy implementation is that truthful revelation of private information is possible if and only if each agent receives his marginal product as a transfer (plus a constant).==== Because the market maker can extract only the social gains from trade associated with a reallocation, but must pay each agent his marginal product to reveal his information, efficient trade is impossible without running a deficit when the sum of the marginal products exceeds social gains from trade.==== Shapley's result thus implies the impossibility of efficient trade for any two-sided allocation problem with one-to-one trading because the complementarity between buyers and sellers implies that the sum of marginal products always exceeds the social gains from trade.====Building on this insight, we extend Shapley's result to develop a new impossibility theorem that applies to environments in which buyers can buy and sellers can produce multiple objects and in which buyers and sellers can trade with multiple agents on the other side of the market. We say that a buyer is ==== if his utility function is an ====, as defined by Ostrovsky and Paes Leme (2015), and extend the concept to sellers.==== The term “decomposable” refers to the fact that such an agent can be decomposed into ==== who can be assigned at most one object. Obviously, this is possible if an agent's payoff function is additive in the stand-alone values associated with each object. However, decomposability is substantially more general than that and includes, among other setups, the seemingly unrelated homogeneous goods model with decreasing marginal values and increasing marginal costs.====We show that a two-sided allocation problem is ====—that is, it can be represented as an assignment game between unit constituents and objects—if and only if every agent is decomposable. We then extend the results of Shapley (1962) to show that efficient trade is impossible in any assignment-representable problem.====We also introduce a new family of payoff functions satisfying what we call ====: Under SDD, an agent's valuation for a package of objects consists of the sum of the stand-alone utilities of these objects minus a discount that depends on the size of the package but not on the objects. We show that SDD is sufficient for decomposability.==== This family includes all payoff functions with unit demand and unit supply, the homogeneous goods model with multi-unit traders, additive payoffs, a version of Ausubel's (2006) heterogeneous commodities model with additively separable payoff functions, and any problems involving a mixture of agents with payoff functions of these forms.====Viewed from this angle, our results thus provide the unifying and, to our knowledge, novel insight that the underlying force behind the impossibility of efficient trade in the two-sided allocation problems of Vickrey (1961), Myerson and Satterthwaite (1983), Gresik and Satterthwaite (1989), and McAfee (1992) is that the payoff functions satisfy SDD. Of course, our results also imply that efficient trade with privately informed agents is impossible in the assignment game of Shapley and Shubik (1972), which is popular in the literature on matching with transfers, but has received relatively little attention in the mechanism design literature.====By providing a general and intuitive impossibility theorem for two-sided allocation problems under conditions that are easy to check because they relate to individual agents' payoff functions, our paper highlights the transaction cost that results from agents' strategic use of their private information about values and costs in two-sided settings. Beyond their implications for economics, along the lines discussed in the opening paragraphs, these results also emphasize a general tension between revenue and efficiency faced by market designers in two-sided settings. Moreover, SDD payoff functions are flexible and encompass as special cases payoff functions that are widely used in the literature. Nonetheless, they impose limited burden for agents' information acquisition, and for communication and computation: the only pieces of information required from an agent are his stand-alone utilities for each object and discounts for each possible package size. This contrasts with allocation problems of heterogeneous objects, for which, absent additional structure, the number of valuations required from every agent is equal to the power set of packages.==== This makes the SDD family of potential interest and value beyond the environments and problems we study in this paper.====The literature on Bayesian mechanism design has predominantly focused on settings with one-dimensional types, with little attention to the connection between allocation problems and assignment games. An important implication of our approach is that a broad class of models are assignment representable, including the two-sided market models of Vickrey (1961), Shapley and Shubik (1972), Myerson and Satterthwaite (1983), Gresik and Satterthwaite (1989), McAfee (1992), and a two-sided, additively separable version of Ausubel's (2006) model with heterogeneous commodities. Rather than being disjoint and independent problems, we demonstrate and characterize a connection among these models.====In a recent paper, Segal and Whinston (2016) derive impossibility results that revolve around tests for a multi-valued marginal core in allocation games with monetary transfers. Yenmez (2015) provides necessary and sufficient conditions for the converse of the marginal core condition to hold. The marginal core condition and its converse are useful in providing general conditions under which the (im)possibility result holds but may be difficult to apply in practice. Our results regarding assignment-representable problems provide a complementary approach for testing for the impossibility of efficient trade. By generalizing the assignment model to accommodate package demand and supply, we derive conditions based on the primitives of the model, namely the payoff functions.====Many matching problems are also assignment representable and thus our paper connects to the literature on matching with transfers. Initiated by Koopmans and Beckmann (1957), Shapley (1962), and Shapley and Shubik (1972), with recent contributions by, among others, Bikhchandani and Ostroy (2002), Echenique et al. (2013), Chambers and Echenique (2015), and Choo (2015), the literature on matching with transfers has, beyond concerns of stability and its relation to the core, paid limited attention to individuals' incentives to reveal what is plausibly their private information. We show that it is impossible to elicit such information without running a deficit if there are least efficient types on both sides of the market that never trade.==== Yenmez (2013) studies incentive-compatible matching with transfers and proves the possibility of efficiency under the converse assumption that all types of all agents always trade.====From a modeling perspective, our paper extends the package assignment model of Bikhchandani and Ostroy (2002) by introducing heterogeneity on the sellers' side.==== Our impossibility result implies that in one-to-one matching markets with transfers, no budget-balanced mechanism exists that is strategy-proof when least efficient types exist that should always remain unmatched. Although the settings differ, this result parallels Roth's (1982) finding that in a marriage market, in which there are no transfers, no mechanism exists that is strategy-proof for both sides and generates a stable matching.====Our paper also relates to the literature on the micro-foundations of the canonical model of price formation in markets. As first noticed by Arrow (1959), the Walrasian model is silent about the institutions that simultaneously discover and set the market clearing prices. Recent contributions by Satterthwaite et al., 2014, Satterthwaite et al., 2015 have focused on the performance of the ====-double-auction in environments with unit traders, allowing for the possibility of correlated types and interdependent values. Our work is complementary to this research agenda. We do not restrict the mechanism that the market maker employs, other than imposing incentive and individual rationality constraints, and we allow for a general trading environment apart from imposing private values.====The remainder of this paper is organized as follows. Section 2 describes the setup. In Section 3, we synthesize existing results to derive an impossibility result when it is efficient for all trades to be one-to-one. Section 4 contains our new result that extends the impossibility result to setups with many-to-many trading, provided that the allocation problems are assignment representable. In Section 5, we characterize assignment-representable problems and show that an allocation problem is assignment representable if all agents' payoff functions exhibit size-dependent discounts. Section 6 concludes. Proofs are in Appendix A.","Two-sided allocation problems, decomposability, and the impossibility of efficient trade",https://www.sciencedirect.com/science/article/pii/S0022053118306793,January 2019,2019,Research Article,298.0
Thomas Caroline,"Department of Economics, University of Texas at Austin, United States of America","Received 12 January 2018, Revised 13 November 2018, Accepted 21 November 2018, Available online 23 November 2018, Version of Record 27 November 2018.",https://doi.org/10.1016/j.jet.2018.11.003,Cited by (7),"This paper adapts the exponential/Poisson bandits framework to a model of reputation concerns. The result is a dynamic signalling game with changing types. We study a decision-maker who must choose the stopping time for a project of unknown quality when she is concerned both about social welfare and public beliefs about her ability, which is correlated with the project's quality. The decision-maker privately observes a Poisson process that is informative about whether the project will succeed or fail. In this setting the decision-maker has incentives to experiment for too long, both in the hope of a last-minute success, and because stopping hurts her reputation. We show, however, that exact efficiency can be achieved in equilibrium for a range of reputation concerns, provided they are not too strong. If the private signal is sufficiently informative, this range can be arbitrarily large. When efficiency cannot be achieved, distortions can take the form of excessive continuation.","Reputation concerns are a common explanation for decision-makers' reluctance to abandon unsuccessful experiments. The argument is that the agent in charge cares not only about the success or failure of her project, but also about outsiders' perceptions of her competence. These two objectives may conflict if a project's merit is informative about the agent's competence, as repealing her project would reveal that the agent's own assessment was sufficiently unfavourable to warrant cancellation. This causes a bias towards inefficient continuation. Even when she is privately convinced of her project's worthlessness, the agent might resist a repeal that would surely damage her reputation. In addition, the project might take a turn for the better, and gambling for resurrection might rescue the agent's reputation.====This paper adopts the exponential/Poisson bandits framework to model experiments. The agent's private information consists of “lumpy” pieces of good news arriving at the jumping times of a Poisson process, leading to discontinuous jumps in her assessment of her project's merit, which is correlated with the agent's competence. Unless she stops the project first, a publicly observed signal ultimately reveals the true quality of the project. We take the view that some investigations – legal probes, commercial or scientific research, policy evaluations – consist of a hypothesis that determines the direction of search, a sequence of smaller, encouraging although inconclusive insights or clues, before the arrival of a major, verifiable breakthrough (breakdown) proving (disproving) the hypothesis and making (breaking) the experimenter's reputation. In the course of legal prosecution, evidence might mount against the defendant until a “smoking gun” is found, providing incontrovertible evidence against the defendant, and allowing the prosecutor to secure a high-profile conviction.==== Scientific research will first produce a sequence of smaller, incremental advances – resolving one of many crucial steps in a proof, successful preclinical trials – before a “eureka moment”. Governments keen to justify their policy positions or to defend their projects might seek evidence from favourably inclined think tanks. Thatcher's reliance on market-orientated analysis supplied by the Institute of Economic Affairs is well documented (Denham, 2005).====The result is a dynamic signalling game with stochastically changing types. Three features are worth highlighting. The good Poisson news assumption implies that the socially optimal policy only stops the project at a countable set of dates, and the difference between two consecutive stopping dates is bounded below. This discretisation plays a central role in the equilibrium analysis, as it imposes a minimum length on deviations if they are to be profitable. Second, the fact that the sender's private information accrues gradually on the path of play introduces new nuances to the analysis of the signalling game, as the sender's type can change over the course of a deviation. In particular, the refinement of off-path beliefs requires some care. A third feature of the model is the assumption that the sender cannot fool the receiver in perpetuity by never stopping the experiment, as the game eventually ends with a public resolution of all uncertainty.====We find that exact efficiency can be achieved even in the presence of sizeable reputation concerns. Although private information and misaligned incentives are a force for inefficient over-experimentation, the socially optimal policy can be adopted in equilibrium when the degree of misalignment between preferences is below a threshold value. Given the intensity of the decision-maker's reputation concerns, whether exact efficiency can be achieved in equilibrium depends on the informativeness of the decision-maker's private signals, and the scope for efficiency can be quite large. In particular, sufficiently improving the quality of the decision-maker's private information, thereby increasing the degree of information asymmetries, can improve welfare. Finally, when the degree of misalignment between preferences is too acute, the inefficiency can take the form of excess delay.====Specifically, we consider a continuous-time signalling game between a decision-maker, who controls the duration of an experiment, and an observer. The decision-maker aims to maximise a weighted average of the expected social payoff from the experiment and her expected ====, modelled as the observer's undiscounted final belief about her competence. The decision-maker's reputation is based only on the publicly available information and, in equilibrium, on her strategy. To fix ideas, it is useful to keep in mind the example of a prosecutor in charge of an investigation, who values the opinion of her peers or the general public. The eventual discovery of evidence leading to a conviction reflects positively on the public perception of the prosecutor's expertise. Conversely, a failure to convict might betray the prosecutor's poor instincts, lack of attention to detail, and so on.====The experiment's state is uncertain and initially unknown to both parties, so that information is symmetric ex-ante. It determines whether the experiment will eventually succeed or fail: In the good state the experiment succeeds at an exponentially distributed random time, provided the decision-maker does not stop it first. Conversely, a bad experiment eventually fails at a random time that is also distributed exponentially, although with a lower parameter. That is, a bad experiment is more likely to result in a repeal due to a lack of success, rather than in failure proper. The idea is that, if a defendant is in fact guilty, the investigation will eventually find a “smoking gun” and secure the defendant's conviction. But if the defendant is innocent, although the inquiry might unwittingly find proof of that innocence and result in acquittal, the more likely outcome is that the inquiry is terminated and the case is dropped because of insufficient evidence. Success and failure – conviction and acquittal – are ==== observed and conclusively reveal the state, thereby resolving all uncertainty and ending the game.====Over the course of the experiment, the decision-maker privately observes a sequence of encouraging although inconclusive pieces of news – clues that are suggestive of the defendant's guilt, but not sufficient to establish it, or not admissible in court. Specifically, one additional clue arrives at each jumping time of a standard Poisson process whose intensity depends on the underlying state: incriminating clues arrive more frequently if the defendant is guilty. Therefore, the arrival of a piece of news is “good news”, although inconclusive, causing the decision-maker's posterior belief regarding an eventual success to ==== up, whereas the absence of news is “bad news” and causes her posterior to ==== down. The private information she has accumulated so far determines the decision-maker's ====. It ==== over the course of the game.====In the absence of additional evidence, when does the decision-maker stop the experiment? Upon stopping, her reputation is based only on the publicly available information and, in equilibrium, on her strategy. Extended investigations have a cost: they are socially worthwhile if the defendant is guilty, but not if he is innocent. But not dropping the case when it is socially efficient to do so has a number of advantages when it comes to reputation: it may convince the observer that she has privately received encouraging information about the experiment. It might also produce the smoking gun and result in a conviction that boosts her reputation, or at least produce encouraging clues that justify the continued investigation in retrospect.====Our first contribution is to identify conditions under which exact efficiency obtains, and to characterise the nature of inefficiencies when it does not. We find that the social welfare maximising policy can be adopted in equilibrium, and ==== efficiency can be achieved, whenever the intensity of the decision-maker's reputation concerns is below a threshold level. The discretisation of stopping under the socially optimal policy implies that a deviation must delay stopping by a fixed interval of time in order to pool with higher types and bring reputation gains. It therefore necessitates strict social losses and is not profitable overall whenever the intensity of the decision-maker's reputation concerns is below a threshold.====As in traditional signalling games, there may be multiple equilibria. However, we show that only the equilibrium in which the planner policy is played survives refinement according to the D1 criterion of Cho and Kreps (1987). This refinement requires that off-path beliefs only put weight on the type(s) with the strongest incentives to deviate from the equilibrium. Applying the D1 criterion in our signalling game with stochastic types is not straightforward, as a player's type might change over the course of a deviation. Our analysis exploits the fact that information can only be gained, not lost.====When reputation concerns exceed the critical threshold, we show that inefficiencies can take the form of excess delay. We restrict attention to policies characterised by a constant threshold belief for types with at least one piece of news. (There may be other equilibria.) Lowering the belief threshold increases the social cost and decreases the reputation benefit of deviating so as to pool with higher types. We construct equilibria in which the investigation is abandoned inefficiently late if there is initial good news, while a decision-maker with no news stops before the social planner would – she is the “lowest” type whom even the lowest possible reputation cannot penalise for stopping early. This difference in behaviour is another consequence of changing types: the arrival of a single piece of news would commit the decision-maker to an inefficient policy, and the lowest type has an incentive to stop early in order to mitigate this possibility. In equilibrium, this type's behaviour is constrained efficient given subsequent distortions.====Our second main finding is that for any intensity of the decision-maker's reputation concerns, there exists an information structure – where the private signal is sufficiently informative – under which she implements the socially efficient decision-rule, and another, distinct information structure – where the private signal is sufficiently uninformative – under which deviations from the socially optimal decision-rule are profitable. Thus, even a prosecutor intensely concerned with her reputation can adopt the first-best decision-rule if her investigative resources are sufficiently good. Conversely, even a thoroughly socially minded prosecutor will behave inefficiently if her investigative resources are too poor.",Experimentation with reputation concerns – Dynamic signalling with changing types,https://www.sciencedirect.com/science/article/pii/S0022053118306781,January 2019,2019,Research Article,299.0
Siemroth Christoph,"University of Essex, Department of Economics, Wivenhoe Park, Colchester, CO4 3SQ, UK","Received 6 February 2018, Revised 27 September 2018, Accepted 13 November 2018, Available online 15 November 2018, Version of Record 15 November 2018.",https://doi.org/10.1016/j.jet.2018.11.002,Cited by (10),"When can policy makers use policy-relevant information from financial market prices and how does policy affect price informativeness? I analyze a novel setting with noise where a policy maker tries to infer information about a state variable from prices to improve policy decisions, and policy in turn affects asset values. I derive a necessary and sufficient condition for the possibility of information revelation in equilibrium, which might not be possible if the policy reaction to prices punishes traders for revealing their information. If the policy maker is uninformed, then policy objectives do not change price informativeness, but they do if the policy maker has independent information about the state. I also analyze policy maker transparency, and find that policy makers with objectives having a large impact on asset values should publish their information before trading to make prices more informative. In other cases, intransparency can be optimal.","One of the key insights in financial economics is that asset prices can reveal the information that traders have. Besides traditional financial markets that can aggregate information about the state of companies or the economy as a whole, prediction markets have been used successfully to forecast non-financial outcomes such as elections (e.g., Berg et al., 2008), fulfilling project deadlines (e.g., Cowgill and Zitzewitz, 2015), or whether scientific studies will be successfully replicated (e.g., Dreber et al., 2015). Thus, financial market prices can potentially be valuable tools in helping policy makers by providing information or forecasts in many domains. For example, regulators might learn about bank health from bond prices and could use it for intervention decisions. And central banks already monitor asset prices to learn about economic fundamentals or inflation expectations and use it to improve policy (e.g., Bernanke and Woodford, 1997).====However, if policy affects asset values, then as soon as policy reacts to asset prices, it can change the incentives of traders and affect the informativeness of prices. Most of the above results were arguably observed in settings where traders did not anticipate that a policy maker would react to (the information contained in) the asset prices. So an important question is if and when traders would still trade in a way that reveals (some of) their information if they correctly anticipate that this information is used for policy purposes.====This paper answers this question by deriving the conditions (in terms of policy maker preferences and asset properties) under which the policy maker can and cannot learn from prices to improve policy in equilibrium in a setting with noise. These results can be useful to design institutions/assets that allow for better information revelation. Moreover, the paper investigates how policy maker preferences/objectives affect how much information is revealed by prices. Finally, I use the model to address the question of policy maker transparency, i.e., under which conditions a policy maker should reveal her private information to other market participants if the goal is to extract information from the market.====To answer these questions, I adapt a CARA-normal asset pricing model with noise to include a policy maker, who moves after and potentially reacts to the financial market. The value of the risky asset depends both on a fundamental state variable ==== (which is standard) and a policy variable ==== (new). Informed traders obtain signals about the state ====, so financial market prices can potentially reveal some information about the state. The optimal policy of the policy maker depends on the state ====, and since the policy maker is not perfectly informed about the state ====, she tries to infer information about the state from financial market prices and in effect policy reacts to these prices.====A potential problem in similar settings without noise—where prices react to policy and policy reacts to prices—is that traders may not reveal their information and there may not even exist an equilibrium (e.g., Bernanke and Woodford, 1997; Bond et al., 2010). This is because the policy reaction to prices can punish traders for revealing their information, so that the pricing problem is a self-defeating prophecy (Siemroth, 2017). To give one informal example of a self-defeating prophecy, consider a bank bond whose value depends on the financial health of the bank (the state ====) and a regulator decision (policy decision ====). Informed traders know something about bank health that the regulator does not know. If bank health is bad, traders can trade at low prices, but this reveals the need for an intervention to the regulator,==== who for example gives a guarantee to the bank which improves the asset value. Thus, the trader forecast of a low asset value is falsified by the regulator decision and traders lose money since they sold below value. If traders, on the other hand, trade at high prices despite low bank health, then it signals that there is no need for an intervention, hence there is no intervention and the asset value remains low. Again, traders lose money since they bought above value. Forward looking traders anticipate these adverse policy reactions and therefore no equilibrium may exist. This problem can occur because asset values are endogenous, so that the action of an agent may punish traders by affecting asset values. To the best of my knowledge, this is the first paper to investigate the problem of self-defeating prophecies in a noisy rational expectations equilibrium (REE) framework.====Adapting a new solution approach for noisy REE (e.g., Breon-Drish, 2015), I can solve for equilibria with an uninformed policy maker who has general preferences allowing for non-linear policy reaction functions, which would be hard to deal with in the standard solution approach. I derive a sufficient and necessary condition for the existence of (partially) revealing equilibria in a fairly general class of equilibria that includes and generalizes the usual linear noisy REE, which answers the question under which circumstances market prices can help policy makers.====This condition requires invertibility of a risk-weighted expectation of the asset value in a noisy statistic of the state, which converges to the asset value evaluated at the optimal policy in the noiseless limit.==== Thus, the condition ensures that the endogenous asset value—after taking into account the policy reaction—really is strictly monotone in the information that the price reveals, otherwise traders would be better off not clearing the market. Put differently, the condition implicitly defines the set of policy maker preferences for which the policy reaction to prices does not punish traders for revealing information. In terms of price informativeness, if the policy maker is uninformed, then the policy maker objectives/policy do not affect how much information is revealed in equilibrium, only whether a (partially revealing) equilibrium exists.====In the online appendix, I also demonstrate that noise can solve the problem of self-defeating prophecies in some settings such as the one considered by Bernanke and Woodford (1997): Because noise prevents full revelation of trader information, the informed retain incentives to trade on information, making prices informative while they would not be without noise. Thus, paradoxically, noise might sometimes improve information revelation in settings where asset values are endogenous to policy.====If the policy maker is informed, i.e., receives imperfect private signals about the state, then policy maker preferences not only determine whether informative equilibria exist, but also how informative they are. Because asset values are affected by policy decisions, a policy maker with independent information introduces additional ‘policy risk’ in asset returns beyond the usual risk over asset fundamentals, since traders do not know the independent information of the policy maker (which in part determines policy). This is why policy maker preferences influence how aggressively the informed trade on information. Moreover, policy risk can induce strategic complementarity leading to multiple equilibria, while the setting with uninformed policy maker features unique equilibria.====Comparative statics show that more extreme policy maker preferences (in the sense that policies with a large influence on asset values are preferred) tend to decrease price informativeness, because the policy reaction adds risk by amplifying the asset value variance and due to the uncertainty underlying the policy reaction.==== Consequently, the model suggests that market-based policy—which uses market information as input—works better with policies that have a smaller impact on asset values. The comparative statics also show that policies which move against fundamental shocks can increase information revelation by markets, because policy dampens the asset variance from the perspective of the traders. Since the policy maker can make better decisions and achieve a higher utility with better information, a larger price informativeness coincides with better real decisions and welfare in this setting. Thus, the model enables an analysis of the impact of information contained in financial market prices on real decisions and welfare.====If the policy maker has private information, then a natural question is under which conditions this information should be revealed publicly before trading, which is especially important in the context of central bank transparency. The model shows that the policy maker should reveal her information if she has extreme intervention preferences. This is because transparency removes the policy risk for traders, while intransparency implies a lot of policy risk for traders and therefore limits trading on information, making prices less informative. Hence, transparency is to the policy maker's benefit especially when policy has a large impact on the market. However, there are cases where intransparency (not revealing the private information) is optimal, for example if the policy maker has mild preferences such that policy moves against fundamental shocks. In this case, policy acts like a dampening force against asset value movements driven by fundamentals. Intransparency has a positive effect here, as it ensures that fundamentals and policy are two negatively correlated shocks from the perspective of traders, which reduces the asset value variance.====This paper is related to the theoretical literature investigating the self-defeating prophecy or “double endogeneity” problem, including Woodford (1994) and Bernanke and Woodford (1997) on central banks, Birchler and Facchinetti (2007) on banking supervision with a good description of the formal problem, Bond et al. (2010) on corrective actions, and Siemroth (2017) unifying some of the earlier contributions and adding applications such as corporate prediction markets. In all of these models, a decision maker reacts to information revealed by the financial market and in turn affects asset values, which can lead to equilibrium non-existence. Moreover, Prescott (2012) and Sundaresan and Wang (2015) study a related problem in contingent capital models, where a mechanical rule instead of a decision maker reacts to financial market prices, which can also cause a self-defeating prophecy problem. None of these models feature a setting with noise, which is crucial to investigate how much information is revealed, since equilibrium prices in the absence of noise are typically fully revealing (e.g., Radner, 1979). This paper adds to the existing knowledge by showing that self-defeating prophecies can occur also in settings with noise, but also that noise can solve the self-defeating prophecy problem in some cases.====Bond and Goldstein (2015) (BG in short) analyze a related CARA-normal noisy REE model with policy maker, but in their setting the asset value does not directly depend on ====, only indirectly via the government action that depends on the state ====. Hence, in their model, traders use their information to predict the government's information and action rather than the state. For this reason, their implications for transparency are different, because as soon as the government's information is public in their setting, there is no need for traders to trade on their information, so price informativeness suffers. In the setup here, the conclusions on transparency are more nuanced, and transparency typically helps the policy maker to obtain more information from the market, while there are some cases where transparency is undesirable. Moreover, the setup in BG does not allow for self-defeating prophecies, so the present paper contributes by analyzing if and when these can arise, which makes a major qualitative difference when equilibrium non-existence is interpreted as breakdown of information revelation. This is especially true in my case of an uninformed policy maker with non-linear policy rules, which allow for a larger set of applications.====BG also investigate when it pays for the government to commit itself to using more or less financial market information to conduct policy (relative to the ex post optimal rule). While commitment is not the focus here, my analysis contributes by parameterizing the informed policy maker objectives and deriving the effect of preference parameters on price informativeness, thereby answering how policy maker objectives affect the financial market. Moreover, I contribute by discussing the possibility and causes of equilibrium multiplicity due to the presence of the policy maker.====The current paper is also related to the literature investigating the real effects of financial markets via an informational channel. In most of this literature, the ‘real effect’ is the financial market information impact on corporate decisions, as in Goldstein and Guembel (2008); Foucault and Gehrig (2008); Ozdenoren and Yuan (2008); Goldstein et al. (2013); Edmans et al. (2015); Dow et al. (2017). Typically, these papers rule out equilibrium non-existence due to feedback via technical assumptions, while I highlight that these cases can be important even from an applied point of view. In the present paper, the real effect includes any effect of price information on third parties, which depending on the application can include central bank policy and their consequences, banking regulation, or corporate decisions.",The informational content of prices when policy makers react to financial markets,https://www.sciencedirect.com/science/article/pii/S0022053118306768,January 2019,2019,Research Article,300.0
"Hillebrand Elmar,Hillebrand Marten","EEFA Research Institute, Muenster, Germany,Department of Economics, Goethe University Frankfurt, Theodor-W.-Adorno-Platz 3, 60323 Frankfurt am Main, Germany","Received 1 June 2018, Revised 5 November 2018, Accepted 7 November 2018, Available online 10 November 2018, Version of Record 13 November 2018.",https://doi.org/10.1016/j.jet.2018.11.001,Cited by (8),This paper develops a dynamic general equilibrium model with an arbitrary number of different regions to study the economic consequences of climate change under alternative climate policies. Regions differ with respect to their state ,"On December 12, 2015, 195 countries joined the Paris Agreement to strengthen the global response to climate change. Its central aim is to keep the increase in temperature relative to pre-industrial level below two degrees Celsius until the end of the century. Individual commitments how to achieve this goal, however, are non-binding and involve voluntary climate policies chosen on the national level, so called ‘Nationally Determined Contributions’. This outcome reflects the complexity of any negotiations about joint climate policies which are shaped by the bargaining power of individual countries and the trade-offs between their political interests.====These observations suggest that climate change is inherently an ==== and understanding the incentives for individual regions to implement a given climate policy is key for the success of any climate agreement. Conceptually, this calls for a theoretical framework which incorporates the trade-off between the interests of different regions and permits to analyze the effects of alternative climate policies at the regional level. In this paper, we develop such a multi-region model and derive an optimal climate policy that each region has an incentive to implement.====To successfully combat climate change, each country must contribute to the common goal by reducing its emissions. A major obstacle to determine these individual contributions is regional heterogeneity. For instance, regions differ considerably in their dependence on fossil fuels and the mix of technologies they use to produce energy inputs. Thus, reducing emissions is more costly for some regions than for others. In addition, various other differences such as state of economic development, future growth prospects, or vulnerability to climate damages play an important role in climate negotiations. This raises a first question that we will address in this paper: ====In market economies, any equilibrium allocation is the outcome of decentralized decisions made by firms and consumers who respond to incentives and taxes set by governments. One way to reduce emissions is to levy a tax on emissions. Thus, a second questions to be addressed is: ====?====For a climate agreement to be successful, each region must have an incentive to implement the proposed policy. Thus, a third question to be answered is: ====The present paper addresses these questions in a multi-region framework which incorporates several sources of regional heterogeneity which play a crucial role in climate negotiations. Our model builds on the single-region framework in Golosov et al. (2014) to which we add several new dimensions. First, we adopt a multi-region structure featuring an arbitrary number of different regions. This is clearly required for our analysis. Second, we devise a different model of the production process which distinguishing explicitly the ==== at which fossil fuels are ==== and the ==== at which they are ==== to produce energy such as electricity or heat. This distinction is important because empirically regions differ considerably not only in their dependence on fossil fuels but also in the mix of technologies to produce energy outputs. Modelling the energy stage explicitly allows us to capture these differences and study how climate policies induce transitions from dirty to clean technologies in each region. Finally, we allow for climate damages to differ across regions. This is another important source of heterogeneity, notably because poorer countries tend to be more vulnerable to climate change. The economic part is complemented by a climate model describing how emissions evolve in the atmosphere and damage the economy.====With these features, our model falls into the class of integrated assessment models which incorporate the full interactions between climate variables and the economic production process. In the literature, a large class of these models is based on the DICE framework pioneered by Nordhaus (1977) and its multi-region extension, the RICE model developed in Nordhaus and Yang (1996) and refined in Nordhaus and Boyer (2000). A typical feature of these models stressed in Hassler et al. (2016) is that solutions are derived as planning problems without explicit market structures and prices. Thus, these models make only limited use of dynamic general equilibrium theory which confines the class of policies that can be analyzed. In addition, the RICE-framework in conjunction with the employed methodology entails strong restrictions on trade between regions.====The model developed in Golosov et al. (2014) takes full advantage of dynamic general equilibrium theory with explicitly defined markets and price formation.==== Hassler and Krusell (2012) provide an extension to a multi-region framework which distinguishes oil-producing and oil-consuming countries. To preserve analytical tractability, they impose strong restrictions on trade between regions which are only allowed to trade oil which is the only fossil fuel.==== A major difference of our model to Hassler and Krusell (2012) is that we allow for trade between regions and intertemporal borrowing and lending on an international capital market. In fact, this assumption will be key for our results.====The general contribution of our analysis to the literature are the answers to the questions posed above. First, we show that there is a unique efficient allocation which determines the optimal level of emissions for each region. Second, we show that this efficient allocation can be implemented by a uniform tax on emissions for which we derive a closed form solution. Third, we devise a transfer scheme which distributes the revenue from taxation such that each region has an incentive to implement the optimal emissions tax.====A crucial feature of our model to obtain these results is that the ==== of determining an optimal emissions tax can strictly be separated from the ==== how global tax revenue should be shared via transfers. This separability result requires a standard restriction on consumer preferences combined with borrowing and lending between regions on a frictionless capital market. Static versions of this result first proved by Bergstrom and Cornes (1983) are well-known in the public goods literature. To the best of our knowledge, we are the first to extend it to a dynamic setting and apply it to the climate problem. This is the methodological contribution of our paper.====The bargaining power of regions in our model is represented by a weighting scheme which aggregates welfare in each region to a single utility index. An important consequence of separability is that the optimal emissions tax does not depend on this aggregation. Thus, determining the optimal carbon tax involves no trade-off between political interests and regions could directly agree on this policy. Based on this finding, our analysis suggests that the major political issue is how tax revenue should be shared via transfer payments. Only the choice of a transfer scheme should therefore be the subject of negotiations.====To determine transfers in accordance with incentive constraints, our final theoretical result proposes a simple transfer policy under which each region is strictly better off relative to the Laissez faire scenario where no measures against climate change are taken. The proposed policy thus satisfies the property of ==== discussed, e.g., in Eyckmans and Tulkens (2003) which seems a minimal requirement for the optimal tax policy to be implemented by each region. We present a calibrated numerical example to quantify the range and size of Pareto-improving transfers between rich and poor countries. The results show that rich countries can afford to transfer initially 1.6% and subsequently up to 2.3% of their GDP to poor countries and would still benefit from a global agreement to implement the optimal tax policy.====The paper is organized as follows. Section 1 introduces the model. The decentralized equilibrium solution under different climate policies is studied in Section 2. Section 3 studies optimal allocations obtained as solutions to a planning problem. Optimal climate policies which implement the optimal solution as an equilibrium allocation are studied in Section 4. A calibrated example presented in Section 5 quantifies our results. Extensions of our basic framework and robustness of our results are discussed in Section 6. Section 7 concludes, mathematical proofs are placed in the appendix.",Optimal climate policies in a dynamic multi-country equilibrium model,https://www.sciencedirect.com/science/article/pii/S0022053118302394,January 2019,2019,Research Article,301.0
"Beauchêne Dorian,Li Jian,Li Ming","MAPP Economics, France,School of Economics, Shanghai University of Finance and Economics, China,Concordia University and CIREQ, Canada","Received 26 January 2017, Revised 11 October 2018, Accepted 21 October 2018, Available online 6 November 2018, Version of Record 22 November 2018.",https://doi.org/10.1016/j.jet.2018.10.008,Cited by (32),We study a persuasion game à la ==== where players are ambiguity averse with maxmin expected utility (,"Ambiguity is present in many settings of persuasion. Countries often keep their foreign policy intentionally ambiguous. Manufacturers of brand-name drugs often emphasize the uncertainty about the effectiveness and safety of their generic competitors.==== Finally, Alan Greenspan has taken pride in perfecting the art of “Fed-Speak,” with which he “would catch (him)self in the middle of a sentence” and “continue on resolving the sentence in some obscure way which made it incomprehensible.”====In all the examples above, a sender who controls access to some information chooses how to communicate with a decision maker who is uninformed—a receiver, whom we will refer to as “Sender” and “Receiver” hereafter. Such environments have been studied by Kamenica and Gentzkow (2011) in a “Bayesian persuasion” framework, where Sender persuades Receiver by selecting a ==== communication device (or “signal,” in the terminology of Kamenica and Gentzkow 2011). In contrast to Kamenica and Gentzkow (2011), who assume that Sender and Receiver are both expected utility maximizers, we allow Sender and Receiver to be ambiguity averse and allow the communication device to be ambiguous. We investigate ==== and ==== Sender can benefit from using ambiguous communication devices.====Going back to the example of a brand name drug producing pharmaceutical company (Sender/“he”) who wants to persuade a physician (Receiver/“she”) to refrain from prescribing the generic competitor of one of his drugs. The pharmaceutical company could commission studies on the (in)effectiveness and (un)safety of the generic drug. If the physician were Bayesian, as Kamenica and Gentzkow (2011) assume, she would form a belief about the effectiveness and safety of the generic drug based on the results of the studies and trade it off against the extra expense of the brand-name drug. She will then make her decision on whether to prescribe the generic drug based on her belief. Under these assumptions, if the physician is predisposed to prescribing the generic drug, then it is not possible for the pharmaceutical company to completely dissuade the physician from doing so. However, as we demonstrate in our example of Section 2, if the physician is ambiguity averse, in particular, if she is maxmin expected utility (EU) maximizers à la Gilboa and Schmeidler (1989), and if the pharmaceutical company has ambiguous tests at his disposal, he is able to achieve just that.====For clarity, we refer to Kamenica and Gentzkow's (2011) communication devices/signals as ==== devices. In our model, we introduce ==== devices. An ambiguous communication device is a set of probabilistic devices. Upon reception of a message, Receiver updates her (unique) prior via the full Bayesian rule (à la Pires 2002; Epstein and Schneider 2007), i.e., she updates her prior with respect to each ==== device, which yields a set of posterior beliefs for each message. Sender and Receiver are ambiguity averse à la Gilboa and Schmeidler (1989)—when evaluating an action/ambiguous device, they compute their expected utility for any possible posterior belief and rank actions/communication devices according to their minimum expected utilities.====The use of ambiguous devices carries both opportunities and challenges. First, Sender benefits from increased leeway regarding how to control the information flow to Receiver. Therefore, he can induce Receiver to act in such a manner that would not be feasible with probabilistic devices alone. On the other hand, Sender introduces ambiguity where there was initially none, which would generically decrease his ex ante utility (given that he commits to a device before learning anything). It is therefore unclear a priori whether the expert can strictly benefit from ambiguity.====In this paper, we provide a characterization of Sender's optimal payoff under ambiguous persuasion by examining two aspects of the environment. First, we present a “====,” namely, a characterization of the possible profiles of posterior sets that are achievable with ambiguous devices, which precisely pins down the extent to which ambiguous devices expand beyond the Bayes plausibility condition given by Kamenica and Gentzkow (2011). Second, we demonstrate that, when using an ambiguous device consisting of a set of probabilistic devices with different expected payoffs, Sender can approximately achieve the highest payoff among them. He does so through mixing probabilistic devices via the use of ====, which are messages that induce the same set of posteriors and therefore lead to the same action from Receiver. In effect, Sender can always hedge against ambiguity created by his own choices. Consequently, with the construction of synonyms, in optimal ambiguous persuasion, every probabilistic device will give Sender the same expected payoff. Building on these two intermediate findings, our main result, Proposition 1, characterizes Sender's optimal payoff as the ==== projection of the ==== of an interim value function, which is defined as Sender's interim expected payoff according to one particular posterior and given that this posterior belongs to Receiver's posterior set. Therefore, our interim value function depends on a vector/set of posteriors, which are generated by full Bayesian updating based on the ambiguous device. Our result is reminiscent of the “concavification” result of Kamenica and Gentzkow (2011), but the upper bound of the projected concave closure, as a consequence of synonyms and hedging, is somewhat surprising, which brings fresh insights that are quite apart from Kamenica and Gentzkow's.====Further, we show that the use of synonyms (albeit in a weaker sense) is also necessary for optimal ambiguous persuasion. We demonstrate that generically either the optimal ambiguous devices use weak synonyms, which are messages that elicit the same Receiver action, or ambiguous persuasion is not more valuable than Bayesian persuasion.====We then proceed to explore the structure of optimal ambiguous device in two special applications.====In the first application, we consider the case when Sender has a most preferred action, which is also “safe” for Receiver, in that it has the highest worst-case payoff for Receiver. Even though this action might not be ex ante optimal, our splitting lemma suggests that, through designing an optimal ambiguous device, Sender can create maximum (ex post) ambiguity at all messages and succeed in persuading Receiver to always take the safe action, which is not possible under standard Bayesian persuasion.====In the second application, we consider the frequently studied “uniform-quadratic” case made popular by Crawford and Sobel (1982). We characterize the optimal simple ambiguous signal structure, where each signal realization is associated with two possible posteriors, both of which are uniform distributions over intervals. The optimal simple ambiguous signal structure features an equal partition of state space into finitely many unambiguous intervals and maximal ambiguity within each unambiguous interval. This is very different from the conclusion from standard Bayesian persuasion, where Sender finds it optimal to perfectly reveal all the information to Receiver, despite the conflict of interest between them. This provides a justification for ambiguity of communication even when Sender appears to have commitment power (for example, rating agencies, who are long-run players and have relatively stable rating categories).====  In this paper, we adopt the full-commitment assumptions of Kamenica and Gentzkow (2011) but extend their model to study ambiguous communication devices. In this respect, a recent paper by Laclau and Renou (2016) is related to our work. Laclau and Renou (2016) consider ====, where Sender sends the same (probabilistic) signal to multiple receivers with heterogeneous prior beliefs, which can be alternatively interpreted as a Bayesian sender persuading an ambiguity-averse Receiver with multiple priors.==== They characterize a splitting lemma, the counterpart of Kamenica and Gentzkow's (2011) concept of Bayes plausibility, and provide a version of “concavification” for the case of multiple priors and a single probabilistic communication device. We differ from their approach by focusing on the case when there is no prior ambiguity while Sender may use an ambiguous communication device. As a result, hedging (via the use of synonyms) plays a unique role in our characterization. We also assume both Sender and Receiver can be ambiguity averse to ensure symmetric information.====We demonstrate in our applications that Sender may want to commit to ambiguous signals, even if Sender is himself ambiguity averse and even if he would like to fully reveal information to Receiver absent ambiguous signals. Thus, our results can be viewed as providing a new justification for the widespread use of vague language in interpersonal and organizational communication. Previous work has focused on cheap-talk communication in the manner of Crawford and Sobel (1982), where Sender does not have commitment power (see Sobel 2013 for a review). Blume et al. (2007) and Blume and Board (2014) show that the presence of vagueness may facilitate communication between Sender and Receiver. Kellner and Le Quement (2017) solve a simplified two actions/two states game where ambiguity is present in Receiver's priors but not as a strategic choice of Sender. They show that Sender would use more messages under this assumption than with the regular Bayesian prior. In a follow-up paper, Kellner and Le Quement (2018) introduce endogenous ambiguous messages into the cheap-talk framework of Crawford and Sobel (1982). They demonstrate that the possibility of ambiguous messages, coupled with ambiguity aversion of Receiver, may improve communication between Sender and Receiver. These two papers' approach differ from ours in that they do not assume that Sender can commit. Lipman (2009) argues that it is puzzling that vagueness of language pervades in seemingly common-interest situations. To a certain extent, our results in the uniform-quadratic example demonstrate that a common-interest situation with expected utility maximizers, in the sense that Sender and Receiver both prefer that information be fully revealed ====, can easily turn into one that is not.====The introduction of ambiguity aversion into game theoretical models are supported by recent experimental findings that people dislike betting on an event with unknown probability.==== This phenomenon has attracted significant interest in theory and applications.==== The literature that explores the role of ambiguity aversion in games and mechanisms has been steadily growing (Ayouni and Koessler, 2017; Bose et al., 2006; Di Tillio et al., 2016; Frankel, 2014; Lopomo et al., 2011, Lopomo et al., 2014; Wolitzky, 2016). Bade (2011) and Riedel and Sass (2014) both study complete information games where the players, in addition to playing mixed strategies, are allowed to use ambiguous strategies in a manner similar to ours. Most relevant to ours, Bose and Renou (2014) introduce similar ambiguous devices in a communication stage preceding a mechanism design problem. Their key insight is that the designer can cleverly use these devices to exploit the ambiguity aversion of the agents. They show that a wider set of social choice functions are implementable with ambiguous communication devices.====The rest of our paper is structured as follows: Section 2 illustrates how ambiguous communication devices can be used to benefit Sender. Section 3 introduces the framework and presents the persuasion game. Section 4 characterizes the value of an optimal ambiguous communication device, which is the maximal projection of the concave closure of sender's interim expected utility function, and demonstrates how to construct an optimal ambiguous device. Section 5 provides two examples in which ambiguous communication devices with synonyms improve upon Bayesian persuasion. Section 6 discusses some additional constraints on ambiguous devices such as dynamic consistency and positive value of information from ambiguous persuasion. Section 7 concludes. Proofs omitted in the main text are relegated to the Appendix.",Ambiguous persuasion,https://www.sciencedirect.com/science/article/pii/S0022053118306653,January 2019,2019,Research Article,302.0
Lang Matthias,"University of Munich (LMU), Department of Economics, Geschwister-Scholl-Platz 1, 80539 Munich, Germany,CESifo, Munich, Germany","Received 11 April 2017, Revised 16 October 2018, Accepted 25 October 2018, Available online 30 October 2018, Version of Record 12 November 2018.",https://doi.org/10.1016/j.jet.2018.10.012,Cited by (4),"Consider managers evaluating their employees' performances. Should managers justify their subjective evaluations? Suppose a manager's evaluation is ====. Justifying her evaluation is costly but limits the principal's scope for distorting her evaluation of the employee. I show that the manager justifies her evaluation if and only if the employee's performance was poor. The justification assures the employee that the manager has not distorted the evaluation downwards. For good performance, however, the manager pays a constant high wage without justification. The empirical literature demonstrates that subjective evaluations are lenient and discriminate poorly between good performance levels. This pattern was attributed to biased managers. I show that these effects can occur in optimal contracts without any biased behavior.","This paper analyzes communication in a principal–agent model in which the principal's performance measure is unobservable to the agent and non-verifiable by third parties. Such subjective measures are widely used in practice because verifiable, i.e., objective, performance measures are often unavailable.==== Their subjectivity allows the principal to choose whether and how to disclose and to justify her evaluation of the agent's work. Hence, a hold-up problem arises: The principal wants to incentivize the agent to exert work effort, but these incentives depend on an appropriate evaluation in the end. Therefore, HR departments and personnel policies place great emphasis on feedback and communication of evaluations.==== Nevertheless, empirically, subjective evaluations are distorted, and wage dispersion for the best evaluations is low.==== These empirical observations are referred to as biases, which supposedly arise from supervisors' mistakes. In response, a whole industry has sprung up to provide training for supervisors. Alternatively, “some companies go so far as to rate employees on a bell curve,” requiring supervisors to match a given distribution with their evaluations, as forcefully advocated by, e.g., Jack Welch, a former and renowned CEO of General Electric (New York Times, 2013).==== I show that both responses may be misplaced because adjusting wages and communication to the subjectivity of evaluations is optimal and does not require any bias. The wage and communication pattern results from optimal contracting with standard preferences.====To explain these empirical observations, I study the communication of subjective evaluations. In the model, an agent (he) works for a principal (she) who privately receives information about the agent's performance. The principal has two options: she directly reports or justifies her evaluation. Justification of subjective evaluations is a common HR practice: “92% require a review and feedback session as part of the appraisal process” (Dessler, 2008, p.==== ====366). If the principal provides justification, she cannot send low messages for good performance. Nevertheless, the principal's message is not necessarily truthful, and providing justification is costly. Below, I discuss some interpretations of this type of justification. The agent replies with an unverifiable message as to whether justification was provided. As the messages are the only third-party enforceable information, the contract only depends on these messages. I investigate the resulting communication pattern: in equilibrium, the principal justifies only poor evaluations. In this case, wages increase in the evaluation. For good evaluations, the principal in equilibrium saves the trouble of explaining them and simply pays a high wage, which yields pooling and wage compression at the top: leniency —, i.e., agents receive the highest wages more often than the best performance occurs — and centrality —, i.e., variation in performance exceeds variation in wages at the top — arise endogenously from optimal contracting.====The intuition for this communication pattern is the following: First, it is never optimal to justify all evaluations because justification is costly. Second, if the agent is evaluated positively, he suspects no deviation by the principal because the principal pays higher wages for better evaluations. If the agent is evaluated negatively, the agent considers two possibilities: his performance was poor or the principal distorted her evaluation downwards to pay lower wages. To counter such suspicions by the agent, the principal justifies poor evaluations. Note that compared to common moral-hazard settings, additional incentives are necessary: ex-ante, the principal wants to justify poor evaluations ex-post. Ex-post, however, she wants to save on justification costs and wage costs. The principal has no commitment power other than the contract. Consequently, she must design contractual terms that make it ex-post incentive-compatible for her to justify the evaluation. Finally, there is a clear intuition for centrality: by eliminating wage differences that would otherwise call for justification, the principal can save some justification costs. Remember that the agent cannot verify the evaluation without justification. Hence, instead of reporting an evaluation yielding higher payments, the principal would deviate and report an evaluation that does not require justification and yields lower payments. Therefore, no wage dispersion is feasible, and there is pooling with respect to wages if the principal provides no justification.==== The resulting communication pattern is not limited to employment relations. It applies more generally to hold-up and moral hazard settings whenever the better-informed party can provide justification.====Technically, justifications are based on type-dependent message spaces for the principal. All messages are contractible but unverifiable. Moreover, only the agent observes whether the principal provides justification. Hence, a third party cannot tell whether a message is truthful or whether justification occurred. Justification limits the principal's scope for deviations in reporting her evaluation of the agent as low messages become unavailable for good performance. In a well-designed contract, the justification ensures that the principal cannot evaluate the agent poorly for good performance. As an interpretation, consider a business analyst at a consulting firm who received praise from a client. Suppose the partners in the firm pretend that the analyst's work was poor and justify their assessment by saying that the client complained about the analyst. Although the business analyst is unaware of the partners' real evaluation of her work, she knows that the partners are lying making such assessments unavailable to the partners. Alternatively, we can interpret an evaluation that comes with justifications as verifying that the agent has failed a well-defined standard.====The remainder of this paper is organized as follows. Section 2 discusses the related literature. Section 3 sets up the model and characterizes the optimal communication pattern. Section 4 contains the concluding remarks. All proofs are relegated to the appendix.",Communicating subjective evaluations,https://www.sciencedirect.com/science/article/pii/S0022053118306744,January 2019,2019,Research Article,303.0
"Bervoets Sebastian,Faure Mathieu","Aix-Marseille Univ, CNRS, EHESS, Centrale Marseille, AMSE, Marseille, France","Received 27 September 2017, Revised 14 September 2018, Accepted 23 October 2018, Available online 30 October 2018, Version of Record 6 November 2018.",https://doi.org/10.1016/j.jet.2018.10.011,Cited by (12)," of the best-response dynamics. This is generally done in games where interactions are global and equilibria are isolated. In this paper, we analyze stability in contexts where interactions are local and where there are continua of equilibria. We focus on the public good game played on a network, where the set of equilibria is known to depend on the network structure (====), and where, as we show, continua of equilibria often appear. We provide necessary and sufficient conditions for a component of ==== to be asymptotically stable vis-à-vis the best-response dynamics. Interestingly, we demonstrate that these conditions relate to the structure of the network in a simple way. We also provide corresponding results for several dynamical systems related to the best response.","The recent literature on games played by individuals who interact on social networks has highlighted the relationships between Nash equilibria and the structure of the network (see for instance Jackson et al., 2016). One important contribution is Bramoullé and Kranton (2007). The authors focus on public goods games played on networks, where best responses are linear, actions are strategic substitutes and individuals' payoffs depend on the sum of neighbors' actions. They analyze the set of Nash equilibria and find that despite multiplicity, only very few Nash equilibrium action profiles are stable, where stability refers to the asymptotic stability of the discrete-time best-response dynamics. The authors reiterate their analysis with a more general class of games in Bramoullé et al. (2014), using the continuous-time best-response dynamics (thereafter, BRD).====In this paper we extend and complement their analysis by comprehensively analyzing stability in these games. As we show, the set of Nash equilibria is a finite union of connected components. In the aforementioned papers, the authors focused on isolated equilibria,==== which are specific types of components. However, as we show, these components generally form a continuum of equilibria (section 2). In this paper, we consider all types of equilibria.====We use the concept of stability of sets standard in the dynamical systems literature (see for instance Conley, 1978), i.e. the concept of ====. In economics, it was introduced by Weibull (1995) under the name of asymptotically stable sets, by contrast with asymptotically stable points. Being an asymptotically stable point and being an attractor are equivalent for isolated equilibria. However, no point can be asymptotically stable in a continuum of equilibria, since any neighborhood of that point contains another Nash equilibrium. Yet the entire component might be an attractor.====Identifying attractors is challenging for at least two reasons. First, there are no general algebraic tools available. In the case of isolated equilibria, the nature of the Jacobian matrix informs us about its stability. However, this does not work for continua. As Seade (1980) puts it in his paper on stability of Nash equilibria in the Cournot oligopoly problem, “Things would get trickier (...) if equilibria happened not to be ====, that is not even locally unique, isolated. This, one can dismiss as a non-generic, ‘unlikely’ occurrence, although that is often a risky stand to take.” In this paper, we tackle the issue in a context where the ‘unlikely’ occurrence is generic.====Second, there are no general results to be relied on concerning the best-response dynamics. It may or may not converge, depending on the game played. In discrete-time versions and/or in games with discrete action space, it is known that the dynamics can diverge; in continuous games, the vivid debate about stability of the Cournot solution (see i.a. Theocharis, 1960, Fisher, 1961, Hahn, 1962, Seade, 1980 and al Nowaihi and Levine, 1985) illustrates how the behavior of the best-response dynamics finely depends on the parameters of the game. Even in very structured games such as potential games, additional conditions are needed to guarantee convergence (see Kukushkin, 2015).====The games we focus on are best-response potential games (see Voorneveld, 2000), and we show that attractors correspond to the local maximizers of that potential function (as in Sandholm, 2001). We also show that attractors are always included in the set of Nash equilibria, and correspond to sets where every point is a local maximum of the potential (section 3).====In section 4, we consider the public good game where actions are perfect substitutes and obtain two results that completely characterize the set of attractors. First, we identify necessary and sufficient conditions for a Nash equilibrium to be a local maximum of the potential. These conditions relate in a simple and tractable way to the topology of the network: either the subgraphs of agents exerting an effort are all complete subgraphs and the Nash equilibrium is a local maximum, or at least one is not complete and the equilibrium is not a local maximum. This property is nice because, although hard to establish, it is easy to check.====Second, our main theorem provides ==== conditions for a set of Nash equilibria to be an attractor. First, every attractor must contain some ==== Nash equilibria, i.e. equilibria in which individuals are either active and exert the autarkic effort, or inactive and exert no effort at all. Second, a component of Nash equilibria is an attractor ==== the specialized equilibria that it contains are themselves local maxima of the potential. Thus, however complex the components of Nash may be, one simply needs to focus on their “extreme points”. The previous result already provides the condition determining whether a specialized equilibrium is a local maximum or not. This theorem is a sharp result that provides an algorithmic method to find the attractors.====One corollary is that at least one stable component exists for every network: any component containing a maximum independent set==== of the network is stable. This is an important departure from Bramoullé and Kranton (2007), who often find no stable equilibria because of their focus on isolated equilibria. Further, we find the same result as them in the case of isolated equilibria: they obtain that an – isolated – equilibrium is stable for the discrete best-response dynamics if and only if it is specialized and the set of active players forms a maximal independent set of order at least two. When equilibria are isolated, our condition is equivalent to theirs.====In section 5 we turn to the public good game with imperfect substitutes. Although we do not get as sharp a result as for the case with perfect substitutes, we extend the results of Bramoullé et al. (2014), where the authors restrict their attention to Nash equilibria in which there are no weakly inactive agents, i.e. agents who do not produce any effort but are just on the verge of doing so.==== They identify the lowest eigenvalue of the subgraph of active agents as the determinant of the stability of a Nash equilibrium. We obtain this result and we also consider Nash equilibria in which agents can be weakly inactive.====When interactions are low, i.e. substitutes are very imperfect, a Nash equilibrium will always be a local maximum of the potential (and hence stable if it is isolated), which will never be true if interactions are high. However, we identify a range of intermediate interaction values for which the analysis gets complex. When an equilibrium falls within that range, we provide two different necessary and sufficient conditions for an equilibrium to be a local maximum of the potential.====The first condition amounts to saying that an equilibrium is a local maximum if and only if the set of active agents cannot be reduced. The second condition considers acceptable action profiles, i.e. action profiles such that no one exerting an effort has an aggregate level of contribution that exceeds his autarkic effort. Acceptable action profiles naturally contain Nash equilibria, but not only. The condition then states that an equilibrium is a local maximum if and only if the sum of efforts in that equilibrium is always greater than the sum of efforts in any acceptable action profile on the set of active and weakly inactive agents. Although these conditions are less easy to interpret than those on perfect substitutes, they still provide valuable insights as well as an algorithmic way of determining whether a set is an attractor.====Finally, in section 6, we investigate several alternative dynamical systems which rely on a best response principle. First, we provide sufficient conditions for any continuous-time dynamical system to share the same set of attractors. Second, we focus on three different discrete-time dynamics. We show that the asymptotic behavior of these systems is quite sensible to the features of the dynamics: the set of attractors is the same as for the (BRD) as long as the discrete systems are either sequential or “smooth” in some sense, while it might differ if agents simultaneously “jump” far away from their current position at each step.",Stability in games with continua of equilibria,https://www.sciencedirect.com/science/article/pii/S0022053118306732,January 2019,2019,Research Article,304.0
Afacan Mustafa Oǧuz,"Faculty of Arts and Social Sciences, Sabancı University, 34956, İstanbul, Turkey","Received 11 April 2017, Revised 28 September 2018, Accepted 26 October 2018, Available online 29 October 2018, Version of Record 1 November 2018.",https://doi.org/10.1016/j.jet.2018.10.013,Cited by (0),"We introduce a novel school-choice with vouchers model. A feasibility notion is defined to incorporate the constraint that students from low-income families need a voucher to go to a private school. We then introduce a stability notion, while emphasizing that the traditional school-choice model and usual stability notion are realized as a special case of our formulation. A class of feasible, stable, and constrained efficient mechanisms is proposed. However, feasibility and stability are incompatible with strategy-proofness in the sense that no mechanism is feasible, stable, and strategy-proof at the same time, implying that any mechanism in our class is manipulable. Given the efficiency and strategic disadvantages of stability, as an alternative solution, we introduce a feasible, efficient, and strategy-proof mechanism. Lastly, we provide a comparative statics analysis.","The idea of funding private-school expenses for students from low-income families using public funds, known as the school-voucher system, dates back to 19th century. Vermont and Maine in the United States were the first states to offer school vouchers. The main goal of school vouchers is to provide a greater school choice to low-income students and to force schools to improve themselves by increasing the competition among them. Today, in the United States, 12 states and Washington D.C. offer school vouchers. Other countries that offer school vouchers include Turkey, Sweden, and Chile.====While there are differences among voucher programs across school districts, common aspects are that only parents whose income level is below a certain limit can apply for a voucher, voucher demand exceeds its supply, and a lottery is used to decide who receives a voucher.==== However, obtaining a voucher does not guarantee a seat at a private school. Parents have to apply to private schools and, depending on their acceptances and preferences, they decide whether to enroll their children at a private school by a certain deadline. If a voucher receiving parent does not enroll their child at a private school, then the voucher may be transferred to the next parent on the waiting list, depending on the school district.====In this study, we introduce a novel centralized school-choice with vouchers model, including both private and public schools, and propose feasibility and stability notions. We then pursue a mechanism design that admits desirable fairness, efficiency, and strategic properties. Note that such a design is challenging and subtle in the presence of vouchers because of possible wasted vouchers and their redistribution for the sake of efficiency while also sustaining a suitable fairness. Here, we introduce two main mechanisms: “efficiency-corrected deferred-acceptance with vouchers and transfers” (====-====) and “top trading cycles with vouchers” (====-====). ====-==== is feasible, stable, and efficient in the class of stable mechanisms, but it is not strategy-proof. However, the lack of strategy-proofness is not specific to ====-==== because there is a general incompatibility between strategy-proofness and feasibility together with stability. On the other hand, ====-==== is feasible, efficient, and strategy-proof, but it is not stable owing to its lack of fairness. Hence, the trade-off between these mechanisms reduces to fairness versus efficiency together with strategy-proofness. Lastly, we study the welfare effects of additional vouchers and voucher ownership transfers on students and schools.====There is an extensive body of literature on school choice, following the works of Balinski and Sönmez (1999) and Abdulkadiroğlu and Sönmez (2003). This study is the first to incorporate vouchers in a school-choice model. Thus, there are no directly related works. However, relevant works include those on the generalization of school-choice problems, such as Erdil and Ergin (2008), Kesten (2010), Dur et al. (2015), Abdulkadiroǧlu (2011), and Afacan et al. (2017).====Note that we place the current research in the school-choice context. However, it can be applied within other contexts as well. More broadly, we study a type of object-allocation problem in which a certain group of objects (e.g., private schools) is accessible only with a special permit (e.g., vouchers for eligible students). In addition to the school-choice with vouchers problem, public-housing assignment is another practical application. For instance, in many public-housing assignment programs in Turkey, only people with a certain seniority can receive a house. However, if they are not willing to opt into the assignment process, they can transfer their assignment rights to others.",School choice with vouchers,https://www.sciencedirect.com/science/article/pii/S0022053118306756,January 2019,2019,Research Article,305.0
"Chi Chang Koo,Murto Pauli,Välimäki Juuso","Department of Economics, Norwegian School of Economics, Norway,Department of Economics, Aalto University School of Business, Finland","Received 7 May 2018, Revised 17 October 2018, Accepted 23 October 2018, Available online 26 October 2018, Version of Record 5 November 2018.",https://doi.org/10.1016/j.jet.2018.10.010,Cited by (8),"We analyze all-pay auctions with affiliated values and binary signals. We analyze the unique symmetric equilibrium with any number of bidders and show that the bidders earn positive rents only if the equilibrium is monotone. We also characterize the symmetric equilibrium of the closely related two-player war of attrition.====We compare expected revenues across these formats. All-pay auctions result in lower expected rents to the bidders than standard auctions, but they also induce inefficient allocations in some models with affiliated private values. With two bidders, the effect on rent extraction dominates, and the all-pay auctions outperform standard auctions in terms of expected revenue. With many bidders, standard auctions may result in higher expected revenue. The war of attrition outperforms the standard auctions in terms of revenue, but its ranking relative to all-pay auctions is ambiguous.","Competition for a scarce resource often takes the form of a contest. In nature, males of animal species expend energy and risk injury when competing for females and trees grow in height to compete for solar energy. Closer to economics, lobbyists spend money and effort in order to secure a decision that is favorable to their cause. Architectural competitions and online contests invite participants to work on a design problem or an algorithm, and the best result is awarded a prize. The key distinguishing feature of these examples is that the contestants have to decide how much effort to sink into the contest and the outcome is determined as a function of these efforts.====Contest models are closely related to auctions. The simplest contest model is called an ====, where the prize is awarded to the contestant with the highest effort. The effort choice corresponds to the choice of a bid in an auction and the sunk cost nature of effort choice is emphasized by the requirement that all the bidders pay their bids. In terms of this analogy, a first price auction would be a contest with a promised effort level when winning. Just as for ordinary auctions, it is natural to consider contest models where the subjective estimates of the value of the prize are private information to the contestants. And just like in the case of auctions, it makes sense to ask how contests with correlated values differ from contests with independent values. Such informational linkages have been analyzed extensively in the auction literature, but have received much less attention in the contest literature.====In a two-bidder model, Rentschler and Turocy (2016) show that correlation makes it harder to assess a priori what kinds of signals on the quality of the prize are advantageous to any given bidder. We put more structure on our model, and we assume throughout that signals are affiliated, i.e. they co-move positively. This means that a contestant (or bidder) with a high signal expects more opponents with high signals than a bidder with a low signal. When winning, the prize is on average more valuable for a high signal bidder. But affiliation also implies that if the other contestants with high signals expend more effort, then the competition that a high signal bidder expects is also tougher. These effects are present for standard first price and second price auctions too, but a crucial difference between the formats is that in contrast to the standard formats, bids in the all-pay auction are sunk. This makes the increased competition much more harmful to bidders.====From an analytical point of view, the key implication of this enhanced competition effect relative to standard formats is that best-response bids or effort choices need no longer be monotonic in the signal. As is well understood, this may preclude the existence of an equilibrium in monotonic strategies, which may explain why the literature on affiliated all-pay auctions and contests is so scarce.==== When monotonicity is lost, equilibrium characterization becomes very difficult. To circumvent the issue of intractability, the previous literature assumes monotonicity of equilibria by a parameter restriction (e.g., Krishna and Morgan, 1997 and Siegel, 2014) or provides algorithmic or partial characterizations of equilibrium in case of two bidders (e.g., Rentschler and Turocy, 2016 and Lu and Parreiras, 2017).==== As we show in this paper, the conditions for monotonicity are quite restrictive and the existing characterizations do not allow for easy results on the qualitative features of the equilibria when monotonicity fails.====Our goal here is to provide a complete characterization of both monotone and non-monotone symmetric equilibria of the all-pay auction with an arbitrary number of symmetric players. For this purpose, we must use a simpler informational model. Rentschler and Turocy (2016) adopt a discrete signal space and they provide an algorithm for computing symmetric equilibria of a two-player all-pay auction for both the monotone and non-monotone case. We further simplify their model by assuming binary signals that are affiliated. Since we cover the case of an arbitrary number ==== of bidders, we find it convenient to assume that the signals are also affiliated with another random variable that we call the state of the world. In the mineral rights model, this common variable can be taken to be the true value of the prize whereas in the case with affiliated private values, it is best thought of as a parameter determining the true distribution of bidder types in the population. Within this formulation, we can analyze the comparative statics of the model in the number of bidders while fixing the signal structure of an individual bidder.====The binary information structure is also ideal for isolating key economic forces that the correlated information brings to contest models. The key insight is the possibility of inefficient allocations (for private values models). We analyze in detail how this is influenced by the correlation structure and the number of contestants. We know from Rentschler and Turocy (2016) that such inefficiencies prevail in a general model. However, their model with a general signal space turns out to be very difficult to extend beyond two bidders, and is even hard to characterize in a systematic way when there are only two bidders.====We provide a full characterization of the symmetric equilibria for our game with any number of bidders. We start by showing the existence and uniqueness of symmetric equilibrium, and prove that the unique symmetric equilibrium takes one of two possible forms: either it is monotone in the sense that higher value bidders win with probability one against lower value bidders. Alternatively it is non-monotone: a bidder with a lower value wins with a positive probability against a higher value competitor. In the latter case, we show that no bidder receives a positive expected payoff in the game, i.e. all rents are fully dissipated. In monotone equilibria, players with high valuations do receive an information rent.====As long as our equilibria are monotone, the results just confirm what Krishna and Morgan (1997) find in their model. The main contribution of our paper lies in the analysis of the non-monotone equilibrium. We show that the unique symmetric equilibrium is in non-monotone strategies for a large set of parameter values. In the mineral rights model, the symmetric equilibrium is always non-monotone if the number of contestants or bidders is large enough. In the affiliated private values model, the equilibrium tends to be non-monotone if the differences in the private values are not too large and as long as the affiliation is not too small. With independent private values, however, the equilibrium is always in monotone strategies since the strength of competition is independent of a contestant's own type.====The all-pay auction can be thought of as a static contest model in the sense that the bidders expend irreversible efforts or outlays before knowing who wins. In order to check the robustness of our insights, we also analyze a closely related dynamic contest model, the war of attrition. In that model, the contestants observe each other expending effort gradually up to the point where the last contestant remaining is declared the winner. Since such a model does not have a symmetric equilibrium for more than two players, we restrict to the two-player war of attrition, which is equivalent to a second-price all-pay auction.==== We show that the unique symmetric equilibrium of this game can also be in non-monotone strategies. The structure of equilibrium is slightly different in comparison to the (first price) all-pay auction. In the war of attrition, it is possible to have a non-monotone equilibrium where the bidders with high valuations earn a strictly positive information rent.====Besides equilibrium characterization, we examine which game induces the highest aggregate effort. In the auction terminology, this is just the revenue comparison across the auction formats. We determine a revenue ranking between the all-pay auction, war of attrition, and the standard auction formats. Their ranking relies on the differences between the auction formats in how much of the bidders' rents they extract, and in how much of the surplus loss results from inefficient allocations.====In models with monotone equilibria, only the rent extraction matters, and then the ranking from Krishna and Morgan (1997) is maintained: war of attrition dominates the all-pay auction which in turn dominates the standard formats. This result holds also for the mineral rights case of the model where there is no allocative inefficiency. The revenue ranking is based on an unambiguous ranking of bidder rents across the formats: standard auctions leave a higher rent to bidders than all-pay auctions which leave a higher rent than the war of attrition.====When considering non-monotone equilibria in the affiliated private values model, the results change. Since non-monotone equilibria inevitably result in an efficiency loss, the relevant comparison is now between the induced inefficiencies and the changes in bidder rents. We show that with two bidders, the ranking between all-pay auctions and standard auctions is as before. However, the ranking between all-pay auctions and the war of attrition is ambiguous. For large numbers of bidders, the rents given up to the high types vanish in all of these auctions, whereas we show that the inefficiencies in the all-pay auction remain. As a result, the previous ranking between all-pay auctions and standard auctions is reversed when the number of bidders grows large.",All-pay auctions with affiliated binary signals,https://www.sciencedirect.com/science/article/pii/S0022053118301534,January 2019,2019,Research Article,306.0
Liang Annie,"University of Pennsylvania, United States","Received 4 October 2016, Revised 16 August 2018, Accepted 23 September 2018, Available online 23 October 2018, Version of Record 20 November 2018.",https://doi.org/10.1016/j.jet.2018.09.010,Cited by (2),"Suppose an analyst observes inconsistent choices from either a single decision-maker, or a population of agents. Can the analyst determine whether this inconsistency arises from choice error (imperfect maximization of a single preference) or from preference heterogeneity (deliberate maximization of multiple preferences)? I model choice data as generated from imperfect maximization of a small number of preferences. The main results show that (a) simultaneously minimizing the number of inferred preferences and the number of unexplained observations can exactly recover the number of underlying preferences with high probability; (b) simultaneously minimizing the ==== of the set of preferences and the number of unexplained observations can exactly recover the choice implications of the decision maker's underlying preferences with high probability.","Let ==== be a finite set of choice alternatives, and consider an analyst who observes choices (by either a single decision-maker, or a population of subjects) from various subsets of ====. Empirical choice data of this nature is often inconsistent, and cannot be explained as perfect maximization of a single preference.====There are two different perspectives for how to interpret such inconsistency. One view is that inconsistency emerges from ====. There is abundant evidence that choices depend on details about the choice context—for example, Einav et al. (2012) find that just over 30% of their subject pool makes decisions across six financial domains that can be rationalized using a common risk preference. Additionally, choice data aggregated over a population of decision-makers often exhibits cross-sectional heterogeneity in preferences—for example, Crawford and Pendakur (2012) study household consumption decisions over different kinds of milk, and find that no more than two-thirds of observations in their data set can be rationalized using a single utility function. In both of these cases, choice inconsistencies are understood to reflect intentional maximization which is welfare-relevant.====Another view is that inconsistencies reflect ====, e.g. the decision-maker may be inattentive, and the analyst may make mistakes while recording observations. In these cases, inconsistency reflects choices that are not indicative of preference.====Preference heterogeneity and error are distinct sources of inconsistency, with different implications for welfare-assessment and for prediction. To take a stark example, compare two hypothetical choice data sets: one generated by ====, and one generated by ====. Application of classical approaches such as Houtman and Maks (1985) can fail to distinguish between these data sets, especially if the same fraction of both data sets is rationalizable using a single preference. Nevertheless, the underlying choice mechanics are quite different: the inconsistency represented in the first data set can be expected to be stable across future observations, while the inconsistency represented in the second is idiosyncratic.====A basic question then is ==== choice domains or subpopulations are present in the data, where a special case of interest is whether there is evidence of multiple preferences or a single preference with error. (The question of “how many preferences” is pursued, for example, in Crawford and Pendakur (2012) in the case of household consumption decisions and Dean and Martin (2010) for individual choices over lotteries.)====At two ends for interpreting the data are the classical approaches of Houtman and Maks (1985) and Kalai et al. (2002), both of which rule out one of the two sources of inconsistency described above. Specifically, we can find a “best-fit” single preference (rationalizing the largest fraction of observations) and interpret the remaining observations as choice errors (Houtman and Maks, 1985), or find the smallest number of preferences that perfectly rationalizes the choice data (Kalai et al., 2002). When preference multiplicity and choice errors are simultaneously present in the data, the Houtman and Maks (1985) solution (weakly) underestimates the number of preferences, while the Kalai et al. (2002) solution (weakly) overestimates. The consequences for welfare evaluation and out-of-sample predictions can be significant.====The purpose of this paper is to develop a method to determine the “best” intermediate solution. I consider data generated according to a (generalized) random utility model. The decision-maker (DM) chooses from choice set ==== by sampling a preference according to a distribution ====, and maximizing the sampled preference. I suppose that each ==== is in fact a perturbation of a “sparse” ====, whose support is a small number of preferences ==== that are constant across choice sets.==== The goal is to recover from the choice data the underlying number of preferences ====.====The proposed approach, presented in Section 4, minimizes a weighted sum of the number of preferences attributed to the decision-maker, and the number of unexplained observations (choices that cannot be rationalized by any of the recovered preferences). The approach thus imposes a cost on each recovered preference, so that a preference is recovered if and only if it explains sufficiently many observations that would otherwise be considered error. The classic Houtman and Maks (1985) and Kalai et al. (2002) solutions are returned for special choices of weights—the former is returned when the cost of preferences relative to unexplained observations is sufficiently high, and the latter is returned when the cost of preferences relative to unexplained observations is sufficiently low.====The main result in Section 5 provides a set of weights (which depend on primitives of the choice model) given which the proposed approach exactly recovers the “true” number of preferences with sufficiently many observations.==== Informally, these conditions require that the ==== preferences are sufficiently differentiated in the sampled data, so that choice inconsistencies that emerge from genuine preference heterogeneity resemble other inconsistencies in the choice data, whereas choice inconsistencies that emerge from error appear idiosyncratic. The Kalai et al. (2002) approach is shown to recover the number of underlying preferences when the probability of choice error is zero; to the best of my knowledge, this is the first statistical justification for the Kalai et al. (2002) approach. The special case of discerning between choice data that is generated by imperfect maximization of a single preference, versus choice data that reflects multiplicity of preference, is considered in Section 5.4.====The set of weights which allow for recovery depends on primitives of the choice model. Next, I explain a way in which we can “test” particular assumptions about these unobservables based on the data. Since the main theorem provides an interval of weights that recover the same solution, we can use the data to determine the actual range of weights over which our inferred solution remains stable. This range can then be used to bound the key primitives (the extent of differentiation of the underlying preferences, and the probability of error), as shown in Corollary 1.====Section 7 revisits an analysis conducted in Crawford and Pendakur (2012), in which the Kalai et al. (2002) approach is used to discover the number of preference types among 500 subjects. Crawford and Pendakur (2012) find that five preferences are needed to perfectly rationalize their data set. I show how the proposed approach can be used to identify some of these preferences as noise.====Section 8 turns to the question of recovering the preferences themselves. Inference of multiple preferences from choice data is an ill-posed problem, and Section 5.1 presents several negative results that help to clarify the reasons for this. In Proposition 1, I show that most sets of orderings are indistinguishable based on their choice implications, so that even in the absence of choice error, most sets of multiple preferences cannot be recovered. This result is very much in the spirit of Ambrus and Rozen (2013), which studies a broad (but different) class of multi-self models and shows that these models have no testable implications without prior restrictions on the number of selves involved in a decision.====In view of these results, I suggest that a more appropriate object of recovery is the set of ==== of the decision maker's preferences—that is, the choice observations that are consistent with maximization of one of these preferences. I define equivalence classes for sets of preferences, where two sets belong to the same equivalence class if they have the same choice implications, and ask whether we can recover the equivalence class to which the true set of preferences belongs. Section 8.2 shows that this is indeed possible, but that penalizing the number of inferred preferences is not the appropriate criterion for this goal. This is because penalizing only the number of preferences results in inference of sets of preferences whose choice implications are as diverse as possible. I propose an alternative criterion: minimizing a weighted sum of the number of unexplained observations and the ==== of the set of preferences, as measured through the number of unique choice implications. Proposition 2 shows that under certain conditions on the choice model described above, this approach will exactly recover the equivalence class of choice implications containing those of the true model.====Finally, Section 8.3 considers a richer kind of data set, which includes auxiliary information on the choice contexts active during different observations. I show that with this additional information, we can (under certain conditions) recover the exact set of preferences.====Taken together, these results suggest that appropriately penalizing the complexity of the inferred choice model—for example, via the number of preferences used or the number of choice implications—can be useful for recovery of stable features of preference from inconsistent choice data.",Inference of preference heterogeneity from choice data,https://www.sciencedirect.com/science/article/pii/S0022053118306112,January 2019,2019,Research Article,307.0
"Levin Dan,Schmeidler David","Department of Economics, University of California, Santa Barbara, United States of America,Department of Economics, The Ohio State University, United States of America,Tel Aviv University, Israel,The Ohio State University, United States of America","Received 22 July 2016, Revised 2 October 2018, Accepted 21 October 2018, Available online 23 October 2018, Version of Record 5 November 2018.",https://doi.org/10.1016/j.jet.2018.10.009,Cited by (3),"We study how subjects with identical public data first make estimates and then bid in common-value environments. The data presented rows of numbers and values associated with them by our (undisclosed) rule. Subjects were asked to estimate the missing value in the last row with only the numbers given, and then bid for that value in a second-price auction. There is no presumption of commonly-known distributions, yet we derive necessary conditions for equilibrium. The strong winner's curse that we observe in our data results from the dispersion of the value estimates and the poorly-chosen bid-strategies. We find that bidding is lower when bidders are more uncertain about the estimates they have made. Finally, the ==== method does well in explaining the estimates of the (common) value.","The winner's curse (WC) is a term used to describe systematic losses in common-value auctions due to non-equilibrium overbidding, resulting in the auction's winner wishing that he or she had not won the auction.==== The existence and persistence of the WC is one of the most robust non-equilibrium findings emerging from experimental work. It has challenged theorists to bridge the gulf between predictions of game theory and the data, without discarding individual rationality, leading to Eyster and Rabin's (2005) “cursed equilibrium,” Crawford and Iriberri's (2007) “level-==== reasoning” and Jehiel's (2005) “Analogy-based expectation equilibrium”.====Previous research on this phenomenon has largely been restricted to (pure) common-value (CV) auctions, where the value of the item auctioned is the same, ====, for all bidders but where each bidder has received his or her own private signal (estimate) of that CV.==== The signals are drawn from a joint distribution function that is assumed to be common knowledge. Nearly all previous analyses of CV auctions adopt the Bayesian paradigm and focus, in first-price auctions, on the symmetric Bayes–Nash equilibrium (BNE) (i.e., Harsanyi's, 1967). Winning in such a BNE implies holding the highest signal, so that each bidder should correct for the adverse selection resulting from this fact.==== The failure to do so is the most common explanation offered for the finding of systematic losses in studies using the Bayesian model.====We consider an environment in which all experimental participants receive ==== related to the CV of a prize. One major novelty lies in the nature of the information, which is here a short list of observations.==== The participants are told that the data are generated by some relationship amongst the variables. We provide rows of data in which the number in the final column is the same function of the other columns' numbers. In the bottom row, we only give the numbers in the other columns and incentivize people to estimate the missing value in the last column of the last row. Our non-trivial estimation problems do not seem more difficult than one might find with decisions in the field.==== Next, people are randomly assigned to groups and participate in a sealed-bid, second-price auction for (the value) of this number.====The typical experimental design (unlike ours) has exogenous private estimates/signals, and the explanation for the WC falls on a) not knowing how to bid; b) not recognizing, addressing, and compensating appropriately for (at the time of bidding) the adverse selection in the event of winning. Our design reveals directly that an important part of the WC is that DMs may reach very different estimates/assessments even with identical and public data. This aspect is much closer to many real and important situations, where there is no induced commonly-known distribution function of the signals.====Our study has two related components, estimation and bidding. Everyone has precisely the same information (and this is common information). Do people make the same estimates? Can we identify a class of rules used to form their estimates? While these questions are interesting and important in their own right, it is also important to examine how our findings affect information aggregation in trading mechanisms, in terms of allocation and prices (revenues). Auctions are a natural trading mechanism to study such questions.====Our identical data environment without an induced (prior) specific belief structure typically results in a large variance in the estimates made. However, nearly everyone is successful in our simple estimation tasks since (almost) all subjects predict the same CV, so here we can abstract from the variance in estimates and demonstrate that errors in bidding strategies ==== can be a crucial element of the winner's curse.====There are several possible channels by which the WC might manifest. Some of these include: 1) People may fail to condition on “pivotal events” as in voting games with common interests. 2) They might fail to correctly estimate the common value. 3) People may not respond optimally to their beliefs about the strategies of others. 4) They might not have rational expectations about the estimates of others regarding the common value. While we cannot derive a unique, accurate point estimate for the relative contributions of these factors, we do provide some evidence regarding the first three of these forces; we focus primarily on the relative effects of bidding and estimation problems on the WC. In addition, we investigate a new issue, the effects of the complexity of the estimation task on estimation and bidding behavior. To the best of our knowledge, we are the first to consider the interplay of these forces.==== We also take a first crack at investigating how (non-trivial) estimates of value vary considerably even with public and identical information; we believe that this variation is a major factor in trade in financial markets, where there are huge and very similar (although not identical) databases.====Our experimental results document that the winner's curse is present in this environment, even when nearly everyone deciphers the true value of the prize; this is the strongest case we have seen of wrong bidding ==== in isolation from estimation. We are able to show (given the multiple bidding/estimation pairs for each individual) that a substantial proportion of the bidding profiles for each group violate the necessary conditions for Nash equilibrium, but are indeed consistent with the losses seen in standard environments. We also study the main heuristics used for estimation in our informational context. People appear to often make their estimates by reasoning in terms of similarity. Nearly half of the respondents indicated that they chose estimates that were similar to the revealed value in cases presented to them. In this regard, the ====-==== method does well in explaining how participants make estimates of value. Our experimental design also enables us to systematically vary the complexity of the function that transforms parameter values into outcomes and study the effect of such complexity on participants' estimates, bidding behavior, and the net effect on the WC. Our work produces the novel finding that more uncertainty about the estimated value results in more disperse estimates of this value and also leads to lower bidding.====It appears that both the dispersion in the estimates made with the same data and non-optimal bidding are important factors in producing the WC. The relative standard error of the ==== increases with the complexity of the process that generates values from the given parameters. On the other hand, the relative standard error of ==== is substantially higher when bidders know (in the simplest case) the actual value, suggesting that people make lower bids (with a given estimate) when they are more uncertain of the value. The first observation tends to increase losses, since higher guesses are well correlated with higher bids; the second effect (ceteris paribus) tends to mitigate the WC. In our data the first effect appears to be stronger than the second. Regarding the effect of group size, the winner's curse is stronger with larger groups of bidders (8), but is also present in smaller groups of bidders (4).==== Bidding is actually ==== with larger groups, which we show (for the symmetric case) is contrary to the theoretical prediction.====Our new information structure raises a natural question: What is a BNE in this game? To answer this, we develop (in subsections 2.2 and 2.3 below) a simple theoretical model that (using sensible assumptions) provides a necessary condition that ==== bidding profile must satisfy to be considered a BNE in such a general environment.==== This necessary condition allows us to rule out many profiles of bidding strategies from being rationalized in any NE and to test predictions concerning changing the number of bidders. We also derive a prediction for the effect of the number of bidders on equilibrium bidding. Thus, producing and studying the winner's curse in our experiment extends our understanding of this phenomenon to include more general and non-Bayesian economic environments.====In an environment such as ours there is an additional and important explanation that we discover from the data. Consider a bidder who i) bids sensibly (say, best-responding to his or her rivals' profile of bidding functions) and ii) also conditions on having the highest estimate (or one of the highest estimates) of the CV. This still does not protect such a bidder from systematic losses, since he or she may still have an estimate of the CV that is much too high and that can result in winning and cursing about losing money from having won the auction. Outside the Bayesian model, complying with (i) and (ii) does not protect one from overly high estimates of the CV, since we have no formal approach for modeling how one arrives at one's estimate.====We move the discussion of the WC closer to field environments where bidders (traders) with conflicting goals do not have common knowledge of the priors and must estimate the CV based on (almost) identical public information rather than mainly private information. Our setting is similar to economic environments, such as financial markets, where parties involved possess the same or quite similar information and yet often take different actions. In such situations we should observe no trades, yet some traders choose to buy while others choose to sell.==== We also promote the view that decision makers may use different (cognitive) processes to assess (almost) identical information and thus may make substantially different estimates. Agents in the field often form beliefs resulting in estimates about a future value of an asset and then act upon it (e.g., bid for the asset) while all of them are exposed to the same (or a very similar) database or one that is very similar. But it is far from certain that they would all use the same theory or cognitive process to form their estimates. Yet this type of winner's curse may have characteristics that differ from those found in the standard Bayesian model.====The remainder of the paper is organized as follows. We present our theoretical development in section 2. We describe our experimental design in section 3 and present and discuss our experimental results in section 4. We conclude in section 5.",An experimental study of estimation and bidding in common-value auctions with public information,https://www.sciencedirect.com/science/article/pii/S0022053118306665,January 2019,2019,Research Article,308.0
"Aït-Sahalia Yacine,Matthys Felix","Department of Economics, Bendheim Center for Finance, Princeton University, United States of America,NBER, United States of America,Dept. of Business Administration, Instituto Tecnológico Autónomo de México (ITAM), Mexico","Received 15 June 2017, Revised 29 August 2018, Accepted 19 September 2018, Available online 26 September 2018, Version of Record 24 October 2018.",https://doi.org/10.1016/j.jet.2018.09.006,Cited by (33),We study the consumption-portfolio allocation problem in continuous time when asset prices follow Lévy processes and the investor is concerned about potential model misspecification. We derive ,"The study of dynamic intertemporal portfolio choice problems in continuous time has a long history, dating back to Merton, 1969, Merton, 1971. In Merton's model, the investor's optimization problem consists of optimizing his consumption and portfolio allocation in a riskfree and risky assets. The sources of risk in this framework are all diffusive so that sudden large changes in the risky assets are unlikely to occur. Since then, the framework has been extended to allow for asset price discontinuities, driven by jump processes, including Poisson, stable or more general Lévy processes, and Hawkes processes.====In these models, however, the parameters of the asset returns distribution, such as the expected return, jump intensity and jump size distribution are treated as if they were known to the investor. In practice, these parameters are in fact largely unknown and difficult to estimate, so the investor faces a considerable amount of model uncertainty. It is well known that neglecting the uncertainty surrounding the parameters may lead to poor portfolio performance: for instance, mean-variance optimized portfolios are very sensitive to small changes in the input parameters.==== As a result, DeMiguel et al. (2009) and others advocate in favor of “====” or equally weighted portfolios over portfolios that are mean-variance optimized, owing in part to the large uncertainty surrounding expected returns. If anything, the presence of jumps should make this problem worse: jumps are rare events, and pinning down their statistical characteristics, even simply their arrival rate, is difficult on the basis of historical return data.====The literature suggests different methods to approach this problem. In Bayesian decision analysis, the investor forms a prior over models before maximizing his expected utility. Model uncertainty involves averaging over models instead of integrating over shocks. Knightian, ambiguity, and uncertainty aversion are closely related concepts. One way of introducing ambiguity aversion is through the formulation of multiple priors preferences as in Gilboa and Schmeidler (1989). Given such preferences, optimal decisions are taken under the premise that state variables are governed by the worst-case probability model among a set of candidate models. Chen and Epstein (2002) formulate an inter-temporal recursive multiple-priors utility problem that incorporates Knightian ambiguity aversion. An extension of this formulation of ambiguity aversion in continuous time is given in Leippold et al. (2007) which combines learning based on optimal Bayesian updating and ambiguity aversion. Lastly, an even more general formulation of ambiguity, called “smooth” ambiguity, is proposed in Klibanoff et al. (2005), which allows full range of ambiguity attitudes, including ambiguity neutrality as well as ambiguity love.====Another way of introducing model uncertainty is by treating it as a robust control problem. The robust control method pioneered by Hansen and Sargent focuses on minimizing the worst case loss over the set of possible models, rather than Bayesian averaging over models.==== In such a framework, the investor has a specific reference model in mind but also considers a set of alternative models (or parameters) when optimizing his decisions. Recognizing that he is unable to know exactly the true underlying model, the investor seeks instead to develop portfolio policies that should perform reasonably well across the set of alternative models which are statistically close to the reference model in the sense of entropy minimization. Trojani and Vanini, 2000, Trojani and Vanini, 2004 solve two versions of a robust control problem and examine its impact on the resulting asset allocation. Robustness with respect to model misspecification has also been applied to models of the term structure of interest rates. Gagliardini et al. (2008), analyze the term structure implications of uncertainty in a continuous-time production economy. Ulrich (2013) employs a robust decision making framework to analyze how model uncertainty with respect to monetary policy affects the term premium on nominal bond yields. Kleshchelski and Vincent (2007) present an equilibrium model of the term structure in a robust control setting where consumption growth exhibits stochastic volatility. They show that policies robust to model misspecification amplify the effect of conditional heteroskedasticity in consumption growth.====In studies of optimal intertemporal consumption and investment decisions, the representative agent typically has expected utility with constant relative risk aversion (CRRA), which ties the elasticity of intertemporal substitution (EIS) of consumption to the inverse of the coefficient of relative risk aversion. Consequently, CRRA preferences make the agent essentially indifferent about the timing of the resolution of uncertainty. The recursive preferences of Duffie and Epstein (1992) disentangle the agent's risk aversion from his EIS and therefore make it possible to investigate how the investor's optimal robust consumption policy changes depending on his preference for the timing of resolution. Intuitively, early as opposed to late resolution of uncertainty seems to be preferred, as when uncertainty about the investor's future wealth is resolved earlier, the investor can optimally smooth his consumption across time. For instance, if the investor knows that he is going to receive a positive income shock sometime in the future, for example higher dividends on his stock investments, he may optimally choose to increase his current consumption even though the dividend payments will not be received until a later date.====In a model without jumps, Maenhout (2004) studies a representative agent with recursive utility who seeks to make robust consumption and portfolio decisions and shows that the demand for equities is significantly reduced. Maenhout (2006) extents the robust portfolio allocation analysis by a allowing for a time-varying mean-reverting risk premium and shows that while the desire for robustness lowers the total equity share, the proportion of the inter-temporal hedging demand is increased. Consumption and investment problems in a stochastic opportunity setting where the agent exhibits recursive utility and EIS is not equal to one, can only be solved using numerical approximation techniques (see for instance Campbell et al., 2004). With jumps, Liu et al. (2005) employ a pure-exchange economy framework with a representative agent who faces model uncertainty with respect to rare events in the underlying aggregate endowment in order to study the equilibrium equity price. However the focus of that paper is not on portfolio policies: in equilibrium, the representative agent must hold the existing supply of the risky asset and none of the riskless asset in zero net supply, and hence he must set his share of wealth in the risky asset to always be equal to one. Drechsler (2013) employs a robust framework in an equilibrium model where the risky assets follow a jump-diffusion process. Since it focuses on an aggregate endowment economy populated by a representative agent, the optimal portfolio allocation for the investor is again to put all his wealth in the consumption claim.====The main contribution of this paper is to derive robust optimal consumption and portfolio policies of an investor with recursive preferences, including CRRA preferences as a special case, when the underlying risky asset follows a Lévy jump-diffusive process. Using robust control, we allow for model misspecification with respect to the drift as well as jump intensity and jump size parameters. As already noted, these parameters are hard to estimate on the basis of historical data on asset returns (harder than, say, volatility), hence our focus on them. We assume a constant opportunity set, i.e., we abstract from time-varying coefficients on the risky asset or on the entropy growth bound. We show that the investor's optimal robust consumption policy is highly sensitive to his EIS. Introducing model uncertainty leads to lower consumption overall. This effect gets amplified if the investor has a high EIS. As for portfolio weights, we find that the introduction of uncertainty causes the investor to reduce his exposure to the risky assets. Although this results seems natural, the reduction in the optimal holding in the risky asset crucially depends on the type of uncertainty we consider since the stock price dynamics under the robust measure change along multiple dimensions. First, the expected return is reduced once we introduce uncertainty, which is a standard result in the robust control literature. Second, seemingly contradictory, the jump intensity is lower under the robust measure, which indicates that jumps occur less frequently on average. On the other hand, if there is a jump, its size is larger under the robust measure compared to the reference one. Comparing jump size against jump intensity uncertainty reveals that jump size misspecification is more harmful to the investor's wealth and consumption. Furthermore, these results depend upon the amount of model uncertainty. To quantify it, we derive a semi-closed form formula for the detection-error probabilities, i.e., for the likelihood that the investor selects the wrong model based on a time series of past asset returns which we use to determine an upper bound for the set of alternative models which are reasonably close to the investor's reference model. We then analyze the different impact of the investor's risk aversion, preference for the timing of resolution of uncertainty and the degree of misspecification of the drift, jump intensity and jump size parameters on the investor's robust optimal policies. Finally, when calibrating the model to data, we find that decreasing the frequency of observation leads to a (potentially large) increase in uncertainty and amplifies these effects.====The remainder of the paper is organized as follows. Section 2 introduces the general robust consumption and portfolio allocation problem. Section 3 derives optimal consumption policies and robust portfolio weights under drift and jump intensity as well as jump size perturbation. In section 4 we provide fully explicit portfolio weights in a simplified setting. Section 5 derives a semi-explicit formula for the detection-error probability when the underlying measure change follows a Lévy jump-diffusive process. In section 6, we estimate the model and analyze the impact of uncertainty by computing certainty equivalents of wealth. Section 7 concludes. The Appendix contains additional models and investor preferences for which explicit robust solutions can also be obtained, an alternative formulation of robustness, the details about the model estimation, as well as all the proofs.",Robust consumption and portfolio policies when asset prices can jump,https://www.sciencedirect.com/science/article/pii/S002205311830591X,January 2019,2019,Research Article,309.0
"Manzini Paola,Mariotti Marco,Petri Henrik","University of Sussex, UK,IZA, Germany,Queen Mary University of London, UK,University of Bath, UK","Received 6 August 2019, Revised 11 September 2019, Accepted 20 September 2019, Available online 15 October 2019, Version of Record 15 October 2019.",https://doi.org/10.1016/j.jet.2019.104944,Cited by (2), (and hence the statement of Theorem 1) in ==== is incorrect as stated. For the claim to hold an additional axiom is required. We correct the mistake in the proof in ====.,None,Corrigendum to “Dual random utility maximisation” [J. Econ. Theory 177 (2018) 162–182],https://www.sciencedirect.com/science/article/pii/S0022053119300961,15 October 2019,2019,Research Article,312.0
"Bando Keisuke,Hirai Toshiyuki,Hatfield John William,Kominers Scott Duke","Faculty of Economics and Law & Department of Economics, Shinshu University, Japan,Faculty of Economics, Hosei University, Japan,McCombs School of Business, University of Texas at Austin, United States of America,Harvard Business School, Department of Economics & Center of Mathematical Sciences and Applications, Harvard University, United States of America,National Bureau of Economic Research, United States of America","Received 25 June 2019, Accepted 2 August 2019, Available online 6 September 2019, Version of Record 6 September 2019.",https://doi.org/10.1016/j.jet.2019.08.003,Cited by (2),We identify an error in the claim by ==== that every stable outcome in the setting of multilateral matching with contracts is efficient. We then show that the result can be recovered under a suitable differentiability condition.," introduced a model of matching in networks with continuously divisible multilateral contracts and transferable utility. They showed that competitive equilibria exist and are efficient when agents' valuations are concave and also claimed that (under the same condition) competitive equilibria correspond to stable outcomes. However, as we show here, the correspondence of stability and competitive equilibrium in the multilateral matching framework requires an additional differentiability condition.",Corrigendum to “Multilateral matching” [J. Econ. Theory 156 (2015) 175–206],https://www.sciencedirect.com/science/article/pii/S0022053119300833,6 September 2019,2019,Research Article,313.0
Bilbiie Florin O.,"University of Lausanne, Switzerland,CEPR, London, United Kingdom","Received 10 December 2018, Accepted 11 December 2018, Available online 28 March 2019, Version of Record 28 March 2019.",https://doi.org/10.1016/j.jet.2019.03.008,Cited by (1),None,None,"Corrigendum to “Limited asset markets participation, monetary policy and (inverted) aggregate demand logic” [J. Econ. Theory 140 (1) (2008) 162–196]",https://www.sciencedirect.com/science/article/pii/S002205311930033X,May 2019,2019,Research Article,317.0
