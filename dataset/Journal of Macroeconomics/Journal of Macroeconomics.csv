name,institution,publish_date,doi,cite,abstract,introduction,Title,Url,Time,Year,Type,Unnamed: 0
"Harrison Andre,Reed Robert R.","Department of Economics, College of Liberal Arts, California State University, Long Beach, CA 90840, USA,Department of Economics, Finance and Legal Studies, Culverhouse College of Business, University of Alabama, Tuscaloosa, AL 35487, USA","Received 20 May 2022, Revised 16 May 2023, Accepted 23 May 2023, Available online 3 June 2023, Version of Record 10 June 2023.",https://doi.org/10.1016/j.jmacro.2023.103540,Cited by (0),"In recent years, there has been a large amount of lending coming from the public sector of many developing countries. At the same time, the financial sectors in many advanced countries have issued a large share of portfolio debt to other countries. What are the implications of these events for the global ==== and overall economic activity? Do they have an impact on the transmission channels of ","Within the last three decades, there have been large swings in the flow of capital across countries. However, while net capital flows have garnered significant attention, much less attention has been devoted to the role of gross capital flows. Indeed, the implications of these different flows by foreign and domestic investors may be quite different (Forbes and Warnock, 2012, Broner et al., 2013, Avdjiev et al., 2018, Avdjiev et al., 2022, Caballero and Simsek, 2020, Davis et al., 2021 among others). For example, Broner et al. (2013) note that holding an asset in a foreign country may be necessary to obtain access to technology, hedge against income earnings, or even to hedge against the possibility of default. Consequently, the effects of different types of capital flows (debt, equity, FDI) may vary significantly as well. Notably, debt flows have constituted the lion’s share of aggregate capital flows over the last few decades.====Interestingly, many of these flows have been “uphill” — from developing countries to advanced countries. Using data on the United States as an example, developing countries held as much as $3.2 trillion in debt securities issued by the U.S. in 2010. Moreover, by 2015, holdings increased to approximately $3.8 trillion.====A breakdown of these debt flows by sector certainly deserves attention. In fact, Avdjiev et al., 2018, Avdjiev et al., 2022 highlight that there are stark differences between the amount of private and public sector gross capital flows across countries at different stages of development. As they show, in emerging markets, the public sector is the main source of funding to other countries. In fact, the public sector in these countries has steadily been increasing its holdings of external portfolio debt. By comparison, among advanced countries, banks have accounted for the largest share of external debt liabilities over time. Avdjiev et al., 2018, Avdjiev et al., 2022 note that since the banking systems in advanced economies are more developed, banks are primarily involved in the allocation of external funds. Moreover, Bruno and Shin (2015a) show that external debt liabilities of the banking sector in advanced countries rose sharply from the mid-1990s until the financial crisis of 2008–09. Even though the rate of increase has slowed since then, these liabilities are still large and well above pre-crisis levels. Further, Bernanke (2005) posits that holdings by the public sector have been funded by issuing debt domestically. Indeed, Bernanke (2005) suggests that such governments were not just fiscal authorities — instead, they took on an additional role as financial intermediaries between their own citizens and participants in international capital markets from other countries.====With these observations, there are a number of important questions that deserve serious consideration. First, what are the implications of “uphill” public sector flows for economic activity in developing countries? This is especially important considering that such funds are acquired by the public sector issuing debt locally in order to hold portfolio assets from the advanced world. Second, how do these “uphill” portfolio asset holdings impact the transmission channels of monetary policy in developing and advanced countries? In particular, do they render monetary policy more or less effective? Furthermore, are there spillover effects from monetary policy when there are capital flows across countries? That is, how does monetary policy in advanced countries impact economic activity in the developing world when the public sector engages in such activities? Finally, is it optimal for developing countries to fund its foreign portfolio holdings by issuing debt domestically?====In this paper, we develop a monetary general equilibrium model to study these important issues. Notably, a central component of our setup is that the government of the developing country holds portfolio debt from the banking sector of the developed country. In addition, this debt is financed by the government issuing bonds that are held locally in the low income country’s banking system. In this manner, we are able to address not only the macroeconomic implications of capital flows across countries but also whether such flows are optimal, given that they are funded by public sector borrowings.====Since we are particularly interested in understanding how “uphill” portfolio debt holdings impact the transmission channels of monetary policy across countries, it is necessary that our model includes important information frictions that endogenously generate a transactions role for money in each country. These information frictions also impede investment activity, and capital flows across countries along with local debt issuance have important ramifications for financial markets and economic activity. In this setting, we also investigate the optimal conduct of monetary policy across countries.====Further, it is understood that monetary policy works by affecting activity in the banking system — thus, the model should also include the role of the banking system in the process of mobilizing savings and capital flows across countries which is missing in existing work such as Aoki et al. (2009). To do so, we build on the seminal work of Diamond and Dybvig (1983) which demonstrates that financial intermediaries (i.e., banks) perform important risk pooling functions for individuals exposed to idiosyncratic liquidity risk.==== In a setting where money is dominated in rate of return, greater exposure to liquidity risk also impedes banks’ ability to allocate resources to assets with the highest returns.====Moreover, in the low income country, there are significant frictions in the banking system as in Dhital et al. (2021) that prevent emerging economies from generating financial assets from real investment. In our framework, bank managers have the ability to abscond with part of the bank’s assets. Hence, there is a haircut on assets that banks have on their balance sheets. In the spirit of Caballero et al. (2008), the haircut is highest for physical capital. Such imperfections can lead to higher demand for government bonds. In turn, the government of the low income country takes on the role of a financial intermediary by raising funds in domestic debt markets and channeling them abroad. Notably, this follows Bernanke’s (2005) argument that sovereign debt issued by developing countries is important in accounting for public sector financial flows to advanced economies. Thus, we construct a model in which a government of a low income country mobilizes the savings of its citizens to lend in international capital markets.==== Our work in Section 3 suggests that the impact of sending more funds abroad lowers the nominal interest rate on locally issued sovereign debt. As the government lends more abroad, it obtains more income from external debt holdings which lowers the amount the public sector borrows domestically. In turn, the additional revenue actually leads to lower nominal interest rates.==== In Section 3, we find that contractionary monetary policy in the sense of an increase in the amount of bonds relative to money reserves crowds out capital accumulation and lowers real economic activity. In particular, the additional bond issuance leads to higher inflation. Further, the effects of changes of in the bonds-to-money ratio clearly depend on the government’s holdings of bonds from other countries.==== To address this important question, Section 4 looks at two large economies. Notably, banks in the advanced country issue portfolio debt to the public sector of the developing country. If the money growth rate in the advanced country is higher, nominal interest rates on bonds issued by banks in the high income country are higher. For a given amount of international lending, higher interest rates bring in more income for the government of the low income country. With more revenue, the government of the low income economy is able to issue more debt. The higher debt issuance increases the interest rate on bonds issued by the government and produces higher inflation. Thus, in contrast to Banerjee et al. (2016) and Miranda-Agrippino et al. (2020), among others, we argue that the spillover effects from monetary expansion in the advanced country operate through an interest rate channel from increased sovereign debt. That is, monetary policy in the advanced country primarily affects government revenues in the developing country by impacting rates on debt issued in the advanced country and thereby the incentives of capital to flow “uphill”.====Conversely, the availability of funding from abroad produces stronger effects of domestic monetary policy in the advanced country. In particular, an expansionary monetary policy increases the deposit base in the advanced country and allows banks to hold more assets. In turn, the effects of an expansionary monetary policy are amplified because the real costs of borrowing funds for banks are lower as capital inflows are higher. Moreover, such a policy lowers the real cost of capital and investment demand increases. The increased demand for capital then drives up the nominal interest rate. In this manner, our framework serves as an example of how the effects of monetary policy can be more pronounced in the presence of capital inflows. This is an important contribution of our work — we provide a rigorous model that explains how gross capital inflows may impact the transmission channels of monetary policy in advanced countries.==== Our analysis indicates that the advanced country benefits from high capital inflows as it supports investment activity in the banking sector. By comparison, there is generally an intermediate level of capital outflows that maximizes welfare in the developing country. Moreover, we find that the optimal amount of public sector lending to advanced countries critically depends on the degree of liquidity risk — if it is relatively high, raising funds in this way to hold portfolio assets in the advanced country is less likely to be warranted. Yet, our results indicate that it can indeed be optimal for lower income countries to lend to the advanced world — lower income countries tend to have less investment opportunities than advanced countries so lending to the advanced world can help increase returns earned by depositors in the banking system.====As a final analysis, Section 5 provides the outlines of a framework in which emerging economies can arise endogenously as net lenders due to differences in financial frictions. In particular, there are frictions in the banking system in the emerging economy due to the ability of bank managers to abscond with bank assets as previously described along the lines of Dhital et al. (2021). As a result of these frictions, we show that the emerging economy can have higher returns to capital but less capital accumulation than the developed economy as resources will tend to flow uphill.====At this point, we would like to contrast our contribution relative to other recent research in the literature on international capital flows. This is presented in the following section. Finally, the Appendix provides a summary of the derivations of key results in our manuscript.","International capital flows, liquidity risk, and monetary policy",https://www.sciencedirect.com/science/article/pii/S016407042300040X,3 June 2023,2023,Research Article,0.0
Cozzi Marco,"University of Victoria, Canada","Received 21 February 2022, Revised 22 April 2023, Accepted 23 May 2023, Available online 2 June 2023, Version of Record 16 June 2023.",https://doi.org/10.1016/j.jmacro.2023.103539,Cited by (0)," and their market conditions are key in explaining the large level effects. The version of the model without equity is computationally easier to solve, allowing to consider transitional dynamics. Taking into account the dynamic adjustment to the new long-run equilibrium, I show that the transitional welfare costs are not large enough to change the sign of the welfare effects stemming from a change in public debt. I find that eliminating public debt would lead to a 0.8% increase in welfare, while moving to a debt/GDP ratio of 100% would entail a welfare loss of 0.5%. A ==== shows that growth accounts for approximately 50% of the overall welfare effects.","The financial crisis and the global pandemic have left several economies with large public debts, prompting governments and researchers to address whether these may represent suboptimal outcomes, and if fiscal adjustments are justified on economic grounds.==== Aiyagari and McGrattan (1998) is a seminal contribution showing that, in the presence of incomplete insurance markets, a sizable stock of public debt can be optimal. Their analysis focuses on steady-state comparisons, and assumes exogenous increases in productivity, implying that any change in public debt policy would leave the income growth rate unaffected. The assumption of exogenous technological progress is made in most papers studying the optimal quantity of public debt within an incomplete markets set-up, such as Floden (2001), Desbonnet and Weitzenblum (2012), Röhrs and Winter (2017), Chatterjee et al. (2017), and Chatterjee et al. (2018). Although the empirical literature has not reached a consensus on the magnitude of the causal relationship between public debt and growth, there is agreement that a big enough public debt will be detrimental for growth.==== A popular view, stemming from the – controversial for some – results in Reinhart and Rogoff (2010), posits the existence of a threshold value for the public debt/GDP ratio, estimated to be below the 100% mark. According to this perspective, public debt/GDP ratios above the threshold will slow down the income growth rate. Any econometric analysis tackling this issue is challenging because it has to deal with pervasive endogeneity problems. Finding valid instruments has proven difficult, and the thorough analysis in Panizza and Presbitero (2014) fails to find such a threshold. Using different data and methodologies, Cecchetti et al. (2012), Woo and Kumar (2015), and Salotti and Trecroci (2016) all find a negative and significant point estimate of measures of public debt on the growth rate of productivity and income. These contributions support the notion that larger public debts are detrimental for growth, even though the estimated effect can be small. Akcigit et al. (2022) use four historical datasets to document that personal and corporate income taxes have significant negative effects on the quantity of innovation, which is a relevant margin for the mechanisms explored in my paper. Similarly, the growing body of empirical evidence on the effects of taxes on the quantity and quality of innovation, reviewed for example by Stantcheva (2021), finds that corporate and royalty taxes have a negative effect on firms’ productivity growth.====Desbonnet and Kankanamge (2017) show that in the Aiyagari and McGrattan (1998) model the assumed (exogenous) growth rate has a large quantitative effect on the equilibrium welfare gains arising from a change in public debt policy. Motivated by these considerations, in this paper I quantitatively characterize the welfare effects of public debt in a Schumpeterian growth framework. I do so in a model featuring productivity-enhancing vertical innovations driven by investment in Research and Development (R&D), infinitely-lived agents facing income risk with incomplete insurance opportunities, and a high degree of wealth concentration generated by either discount factor heterogeneity or the so-called superstar income shocks, first used by Castaneda et al. (2003). The model stresses the importance of monopolistic power, not only for the determination of the growth rate, but also for the size of the associated inefficiencies.====In an endogenous growth framework, Clemens and Heinemann (2015) introduce incomplete markets in standard ==== growth models with knowledge spillovers. They focus on the role of income shocks, but they do not consider creative destruction or the role of public debt, and cannot generate a realistic degree of wealth inequality, partly because they assume a simple stochastic income process.====The representative-agent version of the Schumpeterian growth model with capital accumulation that I extend has been used by Howitt and Aghion (1998) to argue that capital subsidies raise the long-run growth rate, by Nuno (2011) to study the costs of business cycles, and by Cozzi et al. (2021) to assess the relative importance of demand and supply factors in the recent slowdown of U.S. growth. Chu and Cozzi (2018) also study a Schumpeterian growth model with heterogeneous households, but they focus on the role of patents and R&D subsidies on income inequality, abstracting from uninsurable labor income risk.====A key element of the analysis is that R&D is capital intensive, and Howitt and Aghion (1998) provide some empirical evidence supporting this assumption. Any equilibrium effects on the capital costs are going to bring about endogenous responses in both the rate of technological change and the quantity of output produced. Changes in the interest rate affect multiple margins, two of which (the business stealing effect, and the rental cost of capital) have opposite effects on the equilibrium response of expected profits, hence on the growth rate. Moreover, the quantitative analysis is not straightforward, as in this class of models the growth rate could be inefficiently high in the decentralized equilibrium, with excessive investments in R&D displacing private consumption. If this were to be the case, public debt policies negatively affecting the growth rate could improve welfare.====Public debt increases change the conditions in the asset market, and the resulting rise in the interest rate triggers several effects, some of which are absent in the non-Schumpeterian models considered in the literature. Labor market risk decreases because of a reduction in wages. This is coupled by an improvement in the self-insurance properties of saving in a safe asset. Overall, these two margins lead to a reduction in wealth inequality. In my Schumpeterian model, rising interest rates deter R&D investment, which slows down growth. Furthermore, they increase the user cost of capital needed to produce intermediates, affecting negatively the GDP level. Finally, they worsen the monopolistic distortion via an increase in the markup.====The model is calibrated to the U.S. economy to match the degree of wealth inequality, the share of R&D expenditure in GDP, the firms’ exit rate, the average growth rate, and other standard long-run targets. The calibrated model is used to quantify the welfare effects of counterfactual public debt policies, designating the long-run average of the U.S.public debt/GDP ratio as the status quo.====In one formulation of the model, the quantitative findings based on Balanced Growth Path (BGP) comparisons show that the equilibrium response of the endogenous growth rate is rather small. This result can potentially rationalize why some contributions in the empirical literature estimating the causal effect of increased public debts on the growth performance find negative effects, while others find no effect at all. One of the mechanisms that limits the effects on the growth rate is the behavior of one asset, the equity in the innovating firms held by the households. A reduction in the public debt/GDP ratio decreases the aggregate asset demand, triggering general equilibrium effects. Renting capital in the intermediates production becomes cheaper, profits increase, and so do dividends. The increase in the value of dividends causes the price of equity to rise. In this version of the model, the aggregate value of the asset market is relatively stable, as a decrease in the supply of assets stemming from a lower public debt is partially compensated by the opposite response of the value of equity in the innovating firms. This mechanism is what leads to the quantitatively small response of the growth rate, via a muted reaction of the interest rate. To allow for a stronger response of the growth rate, I consider another formulation of the model, without equity and with risk neutral entrepreneurs.====In both versions of the model, when comparing BGPs, I find large long-run welfare gains in equilibria characterized by governments accumulating public wealth. Welfare effects decompositions show that level effects and growth effects reinforce each other. Moreover, the growth component is always an important determinant of the welfare gains in the equilibria characterized by public wealth.====The importance of transitional dynamics has been emphasized by Desbonnet and Weitzenblum (2012), and also in this paper I find that the long-run welfare analysis delivers either upper or lower bounds (depending on whether public debt is reduced or increased, respectively). The path toward a new BGP, triggered by an exogenous decrease in the long-run value of the public debt/GDP ratio, entails some welfare costs. Along the transition, public debt is reduced by increasing taxes, which has a quantitatively important effect. Taking into account the dynamic adjustment to the new long-run equilibrium, I find that the transitional welfare costs are not large enough to change the sign of the welfare effects stemming from a change in public debt. The results show that eliminating public debt would lead to a 0.8% increase in welfare, while moving to a debt/GDP ratio of 100% (====) would entail a welfare loss (gain) of 0.5% (2.9%). A decomposition analysis shows that growth accounts for approximately 50% of the overall welfare effects.====In the literature, the closest papers are Röhrs and Winter (2017), Chatterjee et al. (2017), Chatterjee et al. (2018), and Cozzi (2022), none of which endogenizes the growth rate or features monopolistic distortions. Most of those contributions use the superstar income shocks to match the concentration of wealth found in the U.S.data. Although this is a tractable framework, and I consider it in a robustness check, the implied labor income dynamics are hard to reconcile with the relevant empirical evidence on income shocks. At least since Krusell and Smith (1998), discount factor heterogeneity has been used extensively in quantitative models with incomplete markets, and Epper et al. (2020) provide evidence supporting this channel using Danish experimental data. Röhrs and Winter (2017) identify inequality as the major driver of the welfare effects of public debt/GDP changes. Chatterjee et al. (2017) and Chatterjee et al. (2018) extend that framework to include public infrastructure, finding that this leads to a lower optimal debt level relative to the specification without infrastructure. In my Schumpeterian model, the specific modeling assumption on the income dynamics does not affect the main results. Finally, Cozzi (2022) finds that both open-economy and life-cycle considerations are important factors accounting for the desirability of public wealth for economies with access to international financial markets.====The rest of the paper is organized as follows. Section 2 presents a model with endogenous Schumpeterian growth and incomplete markets. Section 3 is devoted to the model calibration. Section 4 discusses the main results. Section 5 concludes. Four supplementary appendices present the model and the numerical methods used to solve it in more detail, and report the results of additional versions of the model.",Public debt and welfare in a quantitative Schumpeterian growth model with incomplete markets,https://www.sciencedirect.com/science/article/pii/S0164070423000393,Available online 2 June 2023,2023,Research Article,1.0
Meggiorini Greta,"Department of Economics, University of California, Irvine, United States of America","Received 2 March 2022, Revised 8 May 2023, Accepted 15 May 2023, Available online 27 May 2023, Version of Record 6 June 2023.",https://doi.org/10.1016/j.jmacro.2023.103538,Cited by (0),"This paper uncovers the fact that cognitive discounting modeled à la Gabaix (2020) is highly generalizable to alternative models and expectational assumptions by offering a mathematically tractable way of introducing behavioral elements in linearized models. This is not the case for other models of bounded rationality, as most derivations, if not all among those proposed up to today, are algebraically too cumbersome to be of general applicability.====This finding is used to introduce cognitive discounting into the Smets and Wouters (2007) model, hence building and estimating the first micro-founded behavioral medium scale ====, to my knowledge. The empirical estimation shows that the data prefers a substantial degree of bounded rationality even in a model with as many frictions as the Smets and Wouters model.","Traditional macroeconomic analyzes both in the academic environment and in policy institutions analyze the effects of monetary policy using DSGE models under the assumption that agents form their expectations perfectly rationally. Such an assumption is extremely strong, as it hinges on agents’ knowledge of all possible future states and on their ability to formulate fully state contingent plans by solving complex optimization problems. At the same time, most empirical studies of survey data reject the null hypothesis of full information rational expectations (FIRE). See, for example, Coibion and Gorodnichenko (2015), Bordalo et al. (2020) and Kohlhas and Walther (2021).====This evidence inspired a growing literature analyzing the role of the FIRE assumption, exploring the sensitivity of macroeconomic predictions to this assumption, and incorporating behavioral and more realistic components to include limitations in cognition, understanding, or information by agents. Over the last two decades, the literature on expectations in macroeconomics has produced a large number theoretical alternatives, including: adaptive learning (Evans and Honkapohja, 2001, Milani, 2008, Eusepi and Preston, 2018), sticky information (Mankiw and Reis, 2002), rational inattention (Sims, 2003, Maćkowiak et al., 2018), dispersed private information (Woodford, 2003), simple heuristics (de Grauwe, 2011).====More recently, various papers have proposed more or less behavioral adjustments to the benchmark New Keynesian model, often with the goal to increase discounting in order to attenuate the forward guidance puzzle first documented by Del Negro et al. (2012). For example, Woodford (2018) proposed a model with finite horizon planning, Gabaix (2020) with macroeconomic inattention, Angeletos and Lian (2018) with imperfect common knowledge, Airaudo (2020) inserts temptation in the utility function, and García-Schmidt and Woodford (2019) and Farhi and Werning (2019) introduce level-k thinking.====Despite its lack of realism and empirically counterfactual predictions, the FIRE assumption is still widespread in the empirical literature and very little work has been done in incorporating micro-founded behavioral components into larger scale DSGE models. This scarcity can be attributed, in large part, to the fact that the derivation of the equilibrium conditions can be highly complex, intractable and model-specific. The larger the size of a model, the more complicated is the development of its behavioral counterpart. As large scale DSGE models are the core policy tools of the majority of policy institutions, this is especially relevant for large models such as the New Area Wide Model (NAWM) widely used at the ECB or the SIGMA model at the Federal Reserve. Among other implications, behavioral components attenuate general equilibrium effects and help decrease the power of forward guidance which is necessary for a credible laboratory within which to conduct the analysis of monetary policy.====A common workaround has been to introduce ad hoc discounting factors to attenuate the effects of forward guidance. For example, Arias et al. (2020) modify the forward-looking linearized optimality conditions for households and firms in the medium sized model proposed by Del Negro et al. (2015) such that all expectations appearing in household optimality conditions are multiplied by an additional 0.95 compared with their standard coefficients and the coefficient on future inflation in the Phillips curve is multiplied by 0.9. Similarly, Müller et al. (2022) introduce an ad hoc discount factor in the linearized Euler equation of the NAWM, but not in front of any other expectational term in the model. Erceg et al. (2021) replace all forward-looking variables in the linearized first-order conditions of the rational expectations version of the Smets and Wouters (2007) model with their attenuated counterparts. All these modeling assumptions lack micro-foundations.====This paper proposes a departure from these ad hoc assumptions and helps bridge the gap between behavioral models and larger scale DSGE models. While many of the recent behavioral models lead to comparable loglinearized equations for the benchmark New Keynesian model, I uncover the fact that cognitive discounting modeled à la Gabaix (2020) is generalizable to alternative models and expectational assumptions by offering a mathematically tractable way of introducing behavioral elements in any model. This is not the case for other models of bounded rationality, where most derivations, if not all among those proposed up to today, are algebraically too cumbersome to be of general applicability. I make use of this finding to introduce cognitive discounting into the Smets and Wouters (2007) model, hence building the first micro-founded behavioral medium scale DSGE model, to my knowledge. I estimate the model on U.S. data and find that the data prefers a substantial degree of bounded rationality even in a model with as many frictions as the Smets and Wouters model.",Behavioral New Keynesian Models: An empirical assessment,https://www.sciencedirect.com/science/article/pii/S0164070423000381,27 May 2023,2023,Research Article,2.0
"Kamiguchi Akira,Tamai Toshiki","Faculty of Economics, Kindai University, 3-4-1 Kowakae, Higashi-Osaka, Osaka 577-8501, Japan,Graduate School of Economics, Nagoya University, Furo-cho, Chikusa-ku, Nagoya, Aichi 464-8601, Japan","Received 14 September 2022, Revised 25 March 2023, Accepted 11 May 2023, Available online 12 May 2023, Version of Record 18 May 2023.",https://doi.org/10.1016/j.jmacro.2023.103535,Cited by (0)," if dynamic inefficiency exists. In contrast, a balanced budget is preferred in a dynamically efficient economy with low productivity effects of public capital. However, an economy with high productivity effects of public capital might cho ose debt financing. This paper contributes to the elucidation of currently emphasized issues of public investment.","Public investment and its effects on economic performance have been long analyzed as a core issue in the literature on fiscal policy and economic growth. Arrow and Kurz (1970) contributed to the construction of a dynamic framework of analyzing public investment. They also provided a comprehensive analysis. Futagami et al. (1993) extended the sophisticated model of Arrow and Kurz by developing an endogenous growth model with public capital after Aschauer (1989) found empirical evidence of the productivity effects of public capital. Empirical studies have supported the existence of positive growth effects of public investment.==== However, succeeding reports of theoretical studies have described that debt financing for public investment does not enhance the economic growth rate in many cases.====The IMF (2014, Ch.3) reported that public investment has short-term and long-term positive growth effects and that debt-financed public investment increases output growth. This finding suggests that ==== Blanchard (2019) argued that public debt might entail no fiscal costs because safe interest rates are expected to remain below growth rates. That argument suggests that welfare costs through reduction of private capital accumulation might be smaller than typically assumed. The findings presented above are expressed as low fiscal costs because of lower interest rates becoming less than growth rates. Therefore, the controversial outcomes of growth effects of public investment by debt financing arise mainly from the evaluation of debt financing costs, such as interest payments.====Public debt costs are fundamentally associated with dynamic efficiency issues. An economy with oversaving is regarded as dynamically inefficient because individuals can be made better off by increasing their amounts of consumption. Under such circumstances, the oversaving engenders declining interest rates, which become lower than the economic growth rate. Using an overlapping generations (OLG) model, Diamond (1965) demonstrated that private capital can be overaccumulated and demonstrated that public debt can improve efficiency because debt issuance decreases oversaving. Such a dynamically inefficient economy is characterized as having interest rates that are lower than growth rates.====As a counterpart to Diamond's model, Ihori (1978) and Tirole (1985) showed that public debt or bubbles (non-productive assets) can improve efficiency in a dynamically inefficient economy. King and Ferguson (1993) examined dynamic efficiency and Ponzi games in an OLG model of endogenous growth.==== They also show that oversaving occurs, with growth rates higher than interest rates. Therefore, dynamic inefficiency might arise in the OLG models. Public debt can be an important policy instrument if a government has no other way of solving dynamic inefficiency.====In the US economy, the nominal growth rate outweighs the long-term nominal interest rate for almost the entire period of 2008–2021 (Fig. 1), indicating a dynamically inefficient economy.==== Mishkin (1984) presented evidence of this phenomenon using data of seven OECD countries from 1967 to 1979. Von Weizsäcker (2014) recently suggested that the real interest rates have become negative in the OECD countries and China. Abel et al. (1989) developed a criterion for dynamic efficiency in lieu of comparing interest rates and growth rates. They found that major OECD countries are dynamically efficient. Moreover, Geerolf (2018) revisited their results, updating data and adding countries. In contrast to the findings reported by Abel et al. (1989), the study clarified that sufficient conditions for dynamic efficiency are not verified in advanced economies and that a criterion for dynamic inefficiency is verified in Japan and South Korea. In sum, dynamic inefficiency is possible in practice.====Debt financing for public investment is related to capital budgeting.==== Particularly, the budgetary regime is known as the golden rule of public finance (GRPF). Many countries legally adopted GRPF and its variations.==== Musgrave (1939), in pioneering work, developed analysis of the budget rule. Debt-financed public investment influences intergenerational welfare through different long-term benefits and costs. For that reason, debt financing might serve in an important role of improving social welfare. Indeed, Bassetto and Sargent (2006) showed that the GRPF can improve efficiency in an OLG economy. Yet the necessity remains of considering a debt financing rule including the GRPF for identifying the growth effects of debt-financed public investment.====In infrastructure-led growth models without an OLG structure, such as Futagami et al. (1993), dynamic inefficiency does not occur. The presence of debt negatively affects economic performance irrespective of the productivity effect of public capital. To address the possibility of dynamic inefficiency and debt financing for public investment, the OLG model of endogenous growth with public capital must be developed. In the OLG models, Yakita (2008) demonstrated the existence of an initial debt threshold for the sustainability of government budget deficits under the GRPF. In addition, Teles and Mussolini (2014) demonstrated that the level of the debt-to-GDP ratio adversely influences the growth effects of fiscal policy. They specifically examine debt sustainability and the relation between debt and economic growth.====The goal of this paper is to elucidate why debt financing is chosen for public investment. Therefore, we develop an OLG model of endogenous growth with public capital under the generalized version of the GRPF to examine growth effects of debt-financed public investment. Income tax and public bonds are used for financing public investment and for interest payments on public bonds. The borrowing rule requires that issuing bonds be allowed for public investment only. The total amount of debt must not exceed the public capital stock.====First, we show that public investment under debt finance positively affects economic growth in two ways. Public investment financed by increased tax rate enhances economic growth through the productivity effect of public capital. In addition, public investment is positively associated with public bond issuance under the borrowing rule. Then, public investment financed by taxation increases debt and raises the economic growth rate. Public investment financed by bonds might affect economic growth positively if the interest rate is lower than the growth rate. Increased debt hampers private capital accumulation and adversely affects economic growth. However, if the productivity effect of public capital is sufficiently large, then an increase in the debt dependency ratio of public investment raises the economic growth rate. In either case, excessive public investment diminishes the productivity effects of public capital. Therefore, the growth-maximizing level of public investment is verified.====Second, we demonstrate that debt financing for public investment can be politically preferred by existing generations whether the economy is dynamically efficient or inefficient. Existing generations are unwilling to increase public investment for economic growth because benefits provided by economic growth occurring far into the future cannot improve their welfare. Consequently, they aim to reduce their current burdens. People depend on debt financing to maximize their lifetime income rather than the economic growth rate. They can ignore crowding-out effects on private capital accumulation. If the economy is dynamically inefficient, then existing generations choose the GRPF because of the absence of debt costs. Even if the economy is dynamically efficient, debt financing for public investment is chosen with high productivity effects of public capital because the welfare cost is low. However, people prefer a balanced budget scheme when the productivity effect is not large.====Finally, numerical analyses provide further insights into public investment and debt financing. Under debt financing, public investment generates persistent economic fluctuations. Particularly, the economic dynamics exhibits chaos with low total factor productivity. The equilibrium dynamics for a high degree of debt dependency is complicated, irrespective of the total factor productivity. Computed results also indicate that the equilibrium tax rate and degree of debt financing closely approximate those in the real economy. Furthermore, an increase in debt-financed public investment enhances short-term and long-term economic growth in a dynamically inefficient economy. These results are consistent with certain empirical evidence.====The remainder of this paper is organized as follows. Section 2 presents discussion of the related literature. Section 3 presents a description of the basic setup of our model and characterizes the stationary equilibrium and its transitional dynamics. Section 4 describes an examination of the growth effects of debt-financed public investment and presents consideration of an endogenous determination of the income tax rate and borrowing ratio. Section 5 develops numerical analyses for the additional analyses in Section 4. Finally, Section 6 concludes this paper by explaining some possible extensions and issues for additional research.","Public investment, national debt, and economic growth: The role of debt finance under dynamic inefficiency",https://www.sciencedirect.com/science/article/pii/S0164070423000356,12 May 2023,2023,Research Article,3.0
Shahnazarian Hovick,"Uppsala Center for Fiscal Studies, Department of Economics, Uppsala University, Box 513, SE-75120 Uppsala, Sweden","Received 10 November 2022, Revised 18 April 2023, Accepted 9 May 2023, Available online 9 May 2023, Version of Record 20 May 2023.",https://doi.org/10.1016/j.jmacro.2023.103528,Cited by (0),The optimal fiscal stabilization rule presented in this paper is derived from a loss function where the government is assumed to keep the structural balance close to its target level and simultaneously stabilize the GDP and ,"Prior to the Global Financial Crisis (GFC) of 2008-09, the prevailing view among economists was that the best way for fiscal policy to support monetary policy to stabilize the economy was through the use of automatic stabilizers and by maintaining public confidence concerning the long-term sustainability of public finances. There was also a general belief that discretionary fiscal stabilization policies should be used sparingly. After the GFC, many economists noted that in a world of low neutral interest rates, monetary policy alone may not be sufficient to counter a recession. In such cases, fiscal policy may need to shoulder some of the burden to maintain economic growth and macroeconomic stability (see, e.g., Woodford, 2011, Farhi et al., 2013, and Calmfors et al., 2023). Furman (2016) described this as “the new view” of fiscal policy. Recent research has also emphasized that monetary and fiscal policies always interact to jointly determine the overall level of aggregate demand and prices in the economy (See, e.g., Leeper, 2016 and Molteni and Pappa, 2017, for useful surveys of the relevant literature in this area). It is reasonable to assume that countries planning to use fiscal stabilization policies to support monetary policy would benefit from an optimal fiscal stabilization rule that can be used to guide policy decisions (for arguments in favor of the use of optimal simple rules for monetary- and fiscal policy rules, see for example Chadha and Nolan, 2007). The main purpose of this paper is to propose such an optimal fiscal stabilization rule. This is done as follows. First, we assume that monetary and fiscal policies commit to follow different policy rules. Second, we assume that the fiscal and monetary authorities have identical inflation and output objectives, even though they are allowed to make different trade-offs between these objectives. Third, we assume that the fiscal authority also has an additional target for the structural balance (that is the fiscal balance adjusted for cyclical fluctuations). Fourth, we use these assumptions to set-up a loss function for the fiscal authority. The optimal fiscal stabilization rule is then derived by minimizing the loss function of the fiscal authority. This yields a simple and practical policy rule that allows policymakers to calculate the size of the discretionary stabilization measures needed, in addition to automatic stabilizers, to be able to stabilize the business cycle, without compromising the public sector's fiscal balance target. Using this policy rule and a first-order Taylor expansion of the fiscal balance, we also further decompose the automatic stabilizers and the discretionary fiscal policies conditional on the state of the business cycle.====We contribute to four broad strands of literature. First, we contribute to the literature concerning instrument and targeting rules for stabilization policy. Monetary policy rules have long been used by central banks to guide policy (See, e.g., Woodford, 2001). The Taylor rule (Taylor, 1993) is a simple instrument-based rule, in which interest rates respond to the inflation gap and the output gap (Taylor, 1999 adjusts the Taylor rule somewhat and tests the rule´s historical relevance). Svensson (2003) argues that inflation targeting is best understood as a commitment to a targeting rule rather than an instrument-based rule. Instead, he proposes a general targeting rule. That is, to determine the target variable and its target value, and then determine the relative weights to stabilize the target variable around that target value. He shows that, given marginal rates of transformation (which come from an estimated Phillips curve) and the marginal rates of substitution from the loss function, a first-order condition for the optimal policy gives a specific targeting rule. This paper contributes to the literature by providing an optimal targeting rule for fiscal policy that can be used by governments for fiscal policy guidance during business cycles. The optimal path of fiscal stabilization policy works in much the same way as the optimal path of monetary policy.====Second, we contribute to the literature on commitment and discretion in monetary and fiscal policy. Alesina and Tabellini (1987) set up loss functions for both fiscal and monetary authorities by assuming that they have the same explicit goals for inflation, output, and government spending, but different trade-offs between these goals. They find that monetary policy commitments may not improve welfare if the two authorities assign different weights to their respective goals. Dixit and Lambertini (2003) analyze the interaction between monetary and fiscal policies assuming that the central bank is more conservative than the government not only in weights but also in goals. One of their main findings is that the interaction can result in an equilibrium that is extreme in both output (low) and prices (high) because fiscal policy is too contractionary and monetary policy too expansionary.==== The second-best outcome of the interaction requires either a joint commitment to a policy rule or identical goals for the two authorities in charge of monetary and fiscal policy. Calmfors et al., 2023 in their setup, assume that monetary and fiscal policy follow different policy rules and have different targets for the GDP gap and assume that the fiscal authority has an additional target for the fiscal balance.==== In equilibrium, this delegated monetary policy leads to inflation and fiscal deficits because the target for the level of output is higher than the potential level of output. Thus, the central bank will choose a real interest rate higher than the natural rate of interest. This paper contributes to this literature by providing an optimal fiscal policy rule that, together with an optimal monetary policy rule, brings the economy closer to a socially optimal equilibrium. The optimal rule is derived by assuming that even though monetary and fiscal policy follow different policy rules, they have identical targets for the GDP gap and inflation gap but are allowed to make different tradeoffs between these targets. It also assumes that the fiscal authority has an additional target for the structural balance.====Third, we contribute to the literature discussing the combination of fiscal and monetary policy that is most likely to produce macroeconomic stability (Chadha and Nolan, 2007, Woodford, 2011, Farhi et al., 2013, Leeper, 2016, Molteni and Pappa, 2017, and Calmfors et al., 2023). This is done by analyzing the interaction between fiscal and monetary policy in a regime of monetary dominance where monetary policy focuses on price stability and the role of fiscal policy is to enhance macroeconomic stability without jeopardizing the sustainability of public finances.====Fourth, the results of this paper also relate to the tax policy literature. This links the three central areas of research within macroeconomics mentioned above with the central areas of research within public economics. These links will become more important in the future as fiscal policy is used to stabilize the economy along with monetary policy. The reason for these linkages is that fiscal authorities have at their disposal a variety of tax and spending instruments that can be used for stabilization policy purposes. It is therefore important to quantify the contribution of labor taxes, payroll taxes, corporate income taxes, indirect taxes, and expenditures to the cyclical and structural components of the fiscal balance in different states of the business cycle.====The paper is organized as follows: Section 2 introduces the general government´s fiscal balance, section 3 describes the method used to decompose the fiscal balance and section 4 derives the fiscal stabilization rule. Section 5 concludes.",Fiscal stabilization rule,https://www.sciencedirect.com/science/article/pii/S0164070423000289,9 May 2023,2023,Research Article,4.0
"Gandjon Fankem Gislain Stéphane,Fouda Mbesa Lucien Cédric","Faculty of Economics and Management, University of Yaoundé II, Po Box. 1365, Yaoundé, Cameroon","Received 9 October 2022, Revised 7 March 2023, Accepted 5 May 2023, Available online 6 May 2023, Version of Record 15 May 2023.",https://doi.org/10.1016/j.jmacro.2023.103527,Cited by (0),"In this paper, we analyze the feasibility of an African monetary union based on the necessary condition of business cycle synchronization. In this regard, we examine the synchronization of growth cycles between five Regional Economic Communities (RECs) that will have to merge to form an African monetary union: the East African Community; the Economic Community of Central African States; the Economic Community of West African States; the Southern African Development Community; and the Arab Maghreb Union. To do this, we use a new continuous wavelet approach. Results show evidence of heterogeneous synchronization across time and horizons between the RECs. Controlling for the influence of some countries' membership in several RECs at the same time, the synchronization landscape does not improve. Overall, our results suggest that business cycle synchronization across the RECs has not yet reached a sufficient level to allow African countries to benefit from a common ====.","Whether it takes the form of the adoption by third countries of the national currency of another country or the creation of a single currency at the level of a regional area, monetary union has three main advantages (De Grauwe, 2020; Groll and Monacelli, 2020). Firstly, the disappearance of the exchange rate, as a result of the monetary union, eliminates any exchange rate risk. Consequently, in international transactions, there is a reduction in transaction costs and in the cost of hedging the exchange rate risk. This is the advantage linked to international trade. Secondly, insofar as monetary union is a process that is very difficult to reverse, it offers countries with an inflationary tradition a credible nominal anchor. More precisely, either the country will tend to import the inflation of the anchor country or its inflation rate will have to converge towards the average of the regional area forming a monetary union. It is therefore to be expected that inflation expectations will be lowered and that the level of interest rates will be reduced in the countries forming a monetary union. This is the benefit of commitment. Thirdly, the widening of markets associated with monetary union leads to an improvement in the allocation of resources. Consumer welfare is increased by the resulting intensification of competition, with its traditional benefits (on prices, quality and innovations). Producers' production costs are lower due to greater freedom of trade, economies of scale and location optimization. This leads to gains in production and employment (Hauser and Seneca, 2022).====While Europe has enjoyed the benefits of a monetary union around the Euro for two decades, the African Union (AU) has also set the ultimate goal of creating an African monetary union (Couharde et al., 2022). The AU's strategy is based on the creation of monetary unions in the existing Regional Economic Communities (RECs) before they are merged into a single monetary union. Such a project, which has important economic and political consequences for the whole of Africa, requires scientific scrutiny. However, to our surprise, there is no applied research on the feasibility of an African monetary union along the lines of the AU's model. Our aim is to fill this gap.====Theoretically, the feasibility of a monetary union is analyzed through the theory of optimal currency areas (OCAs), which contrasts an exogenous and an endogenous approach (Chari et al., 2020). The exogenous approach or exogenous criteria theory, highlighted by Mundell (1961) and further developed by McKinnon (1963), Kenen (1969), Scitovsky (1967) and Ingram (1969), considers the synchronization of business cycles as an ""ex-ante"" condition for the formation of a monetary union. In contrast, the endogenous approach to OCAs initiated by Frankel and Rose (1998), considers this criterion more as an ""ex-post"" condition for a monetary union: it is the monetary union that would lead to a better synchronization of real business cycles. However, Krugman (1993) points out that monetary union will lead to specialization between countries and consequently to a decrease in the synchronization of business cycles.====Like most of the work on the euro area, our approach is based on the exogenous approach of OCAs theory (De Haan et al., 2008; Gächter and Riedl, 2014; Campos et al., 2019; Stiblarova and Sinicakova, 2020). According to the exogenous approach, a high degree of synchronization of business cycles between member states is crucial for the smooth functioning of the monetary union (Aguiar-Conraria and Soares, 2011b; Aguiar-Conraria et al., 2017). The latter permanently removes the national independence of currencies, monetary and exchange rate policies. The objective of a common monetary policy instrument is then to stabilize fluctuations in output and inflation rates across the area. Strongly synchronized national business cycles will facilitate the implementation of monetary policy, as they allow for a clear definition of the common policy stance and sequencing. Conversely, desynchronization will complicate the functioning of monetary union, as countries at different stages of their economic cycle face different monetary requirements (Beck, 2021). Even if the synchronization of business cycles is not sufficient to ensure that a monetary union is desirable, it is nevertheless a necessary condition (Arčabić and Škrinjarić, 2021).====In order to assess the feasibility of an African monetary union based on the criterion of business cycle synchronization, we use the AU framework set out in the Lagos Plan of Action, the Abuja Treaty and Agenda 2063. Specifically, we examine the synchronization of business cycles between five identified Regional Economic Communities (RECs) that are to merge to form an African monetary union, namely: the East African Community (EAC); the Economic Community of Central African States (ECCAS); the Economic Community of West African States (ECOWAS); the Southern African Development Community (SADC); and the Arab Maghreb Union (AMU). We therefore ask whether the synchronization of economic cycles has reached a sufficient level to allow all these RECs to benefit from a common monetary policy. We capture the level of business cycle synchronization among the African RECs by the co-movement of their real growth rates over the period 1981–2019. The rationale for the choice of RECs and the basis for the AU's strategy to form an African monetary union are presented in the second section of this paper.====While the assessment of business cycle synchronization is primarily an empirical question, there is no consensus on how it should be done (De Haan et al., 2008). However, Massmann and Mitchell (2005) suggest looking for an increased bivariate correlation of cyclical components, a decrease in cyclical disparity or evidence of the emergence of a common factor driving the business cycles of different countries. In this respect, we methodologically distinguish our work from the majority of empirical analyses of business cycle synchronization by using the wavelet approach. ""Wavelets are, by definition, small waves. That is, they start at a finite point in time and die out at a later finite point in time. As such, they must, whatever their form, have a definite number of oscillations and last for a certain period of time or space"" (Crowley 2007, p. 208). Wavelet analysis, the advantages of which have not yet been widely exploited in economics (Aguiar-Conraria and Soares, 2014), brings together time and frequency domain information. Conventional time series techniques, namely vector autoregressive models, generalized heteroskedastic conditional autoregressive processes, capture only information from the time domain and ignore any information from the frequency domain. Moreover, they only provide an average degree of business synchronization over the whole period under study, without distinguishing between short, medium and long term horizons. On the other hand, frequency-based methods, e.g. Fourier transformation techniques, completely ignore information from the time sphere. This makes it difficult to distinguish transient relationships or to identify structural changes. In contrast to approaches that focus on either the time domain or the frequency space, wavelet analysis covers both. The main novelty of wavelets is that they decompose time series data into time-frequency domains. This decomposition into frequency modules allows, unlike other methods, to differentiate between short and long term cycles. We adopt a continuous wavelet approach, given its usefulness in understanding time series characteristics and measuring the co-movement of variables over time and across time horizons (Aguiar-Conraria and Soares, 2011b; Berdiev and Chang, 2015; Aloui et al., 2016). Our study is the first to apply this type of analysis to the assessment of business cycle synchronization across RECs.====Our results show that some African RECs respond similarly to external shocks (ECCAS and ECOWAS), although others have unique and specific responses (AMU). Furthermore, while some RECs show strong synchronization on the long term frequencies (ECCAS and SADC, SADC and EAC, ECCAS and EAC), others show very weak synchronization on all frequencies (ECCAS and UMA, UMA and SADC, ECOWAS and SADC). Even if we neutralize the influence of some countries' membership of several RECs at the same time, the synchronization landscape does not improve. We also see a rejection of the endogeneity hypothesis of the optimal currency area theory, as the RECs that share the same currency (ECOWAS-ECCAS) are very weakly synchronized. Overall, our results suggest that business cycle synchronization among the RECs has not yet reached a sufficient level to allow African countries to benefit from a common monetary policy. Our study is an important contribution to the literature, as it shows that the relationship between synchronization levels exists and is not the same at different points in time and at different frequencies: short, medium and long frequencies. This allows us to know precisely when and how often the actual cycles are correlated or not, in order to better assess the progress of the integration policies implemented.====The rest of paper is organized as follows. Section 2 presents the rationale for the African monetary union project. Section 3 reviews the related literature. Section 4 describes the wavelet methodology. Section 5 provides a preliminary analysis of the data. Section 6 presents and discusses the results of the wavelet analysis. Section 7 concludes.",Business cycle synchronization and African monetary union: A wavelet analysis,https://www.sciencedirect.com/science/article/pii/S0164070423000277,6 May 2023,2023,Research Article,5.0
Zhao Junzhu,"Central University of Finance and Economics, China","Received 18 August 2022, Revised 18 February 2023, Accepted 17 April 2023, Available online 26 April 2023, Version of Record 10 May 2023.",https://doi.org/10.1016/j.jmacro.2023.103525,Cited by (0), rule can be passive in order to avoid indeterminacy depends critically on the degree of preference over wealth as well as the underlying structures and parameters of the model.,"Price stability serves as the primary objective for many central banks. How is the price level determined, and what constitute the appropriate monetary policy framework to achieve stable prices. The determinacy properties of equilibrium play important role in ensuring price stability.==== Hence, equilibrium determinacy is at the core of fundamental questions in monetary economics. This paper revisits the issue of the Taylor principle and determinacy in the New Keynesian (NK) model by introducing wealth in the utility function.====We augment the baseline NK model of Woodford (2003) and Galí (2015) with wealth in the utility function, assuming that households derive utility directly from holding real wealth.==== In doing so, consumer’s Euler equation, and, hence, dynamic IS equation, displays the feature of discounting, warranted by a weaker impact of both current and expected future real rate on today’s real activity.==== We show that the demand channel of the Taylor principle and equilibrium determinacy emerges if consumer’s Euler equation or dynamic IS equation is discounted. The demand channel is originated from the dynamic IS equation on the demand side of the economy, and transmitted through the price inflation New Keynesian Phillips Curve (henceforth, NKPC for short) on the supply side of the economy, and then exerts forces on central bank’s incentive to bring down the strength of endogenous responses of monetary policy, thereby reshaping the Taylor principle and the condition to ensure equilibrium determinacy. Thus, the demand channel is of great importance in determining the Taylor principle and the condition to ensure that the economy has a locally stationary rational expectation equilibrium in NK models.====In the baseline NK models of Woodford (2003) and Galí (2015), the monetary policy rules must conform to the Taylor principle — the central bank varies its interest-rate instrument by more than one-for-one in response to deviation of the inflation rate from its long-run target, to ensure determinacy of the locally stationary rational expectation equilibrium. The Taylor-type policy rule implies that the central bank is committed to always vary its policy instrument in response to deviation of the inflation rate from the long-run inflation target as specified in the policy rule, independent of the history and the state of the economy. The “threat” to adjust nominal interest rate aggressively in response to the inflation rate at all dates is able to refrain the economy from sunspot fluctuations. This argument is well carried over to many other studies on the issue of determinacy and indeterminacy in NK literature, for instance, Clarida et al., 2000, Bullard and Mitra, 2002, Woodford, 2003, Walsh, 2010 and Galí (2015) in baseline NK models, Peter et al. (2008) and Alexis and Aurélien (2016) in NK models with both staggered price and wage settings.====With the presence of wealth in the utility function, the dynamics IS equation is discounted, creating the demand channel of the Taylor principle, that is, to ensure determinacy of the rational expectation equilibrium paths of the model, the monetary policy rule does not have to strictly conform to the Taylor principle. In fact, a passive policy rule which adjusts nominal interest rate by less than one-for-one in response to the inflation rate is able to ensure equilibrium determinacy. Why would the central bank be willing to reduce the “threat” up to the point where it is committed to implement a passive policy rule. Consider a permanent increase in the inflation rate, the monetary policy rule dictates the nominal interest rate to increase, though by less than the rise in the inflation rate, which lowers the endogenous real interest rate. The discounting to dynamic IS equation induces a permanent increase in output gap in response to a lower endogenous real interest rate, which opens up the room for the demand channel. The NKPC implies further rise in the inflation rate accordingly, which to some extent shares the burden of the central bank monetary policy rule in reacting to the inflation rate with the objective of preventing the emergence of multiple equilibria. This is the benefit from the passive response to the inflation rate. On other hand, with a passive policy rule, the direct response of expected future inflation to current inflation is less than one-for-one, which makes it harder to induce an explosive path of the inflation rate and thereby increases the difficulty of ruling out sunspot fluctuations. To ensure equilibrium determinacy, the central bank monetary policy rule needs to strike a balance between the cost and the benefit from a passive response to the inflation rate. As such, a passive policy rule can also rule out equilibrium indeterminacy. However, if wealth in the utility function is absent, the dynamics IS equation does not show any degree of discounting, the implied strong anticipation effect of private sector would not be able to induce a permanent increases in the output gap in face of a lower endogenous real interest rate. As a result, the demand channel of the Taylor principle is ineffective when the monetary authority attempts to inject stimulus to the economy by lowering the real interest rate. The central bank would thus have no incentive to reduce the “threat” up to the point where it is committed to implement a passive policy rule. Hence, in the absence of wealth in the utility function, corresponding determinacy condition indicates that increase in nominal interest rate should be more than one-for-one with increase in the inflation rate. The Taylor principle must be conformed.====In addition, the extent to which monetary policy can be passive in order to ensure equilibrium determinacy is not unconstrained. We show that, with low or moderate degree of preference over wealth, determinacy conditions and determinacy regions hinge critically on the degree of preference over wealth as well as the underlying parameters and structures of the NK models with wealth in the utility function. To be more specific, the greater the degree of preference over wealth, the stronger the extent to which the dynamic IS equation is discounted, the larger the extent to which monetary policy can be passive, the wider the determinacy region; the smaller the intertemporal elasticity of substitution of consumption, the lower the sensitivity of current real activity in response to current interest rate, the larger the extent to which monetary policy can be passive, the wider the determinacy region; the greater the discount factor, the stronger the degree of discounting to the NKPC, the smaller the extent to which monetary policy can be passive, the narrower the determinacy region; the greater the degree of nominal rigidities, the smaller the slope of the NKPC, the lower the sensitivity of current inflation rate to current output gap, the larger the extent to which monetary policy can be passive, the wider the determinacy region.====We further consider the case of forward-looking rules relative to contemporaneous rules. Under forward-looking monetary policy rules the determinacy conditions feature not only a lower bound but also an upper bound. The lower bound of the determinacy conditions under forward-looking rules is exactly the same as its counterpart under contemporaneous rules, lending further support to the effect of the demand channel due to discounted dynamic IS equation in the presence of wealth in the utility function. Moreover, the upper bound of the determinacy region is influenced by the degree of preference over wealth — the greater the degree of preference over wealth, the stronger the degree of discounting to the dynamic IS equation, the larger the value of the upper bound, the wider the area of the determinacy region. So, the presence of wealth in the utility function enlarges the determinacy regions under forward-looking monetary rules.====Then, we augment the baseline NK model with wealth in the utility function and Calvo staggered nominal wage setting to show how the demand channel reshapes the Taylor principle and determinacy of equilibrium in the presence of wage stickiness. Again, the presence of the demand channel due to the discounted dynamic IS equation resulted from wealth in the utility function reduces the needs of central bank’s “threat” to ensure determinacy of the rational expectation equilibrium, as a result of endogenous fall in real rate gap. The central bank is willing to reduce the “threat” up to the point where the central bank is committed to implement a passive policy rule that varies its interest-rate instrument by less than one-for-one in response to the ==== inflation. Moreover, the extent to which monetary policy can be passive in order to avoid indeterminacy is governed by the degree of preference over wealth as well as the constraint on the frequency of nominal wage settings. Overall, the impact of wealth in the utility function on the Taylor principle and equilibrium determinacy is more pronounced when nominal wage rigidities are introduced, due to the fact that the presence of staggered nominal wage setting increases the economy’s overall rigidities.====Finally, we allow for public debt dynamics. Accordingly, bond holdings enters the dynamic IS equation and has a direct impact on output gap, as a result of the wealth effect arising from the assumption that bond holdings is net wealth and tends to stimulate aggregate demand. Thus, the Ricardian equivalence no long holds. As in Leeper (1991) and Woodford (2003), we show that equilibrium is determinate provided that either fiscal policy accommodates monetary policy or fiscal policy plays the dominant role. If fiscal policy accommodates monetary policy, a passive monetary policy is also able to ensure equilibrium determinacy. The extent to which monetary policy can be passive hinges critically on the degree of preference over wealth as well as the specification of fiscal policy. When fiscal policy plays the dominant role, a passive monetary policy rule is able to ensure the existence of a unique bounded rational expectations equilibrium, a well-known result in Leeper (1991) and Woodford (2003). Surprisingly, an active monetary policy can also rule out equilibrium indeterminacy, with the extent being determined by the degree of preference over wealth, the strength of fiscal policy, and the underlying structural parameters. Overall, the presence of preference over wealth makes it easier for the central bank to ensure equilibrium determinacy, and thereby enlarges the determinacy regions, even after taking into account public debt dynamics.==== Given the fact that the Taylor principle is derived by mapping the policy response coefficients to the eigenvalues of the state space representation of the model, the restriction on the policy response must satisfy to ensure determinacy should hinge on the specification of the underlying models. Many researches have been conducted to explore how different specifications of the models affect the Taylor principle and equilibrium determinacy of NK models.====Bullard and Mitra (2002) show rigorously the sufficient and necessary condition to ensure the existence of a unique locally stationary equilibrium and justify the validity of the Taylor principle in the baseline NK model. Clarida et al. (2000) stress that sunspot fluctuations may take place if the nominal interest rate responds too aggressive to the expected future inflation under a forward-looking rule. Peter et al. (2008) examine determinacy of the baseline NK model with both staggered price and wage settings and show analytically the validity of the Taylor principle in a generalized Taylor rule which employs a weighted average price and wage inflation, by transforming the formulation of the model from discrete to continuous time. Alexis and Aurélien (2016) further generalize the results of Peter et al. (2008) and prove that the Taylor principle does hold in the discrete-time version of the NK model with both nominal price and wage rigidities. In these studies, the demand channel of the Taylor principle and equilibrium determinacy is ineffective due to the absence of discounting to the dynamic IS equation. With strong anticipation effect in the consumer’s Euler equation, the monetary policy rule must conform to the Taylor principle to ensure the existence and uniqueness of the locally stationary rational expectation equilibrium.====Another strand of the literature sheds light on introducing positive trend inflation in the NK model with staggered nominal price setting. Kiley (2007) and Ascari and Ropele (2009) examine the effect of trend inflation on the ability of the monetary authority to ensure a determinate equilibrium and find that the standard Taylor principle can be insufficient to ensure determinacy in the standard small scale Neo-Keynesian model that allows for positive trend inflation.==== However, the effect of trend inflation is decreasing with respect to the degree of price indexation. Many studies have shown that the standard Taylor principle would continue to work in the presence of trend inflation so long as some form of price indexation is introduced. See Coibion and Gorodnichenko (2011).====The presence of the cost channel of monetary policy may also affect the Taylor principle and equilibrium determinacy.==== Kurozumi and Van Zandweghe (2010), in a NK model with search and matching labor market frictions, demonstrate that the standard Taylor principle of adjusting nominal interest rate more than one-for-one in response to inflation may not ensure determinacy. Rannenberg (2015) introduces skill decay during unemployment into Blanchard and Galí (2010)’s sticky price model with hiring frictions and real-wage rigidity and shows the implications for determinacy. Tesfaselassie and Schaling (2016) examine the determinacy in Blanchard and Galí (2010)’s sticky price model with hiring frictions and find that the indeterminacy region in the policy space increases with the hiring costs. Again, the demand channel of the Taylor principle and equilibrium determinacy is absent in these studies as a result of the absence of discounting to the dynamic IS equation.====Auclert et al. (2018) provide numerical simulations in a quantitative heterogeneous-agent New-Keynesian model with incomplete markets, finding that determinacy is more likely under pro-cyclical risk but less likely under countercyclical risk. Bilbiie (2019) shows in a heterogeneous-agent New-Keynesian model, when inequality is pro-cyclical, aggregate Euler–IS equation is discounted, Taylor principle is easier to be met and the determinacy region is enlarged. Gabaix (2020), in a NK model with bounded rationality where the agent is partially myopic and does not perfectly anticipate future a typical events (cognitive discounting), shows that agents’ behavior are discounted. A policy action that occurs deep in the future get heavily discounted, with the degree of discounting being determined by the degree of cognitive discounting. Therefore, the Taylor principle is modified and easier to satisfy than under full rationality. Acharya and Dogra (2020) derive an income-risk augmented Taylor principle which depends on the cyclicality of income risk in a heterogeneous-agent New-Keynesian model with incomplete markets and uninsurable idiosyncratic income risk. Procyclical income risk makes indeterminacy less likely, while countercyclical risk makes indeterminacy more likely. Ravn and Sterk (2021) examine an analytical NK model with heterogeneous agents, nominal rigidities and search and matching frictions (HANK & SAM) with endogenous unemployment risks and show that when the endogenous earning risk is countercyclical, the Taylor principle may fail to guarantee local determinacy as countercyclical risk generates compounding to Euler equation. Our paper is most close to Michaillat and Saez (2021), who introduce wealth into the utility function in a continuous-time version of standard NK model, and show that the Euler equation is discounted as people are less forward-looking. They elaborate that, with enough marginal utility of wealth, the Euler–Phillips system in normal times is always a source whether monetary policy is active or passive. As pointed out in Michaillat and Saez (2021), in the NK model with wealth in the utility function, the equilibrium is always determinate, so the central bank does not need to worry about how strongly the monetary policy reacts to inflation, under the assumption that the utility derived from wealth is sufficient large.====The real balance effect may also play important roles in affecting the Taylor principle and equilibrium determinacy. Piergallini (2006) extends the NK model with money-in-the-utility function by introducing Yaari (1965)–Blanchard (1985) overlapping generations framework and derives the real balance effect that explicitly enters the dynamic IS equation. Piergallini (2006) shows that the real balance effect makes the rational expectation equilibrium determinate also under a relatively passive Taylor rule. In a similar setup while allowing for the interaction between fiscal policy and interest rate rule, Annicchiarico et al. (2008) find that a passive monetary policy may ensure determinacy of the rational expectation equilibrium under both Ricardian and non-Ricardian fiscal policy regimes.====Compared with previous studies, our work contributes to the literature in following aspects: (1), Michaillat and Saez (2021) make the assumption that the degree of preference over wealth is sufficient strong so indeterminacy is never a risk. We relax that assumption and allow for low or moderate degree of preference over wealth, showing that the extent to which the monetary policy rule can be passive in order to avoid indeterminacy depends critically on the degree of preference over wealth and the underlying structures of the model. (2), We derive explicitly the condition to ensure that the economy has a unique locally stationary rational expectation equilibrium, and argue that the degree of preference over wealth as well as the underlying parameters and structures of the NK models plays important role in determining the determinacy conditions and determinacy regions. (3), We identify and exposit the demand channel of the Taylor principle and equilibrium determinacy in discrete-time NK models, as complementary to Auclert et al., 2018, Bilbiie, 2019, Gabaix, 2020, Acharya and Dogra, 2020, Ravn and Sterk, 2021 and Michaillat and Saez (2021). (4), Piergallini, 2006, Annicchiarico et al., 2008 assume real money balance enters the utility function directly in a stochastic finite-horizon discrete time version of the Yaari (1965)–Blanchard (1985) overlapping generations model, while we assume households derive utility from holding overall wealth in a infinitely-lived representative agent framework. With money in the utility and finitely-lived individuals as for Piergallini, 2006, Annicchiarico et al., 2008, the dynamic IS equation remains not discounting, but the LM equation is no longer recursive as real money presents directly in the aggregate demand equation. By contrast, we allow for overall wealth into the utility function, which leads to ==== dynamic IS equation even in a infinitely-lived representative agent framework. It is the discounting to dynamic IS equation that creates the demand channel of Taylor principle and equilibrium determinacy.====The remainder of the paper proceeds as follows: In Section 2, we illustrate the standard Taylor principle and determinacy conditions. The demand channel of the Taylor principle and the condition to ensure determinacy in the NK model with wealth in the utility function are examined in Section 3. Two extensions are conducted in Section 4. Section 5 allows for public debt dynamics. Concluding remarks are offered in Section 6.","Wealth in utility, the Taylor principle and determinacy",https://www.sciencedirect.com/science/article/pii/S0164070423000253,26 April 2023,2023,Research Article,6.0
"Apeti Ablam Estel,Combes Jean-Louis,Minea Alexandru","Université Clermont Auvergne, Université d’Orléans, LEO, 45067, Orléans, France,Research Institute of the University of Bucharest, University of Bucharest, Romania,Department of Economics, Carleton University, Canada","Received 14 June 2022, Revised 14 January 2023, Accepted 15 March 2023, Available online 5 April 2023, Version of Record 17 April 2023.",https://doi.org/10.1016/j.jmacro.2023.103523,Cited by (1),"An important literature shows that inflation targeting (IT) adoption improves fiscal discipline. Our impact assessment analysis performed in a large sample of 89 developing countries over three decades shows that this favorable impact covers a composition effect: IT adoption is found to reduce more current expenditure compared with public investment in IT countries relative to non-IT countries. This finding is robust to various alternative specification, related to the structure of the sample, the measurement of the IT regime, or the estimation method. Consequently, aside from its acknowledged benefits for ==== goals, IT appears as an efficient tool to strengthen fiscal policy in developing countries towards lower and more productive public expenditure.","Adopted first by the New Zealand in 1990, inflation targeting (IT) has established as a mainstream framework for conducting monetary policy. Five key elements are usually characterizing an IT monetary framework: the public announcement of medium-term numerical targets for inflation; an institutional commitment to price stability as the primary goal of monetary policy (to which other goals are subordinated); an information-inclusive strategy in which many variables, and not just monetary aggregates or the exchange rate, are used for deciding the setting of policy instruments; increased transparency of the monetary-policy strategy through communication with the public and the markets about the plans, objectives, and decisions of the monetary authorities; and increased accountability of the central bank for attaining its inflation objectives (see e.g. Mishkin, 2000; Minea and Tapsoba, 2014).====The literature on the macroeconomic effects of IT can be organized into two main blocks. The first focuses on the effect of IT on monetary policy efficiency. Based on a sample of 52 developing countries, Lin and Ye (2009) show that IT is an effective monetary policy tool to reduce inflation and its volatility, consistent with the conclusions of e.g. Castellani and Debrun (2001); Ball and Sheridan (2004); Vega and Winkelried (2005); Gonçalves and Salles (2008); Lee (2011); or Samarina et al. (2014). Adding to this evidence, other studies point out to a favorable effect of IT on e.g. the exchange rate level and its volatility, financial dollarization, interest rates, or monetary policy credibility (see e.g. Pétursson, 2005; Batini and Laxton, 2006; de Mendonça and de Guimaraes e Souza, 2009; Lin, 2010; Lin and Ye, 2013).====The second strand of literature analyzes the effect of IT on fiscal discipline (see e.g. Minea and Villieu, 2009a; Lucotte, 2012; Minea and Tapsoba, 2014; Kadria and Aissa, 2016; Combes et al., 2018; Ardakani et al., 2018; Minea et al., 2021). It appears that IT improves countries’ fiscal positions by lowering debt and deficits.==== However, despite this fruitful literature, to the best of our knowledge the impact of IT on the level and the composition of public expenditure remains unexplored. This is surprising, given that these studies insist on the large extent to which fiscal policy is dependent on monetary regimes. As a result, monetary discipline arising from IT might affect government’s fiscal policy behavior in terms of public expenditure.====The goal of this paper is to evaluate the effect of IT on the (level and the) composition of public expenditure in a large sample of 89 developing economies over the period 1985–2016. We tackle the crucial issue of endogeneity in the adoption of an IT framework using an impact assessment analysis. We reveal that IT adoption modifies governments’ fiscal behavior in IT developing countries relative to non-IT ones. Aside from significantly reducing public expenditure, IT adoption triggers a composition effect: the contraction of current expenditure is found to be stronger than that of public investment. Robust to various alternative specifications, related to the vector of covariates, the structure of the sample, the measurement of IT adoption, or the estimation method, this finding still holds when accounting for possible heterogeneity in the impact of IT on the composition of public expenditure. The policy message of our analysis is that, in addition to its various benefits for monetary policy goals, IT may work as an efficient tool to strengthen fiscal policy in developing countries towards lower and more productive public expenditure.====The paper is organized as follows. Section 2 builds our hypothesis of a composition effect of IT adoption using existing theoretical and empirical studies, Section 3 presents the data, Section 4 details the methodology, Section 5 illustrates our main results, Section 6 assesses their robustness and explores the possible heterogeneity in the effect of inflation targeting on current expenditure and public investment, and Section 7 delivers some concluding remarks.",Inflation targeting and the composition of public expenditure: Evidence from developing countries,https://www.sciencedirect.com/science/article/pii/S016407042300023X,5 April 2023,2023,Research Article,7.0
"Caloia Francesco G.,Parlevliet Jante,Mastrogiacomo Mauro","De Nederlandsche Bank, the Netherlands,Vrije Universiteit Amsterdam, School of Business and Economics, De Boelelaan 1105, 1081HV, the Netherlands,Network for Studies on Pensions, Aging and Retirement, the Netherlands,Universiteit van Amsterdam, the Netherlands,CPB Netherlands Bureau for Economic Policy, the Netherlands","Received 16 September 2022, Revised 19 February 2023, Accepted 3 March 2023, Available online 12 March 2023, Version of Record 23 March 2023.",https://doi.org/10.1016/j.jmacro.2023.103521,Cited by (0)," is dominant and contract staggering is relatively pervasive. The main result is that staggered wage setting has no real effect on employment. We find significant employment losses only in sectors covered by contracts with much longer durations than those normally assumed in ==== featuring staggered wages. Instead, we show that firms adjust labor costs by curbing other pay components such as bonuses and benefits and incidental pay. The overall result supports the idea that wage rigidities are not the main source of ====.","This paper empirically investigates the effect of a particular source of nominal rigidity in labor markets, namely the staggered way in which wages adjust to changes in the economic environment. Wages are staggered when contract decisions are taken at different points in time and these decisions are valid for a certain number of periods (Taylor, 1979).==== At any point in time, only some industries reset their wages, and during the bargaining process employers and unions are likely to respond to events which happened in the previous period. Also, as the bargained wage is valid for a number of periods, unions form inflation expectations for the contract period in order to insure workers against real wage losses. Expectations on productivity and profitability are likely to be relevant as well, as unions would make sure that wages grow in line with industry profits. Sudden changes in expectations can therefore induce wage differentials across sectors bargaining in nearby months.====Staggered wages, also known as Taylor contracts, have been used in macroeconomics to understand the persistence of inflation dynamics and the transmission of monetary policy (Olivei and Tenreyro (2007, 2010), Björklund et al. (2019).==== In the most recent macroeconomic literature, staggered wages are the most common method of incorporating labor market rigidities in quantitative macroeconomic models (Taylor, 2016), and represent a nominal friction that helps these models in reproducing the inflation and output dynamics observed in the data (Christiano et al., 2005; Hall, 2005; Smets and Wouters, 2007; Gertler and Trigari, 2009). Models for the labor market have instead questioned whether wage rigidities are ultimately responsible for the fluctuation of unemployment (Shimer 2005; Pissarides, 2009; Kudlyak, 2014).====Empirical studies have well documented that wages are indeed reset infrequently. On the basis of a large-scale survey among European firms, it has been documented that in the majority of firms wage resets take place only once a year or even less frequently (Fabiani et al., 2010). On the basis of the same survey, it has also been documented that firms adjust their wage bill through other means, e.g. by slowing promotions, cutting bonuses, adjusting working hours or choosing cheaper hires to replace workers who leave the firm (Babecký et al., 2012, 2019). Recent research has also confirmed that base wage cuts are rare in Great Britain (Schaefer and Singleton, 2022) and the United States (Grigsby et al., 2021), while the adjustment of flexible pay is more pervasive. Likewise, in case firms offer worker insurance within the firm, they can forego profits to absorb shocks (Guisoet al., 2005). If the scope of such adjustments is large enough, the employment response to staggered wage setting may be more muted.====Differently from most of the existing literature, this paper takes a microeconomic perspective to empirically assess how wage rigidities due to contract staggering affect firms’ wage versus employment labor cost adjustments. The analysis focuses on the Netherlands, where wage resets are established in collective labor agreements (====, CAO hereafter) that apply at the sector level, but also at the company level for larger firms. The case of the Netherlands is particularly interesting because nominal rigidities due to contract staggering are very pervasive, due to the large coverage of collective agreements and the relatively long duration of contracts. We use detailed data on professional forecasts specific to the Dutch economy to identify the timing of a big unanticipated macroeconomic shock that created a substantial differential in wage growth among CAOs negotiated in nearby months. We identify this shock in October 2008, after which average wage growth agreed upon in CAOs fell from 3.5 percent to 1 percent in two quarters. We then use a large matched employers-employees dataset obtained from the combination of several administrative data sources to show the effect of contract staggering on a battery of firm-level labor market outcomes. In particular, we consider firms’ responses on total and flexible (i.e. with a temporary contract) employment levels and the number of new hires in each firm and adjustments to various pay components (the base wage, bonuses, non-wage benefits, and overtime hours paid to the workers), to understand which adjustment margins firms use the most.====Identifying the effect of rigidities arising from collective bargaining systems in micro-econometric analysis is challenging. One particular concern is that the period in which the parties bargain over wage resets may be endogenous, as uncertainty may cause both parties to delay the renegotiation process (Danziger and Neuman, 2005). To address this, we exploit a particular feature of the Dutch context to generate plausible exogenous variation around the timing of the shock: in each sector or company in our data, the start date of the subsequent CAO always coincides with the expiry date of the previous CAO. By exploiting variation in the staggered and pre-determined start dates around the aggregate shock, our identification strategy ensures that the start date of each CAO is independent on the uncertainty generated by the shock. This allows us to estimate the causal effect of the wage rigidities created by contract staggering on the firms and the workers who signed their CAO before the shock and did not anticipate it.====The paper closest to our own research is Díez-Catalán and Villanueva (2014) who study the effect of widespread nominal wage rigidity in Spain, where contract staggering is caused by the presence of province-sector level agreements with a duration of typically two years or even longer. They examine workers’ total wage growth and transition to unemployment after the Lehman Brothers default, which was an external shock to the Spanish economy, and find that this nominal rigidity increases the probability of transition to unemployment, especially among minimum wage workers. Differently from them, in this paper we investigate firms’ labor cost adjustments after the start of the 2009 recession in the Netherlands. We improve on this study along two main dimensions: first, using data on collective agreements in place in the Netherlands in the period 2006–2012, we provide evidence on how bargaining has changed during the crisis period. Then, using an identification strategy based on CAOs pre-determined start dates (instead of the endogenous agreement dates), we identify the causal effect of nominal rigidities on firms’ labor market outcomes.====Our main result is a precisely estimated zero effect of contract staggering on employment. Firms that agreed on high wage growth before the recession did not cut employment levels more than those firms that were able to witness the shock and were able to agree on lower contractual wage growth. Instead, we show that non-anticipating firms have been able to adjust their wage bill by cutting bonuses and benefits and that they seem to have been able to curb the so-called incidental pay component, that we do not observe directly.====This result contrasts with macroeconomic models such as those of Hall (2005) and Gertler and Trigari (2009), in which staggered wage bargaining leads to more volatile responses to an aggregate shock of all labor market indicators, including employment. Our results support the idea that unemployment fluctuations are not the result of wage rigidities (Pissarides, 2009). More precisely, our results are in line with Björklund et al. (2019) who show that, despite persistent real effects on inflation and output, the (un)employment response to monetary policy shocks is not significant in Sweden, a country with a similar degree of employment protection, contract durations and frequencies of wage adjustments. In line with their results, we find significant employment losses only in sectors covered by contracts with duration larger than thirty months, although this effect is not large enough to affect our overall results. Such durations are much higher than what is normally assumed in macroeconomic models for the U.S. economy, such as Christiano et al. (2005), Smets and Wouters (2007), Gertler and Trigari (2009), where wage resets often occur every three to four quarters. Also, our results contribute to the literature of wage rigidities by providing evidence of forms of flexibility in the wage of existing employees, and complements the existing evidence of substantial sensitivity of the wage of new hires to macroeconomic fluctuations (Haefke et al., 2013).====Our results have important implications for the role of collective bargaining. Out of concern for job losses, international organizations have often advocated to reduce nominal rigidities in wage setting, for instance by moving bargaining to the firm level. Yet, such reforms have proven contentious and have raised concerns about the overreliance on wage moderation, the erosion of collective bargaining and the consequences on wage inequality (see Dustmann et al., 2009; Blanchard et al., 2014; Boeri and Jimeno, 2015; Addison et al., 2017). Our analysis shows that the nominal rigidities that often result from collective bargaining do not necessarily come at the cost of employment, and suggests that building more room for discretionary pay components in collective agreements may be a way to alleviate such rigidities.====The paper proceeds as follows. Section 2 discusses the institutional framework. Section 3 describes the data and the descriptive statistics. Section 4 presents the identification strategy being used and the results of the empirical analysis. In Section 5 we present robustness checks and examine heterogeneity in contract duration. Section 6 concludes.","Staggered wages, unanticipated shocks and firms’ adjustments",https://www.sciencedirect.com/science/article/pii/S0164070423000216,12 March 2023,2023,Research Article,8.0
Rainone Edoardo,"Bank of Italy, Directorate General of Economics, Research and Statistics, Via Nazionale 91, Rome, Italy","Received 20 July 2022, Revised 14 February 2023, Accepted 28 February 2023, Available online 10 March 2023, Version of Record 23 March 2023.",https://doi.org/10.1016/j.jmacro.2023.103520,Cited by (0),"This paper analyzes the relationship between tax evasion and the demand for cash by studying the effects of two measures to fight evasion: accessing taxpayers’ bank data and imposing thresholds for cash payments. We study the effects of these policies in Italy, where visibility of bank data and cash thresholds were recently increased. We show that both significantly affected cash holdings, which grew by about 1.5 percent of the GDP. Using unique high frequency data on cash operations and exploiting regional heterogeneity in tax evasion propensity, we find that accessing bank data pushes regions with higher propensity to evade ==== to convert more deposits into cash. On the contrary, higher cash thresholds do not increase cash holdings more in these regions. We rationalize the findings with a simple model of tax evasion and payment choices, where cash and deposits have different degrees of privacy.","Despite the increasing use of electronic means of payment, cash is still widely used.==== If compared to other payment instruments, a unique feature of cash is its complete untraceability, which is highly valuable for hiding taxable transactions and wealth.==== While the role of tax evasion in the demand for cash has been widely explored, studies on the effects of measures to fight tax evasion on cash demand are scant.====Anonymity in payments is a feature inherent to the use of cash: it provides a greater degree of privacy than other means of payment. On the contrary, the ability of different types of money to protect privacy varies, given that any payment system can shield the payee's entire information set or a subset of that information. In fact, in account-based networks, the payer's identity must be identified (Brunnermeier et al., 2021). If individuals are sensible to the number of observers, a demand for payment privacy and anonymity emerges (Borgonovo et al., 2021). In this context, the demand for privacy has two main sources: (i) a demand by individuals involved in illegal transactions, who try to reduce the probability of being incriminated (Masciandaro, 1999, Ardizzi et al., 2014b), and (ii) a demand by agents simply in search for protection from external scrutiny, not necessarily avoiding legal sanctions (Kahn, 2018). Indeed, it partially explains the emergence of new payment architectures and cryptographic procedures used to protect privacy, and the fact that they are viewed as close substitutes of cash for illegal transactions (Hendrickson and Luther, 2022). The literature on money and privacy has mainly focused on these aspects, that is, on improvement in privacy protection that leads to efficiency gains in the demand for money (Kahn et al., 2005), and recently on the introduction of central bank digital currency (Garratt and van Oordt, 2021), which can manage different degrees of financial privacy or different degrees of anonymity (Keister and Sanches, 2021). In this paper, we study tax evasion policies that interact with and alter the relative level of privacy between cash and deposits.====Reducing tax evasion is a priority for many governments,==== especially after the surge of public debts following the recent financial and pandemic crises. The government can fight tax evasion in several ways. The literature has identified two macro-categories. The first is based on ==== and sees the government as a provider of moral and monetary incentives to comply with tax payments.==== The second follows a ====, by which the government implements restrictions, sanctions, and strict monitoring to increase tax revenues. Regarding the latter category, while the literature has mainly focused on the elasticity of tax compliance to the level of income, tax rate, audit probability and penalty rate,==== our paper instead focuses on an understudied empirical issue (Slemrod, 2019): the effect on cash and deposits demand of tax enforcement policies.====The first policy consists in accessing taxpayers’ bank data. The literature has commonly assumed that in the event that an audit occurs, the true income is discovered without error (Chander and Wilde, 1998), but, actually, it is not the case. The taxable income expected by the government is an unobservable random variable and it has to be inferred somehow.==== A growing literature argues that verifying taxpayer reports against third-party information is critical for tax collection.==== In order to control the adequacy of tax compliance, the government can request access to taxpayers’ bank data and infer their true taxable income.==== Such policy obliges banks to report data on balances and transactions of their clients. The second policy is the direct limitation of cash usage for transactions. The government can set a threshold and forbid cash transactions above a certain value, a ====. By limiting cash transactions, and accessing bank data, the government aims to reduce hidden taxable exchanges. Given the recent diffusion of cash thresholds in Europe and elsewhere and the growing possibilities related to effective financial monitoring offered by new information and big data analytics technologies, it seems important to understand the consequences of such policies for the demand of cash and deposits.====We study the effects of these policies in Italy, a country where tax evasion and cash usage are widespread==== and where visibility of bank data to the tax revenue agency and cash thresholds have been recently increased. We estimate that accessing detailed bank data and increasing the cash threshold from 1,000 to 3,000 euro have implied an increase of cash holdings of about 1.5 percent of GDP. Using unique high frequency data on cash operations by banknote denomination, we can identify sharply the effects of changes in cash thresholds and in bank data visibility, as never done in other studies. Exploiting heterogeneity in tax evasion propensity across Italian regions, we find that making bank data visible has a higher impact on cash demand in regions with higher propensity to evade. On the contrary, higher cash thresholds do not increase cash holdings more in these regions. Using a simple conceptual framework of tax evasion and payment choices, we show that bank data visibility affects mostly the ==== for cash to hide funds from the government's eye. Indeed, taxpayers can make offsetting adjustments (Kane, 1981): when monitoring is improved for non-cash types of transactions and wealth, cash is there to take their place. On the contrary, the ==== by agents adapting their cash inventories (Baumol, 1952; Tobin, 1956) seems to drive the reaction of cash holdings triggered by variations of thresholds. It is also shown that both policies involve the demand for large banknotes, which are at the center of an intense policy debate on potential limitations and elimination of cash (Gesell, 1916; Eisler, 1932; Buiter, 2009; Goodfriend, 2000; Rogoff, 2014, 2017; Humphrey, 2016; McAndrews, 2020; Lastrapes, 2018; Garin et al. 2021).====Our evidence contributes to four branches of the literature. First, concerning monetary economics and banking, we identify a new driver for currency in circulation and show that it is a very important predictor of currency in circulation and thus M0. We show that the supply of cash by the central bank and deposits by commercial banks or other financial institutions must take into account the actions of the government to monitor private money balances or limit public money payments. In particular, we estimate that about 2 percent of deposits are converted into cash, shrinking retail funding and potentially credit growth for commercial banks. Second, our evidence highlights the higher value attached to the anonymity provided by cash when advancements in information technology allow to manage huge big financial data, which is relevant for the economics of privacy.==== Third, we inform about the effects of tax evasion policies and relate to the recent studies on the interaction between regulations about cash and cashless instruments and illegal activities.==== Finally, from a methodological perspective, the paper shows that granular data on cash operations, combined with high frequency econometrics, can be used effectively to uncover new relevant determinants of cash in circulation, and better understand the behavior of taxpayers, outperforming commonly used low frequency-aggregate panel data estimates.====The rest of the paper is organized as follows. Section 2 details the Italian institutional background and the measures adopted by the government. Section 3 provides a simple conceptual framework that connects tax evasion and payment choices when privacy of cash and cashless instruments differ and can be altered. Section 4 describes the empirical results on the effects of bank data visibility and cash thresholds on the demand for cash. Section 5 investigates the effects of the policies across regions with different tax evasion propensity. Section 6 concludes and discusses policy issues.",Tax evasion policies and the demand for cash,https://www.sciencedirect.com/science/article/pii/S0164070423000204,10 March 2023,2023,Research Article,9.0
"Hu Mei-Ying,Lu You-Xun,Lai Ching-chong","Department of Industrial Economics Strategy Research, Taishin Sercurities Investment Advisory, Taiwan,School of Business, Nanjing University of Information Science and Technology, China,Yangtze River Institute of International Digital Trade Innovation and Development, Nanjing University of Information Science and Technology, China,Institute of Economics, Academia Sinica, Taiwan,Department of Economics, National Cheng Chi University, Taiwan,Institute of Economics, National Sun Yat-Sen University, Taiwan,Department of Economics, National Taipei University, Taiwan","Received 28 July 2022, Revised 21 February 2023, Accepted 25 February 2023, Available online 2 March 2023, Version of Record 4 March 2023.",https://doi.org/10.1016/j.jmacro.2023.103519,Cited by (0),"Due to the lags in commercialization, the effective life of a ","Based on the agreement on Trade-Related Aspects of Intellectual Property Rights (TRIPS) signed by the member countries of the World Trade Organization (WTO), the patent protection granted by the member countries is required to be valid for at least 20 years from the date of application.==== However, a patent's effective life is usually less than its statutory term. One reason is that a patent is subject to a lengthy patent review after its application. For example, according to the United States Patent and Trademark Office (USPTO), the average review time for patent applications is approximately 40.7 months. The prolonged patent review reduces the effective patent term, which in turn discourages firms from investing in R&D.====In the pharmaceutical industry, new drugs need to complete lengthy clinical trials for safety and efficacy. After completing clinical trials, in the United States, new drugs still have to be reviewed and approved by the U.S. Food and Drug Administration (FDA) before they are allowed to enter the market.==== Using a sample of US pharmaceutical industries, Sloan and Hsieh (2017) find that it takes an average of 12–15 years for a new drug to complete clinical trials and be approved by the FDA. In addition, Philipson and Sun (2008) suggest that the probability of a new drug being successfully developed and brought to market is only a very low 8%. The lengthy clinical trials and the low survival rate jointly reduce firms’ R&D investment in developing new drugs.====Extending patent terms is an important policy instrument for the government to intervene in pharmaceutical R&D. In 1984, the U.S. Congress passed the Drug Price Competition and Patent Term Restoration Act (also known as the Hatch-Waxman Act) to encourage pharmaceutical companies to invest in new drug development. Specifically, the Act allows for a patent term extension of up to five years. Moreover, the Act also grants a data exclusivity period to protect the rights of new drugs after FDA approval.==== The literature on pharmaceutical industries also commonly suggests that extending patent terms is effective in stimulating drug development; see Grabowski and Kyle (2007), Higgins and Graham (2009), Panattoni (2011), and Budish et al. (2015; 2016). For instance, Budish et al. (2015) find that for pharmaceutical industries, extending the patent term by one year can significantly increase R&D investment in pharmaceuticals by 7–24%.====Following Budish et al. (2015), we refer to the lags in the commercialization of a drug resulting from clinical trials and FDA review as ====.==== In this study, we introduce commercialization lags into the quality-ladder model and analyze how patent extensions affect innovation in the pharmaceutical industry. Specifically, the representative household consumes two types of goods: homogeneous goods and pharmaceuticals. The quality of homogeneous goods is invariably constant. However, in the economy, there are a number of R&D firms investing in the development of better drugs. In the presence of commercialization lags, when an R&D firm successfully develops a new drug, it still needs to conduct clinical trials on the drug and apply for a patent from the FDA, thereby causing the effective life of the patent to be shorter than its statutory term. To motivate the firm to invest in R&D in pharmaceutical industries, the government will implement the policy of extending the patent term in response.====Within this framework, extending patent terms makes patents more valuable and thus increases the value of assets owned by the household. As a result, the household's asset income rises and it consumes more homogeneous goods. However, extending patent terms has two opposite effects on the consumption of pharmaceuticals. On the one hand, patent extensions encourage firms to employ more labor for R&D and promote technological progress in pharmaceutical industries. On the other hand, the increase in R&D labor caused by patent extensions leads to a decline in the labor input devoted to pharmaceutical production. Therefore, in the presence of commercialization lags, the effect of patent term extensions on pharmaceutical consumption is ambiguous. Together with the positive effect on the consumption of homogeneous goods, extending patent terms may generate an inverted-U effect on social welfare. Furthermore, we find that when there exists an optimal level of patent term extensions, this optimal level rises with the length of commercialization lags. By contrast, if clinical trials require more labor input, i.e., commercialization lags are more costly, the optimal level of patent term extensions decreases.====Finally, we calibrate the model to provide some numerical analysis. Under our calibrated values, the extension of patent protection does cause an inverted-U effect on social welfare. In addition, we quantitatively analyze how patent breadth – another important patent policy instrument – affects the optimal patent extension. Our results show that increasing patent breadth significantly decreases the optimal patent extension. Intuitively, a higher patent breadth strengthens the protection for an invention against imitations, thereby increasing the market power of monopolistic producers. To correct the distortions caused by imperfect competition, the government has an incentive to set a shorter level of patent term extensions to reduce the effective patent terms.",Patent term extensions and commercialization lags in the pharmaceutical industry: A growth-theoretic analysis,https://www.sciencedirect.com/science/article/pii/S0164070423000198,2 March 2023,2023,Research Article,10.0
"Arin K. Peren,Devereux Kevin,Mazur Mieszko","Zayed University, United Arab Emirates,CAMA, Australia,Peking University, China,ESSCA School of Management, 55 Quai Alphonse le Gallo, 92513 Boulogne-Billancourt, France","Received 2 July 2022, Revised 28 November 2022, Accepted 13 February 2023, Available online 21 February 2023, Version of Record 24 February 2023.",https://doi.org/10.1016/j.jmacro.2023.103517,Cited by (0),We investigate the firm-level investment response to unanticipated narrative shocks to average personal and ,"Fiscal policy continues to receive attention both from policymakers and academics due to increasing income inequality, social problems, and slowing global economic growth in the wake of the COVID-19 global pandemic. Due to nationwide lockdowns and decreased demand, many large companies around the world filed for insolvency; in uncertain times, it is imperative to understand how macroeconomic policy affects firm-level decisions.====In this study, we combine narrative macroeconomic data on unanticipated tax shocks with firm-level microdata to investigate the heterogeneous effects of tax policy on the investment behavior of firms. Firm-level investment is not only an important cause of business cycles but also the most volatile component of output; therefore it must crucially inform macroeconomic policy-making decisions. We contribute to the relevant literature in multiple ways. First, and unlike the previous literature (Eskandari and Zamanian, 2020), we investigate the effect of both corporate and personal narratively-identified tax shocks on firm-level investment behavior. Our second contribution lies in the fact that by using local projection (LP) method by Jordà (2005), we split the sample according to firms characteristics and accompanying macroeconomic environment within a unified framework. As we control for heterogeneous effects over the business cycle, the accompanying monetary policy regime, and over firm-level characteristics simultaneously, we are able to have a more complete picture of the transmission mechanisms.====Our results show that firm-level responses to corporate tax innovations are significantly negative over a horizon of two years. Personal tax shocks, on the contrary, do not generate significant responses. There is some evidence of positive personal tax multipliers during monetary contractions, which is consistent with the recent finance literature arguing that dividends and investments may be substitutes (Wu, 2018) and that the amount of the dividend will be higher, the higher is the tax advantage on the dividend (Von Eije and Megginson, 2008). We also show that the magnitude of small firms’ response to corporate tax innovations is greater than that of large firms. We argue that this result is driven by the lack of access to external financing by small firms. This intuition is further supported by the fact that a similar result prevails when the accompanying monetary policy is ‘tight’, consistent with Jones and Olson (2014), among others. Furthermore, we also show that the constrained firms drive the negative overall response of investment to corporate tax shocks, as evidenced using the financial constraints measure of DeAngelo et al. (2002), and further supported by splitting the sample into period of positive and negative output gaps.====The remainder of the paper is organized as follows: Section 2 discusses previous work, Section 3 reviews data and methodology, Section 4 presents empirical results, Section 5 robustness checks, and Section 6 concludes.",Taxes and firm investment,https://www.sciencedirect.com/science/article/pii/S0164070423000174,21 February 2023,2023,Research Article,11.0
"Bellocchi Alessandro,Travaglini Giuseppe","University of Urbino Carlo Bo, Department of Economics, Society, Politics (DESP), Urbino 61029, Italy","Received 11 August 2022, Revised 31 December 2022, Accepted 14 February 2023, Available online 19 February 2023, Version of Record 24 February 2023.",https://doi.org/10.1016/j.jmacro.2023.103518,Cited by (1),") is crucial to explain the evolution of the labor share. The decline in labor share observed worldwide can be explained by capital accumulation if ====. However, empirical evidence on the value of ==== is mixed. To shed light on this issue, we employ a Variable Elasticity of Substitution (VES) production function where ==== is an ==== under ====. We test the prediction of the model by means of simulations. Mainly, we find that capital deepening, markup and technological change explain a significant part of the observed decline in labor shares. The results suggest ==== between labor and capital in all the countries except the United States.","The constancy of the labor share is a crucial feature of macroeconomic models, “====” (Karabarbounis and Neiman, 2014, p.61). Recently, the labor share has been on a downward trend in many countries. Although debate remains on the extent of this phenomenon due to technical considerations such as the treatment of capital depreciation, indirect taxes, and housing (Rognlie, 2016; Bridgman, 2018; Cette et al., 2019), the apportionment of mixed income (Cette et al., 2019; Gutiérrez and Piton, 2020), and intangible capital (Koh et al., 2020), the consensus is that the fall has been significant.====Ricardo referred to the functional distribution of income as “====” (Ricardo, 1891). Thereafter, the topic was debated in economic theory until the work of Kaldor's (1961), when its “stylized facts” were accepted by economists. For Kaldor, in a ==== equilibrium, real wages end up matching productivity gains, with the result that, in the long run, the labor share can only be constant. Therefore, issues regarding the macro-division of income were marginalized and the focus of economists shifted from ==== to ====.====For more than forty years, the labor share in industrialized countries never ventured far from 68% of national income, remaining stable through expansions, recessions, high and low inflation, and the transition from production-based to service-based economies. Keynes (1939, p.48) referred to this fact as “====”. Consequently, for a long period of time, factor shares were considered constant, which led to a reduction in the role of income distribution in academic discussions (Atkinson, 2009).====The recent worldwide decline in labor share and growing income inequality have brought new life to this field of research. Blanchard (1997) was the first to notice a decline in the share of income earmarked for the remuneration of labor in several European countries. Karabarbounis and Neiman (2014) documented that the global labor share has declined significantly since the early 1980s, with the decline occurring within most countries and industries. The downward trend was later confirmed by Piketty and Zucman (2014), who also noted an increase in capital shares.====The CES production function, with constant returns to scale and competitive markets, predicts a stable relationship between the labor share and capital accumulation, with the value of the (constant) ==== between capital and labor (====) which determines (by being greater or lower than one) a decrease (increase) in ==== as the ==== in the economy increases. Empirical evidence on the subject, however, is conflicting. For example, it was observed that capital-output ratios and labor shares increased together in most advanced economies before 2000, suggesting a ==== lower than one (Bentolila and Saint-Paul, 2003; Saumik, 2019). But their movement had contrasting signs since then (Table A1). Similarly, Karabarbounis and Neiman (2014) and Piketty and Zucman (2014) estimate ==== to be greater than one, in contrast to a large micro-literature that, exploiting time series and variation across firms, documents an average ==== of less than one (Chirinko and Mallick, 2017; Oberfield and Raval, 2021).==== In this paper we focus on this puzzle.====We employ a Variable Elasticity of Substitution (VES) production function that allows the elasticity of substitution to ====. Notably, this may reconcile different estimates of ==== with the observed evolution of the capital-output ratio. In fact, with a VES production function, we can observe a declining labor share even when ==== is lower than one. In addition, a VES can reduce the upward bias of ==== resulting from the estimation of a CES, where the latter “====” (Kazi, 1980, p.169). We test the prediction of the model by means of simulations. Using macro-data for six industrialized OECD countries over a forty-year period we estimate the structural parameters of the VES production function and calibrate the model on the actual level of the variables. We find that changes in wages and capital intensity, profit margins and technological change explain a significant part of the decline in labor share over the past forty years. Notably, the results suggest ==== between labor and capital in all the countries considered except the United States (U.S.). Considering these results, we also draw some important ====. Whether policymakers should pursue the goal of capital deepening depends on how they evaluate the benefits of a higher ==== in the form of ==== and ==== against the loss inflicted on the economy due to the exacerbation of ====.",Can variable elasticity of substitution explain changes in labor shares?,https://www.sciencedirect.com/science/article/pii/S0164070423000186,19 February 2023,2023,Research Article,12.0
Pusateri Nic,"Sacred Heart University, United States of America","Received 2 August 2022, Revised 30 December 2022, Accepted 27 January 2023, Available online 13 February 2023, Version of Record 27 February 2023.",https://doi.org/10.1016/j.jmacro.2023.103505,Cited by (0),"The jobless recovery enigma remains largely unsolved. As a special case of broader unemployment, the term “jobless recovery” describes an economic recovery where output recovers—and even expands—yet employment growth remains anemic. While the effects of these prolonged recoveries are significant—from increased crime to a lifetime reduction in wages—they are not well understood. Building on the insights of labor market matching models that incorporate heterogeneity among workers, this paper sheds light on jobless recoveries, developing a first-of-its-kind index of human capital heterogeneity for the unemployed, and testing that index using of a Structural Vector ====. I demonstrate that the extent to which unemployed human capital is heterogeneous and specific, rather than homogeneous and general, plays a key and under-appreciated role in the labor market; increases in human capital heterogeneity can account for between one-quarter to three-quarters of the joblessness of the past three recoveries in the pre-COVID era.","The jobless recovery enigma remains largely unsolved. As a special case of broader unemployment, jobless recoveries are fairly new. Coined in 1991 by Nicholas Perna, the term “jobless recovery” has been used to describe each of the last three traditional recoveries (1991, 2001, and 2009), where output recovered—and even expanded—yet employment growth remained anemic (Nasar, 1991, Groshen and Potter, 2003, Schmitt-Grohé and Uribe, 2012).==== While the effects of these prolonged recoveries are large, they are not well understood.====One effect of these prolonged recoveries is increased crime; Andresen (2013) finds a robust relationship between unemployment and criminal activity. Another detrimental effect of jobless recoveries is a significant reduction in lifetime earnings for labor market entrants; as potential entrants seek jobs during a jobless era, their reservation wages drop, and they are placed on a lower lifetime wage path (Kahn, 2010). Additionally, prolonged jobless eras have significant policy implications for a wide range of policy issues—from unemployment benefits and retraining programs to fiscal and monetary stimulus.====What has caused these labor market changes? Why have recent employment recoveries lagged behind output? Proposed explanations for these slow, or jobless, recoveries include the increase in modern technology (Goos et al., 2014, Michaels et al., 2014, Graetz and Michaels, 2017), sectoral labor reallocation (Groshen and Potter, 2003, Stock and Watson, 2003, Chen et al., 2011, Burger and Schwartz, 2015, Panovska, 2017), an increase in just-in-time employment (Aaronson et al., 2004), a decrease in labor mobility (Frey, 2009), an increase in globalization and offshoring (Waddle, 2019), and even changes in educational job requirements (Carnevale et al., 2013). Although each of these varied explanations offers a partial answer, they remain incomplete and contested.====To address this puzzle, I employ the literature on labor market frictions. Frictions in the matching market for labor have long been incorporated into standard models (Hosios, 1990, Hall, 1999, Mortensen and Pissarides, 1999). Categorizing frictions as either amplifying or contributing to the persistence of shocks, Hall (1999) concludes that labor market frictions can help to explain how seemingly “small impulses” generate significant contractions (Hall, 1999 p. 41). While much work improved our understanding of the effects of these frictions, the extent to which worker heterogeneity interacts with and magnifies them is less well understood. Pries (2008), Bils et al. (2011), Epstein (2012), Ravenna and Walsh, 2012, Ravenna and Walsh, 2014, Chassamboulli (2013), Mueller (2017), and Gregory et al. (2020) have all demonstrated the importance of worker heterogeneity in labor market models. While conclusions have differed (from Pries (2008) suggesting that worker heterogeneity is critical to explaining slow recoveries to Bils et al. (2011) suggesting that heterogeneity reduces separations and unemployment), a strong consensus has emerged that worker heterogeneity is important to the understanding of labor market frictions.====In addition to labor market frictions, the literature on “capital-based macroeconomics” offers valuable insight into the jobless recovery enigma. Those who work within capital-based macro have long held that physical capital is heterogeneous and multi-specific. They contend that capital should not be considered as a homogeneous “K”—as is often the case in macroeconomic modeling (Solow, 1957)—but rather as a structure of production where the reallocation of capital is costly due to its specificity and heterogeneity (Böhm-Bawerk, 1890, Böhm-Bawerk, 1891, Hayek, 1950, Garrison, 2002). This insight has only recently been applied to human (rather than physical) capital and has yet to be empirically examined (Boettke and Luther, 2012, Burns, 2018).====Building on the insights of labor market frictions and capital-based macroeconomics, I investigate the question of jobless recoveries and suggest that the extent to which unemployed human capital is heterogeneous and specific, rather than homogeneous and general, plays a key, and underappreciated, role in the labor market frictions that drive jobless recoveries. My suggestion is not entirely unique. Using a basic labor-market matching model to analyze productivity heterogeneity among the unemployed, Pries (2008, p. 675) finds that, “relatively small cyclical changes in the composition of unemployment can have a significant effect on firms’ vacancy creation decisions, and thus on job-finding rates and the unemployment rate.” Similarly, Ravenna and Walsh (2012) report that increases in worker heterogeneity significantly contribute to the slow pace of recoveries. Further, Grigsby (2022) finds that the employment dynamics of the Great Recession can be explained by skill heterogeneity in the absence of frictions, highlighting the importance of skill heterogeneity. While not based on human capital heterogeneity specifically, the current labor market matching literature provides a strong theoretical basis for my empirical analysis.====To empirically demonstrate the importance of human capital heterogeneity in the US, I develop a first-of-its-kind index of human capital heterogeneity. Using monthly Current Population Survey (CPS) micro-data, I capture the diversity of skills among the unemployed to build an index of human capital heterogeneity, a valuable addition to the literature, for it is the only empirical measurement of aggregate skill heterogeneity for the unemployed over time. This measure quantifies the diversity of skills among those searching for jobs and allows me to examine the impact of both trends and fluctuations in the diversity of skills on various labor market outcomes. While applied to the question of jobless recoveries herein, this index has many other useful applications.====Using the Unemployed Human Capital Heterogeneity Index (HCHI====), I test the importance of human capital heterogeneity on labor market outcomes using a structural vector autoregression (SVAR), which provides the first empirical analysis of the how human capital heterogeneity has contributed to recent jobless recoveries. While many labor market models recognize worker heterogeneity (Bingley and Westergaard-Nielsen, 2003, Pries, 2008, Macaluso, 2017, Hall and Schulhofer-Wohl, 2018, Mueller et al., 2018, Grigsby, 2022 to name a few), none has explicitly considered the aggregate level of human capital heterogeneity of the unemployed as an important labor market variable. The recent works of Boettke and Luther (2012) and Burns (2018) indicate an important role for human capital heterogeneity, but they perform no empirical test. Macaluso (2017) provides both a theoretical and an empirical investigation into the importance of skill remoteness (a concept that implicitly relies on human capital heterogeneity), however, Macaluso focuses on occupational mismatch rather than heterogeneity and does not link the analysis to jobless recoveries. By testing the theoretical insights of Pries (2008) Boettke and Luther (2012), and Burns (2018), I provide a valuable contribution to the extant literature.====My results show that the movements in human capital heterogeneity for the unemployed are strongly pro-cyclical and that increases in unemployed human capital heterogeneity cause significant decreases in both employment and vacancies. Using counter-factual analysis, I estimate that increases in human capital heterogeneity can account for one-third of the joblessness of the 1991 recovery, one-quarter of the joblessness in the 2001 recovery, three-quarters of the joblessness in the 2009 recovery, suggesting a significant role for Unemployed Human Capital Heterogeneity Index (HCHI====) as a labor market friction.====With high levels of occupational dispersion and increased HCHI==== volatility, labor market policy may help mitigate the effects of these increased frictions. Beyond the direct effect, these frictions have also been found to contribute to increased monopsony power (Dube et al., 2011, Dube, 2019) and decreased wages (Papageorgiou, 2022). Removing policies that generate artificial frictions—e.g., occupational licensing,—adopting new policies that reduce frictions—e.g., hiring subsidies and unemployment benefits, which both been shown to have some small, positive effects (Yashiv, 2004),—or implementing policies that work with the realities of the market—e.g., well-designed minimum wage policies—provide potentially beneficial avenues to explore.",Human capital heterogeneity of the unemployed and jobless recoveries,https://www.sciencedirect.com/science/article/pii/S0164070423000058,13 February 2023,2023,Research Article,13.0
Okubo Masakatsu,"University of Tsukuba, Japan","Received 15 August 2022, Revised 6 January 2023, Accepted 6 February 2023, Available online 11 February 2023, Version of Record 22 February 2023.",https://doi.org/10.1016/j.jmacro.2023.103514,Cited by (0),.,"Since the seminal work of Lucas (1987), researchers have reconsidered the striking finding that the welfare gains from eliminating consumption fluctuations calculated from US post-World War II data are trivially small. As Lucas (2003) reviews, many studies conducted in the 15 years following his initial work found that two modifications of the Lucas calculation can lead to larger, but still small, welfare gains. One involves changing the specification of stochastic consumption streams and the preference that a representative agent uses to value them. Another modification involves incorporating the incompleteness of asset markets and allowing for the heterogeneity of agents. The basic premise that is common to these strands of research and Lucas (1987) is that they consider a reduction in risk with a known probability distribution.====In the last decade, several authors have argued from a quite different perspective that there is a large welfare gain from eliminating consumption fluctuations. Their premise is that agents are concerned about model misspecification, and they consider a set of probability distributions in the proximity of a benchmark probability distribution, i.e., they consider unknown probability distributions. The focus is on reducing model uncertainty rather than risk. Barillas et al. (2009) adopt this approach and argue that the welfare benefits of eliminating model uncertainty are large relative to those based on the Lucas calculation. Ellison and Sargent (2015) develop a model in which the agent is concerned about model misspecification and is subject to both aggregate and idiosyncratic consumption shocks. They find that incorporating agent concern about model misspecification leads to a larger welfare cost of business cycles than found by De Santis (2007) (and consequently Lucas, 1987). These conclusions are based on evidence from US data.====In this paper, we present additional evidence on the welfare gains from eliminating model uncertainty. We examine the following two questions:====Limited evidence exists regarding the first question. Engel et al. (2018) show that there are substantial welfare gains in some emerging countries. However, as their sample size of countries is small (15 including the United States), further investigation is required. The second question is motivated by the literature on business cycles in emerging and poor countries. Pallage and Robe (2003) follow the tradition of Lucas (1987) and show that there are very large welfare costs of consumption fluctuations in developing countries relative to those in the United States. Uribe and Schmitt-Grohé (2017) present excess output volatility and higher relative consumption volatility in emerging and poor countries as key business-cycle facts.==== By addressing question (ii), we aim to reevaluate these differences between poor, emerging, and developed countries through the lens of model uncertainty.====Barillas et al. (2009) rely entirely on simulation to compute their measure of the welfare gain from eliminating model uncertainty. This is because the calibration of the detection error probability, which measures the degree to which the agent fears model misspecification, is based on simulation. As discussed by Djeutem (2014) and Okubo (2018a), there is a closed-form solution for the detection error probability. Our welfare-gain calculations are based on an analytical formula that combines Okubo’s (2018a) closed-form solution of the detection error probability with the formula proposed by Barillas et al. (2009). There are two advantages of using our formula: (i) it yields the exact value of the welfare gain; and (ii) unlike the simulation-based calculation, it provides an explicit decomposition based on the detection error probability. Hence, the welfare-gain estimates are easier to interpret.====We use annual data on 64 countries for the period 1970–2018. We examine questions (i) and (ii) in two ways. First, we classify the 64 countries into three groups—rich, emerging, and poor—based on a measure of economic development and compare their sample statistics within and between the groups, as well as with the US welfare-gain estimates. Then, we carry out regression analysis with our welfare-gain measures as dependent variables to characterize the relationship between welfare gains and economic development. In this analysis, we control for the effects of two factors—country size and trade openness—that may affect business cycles, as pointed out in the literature (e.g., Kose et al., 2003, Furceri and Karras, 2007, di Giovanni and Levchenko, 2009, di Giovanni and Levchenko, 2012, Haddad et al., 2013, and Uribe and Schmitt-Grohé, 2017).====Our main findings can be summarized as follows. The welfare cost of model uncertainty is substantial relative to that of consumption risk for a broad set of countries. Most countries have welfare gains from eliminating model uncertainty that are considerably higher than those of the United States. As for the second question, the comparison of the sample statistics between the groups shows that the welfare gains from eliminating model uncertainty are, on average, at least two times higher in emerging or poor countries than in rich countries. This suggests that higher levels of economic development are associated with lower welfare costs of model uncertainty. The regression analysis shows that this negative relationship is robust to controlling for country size and trade openness, and that it is stronger than the negative relationship between the level of economic development and the welfare cost of consumption risk. In addition, it shows that as agents’ fear of model misspecification becomes milder, the negative relationship between the level of economic development and the welfare cost of model uncertainty becomes weaker.====There is a limitation to the formula of Barillas et al. (2009) that deserves mention at the outset. According to their formula (Eq. (4) or (5) described below), the welfare gain from eliminating model uncertainty is positively linked to the volatility of consumption. This means that international comparisons of the gains from reducing model uncertainty are in fact equivalent to comparing the magnitudes of consumption volatility across countries. Thus, the first part of our findings simply says that countries with higher consumption volatility, which are largely the emerging and poor countries, have higher welfare gains from eliminating model uncertainty, other things being equal. However, our other findings cannot be drawn only from such multi-country comparisons based on consumption volatility. The use of our formula (Eq. (7) described below), which quantifies exactly the effect of agents’ fear of model misspecification on welfare gains, allows us to draw these other findings.====The remainder of the paper is organized as follows. Section 2 describes our formula after reviewing the welfare-gain measures proposed by Barillas et al. (2009). Section 3 describes the data and classification of the sample countries. Section 4 presents our results and provides potential explanations of them by discussing how the results are related to the existing literature. Section 5 explores the effect of idiosyncratic risk by comparing our closed-form based measure and the version obtained by Ellison and Sargent (2015). This analysis is limited to the US case. Section 6 provides a brief conclusion. The separate appendix contains details on parameter settings and estimates for individual countries as well as the regression results that were omitted from the main paper.","Model uncertainty, economic development, and welfare costs of business cycles",https://www.sciencedirect.com/science/article/pii/S0164070423000149,11 February 2023,2023,Research Article,14.0
Hosoya Kei,"Faculty of Economics, Kokugakuin University, 4–10–28 Higashi, Shibuya-ku, Tokyo 150–8440, Japan","Received 21 October 2022, Revised 1 February 2023, Accepted 2 February 2023, Available online 4 February 2023, Version of Record 10 February 2023.",https://doi.org/10.1016/j.jmacro.2023.103506,Cited by (0),"This paper considers the impact of the coronavirus disease 2019 (COVID-19) pandemic on long-term individual lifetime consumption profiles. The framework for the analysis is a model that extends Strulik (2021) to include the government sector, where time preference is determined by individual health damage (deficit) distinct from normal aging. Thus, the health damage caused by COVID-19 changes the rate of time preference and consequently affects the Euler equation for consumption. Our theoretical contribution is the consistent incorporation of public health investment into the existing model to understand the effect of government measures against a pandemic. Numerical analysis based on this model is used to estimate changes in health status over time, trends in the rate of time preference, and individual lifetime consumption profiles, taking into account differences in age at the time of the pandemic and the nature of the government responses. Because the long-term negative economic impact would be enormous, we should avoid advocating for “living with COVID-19” without due consideration. The reopening of the economy must be accompanied by a commitment to the containment and elimination of infections with future novel coronaviruses.","The novel coronavirus disease 2019 (COVID-19) pandemic, caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), has caused great physical, mental, and economic damage to many people around the world, and the effects continue to this day. In this paper, we pay particular attention to the economic damage caused by the pandemic and try to analyze it from a new perspective. Specifically, we introduce the concept developed by ==== and others of a health deficit due to mechanisms other than normal ====. This concept suggests that the loss of health per se, or the increased risk of health damage in a broader sense, determines the change in the rate of time preference in the agent’s lifetime utility. As a result, subjective time preference is endogenously determined depending on the condition of health damage.====In the context of an endogenously determined time preference, how the rate responds to changes in the variable of interest (i.e., the direction of change) is important, and classifications based on differences in the properties of marginal impatience have been used. A change that acts to increase the rate of time preference is known as ==== marginal impatience. An increase in the rate of time preference implies that an individual has become more impatient. In other words, the degree of patience decreases. Here, it is easier to understand if we consider a slightly extreme situation. The question is how people’s preferences change when the health deficit increases significantly and they become much more aware of their proximity to death. One could say that it depends on the individual. However, one of the prevailing ideas is that when individuals become aware of death, they tend to choose a way of life that values the present rather than thinking about the future.==== In the present model, this is expressed as an increase in the rate of time preference. At the risk of sounding trite, the ephemeral nature of human life is closely related to a high rate of time preference.====On the other hand, a formulation that contrasts with increasing marginal impatience is called ==== The corresponding functional property would be decreasing marginal impatience.====Based on the above preparatory considerations, this paper assumes increasing marginal impatience as having a suitable characteristic for the time preference function that is more consistent with the analysis in this paper.==== Accordingly, the formulation of the time preference function is the same as in ====. However, as ==== and similar previous studies have not considered research about COVID-19, part of the numerical analysis in this paper is completely new. To carry out this extended analysis, the theoretical framework itself is extended to consider the infectious disease control measures taken by governments during the COVID-19 pandemic.====The World Health Organization declared the outbreak of COVID-19 a pandemic in March 2020 and as of this writing the pandemic is ongoing, making the whole picture difficult to grasp. However, when examining people’s responses to infectious disease prevention, some basic patterns can be seen. The first is a decentralized voluntary response, such as staying at home and wearing a mask, based on individual judgments regardless of enforcement. Another is an intervention policy enacted by the government, which may involve enforcement such as urban lockdowns, state of emergency declarations and associated measures, provision of extensive knowledge on infectious disease care, and various vaccination initiatives. Assuming that individual voluntary protection against COVID-19 is represented by individual health investment, the government has made public expenditures for various measures. In the case of the recent COVID-19 pandemic, we can say that individuals and society have confronted the disaster with these two measures. Therefore, it is natural to explicitly include the government sector in the model to capture the impact of public investment behavior on infection control, and we believe that this effort has great significance for our analysis.====This background makes clear that the model used in the present paper can be positioned as a practical framework that goes beyond efforts to simply endogenize the time preference itself to address the essential issues of health loss and health formation that are inevitably involved in economic life. By relying on this model, the positive and negative effects of various pathways on health can be comprehensively analyzed. In particular, the implementation of public policy by the government in the model, based on the COVID-19 pandemic, is an important extension when compared with ====. This leads to some important differences in the theoretical consequences.====The rest of the paper is organized as follows. Section ==== provides a comprehensive review of the related literature. In Section ====, we explain the basic framework presented by ==== and clarify some fundamental features before examining the extended model. Section ====, the numerical analyses are developed from various viewpoints. In Section ====, we attempt to derive implications for the medium- to long-term macroeconomic impact of the pandemic based on the earlier individual-level projection results. Finally, Section ==== summarizes the results obtained throughout the paper and discusses challenges for the future.====The following is the Supplementary material related to this article. ",Impact of infectious disease pandemics on individual lifetime consumption: An endogenous time preference approach,https://www.sciencedirect.com/science/article/pii/S016407042300006X,4 February 2023,2023,Research Article,15.0
Saha Anuradha,"Ashoka University, India","Received 11 July 2022, Revised 24 January 2023, Accepted 26 January 2023, Available online 31 January 2023, Version of Record 8 February 2023.",https://doi.org/10.1016/j.jmacro.2023.103504,Cited by (0),"In a three-sector closed economy where housing land is accumulated for the construction of houses, we depict significant effects of housing and land market on sectoral growth. We assume agriculture is the most intensive in the use of land, followed by manufacturing and then services. In the long run, land intensity differences in conjunction with labor growth contribute to sectoral growth gaps. In the short run, due to housing land accumulation, the economy transitions along a non-monotonic convergent path to the steady state. Using the US data, we quantify that land accounts for about one-tenth of long-run and short-run sectoral output growth gaps. We also incorporate sector-specific land-use restrictions to show that, despite these regulations, land and housing have qualitative effects on non-balanced growth.","Structural transformation is a stylized fact of economic development. As an economy grows, the sectoral composition of GDP changes such that the share of agriculture declines, that of services increases and the manufacturing share in the GDP depicts an inverted-u shape (Herrendorf et al., 2014). While the services sector has become the largest sector in most developed and developing economies, the sectoral growth patterns are not uniform. As shown in Table 1, in the period 1995–2018, agriculture was the fastest growing sector in Brazil, the United Kingdom and the USA. Manufacturing was the fastest growing sector in Germany, Japan, South Korea, Russia, and Switzerland, while services was the fastest sector in the rest of the countries.==== Table 1 establishes that countries witness non-balanced growth and the pattern differs across countries. Here, we refer to non-balanced growth as the differential sectoral growth, in terms of real value added, between the three sectors — agriculture, manufacturing and services. This paper discusses the role of land in non-balanced sectoral growth. We postulate that land affects non-balanced growth through two sources: (a) sectoral differences in land-use intensity, and (b) short-run changes in land availability due to land use for housing production. In the long run, the differences in land-use intensity along with a fixed supply of aggregate land present decreasing returns of scale and hence contribute to non-balanced growth. In fact, in the long run, the land intensity differences manifest in conjunction with labor growth. Economies with higher labor growth will witness a larger role of land in non-balanced growth. In the short run, the housing demand for land qualitatively affects the transition path to the steady state as well as the patterns of non-balanced growth.====Conventional explanations behind non-balanced growth are based on demand-side reasons (Kongsamut et al., 2001), or supply-side mechanisms such as TFP growth differences (Baumol (1967) and Ngai and Pissarides (2007)) or capital intensity differences (Acemoglu and Guerrieri, 2008). In the long run, differences in TFP growth rates determine the sectoral growth ranking. In the short run, either non-homothetic preferences or the differences in capital-use intensity across sectors play a role. These models do not allow for sectors to use different inputs and hence do not depict how factor markets affect non-balanced growth. Our paper provides a unified supply-side framework that explains sectoral growth differences based on TFP growth differences, land intensity differences, and capital intensity differences.==== Further, we carefully model housing production to incorporate that land is affixed with housing stock. Residential or housing land first has to be procured and only then a house can be built upon it (Keuschnigg and Nielsen, 1996). This presents a delay in land adjustment and contributes to non-balanced sectoral growth in the short run.====Economists have largely ignored the role of land in economic growth. This is because TFP growth and capital accumulation tend to overshadow the contribution of land. Our paper conforms with this view when we look at the long-run growth, but not in the short run. In the short run, land-use changes from housing purposes to goods production purposes create unevenness in growth trajectories. There is ample evidence that in metropolitan areas the demand for housing land outpaces the supply as people flock to these locations for jobs (The Economist, 2015). Housing land changes slowly, both in developed and developing countries, due to government regulations on land use or due to the huge costs of land development.==== This paper shows that such frictions in land use affect non-balanced growth, i.e. slow growth in some sectors at the expense of others.====The existing literature has highlighted three key channels through which land affects the economy. Land is an essential input for agriculture (Matsuyama (1992) and Restuccia et al. (2008)), contributes to agglomeration economies (Glaeser, 2010) and also plays a crucial role in facilitating population growth (Loupias and Wigniolle, 2013). In this paper, we incorporate land as an input in the production of goods, as a factor embodied in houses and as an accumulating asset.====This paper has three key contributions to the literature. First, we provide an alternative mechanism for structural change based on differences in land intensity and demand for housing. Keeping in view of Table 1, our model does not directly explain the observed patterns quantitatively, but provides a framework which can potentially explain the different country-experiences with regards to structural change. Second, we construct a housing accumulation process which links the stock of houses with the stock of housing land. In the aftermath of the Great Recession, the role of housing in economic development cannot be overemphasized. The literature has explained linkages between land and housing prices (for example, Li and Zeng (2010) and Moro and Nuño (2012)) or economic growth and the housing market (Leung, 2003).==== This paper depicts land and housing interactions and their effects on sectoral growth. Third, we show the short- and long-run growth effects of land market regulation. In recent years, there has been an increase in land-use laws and environmental regulation laws which restrict the conversion of land for agricultural to industrial purposes (Lambin and Meyfroidt, 2011). These frictions have significant effects on welfare.==== In an extension of the baseline model, we incorporate such frictions in the land market. We find that a capital-poor economy with more land allocated to the agriculture sector (compared to the baseline model’s steady state) would have lower physical capital in all periods.====We calibrate the model to the US economy to quantify the effect of land on non-balanced growth. Differences in land-use intensity account for about a tenth of manufacturing-agriculture and services-manufacturing growth gaps, both in the short and the long run. While this suggests that land has a limited role in sectoral growth, qualitatively we find, land has a significant effect along the transition path. Since housing and housing land are stock variables, their accumulation process brings non-monotonicity to the growth path. We find that compared with a standard multi-sector model without housing, the model behaves qualitatively differently off the steady state.====Here is the outline for the rest of the paper. Section 2 builds the baseline three-sector model with housing. We decompose long- and short-run sectoral growth differences. Based on the US economy for the years 1970–2015, in Section 3, we depict the transition dynamics. We quantitatively analyze non-balanced growth and the relative importance of the factor-use changes and TFP growth differences. In Section 4, we include restrictions on inter-sector land transactions. Finally, Section 5 concludes. We collect proofs and supplementary material in an Online Appendix.",Land and housing: The twin forces of non-balanced growth,https://www.sciencedirect.com/science/article/pii/S0164070423000046,31 January 2023,2023,Research Article,16.0
"Freeman Richard B.,Yang Buyuan,Zhang Baitao","Department of Economics, Harvard University, United States of America,NBER, United States of America,Institute of Economics, School of Social Sciences, Tsinghua University, China,School of Public Policy & Management, Tsinghua University, China","Received 17 August 2022, Revised 21 January 2023, Accepted 23 January 2023, Available online 30 January 2023, Version of Record 31 January 2023.",https://doi.org/10.1016/j.jmacro.2023.103503,Cited by (0),"As a newly emerging factor, data can promote economic growth by driving technological progress, and nonbalanced growth between digital ","As a newly emerging factor, data can promote economic growth by driving technological progress, and nonbalanced growth between digital industries and nondigital industries has been notable in recent years. Nonbalanced growth is significant, especially in the places where digital economy is well developed, for example, in the United States and China. Take the United States as an example. As demonstrated in Fig. 1(a), the growth gap between the total economy and the digital economy is remarkably large and persistent. During 2007–2019, the average growth rate of the digital economy was 6.82%, which was much greater than the average growth rate of the total economy, 1.74%. This evidence shows that the growth rate of output in digital industries has been much greater than that in other industries in recent decades (Barefoot et al., 2018). Moreover, this nonbalanced growth may be related to the high level of wage inequality. For example, as shown in Fig. 1(b), in the United States, there is a persistent gap in the average hourly earnings between all private industries and the information industry. In addition, Fig. 2 shows that a relatively high Gini coefficient accompanies the gradual deepening of the digital economy in the United States. Although data have been introduced into the growth model (see Farboodi et al., 2019, Jones and Tonetti, 2020, Cong et al., 2021, and Farboodi and Veldkamp, 2022), the existing literature does not provide a plausible explanation for persistent nonbalanced growth and the high level of wage inequality in the data economy. To fill this gap, we propose a novel nonbalanced growth model with two sectors that differ in the degree of data deepening and the factor structure of the production function.==== To model the nonbalanced growth of the digital economy, we propose a two-sector general equilibrium model. One sector is the unskilled labor-intensive sector, in which two types of factors, skilled labor and unskilled labor, operate according to the Cobb–Douglas form by combining with technology. The other sector is the skilled labor-intensive sector, in which only skilled labor is utilized in a linear production function by combining with technology. The existing stock of data in one sector is the foundation of the specific technology in that sector, as in Jones and Tonetti (2020). The other vital difference between the two sectors is that data are more productive in the skilled labor-intensive sector (compared to the unskilled labor-intensive sector). Hence, we also call the skilled labor-intensive sector the data-intensive (or data-deepening) sector, and we call the other sector the data-extensive sector. For example, artificial intelligence (AI) and machine learning technologies are deemed data-intensive technologies (Beraja et al., 2021). In the model, the data in the two sectors are different from the perspective of property or variety. For example, the type of data in the fintech industry is very different from that in traditional manufacturing industries, like the steel and construction industries, and thus the accumulation of data in the fintech industry can only improve financial technology.====How are the data accumulated? In the digital era, a key feature is that data are a by-product of past economic activities. Consider that ideas are usually recorded and disseminated in the form of data. The existing data can inspire new ideas and thus promote economic growth in the future. As documented in Weitzman (1998) and Alvarez et al. (2013), the source of growth is the constant combination of creative ideas from different fields. The past knowledge in different fields can inspire the ideas of advanced production technology. Within-sector and cross-sector externalities over time and space are remarkable in data (or idea) accumulation. Our model captures these two types of data externalities in a unified framework. The stock of the specific data in one sector in the next period depends not only on the existing stock of data, but also on the economic activities (measured by outputs) of the two sectors in the current period. For example, on a digital platform, the feedback of consumer evaluation data to manufacturers is beneficial for improving product quality; the basic information that manufacturers provide about their products also helps digital platforms make these products attractive to consumers. Thus, in summary, our model has two crucial sources of nonbalanced growth across two sectors: the difference in production efficiency of the data-driven technology and the difference in the factor composition of the production function.====Our model setup is different from that of Jones and Tonetti (2020) in three aspects. First, the two sectors in our model are asymmetric in the degree of data deepening, thus allowing us to study the new phenomenon of nonbalanced growth in the digital economy. Second, in contrast to the homogeneous labor in Jones and Tonetti’s paper, we divide labor into two categories, skilled and unskilled, which allows us to study the evolution of the skill premium in the economy. Third, for simplicity, Jones and Tonetti (2020) assume that data fully depreciate, while our model takes the perpetual inventory of data into account. Although Cong et al. (2021) explicitly assume that data partially depreciate over time, they do not consider the cross-sector and within-sector externalities in data accumulation or the difference in the degree of data deepening across sectors. Moreover, in our model, data are a by-product of the production process rather than consumption activities. Therefore, no consumer privacy concerns are involved.====In static equilibrium, our theory indicates that increases in the stock of data in the two sectors have opposite effects on the allocation of skilled labor between the two sectors. The skill premium (i.e., the wage of skilled labor relative to that of unskilled labor) in the economy decreases with an increase in the fraction of skilled labor employed in the data-extensive sector. Due to the complexity of equilibrium dynamics, we undertake a simple calibration of our model with credible parameter values over 150 years. The calibration results show that the growth rate of the data-deepening and skilled labor-intensive sector is much greater than that of the data-extensive sector in the long run, which explains the nonbalanced growth in the digital era. Meanwhile, the high skill premium is persistent because it decreases by only 13.74% over a long time (i.e., 150 years).====A key contribution of our study is that it incorporates the cross-sector externality of data into the nonbalanced growth model. A multisector setup is a canonical way to analyze economic growth with structural change. There is a large literature on this, for example, Galor and Moav (2000), Hansen and Prescott (2002), Ngai and Pissarides (2007), Acemoglu and Guerrieri (2008), Samaniego and Sun (2016), Alvarez-Cuadrado et al. (2017), and Comin et al. (2021). However, in most papers on structural change, growth is balanced even if a nonbalanced situation occurs in certain periods. For example, Kongsamut et al. (2001) provide a generalized balanced growth model that can be used to analyze balanced growth and structural change simultaneously. Baumol (1967) proposes a nonbalanced (or unbalanced) growth model with two sectors, one “stagnant” and one “progressive”. Baumol et al. (1985) further provide empirical evidence of nonbalanced growth in the U.S. economy. Acemoglu and Guerrieri (2008) present a nonbalanced growth model in which two sectors have different factor proportions and degrees of capital deepening. Few studies consider the effect of data deepening on nonbalanced economic growth. By taking the cross-sector difference in factor composition and the degree of data deepening into account, our model can be used to analyze nonbalanced growth and the high level of wage inequality in the digital era.====Our paper enriches the literature on data and growth. Jones and Tonetti (2020) primarily provide a growth model based on data nonrivalry, and Cong et al., 2021, Cong et al., 2022 and Farboodi and Veldkamp (2022) further develop this theory. However, these papers do not feature asymmetric data deepening across sectors or the heterogeneity of labor, and thus they cannot explain nonbalanced growth and wage inequality in the digital era. Abis and Veldkamp (2021) point out that the combination of data and labor in knowledge production is influenced by big data technology, while they focus on estimating firms’ stock of data and the effect of development of big data technology on the knowledge production function. Xie and Yang, 2021, Xie and Yang, 2022 study the effect of improvement in information carrier technology on knowledge spillovers and economic growth, and Farboodi and Veldkamp (2022) analyze the effect of the development of data processing technology on information choices and market efficiency. In contrast, our model provides a new mechanism (i.e., the disparate impact of data on technology across sectors) to explain nonbalanced growth and high wage inequality in the digital economy.====Our study also contributes to the literature on wage inequality or skill premium. At least since the early 1990s, many studies, theoretical and empirical, have focused on the issue of wage inequality (Katz and Murphy, 1992, Autor et al., 1998, Autor et al., 2008, Berman et al., 1998, Krusell et al., 2000). Skill premium, defined by the ratio of skilled labor’s wage to unskilled labor’s wage, is well-documented in the literature (Acemoglu and Restrepo, 2020, Burstein and Vogel, 2017, Freeman, 1995, Devroye and Freeman, 2001). Digital divide and digital inequality are also widely discussed and studied in the literature. For example, DiMaggio et al. (2001) point out five aspects of digital inequality—in equipment, the autonomy of use, skill, social support, and the purposes of using the technology; Chinn and Fairlie (2007) document that the digital divide is mainly determined by income inequality; Goldfarb and Prince (2008) find that people with higher incomes and higher levels of education are more likely to adopt the Internet in the United States; Barth et al. (2022) show that software capital can widen the inequality of earnings within and across firms. The cross-sector externality of data accumulation (or idea inspiration) and the difference in the data processing ability of unskilled and skilled labor are two of the crucial features of the digital economy. However, data externality and its effect on wage inequality have not been taken into account in these papers. Our two-sector nonbalanced model permits us to clarify the mechanism of persistently high wage inequality in the digital era.====The rest of the paper is organized as follows. Section 2 describes the two-sector nonbalanced growth model, proves the model specification, and characterizes the static equilibrium and dynamic equilibrium of the economy. Section 3 undertakes a simple calibration and presents the simulation results. Section 4 concludes the paper.",Data deepening and nonbalanced economic growth,https://www.sciencedirect.com/science/article/pii/S0164070423000034,30 January 2023,2023,Research Article,17.0
"Ebeke Christian H.,Eklou Kodjovi M.","International Monetary Fund, Washington D.C., USA","Received 30 December 2021, Revised 10 November 2022, Accepted 22 January 2023, Available online 24 January 2023, Version of Record 27 January 2023.",https://doi.org/10.1016/j.jmacro.2023.103502,Cited by (0),This paper investigates whether automation (the use of industrial robots in production) makes fiscal policy less powerful for stimulating ====. It posits that a ,"The future of work in the context of rapid automation is at the center of many policy discussions. Recent surveys show that most people are anxious about automation and other technological changes and highlight the general concern about the implications of such changes (Pew Research Center, 2017). Automation, by inducing a substitution of labor by robots, may destroy existing jobs, particularly those involving a high degree of routine. At the same time, these technological changes are expected to increase productivity and economic growth.====In this paper, we tackle the issue of automation and the future of jobs by investigating the implications for the effectiveness of traditional demand support policies. We investigate whether the rapid pace of automation makes fiscal policy less powerful for stimulating job creation. Based on the intuition from the conceptual framework with a task-based approach of a production function (see Acemoglu and Restrepo (2018a, 2018b, 2019)), we posit that a fiscal stimulus will be less job intensive if the supplementary demand of goods and services that it creates can be met ==== the use of more robots in production processes. At the industrial level, the mechanism can be described as follows (see for instance Nekarda and Ramey (2011)): To boost activity, the government decides on a fiscal stimulus which involves the purchase of goods and services from industrial firms, to meet these new orders, companies may need to ramp up capacity which may translate into additional labor demand, and hence aggregate employment growth. However, in an environment where robots or automation are increasingly replacing routine-based jobs, the new aggregate demand spurred by the fiscal stimulus could be met by robot use and more automated processes. This would translate into a reduced sensitivity of employment to fiscal stimulus in the presence of rapid and significant automation at the aggregate level.====Our paper is related to the literature on the effect of fiscal policy on employment which has generally concluded on a positive effect of an expansionary fiscal stance on aggregate employment. For instance, Monacelli et al. (2010) estimate the effect of government spending on unemployment and find an unemployment-fiscal policy multiplier of around 0.6% point. More recently, Wilson (2012) estimates the impact of the fiscal stimulus in the context of the American Recovery and Reinvestment Act of 2009 and finds that it had generated on impact of 8 jobs per million dollars spent. Bova et al. (2015) for their part estimate the impact of fiscal consolidations and expansions on the sensitivity of the employment rate to output changes (so-called Okun's law). They find that fiscal consolidation has a sizeable, positive, and robust impact on the Okun's coefficient while the impact of fiscal expansion is not statistically significant.====Our paper is also related to the literature on the impact of automation on the labor market. Acemoglu and Restrepo (2020) estimate a negative effect of the increase in industrial robot usage on U.S. employment and wages. They find that an additional robot per 1000 workers reduces employment to population ratio by 0.2 and wages by 0.42% points. Other studies which focus on the link between technology and employment include Autor et al. (2003), Goos and Manning (2007), Autor and Dorn (2013), Gregory et al. (2016), and Grigoli et al. (2020). For instance, Autor et al. (2003) find that computerization is associated with reduced labor inputs of routine manual and routine cognitive tasks, and with increased labor inputs of non-routine cognitive tasks within industries, occupations, and education level groups. Recently, Grigoli et al. (2020) show that automation negatively affects labor supply in advanced economies. Finally, Aksoy et al. (2021) explore another dimension of the impact of automation on the labor market and show that robotization leads to an increase in the gender pay gap.====To the best of our knowledge, the literature has not thoroughly examined the implication of structural shifts in product and labor markets on the effectiveness of aggregate demand management policies, including fiscal policy. One exception is the study by Bredemeier et al. (2020) which focuses on the response of employment by occupations to government spending shocks in the U.S. The authors find that employment tends to rise more in service sector occupations dubbed “pink-collars” (sales, offices, etc.) while employment in “blue collar” occupations (production, construction, transport, and installation) is hardly impacted. The main channel highlighted by the authors is the high substitution between “blue collars” jobs, which are mostly routine-manual tasks, and capital services. As a consequence, the relative demand for pink-collar labor increases which leads to a rise in the pink-collar to blue-collar employment and wage ratio.====Our paper expands on these theoretical foundations and empirical evidence from the U.S. to propose a cross-country empirical investigation of the direct role played by automation in reducing the sensitivity of aggregate employment to fiscal impulses. Hence, our study complements the previous work by Bredemeier et al. (2020) by proposing an empirical test of the role of the use of capital services (proxied in our framework by the use of industrial robots) in shaping how fiscal expansions affect aggregate and sectoral employment. Taking advantage of a panel dataset from a large sample of advanced economies, we provide evidence for a more limited role for fiscal policy in creating jobs in the context of technological change inducing the automation of routine-based tasks at the aggregate level.====We therefore combine new panel data on the pace of use of industrial robots with fiscal with employment data for 18 European countries and across sectors over the last two decades (1997–2016). Our results show that the pace of automation for the average country in the sample has halved the sensitivity of employment to fiscal stimulus. The paper then proposes a battery of tests to verify the validity of the channels highlighted in our baseline hypothesis. First, if the channel at work is similar to the theoretical discussion in Bredemeier et al. (2020)—who posit that fiscal impulses hardly boost blue-collar labor demand due to the higher substitutability with capital services—one should find empirically that fiscal stimulus will be less effective in boosting manufacturing employment compared to services using aggregate data. We indeed confirm this result using a large sample of cross-country data.====We further demonstrate the robustness of our findings by testing whether automation also reduces the effectiveness of fiscal policy in boosting employment for two additional categories that are expected to be disproportionally exposed to the large displacement effects of automation, namely low-skilled workers and women. Our results show that regardless of the specifications, automation strongly reduces the sensitivity of these two categories of employment to fiscal stimulus. Finally, we also provide dynamic estimate using a local projection framework (Jordá, 2005) and find that the results are statistically significant in the very short-run (within two years) suggesting the importance of short-run substitutability between capital and labor (see Bredemeier et al. (2020)). Overall, our findings imply that fiscal stimulus has become less effective in lifting employment, especially routine-based tasks jobs, in presence of increased automation in advanced economies.====The rest of this paper is organized as follows. Section 2 discusses the theoretical intuitions, Section 3 presents the data, the empirical strategy and results. Section 4 concludes and discusses policy implications.",Automation and the employment elasticity of fiscal policy,https://www.sciencedirect.com/science/article/pii/S0164070423000022,24 January 2023,2023,Research Article,18.0
"Demirel Ufuk Devrim,Otterson James","Congressional Budget Office, Washington, DC 20515, United States of America,International Monetary Fund, Washington, DC 20431, United States of America","Received 14 October 2022, Revised 24 December 2022, Accepted 28 December 2022, Available online 14 January 2023, Version of Record 17 January 2023.",https://doi.org/10.1016/j.jmacro.2023.103501,Cited by (1),"We propose a practical approach to measuring the uncertainty of long-term economic projections. The presented method quantifies the uncertainty of economic variables by using simulations from a multivariate unobserved components model in which variables are formulated as sums of stationary and nonstationary components. The method captures the correlations between both the stationary and nonstationary components of the variables and offers a seamless analysis of short- and long-term uncertainty. Experiments on artificial data demonstrate that, despite its simplicity, the method performs fairly well compared with alternative methods in terms of long-term predictive accuracy and coverage.","This paper presents a practical approach to assessing the long-term uncertainty of economic variables. For the purposes of our analysis, long-term uncertainty is defined as the uncertainty of variables’ average values over a long period of time—typically, several decades. For example, across the 30-year periods that occurred since early 1950s, the average growth rate of total factor productivity, or TFP, varied by about 1 percentage point; the long-term uncertainty of TFP growth arises from that variability.====Each year, the U.S. Congressional Budget Office (CBO) publishes 30-year projections for economic variables and federal spending, revenues, and debt. CBO projects that, if current laws governing taxes and spending generally remained unchanged, the U.S. federal debt as a percentage of GDP would surpass its highest level in history (reached shortly after World War II) within a decade and continue to rise over the following several decades (see Congressional Budget Office (2022)). However, those projections are subject to substantial uncertainty, and a large part of that uncertainty arises from the uncertainty of economic variables such as GDP growth, interest rates, and the unemployment rate. Although, throughout the paper, we focus our attention to the economic variables that underpin long-term budget projections, the proposed methodology is applicable to a much broader set of variables.====A significant part of the uncertainty of economic outcomes in the long term stems from persistent changes in trends rather than transitory economic fluctuations, or business cycles (which tend to average closer to zero over longer periods). Examining the variability of those trends is crucial for assessing the risks to the long-term sustainability of federal debt and for designing policies that can help to mitigate the budgetary effects of unfavorable economic developments. Analyzing the likely range of long-term economic outcomes can also help lawmakers better evaluate the size and timing of the policy changes that they may choose to implement to address the long-term budget imbalance.====The proposed approach to measuring long-term uncertainty is based on simulations from a multivariate unobserved components (UC) model that represent a range of potential future paths for economic variables, such as the rate of productivity growth and the real interest rate. In the model, variables are specified as sums of individually unobserved stationary and nonstationary components. Distinguishing between those components is important for accurately measuring the uncertainty of variables over the long term. That is because, although the short-term uncertainty of the variables arises from the variability of both components, the long-term uncertainty stems, in large part, from the variability of the nonstationary component.====Uncertainty in long-term economic projections can be expressed by constructing prediction intervals for long-horizon averages of economic time series. A prediction interval is a range in which a future outcome is expected to fall with a specified probability. For example, in The 2019 Long-Term Budget Outlook, CBO estimated that there was roughly a two-thirds chance that the average annual growth rate of TFP over the next three decades would be in the range of 0.6 percent to 1.6 percent (see Congressional Budget Office (2019)). CBO also estimated long-term prediction intervals for other major economic variables, including the real interest rate and the unemployment rate. Changes in those variables have important effects on the federal budget: Faster TFP growth and a lower unemployment rate would mean higher taxable income and revenues and, therefore, lower deficits and debt. By contrast, higher interest rates would increase the government’s interest payments, causing deficits and debt to be larger than they would be otherwise.====Estimating prediction intervals for long-term averages of economic time series presents significant challenges. First, there is relatively limited information about the variability of long-term averages in the available sample data for most economic variables. Estimating that variability is particularly difficult when the prediction horizon is long relative to the sample size, resulting in very few observations of long-term averages. For example, since the end of World War II, there have not yet been three nonoverlapping 30-year periods, whereas there have been 25 nonoverlapping 3-year periods. In addition, long-term statistical properties of a variable depend on the exact form of persistence that the variable displays. For example, as discussed in Müller and Watson (2018), random walks have different long-term properties than fractionally integrated or serially correlated stationary variables. But there is often limited information in the sample data to precisely distinguish between different forms of persistence.====The recent research offers new insights into long-term prediction and inference. Müller and Watson (2016) develop methods for conducting inference about the long-term variability of economic time series and construct prediction sets for long-term averages. Those methods provide theoretically valid prediction intervals under a wide range of stochastic processes exhibiting different forms of persistence, including local-level, local-to-unity, and fractionally integrated forms. Zhou et al. (2010) propose an alternative approach to constructing prediction intervals based on estimated long-term standard deviations and sample quantiles. Chudy et al. (2020) propose adjustments that improve the predictive performance of the method of Zhou et al. (2010) over long horizons. In a related branch of the literature, Stock (1996) and Phillips (1998) show that standard methods of estimating long-term prediction intervals and impulse response functions (which describe how economic variables respond to various shocks over time) produce biased estimates if variables exhibit a high degree of persistence, and Pesavento and Rossi (2006) propose an approach to constructing confidence intervals for impulse responses when variables are highly persistent.====Compared with some of the recently developed methods, including those of Müller and Watson (2016) and Chudy et al. (2020), the simulation-based UC approach proposed in this paper can more easily accommodate a multivariate framework. Assessing the interactions of multiple variables is particularly important for fiscal analysis because correlations between economic variables matter for budgetary outcomes. For example, if slower output growth is associated with lower interest rates, a reduction in revenues caused by slower growth in TFP would be paired with lower interest payments on debt, which could offset the effect on deficits from slower TFP growth. Müller and Watson (2018) develop a method for assessing the long-term covariability of economic variables. But implementing that method in a setting with more than two variables carries a heavy computational burden.====The simulation-based UC method offers a practical solution to assessing the long-term variability of more than two economic variables. The method is computationally manageable and relatively simple to implement, and it offers a seamless analysis of both short- and long-term uncertainty. It is, however, less robust under alternative forms of persistence that economic variables may display than are some of the recently developed, state-of-the-art methods (including those of Müller and Watson (2018). Nevertheless, a comparison of the predictive performances of different methods based on Monte Carlo experiments indicates that, despite being less robust than some of the recently developed methods, the UC method fares reasonably well in terms of long-term predictive accuracy and coverage. Ultimately, however, all methods are subject to the fundamental limitation that there are very few observations of long-term averages in any limited sample period. Therefore, the amount of low-frequency information that can be gleaned from the available data and our ability to precisely estimate long-term prediction intervals are limited.====The remainder of the paper is organized as follows: Section 2 lays out the UC model and describes how it is used to decompose variables into stationary and nonstationary components. Section 3 discusses the estimation of model parameters and presents the estimated prediction intervals for TFP growth, the unemployment rate, and the real interest rate, which are based on a series of simulations each reflecting an alternative future path for those variables. Section 4 conducts Monte Carlo experiments on simulated data produced by artificial processes that mimic the historical behavior of a set of key macroeconomic variables and also compares the predictive performance of the UC method with that of alternative methods. Section 5 provides concluding remarks.",Quantifying the uncertainty of long-term macroeconomic projections,https://www.sciencedirect.com/science/article/pii/S0164070423000010,14 January 2023,2023,Research Article,19.0
Cheng Wan-Jung,"Institute of Economics, Academia Sinica, 128 Academia Road, Section 2, Nangang, Taipei 115, Taiwan","Received 6 September 2022, Revised 27 December 2022, Accepted 28 December 2022, Available online 9 January 2023, Version of Record 29 January 2023.",https://doi.org/10.1016/j.jmacro.2022.103499,Cited by (0),"This paper uses a political economy perspective to study the endogenous formation of economic policies and its interplay with political institutions. This paper provides a novel view that both the institutions and economic development status are essential factors in endogenously determining economic policies. The model aims to explain both the differences in the degree of adopting industrial policies as well as the differences in the types of industrial policies being implemented. Using a concise framework with two country-specific characteristics, the baseline model can capture three main types of industrial policy platforms of interest. In a country where voters’ political awareness is positively skewed and press freedom is relatively low, pro-heavy ","Both the degree of implementation and the content of industrial policies is dispersed across countries. Some countries have active industrial policies while some do not. Among those with active industrial policies, the objectives and targets of the policies are also diverse . Some countries tend to adopt policies favoring large conglomerates, while others tend to target small and medium-sized enterprises (SMEs). A vast literature, both theoretical and empirical, has been devoted to evaluating the effects of policies that depend on firm size—namely, size-dependent policies—on the industrial structure and aggregate variables. Whereas much attention has been given to analyzing how various types of industrial policies would affect the economy, little has focused on understanding why there exist different types of industrial policies in the first place. Therefore, the purpose of this paper is to contribute to this line of research by studying how industrial policies are endogenously determined and I tackle this question by constructing a political economy model featuring election campaigns and lobbying.====This paper presents a model of electoral competition in which two political parties compete for votes; the one that wins the election will take office and determine the industrial policy. The two parties propose policy platforms prior to the election and must commit credibly to their proposition once elected. Parties can run election campaigns, which are financed by political donations from interest groups, and the effectiveness of the campaign is determined by the party’s influence on mass media. Voters make decisions based on their expected welfare but may be swayed by election campaigns depending on the levels of their political awareness. The industrial policies are assumed to take the form of productive industry-specific infrastructure of which the cost is financed by tax revenue. When the infrastructure is welfare improving, it is straightforward to expect the adoption of the associated industrial policy in equilibrium. Nonetheless, industrial policies that are welfare inferior may still be present if voters are less politically aware. The industry-specific infrastructure, which increases the industry-specific productivity, can benefit firms of that industry. The political party that proposes the policies favoring certain industries can thus raise campaign contributions from this interest group. Therefore, even if an industrial infrastructure is welfare inferior, it could still be chosen by the voters in an economy where election campaigns have sufficient effects on voters’ decisions.====This paper characterizes three main scenarios of industrial policy implementation: first, inclined toward pro-heavy industry policies; second, switching between pro-heavy industry and pro-light industry policies; and the last, no active industrial policies. In Section 5, it will be documented that the industrial policies in South Korea are representative of the first policy scenario, Japan is representative of the second scenario, and the U.S. is representative of the third scenario.====The variation of industrial policies across countries depends on the effects of election campaigns on voters’ decisions, which in turn depend on voters’ political awareness as well as the degree to which the political parties could influence the mass media. In countries where voters have high levels of political awareness, who are usually referred to as informed voters, voting decisions are mainly determined by their expected welfare given the policy propositions. Therefore, parties proposing welfare inferior policy platforms will not be elected. In countries where voters have lower political awareness, who are usually referred to as uninformed voters, industrial policies are often present as voters are more easily influenced by campaigns, and thus, the interest groups have incentives to make political donations for lobbying. In these countries, industrial policies that are welfare inferior may present if political parties can effectively influence public opinion through election campaigns.====Which type of industrial policies would be selected, however, depends on the party’s “production technology” of election campaigns. When a party possesses better technology to influence public opinion, its campaign expenditure can be more effectively transformed to attract votes. Given the fact that the mass media play a crucial role in forming public opinion, election campaigns are considered to be more effective in an economy where the political parties and the government have certain powers over the media. When election campaigns are more effective, meaning each unit of campaign expenditure can be transformed into more efficiency units of campaign outcome, it is more possible that pro-heavy industry policies will present in equilibrium. First, when campaign expenditures can be more effectively transformed into the desired outcome, it is more likely that the effect of the campaigns on attracting votes dominates the effect of the costly industrial policies on discouraging votes. Second, although heavy industry infrastructure is more costly, it also raises the industry-specific productivity more than light industry infrastructure does, and thus, heavy industry firms have a higher willingness to pay for lobbying for favoring policies. Therefore, when election campaigns are more effective, it is more likely that the net effect on votes from pro-heavy-industry policies dominates the net effect from pro-light industry policies. That is, in an economy where there are more uninformed voters and the political parties have a certain influence on mass media and hence could gain their support, it is more likely that pro-heavy industry policies would dominate. By contrast, in an economy where there are more uninformed voters but the media are relatively independent and free, campaign expenditures may not lead to the desired formation of public opinion as effectively as in the previous scenario, and thus, sometimes pro-light industry policies may dominate because of the smaller welfare cost. In this economy, there will be switches between the two types of industrial policies, and which policy will present is contingent on the current economic conditions.====In the literature, size-dependent policies emerge in multiple forms. The first category consists of output taxes or subsidies that target establishment size above or below certain threshold (Restuccia and Rogerson, 2009, Barstelman et al., 2013). The second consists of input taxes or subsidies that are in effect when above or below a particular level of input use, such as credit subsidies (Guner et al., 2008). The third are the regulations that are explicitly contingent on establishment size (Blanchard and Giavazzi, 2003). Empirically, there are also corresponding size-dependent policies (Guner et al., 2008, OECD, 2002, Kim and Leipziger, 1993) : (1) income tax deductions to SMEs in many OECD countries; (2) tax on capital gains of the firms; (3) credit guarantees or favorable interest policies applied to certain industries; (4) exemptions or reductions of entry cost for new entrepreneurs; (5) additional cost for expansion of establishments beyond a certain size; (6) employment protection legislations that depend on size. Although industrial policies are prevalent across countries, a survey by Pack and Saggi (2006) suggested little empirical support for active industrial policies in improving aggregate output or welfare.====In the political economy literature, Baron (1994) and Grossman and Helpman (1996) established the foundation for analyzing the electoral competition of two political parties, interactions between the parties and the interest groups, and lobbying. In their framework, a society consists of informed voters and uninformed voters, and there is a trade-off between attracting informed voters and uninformed voters. Each party needs to choose a mixture of policy platform and election campaign spending to compete for votes. Acemoglu and Robinson, 2006, Acemoglu and Robinson, 2008 built another approach by explicitly modeling the economic incentives for individual agents to contribute to lobbying-type activities. They constructed a dynamic environment and endogenized both the policies and the institutions to study the interplay of the changes in political institutions and the determination of economic policies.====The remainder of the paper is organized as follows. The model is presented in Section 2, and the equilibrium is solved in Section 3. In Section 5, I provide empirical evidence on the mapping between the data and the theoretical results. Section 6 concludes.",A political economy approach to endogenous industrial policies,https://www.sciencedirect.com/science/article/pii/S0164070422000921,9 January 2023,2023,Research Article,20.0
Sim Khai Zhi,"Department of Economics and Finance, Eastern Connecticut State University, 227E Communications Building, Eastern Connecticut State University, Willimantic, CT 06226, United States","Received 8 March 2022, Revised 25 December 2022, Accepted 28 December 2022, Available online 7 January 2023, Version of Record 13 January 2023.",https://doi.org/10.1016/j.jmacro.2022.103498,Cited by (0),This paper uses a theoretical model to analyze the optimal combination of monetary response (lowering of interest rates) and fiscal ==== do not always incentivize banks to monitor their investments if there is a potential contagion from unhealthy to healthy banks.,"During the 2008 financial crisis, central banks of multiple OECD nations including the Federal Reserve, the Bank of England, and the European Central Bank coordinated to simultaneously lower interest rates. This response coupled the enormous bank rescue packages that were introduced by governments across these nations to stabilize their financial systems. Similar actions were taken during the Scandinavian banking crisis in the early 1990s that caused a series of bank failures in Sweden, Finland, and Norway. After massive bailout packages were rolled out by the governments of these nations, their central banks eventually removed the currency pegs to lower interest rates.====Even though historic events have suggested a high possibility for the central bank to lower interest rates following a banking crisis, the intention of the lower interest rates are mainly to reduce the impact of the recession caused by the banking crisis. In this paper, I argue that lowering of interest rates – if implemented concurrently with the bank rescue package – can improve welfare as it reduces the cost of the bank rescue package to tax payers. Moreover, banks are willing to monitor their investments more closely when they anticipate lower interest rates following a banking crisis.====The U.S. Savings and Loan Crisis of the 1980s showcased the ability of lower interest rates in alleviating the intensity of a banking crisis. One of the contributing factors to the crisis was the stark increase in interest rates in the late 1970s and early 1980s. Due to the recession of 1981–1982, the Federal Reserve allowed the federal funds rate to fall, which then led to a fall in market interest rates in 1983. This returned many savings and loan institutions in the U.S. to health, although 35% of them still suffered a loss and 9% were insolvent by GAAP standards.==== This episode shows that lower interest rates do help return the health of financial institutions. However, lower interest rates themselves are not sufficient in preventing widespread bank failures.====The model I introduce in this paper suggests exactly that. The results I present show that it is optimal to combine a lowering of interest rates (i.e. monetary response) with the bank rescue package (i.e. fiscal bailouts) during a banking crisis because lower interest rates reduce the liquidity pressure on banks and thus decrease the amount of bailout required to rescue them.==== Moreover, the combination of monetary response also induces banks to exert higher efforts to monitor their investments. This is because the lower interest rate from the monetary response decreases the cost of banks to raise deposits and thus increases their expected revenues if their investments turn out healthy.==== However, the model also suggests that it is never optimal to combat a banking crisis solely with a monetary response, as evidently seen from the 1983 event during the U.S. Savings and Loan Crisis. Additionally, I also show that when the banks anticipate the regulatory authority to combine a monetary response with fiscal bailouts, stricter capital requirements are not always beneficial.====The model introduced in this paper incorporates elements from Diamond and Dybvig (1983), Keister (2015), Dell’Ariccia and Ratnovski (2019), and Eisert and Eufinger (2019). The model adopts the banking environment similar to Diamond and Dybvig (1983) where banks serve a continuum of depositors each of whom independently faces a liquidity risk. The model also adopts the bailout structure similar to Keister (2015) in which the fiscal authority funds bank bailouts from tax revenue. The network structure of the banking industry is similar to Dell’Ariccia and Ratnovski (2019). There are two banks that can decide on how closely they monitor their respective investments.==== In the extended model, when a bank’s investment becomes unhealthy, there is a potential spillover effect on the investments of the other bank.==== Depositors are assumed to be able to choose between holding risk-free bonds and depositing in banks, which is similar to Eisert and Eufinger (2019). The interest rate on risk-free bonds then directly indicates the cost of banks to raise and retain deposits. This gives the monetary authority (i.e. the central bank) the ability to lower interest rates to alleviate the pressure on the banking system during a banking crisis. Empirical evidence has also suggested that deposit rates are upward-sticky but downward-flexible (Driscoll and Judson, 2013). This implies that the cost of deposits decreases relatively quickly in response to the lowering of interest rates by the monetary authority.====The main contribution of this paper is that it introduces two novel macroprudential roles of interest rate policies that should be considered by the monetary authority: the ability of lowering interest rates in reducing the cost of bank bailout packages and promoting better investment monitoring by banks. These findings add to the line of existing literature that investigates other macroprudential roles of monetary policy including Adrian and Liang (2018), Adrian et al. (2020), and Caballero and Simsek (2019). It is important to acknowledge these macroprudential roles when making decisions on interest rate policies, particularly when they go against the direction for inflation targeting (see Stein (2012)).====This paper adds to the extensive theoretical literature that investigates bailout policies in interbank networks including Bernard et al. (2017), Capponi et al. (2020), Eisert and Eufinger (2019), Erol (2018), Niepmann and Schmidt-Eisenlohr (2013), and Tian et al. (2013). This paper also adds to the literature that investigates the role of the central bank in promoting financial stability including Freixas et al. (2000), Robatto (2019), and Bluhm (2018).==== Freixas et al. (2000) analyzes the coordinating role of the monetary authority in preventing payments systemic repercussions. Robatto (2019) shows that the size of monetary injection required to eliminate a systemic bank failure is smaller if the monetary authority provides loans to banks instead of using an asset purchase policy. Bluhm (2018) shows that the monetary authority’s interest rate policy can effectively mitigate systemic risk particularly in economies with lenient capital requirement.====This paper differs from Rogers and Veraart (2013), Freixas et al. (2000), and Robatto (2019) in that it analyzes the optimal monetary response (i.e. interest rate policy) of the monetary authority and how the monetary response can be combined with fiscal bailouts to lower the burden on tax payers and mitigate the moral hazard caused by fiscal bailouts. I also show that a lenient capital requirement can sometimes increase the efficacy of the monetary response in mitigating moral hazard. Therefore, my result agrees with Bluhm (2018) in that a strict capital requirement is not always desirable.====The interactions and coordination between monetary and macroprudential policies have been studied extensively by existing literature, including Angeloni and Faia (2013), Paoli and Paustian (2017), Van der Ghote (2021), Carrillo et al. (2021), Kiley and Sim (2017), and Gelain and Ilbas (2017), all of which focus on different macroprudential policy instruments. Angeloni and Faia (2013) focuses on capital requirements. Paoli and Paustian (2017) models macroprudential policy as a cyclical tax on the borrowing of firms. Van der Ghote (2021) looks into limits on leverage. Carrillo et al. (2021) investigates a subsidy on lenders to incentivize lending when credit spreads rise. This paper adds to this line of literature by showing that the coordination between monetary policy and fiscal bailouts during a banking crisis can reduce the cost of bailing out failing banks and improve bank monitoring.====The model and results presented in this paper have two limitations. Firstly, when interest rates can no longer be lowered due to the interest rate zero lower bound, monetary policy naturally loses its power to reduce the cost of bailouts and increase bank monitoring. Secondly, the existence of reversal interest rates as presented in Brunnermeier and Koby (2018) and Repullo (2020) is not taken into consideration in this model.==== Bank capital is assumed to be exogenous in the extended model presented in Section 4, therefore reversal interest rates would not exist since the capital requirement is never binding.====The remainder of this paper is organized in the following manner: Section 2 introduces the baseline model, Section 3 solves for and discusses the equilibrium of the baseline model, Section 4 solves for and discusses the equilibrium of the extended model that includes equity capital in banks and contagion, Section 5 concludes the paper.",Monetary and fiscal coordination in preventing bank failures and financial contagion,https://www.sciencedirect.com/science/article/pii/S016407042200091X,7 January 2023,2023,Research Article,21.0
Kim Jeong-Yoo,"Department of Economics, Kyung Hee University, Seoul, Republic of Korea","Received 1 August 2022, Revised 25 November 2022, Accepted 31 December 2022, Available online 7 January 2023, Version of Record 9 January 2023.",https://doi.org/10.1016/j.jmacro.2022.103500,Cited by (0),This paper considers the issue of ==== on the monetary and fiscal policy mix. We show that cross signal jamming whereby the monetary authority and the fiscal authority successfully jams an unfavorable signal of each other does not occur in equilibrium.,"The classical time-inconsistency problem of optimal policy rules was first recognized by Kydland and Prescott (1977). By Kydland and Prescott, optimal policy rules are not credible in a general dynamic setting, because they are time inconsistent in the sense that if a policy rule is believed and used to form expectations of a future policy by private agents, the government has an incentive to deviate from it later on, inducing policy “surprises”. In equilibrium with rational private agents, such policy surprises are ruled out. The equilibrium policy rule must be dynamically consistent, as the feedback equilibrium discussed by Kydland (1977). But once the equilibrium concept of the feedback nature is adopted, the policy rule brings about a lower overall level of welfare. The way out of this trap is to commit in advance to a policy rule. Discretion that allows a policy to be chosen sequentially over time suffers from lack of credibility. In this situation, irreversible commitments are valuable because they lend credibility to policy and enable the policymaker to influence private sector expectations.====Barro and Gordon, 1983a, Barro and Gordon, 1983b and its subsequent large literature focused on the issue of monetary policy and in particular highlighted the role for monetary rules as a potential means to overcome the time inconsistency problem in monetary policy. Lack of commitment to a (low-inflation) monetary rule tempts the central bank to create surprise inflation, leading to the undesirable discretionary outcome with no higher employment and output at the expense of higher inflation. In their model, the discretionary behavior of the central bank means that whenever some information is available in each period, it can be flexible by changing the policy. Thus, in this discretionary regime, the central bank chooses the current rate of inflation ==== inflation expectations are formed. This regime can be justified by the flexibility that the central bank has to react to shocks as opposed to the public who cannot adjust their expectations readily due to their delayed information acquisition. On the other hand, under the rule (commitment regime), the central bank cannot be flexible. It credibly commits to a monetary rule (future actions) by computing the dynamic programming solution. Barro and Gordon (1983a) call the solution of this problem “time-inconsistent solution” à la Kydland and Prescott (1977).==== In other words, the ==== central bank uses ====. So, the central bank precommits to a monetary policy yielding an inflation rate, ==== inflation expectations are formed. The commitment regime can be justified when the central bank does not react to every shock by following a long-term policy. This paper adds two more features that are important in reality, monetary and fiscal policy mix and information problem. These new ingredients may be relevant especially under the current circumstances of the COVID-19 pandemic and Russia–Ukraine war that have induced changes in aggregate supply increasing supply shock volatility and making potential output and employment more difficult to forecast. In fact, the targets for employment and output shift over time due to supply shocks which are often not perfectly known to private agents, although the information can be more readily available to the government or the central bank. In this paper, we examine how such information asymmetry can affect the policy of the central bank or interaction between monetary policy and fiscal policy.====There is also a large literature examining how private information on the part of policy makers affects the economy and policy choices. The presence of private information fundamentally alters the effects of policies on the economy. There are two categories of existing models of monetary policy in the presence of private information. One is private information about variables which policymakers is concerned about. Canzoneri (1985) assumes private information about money demand. The other is private information about the government’s ability to commit. Barro (1986) and Vickers (1986) belong to this. We take the first approach by assuming that the government possesses private information about supply shock.====In this paper, we pay attention to the discrepancy between the public’s preference for correct forecast of the inflation rate and their ability to make a correct forecast. One of the difficulties that the public faces in predicting the inflation rate is that their expectations about the inflation rate itself affects the actual inflation rate, However, the public can correctly predict the inflation rate, which is a fixed point, by using the macroeconomic model that they have, insofar as there is no uncertainty about the underlying state of the economy. If there is uncertainty, the public must rely on the policy of the central bank or the government who is better informed in order to acquire the relevant information about the state of the world. Thus, we take the signaling role of the central bank or the government’s policy seriously and investigate whether information transmission by signaling is possible. We show that Intuitive Criterion developed by Cho and Kreps (1987) selects the unique separating equilibrium in which the low-type central bank who knows that the supply shock is low chooses the interest rate that is higher than its first best level but lower than the level chosen by the high-type central bank. A high-type central bank cannot imitate the lower interest rate, because it would lead to a high inflation rate. Although Stein (1989) considered the role of costless signaling (cheap talk) in the monetary policy, signaling in our model is costly. We obtain an interesting result that discretion may be better than the rule for the central bank if the central bank has private information about a supply shock, contrary to the case of complete information.====We also examine whether cross signal jamming by two policy makers (central bank and the Treasury) with conflicting interests occurs in equilibrium. Cross signal jamming may occur when two informed players with conflicting interests successfully jam an unfavorable signal that the other player sends.==== In our setting, if the central bank and the government share information about the state of the world but the central bank favors a more stable policy while the government favors a more expansionary policy, the government may prevent the signal that the central bank wants to convey to the public, because the information would weaken the justification for its expansionary policy. However, we demonstrate that it is not possible in equilibrium. Moreover, counter-intuitively, we show that the undistorted separating outcome which is the first best under complete information can be an equilibrium in the case of the rule-based policies, mainly due to inability of the two authorities to coordinate. We also show that no equilibrium exists if the policies are discretionary.====Since Friedman (1948), many authors including Leeper (1991), Dixit and Lambertini, 2000, Dixit and Lambertini, 2001, Dixit and Lambertini, 2003, Adam and Billi (2008), Saulo et al. (2013), Stawska et al. (2019) considered monetary and fiscal policy interactions. Leeper (1991) categorizes policies as active or passive, depending on its responsiveness to the constraints. A policy is active if the authority is free to set its policy variable without being restricted to constraints, and it is passive if it is restricted by its constraints. His main result is that the uniqueness of the equilibrium requires that at least one policy authority sets its policy variable actively, i.e., if both policies are passive, equilibria are indeterminate, i.e., the problem of multiple equilibria arise. They also argue that if both policies are active, which corresponds to our model, they can violate the government budget constraint. In a series of papers, Dixit and Lambertini, 2000, Dixit and Lambertini, 2001, Dixit and Lambertini, 2003 considered various possible policy regimes similar to Leeper (1991). While Dixit and Lambertini (2001) assumed that there is no time-consistency problem of monetary policy, Dixit and Lambertini (2003) dealt with the time-consistency problem of both monetary and fiscal policies. They compared the discretionary monetary/fiscal policies and the committed policies in several scenarios (whether one authority has leadership over the other) but they did not assume that there is asymmetric information between the monetary authority/fiscal authority and the public. Dixit and Lambertini (2000) considered the possibility that the monetary policy is disturbed by fiscal policy, with the similar insight of cross signal jamming in our paper. Similarly, Saulo et al. (2013) find the optimal monetary policy and the optimal fiscal policy in three models; (i) one in which both authorities choose their policies independently as a Nash equilibrium, (ii) one in which they move sequentially as in a Stackelberg model, and (iii) one in which they choose the policies cooperatively. They show through numerical simulations that the Stackelberg model in which the monetary authority moves first as a leader leads to the smallest welfare loss in the Brazilean case. Although Dixit and Lambertini (2003) and Saulo et al. (2013) are both closely related to our paper, neither considers the information problem between the government and the public or the signaling possibility. Furthermore, to the best of our knowledge, none has addressed the issue of signal jamming in the interactive model of monetary policy and fiscal policy. Also, Adam and Billi (2008) studied monetary and fiscal policy games without commitment in a dynamic, stochastic setting, and showed that lack of fiscal commitment gives rise to excessive public spending, while lack of monetary commitment generates too much inflation. However, they did not consider the information problem, nor compared the result with the case with commitment. Cui (2016) analyzed the interactions between monetary and fiscal policy in a dynamic model with endogenous liquidity frictions, and Williamson (2018) dealt with the issue of how the fiscal authority can impose constraints on the central bank and showed that the fiscal authority may want to tolerate inefficiency to finance public goods provision. Andolfatto and Martin (2018) paid attention to the tendency of the growing relative importance of government debt over central bank reserves as money and noticed that fiscal policy is likely to play an important role in determining the rate of inflation because the supply of base money is increasingly under the control of the fiscal authority. Thus, they were mainly interested in the economic consequences of a central bak that can influence the yield on government bonds in such a situation in which the fiscal authority controls the supply of base money. Stawska et al. (2019) examined the issue of coordination of monetary and fiscal policies in EU member states facing institutional restrictions, such as the Maastricht Treaty requiring deficit limit. They derived the Nash equilibrium policy mix of budget deficit and interest rate. However, none of the papers addresses the time-consistency issue nor consider the issue of asymmetric information.====The paper is organized as follows. In Section 2, we set up the model. In Section 3, we analyze the complete information case. In Section 4, we provide an analysis for the incomplete information case when a monetary policy can be used. In Section 5, we consider the case that both the monetary policy and the fiscal policy are available in the incomplete information case in order to investigate the possibility of cross signal jamming. In Section 6, we briefly discuss the robustness of our results. Section 7 contains concluding remarks and caveats.","Monetary policy, fiscal policy and cross signal jamming",https://www.sciencedirect.com/science/article/pii/S0164070422000933,7 January 2023,2023,Research Article,22.0
Biolsi Christopher,"Department of Economics; Western Kentucky University, Bowling Green, KY 42101, USA","Received 5 August 2022, Revised 20 December 2022, Accepted 28 December 2022, Available online 5 January 2023, Version of Record 11 January 2023.",https://doi.org/10.1016/j.jmacro.2022.103496,Cited by (0),"I compare the empirical performances of the recently-developed Hamilton and Beveridge–Nelson filters of nonstationary time series, using quarterly data on real gross state product in U.S. states. There is meaningful overlap between the two filters, with average correlation coefficients ranging between 0.60 and 0.97. The Hamilton filter and its more recent modification produce cycles of greater volatility and amplitude than the Beveridge–Nelson filter and appear to outperform in pseudo-out-of-sample forecasting exercises of future GSP growth and ==== (though the outperformance is not generally statistically significant). The Beveridge–Nelson filter is, however, less sensitive to realizations of new data.","Macroeconomists have long recognized the utility of decomposing a nonstationary time series into its stochastic trend and cyclical components, going back at least to the seminal work of Nelson and Plosser (1982). In addition to providing a sense of the current state of the business cycle, the difference between a series’ actual value and its stochastic trend may also be employed in policy contexts, such as in the case of the deviation of U.S. gross domestic product from its trend (or “output gap”) entering as an argument in Taylor (1993)’s monetary policy rule framework. Acknowledging the importance of identifying the cyclical deviation from trend in a series leads next to the question of how to actually do so.==== There have been no shortage of proposed filtering techniques introduced over recent decades, with Clark, 1987, Hodrick and Prescott, 1997, Baxter and King, 1999, and Christiano and Fitzgerald (2003) all providing prominent examples.====Many of these proposed methodologies, however, have come under criticism for a variety of theoretical shortcomings. For example, Morley et al. (2003) point out that there is no difference between the unobserved-components approach in Clark (1987) and the Beveridge and Nelson (1981) decomposition when one relaxes the unnecessary assumption of no correlation between innovations to the trend and cycle components. With respect to the highly popular Hodrick and Prescott (1997) filter, Cogley and Nason (1995) argue that applying it to a difference-stationary series is akin to detrending a random walk with a linear trend, which Nelson and Kang (1981) demonstrated can produce spurious measurements of business cycle fluctuations. Specifically, Cogley and Nason (1995) state that the Hodrick and Prescott (1997) filter “can generate business cycle periodicity and comovement even if none are present in the original data”. Hamilton (2018) makes similar arguments, while also pointing out that typical values of the smoothing parameter used in the filter are quite different from those derived in a statistical set-up. Murray (2003) analyzes band-pass filters (specifically that of Baxter and King, 1999), and shows that such filters tend to overstate the importance of transitory dynamics by passing part of the stochastic trend of the series through the filter.====The difficulties associated with properly decomposing cycle and trend are made all the more relevant by the fact that errors in these decompositions can have important real-world consequences. Famously, Orphanides (2001) illustrated that the supposed monetary policy failures of the 1970s did not owe so much to a lack of a rules-based framework as to the problem of measuring output gaps in real time. Thus, there has been continued research into new means of statistically identifying the output gap, with two prominent recent examples being the Beveridge–Nelson filter introduced by Kamber et al. (2018) and the regression-based approach in Hamilton (2018), itself further extended in Quast and Wolters (2022). This paper does not seek to critique these newly developed filters from a theoretical point of view, but rather offer practitioners some sense of their empirical tendencies. For a sense of how the results of various statistical filtering techniques compare with theoretical notions of permanent or transitory fluctuations viewed through the lens of a standard dynamic stochastic general equilibrium model, see Canova (2022).====Using quarterly data on real gross state product for the 50 U.S. states plus the District of Columbia from 2005:Q1 to 2022:Q2, I find that the output gaps that result from the two novel methods have substantial but not complete overlap. Those that derive from the Hamilton filter (and its yet more recent modification) tend to have greater volatility and amplitude than those resulting from the Beveridge–Nelson filter, and they tend to offer more accurate forecast performance, in a pseudo-out-of-sample forecasting exercise, though there is not sufficient evidence to say that the outperformance is statistically significant. The cycles that are produced by the Beveridge–Nelson filter are more reliable, however, in the sense that they are less sensitive to realizations of new data. Such empirical regularities ought to be useful information for time series analysts in deciding which of these filters to employ in a given empirical context.",Do the Hamilton and Beveridge–Nelson filters provide the same information about output gaps? An empirical comparison for practitioners,https://www.sciencedirect.com/science/article/pii/S0164070422000891,5 January 2023,2023,Research Article,23.0
"Hüpper Florian,Kempa Bernd","University of Münster, Germany","Received 25 May 2022, Revised 21 December 2022, Accepted 28 December 2022, Available online 4 January 2023, Version of Record 6 January 2023.",https://doi.org/10.1016/j.jmacro.2022.103497,Cited by (0),This paper utilizes recent advances in automated text analysis to investigate whether and how the shifting ,"Since the passage of the Federal Reserve Reform Act of 1977, the U.S. Federal Reserve (Fed) is subject to a dual mandate, stipulating that the Fed’s monetary policy goals are to foster economic conditions that achieve both stable prices and maximum sustainable employment. One of the most significant improvements of the dual mandate was the reaffirmation of the unique role of price stability as an operational objective for U.S. monetary policy (Orphanides, 2006). In fact, chairman Alan Greenspan, whose leadership was dominant (El-Shagi and Jung, 2015), saw price stability as a prerequisite for maximum sustainable employment. However, due the difficulty of defining what constitutes a stable price level, he also concluded that communicating a “specific numerical inflation target would represent an unhelpful and false precision” (Greenspan, 2002 p. 6).====In the early years of the chairmanship of Ben Bernanke, who was an early proponent of inflation targeting (Bernanke et al., 1999, Bernanke and Mishkin, 1997), the views within the Federal Open Market Committee (FOMC) on adopting an inflation targeting framework were still mixed. It was generally recognized that communicating a numerical inflation target can achieve an anchoring of inflation expectations by raising the transparency and accountability of monetary policy. However, the fear of giving up monetary policy discretion and concerns that an inflation target would diminish commitment to the employment goal of the Fed’s mandate were instrumental in preventing its adoption at an earlier stage (Lacker, 2020). To overcome such fears, Bernanke emphasized during the January 2012 FOMC meeting which finally led to a consensus statement on the adoption of an inflation target that “…this statement should not be interpreted as indicating any change in how the Federal Reserve conducts monetary policy. Rather, its purpose is to increase the transparency and predictability of policy” [====, January 25, 2012, p. 1].==== The communication of the inflation target was thus framed as a continuation of the existing practice of the FOMC’s price stability objective.====The inflation target was formulated in the “Statement on Longer-Run Goals and Monetary Policy Strategy”, adopted in January 2012. It is couched in the terminology of flexible inflation targeting and formally defines the FOMC’s inflation target as an annual increase in the headline personal consumption expenditures (PCE) index of 2 percent, to be achieved over the longer run. It also disavows the establishment of a formal target for employment. In the years since 2012, the FOMC announced changes and clarifications to the official inflation target, such as an amendment to its monetary policy strategy in 2016 emphasizing the 2 percent target as a symmetric goal. Following the review of its monetary policy framework, in August 2020 the FOMC amended the consensus statement by stipulating that it henceforth follows a flexible average inflation targeting approach. This approach stipulates that inflation shortfalls in previous years are made up by deliberate inflation overshoots in subsequent years.====In this paper we utilize recent advances in automated text analysis to investigate whether and how the shifting inflation focus in the Fed’s monetary policy strategy is reflected in the transcripts of the FOMC meetings over the course of the last quarter century. The FOMC communication has evolved considerably since 1994, when the Fed released the first statement after an FOMC meeting (Davis and Wynne, 2019). We quantify the FOMC transcripts using the structural topic model (STM) proposed by Roberts et al. (2016). A number of previous studies have analyzed FOMC transcripts using probabilistic topic models (e.g. Bailey and Schonhardt-Bailey, 2008, Schonhardt-Bailey, 2013, Fligstein et al., 2014, Acosta, 2015, Hansen and McMahon, 2016, Hansen et al., 2018). However, none of these papers focuses specifically on inflation topics in the FOMC statements.====We go on to estimate a forward-looking Taylor-type rule augmented by the tone-weighted inflation references obtained from the STM. Previous studies have ascertained that FOMC communication indicators influence inflation expectations and significantly explain target rate decisions in estimated Taylor rules (Blinder et al., 2008, Lucca and Trebbi, 2009, Hayo and Neuenkirch, 2010, Hirose and Kurozumi, 2017, Ernst and Merola, 2018, Romelli and Bennani, 2021). Here we analyze whether the tone of the FOMC’s inflation communication has changed after the switch to the inflation targeting regime and how this has impacted the Fed’s interest rate decisions in our estimates of tone-augmented Taylor-type rules.====Previewing our results, we find that inflation references have surged in the FOMC’s communication long before the adoption of the explicit inflation target, even though the latter became a topic in its own right only in 2012. We assess whether the FOMC’s inflation communication had a direct impact on the monetary policy decisions of the Fed by estimating a Taylor-type rule augmented by the FOMC’s tone-measured inflation references obtained from the STM. We find these to induce quantitatively small but positive level effects on the policy rate and the inflation reaction coefficient of the Taylor-type rule. In this sense, the FOMC communication on inflation reflects the Fed’s monetary policy stance quite closely. However, all these effects already occur prior to 2012, whereas the actual implementation of the inflation targeting framework itself elicits no further monetary tightening. This evidence is in line with the FOMC’s communication framing the implementation of inflation targeting at the time as a continuation of the pursuit of the price stability objective rather than as a genuine change in the Fed’s monetary policy regime.====The remainder of this paper is structured as follows: Section 2 describes the structural topic model and the data set, Section 3 uses the model to quantify the FOMC statements, Section 4 reports on Taylor-type rule estimates incorporating the quantitative indicators obtained from the STM, and Section 5 concludes.",Inflation targeting and inflation communication of the Federal Reserve: Words and deeds,https://www.sciencedirect.com/science/article/pii/S0164070422000908,4 January 2023,2023,Research Article,24.0
"Marshall Emily C.,Saunoris James,Solis-Garcia Mario,Do Trang","Dickinson College, United States of America,Eastern Michigan University, United States of America,Macalester College, United States of America","Received 2 September 2022, Revised 14 December 2022, Accepted 15 December 2022, Available online 24 December 2022, Version of Record 6 January 2023.",https://doi.org/10.1016/j.jmacro.2022.103491,Cited by (0),We use a two-sector dynamic deterministic ==== model that specifically accounts for trends among time-series variables to estimate the size of the ,"Underground (or shadow) economies exist in not only developing countries but also developed countries like the U.S. (Cebula et al., 2014, Géidigh et al., 2016, Schneider, 2011, Tanzi, 1983, Wiseman, 2013a). According to recent (2010 to 2017) estimates from Medina and Schneider (2019), the average size of the shadow economy as a percent of GDP is 14.2% in advanced economies, 27.9% in emerging economies, and 36.3% in low-income countries. Shadow economies create unique challenges for governments as they, for example, distort the allocation of resources, reduce tax revenue collections that result in sub-optimal levels and quality of public goods, and distort official macroeconomic statistics used to formulate policy (Gerxhani, 2004, Schneider and Enste, 2000). However, redeeming qualities of the shadow economy include its ability to provide a safe haven from corrupt government officials, a useful safety-net for those individuals shut out from the formal economy, and essential goods and services at reduced costs (Neuwirth, 2012). For these reasons, policymakers and researchers have been interested in the size and structure of shadow economies. Indeed, understanding shadow economy size is imperative to formulating effective policies to combat shadow activities or assist in their transition to formality.====The activities that are considered part of the shadow economy is a matter of considerable debate (Schneider, 2011). There are many different categorizations of what the OECD (2002) considers the “non-observed” economy. For example, there are activities that are deliberately used to circumvent government taxes and regulations and then there are criminal activities that are prohibited by law. Other activities that are commonly discussed under the umbrella of the non-observed economy include DIY or household production and informal (unregistered) activities. In this paper, we use a definition of the shadow economy that consists of economic activity that escapes government detection and is thus not included in formal GDP estimates (Medina and Schneider, 2019).====Among developed countries like the U.S., the size of the shadow economy is estimated to be about 6% of GDP, which amounts to approximately $1.4 trillion of economic activity that is unrecorded (Medina and Schneider, 2019). However, it is not obvious what is driving all this economic activity underground. In the U.S., shadow economies are spread across states that are inherently heterogeneous. For instance, states vary in terms of dominant industry (e.g., oil in Wyoming and tourism in Hawaii), culture, history (original 13 colonies vs. westward expansion), geography (e.g., bible belt vs. rust belt), climate, etc.—all of which have a disparate influence on the development and spread of the shadow economy. That is, the factors driving the shadow economy in Hawaii will be very different than those driving the shadow economy in Wyoming. Indeed, the media abounds with examples of state-level shadow economies.==== Recent evidence has shown that the COVID-19 pandemic has sparked migration of businesses underground in response to the state-level policies used to combat the spread of the virus.====Therefore, the propensity to participate in the shadow economy is driven not only by federal government actions but also by state and local policies. Indeed, the federalist structure of the U.S. states grants each state considerable autonomy (Craig and Sailors, 1986). For all intents and purposes, states can be viewed as individual nations with independent tax and regulations that have differing influences on shadow development (Goel and Saunoris, 2016). State-level differences in tax rates and tax mixes as well as regulations have important influences on the development and spread of the shadow economy. For the calendar year 2022, effective (state and local) income tax rates ranged from a low of 4.6% in Alaska to a high of 15.9% in New York.==== Indeed, spillovers of shadow activity are induced by tax or regulatory policy changes across states; therefore, tax and regulatory policies need to consider the response of individuals and businesses as they opt to “exit” the state by either migrating to another state or migrating underground (Goel and Saunoris, 2016). In addition, occupational licensure laws have increased substantially in the U.S., particularly in service occupations, which are relatively easy to conduct underground (e.g., barber, cosmetologist, landscape contractor, etc.).====Because the shadow economy evolves at a local level and in response to local conditions, a state-level analysis of the shadow economy is a more relevant unit of analysis relative to a country-level estimate of the U.S. Although several shadow economy estimates for the U.S. have been developed (Tanzi, 1980, Tanzi, 1983), estimates of the size of the state-level shadow economy are less forthcoming. An important exception to this is Wiseman (2013a) who constructs the first comprehensive measure of state-level shadow economies for the 50 U.S. states from 1997 to 2008 using the multiple indicators, multiple causes (MIMIC) method. According to his estimates, Delaware has the smallest shadow economy, on average, estimated at 7.28% of GDP, while Mississippi has the largest shadow economy, on average, estimated at 9.54% of GDP.====In this paper, we propose a new methodology for estimating the state-level shadow economy based on the pioneering work of Solis-Garcia and Xie (2018). Following Solis-Garcia and Xie, 2018, Solis-Garcia and Xie, 2022, we use a two-sector dynamic deterministic general equilibrium (DGE) model of the state-level macroeconomy that specifically accounts for trends observed among time-series variables. To better capture the dynamics of the shadow economy, this methodology allows us to differentiate labor-augmenting technological progress across both sectors of the economy based on deterministic trends in the time-series variables. Based on this methodology we estimate the size of the shadow economy for the 50 U.S. states from 1999 to 2019.====The value added of our paper revolves around the application of the methodology to a sub-national level. Solis-Garcia and Xie (2018, 2022) provide examples at the country level but offer no systematic attempt to characterize the properties or evolution of the shadow economy. Their main goal is to document how economic theory can be used to back out a variable that, by definition, is hard or impossible to measure. In this sense, looking at the U.S. states provides an excellent laboratory to explore the methodology and compare the dynamics of a large number of state-level economies when we are able to guarantee homogeneity in data sources and sample years.====Our measure of the state-level shadow economy provides an improvement on the Wiseman (2013a) measure of the state-level shadow economy in several ways. First, we provide an updated measure of the shadow economy that expands beyond the Great Recession. In particular, the Wiseman (2013a) measure of the state-level shadow economy is from 1997 to 2008, and our estimates extend the data to 2019. Second, different from Wiseman (2013a), we use a DGE model that is grounded in economic theory. Finally, our measure displays richer dynamics by exploiting the trends observed in macroeconomic data. In short, the results show that the average size of the state-level shadow economy is 13% of GDP with a high in Georgia (21%) and a low in Wyoming (5%). Moreover, the estimates of the shadow economy show considerably more variation relative to the current state-level measure of the shadow economy provided by Wiseman (2013a).====A deeper understanding of the size and structure of state-level shadow economies has important policy implications. Because shadow activities are considered socially valuable in that they would be included in GDP estimates had they been reported, it is important to account for their activity when estimating state-level production. In fact, the U.S. Treasury uses state-level GDP estimates to calculate a state’s total taxable resources and to determine the distribution of government grants (Moyer and Thompson, 2017). Universities and consulting firms also rely on correct estimates for state-level GDP (Moyer and Thompson, 2017). Thus, it is important to correctly estimate the amount of production for each state.====The remainder of this paper proceeds with a review of the literature on measures of the shadow economy in the next section. Section 3 outlines the theoretical model. Section 4 describes the data and parameterization. Section 5 discusses applications of the shadow economy measure, and Section 6 concludes.",Measuring the size and dynamics of U.S. state-level shadow economies using a dynamic general equilibrium model with trends,https://www.sciencedirect.com/science/article/pii/S0164070422000842,24 December 2022,2022,Research Article,25.0
"Buffie Edward F.,Adam Christopher,Zanna Luis-Felipe,Kpodar Kangni","Department of Economics, Indiana University, United States of America,Department of International Development, University of Oxford, United Kingdom,Institute for Capacity Development, International Monetary Fund, United States of America,Strategy, Policy, and Review Department, International Monetary Fund, United States of America; FERDI, France","Received 12 June 2022, Revised 10 December 2022, Accepted 16 December 2022, Available online 24 December 2022, Version of Record 29 December 2022.",https://doi.org/10.1016/j.jmacro.2022.103492,Cited by (2),"We analyze the medium-term ==== impact of the Covid-19 pandemic and associated lock-down measures on low-income countries. We focus on the impact of the degradation of health and human capital caused by the pandemic and its aftermath, exploring the trade-offs between rebuilding human capital and the recovery of livelihoods and macroeconomic sustainability. A dynamic general equilibrium model is calibrated to reflect the structural characteristics of vulnerable low-income countries and to replicate key dimensions of the Covid-19 shock. We show that absent significant and sustained external financing, the persistence of loss-of-learning effects on labor productivity is likely to make the post-Covid recovery more attenuated and more expensive than many contemporary analysis suggests.","Since the emergence of the Covid-19 virus in early 2020, a flood of research has sought to examine the economic impact of the pandemic on both advanced economies and those in the developing world. Much of this necessarily focused on the most direct, visible effects of the pandemic, often with the aim of assessing financing needs over the immediate two-to-five year recovery phase (for example ====, ====, ====, ====, ====, ====, ==== and ====).====Less attention has been paid to the implications that will play out over the medium- to long-term as a result of the deep scarring that has been inflicted on health and human capital. There is already abundant evidence that in many developing countries the diversion of healthcare resources to the immediate battle against Covid-19 caused an upsurge in excess mortality from a range of other morbidities including tuberculosis, malaria, diabetes, HIV and heart disease.==== Perhaps more alarming for the long run is the growing evidence of harm to children’s health and education – from school closures and reduced access to vaccinations – and increases in malnutrition, stunting and wasting. There is growing consensus that these assaults on health and education will have devastating long-term effects on human capital (====, ====, ====). According to the World Bank, for example, real GDP could permanently decrease 4 percent in less developed countries if “the human capital destruction and disruption of public infrastructure caused by Covid-19 are not quickly reversed.” (====). In short, the lock-downs and containment measures of 2020–21 mark only the beginning of what promises to be a protracted battle to recover from the social and economic damage wreaked by the pandemic.====The remainder of the paper proceeds as follows. In Section ==== we lay out the model. Section ==== discusses the calibration of the model and the composite Covid-19 shock. Section ==== then presents our core simulation results and discusses their implications, while Section ==== presents our welfare analysis. Section ==== concludes with some implications for policy, both domestic and international.",Loss-of-learning and the post-Covid recovery in low-income countries,https://www.sciencedirect.com/science/article/pii/S0164070422000854,24 December 2022,2022,Research Article,26.0
"Serletis Apostolos,Xu Libo","Department of Economics, University of Calgary, Calgary, Alberta T2N 1N4, Canada,Department of Economics, Lakehead University, Thunder Bay, Ontario P7B 5E1, Canada","Received 22 May 2022, Revised 31 October 2022, Accepted 13 December 2022, Available online 23 December 2022, Version of Record 27 December 2022.",https://doi.org/10.1016/j.jmacro.2022.103490,Cited by (0),This paper uses neoclassical demand theory to calculate the welfare costs of ==== and generates inference in terms of long-run welfare costs of ==== that is internally consistent with the data and models used.,"The recognition that the primary goal of monetary policy should be price stability has led to inflation targeting in advanced economies. Moreover, a large number of emerging and developing economies have switched from exchange rate targeting to inflation targeting, and many other countries are moving toward this monetary policy strategy. Most inflation targeting central banks adopted a 2% inflation target, but during the coronavirus pandemic, the Federal Reserve in the United States switched to a new monetary policy strategy that involves targeting an average inflation rate of 2%. After the adoption of inflation targets, inflation rates declined in most countries around the world. However, in the aftermath of the coronavirus pandemic, inflation rose above 8% in the United States and much higher in many advanced and emerging market economies. This has led to renewed interest on the welfare costs of inflation.====One method for calculating the welfare cost of inflation is based on Bailey (1956). In this approach, the welfare cost of inflation is defined as the change in the area under the inverse money demand curve corresponding to the change in the holdings of real money balances. Based on this approach, there are many estimates of the welfare costs of inflation in the United States. Lucas (2000) reports a welfare cost of inflation of about 1% of real income per year if the annual inflation rate is 10%. However, Ireland (2009) estimates the welfare cost of inflation to be around 0.23% of real income per year (if the annual inflation rate is 10%), which is significantly lower than Lucas’s (2000) estimate. Also, Mogliani and Urga (2018) estimate a substantially lower welfare cost of inflation after 1976, Dai and Serletis (2019) find that the welfare cost of inflation declined significantly (by close to 50%) after the 1980s, and Miller et al. (2019) report estimates in the range of ==== with an average of 0.27%, implying smaller effects than in Lucas (2000) and closer to those in Ireland (2009). More recently, Benati and Nicolini (2021) use data for the United States and several other developed countries and report various estimates depending on the specification used for the money demand function.====In this regard, two money demand specifications have dominated the welfare cost of inflation literature — the semi-log, adapted from Cagan (1956), and the log–log, inspired by Meltzer (1963), with Benati and Nicolini (2021) also using the Selden–Latané specification. Although Benati et al. (2021) derive these specifications with a generalized Baumol–Tobin model and Belongia and Ireland (2019) within Sidrauski’s (1967) framework, in these specifications the demand for money depends only on the interest rate and the calculation of the welfare cost of inflation requires that one simply integrate under a constant elasticity or semi-elasticity money demand curve as a function of the nominal interest rate.====Recently, Serletis and Xu (2021) develop a new approach to measuring the welfare cost of inflation, within the Bailey (1956) consumer surplus framework. Instead of assuming money demand specifications such as the log–log, semi-log, and Selden–Latané forms, they take a microeconomic- and aggregation theoretic approach to the demand for money paying explicit attention to the demand interactions among consumption goods, leisure, and money, as suggested by Abbott and Ashenfelter (1976) and Barnett (1979). They estimate money demand functions in a systems context based on the Normalized Quadratic (NQ) flexible functional form, developed by Diewert and Wales (1988), and evaluate the cost of inflation conditional on the price of consumption goods, the wage rate, and the user costs of monetary assets. They also use the Divisia monetary aggregates, as suggested by Lucas (2000) and also used by Dai and Serletis (2019), and report time-varying welfare costs of inflation which trend upward over time.====In this paper we provide time-varying estimates of the welfare cost of inflation by extending the Serletis and Xu (2021) methodology. Instead of assuming that consumer preferences are fixed, we follow Xu and Serletis (2022) and assume Markov regime switching, allowing for complicated nonlinear dynamics and sudden changes in the parameters of the aggregator function and the money demand function. In particular, we model the NQ expenditure function as a function of an unobserved regime-shift variable, governed by a first-order two-state Markov process, and pay explicit attention to the theoretical regularity conditions of positivity, monotonicity, and curvature. As Barnett (2002, p. 199) put it, without satisfaction of all three theoretical regularity conditions “ ... the second-order conditions for optimizing behavior fail, and duality theory fails. The resulting first-order conditions, demand functions, and supply functions become invalid.”====As in Dai and Serletis (2019) and Serletis and Xu (2021), we are motivated by Heckman and Serletis (2014, p. 1), who argue that “the Federal Reserve Board and many other central banks around the world continue officially to produce and supply low quality monetary statistics, inconsistent with the relevant aggregation and index-number theory”, and use the Center for Financial Stability (CFS) Divisia monetary aggregates. We use the monthly data, from 1967:1 to 2021:9 (a period that also includes the coronavirus recession), make comparisons among the narrow and broad Divisia money measures, and generate inference in terms of welfare cost of inflation estimates that make full use of the relevant economic theory and econometrics.====We use monthly data, as in Serletis and Xu (2021), and Markov regime switching, as in Dai and Serletis (2019) and Xu and Serletis (2022). We find that the demand interactions between goods, leisure, and money are of significant quantitative importance and that the regimes of high and low welfare cost of inflation vary over the choice of the Divisia monetary aggregate. The welfare cost of a 10% inflation rate with our preferred Divisia M4 monetary aggregate is 1.34% of GDP in the high welfare cost of inflation regime, consistent with Serletis and Xu (2021) which reports a welfare cost of 1.40% of GDP using constant parameters. Our estimates suggest that raising the inflation target in the United States, as it was suggested during the global financial crisis and also recently in the aftermath of the coronavirus pandemic, would impose significant costs. These costs should be taken seriously by those who think that raising inflation targets is a good way to deal with issues that arise by missing existing inflation targets.====The paper proceeds as follows. Section 2 discusses the method for calculating the welfare cost of inflation in the tradition of Bailey (1956). It also discusses related neoclassical demand theory, applied consumption analysis, and econometric issues. In Section 3, we present the Markov regime switching NQ money demand function. Section 4 discusses the data and Section 5 presents the empirical results. The final section concludes.","Consumer preferences, the demand for Divisia money, and the welfare costs of inflation",https://www.sciencedirect.com/science/article/pii/S0164070422000830,23 December 2022,2022,Research Article,27.0
Jung Alexander,"European Central Bank, Sonnemannstr. 20, Frankfurt am Main 60314, Germany","Received 19 September 2022, Revised 12 December 2022, Accepted 20 December 2022, Available online 22 December 2022, Version of Record 29 December 2022.",https://doi.org/10.1016/j.jmacro.2022.103494,Cited by (0),"Despite considerable efforts of the ==== (ECB) to support bank intermediation after the 2008 financial crisis, the recovery of euro area banks remained incomplete. Although many studies indicate that central banks can influence the stock prices of firms through their policy actions and communication, a knowledge gap exists as to whether the ECB's ==== can influence bank health. Through a high-frequency identification approach, this study reveals that the causal effect of conventional ==== action and communication by the ECB on bank stock prices differed over time, whereas its influence on bank financing costs was robust. This study provides new evidence showing that information effects related to policy easing surprises in the aftermath of the 2008 financial crisis hampered the ECB efforts to improve bank health and that its Odyssean communication signals (related to forward-looking announcements of policy easing) supported bank health during this phase. Local projections suggest that the response of banks to monetary policy shocks displayed some persistence, where ECB policy surprises and communications that shifted up (down) the yield curve were normally positive (negative) for bank health. The findings solicit a new perspective when assessing the influence of the ECB's monetary policy measures on euro area banks.","After the 2008 financial crisis, the European Central Bank (ECB) reduced its policy rates to historic lows and introduced a host of non-standard monetary policy measures to support bank intermediation and the economy. Its macroprudential policies were also supportive, but the recovery of euro area banks remained incomplete. Fig. 1a shows that the stock market performance of the European banking sector remained subdued, despite record low bank funding costs. A popular explanation for the performance problems of European banks is that structural factors related to margin- and cost-pressure hampered their intermediation activities (Simoens and Vander Vennet, 2021). Another explanation is that maintaining low interest rates for an extended period created problems for the banking sector, as has been regularly suggested in banks’ responses to the euro area bank-lending survey (ECB, 2022). Low interest rate levels limit the scope for maturity transformation activities and hamper bank profitability by compressing the net interest rate margin (Borio et al., 2017). A risk-taking channel, if present, implies that monetary policy easing increases credit risk and makes banks more vulnerable to shocks (Altunbas et al., 2014; Jiménez et al., 2012, 2014; Maddaloni and Peydró, 2011). Fig. 1b suggests that after 2008, in line with this channel, monetary policy easing by the ECB was associated with higher credit risk of euro area banks.====This paper is concerned with identifying the effects of the ECB's monetary policy actions and communications on bank health and aims to answer two questions. Are monetary policy effects on banks causal and has the monetary policy transmission through banks changed in the aftermath of the financial crisis? Several event studies demonstrated that monetary policy easing surprises reflecting conventional monetary policy action positively impacted banks’ stock prices (Altavilla et al., 2019b; Ampudia and van den Heuvel, 2018; English et al., 2018). There is ample evidence that the monetary policy transmission to banks was strongly affected by the crisis, and that this effect was amplified in a bank-based system with euro area banks being the main credit intermediaries (Constâncio, 2018; Jannsen et al., 2019; Mongelli and Camba-Mendez, 2018). Meanwhile, recent studies suggest that the pass-through of ECB interest rates to lending rates remained effective close to the zero lower bound of nominal policy rates (Altavilla et al., 2022). Moreover, the targeted longer-term refinancing operations (TLTROs), which the ECB started in June 2014, preserved favorable funding conditions for banks and stimulated bank lending to the real economy (Andreeva and García-Posada, 2021).====This study examines the causal link between monetary policy and bank health indicators through a high-frequency identification (HFI) approach (Cochrane and Piazzesi, 2002; Cook and Hahn, 1989; Kuttner, 2001) and local projections (Jordà, 2005; Miranda-Agrippino and Ricco, 2021) by exploiting banks’ responses to monetary policy shocks following policy announcements by the ECB. The empirical analysis considers the period from January 2002 to April 2017. This study uses high-frequency monetary policy shocks from other researchers applying state-of-the-art techniques (Altavilla et al., 2019b; Andrade and Ferroni, 2021; Jarociński and Karadi, 2020) (Fig. 2, Fig. 3, Fig. 4). These monetary policy shocks have been extracted from tight windows around policy announcements, which are not distorted by other news (Nakamura and Steinsson, 2018) and have an economic interpretation.====The study distinguishes between monetary policy effects on banks owing to ECB action and communication. The distinction matters for the assessment of the transmission to banks because earlier event studies suggested that the unexpected and not actual policy rate changes and related communications by central banks influence financial asset prices (Bernanke and Kuttner, 2005; Gürkaynak et al., 2005; Kuttner, 2001). Changes in asset prices impact both sides of banks’ balance sheets and thus influence bank's profitability (Altavilla et al., 2019a; Borio et al., 2017). Monetary policy announcements are multi-dimensional, and factors extracted from monetary policy surprises have an economic interpretation, if those factors are orthogonal (Gürkaynak et al., 2005, 2007; Gürkaynak, 2005). This study uses the factor decomposition by Altavilla et al. (2019b) for the euro area as a starting point. It identifies four factors related to policy action, the timing between two policy meetings, communication on future interest rates, and large-scale asset purchases.====The recent literature on central bank information effects (Hansen et al., 2019; Nakamura and Steinsson, 2018; Romer and Romer, 2000) discovered that monetary policy surprises not only reflect a monetary policy shock but also new information about economic fundamentals. Information effects, which arise if economic uncertainty is high and markets perceive a credible central bank to have superior information relative to markets, have been a separate channel of monetary policy by which the public sector revises its expectations of the economic outlook and the implied monetary policy response (Bauer and Swanson, 2020). To examine the information effects on banks, the decomposition by Jarociński and Karadi (2020), who identify “pure monetary policy shocks” and “information shocks” for the euro area, is used.====In a high uncertainty environment, public information may have crowded out private information (Morris and Shin, 2005), as was the case in the aftermath of the 2008 financial crisis, when markets continued to perceive the ECB's central bank communication as credible. Although purely qualitative forward guidance (FG) does little to anchor expectations of future policy rates (Campbell, 2019), the publication of an interest rate path may enhance the central bank's leverage on the medium-term structure of interest rates (Andersson and Hofmann, 2009). The present study considers that the ECB's FG has evolved over the last two decades. In the initial years of the ECB's existence, code words were used in the introductory statement and speeches. These signals did not create full certainty for markets about forthcoming interest rates but left ample scope for macroeconomic uncertainty to influence policy decisions (Heinemann and Ullrich, 2007). Between July 2013 and 2022, the ECB provided systematic FG about the path of future policy rates during the press conference, thus strongly influencing expectations and asset prices (Hubert and Labondance, 2018; Moessner and Rungcharoenkitkul, 2019) and lowering the informativeness of market signals simultaneously (Ehrmann et al., 2019). To analyze the effects of different types of FG on banks, this study employs the factor decomposition by Andrade and Ferroni (2021), which identifies Delphic shocks related to news on future macroeconomic conditions and Odyssean shocks related to future euro area monetary policy measures.====Does a single measure for the health of banks exist? As illustrated by earlier studies, bank stock prices are useful to examine the transmission of monetary policy shocks to banks, especially in efficient markets when stock prices reflect all publicly available information about the state of a bank. Institutional assessments by bank supervisors and international organizations consider a host of measures to assess banks’ financial health (ECB, 2018; McGuire and Tarashev, 2008). Chodorow-Reich (2014) uses the quantity of bank lending at different banks as a proxy for bank health and suggests that bank's balance sheet data can provide additional insights on bank health. This study relies on a broad range of market-based measures for which data at (intraday-/) daily frequency are available:==== stock prices, price-to-book (P/B) ratios, banks’ net interest rates margin, bank bond yields, distance to default (DTD), and expected default frequency (EDF), and credit default swaps (CDS) spreads. Based on a selection of these market-based indicators and data on loan quantities (and website search activity for the keyword “credit”), a novel composite measure of bank health was constructed by applying principal component analysis.====This study contributes to the literature by providing new evidence of monetary policy shocks having a causal impact on banks. Our main finding is that the causal effect of conventional monetary policy action and communication by the ECB on bank stock prices differed over time, whereas its influence on bank financing costs was robust. It provides new evidence showing that information effects related to policy easing surprises in the aftermath of the 2008 financial crisis hampered the ECB's efforts to improve bank health and that its Odyssean communication signals (related to forward-looking announcements of policy easing) supported bank health during this phase. Through (Bayesian) local projections, this study showed that the response of bank indicators to monetary policy shocks displayed some persistence, where ECB's policy surprises and communications that shifted up (down) the yield curve were normally positive (negative) for bank health. In addition, it develops novel metrics to measure bank health at high frequency.====This study is structured as follows. Section 2 explains the HFI method and data. Section 3 presents and discusses the empirical results on the causal effect of monetary policy shocks on bank health. Section 4 presents the results of some robustness checks and (Bayesian) local projections. Section 5 concludes.",Are monetary policy shocks causal to bank health? Evidence from the euro area,https://www.sciencedirect.com/science/article/pii/S0164070422000878,22 December 2022,2022,Research Article,28.0
"Pinter Julien,Pourroy Marc","University of Minho, NIPE, Portugal,Université de Poitiers, CRIEF, Department of Economics, France","Received 24 November 2022, Revised 8 December 2022, Accepted 16 December 2022, Available online 22 December 2022, Version of Record 26 December 2022.",https://doi.org/10.1016/j.jmacro.2022.103493,Cited by (0), policy appears as a potential key driver of this decision.,"The conventional assumption has often been that a straightforward limit for central banks fighting against appreciation pressure through foreign exchange interventions was excessive inflation.==== However, recent experiences such as the Swiss franc peg suggest that other limits exist and can be of greater relevance in practice. In January 2015, the Swiss National Bank (SNB) ended the peg policy it had put in place vis-à-vis the euro in 2011 to prevent its currency from appreciating and address deflationary pressures, although inflation was still expected to stay well below the central bank target.==== This episode, marked by a dramatic appreciation of the Swiss franc (Fig. 1), was later referred to as the “Frankenshock” as it let many observers perplexed and without any conclusive explanation.==== Some economists proposed an explanation for this move based on the fear of financial losses (Cecchetti and Schoenholtz, 2015, Hall and Reis, 2015, Krugman, 2015, Amador et al., 2016). However, no framework explains how such concerns can weigh on a central bank decision to exit a peg together with standard decision factors, and no compelling empirical study has been carried out to see whether these could have been relevant in practice for the SNB. This paper intends to fill these gaps.====In this paper, we take a deeper look at the financial constraints faced by central banks defending a peg against appreciation. We consider a traditional inflation-averse and independent central bank in a small open economy implementing a currency peg, such as the SNB in 2011. A key assumption we make is that the peg is officially set as a temporary tool, as was the case for the SNB in practice (Wyplosz, 2015). We also assume that the central bank faces appreciation pressure on its currency. These assumptions imply, in turn, that the central bank has to end its peg at a future date and that foreign exchange reserves are not decreasing during the peg. Under such a framework, we build a model capable of tracking the evolution of the central bank balance sheet and study the relevance of central bank financial loss concerns. We focus on the financial loss arising when the central bank breaks the peg, which ensues from the appreciation of the domestic currency and the corresponding valuation losses on foreign assets. The prospect of such a loss may provide the central bank with an incentive to exit a peg earlier than it would have done absent such concerns: we show under which conditions this applies for a traditional inflation-averse central bank. Two well-known thresholds are considered for the maximum loss a central bank may bear: the central bank equity level and the central bank insolvency threshold. Breaching the first threshold entails political and credibility risks (Jeanne and Svensson, 2007, Cukierman, 2011), while going insolvent implies a loss of control over inflation (Reis, 2013). We then develop a model of exchange market pressure suitable for a peg and consistent with the new central banking framework that is able to forecast the amount of foreign exchange interventions a central bank should carry out to maintain a peg. The model enables us to forecast the domestic effect of unconventional monetary polices in the foreign economy and the effect of changes in the perceived peg credibility. We use the model to predict the loss the central bank might incur when breaking its peg policy at a future date. Finally, we demonstrate how the awareness of such loss concerns among rational speculators can force a central bank to take action prematurely.====Applying our model for the Swiss franc peg case, we predict the SNB foreign reserves holdings and financial loss for various dates if the central bank had broken its peg later than it did. We account for two risks that the SNB may have considered: the risk that the European Central Bank (ECB) implements a Quantitative Easing policy (====)==== (modeled through a shock on long-term interest rates) and the risk of a sharp increase in global financial uncertainty (modeled through a shock on the VIX). The simulations show that the materialization of ECB ==== would have been particularly problematic for the Swiss franc peg. It could have made a late regime shift result in a negative equity position for the SNB, for potentially many years. In a scenario accounting for speculators, and in which the peg credibility is impaired by the ==== announcement, breaking in April instead of January 2015 would have depleted the central bank of its equity. The central bank balance sheet would have more than doubled by then. Some alternative parameterizations indicate that breaking the peg after ==== is announced would have been too late, consistent with the actual timing of the decision. We find that central bank solvency was clearly not at risk, in contrast to what Hall and Reis (2015) assert.====Our work adds to the literature on the sustainability of foreign exchange interventions. This topic has been widely studied for the case of depreciating pressures, where a clear limit arises from the amount of foreign reserves held by the central bank (see Krugman (1979), e.g., among this vast literature). However, in the case of appreciation pressures, few studies have been conducted.==== A straightforward explanation can be found in Krugman (1979), where the author shares the view that a natural limit for policies such as the ones consisting in buying foreign exchange reserves is inflation. However, these concerns are not always valid under the current central banking framework, where most central banks choose to pay interest rates on reserves to control interest rates with a large balance sheet or simply face the zero lower bound.==== In that respect, our work adds to the currency pegs literature: we show how a central bank’s ability to defend its currency against appreciation pressure with foreign exchange interventions can become constrained even in the current world.====On the theoretical side, our paper is close to Amador et al. (2016). The authors present a framework in which a central bank solely averse to financial losses (broadly defined) has an incentive to abandon a peg when the demand for its base money incurs permanent shocks. They then illustrate how their model can explain a move such as the one of the SNB. Our theoretical approach differs from Amador et al. (2016) on several aspects; a key difference is that we consider speculative agents aware of the central bank decision rule in our model, which we show greatly matters.==== On the empirical side, our studies are not truly comparable. The numerical analysis of Amador et al. (2016) is intended to provide an illustration of their model, while ours aims at precisely estimating whether the loss the SNB may have forecast could have exceeded theoretically relevant thresholds.====Our work is also related to the many studies on central bank finances that arose after the global financial crisis. We borrow on several aspects from Hall and Reis (2015) and Reis (2013) to model the central bank balance sheet, and our counterfactual analysis of the SNB loss is quite similar in essence to the analysis of Carpenter et al. (2015) or Greenlaw et al. (2013) for the Federal Reserve Bank (FeD). We introduce new elements in this central bank balance sheet framework from the literature on exchange market pressure models. The peg exchange market pressure model we build captures several aspects of the current central banking reality: it takes into account not only short-term but also long-term interest rate differentials (and thus ==== effects), as well as financial market expectations of the break of a peg.====The reminder of this paper is organized as follows. Section 2 presents the model’s theoretical underpinnings. Section 3 analyzes the optimal decision of the central bank in the presence of loss concerns. Section 4 develops a peg exchange market pressure model, while Section 5 includes speculators to the analysis. In Section 6 we perform our counterfactual estimates of the SNB loss.",How can financial constraints force a central bank to exit a currency peg? An application to the Swiss franc peg,https://www.sciencedirect.com/science/article/pii/S0164070422000866,22 December 2022,2022,Research Article,29.0
"Kim Hyeongwoo,Shao Peng,Zhang Shuwei","Department of Economics, Auburn University, 138 Miller Hall, Auburn, AL 36849, United States of America,Department of Economics, Auburn University, 129 Miller Hall, Auburn, AL 36849, United States of America,Department of Economics, 102 Hepburn, 23 Romoda Dr., St., Lawrence University, Canton, NY 13617, United States of America","Received 4 May 2022, Revised 31 October 2022, Accepted 7 December 2022, Available online 19 December 2022, Version of Record 27 December 2022.",https://doi.org/10.1016/j.jmacro.2022.103489,Cited by (1),"This paper shows that government spending shocks in the U.S. has become ineffective due to lack of coordination between monetary and fiscal policies. Employing the post-war U.S. data, we report strong stimulus effects of fiscal policy during the pre-Volcker era, which rapidly dissipate as the sample period is shifted toward the post-Volcker era. We explain the causes of this phenomena via a sentiment channel. Employing the Survey of Professional Forecasters data, we show that forecasters tend to systematically overestimate real GDP growth in response to positive innovations in government spending when policies coordinate well with each other. On the other hand, they are likely to underestimate real GDP responses when the monetary authority maintains a hawkish stance that conflicts with the ","We often observe sluggish economic recovery from recessions that naturally brought in heated debates on the effectiveness of fiscal policy in stimulating private activity. One group of researchers reports significantly positive output effects of fiscal stimulus, which can be consistent with the New Keynesian (NK) macroeconomic model. However, such effects could be replicated only in heavily restricted models. See, among others, Galí et al., 2007, Devereux et al., 1996, Fatás and Mihov, 2001, Blanchard and Perotti, 2002, Perotti, 2011 and Rotemberg and Woodford (1992).====Many others, on the other hand, are skeptical about the effectiveness of fiscal policy. For instance, Ramey (2011) points out that expansionary government spending shocks tend to decrease consumption due to a negative wealth effect. See also, among others, Aiyagari et al., 1992, Hall, 1986, Owyang et al., 2013, Edelberg et al., 1999, Burnside et al., 2004, Cavallo, 2005, Mountford and Uhlig, 2009, Ramey, 2012 and Ramey and Shapiro (1998).====Another interesting question is whether fiscal stimulus spurs economic activity only during economic downturns. For example, Fazzari et al., 2015, Auerbach and Gorodnichenko, 2012, Albertini et al., 2021, Bachmann and Sims, 2012, Bernardini et al., 2020, and Mittnik and Semmler (2012) claim that fiscal policy tends to have a stronger output effect during times of slack, whereas Jia et al., 2022, Owyang et al., 2013 and Ramey and Zubairy (2018) find no such evidence. Barnichon et al. (2022) also propose a rather depressing nonlinear (sign-dependent) effect of government spending shocks, showing that unexpected decreases in government spending tend to generate more substantial (negative) impacts than positive shocks regardless of the state of the cycle.====Christiano et al. (2011) and Hall (2009) suggest that the government spending multiplier can be greater when the nominal interest rate is bounded at zero. Jo and Zubairy (2021) show that the government spending multiplier is larger in a low inflation recession than in a high inflation recession. Ghassibe and Zanetti (2020) linked the state dependence of fiscal multipliers to the source of the business cycle. Employing large panel of international data, Corsetti et al., 2012, Born et al., 2019 and Ilzetzki et al. (2013) report strong evidence of important roles of country characteristics such as the exchange rate regime and public indebtedness in determining the effectiveness of fiscal stimulus.====In their recent work, Leeper et al. (2017) proposed an interesting theoretical framework that generates substantially weaker responses of private spending to expansionary fiscal policy shocks in an active monetary/passive fiscal policy regime (Regime M) than in a passive monetary/active fiscal policy regime (Regime F).==== This paper employs a similar New Keynesian DSGE model but with alternative identification strategies of the regimes. We note that the federal government had a budget deficit in 79 times out of 93 years from 1929 to 2021, which is about 85% odds.==== In the absence of compelling empirical evidence of contractionary fiscal policy, we focus on the stance of monetary policy that tends to change over time given the expansionary stance of fiscal policy: (1) a dovish monetary policy coordinated well with an expansionary fiscal policy regime (Regime D); (2) a hawkish monetary policy conflicted with an expansionary fiscal policy regime (Regime H).====In what follows, our simulation results demonstrate that private spending positively responds to the government spending shock in regime D, whereas it responds negatively in regime H, resulting in substantially weaker stimulating effects on the total output. Also, our benchmark model shows that the real interest rate plays a key role in generating qualitatively different output effects across the two regimes.====We are primarily interested in investigating the empirical validity of these theoretical predictions. Employing the post-war U.S. macroeconomic data, we report strong evidence of the time-varying effectiveness of fiscal stimulus with a possibility of structural breaks in the propagation mechanism of the government spending shock across time. Specifically, we observed strong effects of government spending on boosting private economic activity in earlier sample periods when the Fed stayed accommodative, while government spending shocks tend to discourage economic activity in the private sector when the Fed shifted to a hawkish stance which may conflict with expansionary fiscal policy. These findings are further justified via a Bayesian approach (time-varying coefficient VAR with stochastic volatility) and counterfactual simulation exercises.====Although these findings are overall consistent with the predictions of our proposed NK model, we notice a negligibly weak role of the real interest rate in propagating fiscal stimulus to economic activity. To resolve this issue, we propose an alternative explanation for the observed time-varying output effects of fiscal policy shocks using a sentiment channel.====It should be noted, however, that we are not the first to introduce the role of sentiment as one of potential drivers of macroeconomic fluctuations or as the instrument to evaluate the stimulus effects of government policies. There is a large and fast-growing literature. Hall, 1993, Blanchard, 1993, Cochrane, 1994, Carroll et al., 1994, Barsky and Sims, 2012, Beaudry and Portier, 2006, Beaudry and Portier, 2007, Akerlof and Shiller, 2010, and Ludvigson (2004) studied the relationship between consumer confidence and consumption expenditures, while Jia et al., 2022, Bachmann and Sims, 2012, and Konstantinou and Tagkalakis (2011) focused on how consumer confidence affects the effectiveness of fiscal policy.====Using the recent experience during the Covid-19 pandemic crisis, Georgarakos and Kenny (2022) find that consumers perceptions that are related to the effectiveness of fiscal polices have a causal effect on consumer spending. An array of recent researches by George-Marios Angeletos are closely related with such views, both theoretically and empirically, in the sense that aggregate demand shocks may cause business cycle fluctuations under the bounded rationality. See among others, Angeletos and Lian, 2018, Angeletos et al., 2020 and Angeletos and Lian, 2022a, Angeletos and Lian, 2022b. For example, Angeletos and Lian (2022a) introduce a feedback loop between real economic activity and consumer expectations that amplifies the business cycle fluctuations that are caused by demand shocks, suggesting the concept of the confidence multiplier. See also Farhi and Werning (2019) and Gabaix (2020) for the policy propagation channel analysis under the New Keynesian models with the bounded rationality.====We address the responses of sentiment to changes in government spending by investigating how market participants revise their economic prospects when they receive new information on the stance of fiscal policy through the lens of the Survey of Professional Forecasters (SPF) data. We show that forecasters tend to over-predict GDP growth when monetary policy coordinates well with fiscal policy, while systemic under-predictions are likely to occur when the Fed adopts a hawkish stance. We view persistent over-predictions as a sign of optimism, while under-predictions reflect pessimistic economic prospects in the market.====We further investigate this conjecture by regressing five-quarter (longer-run) ahead forecasts of real GDP growth on one-quarter (short-run) ahead forecasts of real government spending growth employing a fixed-size rolling window scheme. Results reveal strong positive correlations (optimism) for the pre-Volcker era, while we observe negative correlations (pessimism) when the stance of monetary policy became hawkish. These findings imply that time-varying responses in sentiment may explain the time-varying effectiveness of fiscal policy on private spending. In regime D, fiscal stimulus generates consumer optimism, which boosts economic activity in the private sector. In regime H, however, it generates consumer pessimism, resulting in subsequent decreases in private spending, which ultimately weakens the effectiveness of fiscal policy. We also provide statistical evidence in favor of such views employing structural break tests by Hansen, 1997, Hansen, 2001 and Bai and Perron (2003).====Leeper et al. (2017) also show that fiscal policy can be less effective when the monetary authority stays hawkish. However, their contributions are mostly theoretical because their major findings are based on counterfactual analyses using the full sample period data. On the contrary, we provide historical evidence of the time-varying effects of fiscal stimulus for the post-war U.S. data. Furthermore, we suggest a sentiment channel as an alternative propagation mechanism to the real interest rate channel to explain the output effects of fiscal policy under different policy regimes.====Perotti (2005) suggests similar evidence that fiscal policy became less effective in more recent sample periods using macroeconomic data from 5 OECD countries including the U.S. However, he fails to provide convincing explanations what caused such changes. Bilbiie et al. (2008) also report time-varying effects of fiscal stimulus, but they focus more on the role of different feedback rules of government spending similar to the work of Leeper et al. (2017). They suggest that financial market deregulation made it possible for households to smooth consumption, which makes fiscal policy less effective.==== It seems, however, that these arguments are at odds with the data. In fact, saving rates have substantially declined since the 1980’s when deregulation began in the U.S.====The remainder of this paper is organized as follows. Section 2 reports simulation results that highlight qualitatively different output effects of fiscal stimulus across regimes based on our New Keynesian model in the Appendix. Section 3 presents our empirical models along with data descriptions. We demonstrate time-varying responses of our key economic variables to government spending shocks via both the benchmark frequentist models as well as a Bayesian approach. We report evidence against an important role of the real interest rate in propagating fiscal stimulus over time. We then discuss a possibility of the existence of a sentiment channel as an alternative explanation of which the importance of its role is confirmed by counterfactual simulation exercise. Employing the SPF data, Section 4 provides a novel statistical approach that extracts useful information on how market participants revise their economic prospects when they receive new information on government spending. We show market agents become more optimistic in regime D in response to the government spending shock, while they become pessimistic in regime H, which helps explain weaker output effects of fiscal policy in regime H. Section 5 concludes.",Policy coordination and the effectiveness of fiscal stimulus,https://www.sciencedirect.com/science/article/pii/S0164070422000829,19 December 2022,2022,Research Article,30.0
"Arawatari Ryo,Hori Takeo,Mino Kazuo","Faculty of Economics, Doshisha University, Karasuma-higashi-iru, Imadegawa-dori, Kamigyo-ku, Kyoto-shi 602-8580, Japan,Department of Industrial Engineering and Economics, School of Engineering, Tokyo Institute of Technology, 2-12-1, Ookayama, Meguro-ku, Tokyo, 152-8552, Japan,Institute of Economic Research, Kyoto University, Yoshida Honmachi, Sakyo-ku, Kyoto 606-8501, Japan","Received 11 July 2022, Revised 11 November 2022, Accepted 24 November 2022, Available online 6 December 2022, Version of Record 18 December 2022.",https://doi.org/10.1016/j.jmacro.2022.103486,Cited by (1),"This study examines the relationship between productive ==== and economic growth. An R&D-based model of ==== is used, in which agents have heterogeneous entrepreneurial abilities. We show that if the number of high-ability entrepreneurs is non-negligible, then the relationship between the government expenditure/GDP ratio and the economic growth rate is depicted by an inverted U-shaped curve with a flat top. The flat top of the curve indicates that changes in the size of the ==== have a limited impact on growth. We calibrate the model using U.S. data and empirically confirm our theoretical predictions. The theoretical and numerical results suggest that the debate on the relationship between the size of the government and economic growth may be off the mark unless the size of the government is extremely large or small.","Does the size of the government expenditure affect long-term growth? What is the optimal level of government spending that maximizes growth? Economists have long discussed these questions through a substantial amount of accumulated research. Since the 2008 European sovereign debt crisis, this topic has been at the center of the debate on the fiscal policy. Moreover, fiscal authorities in many countries have substantially expanded their expenditures to cope with the worldwide recession caused by the COVID-19 pandemic. Consequently, the linkage between the size of the government spending and economic growth has attracted a renewed attention.====However, the empirical debate on the nexus between the size of the government and economic growth has not yet been resolved. The empirical studies that have examined the relationship between the size of the government expenditure and economic growth are divided into two groups.==== The studies in the first group suggest that government expenditure has a significant positive or negative effect on long-run economic growth.==== Interestingly, however, other studies suggest that there is a non-significant relationship between the size of government and long-run economic growth.====Theoretical studies have not provided an explanation for the mixed findings of the empirical studies. The relationship between the size of the government spending and economic growth has been discussed in the endogenous growth literature. The endogenous growth models have demonstrated the possibility of an inverted U-shaped relationship.==== These models provide insights into whether the relationship is positive or negative. However, they are inconsistent with the empirical studies that suggest an insignificant relationship between the two. Meanwhile, neoclassical growth models and semi-endogenous growth models have reached conflicting conclusions; these models imply that long-run growth is independent of the size of the government spending.==== These models can explain the insignificant relationship, but do not provide any insights into whether the relationship is positive or negative. In addition, it is clearly counterintuitive to assume that long-term growth rates would remain unchanged even if government spending was zero, which would suggest that infrastructure services were not being provided, or even if the government spent all of the national income by setting a 100% income tax.====Thus, the theoretical challenge is to construct a model that reconciles with the diverse findings of the empirical studies with the intuitions mentioned above. This study aims to present such a model.====This study’s model is based on Romer’s (1990) endogenous growth model and its key feature is that the agents have heterogeneous R&D abilities. In this model, R&D activities that increase various intermediate goods drive the long-term growth of the economy. The final good is produced using a continuum of intermediate goods, and productive government spending is financed contemporaneously by a flat-rate capital income tax. Our analysis also builds on the approaches of Jaimovich and Rebelo (2017) and Arawatari et al. (2018). We assume that agents have heterogeneous R&D abilities with an endogenously determined cut-off level. Agents whose abilities are below the cut-off level disregard innovation as s result, become workers.====Fig. 1 illustrates the main findings of this study.==== The solid and dashed lines illustrate the relationship between the government expenditure/GDP ratio and the economic growth rate in the heterogeneous- and homogeneous-ability economies, respectively. Both the graphs differ numerically, although they show an inverted U-shaped relationship, as in endogenous growth models. This inverted U-shaped curve exhibits a ==== in the presence of heterogeneity. The flat top illustrates that a change in the size of the government has a limited impact on growth, as in semi-endogenous growth models. This result suggests that heterogeneity may be the source of the mixed results of the previous empirical studies on the relationship between the size of the government and long-term economic growth. We analytically derive the ==== (Fig. 1 shows the calibration results). Thus, the ability to generate an ==== under heterogeneous abilities can be easily explained.====Similar to Barro’s (1990) endogenous growth model, in our model, a change in government spending has two opposing effects on economic growth. First, high government expenditure increases monopolistic profits, and thus, stimulates the entry of intermediate-good firms, indicating a positive effect on the long-term growth rate. Second, high government spending indicates a high tax rate, depressing the net benefit of R&D and indicating a negative effect on the long-term growth rate. Therefore, our model generates an inverted U-shaped relationship between the government expenditure/GDP ratio (size of the government) and economic growth. In addition, the inverted U-shaped relationship holds, regardless of the presence or absence of heterogeneity in ability.====If we assume that agents are homogeneous, then the positive or negative impact of government expenditure uniformly affects the R&D incentives. Consequently, the size of the government and economic growth have a normal inverted U-shaped relationship. If agents have heterogeneous R&D abilities, then the government expenditure size non-uniformly affects their occupational choice. When the government expenditure/GDP ratio is sufficiently low or high, R&D’s net benefit is less, and only the high-ability agents becoming entrepreneurs. Therefore, changes in the cut-off level affect the occupational choice of the high-ability agents. Given that this impact is relatively large, a change in the size of the government has a significant impact on economic growth, as in the endogenous growth models. By contrast, when the government expenditure/GDP ratio is moderate, a change in the size of the government generates occupational changes for the low-ability agents. Hence, the impact on the growth of the economy is ambiguous, as in the case of the semi-endogenous growth models. Therefore, an ==== explains all the three relationship types: positive, negative, and ambiguous.====We calibrate the model to U.S. data and empirically confirm our analysis. Assuming that the entrepreneurial ability follows a truncated Pareto distribution, the simulation performs an ==== between the government expenditure/GDP ratio and an economic growth rate under the plausible parameter values. An ambiguous relationship exists if the government expenditure/GDP ratio is approximately between 2% and 20%. However, the long-term growth rate significantly increases or decreases when the government expenditure/GDP ratio is outside this range. The U.S. average general government final consumption expenditure (% of GDP) is approximately 15%. Thus, our numerical example suggests that the U.S. economy is on the flat top of the inverted U-shaped curve (see Fig. 1). Furthermore, a small change in the size of the government is not important to the U.S. economy from an economic growth perspective.====Our theoretical and numerical results provide a new perspective on optimal size of the government. Fig. 1 shows that the correlation between the government expenditure/GDP ratio and the economic growth rate in an economy of heterogeneous abilities is not prominent when the government expenditure/GDP ratio is moderate. This finding implies that the debate on the size of the government is not significant to the economic growth rate, unless the size of the government is extremely large or small.",Government expenditure and economic growth: A heterogeneous-agents approach,https://www.sciencedirect.com/science/article/pii/S0164070422000799,6 December 2022,2022,Research Article,31.0
"Lucidi Francesco Simone,Semmler Willi","Sapienza University, Rome, Italy,New School for Social Research, NY, United States of America,University of Bielefeld, Germany","Received 27 April 2022, Revised 15 September 2022, Accepted 28 November 2022, Available online 6 December 2022, Version of Record 13 December 2022.",https://doi.org/10.1016/j.jmacro.2022.103487,Cited by (0),", ==== (UMP) – such as Quantitative Easing – mitigates the output decline in both the short and the long run. Second, a zero natural ==== curtails the central bank’s ability to adjust the economy and mitigate scars. Third, financial constraints leave the deepest scars even if UMP is active.","Over the last few years, economists focused on the long-run effects of persistent shocks on economic output. Fig. 1 shows the Euro-Area (EA) real GDP and its time trend before and after the global financial crisis; from that period onward, the European economy never reverted to its time trend (red line). The subsequent GDP slacks, one related to the sovereign crisis and the other to the Covid-19 shock – of an unprecedented magnitude – appear both in line with a new time trend (blue line). Despite the recent recovery phase, the upcoming downside effects of the energy crisis – exacerbated by the Russian–Ukrainian conflict in 2022 – are still invisible. Therefore, the adjustment path of the output might be slower than the one expected just a few months ago. This paper focuses on the features contributing to such long-run scars and what central banks can do about them.====The conventional wisdom on long-run scars is close to the classic view on the hysteresis of Clark (1989) – recently reviewed in Cerra et al. (2020). In this view, the “dynamics of the trend react to the cycle and persistence can then be seen as the scars left by recessions”. Persistent shocks can change the macroeconomic trend, and hysteresis is a path-dependent steady state. That implies that the dynamic system exhibits at least one eigenvalue equal to zero (unity, if specified in discrete time). Thus, the steady-state equilibrium depends on the shocks’ history.====From an empirical viewpoint, the scarring effects are long-run echo effects from sudden meltdowns, such as natural disasters, destruction from wars, or large-scale pandemic disease, as in the study of Jordà et al. (2022). The latter leaves – as opposed to war destruction – the productive capacity and capital stock intact but affects employment and labor income. In the empirical results of Jordà et al. (2022), a pandemic meltdown has an echo effect, producing a negative real natural rate even after eight to ten years, which further decreases up to eighteen to twenty years. Jordà et al. (2020) finds that monetary policy too may influence the output dynamics in the long run. However, scarring effects may also arise after large financial shocks if several financial constraints are at play. For example, the scarring effects of the housing crisis have been explored recently by Bhattarai et al. (2021).====In this paper, we reproduce long-run scars by highlighting those features that the economic literature has identified. In particular, we use a small-scale macroeconomic model to show how the macroeconomy adjusts to a deep contraction and how credit dynamics, along with the monetary policy design, may influence the extent of these scars through numerical simulations. We emphasize that the scars result from the nonlinearities characterizing our economic system.====Until the Great Recession, economists had primarily used form of macroeconomic models characterized by a Phillips-curve, output gap dynamics, and a Taylor rule for the conduct of monetary policy. The Linear Quadratic approach (LQ) has been interpreted as an approximation of general equilibrium models (Woodford, 2001) and the New Keynesian version of the Dynamic Stochastic General Equilibrium (DSGE) model (Galí, 2008). However, after the global financial crisis, many economists recognized that the nonlinear effects of credit flows and financial conditions are crucial, although missing features in linear monetary policy models (Mishkin, 2011). Researchers then focused on adding the financial sector components, such as asset prices and financial intermediaries, like in Brunnermeier and Sannikov (2014) and Gertler and Karadi (2015). In the same spirit, we introduce an extended inflation targeting model that includes a dynamic equation for credit flows and financial conditions. That feature highlights the crucial role of financial constraints during and after severe economic meltdowns. Besides, we employ nonlinear sources for the Phillips curve and the intertemporal IS equation, and move from a Linear to a Nonlinear Quadratic (NLQ) model.====With this framework, we study a traditional monetary policy controlling the interest rate and the Unconventional Monetary Policy (UMP). Since the latter affects the credit market and risk premium dynamics, it can be thought of as a Quantitative Easing program (QE) that expands the central bank’s balance sheet. In this context, by intervening in the asset market, the central bank affects credit flow and the risk premia to sustain aggregate demand. This maneuver eases financial conditions, which also depend on macro-prudential regulation; for instance, the level of the Loan-to-Value ratio in the banking system may impair the ability of agents to borrow money.====To calibrate the model, we first estimate it by nonlinear seemingly unrelated regression (Zellner, 1962) on macroeconomic Euro-Area data. We enrich the dynamics of the model presented in Gross and Semmler (2019b) while offering a full-meaningful estimate of the nonlinear state-space that is more robust than the one in Faulwasser et al. (2020). Next, we use the estimated parameters to set up a Nonlinear Programming (NLP) algorithm related to the Nonlinear Model of Predictive Control (NMPC) method to solve various macroeconomic scenarios.====The NMPC method is a well-known procedure in control engineering, but it has also become a recent approach to solving dynamic decision models in economics. The NMPC has been employed over the last few years to deal with nonlinearities in economics (Grüne et al., 2015). For example, researchers use NMPC to study feedback mechanisms between downturns and financial stress (Schleer and Semmler, 2015), debt-deflation mechanisms (Ernst et al., 2017), downward spirals (Gross et al., 2018), the nonlinear Phillips curve (Gross and Semmler, 2019a), a central bank leaning against financial risk (Gross and Semmler, 2019b), the impact of quantitative easing (Faulwasser et al., 2020), and climate economics (Kellett et al., 2019).====To our knowledge, this is the first paper that deals with scarring effects in the NMPC literature. With this solution method, we show that a conventional interest rate policy alone can hardly be sufficient to recover from deep demand contractions in the long run. We conceive a deep contraction as a situation in which the output gap and the inflation fall far below a zero steady-state; this starting condition characterizes a new steady-state equilibrium in which the economy will operate below its potential. We interpret this feature as the model’s ability to generate scarring effects. Therefore, given the small-scale nature of the model, we conceptualize the scarring effect as the distance of the long-run output gap from zero among feasible alternatives. The main message of our study is that deep contractions trigger long-run scars, but the shape of scars depends on the degree of nonlinearities in the system playing with the contingent policy design.====Besides, we also point out three main policy implications from simulating scenarios: first, the QE may mitigate long-run scars, as opposite to standard monetary policy, which can – or possible not – stabilize the economy only in the short run. Second, long-run scars are more pronounced in a lower-real rate environment even if QE is active. That is in line with the idea of a stagnation trap of Benigno and Fornaro (2018): a low growth that depresses the aggregate demand pushes the real interest rates down, and nominal rates close to the zero lower bound, impairing the central bank’s ability to stabilize the economy. Third, financial constraints leave the deepest scars. By seeing those constraints as the consequence of policy-induced innovations, for instance, as some macro-prudential regulation, this result reconciles with the empirical findings of Lucidi and Semmler (2022): the costs of implementing financial regulation during a recession are generally high.====We arrange the remainder of the paper as follows. Section 2 presents our proposed NLQ model. Section 3 describes our solution method, while the appendix details its technical features. Section 4 shows the NLSUR estimation method and the resulting parameters. Section 5 reports the results from model scenarios; Section 6 provides some concluding remarks.",Long-run scarring effects of meltdowns in a small-scale nonlinear quadratic model,https://www.sciencedirect.com/science/article/pii/S0164070422000805,6 December 2022,2022,Research Article,32.0
Maebayashi Noritaka,"Faculty of Economics and Business Administration, The University of Kitakyushu, 4-2-1 Kitagata, Kokura Minami-ku, Kitakyushu, Fukuoka 802-8577, Japan","Received 26 April 2022, Revised 5 November 2022, Accepted 24 November 2022, Available online 26 November 2022, Version of Record 6 December 2022.",https://doi.org/10.1016/j.jmacro.2022.103485,Cited by (0), cannot persist much longer than 30 years (one period in the model). This result will support the pace of consolidation in the EU: the Stability and Growth Pact.,"The default risk on Greek government debt in the 2008–2009 global financial crisis raised concern over the sustainability of public debts or deficits among countries with large public debts. Long-term sustainability is one of the largest concerns of both policymakers and academics (e.g., Fatás and Mihov, 2010, D’Erasmo et al., 2016 for recent studies). In fact, the Stability and Growth Pact (SGP) in the EU identifies fiscal sustainability as the main goal of its fiscal framework. The fiscal consolidation rule in the SGP has two directives; it sets the (i) target of debt levels, and (ii) the pace of reduction in debt. The rule states that member states whose current debt-to-GDP ratio is above the 60% threshold must reduce their ratios to 60% at an average rate of one-twentieth per year. A recent empirical study by Molnar (2012) finds that fiscal rules are associated with a greater probability of stabilizing debt. Many empirical analyses also show that better-designed rules are more likely to reduce fiscal deficits (see the survey by Eyraud et al. (2018)).====Given this background, we consider the fiscal consolidation rule that reduces the public debt in proportion to the difference between the current and target levels of the debt-to-GDP ratio. This rule avoids the explosion of debts and commits a government to reduce its debt. Either a cut in public spending or an increase in tax rate is required to follow this consolidation rule. However, when outstanding debt is substantial, fiscal sustainability may be unassured even under such consolidation rules and plans in the following senses. Expenditure-based (resp. Tax-based) consolidation requires a large cut in public spending (resp. a large tax hike) even with high repayment costs of debts, inducing a too small fiscal space (resp. a too large tax hike) to manage fiscal policy. Reductions in debts under such fiscal consolidation plans are no longer implemented and are regarded as unsustainable situations.====Although the need for fiscal consolidation prevails in OECD countries with high debt, there is little consensus on the paces of fiscal consolidation (see e.g., Rawdanowicz (2014))==== Why is determining the pace of fiscal consolidation difficult? Consider the case of countries with very large outstanding debts (e.g., Japan, Greece, Italy, Portugal, and the US). At a very slow pace, fiscal consolidations may fail to sustain fiscal policy due to the large interest payment of public debt as well as the crowding-out effect of public debt on capital accumulation (for references to the theoretical literature, see, e.g., Diamond, 1965, Chalk, 2000, Mankiw and Elmendorf, 1999 provide a survey of the empirical literature). Furthermore, it postpones the burden of debt payment on future generations, which might not be fair to them. By contrast, a very rapid pace of consolidation may lead to a burden on the current and earlier generations and result in a large loss of social welfare. Then, a common pace of consolidation under the SGP in the EU might be the ====, as classified by Panico and Purificato (2013), for countries with extremely high debts (e.g., Greece, Italy, and Portugal).====When considering the pace of consolidation, it is also important to know the type of consolidation that is more effective between spending cuts and tax increases in the sense that which type of consolidation between these makes fiscal policy more sustainable, increases welfare more, and induce lower intergenerational conflict over welfare (hereafter, fairness of welfare distribution across generations). Earlier studies are not conclusive on this issue. Some studies say that expenditure-based consolidation has a less negative effect on GDP (Erceg and Lindé, 2013), increases welfare more (Maebayashi et al., 2017, Morimoto et al., 2017), and has a more sustainable plan (Morimoto et al., 2017). However, Glomm et al. (2018) show that expenditure-based consolidations are worse than tax-based consolidations in terms of lost income of earlier generations.==== Furthermore, most countries whose debt is huge are concerned with public goods for individual consumption (e.g., healthcare, housing, and education) and must keep fiscal space for these provisions (e.g., Song et al. (2012)). This is more likely when the population aging proceeds. Public healthcare service enhancement and better utilization of existing infrastructure and human capital for both young and older people are necessary for healthy and active aging (e.g., OECD, 2019b, Cristea et al., 2020). In this case, large spending cuts may not be compatible with enough public goods, and therefore government may rather adjust tax rates to keep fiscal spaces for public goods provision.====Accordingly, we tackle the following research questions. (i) How does the pace of fiscal consolidation affect the transition paths of the economy? (ii) How rapid should the pace of fiscal consolidation be to ensure fiscal sustainability? (iii) How does the pace of fiscal consolidation impact each generation’s welfare or social welfare? Is there a trade-off between the two? (iv) What is the different impact on the abovementioned three questions under expenditure- and tax-based fiscal consolidations?====To that end, we study a standard overlapping-generations (OLG) model of a closed economy developed by Diamond (1965). We introduce a debt policy rule under which the government debt relative to the size of the economy is reduced gradually to a targeted debt/GDP level in the long run. Under expenditure-based (resp. tax-based) consolidations, governments adjust their spending with fixed income and consumption tax rates (resp. governments adjust income tax rates with the fixed ratio of government spending to GDP). Diamond’s (1965) model is very useful in studying fiscal sustainability. In Diamond’s (1965) OLG models, fiscal sustainability cannot be ensured when public debt explodes and Ponzi games of debt by the government is possible (e.g., Chalk, 2000, de la Croix and Michel, 2002, Yakita, 2008) in contrast to infinitely lived agent models. The fiscal consolidation rule in this study avoids such explosions of debts. Instead, expenditure-based consolidation (resp. tax-based consolidation) can require such a large cut in public spending that fiscal space into zero (resp. a tax hike into 100%) when outstanding debt is substantial. Reductions in debts under such fiscal consolidation plans are no longer implemented and are regarded as unsustainable situations.====Furthermore, Diamond’s (1965) OLG model is instrumental in sharing the analytical investigations (theoretical contributions) in the literature. Employing analytically tractable settings with inelastic labor supply, log utility, and the Cobb–Douglas production functions, we investigate not only (i) properties of global transition paths that are related to the paces of consolidations but also (ii) the thresholds of public debt for each level of capital in order for the government to sustain fiscal policy are derived analytically and represented in two two-dimensional phase diagrams, each under two types of consolidation plans. We also calibrate the model to the data of Japan, the US, Greece, Italy, and Portugal as examples of countries with very high debt-to-GDP ratios.====In this study, the pace of consolidation plays an important role in turning unsustainable transition paths into sustainable ones. Then, welfare effects of the transition from unsustainable to sustainable paths are highlighted in this study while a large body of previous studies focuses only on the steady states between pre and post policy changes or transitions between these two. We also judge the welfare effect of the fiscal consolidation based on both social welfare and fairness of welfare distribution between each generation. We extend these welfare analyses by introducing CRRA utility, increases in consumption tax during tax-based consolidation, and the consolidation regime mixed expenditure cut with tax increases.====The main findings of this study are summarized as follows.====Let us refocus the discussion on comparing tax hikes vs. spending cuts here to understand the intuitions behind these results. Both under expenditure- and tax-based consolidations, a large outstanding debt relative to capital, generates large crowding out of capital, and a decrease in capital increases the interest rate and the repayment cost of debt. Under expenditure-based consolidation, it reduces fiscal space for public spending. In addition, a large outstanding debt increases the amount of government spending to consolidate by the distance from the targeted debt-to-GDP ratio. Therefore, large public debts beyond certain levels induce fiscal space into zero. Under tax-based consolidations, to keep the ratio of government spending constant and reduce debt by the distance between the current and the targeted ratio of debt to GDP, tax rates must increase according to a high repayment cost of debt. It causes a large negative distortionary effect on the private sector’s savings and shrinks the economy, inducing further increases in tax rates and eventually into 100%.====Expenditure-based consolidations are more likely to make fiscal policy sustainable than tax-based consolidations because they are less distortionary for economic activities and less harmful for economic activity in the short and medium runs. However, tax-based consolidation in this study is not just hikes in tax rates but rather tax adjustments to keep the consolidation strategy with a fixed ratio of government spending to GDP. Therefore, tax-based consolidation can decrease tax levels in the long run, owing to lower long-run debt, indicating that tax-based consolidations can be more welfare-friendly than expenditure-based consolidations with constant tax rates. Furthermore, when the governments are very concerned with public goods provision, large spending cuts cannot be compatible with enough public goods, and therefore governments adjust tax rates to keep fiscal spaces for public goods provision. These results differ from earlier theoretical studies that support expenditure-based consolidations, as we will see below. Furthermore, this compact model allows us to show the following facts regarding the fair distribution of welfare across generations. The important point is which consolidation ends faster or equivalently, which steady state between the two types of consolidations is closer to the initial state of the economy, depending on the size of outstanding debts relative to capital, the productivity of the economy, and tax rate levels.====Both under expenditure- and tax-based consolidation, slower paces of consolidations keep welfare losses of early generations smaller while they cause larger welfare loss for later generations. This is because slower paces of consolidation induce large outstanding debts to persist for longer periods, which not only crowds out capital accumulation strongly but also causes later generations to suffer from lower government services by a large interest repayment. Furthermore, welfare loss of the initial old is small even with a rapid pace of consolidation because they can obtain asset income from a large initial public debt. For these reasons, fiscal consolidations cannot persist much longer than 30 years (one period in the model).====Fiscal consolidation is shown to be productive in the medium and long term in the literature of exogenous growth models. Some studies (e.g., Coenen et al., 2008, Forni et al., 2010, Bi et al., 2013, Cogan et al., 2013, Erceg and Lindé, 2013, Philippopoulos et al., 2017) use new Keynesian dynamic stochastic general equilibrium (DSGE) models while others (e.g., Papageorgiou (2012); Hansen and İmrohoroğlu (2016) use real business-cycle (RBC) models. These studies focus on the effect of fiscal consolidation on transitional dynamics but ignore the optimal paces of consolidation.====The literature on the theoretical links between the paces of fiscal consolidation, growth, and welfare is still small. Growth models that examine optimal paces of consolidation include Maebayashi et al. (2017), Morimoto et al. (2017), and Futagami and Konishi (2018). These studies consider a debt policy rule in line with the SGP’s 60% rule of the debt-to-GDP ratio for welfare analyses of fiscal consolidations, and show that a faster pace of consolidation drives larger welfare gains.==== Rawdanowicz (2014) also sheds light on the pace of consolidation plan to reduce debt from 90% to 60% of GDP within 20 years and to maximize a discounted sum of real GDP growth (or minimize a discounted sum of squared output gaps).====In the literature on spending-based versus tax-based consolidation, Erceg and Lindé (2013) study the issue in a two-country new Keynesian model. They show that spending-based consolidation has far less costly effects on output than tax-based consolidation in the longer-term. Erceg and Lindé (2013) demonstrate that this finding is consistent with the supply side effects emphasized in Uhlig (2010). Maebayashi et al. (2017) show that spending-based consolidations have larger welfare gains than the tax-based consolidations in an endogenous growth model. Morimoto et al. (2017) assess both sustainability and social welfare in a small open endogenously growing economy and show that expenditure-based consolidation can be preferable for both fiscal sustainability and welfare.====However, previous studies on transitional dynamics, optimal paces of fiscal consolidation, and spending-based versus tax-based consolidation assume an infinitely lived agent, and therefore ignore intergenerational welfare losses or gains. Glomm et al. (2018) consider spending-based versus tax-based consolidation in a simulation-type OLG model in line with Auerbach and Kotlikoff (1987) and show that spending-based reforms produce higher output than tax-based reforms in the long run while spending-based austerity reforms are worse than tax-based reforms in terms of lost income in early generations. In contrast to these studies, we show that tax-based consolidations can produce larger welfare gains and induce higher or lower intergenerational conflicts than expenditure-based consolidations in some cases. Furthermore, we conduct analytical investigations as to the properties of global transition paths related to the paces of consolidations and the thresholds of public debt for the government to sustain fiscal policy.====Chalk (2000), de la Croix and Michel (2002), and Yakita (2008) investigate the sustainability of public debt (global transitional dynamics of debt) in OLG models and concluded that a Ponzi game by the governments is possible. The sustainability in OLG models is often defined as the convergence of the public debt to a sustainable level in the long term under some fiscal rules. Constant deficit (or deficit to GDP) rules are examined in Chalk (2000) and Yakita (2008), while a constant debt-to-GDP ratio is imposed in de la Croix and Michel (2002).==== They show that a debt above the threshold level can explode and cannot sustain fiscal policy. These studies however do not conduct analyses of fiscal consolidations that encompass the timeline effect of reduction in debt-to-GDP ratio.====There are additional merits of studying fiscal consolidation policy in an OLG model with finitely lived agents rather than an infinitely lived agent model. As emphasized by Uhlig and Yanagawa (1996), because labor income is paid mostly to the young and capital income accrues mostly to the old in a lifecycle of finitely lived agents, lower productivity of labor and higher labor income tax means that younger people in an economy are left with less income out of which to save and to buy capital stock. In this study, therefore, lower productivity of the economy and high wage income tax rates may fail to increase GDP, consumption, and public spending even after consolidation. Such a possibility is paid little attention in many studies mentioned above that assume infinitely lived agents who are in essence always young.====Furthermore, OLG models allow us to study intergenerational conflicts of welfare caused by politics of public debt in the political economy literature. (e.g., Song et al., 2012, Müller et al., 2016, Röhrs, 2016, Arai et al., 2018, Uchida and Ono, 2021). Arai et al. (2018) and Uchida and Ono (2021) are more relevant to this study because they consider the debt ceiling policy to prevent the overaccumulation of debt in a closed economy model with capital accumulation. They derive a Markov-perfect political equilibrium determined by probabilistic voting and show that a negative growth effect of debt is weakened by the debt ceiling policy but creates a trade-off between generations in terms of welfare. In contrast to these studies, we derive policy implications on (i) the paces of debt reductions and (ii) the comparison between expenditure- and tax-based consolidations under a simple, practical rule based on the SGP in the EU.====When we regard pay-as-you-go social security as an implicit debt, privatization of public pension is like a problem of reductions in debt. Recently, Nishiyama and Smetters (2007) (by a simulation methodology in line with Auerbach and Kotlikoff (1987)) consider the pension reform that erases the intergenerational conflict problem and propose the hypothetical potential for Pareto improving policy if the lump sum redistributive authority is assumed. However, the lump sum redistributive authority is not proposed as an actual government (see Nishiyama and Smetters, 2007, Andersen and Bhattacharya, 2020). Andersen and Bhattacharya (2020) (by a Diamond-type OLG model) study the positive externality effect of public education to eliminate debt without intergenerational conflict of welfare. However, they also consider some unrealistic situations because they consider (i) debt dynamics that occur only from public education finance and (ii) lump sum tax authority. Thus, constructing a fiscal consolidation policy that is actually Pareto improving remains to be a difficult task. Although we also confront this difficult problem, we consider the actual consolidation strategies implemented in the EU member countries with adjustments of tax on wage, capital income, and consumption rather than lump sum tax.====Building on these previous studies, we examine the global transition dynamics of both expenditure- and tax-based consolidations, a sustainable pace of these consolidations, welfare effects, and an optimal pace of consolidations from the viewpoints of both social welfare and fairness of welfare across generations in an OLG economy.","The pace of fiscal consolidations, fiscal sustainability, and welfare: An overlapping generations approach",https://www.sciencedirect.com/science/article/pii/S0164070422000787,26 November 2022,2022,Research Article,33.0
"Hussain Syed M.,Liu Lin","Department of Economics, James Madison University, Harrisonburg, VA, USA,Department of Economics, University of Liverpool, Liverpool, UK","Received 23 May 2022, Revised 15 August 2022, Accepted 12 November 2022, Available online 24 November 2022, Version of Record 9 December 2022.",https://doi.org/10.1016/j.jmacro.2022.103483,Cited by (0),"This paper examines the macroeconomic effects of government spending shocks in Canada for the period of 1949–2012. We construct a novel measure of news about exogenous government spending changes identified through the narrative approach. We use government documents, mostly the budget speech, to identify the size, timing, and principal motivation for all planned major federal government spending changes. To achieve identification, we consider those changes that are unrelated to the contemporaneous movements in the economy. The implied government spending multiplier estimates using our exogenous government spending news series are between 1.08 and 1.69.","The great recession of 2008 and COVID-19 induced economic crisis of 2020 forced governments worldwide into providing their economies with various stimulus plans. These plans have highlighted the importance of our understanding regarding their macroeconomic effects. However, despite their importance for current macroeconomic policy making, there is a surprising lack of consensus over the effects of government spending changes. Moreover, there is little empirical evidence for countries other than the US. The problem that arises in the study of government spending changes is that of simultaneity — while there is no doubt that government spending changes affect GDP, but at the same time, GDP itself can cause changes in government spending. This identification problem has been mainly tackled by two different approaches in the literature — the structural vector autoregression (SVAR) approach (Blanchard and Perotti, 2002) and the narrative approach (Ramey and Shapiro, 1998, Ramey, 2011b).====This paper contributes to the literature by estimating the government spending multiplier for Canada for the period of 1949–2012. We use the narrative record, mostly the budget speeches, to identify the size, timing, and principal motivation for all planned major government spending changes. To achieve identification, we consider those proposed changes that are unrelated to the contemporaneous movements in the economy, called exogenous government spending changes. This is similar to the narrative approach adopted to study the effects of tax changes, pioneered by Romer and Romer (2010). We then construct a new measures of news about exogenous government spending changes along the lines of Ramey (2011b).====The estimation using our new measure of government spending shocks shows that the implied government spending multiplier is 1.08 for Canada, when the elasticity of output with respect to government spending is calculated as the ratios of their peak responses to shock to news about government spending. When calculated through cumulative responses over 2 and 4 years, the implied multipliers are 1.48 and 1.26, respectively. These multipliers are larger than the ones estimated by Owyang et al. (2013) with military spending news series for Canada, ranging from 0.57 to 0.79. They are also larger than the ones estimated with the structural VAR approach, ranging from 0.40 to 0.55.====We also address the issue of the news variable losing its predictive power for government spending once large defense spending changes are removed. Ramey (2011b) noted that their news variable loses its explanatory power for government spending when the observations associated with WWII and Korean War are removed. We find a similar problem with our news variable: it loses its explanatory power for government spending if we remove the observations corresponding to the large defense spending increases associated with the Korean War. This limits our ability to use to this variable to study only those sample periods that include the Korean War years.====To get around this problem, we construct a measure of announced and implemented government spending changes. This includes all those measures that were to be implemented in the same year as they were announced. If a spending change were to be implemented over a number of years then we only include the part that would be in implemented in the same year. We then assign these spending changes to the quarter when the budgets were approved. For midyear announcements about spending changes, we use the announcement dates as implementation dates. While acknowledging that this assumption may result in some bias in our results, we argue that the bias would be small because (1) we omit the observations that are announced in one year and implemented in a future year, (2) we do not find strong evidence of anticipation effects when using our news variable: government spending starts increasing significantly in the same quarter when the announcement is made and increase in output lags increase in government spending, and (3) it is plausible to believe that the finance ministry makes preparations to implement spending changes for upcoming changes before announcing them.====We normalize our measure of announced and implemented spending changes by lagged GDP and use this variable to estimate the government spending multiplier. The estimated multiplier comes out to be 0.92 which is close to other estimates that we get. We also estimate the multiplier using annual data on announced and implemented changes since the anticipation effects are less problematic in annual data. The multiplier that we estimate comes out to be 0.94 which is very close to the estimate from the quarterly data. We also find some evidence that austerity measures of the mid 1980s and 1990s had smaller contractionary effects than the expansionary effects of spending increases. The use of announced and implemented spending changes in our analysis is a significant contribution of our paper. This is because most countries have not experienced the likes of military spending increases that US and Canada have. Furthermore, even with these countries, we have to stretch the sample back to the 1950s to seek identification in the empirical analysis. The analysis using announced and implemented spending changes do not have such requirements and can easily be replicated for other countries.====One important difference between the results from the studies involving SVAR and narrative approaches is the effect of spending changes on consumption. Studies like Blanchard and Perotti (2002), Galí et al. (2007), Perotti et al. (2007), Mountford and Uhlig (2009) and Auerbach and Gorodnichenko (2012) that use the SVAR approach generally find an expansionary effect of spending changes on consumption. On the other hand, studies like Ramey and Shapiro (1998), Ramey (2011b) and Barro and Redlick (2011), and others that use the military spending news variable find contractionary effects of spending increases on consumption. Our results are in line with previous studies using the narrative approach: we find that spending increases in Canada result in consumption decreasing. However, unlike the SVAR studies for the US, we find that SVAR approach for Canada also gives a negative response of consumption to spending increases. However, the response estimated from the SVAR approach comes out to be much smaller and insignificant.====There have been extensive debates over the effects of government spending changes. Ramey (2011a) reviewed those studies for both aggregate and cross-locality estimates on a temporary deficit-financed government purchase increase in the US. Hall (2009) also focuses on the impact of government purchases, through both structural VAR and dynamic model estimations. Our paper is similar to those studies that use the narrative approach to estimate the government spending multiplier.==== Some of the studies using this approach are Ramey (2011b), Ramey and Shapiro (1998), and Barro and Redlick (2011). There are also papers focusing on the asymmetric nature of the government spending multiplier, including Ramey and Zubairy (2018), Owyang et al. (2013), Barnichon and Mathhes (2017) and etc.====The literature studying the macroeconomic effects of government spending changes for countries other than the US is rather sparse. Crafts and Mills (2013) report estimates of the fiscal multiplier for interwar Britain by constructing a defense-news variable. There are also studies of multiple countries, such as by Perotti (2005) on the OECD countries, Beetsma et al. (2008) and Beetsma and Giuliodori (2011) on the EU. Owyang et al. (2013) extend military spending news data for Canada back to 1921. Alesina et al. (2017) and Guajardo et al. (2014) use the narrative record to identify episodes of fiscal consolidation – including both government spending decreases and tax increases – for OECD countries and find strong contractionary effects of such changes. Methodologically, our study is similar to both of these but we focus on a much larger sample, both in terms of time and in terms of types of government spending changes. Compared to Owyang et al. (2013), the main advantage of our approach is that it is replicable for other countries that have not experienced large military build-ups. Also, our approach of using announced and implemented spending changes gets around the potential problem of the news shock series being a weak instrument for overall spending.====The paper is organized as following: Section 2 describes the data and our methodology of constructing the narratives of exogenous changes in government spending. Section 3 provides the estimation results with our newly constructed data series. Section 4 compares the government spending multipliers identified and estimated with other methods. Section 5 examines the effects of announced and implemented government spending changes. Section 6 provides the effects of government spending changes on other economic variables. Section 7 concludes.",Macroeconomic effects of government spending shocks: New narrative evidence from Canada,https://www.sciencedirect.com/science/article/pii/S0164070422000763,24 November 2022,2022,Research Article,34.0
"Sun Xiaojin,Tsang Kwok Ping","Department of Economics and Finance, University of Texas at El Paso, United States of America,Department of Economics, Virginia Tech, United States of America","Received 17 December 2021, Revised 28 July 2022, Accepted 14 November 2022, Available online 21 November 2022, Version of Record 28 November 2022.",https://doi.org/10.1016/j.jmacro.2022.103484,Cited by (0),"The slope of the yield curve has long been found to be a useful predictor of future economic activities, but the relationship is unstable. One change we have identified in this paper is that, between the early 1990s and the collapse of the housing market in 2007, movements at the long end of the yield curve have an increase in predictive power. We use a medium-scale ","The practice of using interest rate spreads as macroeconomic predictors can be dated back to Stock and Watson (1989) who include the long-term and short-term treasury bond yield spread as one of the leading economic indicators. Subsequent empirical studies, including Estrella and Hardouvelis (1991), Estrella and Mishkin (1996), Dueker (1997), and Estrella and Mishkin (1998), among many others, all find that the spread between 10-year and 3-month interest rates is a useful predictor for future recessions and its predictive power significantly outperforms other financial and macroeconomic indicators.====In recent years, there has been renewed interest in the yield spread as a predictor of future economic activities. Chinn and Kucko (2015) examine the predictive power of the yield spread across a group of advanced economies and over various sample periods. Using a probit regression, they find that the yield spread is a strong predictor of future recessions before the 2000s but the predictive power has deteriorated prior to the Great Recession. Walsh (2010) and Aguirre and Vázquez (2020) also show that the correlation between interest rates and output becomes less pronounced since the mid 1980s. Motivated by Wright (2006)’s argument that there is no fundamental reason why movements at the two ends of the yield curve must have the same predictive content for the likelihood of a recession, Chinn and Kucko (2015) include the short-term policy rate in addition to the yield spread in the regression, so that changes in the yield spread capture movements at the long end of the yield curve.====We present similar empirical results in Table 1 based on the following simple probit regressions: ==== using quarterly data, where the recession indicator variable ==== equals one if there is a recession identified by the NBER’s Business Cycle Dating Committee in any quarter between ==== and ====, inclusively. We consider a forecast horizon of ====, over which the yield spread has been found to have important and significant predictive power in general; see Chinn and Kucko (2015). We use the 3-month Treasury Bill secondary market rate obtained from the Board of Governors of the Federal Reserve System as the short-term interest rate. Given that the short-term interest rate stays near zero in the aftermath of the 2008 financial crisis, we add to it the difference between the (Wu and Xia, 2016) shadow federal funds rate and the effective federal funds rate of the Federal Reserve after 2008 to account for the effects of unconventional monetary policy. The long-term interest rate is the 10-year zero-coupon yields calculated by Gürkaynak, Sack, and Wright (2007). The yield spread is the difference between the long- and short-term interest rates. Our sample ranges from 1972:Q1 to 2019:Q4. The start of the sample is restricted by the data availability of the long-term interest rate and the end of the sample is chosen to exclude the economic turbulence caused by the COVID-19 pandemic.====In the top panel of Table 1, we conduct a test for multiple unknown structural breaks based on Eq. (2) using the Bai and Perron, 1998, Bai and Perron, 2003 method. The test statistic well exceeds the critical value at the 1% level and the test suggests three break points at 1990:Q3, 2000:Q1, and 2007:Q1, respectively. However, the second break point is estimated to have an unreasonably wide confidence interval. We therefore split the sample at the two break points, 1990:Q3 and 2007:Q1, and present the subsample estimation results in the bottom panel of the table.====In the early subsample period 1972:Q1–1990:Q3, the yield spread alone is a highly significant predictor of future recessions. A flattening yield curve predicts a higher probability of recessions over the next four quarters. The overall model fit of the simple probit regression in Eq. (1) as measured by the McFadden ==== exceeds 0.5. The predictability turns out to rely more on movements at the short end than the long end of the yield curve, as the estimated coefficient of the yield spread in Eq. (3) exceeds that in Eq. (2) in magnitude. The predictive power of the yield spread deteriorates over the second subsample period 1990:Q4–2007:Q1 and the ==== reduces to 0.28. After controlling for the short-term interest rate, however, the model fit improves. The yield spread remains a significant predictor of future recessions in Eq. (2) where the short-term interest rate is controlled but becomes insignificant in Eq. (3) which instead controls for the long-term interest rate. This result suggests that movements at the long end of the yield curve outweigh those at the short end in predicting future economic activities over the second subsample period, which is not surprising given the fact that it covers the housing boom leading up to the financial crisis. The demand side of the housing market is heavily affected by the long-term interest rate, because, as the Monthly Interest Rate Survey of the Federal Housing Finance Agency shows, about 78% of the mortgages over that period of time are fixed-rate loans with an average maturity of 27 years.==== Over the third subsample period 2007:Q2–2019:Q4, however, movements at the short end of the yield curve regain predictive power.====While all of the aforementioned studies employ reduced-form regressions to examine the predictive power of the yield spread, in this paper we try to understand the predictive power of the yield spread through the lens of a medium-scale structural macroeconomic model. The incorporation of the yield spread into a structural model enables us to study the underlying mechanisms through which the yield spread might affect future economic activities. Motivated by the empirical evidence that the long end of yield curve outperforms the short end in predicting future recessions over a sample period that includes the drastic rise in housing price, we also include a housing sector in our structural model. To our best knowledge, this is the first paper that studies the yield spread as predictor of future recessions in a structural framework.====Through a series of counterfactual analyses based on our estimated model, we find that both nominal wage rigidities and capital adjustment costs affect the predictive power of the yield spread. With higher wage rigidities or lower capital adjustment costs, both residential and nonresidential investment becomes more responsive to structural shocks, and therefore a decrease in the yield spread predicts a larger increase in the likelihood of future recessions. The occurrence between the early 1990s and the collapse of the housing market in 2007 of an increase in nominal wage rigidities and a decrease in capital adjustment costs is identified using a regime-switching version of the model, which matches our empirical finding that movements at the long end of the yield curve exhibit more predictive power over this sample period.====The remainder of the paper is as follows. Section 2 introduces the model economy. Section 3 takes the model to data. Section 4 examines potential factors impacting the predictive power of yield spread through a series of counterfactual analyses. Section 5 introduces regime switching to key parameters in the model economy. Section 6 provides further evidence from actual data. Section 7 concludes the paper.",Yield curve and the macroeconomy: Evidence from a DSGE model with housing,https://www.sciencedirect.com/science/article/pii/S0164070422000775,21 November 2022,2022,Research Article,35.0
"Albertini Julien,Fairise Xavier,Terriau Anthony","Univ Lyon, Université Lumière Lyon 2, GATE UMR 5824, F-69130 Ecully, France,GAINS, University of Le Mans, France","Received 1 April 2022, Revised 4 November 2022, Accepted 12 November 2022, Available online 19 November 2022, Version of Record 28 November 2022.",https://doi.org/10.1016/j.jmacro.2022.103482,Cited by (0),"In the United States, almost half of the workers who separated from their jobs ended their unemployment spell by returning to work for their last employer. In this study, we explore the impact of the experience rating (ER) system on recalls. In states using reserve ratio ER, and for a firm that is not at the minimum or the maximum ","The recall from temporary layoff, frequently ignored in the literature, is an important factor to consider for understanding the functioning and dynamics of the US labor market. In Europe, firms generally respond to economic downturns by reducing hours (for example, through short-time work), while in the US, employers usually rely on temporary layoffs (Moy and Sorrentino, 1981).====A crucial question for policymakers is how the design of unemployment insurance (UI) may affect recalls and the dynamics of the labor market over the business cycle. On the one hand, Feldstein (1976) suggests that the layoff-recall process could be related to the way the UI is funded.==== On the other hand, Katz (1986) suggests that unemployed workers’ entitlements may play an important role in the recall process. In the US, the UI is financed by a payroll tax that is related to the previous benefits collected by the firm’s former employees. Under this experience rating (ER) system, each additional hire increases the firm’s contributions and lowers the tax rate, while each additional layoff of a worker collecting UI benefits increases the benefits charged to the firm and increases the tax rate. This system provides an incentive for firms to recall workers collecting UI benefits.====How frequent are recalls, and to what extent are they related to the design of UI and ER? Answering this question is a hard task.====Empirically, the identification of recalls requires a database that tracks workers over a long period of time, even if they lose or change their job, and that identifies their employer. Several studies highlight the widespread use of the layoff-recall process in the US (Feldstein, 1975, Feldstein, 1978, Murphy and Topel, 1987). However, these studies are generally based on databases such as the Current Population Survey (CPS), which do not allow to identify employers, and thus recalls. Consequently, these studies do not measure the effective number of recalls, but the number of unemployed workers who expect to be recalled. However, a substantial proportion of workers who expect to be recalled are never recalled and find other jobs (Katz and Meyer, 1990). To circumvent this problem, Fujita and Moscarini (2017) take advantage of the Survey of Income and Program Participation (SIPP), which allows to identify both workers and employers over time. They show that between 30% and 50% of the unemployment spells end through recall to the previous employer. This suggests that the layoff-recall is an important feature of the US labor market.====Theoretically, the analysis of the interaction between UI, ER, and recalls is also a challenging task. This requires modeling firms’ decisions (hiring, separation, recall) and workers’ decisions (search, UI take-up), but also the design of ER (that is based on the firms’ layoff history) in a tractable way. A precise quantitative investigation also requires generating heterogeneity in firms’ layoff history (to reproduce the distribution of firms over the tax schedule), for example with idiosyncratic shocks, and to introduce aggregate shocks (to discuss the business cycle properties). To the best of our knowledge, there is no model that includes all these ingredients. On the one hand, Card and Levine (1994), L’Haridon and Malherbet (2009), Albertini and Fairise (2013), and Ratner (2013) investigate the effect of ER on the labor market and show that the ER system stabilizes labor market flows and employment over the business cycle. However, none of these studies distinguish between recalls and new hires. On the other hand, Fujita and Moscarini (2017) introduce a recall option in the canonical search-and-matching model ==== Mortensen and Pissarides (1994) and analyze the cyclical properties of recall. However, the ER system is absent from their paper.====Our study attempts to bridge the gap between the ER and recall literature. We first provide empirical evidence of the impact of ER on recalls at the macro level, taking inspiration from the minimum wage literature (Dube et al., 2010, Hagedorn et al., 2015). We exploit the fact that the intensity of ER differs across states and years. We use data from the Department of Labor, Employment and Training Administration (DOLETA) to compute the marginal tax cost (MTC) of a layoff, defined as the present value of tax liabilities caused by an additional layoff relative to the present value of benefits payments, for each state-year. The MTC can be viewed as a measure of the degree of ER. We then take advantage of the Quarterly Workforce Indicators (QWI) dataset, which provides information on recall at the county level, to determine the recall share==== from hires for each county-year. We compare all contiguous county-pairs in the US mainland that straddle a state border to identify the impact of ER on recalls. Our estimates reveal that the degree of ER has a significant positive impact on recalls. We then analyze the recall process at the micro level. Using data from the Survey of Income and Program Participation (SIPP), we show that, among UI eligible workers, the probability of recall is significantly higher for those collecting UI benefits. To sum up, our empirical investigation provides two interesting results: i) Recalls increase with the intensity of ER; ii) Recalls are more prevalent among unemployed workers collecting UI benefits, even after controlling for age, education, unemployment length, or job tenure. One way to rationalize these two facts is that ER provides an incentive for firms to recall unemployed workers collecting UI benefits. This is the mechanism explored in our paper.====To study how the ER impacts unemployment and recalls in the long- and short-run, we develop a structural model, extending the search and matching model of Fujita and Moscarini (2017) in which hiring and separation are endogenous, and recalls and new hires are two distinct margins of hiring. We introduce a distinction between unemployed workers that collect UI benefits and those who do not. We also introduce tractable modeling of the ER system that depends on the firm’s layoff history. The model accurately captures the incentives faced by firms and workers alongside their experience with unemployment. If a laid-off employee claims UI benefits, the firm UI contribution rate increases gradually. By contrast, if the firm recalls its former worker who avails of UI benefits, the tax rate declines progressively. This may also occur if the UI benefits exhaust or if the unemployed worker does not claim UI benefits because he or she has a high probability of being recalled. Our model includes all these mechanisms, thereby allowing us to analyze the effects of ER across several dimensions. The combination of individual productivity shocks and the ER system generates an endogenous distribution of firms across layoffs history and tax rates, which is very close to the observed distribution. The model can replicate the main features of the labor market, including the effect of ER on recalls found in the empirical analysis.====We first use this model to investigate the steady-state effects of ER. We show, in particular, that an increase in the intensity of ER translates into a higher recall share, especially for unemployed workers collecting unemployment benefits. We then use this model to analyze the labor market dynamics under alternative financing schemes by computing impulse response functions. We show that ER has stabilization virtues—the higher the degree of ER, the less volatile the unemployment rate. Changes in the tax schedule affect the labor market dynamics significantly. However, the effects depend on whether the firm is located on the minimum, the maximum tax rate, or in between.====The rest of the paper is organized as follows. Section 2 is devoted to the presentation of the UI financing scheme. Section 3 describes the data and the empirical framework. Section 4 presents the search and matching model. Section 5 presents the calibration strategy. Section 6 investigates the quantitative impact of ER on recalls and the labor market dynamics. Section 7 discusses issues related to ER and concludes. Section 8 provides a separate appendix with additional simulations, a model description, and the solution method.","Unemployment insurance, recalls, and experience rating",https://www.sciencedirect.com/science/article/pii/S0164070422000751,19 November 2022,2022,Research Article,36.0
"Matusche Alexander,Wacks Johannes","University of Konstanz, Department of Economics, Germany,Deutsche Bundesbank, Germany","Received 8 June 2022, Revised 21 October 2022, Accepted 3 November 2022, Available online 12 November 2022, Version of Record 29 November 2022.",https://doi.org/10.1016/j.jmacro.2022.103474,Cited by (0)," changes in states where wealth is distributed more unequally. Third, we show that ECB monetary policy has stronger real effects in Euro Area countries with higher wealth inequality.","In the US, but also in other advanced economies, wealth inequality has risen considerably since the mid-1980s. In general, wealth inequality displays significant variation both over time and across space.==== At the same time, recent advances in macroeconomics have led to a new discussion about the role of inequality for macroeconomic outcomes. In particular, several models predict that the wealth distribution is important for understanding the transmission of monetary policy.==== We contribute to this debate by documenting empirically that wealth inequality affects the strength of the monetary transmission mechanism to the real economy. We provide three separate pieces of evidence, each showing that monetary policy has larger real effects when wealth inequality in the population is higher. To the best of our knowledge, we are the first to offer a detailed account of this relationship.====Macroeconomists have long neglected wealth inequality when studying monetary policy. This was largely due to the complexity and computational demands of structural models that feature a non-degenerate wealth distribution. However, quantitative research also supported the view that the distribution of wealth had little importance for macroeconomic dynamics (Krusell and Smith, 1998). Recently, however, this view has been challenged, and the question of how “inequality matters for macro” (Ahn et al., 2018) has received new attention.====Several mechanisms have been proposed through which wealth inequality can affect the transmission of monetary policy. Luetticke (2021) finds that greater wealth inequality mutes the response of aggregate investment and amplifies that of consumption, with the overall effect on output approximately canceling out. The reason is that wealthy households with high marginal propensities to invest benefit from contractionary monetary policy, whereas incomes of asset-poor households with high marginal propensities to consume fall. Building on the observation that wealthier households are more likely to invest in stocks, Melcangi and Sterk (2020) argue that, when the rich hold a greater share of wealth, an interest rate cut leads to a stronger rebalancing towards equities. This results in a stock market and investment boom and hence an amplification of the output response. Matusche and Wacks (2021) obtain the same result in a model in which rich entrepreneurs invest strongly into their private firm in response to an interest rate decline due to a portfolio rebalancing motive. Here, we ask whether there is a state dependence of monetary policy transmission on wealth inequality in the data that would support the predictions of these studies.====We tackle the question of how wealth inequality alters the transmission of monetary policy in three ways. We first use separate aggregate time series data for the US and the UK, and show that the effects of monetary policy are state-dependent. To this end, we estimate state-dependent local projections, as suggested by Auerbach and Gorodnichenko (2013), using the top 10% wealth share as a state variable. In the US and the UK, we find that in regimes of high inequality monetary policy had a larger impact on real activity than in regimes of low inequality. The differences are quantitatively important. In the US, while industrial production contracted by 2.1 percent in times of high wealth inequality in response to a 25 basis points (bp) increase in the federal funds rate, it shows no statistically significant contraction in times of low inequality. In the UK, responses of output and unemployment to monetary policy are smaller overall but, as in the US, they are relatively larger in times of high wealth inequality.====While the appeal of our first approach is its simplicity and the availability of high-quality data at the aggregate level, the drawback is that we cannot rule out that other variables that have co-moved with wealth inequality over time drive our results. Therefore, we turn to cross-sectional analyses in the second part of our study, which allows us to control for confounding factors. We use estimates provided by the Internal Revenue Service (IRS) on total wealth held by the richest households in each US state to construct measures of state-level wealth inequality. In line with the results on the aggregate level, we find that both output and unemployment in US states that display more wealth inequality react more strongly to common monetary policy shocks. In a third step, we construct measures of wealth inequality for Euro Area countries using data from the ECB’s Household Finance and Consumption Survey (HFCS). We find that Euro Area countries with high levels of inequality react more strongly to common monetary policy shocks.====Based on the consistent findings in all three settings and after conducting various robustness checks, we conclude that higher wealth inequality is correlated with a stronger transmission of interest rate changes to the real economy. The strength of this correlation differs between the set-ups we study. Regarding the output response, we estimate the largest effect on the US aggregate level. Here, an increase in the top 10% share by one percentage point is associated with a 0.29 percentage points stronger average contraction in industrial production over the first three years after a 25 bp monetary policy shock. The effect is about 0.01 percentage points in the UK and about 0.05 in the cross section of Euro Area countries. Across US states, we find that a one percentage point increase in the top 1% share comes with an increase in the response of state personal income by 0.013 percentage points.==== The effect of an increase in the top 10% wealth share by one percentage point on the average response of the unemployment rate ranges from 0.003 percentage points in the UK to 0.041 in the Euro Area. In the cross section of US states, an increase in the top 1% share by one percentage point raises the unemployment response by 0.005 percentage points. These numbers point to an economically meaningful effect of wealth inequality on the transmission of monetary policy to the real economy.====The reduced-form evidence that we provide in this study can inform and discipline the design of future structural models that analyze monetary policy in the framework of heterogeneous agent models. It is also relevant for the current debate among policymakers about when and by how much to raise interest rates. Given the high levels of wealth inequality observed in many developed countries at the moment, our results indicate that a rate increase would have relatively strong effects on the real economy.====The remainder of our paper is structured as follows. Below, we review related literature. We study the state-dependent effects of monetary policy on the aggregate level in Section 2. In Section 3 we analyze the cross-section of US states, and in Section 4 we turn to a cross-section of Euro Area countries. In Section 5 we conclude.",Does wealth inequality affect the transmission of monetary policy?,https://www.sciencedirect.com/science/article/pii/S0164070422000672,12 November 2022,2022,Research Article,37.0
"Kim Myunghyun,Song Sang-yoon","Department of Economics, Sungkyunkwan University, 25-2 Sungkyunkwan-ro, Jongno-gu, Seoul, Republic of Korea,Bank of Korea, 67 Sejong-daero, Jung-Gu, Seoul, Republic of Korea","Received 21 June 2022, Revised 18 October 2022, Accepted 24 October 2022, Available online 28 October 2022, Version of Record 7 November 2022.",https://doi.org/10.1016/j.jmacro.2022.103473,Cited by (0),This paper explores the heterogeneous effects of ,"As the world (especially developed countries) ages, population aging has become a major economic policy concern in many countries. In line with this, many studies have explored the economic effects of population aging. While most of these studies have focused on the fiscal context, such as the social security implications of population aging, only few studies have associated population aging with monetary policy. Keeping this in mind, we investigate how population aging affects the effectiveness of monetary policy. Specifically, we analyze whether the effects of monetary policy shocks on consumption, one of the crucial pipelines of monetary policy transmission to the real economy, are different across age groups (i.e., workers and retirees).====Monetary policy can have different influences on worker and retiree consumption through two channels.==== The first is the debt channel. Specifically, workers are generally younger than retirees; thus, they usually have more debt (such as mortgages) than retirees. Hence, they are more responsive to changes in interest rates. Accordingly, their consumption can be more sensitive to monetary policy than retiree consumption.==== The second is the income source channel. In general, workers are more dependent on labor income (i.e., wages) than retirees, while retirees rely more on financial income, such as interest earnings and social benefits. Since a contractionary monetary policy shock leads to a fall in economic activity, it has an adverse influence on labor income. In contrast, a rise in interest rates due to the shock increases interest income. That is, the effects of the shock on the incomes of the two groups can differ. Therefore, the consumption responses of workers and retirees to monetary policy shocks can differ. We put forth empirical and theoretical analyses to demonstrate the validity of this second channel.====This study begins by providing empirical evidence of the heterogeneous consumption responses of workers and retirees to monetary policy shocks. Using U.S. household-level data, we first show that the interest income of retirees is much larger than that of workers, whereas the labor income of retirees is far smaller than that of workers. Moreover, according to the data, the weight of labor income in total income for workers is much higher than that for retirees, whereas the ratio of interest income to total income for workers is far lower than that for retirees. Using the data and U.S. monetary policy shock series identified by the narrative approach of Romer and Romer (2004), we find that in response to a contractionary monetary policy shock, retirees’ consumption decreases by less than that of workers. Furthermore, the difference in interest income between workers and retirees is the main source of the heterogeneous consumption responses of the two groups to monetary policy shocks. Finally, we show that these results hold even when different monetary shock series are used, such as those using a recursive VAR model, as in Coibion (2012), and a VAR model with sign restrictions, as in Uhlig (2005, 2017).====In order to study the mechanism of the heterogeneous effects of monetary policy shocks on the consumption of workers and retirees, we extend Gertler’s (1999) life-cycle model by adding sticky prices and monetary policy. According to the simulation results of the model, a contractionary monetary policy shock has two opposite effects on household income. On the one hand, it decreases real wages, which has a negative impact on labor income. On the other hand, it causes an increase in the real interest rate, which has a positive effect on interest income. In our model, the weight of the labor income of workers in their total income is much higher than that of retirees, which is consistent with the data.==== Thus, the shock has a more negative influence on workers’ income than on that of retirees. Consequently, worker consumption falls by more than that of retirees in response to the shock, which is also consistent with the empirical evidence.====Using the model, we implement two further experiments to examine the effects of the introduction of social security and a rise in the relative labor productivity of retirees to workers on the responses of worker and retiree consumption to monetary policy shocks. As for social security, if it is included in the model, retiree consumption decreases by slightly more in response to a contractionary monetary policy shock than in the model without social security, whereas the consumption responses of workers in the two models are very similar. Hence, the difference between the two groups’ consumption responses becomes smaller than that in the model without social security. This is because, in this case, the government needs to pay more interest on its debt due to the rise in the real interest rate following the shock, which leads to a cut in social security benefits. Furthermore, retirees’ consumption also depends on social security benefits. Thus, the shock causes a slightly larger fall in retirees’ consumption in the model with social security.====In the event that retirees have higher relative labor productivity, they decrease their consumption by more in response to a contractionary monetary policy shock compared to the model with lower relative labor productivity. Naturally, the higher the relative labor productivity of retirees, the higher the real wage they can earn, and thus, the more labor they supply. Thus, the weight of labor income in total income for retirees becomes larger than in the model with a lower relative labor productivity, meaning that the shock decreases retirees’ income by more. Consequently, in this case there is a larger decrease in retirees’ consumption following the shock.====There is a relatively small body of literature that explores the heterogeneous effects of monetary policy on consumption across age groups. Similar to our study, Fujiwara and Teranishi (2008) add New Keynesian structures to Gertler’s (1999) model to show the less sensitive responses of retiree consumption to monetary policy shocks; thus, our model is similar to their model.==== Wong (2019) also shows that younger people’s consumption is more responsive to monetary policy shocks. However, the source of this phenomenon differs from that in our study. According to Wong (2019), this is because young homeowners are more active in refinancing their home mortgage loans than old homeowners. While our study focuses on the differences in income composition between workers and retirees,==== Wong (2019) pays more attention to the different financial debts across age groups. By developing a New Keynesian dynamic stochastic general equilibrium model with heterogeneous households, Yoshino and Miyamoto (2017) show that in an economy with more workers, an expansionary monetary policy shock boosts aggregate demand more than in an economy with fewer workers, due to the higher total labor supply that leads to higher consumption in the economy.====The remainder of this paper is organized as follows. In Section 2, we provide empirical evidence that retiree consumption responds less sensitively to monetary policy shocks than worker consumption, using U.S. household-level data. Section 3 describes the life-cycle model. Section 4 presents the model calibration and simulation results of the model. Further experiments that consider social security and the different relative labor productivity of retirees from the baseline model are also presented in Section 4. Finally, Section 5 concludes the paper.",The effects of monetary policy on consumption: Workers vs. retirees,https://www.sciencedirect.com/science/article/pii/S0164070422000660,28 October 2022,2022,Research Article,40.0
"Fiorelli Cristiana,Giannini Massimo,Martini Barbara","University of Naples Parthenope, Department of Economic and Legal Studies, Via G. Parisi 13, 80132, Naples, Italy","Received 13 January 2022, Revised 19 September 2022, Accepted 22 September 2022, Available online 3 October 2022, Version of Record 25 October 2022.",https://doi.org/10.1016/j.jmacro.2022.103470,Cited by (2),"This paper measures the level of total risk sharing across Italian regions and compares the role of private and public risk sharing mechanisms over the period 2000-2016. Our findings suggest that market mechanisms of risk sharing are the main tool to absorb region-specific output shocks: ==== and interregional earnings flows absorbed 38 percent of idiosyncratic shocks against 17 percent of interregional fiscal transfers. Overall, risk-sharing channels smooth about 76 percent of region-specific shocks and their role is even stronger during crisis periods.","The issues linked to the recent COVID-19 crisis, or more generally to economic and financial crises, have encouraged a policy debate on the ways to improve the resilience of federal unions to macroeconomic shocks. The presence of insurance schemes among countries, such as those working through the integration of capital markets (Obstfeld et al., 1996) or fiscal transfers (Atkeson and Bayoumi, 1993, Persson and Tabellini, 1996), help individual countries to cope with country-specific shocks. The literature identifies this feature as international risk sharing. However, the theory of risk sharing can also be expanded to local entities belonging to a single country, such as a pool of regions. In this paper we apply the approach of international risk sharing to the intranational level. We measure the degree of risk-sharing across Italian regions, comparing the role of private and public mechanisms of risk sharing in absorbing output shocks. This allows us to determine which of the two has a greater role in dealing with region-specific shocks to output.====The concept of international risk sharing generally refers to the idea that countries cannot avoid idiosyncratic shocks but can share part of the burden with other interconnected economies, reducing the impact on domestic consumption (Cimadomo et al., 2020). When there is a high degree of risk sharing, countries smooth their consumption by offsetting their specific output shocks via several mechanisms (risk-sharing channels). First, we refer to risk sharing through capital markets when union members share risks via cross-country ownership of productive assets. This mechanism identifies the capital market (or factor income) channel. Second, we refer to fiscal risk sharing (fiscal transfer channel) when country-specific shocks trigger fiscal transfers across members, for instance, with a pooled fiscal budget (Sørensen and Yosha, 1998). Finally, risk sharing may occur through credit markets when shocks on output are smoothed via intertemporal choices about savings and consumption facilitated by the access to international and domestic credit markets (credit market channel). Conceptually, the functioning of risk-sharing mechanism at the subnational level is the same. Cross-regional smoothing takes place through interregional flows of labour market and rental income (====), redistribution of fiscal resources across regions (====), or via private and public savings (====). Additionally, the ==== channel can be considered as a further risk-sharing channel. We can divide these channels into two categories: private and public (Cimadomo et al., 2020). The first one relates to those mechanisms which are mainly market based. The second one relates to fiscal transfers.====Given the above-mentioned scheme of risk sharing, the literature has focused on analysing asymmetric shocks’ absorption capacity in different federal unions. Asdrubali et al. (1996) and Sørensen and Yosha (1998) started a series of contributions that measure the degree of risk sharing achieved by different pools of states, countries, and regions. We contribute to this literature in different ways. First, we analyse the level of total risk sharing among Italian regions over the period 2000–2016 measuring the absorption capacity of each channel. This allows us to determine whether private or public risk-sharing mechanisms have greater impact in absorbing region specific shocks. Second, we study the evolution of private and public risk-sharing mechanisms during the financial downturns.====Since the key assumption of risk sharing rests on the heterogeneity in economic structures, Italian regions may represent a distinctive research case to test the functioning of the risk-sharing approach. The literature on risk sharing already accounts for the redistribution mechanism in the Italian context (Decressin, 2002, Gandullia and Leporatti, 2019). However, these contributions focus mainly on the role of transfers between central and local government, overlooking the functioning of other risk-sharing channels. To fill this gap, our study investigates the full functioning of risk-sharing mechanism in Italy, which is achieved by the four channels, and then, compares the effectiveness of market (or private) channels and the fiscal redistribution (public) channel.====For this scope, we apply a standard empirical framework of risk sharing to study the channels’ behaviour through which the idiosyncratic shock absorption occurs. In particular, we determine the level of total risk sharing among regions obtained through the risk-sharing channels, namely ==== (compensation and rents), ==== , ====, and ====. As common in the literature, we follow the approach proposed by Asdrubali et al. (1996). This approach consists of applying a variance decomposition of shocks to GDP in order to quantify the share of smoothing achieved via the different channels. It is based on national accounting, and the starting point is the disaggregation of regional GDP into gross regional income (GNI), net regional income (NI), net regional disposable income (DIS), and private and public consumption.====The main results suggest that market-based mechanisms can smooth shocks much better than public ones over the period 2000–2016. In details, we find a high level of risk sharing among Italian regions. About 76 percent of region-specific shocks to GDP are smoothed by different risk-sharing channels: 38 percent of shocks to per capita GDP is absorbed by factor income, 5 percent by capital depreciation, around 17 percent through net fiscal transfers from central government, and 16 percent is smoothed by the credit market through savings. Only 24 percent of shocks remain unsmoothed. If we compare the level of total risk sharing in normal and crisis times, it increases from 76 percent to about 80 percent in financial crisis, reaching 88 percent during the sovereign debt crisis. As underlined by the percentage of factor income, the labour mobility and interregional earnings flows are the main mechanisms for absorbing the shocks. A regional-specific shock, in fact, may be smoothed by the income flows of commuters. On the other hand, the public sector shows a narrow redistributive impact across regions, confirming previous findings from the literature (Asdrubali et al., 1996, Decressin, 2002). Unlike the common perception, the transfer of resources between regions takes place more through market-based mechanisms than through the redistribution of resources from central to local governments. It is likely that the more productive regions tend to transfer resources to the less productive ones not only through the fiscal transfer channel but mainly through labour income paid to non-residents. During the sovereign debt crisis, the role of market as a shock absorber, through the factor income, grows thanks to an increase in interregional labour mobility. Additionally, although the credit markets do not seem to play a fundamental role in risk-sharing mechanism in normal times, we find that the credit market channel smooths the idiosyncratic shocks for 36 percent through the same channel in financial crisis. While the pro-cyclical behaviour of capital depreciation is the same during both the normal and recession times.====The remainder of the paper is as follows. In the next section, we discuss the theoretical background and main empirical contributions related to risk sharing. In Section 3, we describe the methodology and data. Section 4 provides the empirical findings, and Section 5 concludes.",Private and public risk sharing across Italian regions,https://www.sciencedirect.com/science/article/pii/S0164070422000635,3 October 2022,2022,Research Article,41.0
"Hinterlang Natascha,Hollmayr Josef","Deutsche Bundesbank, Wilhelm-Epstein-Str. 14, 60431 Frankfurt, Germany","Received 4 February 2022, Revised 23 August 2022, Accepted 2 September 2022, Available online 6 September 2022, Version of Record 14 September 2022.",https://doi.org/10.1016/j.jmacro.2022.103469,Cited by (0),"This paper identifies U.S. monetary and fiscal dominance regimes using machine learning techniques. The algorithms are trained and verified by employing simulated data from Markov-switching ====, before they classify regimes from 1968–2017 using actual U.S. data. All machine learning methods outperform a standard logistic regression concerning the simulated data. Among those the Boosted Ensemble Trees classifier yields the best results. We find clear evidence of fiscal dominance before Volcker. Monetary dominance is detected between 1984–1988, before a fiscally led regime turns up around the stock market crash lasting until 1994. Until the beginning of the new century, monetary dominance is established, while the more recent evidence following the financial crisis is mixed with a tendency towards fiscal dominance.","Since the last financial crisis, U.S. total debt-to-GDP ratio has increased by over 40 percentage points from about 64% in 2008 to 108% in 2019. At the same time, the conduct of monetary policy was characterized by the nominal interest rate being stuck at the zero lower bound and unconventional asset purchase programs. This situation brings back concerns of the fiscal theory of the price level (FTPL) (cf. Sargent and Wallace (1981) and Leeper (1991)) . It states that if the central bank is ==== (does not fight ==== inflationary pressures), the price level might increase to stabilize real outstanding government debt that is not backed by future primary surpluses. Hence, price level changes would not be under control of the monetary authority anymore - a situation that is usually called a fiscal dominance (FD) regime. In contrast, a situation where the monetary authority responds ==== to inflation while the fiscal authority commits to adjust ==== its primary balance to stabilize public debt is known as monetary dominance (MD).====The concepts of MD and FD are of a purely theoretical nature. In reality, the true dominant regime is unobservable and unknown. Distinguishing between both regimes therefore serves as a useful classification application for machine learning techniques since these are shown to be good classifiers in other areas like engineering. We contribute to the literature by providing and applying a new approach for classifying MD/FD regimes in reality. Specifically, the approach consists of four steps. First, we simulate both regimes using a simple Markov-switching dynamic stochastic general equilibrium (MS-DSGE) model as the data generating process (DGP). Second, we use the simulated data to train different machine learning classifiers. Third, we evaluate the predictive performance of the trained classifiers and their robustness with respect to changes in the underlying DGP. Fourth, the trained classifiers are used to identify regimes from 1968 to 2017 with actual U.S. data. We find that all machine learning methods outperform a standard logistic regression with respect to in- and (pseudo-) out-of-sample prediction accuracy using simulated MS-DSGE data. Among the machine learning methods, it is the Boosted Ensemble Trees (AdaBoost) classifier that produces the most reliable predictions with an accuracy rate of about 90%. By applying this trained classifier, we identify historical U.S. regimes from 1968–2017.====In the literature there exist several papers that also try to classify historical periods into FD or MD regimes using different approaches. Favero and Monacelli (2005) estimate fiscal policy rules by Markov-switching (MS) regressions for the period 1960–2002. Davig and Leeper (2011) also estimate Markov-switching fiscal and monetary policy rules over the period from 1948–2008 and incorporate the results in a calibrated DSGE model in order to investigate government spending multipliers. Martin (2015) approaches the interplay between fiscal and monetary dominance from a different perspective. He uses the number of meetings and official phone conversations between the U.S. President and the Fed Chairman as a proxy for central bank independence. Kliem et al., 2016b, Kliem et al., 2016a analyze the monetary–fiscal policy interaction by the low-frequency relationship between inflation and the fiscal stance. The majority of related papers, however, tackles the question by estimating MS-DSGE models with Bayesian methods (see e.g. Bianchi, 2012, Bianchi and Ilut, 2017 and Chen et al. (2019)). Lately, some studies also account for the zero lower bound constraint. Gonzalez-Astudillo (2018), for example, estimates censored MS policy rules and Bianchi and Melosi (2017) estimate a MS-DSGE adding a fiscally-led zero lower bound regime.====Within this literature, there is a broad consensus of a FD regime during the 1970s. However, there is mixed evidence on the switching point. For example, Davig and Leeper (2011) and Bianchi and Ilut (2017) find an explosive regime with both policies being ==== after the appointment of Volcker as the Fed chairman in 1979. MD is established only in early to mid 1980s. Chen et al. (2019), however, find that fiscal policy was ==== until 1995 with monetary policy behaving ==== between 1988 and 1992. The era under U.S. president Clinton is usually associated with an ==== monetary regime accommodated by the fiscal authority (see e.g. Davig and Leeper (2011), Bianchi (2012) and Chen et al. (2019)). In the early to mid 2000s, fiscal policy is found to be ==== again in Davig and Leeper (2011) and Chen et al. (2019). Concerning the more recent periods, Gonzalez-Astudillo (2018) and Bianchi and Melosi (2017) provide evidence for FD after the financial crisis.====We also find clear evidence of FD pre-Volcker with our method, while MD is finally established only in 1984 until 1988. The FD regime is further found to be in place around the stock market crash and the early 1990s recession and after the Dot-com-Bubble crisis in the early 2000s. The evidence for the periods thereafter is mixed with a tendency to FD after the financial crisis.====Machine learning gains more and more attention in economics. Recent applications include forecasting of macroeconomic variables (e.g. Teräsvirta et al. (2005)), early warning predictions for financial crises (e.g. Beutel et al. (2018) and Alessi and Detken (2018)), for recessions (e.g. Ng (2014)) or for default risks (e.g. Badia et al. (2020) and Khandani et al. (2010). However, to the best of our knowledge, this paper is the first one applying machine learning techniques to classify an unobserved economic state using simulated DSGE data.====Concerning our new approach, we would like to highlight four advantages with respect to the existing literature. First, it enables an easier and faster real-time classification of the current regime since the trained algorithm is ready to predict within seconds, given new data of the explanatory variables is available. Second, our classifier is trained and the performance is verified in the first place by using large simulated MS-DSGE datasets, where we know when each regime is in place. Third, and relatedly, due to the simulation, there does not exist a curse of dimensionality because we are not restricted by the time span of our series. Fourth, our procedure focuses on the (nonlinear) interactions of all endogenous variables, while other approaches usually employ only a subset in order to estimate policy-rule parameters and transition probabilities. Thus, given the same DSGE model structure and taken this model as “the truth”, it is possible that our preferred machine learning classifier predicts different regimes than would be implied by directly estimating the model by e.g. Bayesian methods. By experimenting with the number of variables included as predictors, we show that using more information yields overall a better performance in terms of classification accuracy. Our preferred classifier, AdaBoost, is the one that best exploits all given information. Moreover, we can show which variables are especially important to distinguish between both regimes. The AdaBoost classifier attributes a relatively equal importance to all variables slightly favoring interest rate and debt.====The remainder of the paper is organized as follows. Section 2 describes the machine learning methods we employ to classify MD/FD regimes. In Section 3 we present the DSGE models used to simulate data for both regimes as well as the actual U.S. data. In Section 4 we show and discuss the results including robustness checks and variable importance. Section 5 concludes.",Classification of monetary and fiscal dominance regimes using machine learning techniques,https://www.sciencedirect.com/science/article/pii/S0164070422000623,6 September 2022,2022,Research Article,42.0
"Kasuga Hidefumi,Morita Yuichi","Faculty of Economics, Kansai University, 3-3-35 Yamatecho, Suita, Osaka 564-8680, Japan,Graduate School of Economics, Nagoya City University, 1 Yamanohata, Mizuho-cho, Mizuho-ku, Nagoya 467-8501, Japan","Received 23 January 2022, Revised 25 July 2022, Accepted 27 July 2022, Available online 2 August 2022, Version of Record 11 August 2022.",https://doi.org/10.1016/j.jmacro.2022.103461,Cited by (0),"This study investigates the effect of health on the economy by focusing on the health gap within a country. The number of child deaths has declined in most developing economies for several decades, but the gap across income classes within a country remains significant. It is well established that as an economy develops, mortality tends to decline. Consequently, people decrease the number of children and increase the average years of schooling. However, not all people change their lifestyles simultaneously in each country. In this study, we develop a model with heterogeneous households and show that a difference in infant mortality affects the aggregate economy in the long run. The model demonstrates that a larger gap within a country deteriorates the ==== trap and hurts the macroeconomy even if infant mortality declines at the macro level.","For several decades, the average health of the population has improved in most developing economies. The number of deaths among children under five has declined from 12.7 million in 1990 to almost 6 million in 2015 globally (United Nations, 2015). Improving the population’s health per se is a major objective of developing countries. Essentially, better health positively affects education and income at the individual level (Bleakley, 2010; Lucas, 2010). Cohen and Leker (2016) use a cohort-based measure of education and show that life expectancy positively affects education. Earlier studies also show that a decline in child mortality decreases fertility and increases education investment (Kalemli-Ozcan, 2002; Weisdorf, 2004; Soares, 2005). Theoretically, the relationship between child mortality and fertility is not necessarily conclusive.==== In reality, however, infant mortality, fertility, and population growth have declined in most advanced economies since the end of the nineteenth century. As a result, this demographic transition enhances labor productivity by increasing education expenditure per child. The negative relationship between the number of children and education expenditure per child is called the quantity–quality tradeoff, which can explain why low fertility enhances labor productivity.==== Evidently, a large literature shows that when more children survive, fertility rates decline, and education investment increases.==== This result implies that low infant mortality can spur economic growth.====However, empirical evidence does not necessarily support the positive effect of health on economic growth. Acemoglu and Johnson (2007) show that better health can decrease GDP per capita because it enhances population growth. Bucci et al. (2019) show that because of a dilution effect, population growth can harm economic growth. These results imply that while better health leads to population growth, it does not necessarily lead to per capita GDP growth. Furthermore, Cervellati and Sunde (2011) show that life expectancy negatively affects economic growth in countries before the demographic transition. Thus, while microeconomic evidence shows that better health positively affects education and income, macroeconomic evidence suggests that low mortality rates may hurt per capita income growth. We address this inconsistency.====To explain why the observed improvement in the average health of the population can negatively affect the macroeconomy, we focus on the health gap within a country. While the average health of the population has improved in many countries, the health gap remains significant. Not everyone gains from the improved quality of medical care (WHO Regional Office for Africa, 2010). Significant differences persist in infant mortality within a country. Fig. 1 shows infant mortality (per 1,000 live births) by wealth for developing countries during the period 1990–2020.==== The Figure shows infant mortality for Quintile 1 (the poor), Quintile 3 (the middle class), and Quintile 5 (the rich). As a whole, mortality rates are high for the poor and low for the rich, while they have decreased in each class, as the fitted line suggests. While infant mortality has decreased slightly faster for the poor, there are still large gaps between the rich and the poor. For example, Guinea witnessed decreased infant mortality from 70 to 34 for Quintile 5 during the period 1999–2018, but the gap between Quintile 1 and Quintile 5 merely decreased from 49 to 43. In Nigeria, infant mortality decreased from 69 to 40 for Quintile 5 during the period 1990–2018, but the gap increased from 33 to 38.====A difference in infant mortality between the rich and the poor can lead to a disparity in fertility. In fact, a large difference in fertility exists between the rich and the poor in developing countries. Fig. 2 shows the number of children for a woman during the period 1990–2020.==== The Figure shows that the number of children is larger for Quintile 1 (which has higher infant mortality) than Quintile 5. The number has decreased in recent decades regardless of the level of wealth. However, there is a difference in the mean number of children between the rich and the poor. In most countries, after 2010, the number of children is between 4 and 8 for Quintile 1 and between 2 and 5 for Quintile 5. Thus, poor households have more children than rich households.====In addition, the number of children is negatively related to education. The literature shows that fertility depends negatively on tertiary education for women in many developing countries (Ainsworth et al., 1996). This relationship suggests that fertility tends to decline as tertiary enrollment rises. Indeed, tertiary enrollment is rising in many developing countries, yet there remain significant differences across income levels within a country. Fig. 3 shows attendance for tertiary education by income during the period 1997–2020.==== The Figure shows that the attendance ratio is higher for Quintile 5 than Quintile 1. Given the gap in the number of children between the rich and the poor, the educational gap in Fig. 3 implies that having fewer children is associated with longer years of schooling, as shown in the literature. The fitted line shows an increasing trend for Quintiles 1 and 3. Although the slope is flat for Quintile 5, there is an increasing trend for Sub-Sahara African countries (below the fitted line), where the attendance ratios were low in the early 2000s. The ratios were already quite high for many Latin American countries (above the fitted line). In summary, Fig. 1, Fig. 2, Fig. 3 demonstrate that, in recent decades, lifespan and the years of schooling have increased, and people bear fewer children on average. However, there are still immense gaps between the rich and the poor. Earlier studies explain the relationship between mortality, fertility, and education using the national average of variables but ignore disparities within a country. A decline in the average infant mortality rate does not necessarily imply that every child has a longer life expectancy; accordingly, it does not necessarily mean that every household decreases the number of children and increases education investment. In reality, there are disparities within a country. The literature is not conclusive on how a disparity in infant mortality affects the economy.====The first question of this study is how differences in infant mortality across households are related to differences in years of schooling, wages, and the number of children. More specifically, we examine whether and how the mortality gap across income classes increases differences in wage and fertility. To answer this question, we focus on lifestyle changes. When infant mortality is high (before the demographic transition), most people start working in their late teens without receiving higher education and conceive many children. As infant mortality declines (during the transition period), more people go to college and start working in their twenties. Accordingly, college graduates have their first child in their twenties or thirties. Notably, not all people in the economy change their lifestyles simultaneously during the transition period. In the early stage of the transition, only wealthy people receive higher education and delay the age at first birth. Fig. 4 shows the median age at first birth during the period 1990–2020.==== The figure demonstrates that the median age at first birth for Quintile 5 is higher and on an upward trend. It implies that education attainment delays the age at first birth (Brien and Lillard, 1994; Glick et al., 2015). Contrastingly, the median age at first birth for the poor shows little change (the slope of the fitted line is flatter for Quintile 1). Thus, the rich have changed their lifestyles significantly in the last few decades, but the poor have not. To focus on this disparity within a country, we introduce heterogeneity of household behaviors. The model shows that declining infant mortality leads to people’s lifestyle changes, and a difference in infant mortality explains differences in education, income, and the number of children.====Another important question is whether and how a gap in infant mortality between income classes affects macroeconomic outcomes in the long run. We use the model with heterogeneous households to measure economic development as the share of skilled workers who earn more and have fewer children in the economy (rather than average human capital). The share of skilled workers is associated positively with the productivity of the economy and negatively with population growth. Due to the heterogeneity of household behaviors, the model describes the accumulation of human capital and population growth differently from representative agent models. This feature explains how heterogeneity affects the macroeconomy. Studies focusing on household heterogeneity have emphasized that relatively poor households cannot invest in human capital due to credit constraints, and hence disparities can deter economic development (Galor and Zeira, 1993; Galor and Moav, 2004). Other studies focusing on fertility differentials show that households with many children decrease the education cost per child, and hence economies with high fertility rates invest less in human capital (Morand, 1999; Kremer and Chen, 2002; de la Croix and Doepke, 2003; Moav, 2005). Thus, many studies examine the impact of household heterogeneity on economic development. However, little attention has been devoted to differences in mortality within a country. As a result, little is known about how the mortality gap affects macroeconomic outcomes. Fundamentally, the model in this study focuses on the disparity in infant mortality and its effect on the macroeconomy via changes in household behaviors. We show that a gap in infant mortality decreases the supply of skilled labor and increases the wage gap (the skill premium), which negatively affects income per capita.====The mechanism linking the mortality gap to the aggregate economy is as follows. If only wealthy people can benefit from medical progress, such as state-of-the-art treatment, the gap across income classes can rise even when the average mortality rate declines. A larger gap decreases the number of college graduates and positively affects fertility. Thus, the lack of skilled workers deters productivity growth. In contrast, if relatively poor people also enjoy the benefit of medical progress (after infant mortality for the rich has decreased sufficiently), the gap and the average mortality rate decline. Subsequently, more people receive higher education; an increase in skilled workers enhances productivity growth, and fertility declines. Thus, when infant mortality decreases on average, its effect on productivity can differ. This non-monotonic effect of better health is consistent with prior studies. Nonetheless, it is worth noting that our study explains how declining mortality and its gap change household behavior.====The remainder of the paper is structured as follows. Section 2 presents the basic model. Section 3 examines how the cost of education and medical progress affect economic development using a model where the survival gap interacts with the wage gap. Finally, Section 4 concludes the paper.",The health gap and its effect on economic outcomes,https://www.sciencedirect.com/science/article/pii/S0164070422000544,2 August 2022,2022,Research Article,43.0
Coşkun Sevgi,"Istanbul Medeniyet University, Faculty of Political Sciences, Economics Department, Istanbul, Turkey","Received 27 April 2021, Revised 28 April 2022, Accepted 17 July 2022, Available online 30 July 2022, Version of Record 17 August 2022.",https://doi.org/10.1016/j.jmacro.2022.103452,Cited by (1),"We investigate how informal employment (====) is relevant for shaping the aggregate dynamics in EMEs. Empirical evidence suggests that it is (i) countercyclical in Mexico, Colombia, and Turkey and (ii) negatively related to formal employment (==== and business cycles in EMEs and (ii) the extent to which ==== acts as a buffer to the labour market in the face of adverse shocks. We find that this model can capture key stylized facts of the labour markets in these economies. Our model suggests that ==== acts as a buffer and propagates positivity during adverse shocks given its countercyclicality while ==== is pro-cyclical, findings supported by the documented empirical evidence. Regarding volatilities, ==== does not act as a buffer since ==== is more volatile than ==== in the model which contrasts with the evidence in the data for these economies except Colombia.","Business cycles in emerging and developed countries have different characteristics. Specifically, the data used in this study for emerging market economies (EMEs) shows that the relative volatility of consumption to output is approximately 1.10, while in developed countries it is around 0.76. In addition, emerging countries display higher volatility of output and a counter-cyclical trade balance share. Another difference between developed and EMEs business cycles is the behaviour of employment. In contrast to developed economies, EMEs have a lower cyclicality of total employment. In the emerging market business cycles literature, many researchers focus on the sources of business cycles in these economies using dynamic general equilibrium (DSGE) models and discuss how frictions matter for the propagation of technology or interest rate shocks.==== However, they mostly ignore labour market frictions.==== Surprisingly, even much less attention has been given to informal employment and the inclusion of labour adjustment costs (LACs) in the study of labour market employment dynamics and business cycles in emerging economics, despite the existence of large informal labour markets in these economies and the documented heterogeneity in their employment protection laws that induce varying LACs across sectors in emerging economies. These laws are known to vary from very strict, i.e. India, China, Czech Republic etc., to very lax, i.e. Brazil, Costa Rica, Bolivia etc., and to poor enforcement or non-existent, i.e. most African countries Ghosheh (2013), even though these economies are classified homogeneously under the same umbrella as emerging/developing economies (see Fig. 1).==== Also, the size of informal employment varies widely depending on the country and the measure used, but our data reveals that informal employment is between 28% and 80% of the total labour force in these economies.==== Some of the differences between emerging and developed economies over the cycle might be linked to the large informal labour market in EMEs.====This paper is motivated by these observations and has two objectives. The first objective is to systematically document the relationship between informal employment and business cycles in EMEs and to understand how informal employment is relevant in shaping the aggregate dynamics. The second objective is to lay out a small open economy model with both formal and informal labour markets as in Fernandez and Meza (2015) to account for the empirical findings. As there is lack of appropriate data for these economies, the first contribution of the paper is to construct and analyse several alternative time series of informal employment at a quarterly frequency for Mexico, Colombia, and Turkey, as representative of emerging economies.==== Given the various types of informal employment, one should not expect all of them to respond in the same way to business cycles. Moreover, some empirical evidence shows that informal employment acts as a buffer for fluctuations in formal employment, increasing flexibility in the labour market and consequently affecting the transmission mechanisms of shocks to the economy (see Castillo and Montoro (2010)’s work).==== However, we observe that employment protection is high and different among these economies. For instance, severance pay for redundancy dismissal (in salary week) is 22.0 in Mexico, 23.1 in Turkey, 16.7 in Colombia.==== The second contribution of the paper is hence to explore the effects of changes in the degree of employment protection on the informal employment and the business cycles in EMEs to which extent the informal sector acts as a buffer in the face of adverse shocks to the labour market.====There are few papers that explicitly model informality in a DSGE framework even though the idea of working on the informal sector is not new. Fiess et al. (2010) focus on accounting for the cyclical behaviour of informal self-employment. For that, they present a small open economy model where a salaried sector produces a tradable good while the self-employed sector produces a non-tradable good. Restrepo-Echavarria (2014) examines how the relative volatility of consumption to GDP is affected by the presence of a shadow economy and quantifies it in a two-sector DSGE model with formal and informal consumption goods. She finds that including the informal economy accounts for relative volatility of consumption. Solis-Garcia and Xie (2017) build a model that includes an informal economy and distinguish between measured (formal) and total (informal and formal) output to account for the differences in the volatility of measured real GDP per capita between developing and developed countries. For the New Keynesian framework, Castillo and Montoro (2010) use a model with frictions in the labour market by introducing formal and informal labour contracts and analyse the interaction between the two sectors and monetary policy. They find that informal economy generates a buffer effect that diminishes the pressure of demand shocks on aggregate wages and inflation.====We first highlight the differences regarding aggregate employment between EMEs and Canada, as a similar developed counterpart. The relative volatility of employment is higher in Colombia and Turkey but lower in Mexico. Furthermore, the correlation of employment with output is substantially lower in these economies compared to Canada. We then document the facts about the informal employment in EMEs. The results show that the behaviour of informal employment varies depending on the measure and the country. It will be a challenge for us to evaluate the model regarding its performance along the second moments for these economies. Hence, we assume that people are self-employed in the informal sector as it is a good proxy for informality in these economies.==== Based on this measure, informal employment is countercyclical in Mexico, Colombia, and Turkey. Besides, it is negatively correlated with formal employment in Mexico but positively correlated in Colombia and Turkey. Also, informal employment is more volatile than formal employment in Mexico, but it is less than that of Colombia and Turkey. We claim that the differences in the degree of employment protection among countries can explain the differences in data moments between EMEs.====Motivated by these stylized facts, the model in this paper builds on the works of Fernandez and Meza (2015) where there is a small open economy with both formal and informal labour markets. In this model, we assume that households choose how much labour to allocate to each market. As in Aguiar and Gopinath (2007), technology in the formal sector is shocks to the growth rate of labour-augmenting productivity. These shocks in the formal sector are pass-through to the informal sector. More specifically, the shocks to both markets are imperfectly correlated. It is worth emphasizing that our model shares many of the features of that of Fernandez and Meza (2015) since both models try to understand informal employment over the business cycle. However, their model does not answer our questions properly as they ignore labour market regulations, such as employment protection laws, which are highly different among EMEs. We hence introduce labour adjustment costs (LACs) in their model following Fairise and Langot (1994) and Janko (2008). The inclusion of LACs aims to reflect or capture the penalty or costs borne to adjust labour due to the existence of implementable employment protection laws in these economies. We also consider temporary productivity shocks. These shocks are important because if we only include permanent shocks in the model, then the firms must fire the workers as employment protection may be a temporary rigidity.==== In addition, we estimate the model by minimizing the distance of a set of second moments with respect to the data for each country. We do so for our key parameter values in order to show how the differences in LACs affect the business cycles in these economies as well as the standard deviations of shocks and the pass-through of the shocks. This will then allow us to explore which extent the informal sector acts as a buffer for the formal sector for these economies, given that level of protection.====The model is then evaluated regarding its performance along the second moments that describe business cycles in EMEs. In our benchmark model, we estimate the model by assuming that the pass-through of both shocks from the formal to the informal sector is different as well as LACs are not equal to each other among sectors (Case 1). We find that this model produces the negative (positive) relationship between formal and informal employment in Mexico (Colombia). It can also generate the relative volatility of formal employment and total employment for Mexico. For Colombia, this model can satisfactorily explain the relative volatility of consumption, the correlation between consumption and output as well as the correlation between total employment and output. For Turkey, this model tends to perform well in explaining the volatility of output, autocorrelation of output and the correlation between formal employment and output. In addition, we observe that informal employment acts as a buffer as it is countercyclical while formal employment is procyclical. This supports the evidence in the data for these economies. Regarding volatilities, the model shows that the formal employment is more volatile than the informal employment. That is, informal employment does not act as a buffer in the formal sector which contrasts with the evidence in the data for these economies except Colombia.====To make a comparative analysis, in Case 2, we then assume that there are different pass-through of shocks and no LACs in the informal sector. In Case 3, we estimate the model by assuming that there are different pass-through of shocks and no LACs in both sectors. In Case 4, we estimate the model by assuming that labour adjustment costs are not equal to each other between sectors while the degree of propagation of shocks from the formal to the informal sector is the same across shocks. In Case 5, the model has the same pass-through of shocks in both sectors but LACs only exists in the formal sector. Lastly, in Case 6, the model has the same pass-through of shocks in both sectors but no LACs in both sectors.====Overall, we find that these alternatives improve the performance of our model to account for the positive/negative correlation between formal and informal employment as well as the countercyclicality of informal employment in these economies compared to Fernandez and Meza (2015)’s work. Our observation is that a country which has a lower LACs tends to have higher formal employment volatility, lower counter-cyclicality of informal employment, lower pro-cyclicality of formal employment and, consequently, lower pro-cyclicality of total employment. Hence, the degree of countries’ employment protection laws, reflected in their LACs, is important for understanding how employment sectors respond to economic shocks. To the best of our knowledge this finding has never been documented for the informal sector. Nonetheless, we note that the size of the shocks has a stronger impact on outcomes in the formal and informal sectors compared to the LACs.====In addition, the size of shocks in our model is lower compared to earlier studies in the literature such as Aguiar and Gopinath (2007). Nonetheless, despite the relatively lower size of shocks in our model, we still managed to correctly replicate the dynamics of the aggregate variables for these economics. Thus, our model with informal sector must possess a distinctively powerful propagation mechanism for shocks to be able to successfully replicate the aggregate variables in these economies despite the smaller size of shocks applied. Lastly, the value of pass-through of shocks is important; as it falls, volatility in the labour market increases. Our study motivates the empirical research presented in this paper towards the estimation of LACs, the standard deviation of these shocks, and the pass-through of shocks in EMEs as the behaviour of the employment variables change in response to changes in these parameter values. We also examine whether informal sector employment acts as a buffer for employment in the formal sector.====The rest of this paper is organized as follows. Section 2 presents the role of labour adjustment costs in the real world. Section 3 provides the data and the empirical findings. Section 4 presents the model, solution of the model and the calibration. Section 5 presents the main results. Finally, we conclude in Section 6.",Informal employment and business cycles in emerging market economies,https://www.sciencedirect.com/science/article/pii/S0164070422000489,30 July 2022,2022,Research Article,44.0
"Salas Sergio,Odell Kathleen","Facultad de Administración y Economía, Universidad Diego Portales, Chile,Dominican University, United States of America","Received 4 June 2021, Revised 21 December 2021, Accepted 17 July 2022, Available online 29 July 2022, Version of Record 9 August 2022.",https://doi.org/10.1016/j.jmacro.2022.103459,Cited by (0),"Mounting evidence suggests a non-monotonic relationship between ==== and growth: increases in credit over GDP lead to increases in growth at diminishing rates, and after a point growth decreases. We propose a theory based on liquidity risks that delivers this result. Our theory features a tension between the high return to capital and its illiquid nature. This tension is alleviated by the availability of credit. Initially, further access to credit offsets the detrimental effects of illiquid capital, facilitating investment and growth. Beyond some threshold however, expanded access to credit induces private bonds to compete with capital as a means of savings, which becomes detrimental to growth.","The relationship between finance and growth has been the subject of ongoing research since seminal contributions such as Schumpeter (1912), Gurley and Shaw (1955), Goldsmith (1969) and McKinnon (1973). Recently, the empirical literature has identified a non-monotonic relationship between the two, where there is a threshold level after which finance becomes detrimental to growth. Examples can be found in Cecchetti and Kharroubi (2012), Law and Singh (2014) and Arcand et al. (2015). This paper helps explain this non-monotonic relationship within the context of a general equilibrium optimizing model.====In the spirit of Diamond and Dybvig (1983), Levine (1991), and Bencivenga and Smith (1991) who use liquidity risks as a driver for the existence of financial transactions, we construct an endogenous growth model that links credit markets with the growth rate of output. We assume that the economy’s production function is of the “AK” type, hence the stock of capital is a broad concept that may include physical, human, and other types of capital that can be accumulated.====We model an economy with infinitely lived agents in discrete time, who at the beginning of each period decide on their capital investments for the next period. The defining characteristic that elicits the emergence of credit markets is that within each period, after capital decisions are made, agents receive privately observed, idiosyncratic, and uninsurable liquidity shocks that alter their marginal utility of consumption. The shocks therefore induce them to modify their consumption-savings decisions. We abstract from securities being traded over capital, but assume the emergence of a credit market that facilitates the transfer of resources among agents that differ in their liquidity needs. The “AK” assumption therefore, not only helps to build a parsimonious model of finance and growth, but, since it is hard to justify the emergence of securities traded over a broad concept of capital, shifts the focus and gives relevance to credit markets.==== The modeling choice to introduce credit markets as the institution facilitating the transfer of resources among agents accords well with the empirical findings we aim to explain. Empirical researchers have almost unanimously used credit over GDP as the notion of finance in uncovering the non-monotone result on finance and growth.====At the beginning of each period, agents facing the liquidity risk have an incentive to cut down on investments, which due to the “AK” assumption on technology, directly impacts growth. Credit markets, however, have an influence on their investment decisions, since the ex-post liquidity availability through credit markets would decrease the ex-ante expected cost of illiquid capital investments. Our model imposes limitations on the credit available to each agent using a simplified approach. Credit frictions are determined by an exogenous parameter ==== which indicates the maximum fraction of the beginning-of-period capital that can be used as collateral for borrowing. Although ==== is a reduced form parameter, the strength of this approach is that we do not need to identify a particular mechanism to introduce better access to finance. Changes in regulatory environments, financial deregulation, and efficiency of the financial markets are all notions that can be linked to changes in ====.====We study successive relaxations in the credit constraint parameter ==== and its implications for long-run growth, and find that the economy’s rate of growth is non-monotonic in ====. Initially, a less credit-constrained economy delivers higher growth, but after some threshold, growth decreases. However, credit over GDP, a common measure of “financial depth” in the empirical literature, does not show this non-monotone behavior, but rather increases continuously with ====. Therefore, our model delivers the “too much finance” result empirically identified by Arcand et al. (2015) and others.====A looser credit constraint causes two counter-directional responses in agents’ decisions at the beginning of each period. First, the expected marginal cost of investment falls as the collateral value of capital is increased. This incentivizes investment and growth. Second, higher collateral and an increase in borrowing demand push the equilibrium interest rate upwards. This tends to disincentivize capital investments as agents choose to save, taking advantage of the high interest rate, rather than invest in capital which due to the “AK” assumption has a fixed return. As a result, the expected marginal benefit of investment falls leading to a decrease in investment and growth.====It is these two effects, working in opposite directions, that drive the non-monotonic effect on growth. In the initial relaxations of the credit constraint parameter ====, while the interest rate is relatively low, the decrease in expected marginal cost due to more credit availability outweighs the decrease in the expected marginal benefit of investment, and investment and growth increase. Eventually, further relaxations of the credit constraint induce interest rates to be high enough as to make the decrease in the marginal benefit of investment the dominant effect. As agents substitute away from investment toward bonds as a means of savings, investment and hence growth decline.====Methodologically, our paper differs from previous literature on the finance and growth relationship by assuming infinitely lived agents that make a portfolio decision in two stages. First they decide on the illiquid capital investments, and then, once liquidity shocks are observed, they use private bonds to finance their consumption needs. The idiosyncratic liquidity shocks will induce changing wealth distributions that are persistent over time. Without sacrificing generality, using a Guess-and-Verify method, we are able to obtain closed form solutions for their policy functions that are linear in their wealth and hence aggregation is straightforward. Similar methods of solution have been used in portfolio models since Samuelson (1969), but the model we develop has not been proposed in the literature, to the best of our knowledge. Specifically, the timing structure of investment decisions and the information revelation for the liquidity shock, under a general equilibrium growth model of infinitely lived agents, have not been used before, and the standard Guess-and-Verify method was modified to fit our particular problem.====Our paper is related to the earlier finance growth theoretical models such as Levine (1991), Bencivenga et al. (1995) and Bencivenga and Smith (1991). Levine (1991) is a seminal contribution where there is an endogenous formation of equity markets and growth is endogenous through an externality in the accumulation of human capital. Our model differs in several methodological aspects. For example, not relying on an overlapping generations model and abstracting altogether from equity markets. We do, however, rely on a similar modeling device to introduce liquidity needs. Bencivenga et al. (1995) focus on the nexus of liquidity and the adoption of technologies; they find that depending on the capital structure, transaction cost reductions may enhance or reduce growth. Bencivenga and Smith (1991) develop a growth model in which impediments to the emergence of liquid equity markets give banks a role in increasing investment in a high return, illiquid asset, which increases growth. The notion that a credit market is beneficial in the absence of a sophisticated equity market is implicitly present in our set up, but as explained before, we make simplifying assumptions regarding the illiquidity of capital investments and the role of credit markets. These assumptions capture the central forces introduced in previous studies, and underpin our main contribution: we develop a flexible environment in which the question of the non-monotonic relationship between credit and growth can be analyzed. Such analysis has not been pursued in earlier models of this type.====Recently, there have been some theoretical contributions attempting to deliver the bell-shaped relationship between credit and growth using less explicit modeling approaches. Recent valuable efforts are (Bucci and Marsiglio, 2019) and Bucci and Marsiglio (2020), who impose ad-hoc but useful ways in which financial depth may increase productivity but at the same time reduce capital investments through crowding out. Another contribution is (Salas, 2017), where such a non-monotonic result is found, and depends on the distribution of costs of investment which are assumed to be heterogeneous across the population. This result is found, however, in a highly stylized model of financial frictions.====The rest of this paper is organized as follows. Section 2 presents the model. Section 3 presents the main analysis, including the derivation of the bell-shaped relationship between credit and growth. Section 4 presents the calibration of the model as well as a sensitivity analysis. Section 5 discusses some model extensions to check the robustness of our results. Section 6 presents suggestive evidence of the model’s mechanism. Section 7 offers some concluding remarks and in the Appendix we present the details of Sections 5 Robustness checks, 6 Evidence on the mechanism.",Illiquid investments and the non-monotone relationship between credit and growth,https://www.sciencedirect.com/science/article/pii/S0164070422000532,29 July 2022,2022,Research Article,45.0
"Guo Jang-Ting,Zhang Yan","Department of Economics, 3133 Sproul Hall, University of California, Riverside, CA, 92521, USA,Wenlan School of Business, Zhongnan University of Economics and Law, 182 Nanhu Avenue, Wuhan, 430073, People’s Republic of China","Received 20 January 2022, Revised 20 May 2022, Accepted 17 July 2022, Available online 29 July 2022, Version of Record 5 August 2022.",https://doi.org/10.1016/j.jmacro.2022.103455,Cited by (0)," are stabilizing instruments that will not lead to business cycle fluctuations driven by animal spirits. Moreover, this no-indeterminacy result and associated dynamic equivalence between endogenous labor versus consumption taxation remain qualitatively robust to various modifications in the household-preference or firm-production formulations.","Understanding the business-cycle (de)stabilization effects of a balanced-budget rule has attracted much attention in the macroeconomics literature. This is an important research topic not only for its theoretical insights, but also for its broad implications for the design, implementation and evaluation of stabilization fiscal policies. In the context a standard one-sector real business cycle (RBC) model characterized by perfect competition and constant returns-to-scale in production, Schmitt-Grohé and Uribe (1997, section II) analytically show that when the balanced-budget policy rule is postulated to consist of constant government spending and proportional taxation on the household’s labor income, a Laffer curve-type relationship between the labor tax rate and the resulting tax revenue will emerge, which in turn may lead to the existence of two interior stationary equilibria. These authors then derive the necessary and sufficient condition under which their model’s low-tax steady state is an indeterminate sink that can be exploited to generate endogenous cyclical fluctuations caused by animal spirits.==== Subsequently, Giannitsarou (2007, section 1) explores the equilibrium dynamics of an identical RBC macroeconomy, but with a slightly different tax scheme: a pre-set fixed level of public expenditures are financed by endogenous taxation on the household’s consumption spending. In this environment, the economy is found to possess a unique interior steady state that is always a locally determinate saddle point; hence indeterminacy and sunspots will be completely ruled out. From a policy perspective, these earlier results altogether suggest that if the government intends to insulate the macroeconomy from business cycles driven by agents’ self-fulfilling expectations, a balanced-budget rule with endogenous consumption taxation (instead of labor income taxation) is called for.====Parallel to the above-referenced work and many previous pieces of related research on a representative-agent macroeconomy, we will examine the aggregate (in)stability effects of the same type of balanced-budget formulation within another “workhorse”analytical framework in modern macroeconomics – a stylized two-period overlapping generations (OLG) model ==== Diamond (1965) – whereby constant government purchases are financed by endogenously determined labor or consumption taxes. Each agent is postulated to supply labor hours as well as accumulating physical capital when young; and consume in both time periods of her lifetime. As in the baseline setting of Schmitt-Grohé and Uribe (1997) and Giannitsarou (2007), our analysis begins with (i) an additively separable utility function that is logarithmic in consumption and convex in hours worked, and (ii) a Cobb–Douglas production technology that exhibits constant returns-to-scale in capital and labor inputs. These preference and technological specifications will facilitate the comparison of this paper’s findings versus those from the existing RBC-based studies in a direct and transparent manner.====Under endogenous labor income taxation, we find that due to the opposite directions and identical strength of the income versus substitution effects, each young individual’s optimal labor supply is a fixed constant that is independent of the time-varying labor tax rate. In addition, the model’s equilibrium dynamics is governed by a scalar difference equation in capital that turns out to exhibit a negative vertical intercept, followed by an increasing concave curve which will intercept the ====-degree line twice provided the pre-specified level of public spending is lower than the revenue-maximizing counterpart. It is then straightforward to analytically show that the low-capital (high-tax) steady state is asymptotically unstable, whereas the high-capital (low-tax) steady state is asymptotically stable. When households become optimistic about the economy’s future, they will choose to consume less and invest more today. While the resulting increase in the future capital stock raises next period’s output and consumption (but no impact on the equilibrium level of hours worked), it also leads to a decrease in the return of today’s investment spurt because of diminishing marginal product of capital.==== It follows that agents’ initial optimism cannot validate this alternative dynamic trajectory as a self-fulfilling equilibrium, and that both interior stationary equilibria will be locally isolated.==== In sharp contrast to Schmitt-Grohé and Uribe (1997, section II), these results illustrate that macroeconomic instability caused by endogenous belief-driven cyclical fluctuations do not arise in the benchmark version of our two-period overlapping generations model with endogenously determined labor taxes.====When government purchases are financed by endogenous consumption taxation over both periods of an individual’s lifetime, we derive that her optimal labor supply remains at the same constant level as that under labor income taxation, indicating the exact cancellation of intratemporal income and substitution effects as well.==== In this case, the scalar difference equation in capital that characterizes this setting’s equilibrium dynamics is analytically shown to exhibit a unique and asymptotically stable positive steady state. Upon the anticipation of a decrease in the future consumption tax rate, the young agent chooses to consume less (thus save more) today and raise her old-age consumption due to the stronger intertemporal substitution effect. The associated increase in the next period’s capital stock will reduce the corresponding real interest rate because the firms’ production technology is strictly concave. We find that the overall effect turns out to be a lower rate of return on capital investment adjusted for consumption taxes, which in turn invalidates households’ expectation of a subsequent tax cut. It follows that the economy’s unique interior stationary state is locally isolated around which animal spirits cannot be a driving force of business cycle fluctuations, and that Giannitsarou’s (2007, section 1) determinacy result will continue to hold within our two-period OLG macroeconomy.====From a comparative perspective, the proceeding analysis illustrates that under the same baseline preference and technological formulations, the aggregate (in)stability effects of an endogenous-tax policy rule within Diamond’s (1965) two-period overlapping generations model are quite different from those of Schmitt-Grohé and Uribe (1997, section II) and Giannitsarou (2007, section 1) for a one-sector RBC macroeconomy. In particular, our OLG framework always exhibits local determinacy and equilibrium uniqueness, hence both labor and consumption taxation are operating as automatic stabilizers that will not induce cyclical fluctuations driven by agents’ changing non-fundamental expectations. This finding in turn provides an interesting extension of Atkinson and Stiglitz (1980, pp. 69-72) dynamic equivalence between proportional constant labor versus consumption tax rates to a macroeconomic context with time varying and endogenously determined taxation systems.====Our sensitivity analyses begin with considering the young agent’s preference formulation to possess a constant degree of relative risk aversion ( ==== the inverse for the intertemporal elasticity of substitution in consumption) that is not equal to one. Under endogenous labor income taxation, we first analytically derive the respective necessary and sufficient conditions with which the model’s normalized steady state (NSS) is a saddle point/sink/source. In a calibrated version of this CRRA macroeconomy, quantitative simulations show that the low-tax stationary equilibrium exhibits local determinacy, and that the high-tax stationary equilibrium is a totally unstable source. Under endogenous consumption taxation, a general characterization of macroeconomic dynamics turns out to be exceedingly cumbersome to comprehend, thus our stability study of this case is restricted to the same empirically-plausible parametric region as that with labor taxes. This in turn significantly streamlines the subsequent exposition and discussions to be understandable. For each feasible parameterization, we obtain the most binding restriction among the analytical if-and-only-if conditions for local uniqueness, and then numerically verify that the economy’s sole positive steady state ( ==== the NSS) is always saddle-point stable. Intuitively, the equality of the relevant consumption Euler equation will no longer hold upon an optimistic belief about the macroeconomy’s future, thereby eliminating the possibility of sunspot-driven business cycles within either fiscal environment.====We further find that our no-indeterminacy result discussed above will continue to prevail with the following two variations to the benchmark utility or production setup: (i) when the social technology displays increasing returns-to-scale due to positive productive externalities from aggregate capital and labor inputs; and (ii) when each young agent’s labor supply decision exhibits no income effect with respect to a change in the labor-income/consumption tax rate. Under endogenous labor income taxation, the economy’s low-tax steady state is found to be asymptotically stable in both settings; whereas the high-tax stationary equilibrium is asymptotically unstable in case (i), and asymptotically stable/unstable in case (ii). Under endogenous consumption taxation, the economy’s unique interior steady state is asymptotically stable within either specification. It follows that our two-period overlapping generations model always exhibits local determinacy and equilibrium uniqueness under the postulated balanced-budget rule along with various modifications to the household-preference or firm-production formulations; and that both labor and consumption taxation always serve as stabilizing instruments which do not lead to endogenous cyclical fluctuations.====The remainder of this paper is organized as follows. Section 2 describes our two-period overlapping generations model and analytically examines its equilibrium dynamics under endogenous labor income taxation. Section 3 studies the same baseline framework’s local (in)stability properties under endogenous consumption taxation, and then discusses the linkage of our no-indeterminacy result with the Atkinson–Stiglitz dynamic equivalence. Section 4 investigates our OLG macroeconomy with the household utility that exhibits a constant and non-unitary degree of relative risk aversion. Section 5 considers two additional extensions to re-examine the benchmark economy’s equilibrium (in)determinacy attributes. Section 6 concludes.",Balanced-budget rules and macroeconomic stability with overlapping generations,https://www.sciencedirect.com/science/article/pii/S0164070422000507,29 July 2022,2022,Research Article,46.0
"Kelly Mark,Kuhn Michael","Baylor University, United States,International Institute of Applied Systems Analysis (IIASA), Laxenburg, Austria,Wittgenstein Centre (IIASA, OeAW, University of Vienna), Vienna Institute of Demography, Vienna, Austria","Received 12 April 2021, Revised 16 May 2022, Accepted 17 July 2022, Available online 28 July 2022, Version of Record 3 August 2022.",https://doi.org/10.1016/j.jmacro.2022.103451,Cited by (1), response of the economy of shocks to productivity/income and medical effectiveness. Our analysis suggests that the optimal response to an increase in the demand for health care depends strongly on whether it is due to an increase in income or medical effectiveness. We also show that there is disagreement across age-groups on the preferred policy.,"Between 1990–2014 the average annual growth rate of the output share of health care among OECD nations was approximately 1.34%. Moving forward, the rapid rise in the utilization of government-financed health care services threatens fiscal sustainability of many OECD countries. Consequently, most developed nations have pursued cost containment reforms that attempt to slow the growth rate of health care expenditures. Such policies can be broadly classified into one of two categories; price controls and resource rationing. Price controls exist in virtually all public insurance schemes and are implemented in order to directly influence the cost of financing health care by fixing the prices that suppliers can charge for the services they provide.====Resource rationing, on the other hand, is slightly less common and is most effective in health care systems where the government is a major provider of health care services. In this case, the government is attempting to contain the cost of health care indirectly by limiting its supply. There are many different varieties of resource rationing, many of which rely on waiting (see e.g. Siciliani et al., 2014 for a recent survey) as a more or less explicit rationing mechanism. More generally, waiting can be understood as a form of congestion. We should stress at this point that although we broadly frame our paper in the context of waiting, not the least because we use waiting time data for the calibration of our model, our general notion of congestion is broader and includes other more implicit forms of rationing, such as reductions in consultation times or reductions in the quality of care.====Specifically, our study focuses on a rationing regime whereby the policymaker decides on a given level of supply of health care services (as measured in total hours of treatment available) and allocates it, in the event that demand exceeds the fixed supply, via a waiting list, such that the effective utilization (hours demanded net of waiting time) equals the supply. By reducing the effectiveness of health care and increasing the time cost to the individual, waiting serves as a mechanism to contain demand. By maintaining a larger capacity within the health care system, the policymaker can to some extent control waiting times. This is well-illustrated by the English National Health Service (NHS), where the development of waiting times in NHS hospitals over the time span 1999–2017 is following a trend that is broadly opposite to the trend in the NHS spending share (see Fig. 1).====Considering the macroeconomic efficiency of the public provision of health care services is subject to congestion, three key issues emerge: (i) Which mechanisms determine the allocation of health care and its outcomes through waiting and what are the macroeconomic repercussions that arise from funding and from health-related changes in longevity and labor supply? (ii) What constitutes an efficient level of public supply when (a) it determines waiting times and, thus, the effectiveness of medical care (both directly and indirectly through its effect on individual demand) and when (b) timely access to health care impacts health outcomes, in particular, longevity, the benefits of which trade-off against the resource costs of the health care sector, including distortions arising from its funding? (iii) What policy recommendations can be made with respect to the public provision of health services and, more specifically, what rules – targeting fiscal sustainability (i.e. maintaining a constant health expenditure share) as opposed to a maximum waiting time – are commendable, in the face of productivity growth and medical progress as two well-known drivers of the joint expansion of health care expenditure and longevity (Hall and Jones, 2007, Jones, 2016, Kelly, 2017, Böhm et al., 2018, Fonseca et al., 2021, Frankovic et al., 2020a, Frankovic et al., 2020b)?====In order to analyze these issues, we have chosen to employ a representative agent, life-cycle model of health care demand introduced by Dalgaard and Strulik, 2014, Dalgaard and Strulik, 2017. Dalgaard and Strulik, 2014, Dalgaard and Strulik, 2017 advances on the seminal work of Grossman (1972), by incorporating a more realistic and empirically valid biological aging process that motivates health care demand. While this approach potentially misses some important microeconomic aspects of health care that are captured by models of idiosyncratic health shocks,==== we contend that the level of complexity of these models limits the intuition that can be derived from the model itself. Further, given the scope of our study, analysis of the results would rely heavily on averaging across different segments of the economy. Therefore, to address the issue of intra-generational heterogeneity in health status that is important in determining the welfare implications of health care policy, we have adapted Dalgaard and Strulik’s model to include within cohort heterogeneity in health endowments, allowing us to evaluate how policy preferences may differ both within and across generations. We reserve a more detailed discussion of the limitations of our work for the conclusion.====The model economy is composed of a continuum of finitely-lived individuals born into distinct birth cohorts. Individuals, in turn, are assumed to differ in their health status. Each individual derives utility from a non-medical good and leisure time and accumulates health deficits, which can be reduced by the individual consumption of medical services. By slowing down the accumulation of health deficits the individual can expand its longevity in the spirit of Dalgaard and Strulik, 2014, Dalgaard and Strulik, 2017. Here, unhealthy as opposed to healthy individuals are assumed to start with a higher level of deficits. We assume that the government is the sole provider of health care services and supplies a predetermined level of medical capacity which is financed out of an earnings tax. Health care services are provided for free at the point service and are therefore allocated according to a queuing rule, where congestion (i.e. waiting time) becomes both the cost of obtaining medical services and a negative externality that reduces the effectiveness of health care services. Congestion will fall whenever the supply of health care services grows in excess of the aggregate demand for them. The health care system is embedded in an economy that features a private and competitive final goods production sector besides the health care sector. Aside from choosing time-intensive health care, individuals make decisions on consumption and saving as well as on their retirement, the latter then determining the aggregate supply of labor.====We calibrate the model to match data from the UK from 2007–2016. The British health care system is run by the NHS which functions as both the primary financier of health care and the primary provider of all medical services, and therefore fits our theoretical framework.==== We then engage in the analysis of three numerical experiments: (i) a 10% expansion of NHS supply; (ii) a 10% increase in total factor productivity in the production of final goods, which is tantamount to an economic growth impulse; and (iii) a 10% increase in the effectiveness of health care in curbing the accumulation of health care deficits. For the latter two experiments we compare the outcomes under three different policies: (a) a status quo policy, in which the NHS supply is held constant; (b) a policy aimed at maintaining the NHS expenditure share as a fiscal target; and (c) a policy aimed at meeting a waiting list target.====Our key results suggest that for our calibration, a 10% expansion of the NHS improves instantaneous welfare (across all cohorts) even if this comes at the cost of a lower per capita income and consumption. Notably, the expansion of supply leads to a reduction in waiting times despite the increase in demand both at the intensive margin, reflecting an increase in individual demand from each cohort, and at the extensive margin, reflecting the demand from cohorts who now survive to an older age. Thus, the supply expansion has allowed both for greater consumption of health care and, through the reduction in waiting/congestion, has rendered health care more effective. This translates into a sizeable slow down in deficit accumulation and, thus, to an increase in longevity. At the same time, our analysis shows that both the health share and the health care tax increase substantially, with the latter leading to a reduction in labor supply. Overall, the increase in longevity raises instantaneous welfare when measured across all cohorts, but from a cohort perspective the welfare gain is highly skewed towards the elderly. With the tax burden and loss in lifetime consumption predominantly affecting the young. Individuals a little below age 40 tend to suffer from a reduction in their life-cycle utility, implying that the capacity expansion fails to satisfy Pareto optimality. While the welfare impacts are somewhat more pronounced both positive and negative for the unhealthy, there is no qualitative difference by health status.====Unsurprisingly both productivity growth and medical progress contribute to an increase in welfare and in this case both from an instantaneous cross-cohort perspective as well as from a life-cycle utility perspective. However, the overall impact of these shocks depends on the target that governs the policy response. We find that aiming for the fiscal target tends to boost the instantaneous cross-cohort welfare gain in the presence of productivity growth, whereas aiming for a waiting list target tends to boost the instantaneous cross-cohort welfare gain in the presence of medical progress. While individuals from all age groups benefit from productivity growth and medical progress regardless of the accompanying policy response, there is again no unanimous agreement on the preferred response. Both the pursuit of a fiscal target in the presence of a productivity shock and the pursuit of a waiting list target in the presence of medical progress imply the most expansionary policy with respect to NHS capacity and the associated tax. While these policies are maximizing cross-cohort welfare as well as the life-cycle utility of the elderly, again the youngest cohorts would prefer the least expansionary policy, i.e. a constant capacity. Again this holds for healthy and unhealthy individuals alike.====While shadowing a microeconomic literature on waiting times in the public provision of health care (see Siciliani and Iversen, 2012 for a recent survey) the domain of our paper lies more with the macroeconomic modeling of health care. As such it is most closely related to Gaudette (2014) who consider a similar macroeconomic set-up of overlapping generations of individuals who are consuming public health care in order to improve health and survival over their life-cycle while being subject to a waiting time price. While Gaudette (2014) also studies the role of various payment and tax policies aimed at internalizing the waiting time externalities and optimizing welfare, waiting is depicted in a very abstract way as a parameter that raises a patient’s time cost in a way that equalizes the aggregate cost of private health care provision with the public health care budget. Moreover, his model does not feature an explicit production function for either of the two sectors (i.e. final goods and health care). Altogether, this rules out an analysis of how changes to the (physical) supply of public health care interact with the waiting time and, thus, the (physical) demand for health care, which our analysis shows to be crucial for understanding the macroeconomic effects. Furthermore, the lack of an explicit modeling of production and factor markets does not allow for the analysis of general equilibrium repercussions, which again we show to be of relevance.====A second paper that is relatively closely in line with our study, if only as it is also based on a calibration for the English NHS, is Böhm et al. (2018). This study features a general equilibrium model with overlapping generations of individuals who are subject to deficit accumulation and examines how the rationing of public health care bears on welfare if public health care investments induce medical innovations. While our model lacks much of the dynamics present in Böhm et al. (2018) it provides a more thorough modeling of waiting/congestion, which is not an issue in Böhm et al. (2018). Other papers featuring a public health care sector are Kuhn and Prettner (2016) who study the role of publicly provided health care in the presence of R&D-driven economic growth and Grossmann and Strulik (2019) who employ a model of deficit accumulation to study the role of social security reform in Germany.====More distantly, our paper ties in with a large literature centering on the role of health care reform and/or medical progress in calibrated macroeconomic models of the US economy (e.g. Zhao, 2014, Juergen and Tran, 2016, Kelly, 2017, Kelly, 2020, Conesa et al., 2018, Frankovic and Kuhn, 2019, Frankovic et al., 2020b), the big differences being that health care rationing in that context is through price rather than waiting times, implying also that the size of the health care sector is determined by market forces, with only indirect scope for policy making.====From a modeling perspective, our approach is following the literature on health deficit accumulation, a concept pioneered by epidemiologists (e.g Mitnitski et al., 2002) and first adapted to economic analysis by Dalgaard and Strulik (2014). In this approach individuals are assumed to accumulate health deficits (or frailty) over their life-cycle, a process that can be delayed but not reversed through health care investments. This approach can be shown to generate realistic correlations between health care spending and age or health status. Recent work has brought forth the importance of heterogeneity in the processes of frailty accumulation (Hosseini et al., 2022). We take account of this and explore its role in respect to the allocations and policies studied by distinguishing healthy and unhealthy individuals.====The remainder of the paper is laid out as follows. The model and its solution is detailed in Section 2. Section 3 describes the data and calibration procedure. Section 4 contains the numerical analysis covering three sets of numerical experiments. Section 5 concludes.",Congestion in a public health service: A macro approach,https://www.sciencedirect.com/science/article/pii/S0164070422000477,28 July 2022,2022,Research Article,47.0
Sargent Kristina,"Middlebury College, United States of America","Received 1 June 2021, Revised 9 May 2022, Accepted 17 July 2022, Available online 27 July 2022, Version of Record 3 August 2022.",https://doi.org/10.1016/j.jmacro.2022.103454,Cited by (0),This paper investigates the impact of ,"Despite the trope of migration as a negative force for workers within the US and Europe, the model in this paper shows that migration is not necessarily bad for workers in terms of the impact on wages. Using a search theoretic model of the labor market that endogenizes the migration decision for workers, this paper identifies the impact of international migration on wages across groups of workers in the presence of labor market frictions. I modify the standard single-country model from Diamond Mortensen Pissarides (DMP) (Mortensen and Pissarides, 1994, Mortensen, 2011) to include costly international migration for workers. In this two-country model with ex-ante identical workers, the presence of search frictions in the labor market, and costs to workers to migrate, I show that substantial wage dispersion can be generated by incorporating only those factors. An empirical parameterization of the model demonstrates the ability of the model to predict observed migration and wage conditions in the European Union from 1998–2016. These pan-European results are then compared to findings from related work to determine how much of existing wage dispersion can be explained by the frictional labor market and migration channels relative to the imperfect substitutability between skills channel which is more commonly used in the literature to generate wage dispersion in the presence of migration. In summary, the application indicates that incorporating endogenous migration for workers and the costs associated with such a move has dramatic effects on workers’ wages. Ignoring either channel, even within skills, therefore distorts the effect of migration on wages, and distorts any welfare analysis resulting from such a study. Comparisons with cross-skill findings demonstrate the large mitigating impact that the substitution channel has on workers’ wages.====In the model, labor markets are connected through the unemployment pool, and therefore, market tightness. The impact of each market on the other through the overlap of market tightness helps to explain the ambiguous effects documented in the empirical literature. Similar to single-country search models, workers and firms do not take into account their own influence on the domestic labor market when making individual search and job posting decisions, but now there are additional international spillovers from these decisions as well. This imposition of externalities motivates the need for a theoretical model to understand the long-run effects of an individual’s decision to migrate on their own wages, and the wages of workers in sending and receiving countries.====The theoretical search model in this paper shows that incorporating both move and flow costs faced by workers generates equilibrium wage dispersion within skill. These costs to workers lower the overall surplus from a given job-worker match, and workers then receive a lower wage when those costs are high. Not surprisingly, costs faced every period have a larger impact on bargained wages than one-time costs to move. Additionally, differences in labor market characteristics across the two countries frequently have a smaller, but not insignificant, effect on wage dispersion. Even when one country has more favorable productivity and bargaining power conditions for workers, some workers choose to remain in the less “attractive” country. The lack of complete pooling in the more “attractive” country is due to a tradeoff faced by workers between job attractiveness and the probability of matching with such an attractive job. A relatively simple model of job search between two countries is therefore able to generate wage dispersion for ex-ante identical workers in a long-run equilibrium environment. Comparative statics exercises generate realistic outcomes for migration and wages across countries with changes in productivity, unemployment benefits, and other labor market characteristics.====A selection of existing work highlights the importance of migration in European labor markets. Dustmann et al. (2012) shows in a competitive model with no unemployment risk to workers that exogenous changes in migration to the UK can induce wage dispersion and heterogeneous effects across workers with different migration status. This requires skill differentials and imperfect substitution between workers in production in their model. Chassamboulli and Palivos (2014) builds upon the findings in Dustmann et al. (2012), and notes that the addition of search frictions is important for understanding the labor market effects of migration. Their findings also rely on imperfect substitution across skills, exogenous immigration, and differential job search costs for migrants compared to native workers. In an application of the model in Chassamboulli and Palivos (2014), Chassamboulli and Palivos (2013) examines the impact of the inflow of workers to Greece from 2000–2007 and find that skilled workers benefit from migration through skill complementarity while unskilled native workers face ambiguous effects from unskilled migrant workers. While these papers demonstrate the importance of migration in assessing worker’s wages, they rely on skill heterogeneity across workers. In this paper, I instead look within skills to unpack the ambiguous effects demonstrated by these important foundations of migration and frictional labor markets while also adding endogenous migration decisions. In application exercises, I expand the geographic focus to look at the EU-15 and both sending and receiving countries simultaneously.====Search models are useful to examine the existence of wage dispersion in economies, but typically rely on either search costs or worker heterogeneity to generate different wages in equilibrium. For example, Gaumont et al. (2005) are able to generate no more than two wages in equilibrium in their models, while Albrecht and Vroman (2002) generates three equilibrium wages in the pooling equilibrium with heterogeneous workers. Other work is typically limited to generating wages based solely on either worker or firm heterogeneity, or out of equilibrium.====Sato (2004) employs a model of domestic commuting costs to investigate the role of search frictions in generating migration between urban and rural areas within a country. Adding frictions to urban employment results in wage heterogeneity and sub-optimal outcomes for labor in the absence of government intervention. The paper in this study compliments these findings by showing how international migration can alleviate the risk of unemployment, as well as increase the welfare of workers under particular conditions and by exploring the separate impacts of stock and flow costs to migrating workers. Migrating workers in Sato face no cost to migration, nor do they have the option to move back and forth between the urban and rural sectors. This potential for circular migration internationally and endogenously, is a key contribution of the current paper, but is not possible in Sato.====Of particular relevance to the current study, Ortega (2000) outlines a model structure similar to the model in this paper, and provides a strong foundation for understanding the effects of international migration on labor markets, but examines only the effects on native workers in the destination from migration inflows. The main differences in the model in this paper are that in Ortega (2000), migration always makes everyone better off and the equilibrium can be characterized by differences in separation rates alone. This is partially due to the lack of disaggregated move and flow costs to workers which is a philosophically different way to think about the workers’ problem from the approach in this study, but also has modeling implications such as potentially heterogeneous effects on welfare from migration. Also of importance, Ortega generates non-infinite migration by instituting a single cost to search for workers. Ortega (2000) imposes the cost at the time of choosing a search location, regardless of whether the searching worker is matched with an employer abroad. In this paper, workers only incur the costs associated with migrating when actually doing so. In contrast, the current study implements a dual cost to workers to further understand the differential impacts of the timing of costs to workers on migration decisions and labor market outcomes. Search costs, productivity, and posting costs to firms are also assumed to be identical for all workers in Ortega (2000), restrictions which are all relaxed in this paper. The potential for asymmetry allows for examination of heterogeneity of country-level characteristics and applications to a broader range of country pairs for subsequent empirical studies and applications. Finally, Ortega (2000) does not account for unemployment benefits, or any value to being unemployed for the worker; this is a problematic assumption if welfare states and/or quality of life differ between the locations of interest.====Given that existing literature ignores migrants’ experiences and the effects on the country of origin of migrants, the main contribution of this paper is to connect migrant experiences with origin and destination country effects. Consistent with existing work, this paper focuses on one labor market outcome, wages, but examines the effects on both migrants and natives in both sending and receiving countries simultaneously.====Peri (2014) summarizes the empirical literature, concluding that the majority of studies find a near-zero effect on wages for workers as a result of migration, with heterogeneous effects across native and migrant workers. The model with endogenous migration in this paper does not rely on worker skill heterogeneity or substitutability in production, nor does it require firms to discriminate between worker country of origin to generate equilibrium wage dispersion. This paper seeks to understand the role played by the costs workers face when migrating to generate differing wage outcomes when workers are ex-ante homogeneous and firms cannot discriminate in frictional labor markets.====In the next section, I outline the model framework, define the equilibrium, and then perform comparative statics and sensitivity checks in Section 3. In Section 4, I then explore the model fit with data from the European Union with a subset of the current membership to include the first 15 members of the union. Finally, Section 5 concludes.",The wage dispersion effects of international migration in the European Union,https://www.sciencedirect.com/science/article/pii/S0164070422000490,27 July 2022,2022,Research Article,48.0
"Jeong Jaehun,Shim Myungkyu","School of Economics, Yonsei University, 50 Yonsei-ro, Seodaemun-gu, Seoul 03722, South Korea","Received 22 February 2022, Revised 16 June 2022, Accepted 17 July 2022, Available online 25 July 2022, Version of Record 30 July 2022.",https://doi.org/10.1016/j.jmacro.2022.103456,Cited by (0)," level, a finding different from the previous literature that argues that there is a non-monotonic relationship between the two, is shown to be endogenous labor supply. Finally, we analyze the short run consequence of rising income/wealth inequality on welfare cost.","Are business cycles beneficial or harmful to consumers? If so, how much is the cost? After the seminal work by Lucas (1987) that first addresses these questions, literature on the welfare cost of business cycles has flourished and has been developed in many dimensions: See Barlevy (2004), Ramey and Ramey (1995), Otrok (2001) and Storesletten et al. (2001) among many others. This paper aims to contribute to this literature by merging two different strands of the research in this area with a unified framework.====The first strand of the literature that we consider is relatively new; two recent papers, Cho et al. (2015) and Lester et al. (2014), find that business cycles are welfare-improving rather than welfare-detrimental in the class of representative real business cycles (henceforth RBC) model when production factors are endogenously determined.==== This is because agents can take advantages of business cycles in favor for them by varying production factors, and that benefit generally dominates the cost associated with the fluctuations due to risk-aversion. The second strand of the literature is to study heterogeneous aspects of the welfare cost of business cycles across agents (see Krusell et al. (2009) and Mukoyama and Şahi̇n (2006)): While the average welfare cost might not be sizable as shown by Lucas (1987), the cost might be more substantial for agents with low income while be less substantial for agents with high income. For instance, Mukoyama and Şahi̇n (2006) found that the welfare cost is much greater for unskilled workers.====On the one hand, while previous papers with heterogeneous agent model have documented the possible heterogeneity in the welfare cost across agents, to our best knowledge, none of them explicitly consider the role of endogenous labor supply: As is emphasized by Cho et al. (2015) and Lester et al. (2014), however, endogenous labor supply is particularly important in studies of the welfare cost. On the other hand, these two recent papers only consider the aggregate welfare cost, and hence has neglected the role of heterogeneity among workers. We try to synthesize these two perspectives by introducing endogenous labor choice into otherwise standard Aiyagari type model.====Our model framework is a version of Chang and Kim (2007) except that we mainly consider the intensive margin of labor.==== Heterogeneity is introduced in the form of idiosyncratic labor productivity across the workers. We assume that there is an incomplete asset market and hence wealth distribution is non-degenerate. The most important distinction between our model and the model introduced in Krusell et al. (2009) and Mukoyama and Şahi̇n (2006) is that hours worked is endogenously determined by the household. A representative firm exists in the economy following the convention and all markets are perfectly competitive. Importantly, the source of business cycles is the shock to aggregate TFP, which enables our analysis to be comparable with the previous literature.====Main findings can be summarized as follows. First, while there exists welfare gain at the aggregate level even in the economy with heterogeneous agents, the welfare is not evenly distributed across the agents; the welfare gain increases monotonically and convexly with wealth level. In particular, agents with more wealth (the rich) would prefer the economic fluctuations while those with less wealth (the poor) would hate them. This is particularly interesting when compared to the previous findings: With the heterogeneous agent model with exogenous labor supply, Mukoyama and Şahi̇n (2006) and Krusell et al. (2009) found that there exists an inverse-U relationship between wealth level and welfare gains. In their model, welfare gain is relatively high for the agents at the middle of the wealth distribution and is relatively low for the agents at the top and bottom of the wealth distribution. We argue, by decomposing the welfare gain into the mean effect and the fluctuations effect at the individual level, that this discrepancy arises from the fact that our model allows the agents to utilize the business cycles as they want with flexible labor supply: Agents with more assets have higher labor productivity and hence the mean effect, the channel through which each agent can increase consumption level under uncertainty (Cho et al., 2015) and hence enjoys higher utility, from the fluctuations is greater for them. Agents with less assets do not have high labor productivity as well as enough assets to enjoy higher interest rate in such an economy and hence would have lower mean effect. This channel shuts down (or is lower) in the model economy without endogenous labor.====We then show that the dimension of inequality matters when analyzing the extent to which rising inequality affects the distribution of the welfare cost. In particular, we first consider rising ==== inequality by introducing mean-preserving spread into idiosyncratic labor productivity and then introduce rising ==== inequality by allowing households to have different discount factors, following Krusell et al. (2009). While the main finding that there is a monotonic relationship between wealth level and the welfare cost are preserved in any cases, the way considering rising inequality has strikingly different welfare consequences: Greater income inequality dampens the degree of convexity while greater wealth inequality tends to enhance it. We argue that such a difference results from the fact that mean effect is reinforced in the presence of rising wealth inequality while becomes lower in the presence of rising income inequality.====Lastly, we further show that our main findings are preserved under various circumstances: (1) Introducing labor indivisibility, (2) alternative value for borrowing limit, (3) alternative Frisch labor supply elasticity, (4) different magnitude of TFP shocks, and (5) persistence of idiosyncratic labor productivity.====There are two main contributions of this paper to the literature. First, our work is the first paper to analyze the welfare consequences of the business cycles with (1) endogenous labor supply and (2) worker heterogeneity in a unified framework. Second, we unveil the extent to which rising inequality, the long-run phenomenon, affect the welfare cost of business cycles, the short-run welfare consequences, which has not been investigated before.====Remaining sections are organized as follows. Section 2 introduces the model and then Section 3 studies the role of labor market heterogeneity in determining the welfare cost of business cycles. Section 4 concludes.",On the welfare cost of business cycles: The role of labor-market heterogeneity,https://www.sciencedirect.com/science/article/pii/S0164070422000519,25 July 2022,2022,Research Article,49.0
"Kohlbrecher Britta,Merkl Christian","Friedrich-Alexander-Universität (FAU) Erlangen-Nürnberg, Germany","Received 1 March 2022, Revised 19 April 2022, Accepted 17 July 2022, Available online 22 July 2022, Version of Record 28 July 2022.",https://doi.org/10.1016/j.jmacro.2022.103458,Cited by (1),This paper shows that a search and matching model with idiosyncratic training cost shocks can explain the asymmetric movement of the job-finding rate over the business cycle and the decline of matching efficiency in recessions. Large negative aggregate shocks move the hiring cutoff into a part of the training cost distribution with higher density. The position of the hiring cutoff in the distribution is disciplined by the empirical elasticity of the job-finding rate with respect to market tightness. Our model explains a large fraction of the matching efficiency decline during the ==== and generates state-dependent effects of policy interventions.,"There is a long-established literature that documents that employment and unemployment in the United States behave asymmetrically over the business cycle, i.e. the downward movement of employment in recessions is much stronger than the upward movement in booms (e.g. Neftçi, 1984, McKay and Reis, 2008, Abbritti and Fahr, 2013). The experience of the Great Recession gave rise to a separate literature documenting a decline of matching efficiency (e.g. Barnichon and Figura, 2015) and an outward shift of the Beveridge curve in recessions (Diamond and Şahin, 2015).====While both facts – unemployment asymmetry as well as shifts of the Beveridge Curve and matching efficiency – are well known to the literature, we are the first to show that these are two sides of the same coin. We document that an asymmetric job-finding rate is an important driver of unemployment asymmetry. The time-series distribution of the job-finding rate is skewed to the left, which means that in severe recessions, the downward adjustment of the job-finding rate is particularly strong. By contrast, market tightness moves more symmetrically over the business cycle. In severe recessions, the decline of the job-finding rate is typically stronger than predicted by a standard Cobb–Douglas constant returns matching function that features a constant elasticity between the job-finding rate and market tightness (vacancies divided by unemployment). Through the lens of a standard matching function, this pattern of the data is interpreted as a decline of the matching efficiency.====We provide an economic mechanism explaining all these facts by enhancing a standard search and matching model with a selection stage (Chugh and Merkl, 2016, Sedlac̆ek, 2014). In our model, matching is a two-stage process. First, workers and firms have to meet. We call this the contact stage, during which contacts form according to a standard Cobb–Douglas contact function. At a second stage, firm and worker assess whether they are a good fit and the firm decides whether to hire the worker. We call this the selection stage. For ease of disposition, we model the selection stage in our baseline model as a draw from a match-specific training cost distribution. However, the mechanism easily extends to more general forms of idiosyncratic productivity shocks as we show in the Appendix. Idiosyncratic training costs (or idiosyncratic productivity shocks) imply that firms will only select a fraction of contacts for hiring. Over the cycle, firms endogenously adjust their hiring standards. In a recession, when profit outlooks are bleak and market tightness is slack, firms will increase their hiring standards and only select workers with relatively low training costs. The hiring threshold for training costs and hence the selection rate decrease. In a boom, when profit outlooks are favorable and the market is tight, firms will increase their hiring threshold and accept workers with higher match-specific training costs. Thus, the selection rate increases. We show that the response of the selection rate and hence the job-finding rate is stronger in recessions than in booms whenever the hiring cutoff is at a downward-sloping part of the training cost distribution. We prove that this condition is satisfied for empirically plausible values of the elasticity of the job-finding rate with respect to market tightness: in our model, this moment is closely tied to the curvature of the idiosyncratic training cost distribution at the hiring cutoff.==== Furthermore, the nonlinearity inherent in the selection mechanism also implies that the elasticity of the job-finding rate with respect to market tightness is no longer constant in our model – as assumed with a standard matching function – but changes when the hiring cutoff moves over the cycle. Looking at the data through the lens of a constant elasticity matching function, this appears as a decline of matching efficiency in recessions. Thereby, our model can explain both phenomena from the data: (i) asymmetric reactions of the job-finding rate, employment and unemployment over the business cycle, (ii) the decline of the estimated matching efficiency and the outward shifts of the Beveridge curve in recessions. Quantitatively, the reaction of the job-finding rate in our model is substantially larger in response to a negative shock compared to a positive shock. For a realistic slump of the labor market, our model endogenously accounts for the majority of the decline of matching efficiency during the Great Recession. In addition, we show that the proposed mechanism is of major importance for understanding the quantitative effects of policy interventions in booms and in recessions. We compare the labor market effects of an ex-ante vacancy posting subsidy and an ex-post hiring subsidy in booms and recessions. These two subsidies are equivalent in a standard search and matching model. However, once the selection margin is present, substantial differences emerge. As the ex-post hiring subsidy affects the value of a match and thus the selection margin, the subsidy is very effective in recessions but not so in booms. By contrast, an ex-ante vacancy subsidy is similarly effective during booms and recessions.====We believe that labor selection (i.e. endogenous hiring standards) is an economic mechanism that is of great relevance. Based on the cross-sectional Employment Opportunity Pilot Project (EOPP), Barron et al. (1985, p. 50) document for the United States that “(...) most employment is the outcome of an employer selecting from a pool of job applicants (...)”. More recently, Faberman et al. (2022) show based on a supplement to the Survey of Consumer Expectations that only a fraction of worker–firm contacts translate to job offers. While to our knowledge there is no time-series evidence for the United States yet, Hochmuth et al. (2021) use the German IAB Job Vacancy Survey to construct aggregate time series (national, sector, state level) for the share of applicants that is selected over time (selection rate). They document that the selection rate moves procyclically over the business cycle and shows much stronger fluctuations than real GDP. Importantly, the selection rate accounts for about half of the movement of the job-finding rate over the business cycle. Kohlbrecher et al. (2016) fit an idiosyncratic productivity distribution to match the distribution of residual entry wages in Germany and find that selection drives a major part of the movement of the job-finding rate. Thus, Kohlbrecher et al. (2016) and Hochmuth et al. (2021) provide evidence for Germany that (in addition to the well-established contact channel) labor selection is an important margin for hiring over the business cycle.====Furthermore, the business cycle behavior of labor selection in our model is well in line with the empirical evidence on recruiting intensity. According to Davis et al. (2013) and Gavazza et al. (2018), a collapse of recruiting intensity can explain the decline of matching efficiency in the Great Recession. Increased hiring standards could be one important dimension of recruiting intensity, although there may be many others. While our paper focuses on the time series dimension of the data, Baydur (2017) shows that a selection model can also replicate important cross-sectional dimensions of the data (e.g. the cross-sectional behavior of vacancy yields, as outlined by Davis et al., 2013). Lochner et al. (2021) provide direct empirical evidence for the cross-sectional behavior of the selection rate in Germany, which is in line with their theoretical framework.====How do our results compare to the emerging literature on the ability of search and matching models to generate labor market asymmetries? Several recent papers show that the standard search and matching model can generate asymmetric responses of the job-finding rate and unemployment. Abbritti and Fahr (2013) propose a model with downward real wage rigidity, while Petrosky-Nadeau and Zhang (2017), Petrosky-Nadeau et al. (2018), and Ferraro and Fiori (2018) use calibrations in the spirit of Hagedorn and Manovskii (2008) to generate asymmetric effects.==== We differ from these papers in several dimensions. First, none of these papers has a second stage of hiring. Thus, by definition the job-finding rate is a stable function of market tightness. By contrast, our approach not only explains the asymmetry of unemployment and the job-finding rate but also the observed decline of the matching efficiency in recessions. We are therefore the first to explain and relate both empirical phenomena. Second, the ability of a model to generate quantitatively meaningful asymmetric labor market responses is tightly connected to amplification. Given the well-known lack of amplification in the standard search and matching model (Shimer, 2005), the existing literature either relies on some form of wage rigidity (as in Hall, 2005) and/or a small surplus calibration (as in Hagedorn and Manovskii, 2008). However, the cyclicality of real wages over the business cycle is a highly debated issue (e.g. Haefke et al., 2013, Gertler et al., 2020). Finally, we show that similar to the search and matching model without labor selection (see e.g. Petrosky-Nadeau and Zhang (2021)), the asymmetry of the job-finding rate generated by the selection mechanism is tightly connected to the elasticity of the job-finding rate with respect to market tightness although the mechanism is very different. We show that for a given elasticity and a given amplification of the job-finding rate, the asymmetry generated by the model with labor selection is substantially larger (see online Appendix====). Kohlbrecher et al. (2016) show in a simple numerical illustration that the job-finding rate can be affected asymmetrically by aggregate shocks in a model with labor selection calibrated to German data. However, they do not link this to shifts of the Beveridge curve and matching efficiency shifts in recessions nor to state-dependent policies.====The focus of our paper is to explain the asymmetric nature of job creation and its connection to the decline of matching efficiency in recessions. It is well known that the separation rate into unemployment (although not necessarily the total separation rate) is highly skewed over the business cycle (see e.g. Ferraro, 2018). Pizzinelli et al. (2020) show in a TVAR that both the job-finding rate and the separation react stronger in response to technology shocks when aggregate productivity is low. We show in the Appendix an extension of our model that includes endogenous separations and persistent idiosyncratic shocks. As expected, the separation margin further adds to the skewness of the unemployment rate. Importantly, our results regarding the job-creation margin and the matching efficiency are unaffected by this extension. The qualitative and even quantitative responses of the selection and job-finding rate are very similar and matching efficiency drops in recessions. A well known caveat of the endogenous separation model is the collapse of the Beveridge curve. Of course, this might be addressed with a more complex model. As this is not the focus of our paper, we prefer to keep our model simple and tractable.====One of the key insights of our paper is that labor selection can generate asymmetric responses of the job-finding rate and shifts of matching efficiency at the same time. Would other economic mechanisms generate similar results? One potential alternative explanation is endogenous search effort. If workers search less in recessions or the composition of the unemployment pool changes towards long-term unemployed with lower search effort (see Hall and Schulhofer-Wohl, 2018), this could lead to a decline of estimated matching efficiency and asymmetries of the job-finding rate. However, Mukoyama et al. (2018) show that search effort is actually countercyclical in the US and Hornstein and Kudlyak (2016) show that this countercyclicality and compositional shifts of the unemployment pool cancel out during the Great Recession.====The rest of the paper is structured as follows. Section 2 presents stylized facts on business cycle asymmetries and the cyclicality of matching efficiency. Section 3 presents a search and matching model with labor selection. Section 4 provides analytical results. Section 5 outlays our calibration strategy and Section 6 presents numerical results in the fully nonlinear setting. Section 7 puts our results in perspective to the existing literature.",Business cycle asymmetries and the labor market,https://www.sciencedirect.com/science/article/pii/S0164070422000520,22 July 2022,2022,Research Article,50.0
"Peng Yulei,Zervou Anastasia","School of International Trade and Economics, University of International Business and Economics, 10 Huixin East Road, Beijing 100029, China,Department of Economics, University of Texas at Austin, 2225 Speedway, Austin, TX 78712, United States of America","Received 22 July 2021, Revised 18 June 2022, Accepted 21 June 2022, Available online 27 June 2022, Version of Record 11 July 2022.",https://doi.org/10.1016/j.jmacro.2022.103448,Cited by (1)," is high. At the same time, inflation targeting insures against inflation, resulting in nominal bonds becoming attractive assets. Our model suggests that monetary policy objectives play a key role in affecting risk sharing, asset returns, and the equity premium.","The impact of monetary policy on financial markets is profound, for it is transmitted in the economy via the financial system. We present a model that studies how monetary policy objectives influence equity and bond risk, and returns. In particular, we use a segmented financial markets model to study the asset pricing implications of the optimal monetary policy in an economy that is hit by financial shocks. We compare those implications to those resulting from employing other, commonly used monetary policy rules, like inflation targeting. We find that asset prices depend crucially on the monetary policy objective, and different monetary policy rules are associated with different asset risk. We present analytical results and perform a quantitative exercise to support our argument.====In the segmented financial market model with dividend income shocks, optimal monetary policy has risk-sharing considerations, redistributing the stock market risk that financial market participants face to all agents in the economy (see Zervou, 2013). In this paper, we show that through its distributional consideration, optimal monetary policy implies low risk and thus low return on the risky asset compared to other policies, including inflation targeting. We also derive that the optimal policy implies a countercyclical inflation response, and thus leads to a higher real return for the short-term nominal bond compared to inflation targeting. As a result, the equity premium produced under the optimal monetary policy rule resembles that produced by the representative agent model, which the previous literature has found to be small.==== This is in contrast to the inflation targeting policy which is not concerned with stock market risk and implies high equity return. However, inflation targeting insures agents against inflation risk, making the bond a safe asset. The result is a high equity premium under the inflation targeting policy and a low equity premium under the optimal policy. In our quantitative exercise, we find a 1.56% equity premium under the optimal policy, and a 6.96% premium under the inflation targeting policy, i.e., almost four and a half times higher. We also find that the premium is seven times more volatile under the inflation targeting policy compared to the optimal one.====We compare our model predictions with the data. In our quantitative exercise, we find that the equity premium produced under the inflation targeting policy through our model is similar to the reported equity premium in the developed world. Also, in our model, under inflation targeting, the equity premium is high mostly because of the low return of short-term bonds, as also found in the recent data (e.g., Mehra and Prescott, 2008). Moreover, the Sharpe ratio calculated through our model under inflation targeting is similar to that estimated with US data (e.g., Lettau and Ludvigson, 2010). We take the model predictions to the data and show that the inflation targeting considerations had been dominant in determining the path of money growth relative to the optimal policy considerations in recent years. Finally, through data comparisons, we find that the bond returns of countries and periods when inflation targeting was followed are lower than those of countries and periods when other policies were followed. Through those data comparisons, we argue that the developed world in the recent years resembles closer the inflation targeting policy than the optimal monetary policy in our model, and thus inflation targeting, through its low bond and high equity returns, might contribute to a high equity premium.====We do various robustness tests. Choosing different inflation targets does not affect our results, revealing that it is the policy consideration that affects the premium, and not the specific choice of target (i.e., ====, ====, or 10%). Having a different degree of financial market segmentation does not affect our results either. This is because, as soon as there is some degree of segmentation, the optimal policy implies risk-sharing and this is not affected by the extent of segmentation. Similarly, the inflation targeting policy keeps inflation on its target, and again, this is not affected by the extent of segmentation. Lastly, imposing a dividend shock with lower volatility than our benchmark one produces a lower premium for both policies, yet, the premium under the inflation targeting policy is four and a half times higher than that under the optimal policy. This result emphasizes the role of policy in affecting the correlation of financial participants’ consumption with the dividend return, even for relatively low dividend shocks. We find similar results when we add a labor income shock to the model; the premium is lower than the benchmark, but the policy consideration still matters, so inflation targeting produces almost three times higher equity premium than what the optimal policy does.====We use a model with financial market segmentation because this is a market characteristic that has been well documented (Mankiw and Zeldes, 1991; Guiso et al., 2002; Vissing-Jørgensen, 2002) and used before for the study of asset pricing. Specifically, it is often used for differentiating preference parameters, i.e., risk aversion and elasticity of intertemporal substitution, between the financial market participants and non-participants, or emphasizing their differences in wealth holdings (e.g., Vissing-Jørgensen, 2002; Brav et al., 2002; Guvenen, 2009; Dong, 2012). In addition, financial market segmentation has been used in a parallel literature of monetary theory in order to differentiate agents regarding their connectivity to monetary policy, and emphasize that monetary policy has real effects through distributional considerations (e.g., Grossman and Weiss, 1983; Rotemberg, 1984; Lucas, 1990; Fuerst, 1992; Alvarez et al., 2001; Occhino, 2004; Williamson, 2005, Williamson, 2006, Zervou, 2013; Alvarez and Lippi, 2014; Azariadis et al., 2019; Enders, 2020).==== We connect those two strands of literature by emphasizing the asset risk implications of monetary policy rules, within the segmented markets monetary model.====The monetary segmented markets models mentioned above aim to capture that monetary policy directly affects the financial market participants, who are at the receiving end of monetary policy actions, although the non-participants are affected only indirectly, through inflation. As a result, those models suggest that a monetary policy expansion increases the consumption of financial market participants but increases inflation and hurts non-participants. Given how Open Market Operations (OMO) function, through large financial institutions, the assumption of differential agent’s connectivity that the segmented markets monetary model adopts is intuitive; however, there is no strong empirical evidence of the model’s implication in terms of consumption effects. In particular, Coibion et al. (2017) find the opposite evidence; yet, the data do not include the top 1% of the income distribution, that is expected to be the active traders in the financial markets and thus could be more affected by monetary policy actions. Ait-Sahalia et al. (2004) for example, find that consumption of luxury goods responds more to stock prices than aggregate consumption. If monetary policy affects asset prices, as it has been found to do (e.g., Thorbecke, 1997; Bernanke and Kuttner, 2005; Basistha and Kurov, 2008; Jansen and Tsai, 2010; Demiralp and Yılmaz, 2012; Jansen and Zervou, 2017), then the monetary policy objective as well, could affect asset prices. This paper studies, within the setting of a theoretical model and quantitative exercise, how monetary policy objectives influence asset risk and returns.====We are not the first to use the segmented markets monetary model to emphasize connections between monetary policy and asset prices. Lahiri et al. (2007), Mizrach and Occhino (2008) and Kim and Moon (2009) use such a model to successfully match many asset pricing facts. Moreover, Airaudo (2013) and Carlstrom et al. (2017) use those models to suggest that monetary policy should react to asset prices. However, none of those papers explore the effect of various monetary policy objectives and how those might influence asset risk, as we do.====Previous literature (Gust and Lopez-Salido, 2014; Drechsler et al., 2018) has studied the effect of monetary policy actions, i.e., expansion or tightening, in affecting the equity premium. Our work differs from this literature because it emphasizes the importance of monetary policy’s objective, i.e., the policy rule followed, in affecting asset returns. In that aspect, our work relates to the work of Benigno and Paciello (2014) who find that the inflation targeting policy is less cyclical than the optimal one in their New Keynesian sticky-price model with private sector doubts regarding the state probability distribution, and ambiguity aversion; as a result, in their model, the inflation targeting policy produces a lower premium than the optimal one. We find that the inflation targeting policy is more cyclical than the optimal policy implied by our flexible price model; as a result, in our model, the inflation targeting policy produces a higher premium than the optimal one. Among other modeling differences that drive the deviations in the optimal policy results, the two models also differ in their assumption of price stickiness, or not. Wei (2009) shows that the assumption of flexible or sticky prices is important for understanding how the stochastic discount factor changes; we show that it also affects the cyclicality or not of the inflation targeting policy, which further affects bond returns and the equity premium.====The rest of the paper is organized as follows. Section 2 describes the model and derives the equity premium as a function of monetary authority’s considerations. Section 3 introduces a quantitative exercise in order to quantitatively study asset pricing through the optimal and the 2% inflation targeting monetary policy rules. It also includes robustness tests and data comparisons. Section 4 concludes.",Monetary policy rules and the equity premium in a segmented markets model,https://www.sciencedirect.com/science/article/pii/S0164070422000453,27 June 2022,2022,Research Article,51.0
"Fratianni Michele,Gallegati Marco,Giri Federico","Kelley School of Business, Bloomington, IN, USA,Università Politecnica delle Marche, Piazzale Martelli n 8 Ancona, Italy","Received 5 August 2021, Revised 17 June 2022, Accepted 21 June 2022, Available online 26 June 2022, Version of Record 2 July 2022.",https://doi.org/10.1016/j.jmacro.2022.103450,Cited by (1),"The debate on the inflation–unemployment relationship has focused almost exclusively on the distinction between the “short-run” and “long-run” Phillips curves, while virtually ignoring the “middle” horizons. Using a historical perspective, we show that the UK wage Phillips curve is essentially a medium-run phenomenon. At the frequency range beyond business cycle frequencies, that is 8 to 16 years, there is significant evidence of a negative and stable relationship between money wage ==== and unemployment.","Sixty-one years after A.W. Phillips (1958) published the “The relation between unemployment and the rate of change money wage rates in the United Kingdom, 1861–1957”, the empirical validity of its Curve (PC for short) remains a hotly debated issue both in the world of academic research and policy applications. The very different opinions, “dead” or “alive”, “flatter” or “steeper”, about the status of the PC suggest that there is still much to be done despite the vastness of the contributions in the field; a sample of significant papers includes, among others, Samuelson and Solow (1960), Phelps (1967), Friedman (1968), Lucas, 1972, Lucas, 1973, Sargent and Wallace (1975), Gordon, 1982, Gordon, 2011, King and Watson (1994), Gali and Gertler (1999), Staiger et al., 1997b, Staiger et al., 1997a, Haldane and Quah (1999), Mankiw (2001), Mavroeidis et al. (2014), Galí (2011) and Hall and Sargent (2018).====In the last decade, the pattern of unemployment and inflation has raised doubts on the quality of the PC as a framework capable to explain satisfactorily how the disequilibrium in the goods or labor market impacts the inflation rate, leading to conclusions that the PC has been flattening and inward shifting (Cunliffe, 2017). Del Negro et al. (2020) indicate several explanations for this transformation of the PC. The first is related to the possible measurement error in both inflation and economic slackness. The second is how to deal with structural changes in the labor market that might have weakened the link between (wage) inflation and real economic activity; The third is the underlying monetary policy paradigm that emerged in the early 80 s, in response to strong inflationary pressure, and flattened the aggregate demand curve and hence weakened the link between inflation and output. A crucial point emerging from many of these contributions is the focus on business cycle frequency dynamics, that is the time span generally defined as the interval between 2 and 8 years. Exploring what happens to the slope of the PC when we include lower frequencies, the so-called medium run (Borio, 2014), is the key point of the present paper.====The usefulness of frequency analysis applied to the PC was first raised by Desai (1975) who pointed out that Phillips’ original averaging procedure, was not only crucial to his results, but also of general statistical and economic interest. The short-run trade-off between inflation and unemployment was not Phillips’ original motivation. In fact, he dismissed price inflation as a contributing factor to his empirical model and argued that an excess demand for labor would have induced firms to bid up money wages independently of inflation. This assumption and the restriction that changes in unemployment were essentially set to zero, by averaging observations over the selected time intervals, suggest that Phillips was interested in a longer-run relationship.==== Desai (1975) and, recently, Gallegati et al. (2021) show that Phillips’ unorthodox data transformation is crucial for assessing the frequency resolution of Phillips’ findings, meaning that his averaging procedure identifies a lower frequency relationship between wage inflation and unemployment.====Another novelty of our contribution is that it focuses on the UK using a sample that covers a long time span from 1857 to 2016. Much of the debate is centered on the US, evidence is more scarce for other countries. In this regard, Haldane and Quah (1999, Figure 3) produce a conventionally sloped price inflation–unemployment scatter plot from 50 years of post-WWII monthly data. The authors find that the UK PC is virtually vertical from 1948 to 1980 and then progressively flattens. Cunliffe (2017, Chart 6) fits trend lines through scatter plots of the UK money wage inflation–unemployment rate for the periods 1971–1997, 1998–2012, and 2013–2017 and concludes that the PC relationship has continuously shifted inward as well as flattened. Empirical evidence suggests that the flattening of the PC in the UK follows the same pattern found elsewhere.====It is a central result of our paper that lower-frequency trends, defined as the cyclical components in the 8-to-16 years’ frequency band, are statistically significant and economically important in the wage PC. In our investigation, we will revisit the UK PC using both static and forward-looking specifications with both Phillips’ original data and Bank of England historical data covering the 1861–2016 period (Ryland et al., 2010). To do that, we employ wavelet methods (Crowley, 2007; Aguiar-Conraria and Soares, 2014) to analyze data both in the time and frequency domain. Our results suggest that the UK wage PC is stable at the medium-run frequencies even when it disappears at the business cycle frequencies.====Relatively few contributions have estimated the PC in the frequency domain using spectral analysis (King and Watson, 1994; Iacobucci, 2005; and Reinbold and Wen, 2020). Their conclusion is that the PC has a negative slope at the business cycle frequencies, but not at the higher frequencies (less than 18 months). Reinbold and Wen (2020), instead, find evidence that the relationship between unemployment and inflation is still negative and statistically significant at the medium run frequencies (between 6 to 50 quarters).====The wavelet methodology offers several appealing features: they can deal with stationary and non-stationary data, and can perform multi-resolution decompositions. Gallegati et al. (2011) were the first to estimate the wage PC for the US using wavelet tools, and found strong evidence of a negative relationship especially at the “major” NBER-dated business cycle frequency,16 to 32 quarters.==== Other recent contributions have relied on continuous wavelet tools to investigate the shape of the PC. Mutascu (2019) supports the idea that the US PC is unstable over time with episodes of strong negative correlation both in the short and medium term. Aguiar-Conraria et al. (2019) apply the continuous wavelet transform to investigate the unemployment and price relationship in the context of the NKPC for the US and find evidence of a short-term correlation but no support for the medium-run hypothesis.====Our paper delivers four key results. First, over the period of direct interest to Phillips, 1861–1913, the negative relationship between wage inflation and unemployment is mainly present beyond the typical business-cycle frequency range. Second, this medium-run relationship survives until present days using a long stretch of historical annual data. Third, our exercise suggests that, while at business-cycle frequencies there is a steady decline of the PC slope, at the medium-run frequencies the slope is still negative and statistically significant.====The structure of the paper is as follows. Section 2 presents the mathematical details of the wavelet methodology. Section 3 applies the wavelet techniques to the original Phillips’data. Section 4 discusses the New Keynesian wage PC (NKWPC), our measure of the unemployment gap, empirical findings of annual data estimation over the longest available time, and a robustness exercise with post-WWII quarterly data. Section 5 offers concluding comments.",The medium-run Phillips curve: A time–frequency investigation for the UK,https://www.sciencedirect.com/science/article/pii/S0164070422000465,26 June 2022,2022,Research Article,52.0
"Kang Jihye,Kim Soyoung","Seoul National University, San 56-1, Sillim-Dong, Gwanak-Gu, Seoul 151-746, South Korea,Department of Economics, Seoul National University, San 56-1, Sillim-Dong, Gwanak-Gu, Seoul 151-746, South Korea","Received 9 July 2021, Revised 21 May 2022, Accepted 30 May 2022, Available online 8 June 2022, Version of Record 28 June 2022.",https://doi.org/10.1016/j.jmacro.2022.103446,Cited by (1),We identify government spending news and surprise shocks based on the Survey of Professional Forecasters (SPF) data. After clarifying the nature of the two news measures used in ====) and inconsistent with the neoclassical view emphasizing the negative wealth effect.,"After the 2008 global financial crisis, the sluggish economic recovery and the limitation of the monetary policy under low interest rates have led to a renewed research interest in the conduct and consequences of fiscal policy.==== However, empirical studies have reached divergent conclusions. The estimated government spending multiplier falls in a wide range, mainly from 0.5 to 2, and is at times even negative.==== Moreover, the estimated responses of key macro variables, such as consumption and real wage, considerably differ. They tend to increase in studies using standard vector auto-regression (VAR) models (e.g., Fatás et al., 2001, Blanchard and Perotti, 2002, Galí et al., 2007, Perotti, 2007, Pappa, 2009), but decrease in those using a narrative approach (e.g., Ramey and Shapiro, 1998, Ramey, 2011b).====The fiscal foresight issue has been considered as one of the culprits of this lack of empirical consensus on government spending using VAR models. Ramey (2011b) asserts that private agents can anticipate government spending shocks identified by the popular VAR framework, such as that of Blanchard and Perotti (2002). Without consideration of this anticipation effect, the identified shocks in the VAR model may not be exogenous and lead to spurious results. To address this issue, Ramey (2011b) constructs and incorporates news or expectation measures on government spending, and identifies government spending shocks as the innovations in the measures. The resulting effects (Ramey, 2011b) substantially differ from the results of the standard VAR model without such measures (e.g., Blanchard and Perotti, 2002, Galí et al., 2007). In particular, consumption and real wage decline under positive government spending shocks, and the government spending multiplier is less than one or even negative. Ramey (2011b) suggests that such results support the neoclassical view and that the timing issue is crucial in explaining why the narrative approach, such as that of Ramey and Shapiro (1998), often finds adverse effects on consumption and real wage.====However, confusion has arisen regarding the nature of the measures and identified shocks in Ramey (2011b). Ramey (2011b) suggests two types of measures: one based on defense news (hereafter DN PDV) from newspapers, and the other on the Survey of Professional Forecasters (hereafter SPF FE) data. By employing (one of) the measures of Ramey (2011b), many subsequent studies have considered explicitly and/or controlled the anticipation effect of government spending shocks to explore various issues on government spending.==== However, their interpretations of the measures and shocks are not consistent in terms of news-surprise representation.==== The shock to the DN PDV variable is interpreted as a surprise (or unanticipated) shock in some studies (e.g., Born et al., 2013, Forni and Gambetti, 2016, Rodrigues-Lopez and Solis-Garcia, 2018) but as a news (or anticipated) shock in others (e.g., Ben Zeev and Pappa, 2017, Caldara and Kamps, 2017). Meanwhile, several studies, such as Auerbach and Gorodnichenko (2012) and Bachmann and Sims (2012) use DN PDV as a control variable to identify surprise shock. In addition, although the SPF FE shock is mainly considered as a surprise shock, a few studies, such as Caldara and Kamps (2017), interpret it as a news shock.====Motivated by this lack of consensus, the present study extends Ramey’s (2011b) work across two dimensions. First, we analytically reexamine the two news measures of Ramey (2011b) and suggest that the two measures (and shocks to the two measures) differ in nature. Aside from their data sources, Ramey (2011b)’s two measures are constructed in different ways. DN PDV is built based on changes in the expected present discounted value (PDV) of defense spending, and SPF FE on the difference between actual and expected government spending. In a simple analytical example, we verify the ability of the two measures to capture two distinct government spending shocks: news and surprise shocks.====Second, we explore the differences between surprise and news shocks of government spending in terms of one additional aspect: ====. Studies that compare these shocks focus mainly on the timing aspect, despite the difference in the persistence shown in the literature, especially in Ramey (2011b). In response to the DN PDV shock, government spending persistently increases and shows a delayed and hump-shaped response. However, in response to the SPF FE shock, government spending spikes on impact, drops rapidly after that, and later even becomes negative. However, in most cases, including Ramey (2011b), the difference in the level of persistence for government spending news and surprise shocks has not been much emphasized. To the best of our knowledge, this study is the first to relate the persistence of government spending responses to the nature of news shocks.====Given the weak explanatory power of the DN PDV for the recent period – period after the Korean War when the DN PDV of Ramey (2011b) loses its explanatory power – and the SPF FE shocks for capturing surprise shocks rather than news shocks, a measure for government spending news shock is needed to compare news and surprise shocks for the recent periods. We build an alternative measure to DN PDV (hereafter SPF PDV) to identify government spending news shock for the recent periods (1981:3–2008:4/2016:4). Using the SPF data, we construct the measure as changes in PDV of federal spending. Ramey (2011b) emphasizes that for the test of Neoclassical theory (the wealth effect of government spending shocks), the timing of the news on a major change in the present discounted value of government spending matters, rather than that of the actual changes in government spending. In this regard, the measure of innovations in expected PDV can be a suitable news measure. In addition, a PDV measure is useful because we try to identify persistent changes in expectations, which will be implemented into government spending. While most of the literature has employed SPF data as growth rate because of the base-year changes of SPF, we rebase the raw SPF data as a level to build a PDV series, following Leeper et al. (2012).====Third, we identify news and surprise shocks in the VAR model with the SPF PDV and the SPF FE. The identified news and surprise shocks show strikingly different effects on the economy. Government spending persistently increases after a short delay in response to news shock, but decreases shortly after an immediate rise in response to surprise shock. Moreover, the surprise shock reduces GDP, consumption, and investment and affects real wage insignificantly. However, the news shock has significant positive effects on almost all variables, which is inconsistent with Ramey’s (2011b) finding. The present results, particularly the increases in consumption and real wage, are also inconsistent with the neoclassical view, which emphasizes the negative wealth effect.====Next, focusing on the persistence feature, we suggest explanations for why news shocks have stimulus effects on the economy, but surprise shocks do not. One is based on the capital accumulation channel combined with non-savers (or rule-of-thumb consumers). Dupaigne and Fève (2016) suggest that the capital accumulation channel with non-savers can explain the positive responses of consumption and investment to a persistent government spending shock. If government spending is sufficiently persistent, firms will spend further investments to cope with future expected increases in employment due to the negative wealth effect. Thus, persistent government spending can raise investments and magnify the multiplier.====Another possibility is “productive” government spending, that is, public investment, which is likely to have a more prolonged implementation period than government consumption. Moreover, government investment can stimulate the economy (e.g., Baxter and King, 1993, Leeper et al., 2010, Bachmann and Sims, 2012) and tends to have a larger multiplier than other fiscal policy tools (Gechert and Rannenberg, 2018). Consistently, we find that news shock leads to an increase, whereas surprise shock leads to a decrease in the ratio of government investment to government consumption.====Finally, we briefly discuss why SPF PDV shock is more expansionary than DN PDV shock. DN PDV shock captures changes in defense spending, while SPF PDV changes in general spending. Compared with general government spending, military spending is likely to be less persistent and less delayed with weaker anticipation effects because it is urgent spending, especially during war. In addition, military spending likely involves a small component of government investment than general government spending.====Other related studies use different identification strategies and econometric methodologies. Mountford and Uhlig (2009), Mertens and Ravn (2010) and Ben Zeev and Pappa(2015, 2017) investigate the effects of government spending news and surprise shocks. Using a method similar to ours, Forni and Gambetti (2016) investigate the open-economy effects of news and surprise shocks, focusing on the real exchange rate depreciation puzzle documented by Kim and Roubini (2008). In terms of timing and persistence, our results on overall patterns of government spending impulse responses to the shocks are not inconsistent with those in the literature.====The rest of the paper is organized as follows. In Section 2, we reexamine the two measures of Ramey (2011b) in terms of timing and persistence to show why an alternative measure for news shock is needed. We propose a new measure considering both features. Using our new measure, we provide in Section 3 our results on the effects of government spending news and surprise shocks in recent periods. Section 4 presents an extended discussion, and Section 5 concludes with a summary of the findings.",Government spending news and surprise shocks: It’s the timing and persistence,https://www.sciencedirect.com/science/article/pii/S016407042200043X,8 June 2022,2022,Research Article,53.0
"Di Bartolomeo Giovanni,D'Imperio Paolo,Felici Francesco","Department of Economics and Law, Sapienza University of Rome and Department of Economics, University of Antwerp Italy,Treasury Department, Ministry of Economy and Finance, Italy,Ministry of Tourism, Italy","Received 24 October 2021, Revised 17 April 2022, Accepted 1 June 2022, Available online 2 June 2022, Version of Record 16 June 2022.",https://doi.org/10.1016/j.jmacro.2022.103447,Cited by (2),"The COVID-19 pandemic is an unprecedented worldwide event with a massive impact on the economic system. The first Western country that had to face the COVID-19 crisis was Italy, which therefore represents a natural “case study.” By using the microdata and granular policy information available at the Italian Ministry of Economy and ",": 138).====The economic effects of the two pandemic waves are introduced considering four transmission channels. Following ====Everything else equal, we find that without the emergency fiscal measures the Italian GDP would have fallen by 13.4% in 2020 against the 8.9% observed. The impact of public interventions on the dynamics of investments is particularly significant. Without the liquidity support measures, investment would have fallen by 21.7% in 2020, compared with 9.2% in the observed scenario that includes the Government's measures.====After COVID-19 pandemic, interest in evaluating its impact on the economy and the policies implemented to contain its effects grew rapidly. A complete review of this literature is outside the scope of the present paper,==== our aim here is to mention the studies directly related to our assessment. In this respect, the most important are ==== and ====We borrowed most of the transmission mechanisms of the pandemic shock from ====. They also analyze the economic impact of the COVID-19 pandemic and its associated containment measures, but they focus on an aggregate EU perspective. By contrast, we evaluate a specific observed country case (i.e., Italy) and a specific package of policy measures quantified through granular information.==== ==== implement the COVID-19 shock as a mix of demand and supply constraints==== finding, on average, that the response of EU fiscal authorities would reduce the output loss by around four percentage points, one fourth of their assumed negative economic impact of the pandemic.====In their extensive study, ==== quantify the potential economic costs of seven possible COVID-19 outbreak scenarios by using a global intertemporal general equilibrium model with heterogeneous agents (the G-Cubed Multi-Country Model).==== ==== show that even a contained outbreak could significantly impact the global economy. They also emphasize the tension between the short and long run. COVID-19 outbreak is a crisis with several facets that requires monetary, fiscal, and health policy responses in the short term, but long-term policies are equally important and require greater investment in public health system. We explicitly focus on the short-run issue.====We are related to ==== model augmented with a detailed public sector, ==== On the methodological point of view, we formalize the idea that public guarantee were able to avoid the collapse of credit to firms with limited effect on public deficit.==== Specifically, as ==== investigate the combined role of monetary and prudential authorities in supporting bank-lending conditions. According to their results, monetary measures prevented the materialization of financial market volatility and the contraction of bank lending, with positive effects on firms’ employment. Our paper complements their analysis, focusing on the role of fiscal measures in supporting bank-lending conditions, mostly through public guarantees.====In the above respect. our paper is also related to the growing literature on the effectiveness of government guarantee schemes. ==== investigate the effectiveness of public guarantees exploiting discontinuities in the US federal rules. ====Closer to our contribution, ==== find that during the pandemic guaranteed loans were successfully extended to small firms operating in sectors severely affected by the restrictions. However, they also find some substitution of pre-existing non-guaranteed debt with guaranteed loans, which for Italy would be close to 10 per cent.====Finally, ==== focus on the monetary and fiscal policy interactions in a stylized a two-region monetary union after a pandemic shock, formalized as a mix of recessionary demand and supply simultaneous and symmetric shocks. In this setup, where an effective-lower bound for monetary policy is also considered, they show that expansive fiscal policies and monetary policies designed to limit the increase in long-term rates by purchasing sovereign bonds are required to effectively mitigate the union-wide recession. Moreover, effectiveness requires that a supranational fiscal authority issues a safe bond when investors perceive the bonds of one of the regions as riskier.====The rest of the paper is organized as follows. The next section provides an overview of the methodology on which our simulations are built. ==== describes the fiscal measures implemented in the model as well as the calibration of the main parameters, great ratios, and of the transmission channels related to the pandemic shock. ==== illustrates our findings. It reports the impact of the public intervention on the main macroeconomic variables and it disentangles the relative relevance of the different fiscal measures and shock transmission channels. A final section provides some concluding remarks.",The fiscal response to the Italian COVID-19 crisis: A counterfactual analysis,https://www.sciencedirect.com/science/article/pii/S0164070422000441,2 June 2022,2022,Research Article,54.0
Cavallari Lilia,"University of Roma Tre, Italy","Received 8 September 2021, Revised 12 March 2022, Accepted 20 May 2022, Available online 31 May 2022, Version of Record 8 June 2022.",https://doi.org/10.1016/j.jmacro.2022.103445,Cited by (0),", promotes consumption in the Home country and also production in the Foreign country.","International real business cycle (IRBC) models Backus et al., 1992, Backus et al., 1995 have been successful at reproducing important aspects of the propagation of shocks in open economies. Still, it is well-known that they imply unplausible (negative) comovements of output, labor and investment across countries (the comovement puzzle) because of the incentive to shift production to more productive countries, and excessive cross-correlation of consumption because of risk sharing. Moreover, most extensions with multiple goods, monopolistic competition and constant markups are unable to replicate realistic fluctuations in relative prices and the terms of trade. A vast literature has shown that nominal price rigidities, imperfections in labor and financial markets, distortionary taxes, non-traded goods, trade costs and demand shocks can help solve these anomalies (Obstfeld and Rogoff, 2000, Chari et al., 2002, Benigno and Thoenissen, 2008). The (more modest) goal of this paper is to document that a plausible demand structure can address these anomalies in the absence of other frictions.====I argue that a standard flexible-price model with monopolistic competition can be reconciled with evidence on the international business cycle once a more general microfoundation of the demand side is adopted. The analysis focuses on non-homothetic preferences and the general-equilibrium links between variable markups, intertemporal substitution and the propagation of shocks. These links are absent within the traditional Dixit and Stiglitz (1977) setup based on preferences with constant elasticity of substitution (CES). Constant markups, in fact, are neutral on the propagation of shocks under flexible prices.==== Non-homothetic preference aggregators, instead, generate an intratemporal elasticity of substitution, and therefore a demand elasticity, that changes with the consumption level and induces firms to modify their desired markups in response to shocks.==== Evidence on the lack of homotheticity is abundant in studies based on consumer behavior. The demand elasticity and the marginal propensity to consume out of disposable income are far from constant and they actually increase with the level of income (Attanasio and Browning, 1995). Moreover, individuals experiencing income decreases have a higher propensity to consume compared to individuals experiencing income increases ((Attanasio and Weber, 2010)). Yet, research is scant about their implications for the business cycle.====In closed economies, increasing demand elasticity (and countercyclical markups) amplify business cycle fluctuations because of the incentive for households to anticipate consumption and labor supply in periods – like booms – in which prices are relatively low and wages are relatively high (Cavallari and Etro, 2020). Here I consider an open economy and focus on a more flexible microfoundation of the demand side, allowing for variation in both the intratemporal and the intertemporal elasticities of substitution. I show that increasing demand elasticity magnifies the propagation of shocks within as well as across countries. In fact, it generates markups fluctuations in domestic and foreign markets (international price movements) that lead to positive spillovers across countries, while reducing the cross-correlation of consumption.====The theoretical analysis departs from a standard IRBC framework with monopolistic competition for final goods by replacing isoelastic demand functions associated with CES intratemporal preferences with general demand functions derived from a directly additive aggregator. This delivers simple markup rules under monopolistic competition, characterized by markups that depend on aggregate consumption and therefore change over the business cycle. Countries can trade goods and non-contingent bonds (Kollmann, 1995) and are subject to correlated shocks as in Backus et al., 1992, Backus et al., 1995.==== Markets for goods are segmented so that firms can choose different prices for each market (as in Betts and Devereux, 2000). I also consider the case of financial autarky (Cole and Obstfeld, 1991, Heathcote and Perri, 2002).====The analysis is based on a polynominal specification nesting CES preferences as a special case. An empirical model validation exercise combines standard calibration for technological parameters and Bayesian estimation for preference parameters, using U.S. data. The estimates support an intratemporal elasticity of substitution that increases with consumption, implying countercyclical markups under monopolistic competition, and an intertemporal elasticity slightly more than unitary and also increasing in consumption. To gauge the role of markup variation for shock propagation, the performance of the model under monopolistic competition is compared with its equivalent under perfect competition. The variability of output increases substantially, mainly due to an increase in the reactivity of labor supply, while consumption becomes marginally more volatile compared to perfect competition. The model generates positive comovements of output, labor and investment and reduces drastically the correlation of consumption between countries. These outcomes are due to endogenous pricing to market under monopolistic competition. A positive temporary technology shock in the Home country, by reducing Home markups, improves the Home terms of trade and depreciates the real exchange rate. This leads to an expansion in relative Home consumption, while promoting production, investment and labor supply also in the Foreign country to exploit the increased profitability of exports.====Endogenous markup variation has been extensively studied in macroeconomics. Standard New-Keynesian models generate countercyclical markups due to price stickiness, while here price are flexible and inflation is overlooked.====
 Rotemberg and Woodford, 1992, Rotemberg and Woodford, 1999 and, more recently, Bilbiie et al. (2012) have introduced countercyclical markups due to supply-side effects of either implicit collusion or endogenous entry.==== The focus here is instead on markup variability generated on the demand side. The closest papers are those considering non-homothetic preferences. Ravn et al., 2006, Ravn et al., 2008 have studied deep habits at the good level and Stone–Geary preferences to generate countercyclical markups under monopolistic competition. Cavallari and Etro (2020) have considered increasing intratemporal elasticity while intertemporal elasticity is constant. These works focus on closed economies. In open economy, Ghironi and Melitz (2005) have modeled endogenous entry of heterogeneous firms generating variable average prices (but not variable markups), while Scott and Huang (2011) have analyzed markups that vary between countries because of trade costs and imperfect competition among a fixed number of firms. However, these works build on CES preferences. I am not aware of IRBC models featuring endogenously variable markups that originate from changes in demand over the business cycle.====A contribution of this paper is to show that variable demand elasticity (and variable markups) can reconcile international business cycle models with the evidence on comovements in aggregates across countries. The result is important because it shows that standard real international business cycle models are not fundamentally at odds with evidence once a plausible demand structure is allowed. In addition, a variable elasticity of substitution between home and foreign goods (the Armington elasticity) adds a new dimension in the ample debate on the role of trade elasticity for the propagation of shocks worldwide. It is well-known that trade elasticity is important for capturing the variability of international prices and the comovements across countries (e.g., Corsetti et al., 2008, Benigno and Thoenissen, 2008, Enders and Müller, 2009, Rabitsch, 2012). Yet, there is large uncertainty regarding its value. The contribution of my analysis is to clarify that it is not only the value of the trade elasticity which is important, but also its variation. A trade elasticity varying at business cycle frequency, in fact, generates novel substitution effects and novel mechanisms of international interdependence that help bridge the gap with the data.====The work is organized as follows. Section 2 presents the baseline model under international bond trade. Section 3 discusses the quantitative approach. Section 4 provides simulation exercises. Section 5 concludes.",The international real business cycle when demand matters,https://www.sciencedirect.com/science/article/pii/S0164070422000428,31 May 2022,2022,Research Article,55.0
Cone Thomas E.,"Department of Accounting, Economics, and Finance, 119 Hartwell Hall, SUNY Brockport, Brockport, NY, 14420, United States of America","Received 28 July 2021, Revised 21 February 2022, Accepted 10 May 2022, Available online 25 May 2022, Version of Record 4 June 2022.",https://doi.org/10.1016/j.jmacro.2022.103441,Cited by (0),", which are functions of their beliefs about those variables. The learners’ models are misspecified because they do not observe the regimes. The novel dynamics occur if the regime switches are slow relative to the learning speed: Learners’ beliefs do not converge to the single process that usually describes their asymptotic beliefs, but to a Markov switching process. Checking for this possibility is crucial: When it occurs, the standard analysis gives incorrect results about the equilibria to which beliefs and other endogenous variables will converge. In an extension in which the learners choose their learning speed optimally, the result is strengthened.","This paper considers a New Keynesian economy with learning agents and introduces new dynamics, which affect the equilibria to which the economy can converge. If the economy has slow regime switching with fast learning (in a sense made precise below), learners cannot learn the standard equilibria. This is because the mean path of their beliefs over time is not the standard single process that results from learning; it is a regime switching process. That is crucial because it means that rational expectations equilibria (REE) that would normally be stable under learning, or E-stable, are not convergence points of the learning process. In fact agents learn their way to points that are not REEs.====The results are shown in the New Keynesian model with regime switching monetary policy as in Davig and Leeper (2007), with a modified information structure. Instead of knowing the stochastic law of motion for output and inflation, private sector agents attempt to learn it over time. However, their models are misspecified because they do not observe the regimes. I use simulations to compare two candidate limits for the learners’ beliefs, the single ordinary differential equation (ODE) which is standard in the adaptive learning literature==== and a new Markov switching ODE. It is the switching ODE which correctly identifies the points of convergence of the simulations.====In more detail, the learners are trying to learn the coefficients on shocks to inflation and output. E.g. for output ==== they are trying to learn the coefficients in ====, where ==== and ==== are demand and supply shocks. The equation for inflation is similar. The coefficients are equilibrium objects; they are affected by the learners’ beliefs about them. Their REE values change when the regime changes, but the learners do not handle this correctly due to the misspecification. Because the coefficients are affected by learners’ beliefs, correct predictions about the shocks’ effects require a correct theory of how the beliefs evolve over time. This was studied by subjecting output and inflation to one-standard-deviation shocks in simulations, and the forecast errors relative to simulation results were compared for the standard single-ODE theory and the novel switching ODE for the learners’ beliefs. While for inflation the new theory is only slightly better than the standard one, the story is different for output: the standard theory yields total output forecast errors of 139 basis points for the supply and demand shocks summed. The switching theory generates forecast errors of one basis point. These forecast errors are for a single period in a monthly parameterization; they’d be larger at the yearly frequency.====Three features combine to generate Markov switching ODEs:==== The first is constant-gain learning, in which the gain (the speed at which learners’ beliefs respond to new data) does not asymptote to zero over time. The second is Markov policy regimes that the learners do not observe. These two features naturally go together, as constant gain is an appropriate way to learn for agents who suspect there is occasional economic change but are unsure of its form (e.g. Cho et al. (2002), Evans and Ramey (2006)). The third feature is that the speed at which the learners learn is high relative to the speed of the regime changes. Formally, there is a (strictly positive) lower bound on the ratio of the gain to the mean regime switching speed as both are taken to zero==== in a comparative statics sense. It is this feature which differentiates the current results from extant theorems in the economic learning literature. It is important to emphasize that the result is not produced simply by assuming exogenous slow regime switching: both the switching speed and the learning speed are crucial. In fact, for any regime switching speed, no matter how low, there is a learning speed low enough to preclude belief switching. Thus the learning must be fast enough relative to the regime switching.==== Intuitively, slow switching with fast learning leads to the belief switching dynamics because it makes it easier for the learners’ beliefs to track the regime changes, even though regime changes are not explicitly in their models. Experimenting with simulations reveals that as the policy switching becomes slower, the gain that is just consistent with belief switching falls. I.e., for slower regime switching, slower learning can produce the endogenous switching of beliefs.====Though the original analytical results==== involve taking key parameters to zero, this is not at all necessary to produce the novel behavior of beliefs. In the simulated New Keynesian model it appears with quite reasonable values of the switching speed and gain, and with the rest of the parameterization also standard.====All the results are derived for E-stable switching and single ODEs—if they are unstable then beliefs are explosive (as confirmed by simulations), and there are no novel implications in that case.====I study the learning dynamics by deriving a switching ODE and comparing its (stable) rest points with the convergence points of learners’ beliefs in the simulations. The learners converge to the regime-specific rest points of the current regime, as predicted by the regime-switching ODE, not to the rest point of the standard single ODE. For comparison I then vary the gain and the switching speed, simulating the same economy but with very small gain, and then with fast regime switching. For both variants I expect the relatively slow learning to blur the regimes together from the learners’ point of view, leading to a single asymptotic belief process. The results bear this out, with the standard single ODE predicting the beliefs’ points of convergence.====After demonstrating switching dynamics, I remove the exogenous constant gain and study the economy with learners who choose their gain optimally, adjusting it over time to minimize their mean squared error. In this case the switching is even stronger.====The rest of the paper is organized as follows. Section 2 presents the New Keynesian model with two monetary policy regimes as in Davig and Leeper (2007). Section 3 discusses the learning and the two types of ODE. Section 4 derives analytical stability results for the ODEs. Section 5 presents simulation results and compares them to the predictions of the ODEs. Section 6 develops and presents simulation results for endogenous gain. Section 7 concludes.",Learning with unobserved regimes,https://www.sciencedirect.com/science/article/pii/S0164070422000398,25 May 2022,2022,Research Article,56.0
"Chanda Areendam,Cook C. Justin","Louisiana State University, Baton Rouge, United States of America,University of California, Merced, United States of America","Received 14 March 2022, Accepted 5 May 2022, Available online 24 May 2022, Version of Record 15 June 2022.",https://doi.org/10.1016/j.jmacro.2022.103438,Cited by (0)," regions and ==== households experienced relative and absolute increases in economic outcomes over the year and a half that followed. Utilizing monthly night-light data, we estimate that districts in the poorest quintiles experienced an increase in GDP per capita 11% greater than the richest. Using a longitudinal survey of expenditures and incomes for more than 140,000 households, we also show that the poorest quintiles had relative increases in expenditures and incomes of 35% and 18% respectively in the following eighteen months.","On November 8, 2016, the Indian government announced large currency denomination notes would no longer be considered legal tender and, with few exceptions, had to be deposited into banks by the end of the calendar year. These notes (in ₹500 and ₹1000 denominations) accounted for 86% of currency in circulation. For a country like India, where almost ninety percent of transactions take place in cash, such an abrupt announcement threw the economy into chaos. The avowed goals of the policy were reducing the volume of the “black economy”, increasing the tax base, and reducing funding sources for terrorist activities. There is considerable, if not outright, skepticism whether the policy achieved any of these. It is generally accepted that it reduced economic growth in the last quarter of 2016 and also to some extent in the first quarter of 2017. Very little is known about the longer run effects of such a dramatic policy. In this research, we investigate regional and household outcomes over approximately a year and a half following demonetization (till April 2018). For the regional analysis, we consider three pre-demonetization measures of socio-economic characteristics at the district level — a measure of the poverty rate (headcount ratio), a measure of financial inclusion (number of deposit accounts per capita), and a measure of economic activity (night-light density). For all three measures, we show districts that were placed in the lowest quintile, experienced greater increases in nighttime lights throughout 2017 and early 2018. We complement our regional results by examining household level data from a large longitudinal panel, and show that those in the poorest quintiles of household expenditures experienced larger increases in both expenditures and income relative to the richest.====Since the entire country was subject to the treatment simultaneously, our estimation strategy exploits two sources of variation for the regional analysis. First, we use a monthly district-level panel of nighttime lights that allows us to separate the pre, during (the two implementation months), and post-demonetization periods. Second, we exploit cross-sectional variation of pre-demonetization conditions at the district level by using the three aforementioned variables. To make the analysis easier to comprehend, we group districts based on quintiles of these variables. Our approach, which relies on the interaction between two sources of variation, is a difference in difference strategy with simultaneous treatment, where the entire population is treated at different degrees of intensity. Thus, we uncover the relative effects. For the household analysis, we use the Consumer Pyramids Household Survey (CPHS) conducted by the Centre for Monitoring Indian Economy (CMIE). Described in detail later, the proprietary survey follows approximately 160,000 households (approximately 143,000 in our sample) and records monthly income and expenditure details, as well as other household characteristics. We follow a strategy similar to the regional analysis, and divide households into quintiles based on 2015 real expenditures. We extract the relative effects of demonetization on the poorer quintiles relative to the richest.====After presenting the main set of results for districts and households, we show our estimated effects are robust to a range of sensitivity and robustness checks. While our respective baseline specifications already control for a variety of geographic and household factors, we also check for parallel and pre-trends, placebo tests, intensive margin of nightlights, continuous instead of discrete treatment, etc. We also investigate some associated channels. In the case of regional analysis, we consider the geography of deposit growth and uptake in a rural workforce program (NREGA), while for the household analysis we examine the sources of income increases.====The findings in this study stand in sharp contrast, though not necessarily contradictory, to much of the popular narrative and the emerging research that provides evidence of declines in economic activity and household welfare in the few months immediately following the announcement, e.g. Chodorow-Reich et al. (2020) and Karmakar and Narayanan (2020). Our results, at first glance, appear counter-intuitive, especially since the policy seemed to have failed in its goal of removing black money from the economy. Nevertheless, there are a number of reasons motivating the research.====First, for such a dramatic reduction of currency in circulation that took more than a year to replenish, the aggregate effects as measured by official real GDP statistics seem to have been rather benign — about 0.5 to 1% reduction in annual growth. Chodorow-Reich et al. (2020), also using night-light data, estimate a 2.5% reduction in GDP during that quarter, with the negative shock dissipating thereafter. Indeed, Lahiri (2020) notes the discrepancy between the aggregate outcomes reflected in the GDP numbers, and the negative short run effects in some of the research using household or firm level data. Fig. 1(a) highlights real quarterly GDP and its major constituents, after removing quarter effects. Clearly there is no visible decline immediately following demonetization. Further, Fig. 1, Fig. 1 which track the monthly night light data and surveyed household income and expenditures also show clear upward trajectories during most of 2017 despite being relatively stagnant between 2015 and 2016.====Second, despite the economic chaos during the initial two months, the policy was viewed favorably, suggesting that the government might have successfully altered expectations regarding future policy making and growth, especially among the poor and middle class. Indeed, Narender Modi’s Bhartiya Janata Party (BJP) was re-elected resoundingly in the national elections held in April 2019.====Third, the policy was considered a failure because 99% of the currency in circulation made its way back to the banks. The government had counted on a substantial fraction of notes never being returned. Clearly, those who had undisclosed currency figured out ways to “launder” their cash. These methods, which emerged quickly and were widespread, included channeling currency notes through accounts of employees, brokers who found low income households or zero balance account holders, advance salary payments to employees, settlement of debts, or even finding businesses (usually informal) who were willing to sell goods at a markup if purchased with the defunct notes. Invariably, all of these methods constituted an informal redistribution that involved paying premiums or commissions to the intermediaries and ultimately depositors. Bhagwati et al. (2017) and Koning (2017) are early essays speculating on unintended redistribution.==== An obvious question that follows is whether the potential redistribution through channels such as money laundering could be sizable enough to actually have any expansionary effect that might offset what would have otherwise been a major decline in economic activity. An accurate calculation would require knowledge regarding the amount of unaccounted cash in the country preceding demonetization — still very much an unknown. While there is no smoking gun, we can make a calculated guess. In an update to their earlier widely cited studies, Medina and Schneider (2018) estimate India’s shadow economy to be on average almost 25% of GDP for the period 2004 to 2015. Assuming that the cash-GDP ratio in the shadow economy is the same as that of the regular economy, i.e. 12%, unaccounted cash would amount to approximately 3% of GDP.==== If approximately 30% was redistributed during the money laundering process, this amounts to 1% of GDP. As a point of reference, refundable tax credits and assistance to households in 2010 as part of the American Recovery and Reinvestment Act (ARRA) stood at 0.36% of GDP. Thus, even if only 10% was redistributed, the relative value would be similar to ARRA rebates.====Laundering is not the only way redistribution might happen. Increased spending by the poor relative to the rich might also happen due to other factors. Indeed, a perverse reason might be intra-household regressive redistribution. Most Indian female homemakers do not have independent bank accounts, and regularly undertake “secret savings” — small amounts of cash accumulated over a long period of time, the true magnitude of which is often unknown to the male head of household (BBC, 2016). From the perspective of the head of household, revelations regarding these amounts would have been a positive shock. Heads of poorer households are likely to have a higher marginal propensity to consume out of this “newly discovered” wealth.====Fifth, the government realizing that they had under-estimated the initial chaos relied on a number of pre-existing transfer schemes, in particular, the National Rural Employment Guarantee Act, to alleviate short term hardship. It is certainly possible this might have also had a redistributive effect. Sixth, a rationalization for demonetization was formalization and push to digital banking. Whether this only happened more permanently is an open question, it may also have contributed to increased economic activity in poorer areas where the marginal effects would be greater.====Finally, even though the demonetization policy may have failed in its primary goal, it could have altered the behavior of wealthy households who may have become more cautious about their spending which in turn led to a compression. This provides further rationale for our two pronged strategy of looking both at regional and household effects.====To summarize, our hypothesis of an unintended redistribution can easily reconcile the facts that 99% of the money was returned indicating almost no wealth was lost in the aggregate, any negative GDP effects were mild and short lived, and the ruling party, BJP, did not pay a political price (and, on the contrary, increased its support).====The rest of the paper is organized as follows. In the next section, we discuss demonetization in some more detail and the related literature. In Section 3, we discuss the various data sources and provide an overview some of the important patterns. We cover the empirical specifications and present our results, both regional and household, in Section 4. Section 5 presents robustness results. In Section 6, we discuss some of the associated channels. Section 7 concludes.",Was India’s demonetization redistributive? Insights from satellites and surveys,https://www.sciencedirect.com/science/article/pii/S0164070422000374,24 May 2022,2022,Research Article,57.0
"Özmen M. Utku,Tuğan Mustafa","Department of Economics, TOBB University of Economics and Technology, Söğütözü Caddesi No:43, Söğütözü, Ankara, 06560, Turkey,Department of Economics, Middle East Technical University, Northern Cyprus Campus, Office T-137, Kalkanli, Guzelyurt, KKTC, Mersin 10, Turkey","Received 26 November 2021, Revised 6 May 2022, Accepted 9 May 2022, Available online 20 May 2022, Version of Record 31 May 2022.",https://doi.org/10.1016/j.jmacro.2022.103439,Cited by (1),This paper investigates the sectoral price and quantity responses to an exogenous ,"In this paper, we first revisit the question of whether sectoral prices exhibit common or divergent dynamics in response to shocks to monetary policy in the United States. We then analyze whether monetary policy shocks cause different price dynamics in fast-adjusting sectors where prices often change compared to the slow-adjusting sectors where prices change infrequently.====We document two empirical observations. Regarding the sectoral heterogeneity in price and quantity responses to monetary policy, we show that monetary policy shocks in the United States lead to divergent sectoral price dynamics, as found in Balke and Wynne (2007). Indeed, while some sectors have muted price responses to monetary shocks, others have significantly positive or negative responses. Also, we show that the shocks can have long-lasting relative price effects on the disaggregate personal consumption expenditure (PCE) categories. This is similar to the findings of Lastrapes (2006) that commodity prices significantly differ in their responses to a money supply shock even in the long-run, resulting in such a shock leading to a permanent rise in the dispersion of relative commodity prices. Regarding the link between the frequency of price changes in sectors and their response to a contractionary monetary policy shock, we find that the correlations between the frequency of price changes and the price and quantity responses are weak and virtually never significantly different from zero.==== This suggests that the degree of price rigidity in a sector does not play a decisive role in its price and quantity responses to monetary policy shocks.====Owing to the heterogeneity in sectoral frequency of price changes documented in the literature (see, e.g., Nakamura and Steinsson (2008)), several earlier studies have incorporated multi-sector features into the DSGE models; see Nakamura and Steinsson (2013) for a survey. Some of these studies differentiate sectors only on their frequency of price changes; for instance, Carvalho (2006), Nakamura and Steinsson (2013) and Carvalho and Nechio (2018). However, we show that a multi-sector Calvo-type model where sectors differ only in the frequency of price changes has difficulty in accounting for the low initial correlation of sectoral frequencies of price changes with sectoral price responses to shocks to monetary policy. The benchmark model we consider is a multi-sector version of the model in Giannoni and Woodford (2005) with four modifications: (i) firms have to pay the wage bill in advance, (ii) the model includes capital, (iii) we allow for sectoral heterogeneity in both price flexibility and production technologies, (iv) the decision on real expenditure is made one period in advance. We calibrate the bulk of the model parameters and estimate the remaining ones using the method of moments by matching several model-based correlations and impulse responses with the corresponding VAR-based ones.====First consider the case when we only allow for sectoral heterogeneity in the frequency of price changes. We show that in this case the model predicts no long-lasting relative price effects and excessively high initial correlation between sectoral price responses and sectoral frequencies of price changes. One straightforward explanation for the latter prediction is that firms in each sector can adjust to the shock only after receiving a price-change signal. Since the fraction of firms with such a signal is larger in flexible-price sectors than in sticky-price sectors, in early periods after the shock the prices in the former will give a more pronounced initial response than those in the latter. This eventually causes an almost perfect correlation between the initial sectoral price responses to the shock and the sectoral frequencies of price changes. We offer a more subtle explanation based on what (Christiano et al., 2005) refer to as the front-loading behavior of firms. This behavior results in price-adjusting firms in sticky-price sectors having less pronounced price responses in early periods than those in flexible-price sectors. This pricing behavior of firms in sticky-price sectors reinforces the muted price response due to the small fraction of firms receiving a price-change signal in these sectors and is related to the non-monotonic dynamics in the real marginal cost. Indeed, the marginal cost initially shows a substantial increase after an unexpected rise in the interest rate due to the working-capital channel in the model. In contrast, the marginal cost falls in subsequent periods with the falling rental rate and the real wage. Since price-adjusting firms in sticky-price sectors must consider that a new price-change signal may arrive with a long delay, they assign significant weight to real marginal costs in subsequent periods when setting their prices. This situation results in their initial price responses staying muted since lower marginal costs in subsequent periods lessen the impact of higher marginal costs in earlier periods. In contrast, price-adjusting firms in flexible-price sectors are mainly concerned with marginal costs today since a price-change signal arrives at them frequently. Consequently, when faced with higher marginal costs in the early periods, they do not hesitate to raise their prices.====Next, we consider our extended model featuring sectoral heterogeneity not only in the frequency of price changes but also in production technologies.==== We show that this elaboration kills two birds with one stone. Indeed, the model successfully explains both the long-lasting relative price effects and the insignificant VAR-based correlations between the sectoral price responses and the sectoral frequencies of price changes after monetary shocks. However, this success critically hinges on whether the model features the working-capital channel, sticky wages, and the sectoral heterogeneity in the production structure. We show that dropping any of these three features results in a remarkable failure of the model.====Notably, several papers such as Barsky et al. (2007), Bouakez et al., 2009, Bouakez et al., 2011, and Eusepi et al. (2011) also incorporate sectoral heterogeneity both in the frequency of price changes and in the production structure. Indeed, some of them are more sophisticated and have richer dynamics than our model in some aspects. For instance, Bouakez et al., 2009, Bouakez et al., 2011 allow for sectoral interactions through input–output linkages as well. However, these models include neither sticky wages nor a working-capital channel that are critical in explaining the empirical feature of low correlation between sectoral price responses and sectoral price change frequencies after a monetary shock, as discussed in the paper.====Our paper aims to contribute to the existing literature in three respects. First, we find that the evidence favoring significant relative price effects from monetary shocks is at least as substantial in the long run as in the short run. This finding questions the critical postulate of a typical New Keynesian DSGE model that responses of all prices to an unexpected change in the policy rate are same in the long run. It also adds to the result in Bils et al. (2003), Balke and Wynne (2007), and Boivin et al. (2009) that monetary shocks have relative price effects in the short run. Second, we shed some analytical light on the different degrees of the front-loading argument among sectors due to the heterogeneity in the frequency of price changes. Third, we show that our multi-sector model, equipped with the working-capital channel, sticky wages, and sectoral heterogeneity in price stickiness and the production structure, can successfully explain the two main empirical findings. In contrast, we argue that the existing multi-sector models in the literature have difficulty accounting for the insignificant correlation between sectoral frequencies and sectoral price responses after a monetary shock since they incorporate some -but not all- of these three features.====The organization of the paper is as follows. Section 2 provides a brief review of the related literature. Section 3 presents the empirical strategy for identifying monetary shocks in the United States and analyzes the impulse responses of sectoral prices and quantities to monetary policy shocks. Section 4 develops the theoretical models and evaluates the success of these models in explaining the strong and long-lasting relative price effects after a monetary policy shock and the weak correlation of the frequency of price changes with the responses of sectoral prices and quantities to the shock. The last section concludes the paper.",Heterogeneity in sectoral price and quantity responses to shocks to monetary policy,https://www.sciencedirect.com/science/article/pii/S0164070422000386,20 May 2022,2022,Research Article,58.0
"Lim Sokchea,Khun Channary","John Carroll University, OH, USA,Southern Illinois University Carbondale, USA","Received 17 May 2021, Revised 23 February 2022, Accepted 10 May 2022, Available online 19 May 2022, Version of Record 24 May 2022.",https://doi.org/10.1016/j.jmacro.2022.103443,Cited by (2),"We examine the impacts of ==== and permanent immigrants, and they are allocated between consumption and domestic investment through the household’s utility maximization. The results from extensive calibration exercises show that remittances in the presence of ==== hurt the traded sector of the developing economy, leading to a contraction in the aggregate output. Albeit to a lesser extent, the contraction persists even with the expansionary impacts of remittances through the collateral effect. In addition, the migration policy of the developing countries can weaken other development efforts, giving rise to a phenomenon known as a migration-remittance trap.","In a seminal work on economic development, Lewis (1954) argues that industrialization is feasible when there is an unlimited supply of labor from subsistence sectors. In low-income countries where population exceedingly outnumbers capital, there exists sectors including agriculture, petty retail trading, domestic service and the like where the supply of labor far exceeds the demand at a subsistence wage. Thus, new industries can be created or old industries expanded without experiencing labor shortages. Once capital accumulation catches up with labor, the capitalists can avoid the bottleneck by either importing labor or exporting capital to countries with abundant labor. Today, the relevancy of this postulation could not be overstated given the fact that millions of workers have migrated across national borders and the economic impacts of cross-border labor mobility have been front and center in policy debates in many developing countries.====For host countries, cross-border labor mobility evidently brings about unlimited supply of labor as put forth by Lewis (1954). In return, wages earned by migrant workers generate remittance inflows to home countries, resulting in a substantial accumulation of foreign exchange reserves. Between 1980 and 2019, the World Bank estimated that migrant remittance inflows to low and middle income countries increased from around US$18 billion to approximately US$548 billion. They account for well over a third of GDP in countries such as Haiti, Lebanon, South Sudan, and Tonga. The growth in foreign currency reserves has helped improve the countries’ credit rating and expanded their borrowing capacity in the international market (Bugamelli and Paterno, 2009, Chatterjee and Turnovsky, 2018, IMF and World Bank, 2009). In this respect, promoting labor mobility and remittance inflows make economic sense for low-income, labor abundant countries. After all, one of the United Nation’s Sustainable Development Goals is to facilitate safe, responsible labor migration and to reduce transaction costs of remittances.====From the existing literature, the evidence of the remittance impacts on home economies is mixed. While some studies find a positive relationship between remittances and economic growth (Faini, 2007, Lim and Basnet, 2017, Ramirez and Sharma, 2008, Ziesemer, 2009), others find a negative or no relationship (Chami et al., 2005, Donou-Adonsou and Lim, 2016, Gupta, 2005, IMF, 2005, Lim and Simmons, 2015). Using a stochastic general equilibrium framework, Bahadir et al. (2018) show that remittances are contractionary when they go to wage earners but expansionary when they are funneled to credit-constrained entrepreneurs. Chatterjee and Turnovsky (2018), on the other hand, utilize a two-sector small open economy model to demonstrate that remittances can be growth-enhancing through collateral effect. The improvement in the borrowing capacity of a country enables it to expand the formal sector and thus the aggregate output. However, these studies have treated remittances as an exogenous inflow of funds, abstracting the effect of the cross-border labor movement from the models. As evidenced in a study by Lim and Morshed (2015), the increased flow of remittances in recent decades comes from the surge in migration. In addition, based on a simple one-sector model capturing the link between labor migration and remittances, Lim (2021) argues that despite the positive effect of remittances through the collateral channel, labor migration causes a decrease in domestic capital accumulation leading to an economic contraction in the long run.====
 Lim (2022) extends the model to include a two-sector economy with a traded sector using foreign capital and a non-traded sector employing only labor. He shows that a poor country should promote foreign direct investment rather than encouraging labor migration as a policy for its economic development.====Moreover, there is compelling evidence indicating that countries that have actively promoted overseas migrant work have received a significantly smaller amount of foreign direct investment (FDI) than remittances while those that send a relatively smaller share of migrant workers receive a larger amount of FDI than remittances. Table 1 compares three South Asian countries with a large share of migrant workers with other three Asian countries with a relatively lower share. Bangladesh, India, and Pakistan have, for decades, sent a huge number of migrant workers abroad. Over the past three decades, the migrant stock in these countries on average accounted for 11.8%, 2.5%, and 9.6% of their respective labor force.==== They also received large remittances, amounting to an annual average of 5.8%, 2.7%, and 4.4% of their respective GDP. Surprisingly, every year these countries consistently received relatively smaller amounts of FDI – only 0.6% of GDP for Bangladesh, 1.2% for India, and 1.1% for Pakistan. However, countries like Mongolia, China, and Indonesia whose remittances only amounted to 2.6%, 0.2%, and 0.8%, respectively, received relatively larger amounts of FDI – approximately 7.4%, 3.4%, and 1.2%, respectively.==== The contrast between India and China  in terms of the level of migration tells a remarkable story about their development paths. China benefits more than India from its ability to attract foreign direct investment, thus putting its economy on a higher growth path in the past decades.====There are two important features of migration in South Asia: permanent and temporary migration flows. Workers who migrate from Bangladesh, India, and Pakistan to the Middle Eastern countries are generally temporary or short-term migrants who work on fixed-term contracts (Khadria, 2008, Wickramasekara, 2011). There are bilateral agreements between the governments of the sending and receiving countries and private recruitment firms act as a middle man, who posts job ads, interviews candidates, provides training, processes work visa, and sends migrant workers to the employers. The process also ensures that migrant workers return home after the end of their contracts. Doherty et al. (2014) estimate that the rate of return migration for Bangladesh and India during 2001–10 was about 49% and 39%, respectively. This type of migration is new and distinctive to the migration to other parts of the world, especially the US and UK, where the relaxed immigration laws often allow migrant workers to gain permanent residence and citizenship. Fig. 1 provides the trends of labor migration from Bangladesh, India, and Pakistan to different regions of the world and the share of remittances from those regions. The migrant stock in the Middle East increased since the early 1970s and more rapidly in the last two decades after the governments of these countries established the formal migrant recruitment processes. By 2017, there were over 3 million Bangladeshis, 3 million Pakistanis and 9 million Indians in the Gulf states – about 6 times, 3 times, and 3 times their respective presence in the US and UK combined. These migrant workers in the Middle East sent home more than half of the total remittances received by their respective countries. According to the World Bank’s estimates, in 2015 Bangladesh received 55% from the Middle East, India received 52%, and Pakistan received 61%.====Against the backdrop of possible counteractive effects between remittances and FDI, the paper sets out to contribute to the understanding of the role of remittances in the presence of labor migration. While many empirical studies suggest various channels through which remittances influence economic growth, we contend that a rather comprehensive study entails accounting for the role of migrant workers. To that end, we build upon the work of Lim (2022). First, Lim (2022) assumes that the non-traded sector employs only labor. Without domestic capital, remittances are only directed toward consumption, abstracting from the fact that they have been invested in acquiring assets and running small businesses. In contrast, we incorporate domestic capital into the non-traded sector and allow remittances to be allocated between investment and consumption according to the household’s preferences (utility maximization). Second, this study differs from the earlier study in its focus. While Lim (2022) compares and contrasts the policy to promote FDI and the policy to encourage labor migration, we examine two types of remittances: remittances from permanent immigrants vs endogenous remittances, the latter of which comes from temporary migrant workers whose movement is driven by either push (i.e. a reduction in migration costs) or pull (i.e. a productivity increase in the host country) factors. Finally, we tighten the calibration of the model by choosing more relevant small open economies for the host country, thus more reasonable parameters.====The cornerstone of our analysis is rested on the framework of a two-country, two-sector model: advanced and developing countries, and traded and non-traded sectors. The advanced economy produces a traded good using private capital and labor provided by its own citizens and migrant workers. While the private capital is converted from the traded good, firms in the advanced economy also decide on investing in the developing economy in the form of FDI. We assume that converting the traded good into private capital incurs an adjustment cost and investing in the developing economy also incurs a cross-border mobility cost. In addition, foreign capital income is taxed by the government of the developing country. We consider a reduction in the tax as an FDI policy.====There are two sectors in the developing economy. The non-traded sector can be considered as a traditional agricultural sector or petty retail trading that uses domestic capital and labor for its production whereas the traded sector represents a manufacturing sector that employs foreign capital and domestic labor. The household in the developing country consumes both traded and non-traded goods and decides on the allocation of time among leisure, domestic labor supply (in both sectors), and migrant work; thus labor migration is chosen optimally based on the wage differential between the two countries. However, migration incurs costs that are associated with job search, work permits, transportation, immigration laws, and so on. We consider migration cost reduction as the migration policy of the developing country to promote labor migration.====The household in the developing country receives remittances from two sources: short-term migrant workers and permanent immigrants who have long left the country. This feature is in line with the data of the South Asian countries that we discussed earlier. The short-term migrant workers earn a wage in the advanced economy and, after fulfilling their own consumption needs, remit a portion of the earnings back home. This generates an endogenous flow of remittances that is driven by labor migration. The remittances from the permanent immigrants are deemed negatively related to the output of the non-traded sector. This characterization is in line with the evidence for the altruistic behavior of the immigrants who remit more in response to economic hardship at home (see Acosta et al., 2009, Mandelman, 2013).====In addition, we assume that the households in both countries have access to the international financial market, but they face an interest rate that is above the given world interest rate. The borrowing premium reflects the country’s risk which is associated with its repayment capacity, measured by the ratio of its debt to GDP. For the developing country, the repayment capacity is augmented by remittance receipts, giving rise to the collateral role of remittances. More specifically, the developing country is able to securitize remittances to raise more funds from the international financial market. This characterization is consistent with Chatterjee and Turnovsky (2018).====Having derived the macroeconomic equilibrium, we calibrate the model to three representative developing economies including Bangladesh, India, and Pakistan and nine representative advanced economies, namely Hong Kong, Japan, South Korea, Kuwait, Malaysia, Qatar, Saudi Arabia, Singapore, and United Arab Emirates. In 2017, the three Asian countries sent a combined 15 million migrant workers to those developed nations. In addition, the former also accumulated approximately US$45 billion USD in FDI stock from the latter in 2012.==== As discussed earlier, the three South Asian countries receive remittances from both permanent immigrants and temporary migrant workers. This allows us to examine these two sources of remittances separately: exogenous vs endogenous remittances. For the latter, we consider both push and pull factors. The push factor comes from the migration policy on the part of the developing country to cut the migration costs so as to make it easier for its citizens to move for overseas work. The pull factor constitutes the productivity increase in the advanced economy which causes an increase in its labor demand. We also introduce the collateral impact of remittances. This enables us to identify the diverse channels through which remittances impact the developing economy. In addition, to examine the significance of migration policy for the development efforts of the developing country, we consider two combined structural changes: the FDI and migration policy mix and an increase in traded sector productivity together with the migration policy.====We find that although remittances boost consumption, they have a contractionary effect on the home economy as a whole. The output contraction is even greater after accounting for labor migration. That is, the negative effect of labor loss due to migration outstrip the positive collateral effect resulting from the greater accumulation of foreign reserves, thereby suppressing aggregate output in the long run. In the presence of diminishing return, labor migration reduces the marginal product of capital leading to the shrinkage in the traded sector. The non-traded sector benefits from remittance inflows; however, the traded sector suffers from the outflow of labor and the reduction in FDI. The study lends support to a paradox in international finance which noted that FDI does not appear to flow to developing countries and the constant outflow of migrant workers can be one of the contributing factors.====The developing country can benefit greatly from its FDI policy and an increase in the traded sector productivity. A cut in the foreign capital income tax rate or the efforts to raise the productivity of the traded sector attracts foreign capital, boosting the traded output as well as the aggregate economy. However, this development can be undermined by the migration policy. The response of migrant workers to a reduction in migration costs is instantaneous and long-lasting. The traded sector is adversely affected. The impact is amplified by an even lower domestic labor supply caused by the resulting increase in remittances enabling the households to enjoy more leisure. As a result, the aggregate output is lower or grows more slowly. The findings underscore the importance of attracting foreign capital and increasing the productivity of the traded sector while retaining workers for economic development.====This paper contributes to the literature in some important ways. First, we add more understanding of the impacts of remittances on economic development in the presence of labor migration. For example, Bahadir et al. (2018) and Chatterjee and Turnovsky (2018) examine the macroeconomic effects of remittances on the aggregate economy, but with a one-country model they only capture remittances exogenously. In this paper, we set up a two-country model which enables labor movement from a developing country to an advanced economy. Thus, we are able to capture both endogenous remittances which result from short-term labor migration and exogenous remittances that come from permanent immigrants. These features are consistent with the stylized facts in South Asia today. Second, while Chatterjee and Turnovsky (2018) show that remittances have an expansionary effect on the output through the collateral effect, we show that the contractionary impact of labor migration outweighs that of the collateral effect, thus leading to an output decline in the long run. In a related contribution, we also show that the migration policy of the developing countries weakens their own development efforts. The quantitative results are consistent with the empirical data that remittances and FDI inflows are on the rise; however, labor migration triggered by a reduction in migration costs has slowed down the FDI inflows, leading to lower long-run output. Finally, different from many previous studies including Bahadir et al. (2018), Chami et al. (2005), Chatterjee and Turnovsky (2018), Lim (2021), and Lim et al. (2021), the two-country model that incorporates capital flows in the form of FDI from the advanced economy to the developing country enables us to explain an observation in international economics that capital does not appear to flow to developing countries. The fact that the developing country uses foreign capital in its manufacturing, traded sector, and domestic capital in the traditional, non-traded sector is in line with the current characteristics of many developing countries today.====The remainders of the paper proceed as follows. Section 2 details the model while Section 3 derives the macroeconomic equilibrium. Section 4 discusses the calibration exercises together with the sensitivity analyses. Finally, Section 5 concludes the findings and provides policy implications.","Macroeconomic impacts of remittances: A two-country, two-sector model",https://www.sciencedirect.com/science/article/pii/S0164070422000404,19 May 2022,2022,Research Article,59.0
Le Riche Antoine,"School of Economics, Sichuan University, China,CAC – IXXI, Complex Systems Institute, France","Received 3 January 2021, Revised 12 April 2022, Accepted 18 April 2022, Available online 29 April 2022, Version of Record 6 May 2022.",https://doi.org/10.1016/j.jmacro.2022.103428,Cited by (0),This paper analyzes the interaction between ==== rule and distortionary balanced-budget tax policy rules on the stability properties of a one-sector Ramsey economy. The demand of money is motivated by a fractional cash-in-advance constraint on consumption expenditures. The monetary authority pegs the money growth factor while the fiscal authority uses a distortionary income tax or a distortionary ,"Balanced-budget rules recommendations to governments have been a debate since the European sovereign debt crisis, concerning mainly their consequences in terms of government debt sustainability. Nowadays, as shown by Schaechter et al. (2012), most OECD countries have adopted a type of balanced-budget rule.==== There is an important economic literature looking at balanced-budget rule. Both traditional business cycle literature (see among others Eggertsson, 2008) and political economy literature (see Azzimonti et al., 2016) study, under different perspectives, the normative properties of adopting such a rule. A different strand of literature focuses on its stability properties.====Balanced-budget rules have been criticized concerning their economic stabilization features, especially on the use of income tax rate and of consumption tax rate. On the one hand, the pioneer work of Schmitt-Grohé and Uribe (1997) shows that balanced-budget fiscal rule with income tax rate is likely to generate expectation-driven fluctuations.==== In their framework, balanced-budget rule implies that the tax rate is endogenous, non-linear and countercyclical with respect to its tax base. They find empirically plausible conditions for indeterminacy in the U.S. economy. On the other hand, Giannitsarou (2007) shows that local indeterminacy cannot occur with consumption tax rate if preferences are additively separable between consumption and labor. However, Nourry et al. (2013) show that her result is not robust to non-separable preference, and Nishimura et al. (2013), further, demonstrate that local indeterminacy could also arise in a multisector model.==== Both Nourry et al. (2013) and Nishimura et al. (2013) conclude that consumption taxation can be a source of expectation-driven instability for most OECD countries.==== In all of these papers, the main result is that countercyclical tax rates promote local indeterminacy, triggering cycles driven by self-fulfilling volatile expectations, that would not exist in the absence of a government. In contrast, procyclical tax rates are able to maintain saddle path stability.==== These works focus on the real economy and, thus, assume away the presence of monetary policy. The aim of our work is to fill this gap by looking at the effect of balanced-budget fiscal rules when a monetary authority follows a money growth pegging rule.====We examine how the interaction between fiscal and monetary policies alter local stability. We consider a one-sector model where the demand for money is motivated by a partial Cash-In-Advance (hereafter ====) constraint in the spirit of Hahn and Solow (1997). We assume that the monetary authority pegs the money growth factor while the fiscal authority levies taxes to finance its public spending. The fiscal authority could follow two different fiscal policies. In the first regime, the fiscal authority levies income tax rate, while in the second one, the fiscal authority uses a consumption tax rate. We show that regardless a tax rate is implemented or not a unique steady state emerges.====As a seminal work and the benchmark case of our model, Bosi and Magris (2003) show that, without balanced-budget fiscal rules, local indeterminacy requires either a small enough value for the share of the liquidity constraint or a high value together with a small enough intertemporal elasticity of substitution (hereafter IES) of consumption.==== By introducing balanced-budget fiscal rules, we, first, show that when the fiscal authority follows uses distortionary income tax rate, local indeterminacy is bound to prevail under similar conditions as without tax rate provided that the variability of the income tax rate has intermediate values. Second, when the fiscal authority finances the public spending by distortionary consumption tax rate, we prove that local indeterminacy is possible for a wide range of consumption tax rate variability. In this configuration, two pictures for local stability occur. On the one hand, for a low consumption tax rate variability, the unique steady state is locally indeterminate as soon as the IES of consumption is low enough. On the other hand, when a certain negative threshold of the consumption tax rate variability is attained, local indeterminacy occurs under similar restrictions as without tax rate.====We compare the local stability conditions without and with a balanced-budget fiscal rules. We show that both countercyclical and procyclical taxation could be (de)stabilizing depending on the amplitude of the liquidity constraint and the IES of consumption. Based on plausible empirical values, we present a simple numerical example considering countercyclical taxation. For both income and consumption tax rates, we find that the impact of a balanced-budget fiscal rules could is stabilizing. We prove that procyclical taxation do not always have a stabilization properties but, on the contrary, procyclicality of both income and consumption tax rates could be destabilizing by increasing the likelihood of the local indeterminacy region, whose existence is due to monetary frictions.====This paper is related to two strands of literature. The first one considers the effects of balanced-budget fiscal policies. The difference between our results and those of Schmitt-Grohé and Uribe (1997), Giannitsarou (2007) and Nourry et al. (2013) show that the impact of balanced-budget fiscal policy rules on local indeterminacy not only depends on preferences but, crucially, counts on the financial market imperfections existing in the economy. Contrary to them, we are able to show that local indeterminacy occurs for both countercyclical and procyclical (consumption or income) tax rates and, more importantly, does not require any trade-off between consumption and labor. The second one considers the implications of the interaction between monetary and fiscal policies on macroeconomic stability. Xue and Yip (2019) and Fu and Le Riche, 2021, Fu and Le Riche, 2022 are the closest papers to our and, as our, analyze how balanced-budget fiscal rules together with money growth pegging rule and cash-in-advance constraint alter local stability. On the one hand, Xue and Yip (2019) and Fu and Le Riche (2021) consider consumption tax rates. Xue and Yip (2019) consider a continuous time model where the fiscal authority uses both consumption tax rate and inflation while Fu and Le Riche (2021) consider an endogenous growth model with progressive consumption tax rate. We are different from them by showing that countercyclical income tax rate could be stabilizing. On the other hand, Fu and Le Riche (2022) analyzes the role of a flat income tax rate together with public spending externality incompressible public spending and variable public spending. We differ from Fu and Le Riche (2022) by considering instead a balanced-budget fiscal rules with distortionary income tax rate.====The remainder of the paper is organized as follows. In Section 2 we present the economy; we describe the fiscal policy and the monetary policy pursued by the public sector, the firms’ technology and the households’ behavior. Section 3 describes the intertemporal equilibrium and provides the steady state analysis. Section 4 is devoted to the analysis of the stability properties around the unique stationary solution without and with tax rate. In Section 5 some numerical examples are procured. Some concluding remarks are left to Section 6. The proofs are gathered in the online Appendix.",Balanced-budget fiscal rules and money growth pegging,https://www.sciencedirect.com/science/article/pii/S0164070422000283,29 April 2022,2022,Research Article,60.0
Ferreira Leonardo N.,"School of Economics and Finance, Queen Mary University of London, United Kingdom,Banco Central do Brasil, Brazil","Received 5 October 2021, Revised 6 February 2022, Accepted 6 April 2022, Available online 27 April 2022, Version of Record 6 May 2022.",https://doi.org/10.1016/j.jmacro.2022.103423,Cited by (1),Central banks have usually employed short-term rates as the main instrument of ,"Central banks have usually employed short-term interest rates as the main instrument of monetary policy. The extent to which such instrument is effective depends upon its ability to affect the path of expected future short-term real interest rates since, according to standard macroeconomic theory, such as Woodford (2003) and Galí (2015), consumption and output are driven by the sum of all future short-term real rates: the long-term real rate.====In recent years, a prominent alternative way to affect long-term interest rates has been intensively used: the communication about the likely future course of monetary policy, known as forward guidance. In this framework, if central banks can commit to a future path of interest rates, their communication may affect the economy even in the absence of changes in the short-term policy rate. Hence, forward guidance (or more broadly, communication) also becomes a policy tool.====These two policy instruments (short-term interest rates and forward guidance) are obviously intrinsically connected. First, forward guidance matters for the identification of the conventional monetary policy shocks in that such shocks cannot be properly recovered unless anticipated changes in the policy rates are taken into account. Second, forward guidance is one reason why, as pointed out by Ramey (2016), estimating the causal effects of conventional monetary policy has become a challenge. With anticipation effects and monetary policy conducted more systematically, finding truly exogenous monetary policy shocks in recent samples has become increasingly difficult.====On the other hand, forward guidance shocks can be a valuable source of not so systematic policy. This tool became prevalent during the zero lower bound (ZLB) period when the use of the conventional policy rate was constrained and episodes of truly exogenous forward guidance shocks can be found. Campbell et al., 2012, Campbell et al., 2017 show effectiveness of forward guidance shocks in moving long-term government bond rates. But what about the dynamic responses of macroeconomic and financial variables to these shocks?====This paper tackles this question by disentangling forward guidance and conventional monetary policy shocks in an innovative way: combining two sources of extraneous information with sign restrictions in a structural vector autoregressive (VAR) model estimated using data since the 90s, which is when the Federal Open Market Committee (FOMC) started to issue statements immediately after each meeting.====The first source of extraneous information is based on high-frequency futures prices and it builds on Kuttner (2001) and Gürkaynak et al. (2005). The use of high-frequency surprises around FOMC announcements is important to address endogeneity concerns as well as to help in the decomposition of the shocks. Specifically, the vector of variables of the VAR incorporates Gürkaynak et al. (2005)’s target and path factors, which capture surprises in the current and future rates respectively. Their inclusion together with the other variables in the spirit of Jarociński and Karadi (2020) is an alternative to their use as external instruments in Proxy SVARs.====Jarociński and Karadi (2020) combine sign restrictions and high-frequency surprises to identify monetary policy and information shocks, which is their object of study. In this paper, however, this combination will be used to cleanse the shocks of interest from any informational advantage the central bank may have. This is an alternative to the customary use of the Greenbook forecast data, with the advantage of not limiting the sample.====Nonetheless, sometimes sign restrictions may have to be complemented with additional restrictions in order generate a sufficiently rich shock structure as pointed out by Inoue and Kilian (2013) and Arias et al. (2019). The second source of extraneous information, which is the narrative account of some particular episodes, is then used to enhance and refine the identification. The idea was formalised by Antolín-Díaz and Rubio-Ramírez (2018) as narrative sign restrictions.==== First, sign restrictions consistent with economic theory are placed not only on the standard variables but also on the factors in order to properly isolate the shocks of interest. Then, uncontroversial episodes of forward guidance and conventional monetary policy shocks are used to refine the credible set.====Most importantly, following Uhlig (2005), the sign restrictions are agnostic. Therefore, the VAR model does not place any restriction on the responses of the industrial production and lets the data and the adjacent restrictions “decide” them, avoiding the circularity pointed out by Cochrane (1994). As in Uhlig (2005), the idea is to leave the question of interest open, but using prior information about the behaviour of the other variables through the sign and the narrative sign restrictions.====This agnostic approach is especially important because, notwithstanding the relevance of the topic, there is still a lack of consensus among researchers and policy-makers about the effects of forward guidance. For example, McKay et al. (2016) find that the effect on GDP in models with incomplete markets is much lower than in models with complete markets.==== Nonetheless, a few quarters of forward guidance is still powerful enough to effectively prevent recessions. In contrast, after adding several features to McKay et al. (2016)’s model to bring it closer to the data, Hagedorn et al. (2019) find that the effects of forward guidance are, in fact, negligible.====VAR models can then help shed some light on New Keynesian models, providing them with some reference and bringing them even closer to the data. Being Bayesian, it also allows for a formal comparison between the effects of forward guidance and the effects of conventional monetary policy in a high posterior density interval (HPDI) sense.====Therefore, following a trend that involves the combination of different strategies to improve inference in SVARs,==== this paper uses different sources of extraneous information in the identification of structural shocks and contributes to the forward guidance literature, showing that this tool is at least as strong as conventional monetary policy. The use of narrative evidence is particularly convenient since forward guidance is itself a narrative policy instrument, and combining it with high-frequency surprises sharpens the results and helps in the disentangling of the shocks.====Specifically, results show that the direction of the effect of conventional monetary policy is the expected even in a recent US sample, in contrast with the evidence revisited by Barakchian and Crowe (2013) and Ramey (2016). Results also show that the effect of forward guidance on industrial production are not different from the effect of conventional monetary policy in a HPDI sense.====The papers most closely related to this one can be divided into two groups. In the first group, forward guidance is mixed with conventional monetary policy. By using futures contracts whose horizon comprises at least the next FOMC meeting, the shocks coined as monetary policy shocks in the next two papers incorporate the impact of forward guidance. Andrade and Ferroni (2021) employ market-based measures of inflation expectations and future interest rates together with sign restrictions to identify Delphic and Odyssean monetary shocks. In a similar endeavour, Jarociński and Karadi (2020) explore the co-movements of interest rates and stock prices around the announcements combined with sign restrictions to identify monetary policy shocks and central bank information shocks. Debortoli et al. (2019) estimate a time-varying VAR that uses the 10-year government bond rate as a policy indicator and find that the responses to different shocks do not present material differences in the ZLB. The corollary is that unconventional monetary policy (including forward guidance) acted as a substitute for conventional monetary policy.====In the second group, forward guidance is isolated from conventional monetary policy. Similar to this paper, D’Amico and King (2015) combine measures of expectations with sign restrictions. Differently, however, they use survey-based measures of macroeconomic variables, which may respond with some delay as pointed out by Coibion and Gorodnichenko (2012) and Coibion and Gorodnichenko (2015) and may not fully isolate forward guidance shocks from other shocks affecting expectations. In fact, D’Amico and King (2015) acknowledge that any information, not only the shocks generated by forward guidance, which causes agents to change beliefs about the future course of monetary policy, should be captured in their identification. They see it as an advantage as they seem to be interested in overall anticipated monetary policy. Nevertheless, for the purpose of this paper, disentangling forward guidance shocks from conventional monetary shocks or any other kind, this would be a weakness.====Ben Zeev et al. (2019) identify anticipated monetary shocks following the literature on news shocks. The monetary news shock is orthogonal to current policy residual and maximises the sum of contributions to its forecast error variance over a finite horizon. As in D’Amico and King (2015), this captures the effects of forward guidance shocks but not only. Ben Zeev et al. (2019) also acknowledge that their “approach allows for any channels through which changes in expectations may arise”. Moreover, by estimating a quarterly VAR they are not able to capture near-term since the next meeting is approximately half of the time within the quarter.====The approach carried out by Ben Zeev et al. (2019) and the one undertaken in this paper can be seen as two extremes. On the one hand, one approach only uses information coming from the meetings. On the other hand, his news shock can capture much more than forward guidance. The choice made in this paper is based on the potential of high-frequency surprises to ameliorate the identification problem and on the fact that, despite using more speeches and other forms of communication, the statements are the main outlet to communicate forward guidance. In this extent, this paper is closer to the following 2 papers.====Lakdawala (2019) uses market-based measures of expectations, specifically the Gürkaynak et al. (2005)’s target and path factors, as external instruments in a VAR to decompose the effects of monetary policy. Here, on the other hand, these factors are incorporated into the vector of variables of the VAR, what makes the inference valid even if the VAR without them is not fully or partially invertible. Moreover, Lakdawala (2019)’s sample starts in 1979 and includes the Volcker disinflation period, what may affect the findings, while this paper focuses on a more recent sample, which corresponds to the period experiencing an increase in FOMC communication.====Bundick and Smith (2020) examine the macroeconomic effects of forward guidance at the ZLB using a modified path factor. They order this measure after real activity and the price level but before the 2-year rate in a recursive VAR. A caveat is that by restricting their sample to the ZLB, their estimation disregard numerous episodes of forward guidance that took place in the periods pre or post-ZLB. They work around this issue by also estimating the model over the pre-ZLB period. They find that forward guidance shocks produce similar results.====Hansen and McMahon (2016) follow a different path. They use tools from computational linguistics to extract and measure the information released by the FOMC on the state of economic conditions and on forward guidance, which is inputted in a factor-augmented VAR model identified recursively. Zlobins (2019) studies the effects of ECB’s forward guidance and also employs sign, zero and narrative sign restrictions. However, because he covers a period dominated by the ZLB and uses the 3-month EURIBOR rate, which also captures near-term forward guidance, for the identification of conventional monetary policy, shocks may be not properly disentangled.====This paper complements this recent literature by combining the advantages of high-frequency identification with the appeal of narrative sign restrictions to identify the dynamic responses of important macroeconomic and financial variables to conventional monetary policy and forward guidance shocks. The rest of the paper is organised as follows. Section 2 describes the econometric approach. Section 3 presents the results. Section 4 concludes.",Forward guidance matters: Disentangling monetary policy shocks,https://www.sciencedirect.com/science/article/pii/S0164070422000246,27 April 2022,2022,Research Article,61.0
Sáenz Luis Felipe,"Department of Economics University of South Carolina, 1014 Greene Street., Columbia, SC 29208, United States","Received 7 September 2021, Revised 14 April 2022, Accepted 18 April 2022, Available online 26 April 2022, Version of Record 7 May 2022.",https://doi.org/10.1016/j.jmacro.2022.103429,Cited by (0),"Manufacturing’s share of employment is known to follow a hump-shaped pattern as economies structurally transform. Motivated by the observation that sectoral capital intensities evolve over time, this paper examines whether such changes are important in accounting for this pattern. It does this by putting forth a structural transformation theory that allows for time-varying capital intensities, heterogeneous TFP, ==== and trade. The model is calibrated to South Korea (1970–2010), a country for which the rise and decline of the manufacturing employment share occurred within four decades for which comprehensive measures of sectoral capital income shares are available. I show that whereas traditional drivers matter for various elements of development, time-varying capital intensities are critical for generating the complete hump-shaped pattern in manufacturing employment: Time-varying capital intensities trigger an additional “push” of labor out of manufacturing, generating differences between employment and value added shares that are in line with the observed patterns of structural transformation in South Korea.","A puzzle in economic development is to explain why manufacturing shares of employment have a hump-shaped pattern among countries that have undergone structural transformations. Fig. 1 shows this pattern by plotting manufacturing employment shares as a function of income for several countries during the period 1950 through 2010. The manufacturing employment share rises during early stages of development and declines at later stages while services take off and dominate the employment participation. For countries that started their structural transformations during the 19th century, such as the United States and Great Britain, manufacturing employment shares were already in their declining phases during the post-WWII period, but for late starters, such as South Korea, both the rising and declining part of the hump are evident in a span of four decades since 1970. For India, a country that started to structurally transform even later, the employment share in manufacturing is still rising.====This paper proposes a new theory whereby changes in sectoral capital intensities across time account for the hump-shaped pattern in manufacturing employment shares. The theory is motivated by the fact that capital intensities across sectors sector do evolve over time, contrary to the standard assumption that production technologies at the sectoral level have constant (possibly heterogeneous) capital intensities. The observation of capital intensities in manufacturing growing over time at later stages of development is consistent with the modernization witnessed during industrial maturity, as documented by Rostow (1960, [p. 10]): During the industrial take-off, the production of manufacturing output gradually becomes more labor intensive as low wages allow for the deployment of labor in large scale manufacturing plants. At later stages, as the economy embraces the capacity of using “(...) the most advanced fruits of (then) modern technology”, technological developments bring new production processes that are more capital intensive as wages grow.====To test my hypothesis, I use the workhorse model of the structural transformation developed by Herrendorf et al. (2014), to which I adapt the preferences crafted by Comin et al. (2021). The advantage of these preferences is that the implied Engel curves do not level off as the economy grows richer.==== With this model structure, it is possible to consider income effects driven by nonhomothetic preferences, price effects due to varying rates of technological rates, and the role of trade in shaping the structural transformation separately. Although extant literature considers these factors as plausible candidates to understanding labor reallocation across sectors, the possibility of time-varying capital intensities in each of the three production sectors as an additional driver of the structural transformation has not been considered, and is the novel feature of this paper.====The conclusion of the analysis is that time-varying capital intensities are critical to generate the deindustrialization observed after the peak of the manufacturing employment share. Accounting for the growing bias toward capital in the production of manufacturing output allows the model to generate the additional “labor push” of (redundant) labor out of manufacturing to explain the declining part of the hump.==== Whereas income effects through nonhomothetic preferences, trade effects, and differences in technological growth rates across sectors are important to explaining the rise in the manufacturing employment share, they are unable to generate a hump-shaped pattern that follows the data closely.====I reach this conclusion by calibrating the model to the development experience of South Korea between 1970 and 2010. I construct a calibration algorithm that uses the first and last periods to assign values to preference parameters by matching the initial employment shares perfectly by construction. I then feed in the observed capital income shares for each sector to identify time-varying capital intensities, combined with data on labor productivity growth at the sector level. I compute a shooting algorithm to obtain the aggregate time-paths of real expenditure in consumption, wages and the real interest, and feed in these series in the model’s equilibrium allocations to generate predictions for employment and value added shares. I find that with time-varying capital intensities, the model is capable of generating the observed hump-shaped pattern for the manufacturing employment share in South Korea. I demonstrate that in absence of time-varying capital intensities, the model fails to account for the observed decline of the manufacturing employment share after it reaches a peak. Using a set of counterfactual experiments, I demonstrate that traditional drivers of the structural transformation (====, Engel curves, heterogeneous technological rates, and trade) do not deliver the hump-shaped path of manufacturing employment share, though they do matter when matching other dimensions of the structural transformation.====Traditional theories of structural transformation cannot account for differences between employment and value added shares, and while Herrendorf et al. (2014) document that these two shares have roughly similar patterns, Buera and Kaboski (2009) argue that the discrepancy between employment and consumption/output shares are an important puzzle for standard theories of structural transformation. This is relevant for the particular case of South Korea (and for several late starter countries), as one does observe that the value added share in manufacturing does not fall at late stages of development, in contrast to the employment share. Fig. 2 compares the evolution of employment and value added shares for the period 1970–2010. Panel 2(a) shows a similar patterns for employment and value added shares in agriculture. Although both patterns are not strictly parallel to each other, they do display a monotonic decline. Panel 2(b) shows more stark differences between employment and value added shares in manufacturing. Whereas the employment share displays a well-defined hump-shaped pattern, the value added share does not have a drastic deceleration after the peak in 1992. In fact, the value added share in manufacturing for 2010 is 40 percent, about 1.5 percent points below the “peak”. This process can hardly be called deindustrialization. Panel 2(c) shows that the rise in services is substantially steeper in employment ==== value added. Whereas the employment share rose about 40 percentage points between 1970 and 2010, the value added share rose about 10 percentage points during the same period.====I show that the introduction of time varying capital intensities allows for differences in the prediction of employment ==== value added shares. Whereas both shares are affected via price effects brought by sectoral TFPs and evolving capital intensities, there is an additional pure technical bias effect independent of prices for employment that is absent in the predictions for value added. This difference in sufficient quantitatively to generate a gap between employment and value added shares in line with the data.====Recently, several models have been constructed to reconcile the structural transformation’s stylized facts documented by Kuznets (1966) with the so-called Kaldor facts where the shares of labor and capital income, the capital–output ratio, the growth of capital and output per worker, and the real interest rate are constant (Kaldor, 1961). Although this model only reaches a Balanced Growth Path asymptotically when the economy converges to a single sector as in Acemoglu and Guerrieri (2008), I propose the concept of ==== (ABGP hereafter) whereby the aggregate variables are constant before the economy converges to services due to a compensating effect that takes place during the structural transformation: The growing bias toward capital in the production of manufacturing output is matched with labor intensive services and their greater participation in the economy, implying that the aggregate capital income share is constant. Then, along the ABGP the notion of different capital intensities over time across sectors does not necessarily contradict the Kaldor facts.====This paper relates broadly to the literature of structural transformation. This literature stands on the shoulders of the traditional development literature, such as Lewis (1954), Chenery (1960), Rostow (1960), Kuznets, 1966, Kuznets, 1968, Baumol (1967), and Harris and Todaro (1970), among many others.==== This paper belongs to the supply-side explanations of the structural transformation, which focuses mainly on disparities in rates of technological changes and their influences through price effects. The seminal contribution in this area is Ngai and Pissarides (2007), who formalize the concept of “Baumol’s cost disease” to explain labor reallocation due to heterogeneous technological rates in each production sector. It also relates closely to Acemoglu and Guerrieri (2008) who introduced differences in ==== capital intensities across sectors that delivers structural transformation through heterogeneous capital deepening. In their words (p. 468), “(...)[d]ifferences in factor proportions across sectors (====, different shares of capital) combined with capital deepening lead to nonbalanced growth because an increase in the capital-to-labor ratio raises output more in sectors with greater capital intensity”. Echevarria (1997) also considers a model in which sectors have differences in technology growth rates and time-invariant but heterogeneous factor intensities.====Noted above, from the income side of the structural transformation the nonhomothetic structure used here departs from standard use of Stone–Geary preferences used by Kongsamut et al. (2001) and follows Comin et al. (2021). With the introduction of long-run Engel curves, Comin et al. (2021) do account for the rise in services during later stages of development, but their model does not generate a ==== deindustrialization after the peak of the hump-shaped employment manufacturing share.====This paper also investigates the connection between trade and structural transformation, a research agenda influenced heavily by Matsuyama (2009), but it is more closely related to Uy et al. (2013), who investigate the hump-shaped manufacturing employment share in South Korea as consequence of greater openness and international trade. Their argument incorporates income and price explanations for the structural transformation in an open economy with a variation of the Ricardian theory of comparative advantages developed by Eaton and Kortum (2002). Although their model explains salient features of the structural transformation, it fails to generate the hump-shape for manufacturing employment.====
 Betts et al. (2013) also account for trade liberalizations in a structural transformation model calibrated for Korea. Although trade and trade reforms are quantitatively relevant when accounting for the structural transformation, the model still fails to deliver a hump-shaped pattern in manufacturing employment.====Two papers closely related to the hump-shaped industrial pattern during the structural transformation are Comin et al. (2021) and García-Santana et al. (2019). Comin et al. (2021) show that their model generates, predominantly through income effects, a hump for the Korean manufacturing employment share, but the predicted peak is not evident (the predicted employment share values for the late 1970s are about the same as the ones for the late 1990s), and there is no steep decline followed after the peak. This prediction is in line with my finding that absent of time-varying capital intensities one cannot get the observed steep decline in industrial employment at later stages of development even with long-run Engel curves.==== García-Santana et al. (2019) argue that the role of investment during the transitional dynamics is an important driver of the structural transformation and document a correlation between the hump-shaped investment rate pattern outside the Balanced Growth Path and the hump in the manufacturing ==== share.==== Both of these models deliver mechanisms that can generate the hump-shaped pattern in manufacturing without time-varying capital intensities as an additional driver, but they cannot generate different predictions for employment and value added shares, and quantitatively they do miss the sharp declined in manufacturing employment observed after the peak.====This paper is organized as follows: Section 2 provides empirical support for the relevance of time-varying capital intensities for the structural transformation and its relation to the Kaldor Facts. Section 3 describes a structural transformation model economy that allows for time-varying capital intensities over time and across sectors. Section 4 presents the calibration strategy, discusses the test of the theory, and illustrates the prediction of the model in light of the South Korean development experience between 1970 and 2010. Section 5 compares the predictions of employment and value added shares and shows that the model delivers different time paths for these shares that are fairly close to the data. Section 6 describes alternative hypotheses to account for the structural transformation in a set of counterfactual experiments. Section 7 concludes the paper.",Time-varying capital intensities and the hump-shaped evolution of economic activity in manufacturing,https://www.sciencedirect.com/science/article/pii/S0164070422000271,26 April 2022,2022,Research Article,62.0
"Hankins William B.,Cheng Chak Hung Jack,Stone Anna-Leigh","Department of Finance, Economics, and Accounting, School of Business and Industry, Jacksonville State University, 700 Pelham Road North, Jacksonville, AL 36265, USA,George Dean Johnson, Jr. College of Business and Economics, University of South Carolina Upstate, 160 East St. John Street, Spartanburg, SC 29306, USA,Department of Economics, Finance, and Quantitative Analysis, Brock School of Business, Samford University, 800 Lakeshore Drive, Birmingham, AL 35229, USA","Received 12 February 2021, Revised 21 December 2021, Accepted 10 April 2022, Available online 25 April 2022, Version of Record 11 May 2022.",https://doi.org/10.1016/j.jmacro.2022.103426,Cited by (0)," mix is an important channel through which uncertainty shocks affect employment growth. In particular, a state with a larger manufacturing sector experiences a larger decline in employment growth.","Uncertainty-induced business cycles, a phenomenon studied by Bloom (2014), occur when heightened uncertainty encourages firms and individuals to adopt a “wait-and-see” approach to planning for the future. Heightened uncertainty can also affect the allocation of resources and detrimentally affect productivity, employment, and economic growth. Bloom et al. (2018) study the impact that uncertainty shocks have on real business cycles. In a dynamic stochastic general equilibrium model, they estimate a reduction in aggregate output as a result of an aggregate uncertainty shock that manifests itself through declines in labor, capital, and productivity. Uncertainty lowers employment, they argue, because firms delay hiring following the uncertainty shock and do not replace workers who quit.====We combine the U.S. macroeconomic uncertainty index developed by Jurado et al. (2015) with state-level employment data to estimate how macroeconomic uncertainty impacts state-level employment growth. Using a factor-augmented vector autoregressive (FAVAR) approach we first find that a macroeconomic uncertainty shock is associated with reductions in the growth of aggregate GDP, consumption, and investment. We then show that macroeconomic uncertainty tends to trigger a decline in state-level employment growth, but that these declines are not uniform across the states and vary with respect to timing and severity.====In order to understand the variation in the state-level responses to uncertainty shocks we study the interaction between macroeconomic uncertainty and structural characteristics of the states. In the spirit of Mumtaz et al. (2018), we establish several structural economic factors that could plausibly transmit such a shock. Then, we leverage state-level variation in these transmission channels to explain the different responses of employment growth. This final analysis shows that states with a larger manufacturing sector experience larger reductions in employment growth following an uncertainty shock. This finding complements recent work by Leamer (2021), who shows that the depth and length of recessions, as well as the length of recoveries, at the state level depends upon each state’s industrial mix, with the role of the manufacturing industry being particularly important.====Our paper contributes to the general macroeconomic and labor economics literatures by adding to our knowledge about the factors that influence employment growth. Importantly, we provide estimates for the diverse state-level responses to a macroeconomic uncertainty shock. Secondly, our paper provides information about the state-level characteristics that can potentially affect the propagation of an aggregate uncertainty shock. In particular, these results can provide useful insights into the type of structural characteristics that could hamper economic recovery and require economic policies to alleviate the harm done to individuals unable to regain employment quickly.",The impact of uncertainty shocks on state-level employment,https://www.sciencedirect.com/science/article/pii/S016407042200026X,25 April 2022,2022,Research Article,63.0
Zhang Haiping,"Department of Economics, University of Auckland, 12 Grafton Road, 1010 Auckland, New Zealand","Received 14 November 2019, Revised 13 March 2022, Accepted 6 April 2022, Available online 18 April 2022, Version of Record 23 April 2022.",https://doi.org/10.1016/j.jmacro.2022.103425,Cited by (1),"The recent decades have witnessed upstream financial flows and rising intangible investment. Given heterogeneous pledgeability, we find that financial inflows have opposite short-run and long-run effects on intangible–tangible investment composition and the efficiency of capital formation. By reducing the investment elasticity along the extensive margin, rising wealth inequality undermines the efficiency gains from financial inflows. Similarly, market frictions and policy distortions that hinder entrepreneurship may also reduce the investment elasticity. Thus, our mechanism offers a new perspective for understanding cross-country differences in intangible–tangible investment composition.","The world economy has witnessed two prominent phenomena in recent decades. First, financial capital flows are “upstream” from poor to rich countries (Prasad et al., 2006, Ju and Wei, 2010), which is regarded as a cause of the declining interest rates in major advanced countries.==== Second, many advanced countries are moving towards the knowledge-based economy where intangible capital (e.g., computerized information, patents and brands, and organizational capital) becomes increasingly important.==== As shown in Fig. 1, intangible investment exceeds the tangibles in Finland, France, the Netherlands, Sweden, the United Kingdom and the United States between 2000 and 2013, while the opposite applies to other European countries (Corrado et al., 2018).====How would upstream financial flows and declining interest rates affect intangible–tangible investment composition and the efficiency of capital formation? We build an overlapping generations (OLG, hereafter) model and get three major findings.====Our findings hinge upon three key assumptions. First, both tangible and intangible investments are essential for capital formation, while only the tangibles can be pledged as collateral. As entrepreneurs have to finance the intangibles exclusively with their own funds, the unit cost of intangibles is constant at one. In contrast, the unit cost of tangibles is one minus the collateral value per unit of tangibles. The unit-cost differential induces entrepreneurs to invest inefficiently more (less) in the tangibles (intangibles), which distorts capital formation. Allocative efficiency is measured by the input–output ratio of capital formation. For simplicity, we call it productivity.====Financial inflows affect the unit cost of tangibles in two ways. First, the inflows of cheaper foreign funds reduce the domestic interest rate. Second, by augmenting aggregate investment, financial inflows lead to a fall in the marginal product of capital (MPK, hereafter) in the next period. The interest rate effect raises the collateral value and reduces the unit cost of tangibles in the current period, while the MPK effect works in the opposite direction.====Upon financial integration, the interest rate effect dominates the MPK effect so that the unit cost of tangibles falls immediately. It induces entrepreneurs to shift the investment towards the tangibles, which amplifies the initial distortion and reduces productivity. Along the convergence path, income growth raises the aggregate investment demand, which further attracts financial inflows and stimulates capital formation. As the domestic interest rate is aligned with the world interest rate, the interest rate effect is mute. The MPK effect raises the unit cost of tangibles, which induces entrepreneurs to shift investment towards the intangibles. Thus, the investment composition improves, and the productivity rises over time. ==== This is our first finding.====Can the productivity eventually exceed its initial level? Given the interest rate differential, the more elastic the aggregate investment demand, the larger the financial inflows and the investment expansion, the stronger the MPK effect, the smaller the short-run fall and the larger the subsequent rises in productivity, the more likely the productivity eventually exceeds its initial level.====To characterize the investment elasticity analytically, we introduce two additional assumptions, i.e., agents differ in net wealth, and capital formation is subject to a minimum investment requirement (MIR, hereafter).==== Given the binding borrowing constraints, only those with sufficiently high net wealth can meet the MIR and invest, while the others just lend out their net wealth. This way, the borrowing constraints and the MIR jointly act as a barrier to entrepreneurship, which endogenizes the mass of entrepreneurs. A fall in the interest rate and/or a rise in the current income allow more agents to meet the MIR and invest. The higher the wealth inequality, the less responsive the mass of entrepreneurs to these changes, the less elastic the aggregate investment demand along the extensive margin, the less likely the productivity exceeds its initial level in the long run. This is our second finding.====We derive the first two findings in the scenario where financial inflows ==== reduce the domestic interest rate, and the economy converges to a new steady state ====. We then move from the marginal to the global analysis and study the model dynamics over the entire state space. As shown in Matsuyama (2004), the extensive margin of investment is a channel through which financial integration amplifies income changes; if the extensive margin effect is strong enough, multiple steady states arise.==== By endogenizing the productivity in Matsuyama’s framework, we show that ====. This is our third finding. It suggests that a country should seriously consider the structural characteristics of its economy and the dynamic patterns of world interest rates, when it plans to liberalize its capital account or adopt capital flow management policies (IMF, 2012, IMF, 2020).==== The literature shows that financial integration may improve allocative efficiency via the cross-firm and/or cross-project resource allocation (Obstfeld, 1994, Acemoglu and Zilibotti, 1997, Varela, 2018, Alessandria and Qian, 2005). We propose the ====, intangible–tangible investment composition as a novel channel through which financial inflows have opposite short-run and long-run efficiency effects.====Although the literature proposes various channels through which financial integration fosters productivity growth, the empirical evidence is mixed (Kose et al., 2009a). Bonfiglioli (2008) and Bekaert et al. (2011) find the positive productivity effect of financial integration. Kose et al. (2009b) argue that debt flows are negatively correlated with TFP growth, while this negative relationship is partially attenuated in the economies with better financial markets and institutional quality. Kose et al. (2011) show that an economy needs to attain certain “threshold” levels of institutional quality and financial development to gain from financial integration.====In our model, debt flows trigger the ====, while the long-run effect depends on the investment elasticity. Besides the MIR and the borrowing constraints, other market frictions and policy distortions that hinder entrepreneurship would dampen the growth of incumbent firms (Klapper et al., 2006, Gutierrez et al., 2021) and reduce the investment elasticity. Innovations and reforms that mitigate these frictions and distortions may raise the investment elasticity, stimulate intangible investment and enhance the productivity gains from financial inflows. Consistent with the spirit of Kose et al., 2009b, Kose et al., 2011, our findings suggest that identifying the institutional factors relevant to investment elasticity may help improve the empirical estimates of the productivity effects of financial integration.====Heterogeneous pledgeability between tangible and intangible assets is well documented in the literature. Almeida and Campello (2007) find that firms with more tangible assets are less likely to be financially constrained. Bates et al. (2009) show that high-tech industries have to use internal funds to finance their R&D, and Gatchev et al. (2009) show that marketing expenses and product development are financed mostly out of retained earnings and equity. Falato et al. (forthcoming) show that only 3% of U.S. secured syndicated loans use patents or brands as collateral. Benmelech et al. (2020) find that U.S. non-financial firms have witnessed a secular decline in secured debts, and Dell’Ariccia et al. (2021) estimate that rising intangible capital explains around 30% of the secular decline in the share of U.S. commercial lending in banks’ loan portfolios. The literature has explored the implications of rising intangible capital in the closed-economy setting (Giglio and Severo, 2012, Lopez and Moppett, 2018, Wang, 2017), while we study this issue in the open-economy setting. ====Antunes and Cavalcanti (2013) and Jaumotte et al. (2013) show that financial globalization affects wealth inequality and welfare, while the impacts of inequality on the consequences of financial globalization have not been well studied in the literature. We propose a novel mechanism through which rising wealth inequality reduces the investment elasticity, dampens financial inflows, and worsens allocative efficiency.====The remainder of the paper is organized as follows. Section 2 presents the model, and Section 3 analyzes the autarkic equilibrium. Section 4 explores the productivity dynamics under financial integration, and Section 5 studies the implications of multiple steady states. Section 6 discusses two scenarios of world interest rate hikes. Section 7 concludes. Appendix A discusses several model extensions. Technical proofs and additional model extensions are available in the online appendices.","Upstream financial flows, intangible investment, and allocative efficiency",https://www.sciencedirect.com/science/article/pii/S0164070422000258,18 April 2022,2022,Research Article,64.0
Waki Yuichiro,"Department of Economics, Aoyama Gakuin University, 4-4-25 Shibuya, Shibuya-ku, Tokyo 150-8366, Japan,School of Economics, the University of Queensland, St. Lucia, 4072 QLD, Australia","Received 19 October 2021, Revised 28 February 2022, Accepted 20 March 2022, Available online 1 April 2022, Version of Record 5 April 2022.",https://doi.org/10.1016/j.jmacro.2022.103421,Cited by (0),". However, Farmer’s solution implicitly assumed that the natural borrowing limit never binds. A counterexample is provided herein in which the correct solution is nonlinear because the natural borrowing limit binds. To resurrect the linearity, one needs to restrict carefully the shock process of flow income and investment return so that the natural borrowing limit never binds. By doing so, however, one could throw away some important classes of problems with income shocks. Two models with temporary and persistent income shocks are used to show that the linear solution violates the natural borrowing limit with non-negligible probability in realistic settings.","Consumption-saving problems in the presence of shocks to income and return on saving do not generally permit closed-form solutions. These problems have been central to heterogeneous-agents incomplete-market models in which households are hit by uninsurable, idiosyncratic shocks to their income or the rate of return on their saving (Bewley, 1983, Imrohoroglu, 1992, Huggett, 1993, Aiyagari, 1994). The lack of closed-form solutions has led researchers to use numerical methods to solve these problems. In general, there is no explicit aggregation theorem for decisions of heterogeneous households particularly when both shocks to their income and the rate of return are idiosyncratic.====Farmer (1990) argued that a closed-form solution is available for a particular class of preferences, which he coined the RIsk-Neutral Constant Elasticity (RINCE) preferences. It is a special case of the Kreps–Porteus preferences (Kreps and Porteus, 1978, Kreps and Porteus, 1979) and assumes the risk neutrality but the intertemporal elasticity of substitution is finite and constant. He obtained the closed-form solution while allowing for an arbitrary joint shock process for income and for the rate of return on saving.====An important property of this closed-form solution is that both the value and the policy functions of a decision-maker are linear in her wealth. The decision-maker’s past actions affect the current value and policy functions only through her current wealth. All coefficients on wealth in these functions are determined exogenously by the stochastic process of real interest rates and the preference parameters. The linearity of a solution has been exploited to obtain aggregation in macroeconomics models since Gertler (1999).====This paper shows that there was an implicit assumption made by Farmer (1990) when he obtained the linear closed-form solution. The paper provides a counterexample in which this assumption is invalid. When the assumption is violated, the true solution is no longer linear in wealth, and linear aggregation fails.====The assumption is that the “natural” borrowing limit (Aiyagari, 1994) never binds; that is, the decision-maker never finds it optimal to borrow more than she can repay with certainty. When formulating the recursive version of the decision problem, Farmer (1990) only required that the decision-maker not leave any debt upon dying (i.e., in the terminal period), but imposed no borrowing constraints before the terminal period. His linear closed-form solution is the solution to this recursive problem.====However, when the aforementioned implicit assumption is violated, the closed-form solution in Farmer (1990) implies that the decision-maker accumulates so much debt that she cannot repay when her flow income turns out to be low. In other words, it is not a solution to the original, nonrecursive problem when the natural borrowing limit binds.==== The counterexample in the present paper is a two-period example; in the second period, flow income is random and may become zero with strictly positive probability. Given that the second period value function is linear in wealth as described in Farmer (1990) and that no borrowing constraint is imposed in the first period, the decision-maker has an incentive to borrow in the first period. This incentive is because her future income is, on average, higher than the current cash-on-hand and because she has a consumption-smoothing motive. However, if her flow income in the second period turns out to be zero, she cannot repay debt while entertaining non-negative consumption.====Therefore, a valid recursive formulation requires some borrowing constraints not only in the terminal period but also in the periods beforehand; however, then the solution is no longer linear in wealth. Once we impose some borrowing constraints, whether natural or ad hoc, we have occasionally binding constraints in the decision problem, and these create kinks in the value and the policy functions. I demonstrate this point by adding a borrowing constraint to the above counterexample.====Losing the linearity means much. Since (Gertler, 1999), the RINCE preferences have been widely used in macroeconomic models because the linearity of the value and policy functions can be exploited to facilitate aggregation in the presence of uninsurable, idiosyncratic shocks.==== Given that all households face the same real interest rate process, total consumption and saving within a group of households that share the same RINCE preferences are linear in within-group aggregate wealth. This property makes the model tractable because we do not need to keep track of the within-group wealth distribution when computing aggregate variables. However, without linearity, such aggregation is impossible.====It is possible to preserve the linearity of a solution by imposing more restrictions on the problem structure so that the borrowing constraints never bind. One source of the problem is that Farmer (1990) allowed the flow income and the real interest rates to follow ==== non-negative joint stochastic process. This approach is too flexible, and one may be able to restrict the stochastic process and entertain the linearity of a solution while keeping the problem both realistic and interesting. Section 4 makes this point.====However, as I will demonstrate in Section 5, some important classes of problems cannot be analyzed while entertaining the linearity. For this purpose I use two models to evaluate how likely the linear solution in Farmer (1990) violated the natural borrowing limit. One model features a temporary income shock and an income buffer that is provided by either a formal or informal insurance arrangement. The other features a large income shock due to job loss that may become persistent because of a search and matching friction and a simple yet realistic unemployment insurance. In the former model, I prove an asymptotic result that the probability that the linear solution violates the natural borrowing limit in period ==== converges to one half as ==== goes to infinity. This result does not depend on the details of the model. Conversely, I demonstrate the possibility that the natural borrowing limit does not bind for a sufficiently long period when there is an income buffer that protects households from downside income risk. This result depends crucially on the distribution of the before-transfer income, and for some distributions, too many households need to be protected by the income buffer in order to prevent the natural borrowing limit from binding for a sufficiently long period. In the latter model, the probability of the linear solution violating the natural borrowing constraint is sizable, and that the result is robust to changes in key parameter values, such as the unemployment benefit duration and the replacement rate.",A cautionary note on linear aggregation in macroeconomic models under the RINCE preferences,https://www.sciencedirect.com/science/article/pii/S0164070422000222,1 April 2022,2022,Research Article,65.0
"Das Sonali,Magistretti Giacomo,Pugacheva Evgenia,Wingender Philippe","International Monetary Fund, 700 19th St NW, Washington, D.C. 20431, United States","Received 27 August 2021, Revised 21 March 2022, Accepted 28 March 2022, Available online 31 March 2022, Version of Record 25 April 2022.",https://doi.org/10.1016/j.jmacro.2022.103422,Cited by (2),We examine the ,"The unusual sectoral effects of the COVID-19 pandemic on economies have renewed interest in the propagation and amplification of localized shocks in the aggregate economy.==== As the coronavirus spread widely across the globe, so did the economic effects of shifts in consumer preferences and containment measures put in place to try to curb its diffusion. The shocks, while stemming initially from restrictions in high-contact sectors, rapidly spilled over to other industries and to aggregate demand, both domestically and to other countries through trade and financial linkages.====In this paper, we study empirically the extent to which shocks originating in certain sectors spill over and end up affecting other sectors, in both the same country and abroad. Recent research has emphasized the importance of economic networks in amplifying sectoral shocks in the US economy (Acemoglu et al., 2016a, 2016b, henceforth AADHP and AAK respectively) and also found that complementarities in production can lead sectoral shocks to have large aggregate effects (Foerster et al., 2011; Di Giovanni et al., 2014; Atalay, 2017; Baqaee and Farhi, 2019). We conduct an empirical analysis that quantifies the size and persistence of sectoral shocks on a sector's subsequent activity and relative size, considering shocks originating in the same sector as well as in upstream and downstream sectors, both domestic and foreign. The international dimension of our estimation framework is, to the best of our knowledge, new for the kind of large-scale empirical exercise we conduct.====Our results are obtained for a large panel that covers economic activity in 32 sectors, in up to 43 countries and over the period from 1995 to 2014. In particular, we rely on inter-country input-output tables to quantify linkages across countries and sectors in the world economy. We consider both a supply-side shock, in the form of changes in sectoral total factor productivity (TFP), and a demand-side shock, in the form of changes in sectoral government purchases.====Applying local projections methods, we find that shocks originating in other ==== industries give rise to large and persistent spillover effects on the growth of real gross value added (GVA) in a sector, as already documented in AAK for the US. Moreover, we show that ==== sectoral spillover effects are also substantial, and amount to around half and two thirds of domestic ones for supply and demand shocks, respectively. Overall, total spillover effects of supply shocks propagating from supplier to client sectors are almost twice as large, on average, as the effects from shocks originating within a sector. For demand shocks, spillovers are up to seven times larger than own shocks. Moreover, we also document that a sector's share in a country's GVA remains persistently lower after negative supply shocks, especially for those originating from the same sector.====We provide an application of our framework to the current COVID-19 crisis. Our interest is again in the size of the spillover effects relative to the direct impact of shocks originating in any given industry. By combining measures of actual COVID-19 shocks by country and sector with results from our historical analysis, we show that almost half of the impact to activity in a given sector can be attributed to spillovers from shocks originating in other sectors, predominantly from shocks to domestic suppliers. The estimated spillovers are sizable and demonstrate the meaningful role spillovers played in amplifying the COVID-19 crisis. Compared to the average historical shock from our sample, they are somewhat smaller. This is driven by the fact that the pandemic shocks emanated mainly from contact-intensive services sectors that are relatively less central to production networks. We also document that, as expected, the decline in GVA due to spillovers vis-à-vis own shocks is larger for low-contact sectors, where the direct impact of lockdowns was less pronounced.====Our paper contributes to the literature on the macroeconomic effects of sectoral—and, more generally, microeconomic—shocks.==== This line of research has two main recent delineations. One branch adopts a model-based perspective to study the impact of sectoral shocks on aggregate volatility, macroeconomic tail risk, and output.==== Our paper relates more closely to the second branch of the literature, which takes an empirical and estimation-based approach to the subject. We build on the methodology of AAK, who develop an estimation framework to study the impact of various types of domestic network shocks on sectoral activity.==== As in AAK, we analyze productivity and government spending shocks.==== We, however, go beyond the US-centric analysis of AAK and consider spillovers across a large number of countries. Our framework is therefore more general and allows to study spillovers not only within, but also across countries, by making use of inter-country input-output linkages to construct foreign network shocks.====Our COVID-19 application is also related to the literature that examines the anatomy of the COVID-19 economic shock (Baqaee and Farhi, 2020 and forthcoming; Bekaert et al., 2020; Brinca et al., 2020; del Rio-Chanona et al., 2020; Guerrieri et al., 2022) and its transmission through global value chains (Cerdeiro and Komaromi, 2020; Bonadio et al., 2021). Several papers have also looked at past pandemics and recessions to draw lessons, with due contrasts, for the COVID-19 economic crisis and its recovery (Barro et al., 2020; Ma et al., 2020; Barrett et al., 2021; Eichengreen et al., 2021; Jordà et al., 2022). Our focus is on the sectoral dimension of the COVID-19 shock and its propagation through the domestic and international production and distribution network.====The rest of the paper is organized as follows. Section 2 outlines the methodology we follow to construct the sectoral shocks (II.A) and the data we use (II.B). We present our results in Section 3, considering the impact of shocks on sectoral GVA (III.A) and on the size of a sector (III.B). In Section 4, we describe the application of our framework to the COVID-19 shock. We conclude and present some policy implications stemming from the analysis in Section 5.",Sectoral spillovers across space and time,https://www.sciencedirect.com/science/article/pii/S0164070422000234,31 March 2022,2022,Research Article,66.0
"Crane Leland D.,Decker Ryan A.,Flaaen Aaron,Hamins-Puertolas Adrian,Kurz Christopher","Federal Reserve Board, United States of America","Received 10 February 2022, Revised 2 March 2022, Accepted 4 March 2022, Available online 22 March 2022, Version of Record 12 April 2022.",https://doi.org/10.1016/j.jmacro.2022.103419,Cited by (13)," have likely seen lower-than-usual exit rates, and exiting businesses do not appear to represent a large share of U.S. employment. As a result, exit appears lower than widespread expectations from early in the pandemic."," While the many indicators made available by private firms have improved understanding of recent economic developments, it is critical to be aware of historical patterns of business shutdown and how popular alternative indicators compare.====In this paper we review official data on business closures and deaths before the pandemic, providing a set of stylized facts that are necessary for evaluation of alternative indicators of business shutdown. We then evaluate a range of alternative indicators—including several new measures of exit we formulate—and discuss what they suggest about business exits during the first year of the COVID-19 pandemic.====Official data reveal business death to be a common occurrence, with about 7.5 percent of firms and 8.5 percent of establishments exiting in a typical year. Various measures of business closure—temporary and permanent—have been countercyclical in the past and rose notably during the Great Recession. Levels and cyclicality of business death are driven primarily by extremely small firms and establishments—those with fewer than 5 employees—though larger firms often permanently close individual establishments (locations) as part of geographic or ==== restructuring. The historical facts we document are interesting independent of COVID-19 considerations.====Alternative indicators of exit during the pandemic’s first year, on balance, suggest that exit has been elevated at least among small firms and establishments and particularly in the sectors most exposed to social distancing, though this elevated exit was partially offset by reduced exit in pandemic-friendly industries. A rough estimate is that the most troubled sector, other services (NAICS 81, which includes barber shops and nail salons), saw the permanent exit of more than 100,000 establishments ==== during the 12 months of March 2020 through February 2021. Results for other sectors may have been more mixed; for example, within the leisure and hospitality sector, some businesses—like full-service restaurants—saw significantly elevated exit, while other businesses—such as those focused on outdoor recreation—saw exit rates similar to, or even below, those of previous years. The retail trade sector appears similar in that some industries—such as clothing stores—saw elevated exit, while others—such as grocery stores—saw below-normal exit. Our best non-traditional measures are more indicative of establishment than firm exit, though we do have some firm-based indicators with useful insights.====Taken together along with some prudent guesswork, our sector-level results suggest economywide ====—that is, exit above and beyond pre-pandemic rates—was likely below 200,000 establishments during the first year of the pandemic, implying an exit rate about one-quarter to one-third above normal. This is roughly consistent with what we find from rough, preliminary estimates based on existing official Business Employment Dynamics (BED) data, which suggest roughly 185,000 excess establishment exits during the calendar year of 2020. We have less insight into ==== exit, though given historical patterns 200,000 excess establishment exits would imply roughly 130,000 excess firm exits. Relative to popular discussion and early expectations, our results may represent an optimistic update to views about pandemic-related business failure. Throughout the paper, though, we emphasize the limitations of our non-traditional data.====We draw these inferences from a number of timely, high-frequency business exit indicators, some of which have been used in existing literature. Two key contributions in this respect, however, are the construction of employment-weighted shutdown indicators from ADP payroll data and a permanent business exit measure based on SafeGraph cell phone geolocation data. We also review more commonly used data on small business operations from Womply, Homebase, and the Census Bureau’s Small Business Pulse Survey. As we show, these alternative measures are most useful in the context of historical patterns of business exit.====A key challenge is distinguishing between temporary shutdown and permanent shutdown (exit), since temporary shutdown was widespread in the early pandemic months (e.g., ====). U.S. statistical agencies provide data on business exits, but identifying exits is more difficult in alternative data sources. Typically what can be measured is whether a business is engaged in normal activities—e.g., receiving customer traffic, completing transactions, or paying workers. We use the term “shutdown” to refer broadly to businesses not engaged in normal activities, whether temporarily or permanently, and we attempt (loosely) to make guesses about actual exits based on how long businesses have been inactive. We use the terms “exit” and “death” interchangeably to refer to likely ==== shutdown.====A handful of papers study business closure early in the pandemic using, for example, new surveys (====) or official data on self-employment (====). ==== and ==== than in prior years (though, importantly, ==== is a different concept from exit). ==== and ==== measure business closures in Homebase data, and ==== measure early closures in Womply data. ==== uses Womply and Yelp data to estimate that as of July 2020, roughly 400,000 businesses had permanently closed during the pandemic; this number has been widely cited but is likely to be an overestimate in light of official and non-traditional data we review here, which were not available at the time ==== was written.====A number of studies by BLS researchers (====, ====, ====) track business closure in official business data; the authors use establishment microdata from the Current Employment Statistics (CES) along with establishment microdata with firm identifiers associated with the Quarterly Census of Employment and Wages (QCEW). These papers confirm that many establishment closures during April and May 2020 were temporary, though they note that closure rates stabilized somewhat by July. While the authors find that early establishment closure was far more elevated among small firms than among large firms, in late 2020 they observe an uptick in closures of establishments of firms with 250–500 employees; employment-weighted closures in this group remained historically elevated (by roughly 3 percentage points) into early 2021.====We first provide general background on the importance of business exit, drawing from the literature and the unique aspects of the COVID-19 pandemic (Section ====). We explore historical patterns of business exit, summarizing them as a list of stylized facts, in Section ====. We review a range of official and non-traditional measures of business shutdown during 2020 in Section ====. We take stock and conclude in Section ====.====The following is the Supplementary material related to this article. ",Business exit during the COVID-19 pandemic: Non-traditional measures in historical context,https://www.sciencedirect.com/science/article/pii/S0164070422000210,22 March 2022,2022,Research Article,67.0
"Corrado Luisa,Silgado-Gómez Edgar,Yoo Donghoon,Waldmann Robert","Department of Economics and Finance, University of Rome Tor Vergata, Italy,Central Bank of Ireland, New Wapping Street, North Wall Quay, Dublin, Ireland,Institute of Social and Economic Research (ISER), Osaka University, Japan","Received 21 May 2021, Revised 16 December 2021, Accepted 18 February 2022, Available online 9 March 2022, Version of Record 2 April 2022.",https://doi.org/10.1016/j.jmacro.2022.103412,Cited by (1),"We study information and consumption and whether consumers respond symmetrically to good and bad news. Our news variable captures the influence of consumers’ additional information beyond the fundamental (income or productivity) on current consumption. We use the information structure in which consumers see current (and past) productivity and receive noisy signals about future evolution of productivity. Our news measure identifies the contribution of those signals to current consumption and show that it has explanatory power. To obtain this series, we structurally estimate a simple permanent income consumption model with imperfect information using the aggregate U.S. time series. We then use the Panel Study of Income Dynamics (PSID) to analyze the response of households’ consumption to news about aggregate future income and test the hypothesis that consumers react more to bad news than good news. Our general results confirm the importance of the arrival of new information when households set their consumption decisions. We find that our news variable helps one predict households’ consumption change and that consumption responses are larger following negative (bad) news than positive (good) news. We suggest that observed asymmetric consumption responses could be due to agents’ aversion to ambiguous information.","Heterogeneity in consumption responses to income shocks has been widely discussed in the empirical literature. For instance, Fagereng et al. (2021) document heterogeneity in households’ responses to unanticipated income shocks, identified through lottery prizes using data from tax and income records of Norwegian households. Jappelli and Pistaferri (2014), using the 2010 Italian Survey of Household Income and Wealth, show that marginal propensity to consume is substantially higher for households with low cash-on-hand than affluent households.====One of the interesting empirical finding in the literature is consumption responses being asymmetric in the sense that the marginal propensity to consume (MPC) following negative income shocks is larger than that following positive ones (see, for example, Bunn et al., 2018). Asymmetric consumption responses may be attributed to a number of complementary factors. According to Carroll, 1992, Carroll, 1994, and Caballero (1990), the precautionary saving motive induces households not to react as much as they otherwise would following positive income changes. Given uncertainty regarding their future income, households choose to create a precautionary buffer to smooth out income shocks.====Similarly, with imperfect credit market accessibility, financially constrained consumers are not able to perfectly smooth out consumption following negative income changes. For example, as in Deaton (1992), if households are unable to substitute consumption across time due to their imperfect access to credit markets, when negative income shocks come along, households are unable to smooth out consumption and will have to reduce consumption substantially, delivering a large marginal propensity to consume. There are also transaction costs on borrowing or dis-saving, which may generate asymmetric responses in consumption (Jappelli and Pistaferri, 2014). Accordingly, the buffer stock theory predicts an important relationship between consumption in nominal terms and total liquid financial resources of consumers, i.e., cash-on-hand. In fact, if there is a positive income shock, even households with low levels of cash-on-hand might be able to save, generating a smaller consumption response than in the case of a negative shock.====In this paper, we examine an additional source of asymmetric consumption behavior due to information processing. We study whether consumers are neutral to the favorability of information they receive. Specifically, in an environment where permanent income consumers receive information about their long-run income, we show that consumption responses are asymmetric in that the size of consumption responses are larger following negative (bad) information about the future than positive (good) one. We focus on the information other than lagged and current income, and we label such perceived information about future income as ====. Our news variable captures the influence of consumers’ additional information beyond the fundamental (income or productivity) on current consumption. We apply a statistical model of noisy information about productivity to estimate our news measure using national accounts. We use the information structure in which consumers see current (and past) productivity and receive noisy signals about future evolution of productivity. Our news measure identifies the contribution of those signals to current consumption.====To construct this news variable, we follow the imperfect information literature where agents receive noisy news about the future as in Lorenzoni (2009), Blanchard et al. (2013), and Cao and L’Huillier (2018) among others. The theory is based on a model of business cycles driven by shocks to agents’ expectations regarding productivity where agents form anticipations about the future by observing noisy signals about productivity. These signals can be either news or noise, and agents must solve a signal extraction problem in order to determine how much to consume. If ex-post information proves to be news later agents adjust their expectations upward, and the economy gradually adjusts to a new level of activity; if ex-post information proves to be noise, the economy returns to its previous state of activity. Specifically, we use the methodology discussed in L’Huillier and Yoo (2017) and Yoo (2019) to decompose the effects of observing multiple signals on consumption fluctuations.====For our theoretical discussion, we consider a simple consumption model that hinges on a particular form of information structure and agents’ preferences. Specifically, we assume that information is ambiguous, so households’ attitudes toward ambiguity play a crucial role in determining consumption spending.==== Williams (2015) also shows that, in an ambiguous financial market environment, investors receiving earnings news behave cautiously and take a conservative approach by choosing the worst-case distribution of returns. This pessimistic approach to decision-making under ambiguity generates asymmetric behavior; that is, investors place more weight on bad than good news. In contrast, in absence of ambiguity, investors respond symmetrically to good and bad news, consistent with decision-making under conditions of risk, where any uncertainty is over payoffs rather than probabilities.====In our paper we postulate that when households exhibit aversion toward ambiguity in terms of consumption, they reduce spending more with negative information than they increase it with positive information. On the contrary, if consumers are ambiguity loving, they tend to increase spending more when receiving good information than they reduce consumption spending after receiving bad information. We build a news series by assuming that the representative consumer does not face possible ambiguity. Under the null hypothesis that consumers do not consider information to be ambiguous, the news variable should have a simple direct symmetric effect on household’s consumption. In this paper we test this null hypothesis under different model specifications.====Analysis of Panel Study of Income Dynamics (PSID) data shows that ==== our news variable helps one predict the change of households’ consumption and that ==== households’ consumption responses to news are asymmetric as they react more to bad news than good news, where good and bad news respectively refer to information that induce positive and negative consumption changes. That is, the data suggest that the average household in the PSID sample is ambiguity averse.====The rest of the paper is organized as follows. Section 2 presents the model. Section 3 describes our identification strategy and discusses quantitative results. Section 4 concludes.",Ambiguous economic news and heterogeneity: What explains asymmetric consumption responses?,https://www.sciencedirect.com/science/article/pii/S0164070422000155,9 March 2022,2022,Research Article,68.0
"Carrillo Julio A.,Peersman Gert,Wauters Joris","Banco de México, Directorate General of Economic Research, Calle 5 de Mayo #18, C.P. 06069, Mexico City, Mexico,Ghent University, Department of Financial Economics, Sint Pietersplein 5, B-9000 Gent, Belgium,National Bank of Belgium, Economics and Research Department, de Berlaimontlaan 14, B-1000 Brussels, Belgium","Received 13 January 2021, Revised 22 December 2021, Accepted 22 February 2022, Available online 9 March 2022, Version of Record 25 March 2022.",https://doi.org/10.1016/j.jmacro.2022.103417,Cited by (0),New Keynesian ,"Price and wage inflation can be very persistent. To replicate this feature, New Keynesian dynamic stochastic general equilibrium (DSGE) models feature structural sources of persistence that are either “inherited” from the driving forces of inflation (e.g., the output gap), or an “intrinsic” part of the inflation process (Fuhrer, 2010). For example, in addition to nominal rigidities, a standard assumption is that wages and prices are (partially) indexed to past inflation. In particular, this degree of indexation is hardwired as a constant and policy invariant parameter.====This assumption has been criticized for being specified in an ad-hoc way, rather than being part of the optimization process (Taylor, 2016). Moreover, a constant degree of indexation has been rejected by institutional and empirical evidence, particularly for wages. For instance, in the United States (US), contractual clauses indexing wages to past inflation were prominent during the ==== in the late 1970s, whereas such clauses practically disappeared during the ==== (we discuss this evidence in Section 5.1).====The degree of wage indexation to past inflation (====, for short) is very important for macroeconomic fluctuations and policymakers. When wage indexation is high, changes in inflation can trigger a reinforcing feedback loop between wages and prices that amplify the effects of shocks on inflation, the so-called second-round effects. Accordingly, larger changes in the policy interest rate are required to bring inflation back to the target. Therefore, the degree of wage indexation is crucial for the inflationary consequences of shocks hitting the economy. In this context, Hofmann et al. (2012) find that the decline of wage indexation between the ==== and the ==== in the US implies a reduction in the long-run impact of supply and demand shocks on prices of 44% and 39%, respectively.====This paper seeks to explain changes in wage indexation over time. To this end, we build a simple model of aggregate fluctuations with staggered labor contracts that endogenously determines the equilibrium level of wage indexation. Since sticky wages prevent a worker from optimally adjusting his or her labor supply to shocks, a gap emerges between his or her actual labor supply and the utility-maximizing level. The novelty of our model is that in periods when a worker’s wage is re-optimized, he or she can choose between indexing the wage to either past inflation or to the central bank’s inflation target (i.e., trend inflation, which may vary) until the labor contract can be re-optimized. Thus, a worker’s indexation choice is micro-founded in the model since it maximizes the expected utility that can be obtained between the two indexation schemes (subject to the average length of the labor contract and the specific structure of the economy). We also assume that wage setting takes place at a decentralized level, i.e., at the individual worker or firm level, which is consistent with the institutional evidence for wage bargaining in the US.==== We solve the non-linear model to compute the welfare criterion of workers. The sum of all workers’ decisions determines the degree at which nominal wages are indexed to past inflation, which we denote as the degree of ==== in the economy. We implement an algorithm that computes the equilibrium level of aggregate wage indexation given the economic regime, i.e., the specific market structures, stochastic shocks, and economic policy frameworks that agents face.====We present three main results. First, at the decentralized equilibrium, workers prefer to index wages to lagged inflation when permanent shocks to technology or to trend inflation drive output fluctuations, which entails a high degree of wage indexation to past inflation. In contrast, when shocks to aggregate demand dominate, workers prefer to index wages to the central bank’s inflation target, which leads to low wage indexation.====Second, we find a ==== among workers’ decisions. More precisely, the social planner’s solution is to index wages to trend inflation in regimes driven by technology and permanent inflation target shocks and index wages to past inflation in regimes driven by aggregate demand shocks — exactly the opposite to the decentralized equilibrium. Interestingly, the social planner’s solution is consistent with the work of Gray (1976) and Fischer (1977) on the socially optimal degree of wage indexation. These studies show that, to reduce output fluctuations, wage indexation should be high when ==== or demand-side shocks are important, whereas it should be low when ==== or supply-side shocks dominate.====Third, we show that the decentralized wage indexation equilibrium explains the observed time variation in aggregate wage indexation in the US. Using an empirically suitable model, we show that the decentralized wage indexation equilibrium matches the stylized facts reported in Hofmann et al. (2012): a high degree of wage indexation for the ====, and a low degree for the ====. In the first regime, technology shocks were highly volatile, and trend inflation drifted, while in the second regime, aggregate demand shocks gained relevance relative to other shocks. Counterfactual exercises (shown in the Online Appendix) reveal that the high degree of wage indexation in the 1970s was primarily the result of volatile supply-side shocks, whereas wage indexation vanished when these shocks became less volatile in more recent periods. In contrast, changes in monetary policy, including the anchoring of trend inflation, only played a minor role in determining aggregate wage indexation for the two periods.====One caveat is that we build on a standard New Keynesian DSGE model that does not provide the most detailed description of the labor market. Nevertheless, our model provides insight into workers’ wage indexation choices and shows that making a distinction between the centralized and decentralized equilibrium matters both theoretically and empirically.",Endogenous wage indexation and aggregate shocks,https://www.sciencedirect.com/science/article/pii/S0164070422000192,9 March 2022,2022,Research Article,69.0
"Sriket Hongsilp,Suen Richard M.H.","Department of Economics, Kasetsart University, 50 Ngam Wong Wan Rd, Lat Yao Chatuchak, Bangkok 10900, Thailand,School of Business, Department of Economics, Finance and Accounting, University of Leicester, Leicester LE1 7RH, United Kingdom","Received 27 July 2021, Revised 17 February 2022, Accepted 21 February 2022, Available online 5 March 2022, Version of Record 21 March 2022.",https://doi.org/10.1016/j.jmacro.2022.103416,Cited by (0),This paper re-examines the conditions under which endogenous economic growth can emerge in neoclassical models with non-renewable resources. Our analysis is based on a general production function which encompasses the Cobb–Douglas specification. We show that ,"Economists have long been concerned with natural resource scarcity and its implications on economic growth. In a seminal paper, Stiglitz (1974) examines these issues using the now-familiar neoclassical growth model with infinitely-lived consumers. It is shown that perpetual growth in per-capita output is sustainable even when natural resources are limited in quantity but essential for production. Most importantly for the present study, Stiglitz’s model is one of endogenous growth. This means the economic growth rate is not ==== determined by some exogenous technological factors, but rather it is derived within the model and can potentially be influenced by the choices of consumers, firms and the government. In a more recent study, Agnani et al. (2005, henceforth AGI) show that this endogenous growth result can also be obtained in a similar neoclassical framework but with overlapping generations of finitely-lived consumers. These findings have far-reaching implications for both resource economics and economic growth theory, as they suggest that practices and policies in natural resource management can influence the long-term prospects of an economy. One such policy is resource taxation.==== Existing studies typically focus on the microeconomic aspects of resource tax, e.g., how such a tax will affect a mining firm’s exploration and extraction decisions.==== Very few have examined the impact of resource taxation on the wider economy and economic growth.==== For most resource-producing countries, resource taxation is a significant part of the economy. Bornhorst et al. (2009) report that for a sample of 30 resource-producing countries (with various degrees of economic development), resource taxation on average accounts for 49.1% of total government revenues and 16.2% of GDP over the period 1992–2005. The sheer scale of this type of taxation warrants a thorough understanding on how it will impact the wider economy.====At the same time, there is a separate strand of empirical research which estimates the elasticity of substitution between capital input, labour input and resource input (typically proxied by commercial energy consumption) in the production process.==== While the estimates produced by this literature may vary across datasets and estimation methods, the general consensus is that the elasticity of substitution between any two of these inputs is not equal to one [see, for instance, Kemfert (1998), Kemfert and Welsch (2000), van der Werf (2008), Su et al. (2012) and Henningsen et al. (2019)]. These findings are directly at odd with one of the key assumptions shared by Stiglitz (1974) and AGI, namely a Cobb–Douglas production function in these three inputs.==== Motivated by these empirical findings and the real-world significance of resource taxation, the present study aims at answering two questions: First, will the endogenous growth result in Stiglitz (1974) and AGI remain valid under more general and empirically plausible specifications of production technology? Second, how will this affect the model’s policy implications, especially in regard to resource taxation? These are fundamental questions in understanding the relations between natural resource management and economic growth. Yet, they have been largely overlooked by existing studies. The present study is intended to fill this gap.====In order to address these two questions in the most direct manner, we adopt the same theoretical framework as in AGI and make only two necessary changes. First, instead of assuming a Cobb–Douglas production function, we consider several more general specifications that are in line with empirical research. Second, a constant flat tax on resource input is introduced. In our benchmark model, we begin with a general class of constant-returns-to-scale (CRTS) production technology in which capital input is functionally separable from effective labour input and effective resource input.==== The elasticity of substitution between the two effective inputs is denoted by ====, while the elasticity of substitution between capital input and these inputs is denoted by ====. In AGI’s specification, both ==== and ==== are constant and equal to one. These restrictions are removed in our benchmark specification. Similar to AGI, we focus on characterising balanced growth equilibria. In particular, we show that two types of balanced growth equilibria are possible, depending on the value of ====. These results are formally stated in Theorem 1, Theorem 2 in Section 3. Our first theorem shows that if ==== is constant and equal to one, then the common growth factor in a balanced growth equilibrium is endogenously determined as in AGI. This does not require any restrictions on ====, which can be variable and different from one. Hence, our Theorem 1 subsumes and generalises the endogenous growth result in AGI. Our second theorem then considers what will happen if ==== is ==== equal to one (in particular, it can be variable but bounded above or below by one). We show that in this case, perpetual economic growth is solely driven by an exogenous labour-augmenting technological factor as in the standard neoclassical growth model. Taken together, these two theorems showcase the central role of ==== in generating endogenous economic growth.====The main ideas behind these results are as follows: As is well-known in the economic growth literature, perpetual growth in per-capita output requires certain factor (either exogenous or endogenous) that can counteract the diminishing marginal return of physical capital (Jones and Manuelli, 1997 Section 2). Such a factor is dubbed as the “engine of growth”. In our benchmark model, if ==== is constant and equal to one, then total factor productivity (TFP) and resource input jointly serve as the engine of growth. While the growth rate of TFP is assumed to be exogenous, the utilisation rate of non-renewable resources is endogenously determined. This opens up a door through which other factors (such as consumers’ preferences and government policies) can affect the engine of growth. But if ==== is not equal to one, then a CRTS production technology will require effective resource input and effective labour input to grow at the same rate in balanced growth equilibria. This imposes a restriction on the utilisation rate of non-renewable resources. In particular, this rate is now pinned down by the exogenous growth rate of labour input and technological factors. As a result, the engine of growth is solely determined by exogenous forces.====The present study is also related a growing literature which shows that, in most (if not all) of the existing economic growth models, balanced growth equilibria are possible only under some “knife-edge” conditions [see, for instance, Groth and Schou (2002), Growiec (2007) and Bugajewski and Maćkowiak (2015)]. These existing studies are primarily concerned about balanced growth equilibria in general, without distinguishing between exogenous and endogenous growth. This distinction, however, is the subject of our analysis. In particular, our results suggest that even if the conditions for balanced growth equilibria are met (e.g., production function exhibits CRTS), endogenous growth will require yet another “knife-edge” condition (namely, a unitary elasticity of substitution between certain inputs).====Despite the simplicity of our benchmark model, it is able to produce a rich set of predictions regarding the effects of resource taxation. The two elasticities, ==== and ====, again play a critical role in this matter. To sharpen our predictions, we adopt a two-stage, nested constant-elasticity-of-substitution (CES) production function in this part of the analysis (i.e., both ==== and ==== are now scalar parameters). Nested CES production functions are commonly used in models with more than two productive inputs.==== We present two main findings regarding resource taxation. First, if ==== is one and ==== is no less than one, then a unique balanced growth equilibrium exists under some additional conditions (Proposition 1) and resource taxation is growth-enhancing (Proposition 2). The intuition is as follows: A higher resource tax rate means that it is now more costly to use resource input for production. This will deter the utilisation of non-renewable resources. As a result, a larger stock of resources will be available for future use. By the complementarity between capital input and resource input in the production process, this will raise the marginal product of capital and the return from capital holdings. This will then promote capital accumulation and economic growth. If ==== is one but ==== is ==== than one, then multiple balanced growth equilibria may emerge and resource tax is either growth-enhancing or growth-prohibiting depending on the equilibrium in question. This is shown by means of a numerical example. Our second major finding is that if ==== is ==== equal to one, then any changes in resource tax will only affect the level of per-capita variables but not their growth rate (Proposition 3). There are again two possible scenarios, depending on the value ====. If ==== is strictly less than one, then an increase in the resource tax rate will raise the after-tax price for resource input and discourage its use. By the complementarity between capital input and resource input, this will then lower the level of physical capital in the balanced growth equilibrium. The opposite results hold if ==== is strictly greater than one. These results demonstrate the importance of ==== in analysing the effects of resource taxation.====Whether ==== is equal to one is ultimately an empirical question. In the aforementioned empirical literature, studies typically report a less-than-unity elasticity of substitution between labour input and energy consumption [see, for instance, Kemfert (1998), Kemfert and Welsch (2000) and van der Werf (2008)]. When combining with these estimates, our benchmark model suggests that (i) any changes in the tax rate on resource input will have no effect on economic growth rate, and (ii) a higher resource tax rate will negatively impact capital formation and aggregate output. These predictions are in stark contrast to those obtained under a Cobb–Douglas production function.====The rest of the paper is organised as follows: Section 2 describes the setup of the benchmark model. Section 3 presents the baseline results concerning the balanced growth equilibria of the model. Section 4 provides some discussions and robustness checks on our baseline results. Section 5 concludes.",Sources of economic growth in models with non-renewable resources,https://www.sciencedirect.com/science/article/pii/S0164070422000180,5 March 2022,2022,Research Article,70.0
"Bettoni Luis G.,Santos Marcelo R.","Insper, Brazil","Received 23 August 2020, Revised 21 February 2022, Accepted 22 February 2022, Available online 5 March 2022, Version of Record 19 March 2022.",https://doi.org/10.1016/j.jmacro.2022.103418,Cited by (0),"An important stylized fact about public sector employment is that it predominantly hires skilled and more experienced workers. In this paper, we consider a search and matching model with public sector and on-the-job human capital accumulation that incorporates this stylized fact to study how the public sector employment affects the ====. In the model, public sector employment affects aggregate fluctuations by changing the composition of workers employed in the private sector. Because workers accumulate human capital and become more productive when employed, the flow of benefits from forming a match are spread over time. In this environment, if the flow into the public sector increases with human capital, then the government hiring policy decreases the firm’s benefit of hiring and the matching surplus, increasing the responsiveness of labor market tightness to shocks. We calibrate the model for the Brazilian economy and show that this mechanism amplifies the effects of public employment on vacancy creation and private sector employment volatility.","The public sector is the largest employer in the economy, both in advanced and developing economies. Behar and Mok (2019) report that Governments of OECD countries account for 18 percent of total employment. In addition, the hiring process in the public sector is biased towards skilled and more experienced workers. Giordano et al. (2011) report that the average share of workers with a tertiary education is 2.6 times higher in the public than in the private sector in Euro Area countries. In the case of Latin America, Mizala et al. (2011) report that the average years of education in the public sector are 3 to 6 years higher than in the private sector. Fontaine et al. (2020) show that the public sector represents a larger fraction of employment of older workers, accounting for 25 percent of their employment in France and the UK and 22 in Spain and the US, while it accounts for only a small fraction of total employment for young workers.====These facts indicate that the public sector employment may have important consequences for the overall performance of the labor market as the selection of skilled and more experienced individuals into the public sector affects the vacancy creation of firms and the searching decisions of workers in the private sector. In this paper, we consider a version of the Mortensen–Pissarides search and matching model with public sector that incorporates the stylized facts above to study the effects of hiring policies in the public sector on the private sector employment and labor market volatility.====Fig. 1 presents the cross-country relationship between the share of public sector employment in the labor force and the labor market volatility for a large set of developing and advanced economies from 1980 to 2018. In the top panel, we show in the left figure the relationship for private employment, while the right figure shows the relationship for the unemployment rate. In both cases, volatility is measured by the average standard deviation and the solid line is the best linear fit. It can be seen that the higher the share of public sector employment, the bigger the standard deviation of both the private employment and the unemployment rate. In addition, in the bottom panel we show that the relationship also holds when one looks at the share of civil servants with tertiary education. We take the correlations in Fig. 1 as suggestive evidence of the effect of public employment on labor market volatility.====In our model, households have preferences for consumption smoothing and individuals are organized in families that pool idiosyncratic risks as in Merz, 1995, Andolfatto, 1996. The economy is closed so changes in aggregate productivity generate changes in aggregate consumption and thus discount rates. Lifespan is uncertain and agents survive from one period to the next with a given probability. This stochastic OLG environment allows the model to accommodate differences in life-cycle earnings growth between private and public workers in a very tractable way. Individuals labor productivity is determined by two components. First, agents are ex-ante heterogeneous with respect to their ability, which can be described as pre-market skills obtained through education. The second component is interpreted as human capital or experience and accumulates in a learning-by-doing fashion. In particular, we build on the work of Ljungqvist and Sargent (1998) in assuming that worker’s productivity changes over time according to laws of motion that depend on whether the worker is employed or not. Employed agents experience increases in productivity on average, while non-employed agents experience declines in productivity on average. In addition, government hires civil servants to produce a public good that enters in the individuals utility functions. Both private firms and the government can direct their search by posting vacancies for agents of a given level of education and human capital. Job creation is exogenous in the public sector and is calibrated to reproduce the fact the government hires disproportionately more high skilled workers. The process of human capital upgrading and downgrading generates endogenous changes in wages and labor market states. For example, a private sector employee who upgrades his human capital may endogenously negotiate a higher wage or quit the job relationship in order to transit to the public sector.====The model economy is calibrated to be consistent with micro and macro evidence for the Brazilian economy. Brazil is an interesting laboratory since the public employment is relatively large and, as shown below, employs mostly skilled and more experienced workers. The model is able to match very closely the separation and job finding probabilities for each sector, as well as the average wage growth of continuously employed workers. In addition, vacancy creation in the public sector is treated as exogenous and is calibrated to match the share of civil servants by income quintile. Consistent with the data, public sector jobs in the benchmark model are concentrated at the right end of the skill distribution in such a way that civil servants correspond to nearly 42% of the top 20% earners.====We then use the calibrated model to study how the government hiring policy affects the job creation in the private sector and labor market volatility. To this end, we consider the following experiment. Starting with the economy at its steady state, we compute the impulse response function for the private sector employment in response to an unanticipated 1% drop in aggregate productivity. Next, we repeat the experiment in a counterfactual economy where job creation in the public sector is less biased towards skilled and more experienced agents, but keeping the size of government constant. We find that the fall in employment is nearly 35% smaller in the counterfactual economy compared to the benchmark economy.====We show that human capital accumulation is the key ingredient driving the differences in employment responses in our model.==== The channel through which the government hiring policy affects the private sector employment and the aggregate fluctuations is by reducing the expected return of a match for private sector firms. The intuition is that hiring workers is an investment activity in which costs are paid up front and benefits accrue gradually as workers’ productivity increase. This is so because the flows of benefits from forming a match are much longer-lived with on-the-job human capital accumulation. In this environment, if the flow into the public sector increases with the workers’ human capital then the government hiring policies reduce the expected benefits of a match for the firm, increasing the responsiveness of labor market tightness to shocks. This is the same intuition behind the results of Hagedorn and Manovskii (2008), who show that an increase in the flow value of unemployment results in a stronger response of market tightness to shocks due to smaller matching surplus.====This paper contributes to a growing literature analyzing the interactions between public sector and unemployment volatility. Papers in that vein include Quadrini and Trigari, 2007, Afonso and Gomes, 2014 and Gomes (2015), which study how public sector wage and compensation policy affects private sector employment and unemployment volatility. Hörner et al. (2007) and Boeing-Reicher and Caponi (2016) also analyze this issue, but focusing on other public sector instruments. In turn, Esteban-Pretel et al. (2021) evaluates how changes in Japanese fiscal policy throughout the 1990s relates to the unemployment growth in the period.====We also relate to the literature studying the effects of the concentration of more educated workers in the public sector. Baerlocher (2021) studies the relationship between the size of public sector employment and level of human capital and economic growth. Gomes (2018) examines the effects of a public-sector wage reform that eliminates the wage premium for all types of public-sector workers. Chassamboulli and Gomes (2019) study the effects of wage policies in the public sector on unemployment and education decisions. Geromichalos and Kospentaris (2020) study how meritocratic government hiring can have unintended negative consequences on macroeconomic aggregates in the long-run. Navarro and Tejada (2021) study the interaction between public-sector employment and the minimum wage. Garibaldi et al. (2021) analyze the relationship between public sector wage gap and overqualification and the higher concentration of more educated workers in the public sector.====Finally, we relate to the literature studying the unintended negative impact of public employment focusing on Brazil as an important case study. Glomm et al. (2009), for instance, evaluate how the public sector pensions affect output and welfare. Bettoni and Santos (2021) study how job security in the public sector affects household savings behavior and the aggregate performance of the economy in the long-run. Cavalcanti and Santos (2021) analyze how public sector wages affect aggregate productivity.====We contribute to the existing literature by studying the role of on-the-job human capital accumulation as a channel though which public sector employment distorts vacancy creation and affects labor market volatility. In addition, most of the literature has focused on the effects of the public sector wage premium, while the focus of our paper is on the implications of a government that predominantly hires skilled and more experience workers.====The remainder of the paper is structured as follows. In order to motivate our study Section 2 presents some empirical evidence. In Section 3 we present the model economy, followed by the calibration and the quantitative analysis in Section 4. Finally, Section 5 contains concluding remarks.",Public sector employment and aggregate fluctuations,https://www.sciencedirect.com/science/article/pii/S0164070422000209,5 March 2022,2022,Research Article,71.0
"Futagami Koichi,Sunaga Miho","Department of Economics, Doshisha University, Karasuma Imadegawa, Kamigyo-ku, Kyoto, 602-8580, Japan,Osaka School of International Public Policy, Osaka University, Machikaneyama, Toyonaka, Osaka, 560-0043, Japan","Received 24 November 2021, Revised 21 February 2022, Accepted 21 February 2022, Available online 5 March 2022, Version of Record 8 March 2022.",https://doi.org/10.1016/j.jmacro.2022.103415,Cited by (0),"We analyze how increasing longevity affects economic development based on differences in the risk attitudes of young and old individuals. We construct an overlapping generations model given an economy grows with the help of the capital and ==== produced by individual activities. The outcomes of these activities are stochastically determined. We analytically and numerically show that increasing longevity hinders capital accumulation in the economy when old individuals are more risk-averse than young individuals. Thus, if old individuals are less willing to take risks in the economy, population aging will consequently slow economic growth.","The United Nations Population Division, 2019, United Nations Population Division, 2020 states that the number of people aged 65 years and older is projected to more than double by 2050. Increasing longevity is a key driver of population aging, and most countries have experienced substantial increases in life expectancy since 1950. In particular, Japan, Italy, Germany, and Korea face significant challenges from aging. Although these facts and forecasts could threaten sustainable economic growth, the extent to which the behavior of young and old individuals affects this issue remains underexplored. We bridge this gap in the literature in this study by focusing on the difference between the risk attitudes of young and old individuals to analyze how increasing longevity affects the economy.====Capital accumulation is important for economic growth. However, the activities that promote capital accumulation are associated with various risks and individuals’ responses to those risks change throughout their lives. Moreover, the risk attitudes of individuals in a country experiencing increasing longevity differ from those in a vigorous economy comprising many young individuals. For example, Rolison et al. (2013), Schurer (2015), and Dohmen et al. (2017) show that old individuals are more risk-averse than young ones.==== These empirical results imply that aging populations face changes in society’s response to risk.====To examine the relationships among increasing longevity, the risk attitudes of the young and old, and economic growth, we construct an overlapping generations model of an economy that grows by capital accumulation in which individuals’ production activities are accompanied by risks. In this model, final goods are produced by capital and intermediate goods. The outcomes of intermediate goods production are determined stochastically. Both young and old individuals produce intermediate goods that are accompanied by risks. Individuals can allocate resources to consumption spare before they obtain the return on these goods. Further, we consider an economy in which old individuals are supposed to be more risk-averse than young individuals. The model shows that old individuals increase their consumption spare as they become more risk-averse. We examine how the risk attitudes of old individuals affect capital accumulation in an economy considering increasing longevity.====The main results are as follows. The effects of increasing longevity on an economy depend on the degree of the risk aversion of old individuals. We numerically and analytically show that increasing longevity hinders capital accumulation in an economy in which old individuals are much more risk-averse than young individuals. The intuition behind these results is as follows. In this model, the outcomes of the production of intermediate goods and thus their revenue are stochastically determined. When old individuals are significantly more risk-averse than young individuals, they allocate most of their resources to consumption spare, which they can obtain without any risk, whereas young individuals increase the inputs required for the production of intermediate goods associated with risks instead of saving. Therefore, capital stock per capita in such economies decreases due to old individuals’ consumption spare. This result implies that the risk-averse behavior of old individuals hinders capital accumulation in an economy in which they are significantly more risk-averse than young individuals.====Early studies of increasing longevity, capital accumulation, and economic growth include Pecchenino and Pollard (1997) and Futagami and Nakajima (2001).==== Using endogenous growth models, they show that increasing longevity raises the savings rate and promotes economic growth.==== Recently, several studies such as Acemoglu and Johnson (2007) have shown the negative effects of increasing longevity on economic growth.==== Hansen and Lønstrup (2015) show that some developed countries have experienced the negative effects of increasing longevity on GDP growth. Bloom et al. (2011) suggest that OECD countries are likely to see modest declines in the rate of economic growth with population aging during 2005–2050. Maestas et al. (2016) use U.S. data over 1980–2010 and show that annual growth in GDP slows as the older population increases. Using a growth theory with the endogenous replacement of physical capital accumulation by human capital accumulation, Minamimura and Yasui (2019) show that life expectancy in some countries lowers human capital investment and economic growth in line with the empirical results of Acemoglu and Johnson (2007). Our study also shows the negative effects of increasing longevity on economic growth; however, we focus on the risk-averse attitude of old individuals. Thus, our study sheds new light on the relationship between increasing longevity and economic growth. Cervellati and Sunde (2011) show a non-monotonic or negative relationship between life expectancy and growth in per capita income in countries before the onset of the demographic transition, whereas there are positive effects thereafter. Lee and Shin (2019) also show that some countries have experienced a negative effect of increasing life expectancy on economic growth, while others have experienced positive effects. Kuhn and Prettner (2018) theoretically and empirically show that population aging, including increasing longevity, decreases the growth rate of consumption. Thus, consensus on how increasing longevity affects capital accumulation and economic growth is lacking. Our study contributes to addressing this gap in the literature by showing that increasing longevity reduces capital accumulation in an economy in which old individuals are much more risk-averse than young ones. The results of this study shed light on the problem that aging societies face: Can they accumulate capital as much as a vigorous economy with many young individuals can==== Our results indicate that aging societies can accumulate capital if old individuals take more risks.====The remainder of this paper is organized as follows. Section 2 presents the basic structure of the proposed model. Section 3 describes its dynamic system. Section 4 describes the steady states of the model and analyzes how increasing longevity affects the economy in the steady state. This section also presents the numerical analyses of the comparative statics with respect to the steady state. Concluding remarks are presented in Section 5.",Risk aversion and longevity in an overlapping generations model,https://www.sciencedirect.com/science/article/pii/S0164070422000179,5 March 2022,2022,Research Article,72.0
"Alba Carlos,McKnight Stephen","Dirección General de Investigación Económica, Banco de México, Calle 5 de Mayo No. 18, Colonia Centro, Mexico City 06069 Mexico,Centro de Estudios Económicos, El Colegio de México, Carretera Picacho Ajusco No. 20, Colonia Ampliación Fuentes del Pedregal, Mexico City 14110, Mexico","Received 3 August 2021, Revised 8 February 2022, Accepted 17 February 2022, Available online 26 February 2022, Version of Record 6 March 2022.",https://doi.org/10.1016/j.jmacro.2022.103411,Cited by (0),". While Chile can maximally increase tax revenues by 11% with higher labor taxes, Brazil can increase tax revenues by 6% by cutting labor taxes. The budgetary gains under capital income taxation are found to be even smaller, where the fiscal space does not exceed 4% in any country. Using ","A central issue in public finance is how much government revenue can be raised from changes in taxation. According to the Laffer curve, which postulates an inverted U-shaped relationship between tax revenues and tax rates, the fiscal space available depends on how far the country is from the peak.==== A recent literature has used dynamic general equilibrium models to characterize quantitatively the Laffer curves for several developed countries.==== However, little attention has been given in the literature to understanding Laffer curves in emerging countries. This paper aims to fill this gap. While emerging market economies face a number of diverse challenges in raising tax revenues, one important factor is the presence of a large informal economy in these countries (La Porta and Shleifer, 2014).==== In Latin America, for example, the informal economy accounts for nearly 40% of the region’s GDP (see Table 1). The goal of this paper is to characterize the Laffer curves for the largest Latin American economies and quantify how informality affects the fiscal space of each country.====We consider a two-sector neoclassical growth model that consists of both a formal and an informal sector. Both sectors are (initially) assumed to be competitive that use labor and sector-specific capital to produce goods. Consistent with the empirical evidence for Latin America, formal firms are assumed to have relatively higher productivity levels. Households choose how much labor to supply to each sector, where labor is assumed to flow freely between the two sectors.==== Households can work in the formal sector and pay income taxes, or work in the informal sector and avoid taxes, but risk being audited and fined by the government. The government provides a public good and generates revenue from taxing labor and capital income in the formal sector and from levying a tax on formal goods consumption. Income from informal activities and the consumption of informal goods are not taxed. The model is calibrated using data for five major Latin American economies: Brazil, Chile, Colombia, Mexico, and Peru that all have significant informal economies (see Table 1).==== Average (effective) tax rates are calculated using the methodology of Mendoza et al. (1994). As in Trabandt and Uhlig (2011), the Laffer curves for labor, consumption, and capital income taxation are computed along the balanced growth path by varying the average tax rate. The sensitivity of the revenue-maximizing tax rates to variations in key model parameters is explored.====Our main findings are summarized as follows. Informality significantly changes the level and location of the peak of the labor tax Laffer curve for all countries. In response to an increase in the labor tax rate, the tax base of the economy shrinks as agents work more in the informal sector, shifting the peak of the labor tax Laffer curve (in terms of maximal tax revenues) downwards and to the left. The extent of the reduction in the revenue-maximizing labor tax rate crucially depends on the elasticity of substitution between formal and informal goods. As formal and informal goods become more substitutable, the more labor will be allocated to informal activities, reducing the additional revenue that can be raised from labor taxes. For values of the elasticity of substitution commonly used in the emerging market business cycle literature, we find that the tax revenue implications of informality is severe for all five countries. Our results suggest that the fiscal space is largest in Chile, where tax revenues would increase by 11.3%, whereas the budgetary gains of raising labor taxes for Peru (1.7%), Colombia (2.5%), and Mexico (4%) would be small. For Brazil, the labor tax rate is found to be higher than the peak of the Laffer curve, such that tax revenue would actually improve by 6% from cutting labor taxes. These results are in stark contrast to fiscal space estimates obtained in the absence of informality, where four out of the five countries can achieve additional tax revenues of more than 100% by setting the revenue-maximizing labor tax rate.====Similarly, we also find the consumption tax Laffer curves to be single peaked with an inverted U-shaped relationship between tax revenues and the tax rate. Consumption taxation generates a substitution effect towards the untaxed informal goods, reducing the tax base of the economy. This is in stark contrast to the formal-only model, where the tax revenue benefits always outweigh the marginal distortion of higher taxes, resulting in consumption tax Laffer curves that are strictly increasing without a peak. Analogous to labor income taxation, the consumption tax Laffer curves are found to be highly sensitive to the degree of elasticity of substitution between formal and informal goods. For low values of this elasticity, Brazil (24.3%), Peru (39.9%), and Colombia (64%) could generate significant additional tax revenue from reaching the Laffer curve peak. However, the fiscal space shrinks to less than 1% in all three countries under a commonly-used higher elasticity.====For capital income taxation, we find the capital tax Laffer curve is flat for Brazil suggesting a low elasticity for capital tax revenue. Consequently, tax revenue would only increase in Brazil by an additional 4.8% by reaching the Laffer curve peak in the formal-only version of the model and an additional 0.6% with informality. For the remaining countries, informality can induce sizeable crowding-out effects on the capital tax base, shifting the peak of the capital tax Laffer curve downwards and to the left. For example, for low values of the elasticity of substitution between formal and informal goods, the additional revenue gains from reaching the peak are reduced by 56% in Colombia and 64% in Peru compared to the estimates obtained in the absence of informality. Under a commonly-used higher elasticity of substitution, the fiscal space does not exceed 4% in any country. In addition, we find that all five countries are located on the slippery side of the peak of the Laffer hills with respect to capital taxes. Our results suggest that in order to maximize total tax receipts, Latin American countries should in general raise both labor and consumption taxes, and reduce capital income taxes.====Finally, we consider the implications of market power in the formal sector for the Laffer curve estimates.==== We find that the capital tax Laffer curves associated with monopolistic competition are higher than their perfectly-competitive counterparts. The capital tax base is larger under market power due to the increase in (taxable) profits of formal firms, widening the fiscal space available. A similar finding is found for the consumption tax Laffer curves, where market power reduces the substitution effect towards the untaxed informal goods, thereby increasing the tax base of the economy relative to the perfect competition baseline. In contrast, monopolistic competition reduces the labor tax Laffer curves resulting in a small reduction in fiscal space.====This paper is related to a recent literature that investigates Laffer curves using dynamic general equilibrium models. Trabandt and Uhlig (2011) characterize Laffer curves for labor and capital income taxation using a neoclassical growth model and find that the U.S. could increase tax revenues considerably more than the EU-14, by raising labor taxes and lowering capital taxes. Nutahara (2015) performs a similar exercise for Japan. Lozano and Arias (2021) estimate Laffer curves for several Latin American countries employing a human capital growth model and conclude that the fiscal space available from raising labor and capital taxes is potentially large. However, their analysis importantly abstracts from informality. Auray et al. (2016) investigate how the structure of international financial markets affects the Laffer curves of several European countries. Feve et al. (2018) consider how allowing for liquidity-constrained agents can affect the shape of the Laffer curve, whereas Guner et al. (2016) and Holter et al. (2019) examine how the degree of tax progressivity can affect the labor tax Laffer curve for the U.S. This paper complements the existing literature by investigating the role informality plays in the shape of the Laffer curves for emerging market economies.====The remainder of the paper is organized as follows. Section 2 outlines the model economy and discusses the calibration of the model. Section 3 presents the main results and sensitivity analysis. Section 4 considers the role of monopolistic competition and Section 5 briefly concludes. An online supplementary Appendix reports additional results and robustness exercises.",Laffer curves in emerging market economies: The role of informality,https://www.sciencedirect.com/science/article/pii/S0164070422000143,26 February 2022,2022,Research Article,73.0
"Inaba Masaru,Nutahara Kengo,Shirai Daichi","Kansai University, 3-3-35 Yamate-cho, Suita, Osaka 564-8680, Japan,The Canon Institute for Global Studies, 1-5-1 Marunouchi, Chiyoda-ku, Tokyo 100-6511, Japan,Senshu University, 2-1-1 Higashimita Tama-ku, Kawasaki, Kanagawa 214-8580, Japan,Tohoku Gakuin University, 1-3-1 Tsuchitoi, Aoba-ku, Sendai, Miyagi 980-8511, Japan","Received 7 September 2021, Revised 24 January 2022, Accepted 7 February 2022, Available online 25 February 2022, Version of Record 3 March 2022.",https://doi.org/10.1016/j.jmacro.2022.103402,Cited by (1),"The literature has empirically shown that the labor wedge worsens during recessions. Taking this statement into consideration, this study poses two questions: First, what is the main driving force of the labor wedge, and second, is the main driver of the labor wedge the same as that of business cycles? In this study, we employ a commonly used medium-scale ","It is well-known and empirically confirmed that the labor wedge, defined as the gap between the marginal rate of substitution and the marginal productivity of labor, worsens during recessions, as was the cause during the 1930s Great Depression and 2007–2009 Great Recession. Indeed, researchers have deeply investigated the labor wedge as an important variable to understand business cycle fluctuations.====The main research questions of this study are as follows. (i) What is the main driving force of the labor wedge in a commonly used medium-scale dynamic stochastic general equilibrium (DSGE) economy? (ii) Is the main driver of the labor wedge the same as that of business cycles?==== Recent literature on the medium-scale DSGE model shows that such a model can account for the salient aspects of business cycle fluctuations. In the literature, the labor wedge is often treated as an exogenous variable, and there exist few studies on the source of the fluctuations of the labor wedge as an endogenous variable.==== If the labor wedge is an important variable to understand business cycles, then the main driving force of the labor wedge should be the same as that of business cycles. Thus, using Japanese data, we analyze which structural shocks drive the fluctuation of the labor wedge and business cycles using a commonly used medium-scale DSGE model, which has many nominal and real frictions as well as structural shocks.====One of special features of this study is the estimation strategy. In the standard Bayesian estimation of DSGE models, the prior distribution of the parameters for the standard deviations of the shocks is the inverse gamma distribution. This does not support zero value, and, therefore, the existence of structural shocks is assumed. Imposing a non-existent structural shock can have a serious consequence on inference, in that the estimated driving source of the business cycles and the labor wedge might be unreliable. To overcome this limitation, we employ a more flexible prior distribution of the parameters for the standard deviations of shocks and measurement errors to allow for the non-existence of shocks and measurement errors. Following Ferroni et al., 2015, Ferroni et al., 2019, we employ a normal distribution.====Under the standard prior distribution of the parameters for the standard deviations of structural shocks, our estimation results imply that the labor wedge is mainly driven by preference and transitory technology shocks, whereas the investment adjustment cost shock is the most important for business cycle fluctuations. Meanwhile, under our relaxed prior distribution, which allows for the non-existence of shocks, the estimation results show that both the labor wedge and the business cycles are mainly driven by permanent technology and investment adjustment cost shocks.====As a result, if we employ an inverse gamma distribution – which is standard in the literature, but imposes the existence of structural shocks – then the investigation of the source of the labor wedge fluctuation might not be promising to understand business cycles. This is because the source of the labor wedge is different from that of business cycles. However, by using our more relaxed prior – the normal distribution – to allow for the non-existence of structural shocks, the investigation of the labor wedge would be more promising for understanding business cycles, that is, both the labor wedge and business cycles are driven by the same structural shocks.",What drives fluctuations of labor wedge and business cycles? Evidence from Japan,https://www.sciencedirect.com/science/article/pii/S0164070422000064,25 February 2022,2022,Research Article,74.0
"Blotevogel Robert,Imamoglu Eslem,Moriyama Kenji,Sarr Babacar","Fiscal Affairs Department, IMF, 700 19, Street NW, Washington, DC 20431, USA,Chief Economist Department, European Stability Mechanism (ESM), 6a Circuit de la Foire Internationale, L-1347 Luxembourg","Received 1 October 2021, Revised 17 February 2022, Accepted 18 February 2022, Available online 24 February 2022, Version of Record 23 March 2022.",https://doi.org/10.1016/j.jmacro.2022.103413,Cited by (2)," Gini and Top10 income shares. Finally, we show that the distinction between short- and long-run effects of inequality also becomes empirically less relevant when we allow for measurement uncertainty. We do not find a unique and stable structural relationship between inequality and the transmission channels.","This paper makes the case that the measurement of income inequality has a first-order impact on its estimated empirical relationships. We specifically explore the empirical association between different measures of income inequality and the growth transmission channels. In theory, income inequality can affect economic growth through the transmission channels of human capital, fertility, capital services (the input of capital into production), total factor productivity (TFP), and political stability. Much of the existing literature on inequality and economic growth has focused on the direct reduced-form relationship between inequality and growth, without differentiating between different inequality measures and transmission channels. To the best of our knowledge, we are the first to explore the interplay between growth transmission channels and inequality measures. Our bottom-line finding echoes Atkinson and Brandolini (2001) in their analysis of secondary income inequality statistics: “the choice of data matters.”====We use three distinct measures of two widely used income inequality indicators: the Gini coefficient and the Top10 income share. Each measure depends on its own unique set of technical measurement choices. We do not take a stand on the most appropriate measurement methods, as this judgement will depend on the specific empirical question at hand and the underlying data constraints (more below). Instead, the sheer possibility of measuring incomes (and income inequality) in different ways gives rise to measurement uncertainty.====The empirical pattern between inequality and the transmission channels is acutely sensitive to the specific inequality measure—different inequality measures, even if based on the same theoretical concept (Gini or Top10 income share), exhibit considerably different empirical relationships with the transmission channels. Empirical statements about the effects of inequality are therefore best thought of as being contingent on a particular inequality measure—relationships found to be statistically significant with one measure tend not to replicate with another. Moreover, a single inequality measure can exhibit opposite empirical relationships with two or more of the transmission channels simultaneously, complicating efforts to aggregate individual effects into a net effect on economic growth. Finally, we show that the ==== variation of inequality measures (due the measurement choices that make one Gini coefficient different from the next) is more important than the ==== variation (due to the conceptual difference between the Gini and the Top10 income share).====Measurement has posed a long-standing challenge to inequality statistics. Until recently, the focus has been on the lack of consistency in secondary income inequality statistics: the “apples and oranges” problem. Early vintages of cross-country inequality datasets were not internally consistent, which created problems of cross-country comparability and led to fragile empirical results (Anand and Segal, 2008; Atkinson and Brandolini, 2001; and 2009; Deininger and Squire, 1998; Fields, 1994; Knowles, 2005). Researchers have since made big strides in producing consistently measured inequality statistics with broad country-year coverage (Alvaredo et al., 2016; LIS, 2018; Solt, 2020).====But measurement challenges do not stop with cross-country comparability. Even in a single country like the United States, measuring income is complicated. Several important components of people's income do not show up in surveys or administrative tax data, and the incidence of taxes and transfers has to be estimated. Technical assumptions are therefore part and parcel in the task of measuring income inequality. For too long, these assumptions have operated silently in the background, despite exerting a first-order influence on the empirical behavior of inequality measures Auten and Splinter (2019)., for example, show that the increase in the top 1 percent income share in the United States since the 1980s varies by a factor or two (or more) under different assumptions. In the same vein, Splinter (2022) shows that the fraction of the increase in annual inequality attributable to labor mobility varies between zero and 75 percent depending solely on measurement choices.====We show that concerns about the far-reaching effects of measurement choices, which so far have been discussed mainly in the context of the top 1 percent income shares in the United States (see Atkinson et al., 2011; Auten and Splinter, 2019; Kopczuk, 2019; Piketty et al., 2018; Piketty and Zucman, 2003; Saez and Zucman, 2019; Smith et al., 2021), apply more broadly.====Measurement uncertainty and the associated sensitivity in empirical relationships casts new light on the large, and inconclusive, literature that analyzes inequality and its effect on economic growth. The range of results in this literature is wide and includes: (i) positive effects (Forbes, 2000; Li and Zou 1998); (ii) negative effects (Alesina and Perotti, 1996; Alesina and Rodrik, 1994; Berg et al., 2018; Dabla-Norris et al., 2015; Deininger and Squire, 1998; Galor and Zeira, 1993; Persson and Tabellini, 1994; Perotti, 1996); (iii) heterogenous effects (Barro, 2000; Brueckner and Lederman, 2018; Grigoli et al., 2016; Grigoli and Robles, 2017; Halter et al., 2014; Knowles, 2005); (iv) uncertain effects (Banerjee and Duflo, 2003; Ferreira et al., 2018; Marrero and Servén, 2021; Neves et al., 2016; Voitchovsky, 2005 and (Voitchovsky, 2009)), and (v) no effects (Ravallion, 2012). Similarly, a sub-strand of the literature zooms in on the specific economic mechanisms that link inequality to growth (see Neves and Silva, 2014, for a review), highlighting the role of: (i) fertility and schooling (Berg et al., 2018; Deininger and Squire, 1998; De la Croix and Doepke, 2003; Gründler and Scheuermeyer, 2018); (ii) distortive taxation (Alesina and Rodrik, 1994); (iii) socio-political stability (Alesina and Perotti, 1996; Keefer and Knack, 2002; Perotti, 1996); (iv) savings (Knowles, 2005), and institutional development (Glaeser et al., 2003; Castells-Quintana and Royuela, 2017).====One neglected reason for the wide dispersion in published results is measurement uncertainty. The typical empirical result of the literature uses a single measure of inequality to make statements about the causal effect of inequality. We show that relying on a single measure only is risky, as empirical results are highly contingent on the specific measure of inequality used. The quest to understand the economic consequences of inequality need to be clear-eyed about the challenges posed by measurement uncertainty. At this stage, it seems injudicious to be categorical about the effect of income inequality.====We use three different empirical methods, to mitigate concerns that our results depend on a specific estimation technique: event studies, weighted-average least square regressions (WALS), and Pooled-Mean Group (PMG) regressions. As a secondary contribution of our paper, we select these methods to address neglected problem in the empirical inequality literature. But we are not wedded to a strong causal interpretation of our results, and instead highlight throughout our paper the empirical sensitivities and the contingent nature of the inequality-growth nexus.====The rest of the paper flows as follows. The next section describes our data on income inequality and the transmission channels. The third section presents the three different estimation methods we employ to study the relationship between inequality and the growth transmission channels. The results are in Section 4, along with several robustness checks. The final section concludes and offers thoughts for improving our understanding of the economic effects of income inequality.",Income inequality measures and economic growth channels,https://www.sciencedirect.com/science/article/pii/S0164070422000167,24 February 2022,2022,Research Article,75.0
"Germaschewski Yin,Wang Shu-Ling","Department of Economics, University of New Hampshire, Durham, NH 03824, United States of America,Department of Economics and Management, Gustavus Adolphus College, Saint Peter, MN 56082, United States of America","Received 2 June 2021, Revised 8 January 2022, Accepted 15 January 2022, Available online 5 February 2022, Version of Record 9 February 2022.",https://doi.org/10.1016/j.jmacro.2022.103398,Cited by (0),"Fiscal stabilization without monetary autonomy can be challenging, especially in high-debt economies. This paper studies the welfare outcomes of six fiscal stabilization rules in Greece, a highly indebted country in a monetary union. We introduce rich fiscal policy instruments into a ==== ==== and estimate it using ","The COVID-19 pandemic has prompted many countries to take unprecedented steps to stimulate their economies and minimize citizens welfare losses. In 2020, the US economy launched its largest fiscal stimulus of over $2.6 trillion, and more stimulus packages are expected in the future. The European Union (EU) also issued an $857 billion stimulus package to rescue its economies from the ravages of the pandemic. The fiscal positions of these governments have rapidly deteriorated. In high-debt economies, the pandemic further exacerbates the already strained fiscal situation. Falling tax revenues and rising government spending widen the fiscal imbalance, increasing the need for rolling over debt and resulting in heavier interest burdens because these countries typically face large interest rate spreads tied to their fiscal positions. For countries that belong to a monetary union, the role of monetary authority is diminished, and fiscal policy must ensure the stability of public sector debt while improving citizens’ welfare.====Numerous studies seek to quantify the effects of fiscal policy on output growth and debt reduction, but it is unclear how these policies affect agents’ welfare. In particular, fiscal stimulus can worsen welfare if higher output requires a large increase in labor supply (Drautzburg and Uhlig, 2015). Fiscal consolidation, on the other hand, could trigger a recession and thereby lower welfare if a lower debt-to-GDP ratio is achieved through large cuts in productive public spending or lump-sum transfers (Cavallo et al., 2018, Fatás and Summers, 2018, Forni and Pisani, 2018). What is the welfare-maximizing fiscal policy that the government in high-debt economies could employ to stabilize debt?====To address this question, this paper compares the welfare effects of various fiscal stabilization rules in which fiscal policy responds to the level of public sector debt and the state of the economy in a high-debt economy without monetary independence. A rich set of fiscal instruments is introduced into a small open economy dynamic stochastic general equilibrium (DSGE) model, including public investment, employment, and consumption, as well as taxes on consumption, labor income, and capital earnings. We apply our model to Greece and estimate it using Bayesian methods for the period 1999Q1 – 2020Q4. Bayesian estimation is a powerful tool for fitting the DSGE model to the data, making our analysis empirically relevant. A second-order grid-based search method is used to find the fiscal rules that are implementable and welfare-maximizing. This grid-based technique allows us to simultaneously search for high-dimensional optimal policy coefficients, which is particularly useful for studying welfare effects.====The model economy is a small open economy that belongs to a currency union, and the nominal exchange rate is set exogenously. We apply a small open economy model to Greece as opposed to a two-country setup as in Erceg and Lindé (2013), since Greece’s fiscal policy does not have sizable spillover effects on large economies, as shown in Stähler and Thomas (2012). Therefore, a small open economy framework is better suited for studying the effects of fiscal policy on the domestic economy. There is a debt-elastic interest rate premium on public sector debt (García-Cicco et al., 2010). The model features a large presence of the public sector, as in Greece. The government not only purchases consumption goods, but also invests in physical capital and hires labor to produce public services that enhance households’ utility and infrastructure that boosts firms’ productivity. We clearly distinguish between productive and unproductive government spending, with respect to which Ardagna (2004) finds that the composition of public spending programs is important for welfare analysis. The model economy has two types of agents: savers and hand-to-mouth households. Savers have access to international asset markets and physical capital, while hand-to-mouth households do not borrow and save. This feature, together with nominal price rigidity in the goods sector and wage stickiness in the labor market, allows the model to better match the data moments.==== In addition, this heterogeneity among agents enables us to quantify the distributional effects of fiscal stabilization policies.====Four key results emerge from our quantitative analysis. First, the estimated fiscal policy rules suggest a lack of fiscal stabilization during the sample period. Fiscal policy shocks play a minor role in determining aggregate volatility, as evidenced by historical decomposition. Shocks that are unrelated to fiscal policies are the main contributors to Greek’s rising debt, such as country premium shocks, productivity shocks, and foreign interest rate shocks.====Second, all fiscal stabilization policies improve welfare regardless of spending or tax-based rules. This increase in welfare mainly stems from the policy’s influence on agents’ expectations. The stabilization of the debt-to-GDP ratio incentivizes forward-looking agents to consume more, since taxes will be lower and public spending will be higher in the future, giving rise to a positive wealth effect that strongly boosts welfare.====Third, the optimal (i.e., welfare-maximizing) stabilization policy features a simultaneous adjustment of all spending and taxes. For instance, in response to negative productivity shocks, the optimal policy is to expand public consumption and employment and cut consumption and capital tax rates to boost the economy, while temporarily reducing public investment and raising the labor tax rate to stabilize debt. This policy mix dominates individual policy rules in welfare terms and produces the largest welfare gains for the economy through two channels. First, a large increase in public consumption boosts aggregate demand, generating a positive income effect that enhances welfare. This effect is particularly large in an economy with a sizable fraction of hand-to-mouth households as shown in Albonico et al. (2019).==== In addition, a cut in the consumption tax rate further promotes households’ motives to consume. Second, lowering public investment spending and raising the labor tax rate in the short run reduces the need for future tax hikes, promoting consumption and leisure. This policy mix also strongly stabilizes debt.====Lastly, there are sizable distributional effects across savers and hand-to-mouth households. The welfare gains for savers are generally greater than those for hand-to-mouth households. This is because fiscal stabilization of public debt reduces the interest rate premium in international asset markets, lowering the cost of borrowing. Savers borrow more to invest in productive activities, boosting their welfare. In addition, fiscal adjustment strongly stabilizes debt, making future tax hikes or spending cuts less likely. Even though both agents benefit from this positive expectational effect, hand-to-mouth households do not factor in the lower future taxes or higher public spending associated with current fiscal contraction. Consequently, their welfare gains are smaller.====Three exercises are conducted to examine the fiscal policy responses to debt and the state of the economy. We first split our full sample into three sub-sample periods and find that fiscal policy responds weakly to rising debt before and during the Greek debt crisis, further supporting our baseline results. We then compare fiscal policy responses in an economy without COVID-19 crisis to our baseline economy. Our results show that fiscal policy rules are less stimulative and the uncertainty associated with these rules is much smaller without COVID-19 crisis. We also show that a stronger government commitment to debt stabilization further strengthens the welfare gains of the fiscal stabilization policy.====This paper contributes to the large strand of literature on the effects of fiscal policies in three dimensions. First, existing studies mainly focus on fiscal stimulus or austerity measures; few evaluate fiscal stabilization policy from a welfare perspective (e.g. Forni et al., 2009, Romer and Romer, 2010, Davig and Leeper, 2011, Coenen et al., 2013, Erceg and Lindé, 2013, Ilzetzki et al., 2013, Alesina et al., 2015, Albonico et al., 2016, Leeper et al., 2017, Camous and Gimber, 2018, Wang, 2021).==== For example, on the stimulus side, Albonico et al. (2016) find that both public consumption and public transfers have large multiplier effects, especially in the Euro area countries with a large fraction of hand-to-mouth households. Wang (2021) studies the stimulus effects of various fiscal instruments and finds that public consumption produces short-lived stimulus effects, while public investment generates long-lasting stimulus outcomes for Greece. On the austerity side, Stähler and Thomas (2012) show that fiscal consolidation through cuts in public sector wages is less damaging to output and employment, but lowering public investment can have detrimental effects on the economy. However, as we illustrated earlier, it is unclear from these studies that fiscal stimulus or consolidation necessarily improves citizens’ welfare; this depends on which policy instruments are used and the transmission mechanism through which fiscal policy affects the economy. Our analysis fills this gap by studying a wide variety of policy tools and using Bayesian estimation to quantify the welfare effects of each policy.====Second, most studies assess the effects of fiscal policies in countries with monetary independence (e.g. Zubairy, 2014, Bi et al., 2016, Alloza et al., 2019, Roettger, 2019, Bianchi et al., 2020, Galí, 2020 and Germaschewski (2020)). Governments in these economies can use inflation and seigniorage revenues generated by printing money to reduce the actual value of nominal debt payments. This paper complements existing research by carefully evaluating the welfare outcomes of stabilization policies without an independent monetary policy.====Third, there is no consensus among existing studies on the transmission mechanism through which fiscal policy affects the economy. One strand of studies concludes that fiscal policy affects the economy through the labor supply channel, as fiscal stabilization reallocates workers from the public sector to private firms, boosting private production. Another strand shows that the main transmission channel of fiscal policy is its influence on agents’ expectations about future policy stance. Both channels can have different effects on agents’ welfare, with the former potentially lowering welfare through a large increase in labor supply and the latter improving welfare. Our study contributes to this debate by examining the transmission mechanism of fiscal policy in a highly indebted country. We find that, in the case of Greece, fiscal policy is mainly transmitted via the expectation channel, as supported by the sizable increase in welfare produced by debt stabilization. The reason for this is that in a high-debt economy, the current policy stance has a strong effect on agents’ anticipation of future policy trajectory, generating a feedback loop that affects agents’ current consumption and investment decisions.====The rest of the paper is organized as follows. Section 2 outlines the model. Bayesian prior and posterior analysis of the parameter values and equilibrium solutions are discussed in Section 3. Section 4 presents the main results. Section 5 conducts three counterfactual experiments. Section 6 concludes.",Fiscal stabilization in high-debt economies without monetary independence,https://www.sciencedirect.com/science/article/pii/S0164070422000027,5 February 2022,2022,Research Article,76.0
"Chu Angus C.,Furukawa Yuichi,Wang Xilin","Department of Economics, University of Macau, Macau, China,Faculty of Economics, Aichi University, Nagoya, Japan,China Center for Economic Studies, School of Economics, Fudan University, Shanghai, China","Received 21 May 2021, Revised 12 January 2022, Accepted 17 January 2022, Available online 5 February 2022, Version of Record 8 February 2022.",https://doi.org/10.1016/j.jmacro.2022.103399,Cited by (1),"This study explores how the rent-seeking behavior of the government may impede economic development and delay industrialization. Introducing a rent-seeking government to a Schumpeterian growth model that features endogenous takeoff, we find that a more self-interested government engages in more rent-seeking taxation, which delays the economy’s transition from pre-industrial stagnation to modern economic growth. Quantitatively, a completely self-interested government could have delayed industrialization, relative to a benevolent government, by about two centuries in the UK.","The Industrial Revolution is arguably one of the most important events in the history of economic development. As a result of industrialization, the average annual growth rate of real GDP per capita in Britain accelerated from 0.4% in the 18th century to 1.0% in the 19th century and reached 1.7% in the 20th century.==== An early study by DeLong and Shleifer (1993) documents evidence that the rent-seeking behavior of ruling elites can impede economic development and delay industrialization. Allen (2011, p. 15) also argues that “economic success is the result of secure property rights, low taxes, and minimal government. Arbitrary government is bad for growth because it leads to high taxes [...] and rent-seeking”.====To provide a growth-theoretic analysis on this issue, we introduce a rent-seeking government to a recent variant of the Schumpeterian growth model that features endogenous takeoff. We find that a self-interested government that is subject to weaker constitutional restrictions engages in more rent-seeking taxation,==== which delays the transition of the economy from pre-industrial stagnation to modern economic growth. This result captures the idea in the influential work of Acemoglu and Robinson (2012) on extractive political institutions stifling economic development. Furthermore, our growth-theoretic framework enables us to perform a quantitative analysis, which shows that a completely self-interested government could have delayed industrialization, relative to a benevolent government, by about two centuries in the UK.====The intuition of our results can be explained as follows. Rent-seeking taxation imposed by the government creates a distortion that shrinks the level of output in the economy and the market size, which in turn reduces incentives for the entry of firms. Therefore, rent-seeking taxation delays the endogenous takeoff of the economy and stifles economic growth in the short run. However, the reduced entry of new firms eventually increases the size of incumbent firms, which gives rise to a positive effect on quality improvement and economic growth. In the long run, the positive and negative effects cancel each other rendering a neutral effect of the tax rate on the steady-state growth rate. These results show that rent-seeking taxation could have a severe impact on the takeoff of an economy even when its effect on long-run growth is neutral, highlighting the importance of considering the effects on the long-run transition of the economy from stagnation to growth.====This study relates to the literature on growth and innovation. Seminal studies by Romer (1990), Segerstrom et al. (1990), Grossman and Helpman (1991) and Aghion and Howitt (1992) develop the R&D-based growth model in which either the development of new goods or the quality improvement of goods drives innovation in the economy. Subsequent studies by Peretto (1994) and Smulders (1994) combine the development of new goods and the quality improvement of goods to develop the Schumpeterian growth model with endogenous market structure.==== An advantage of the Schumpeterian growth model with endogenous market structure is that its implications are supported by empirical evidence.==== A number of studies, such as Peretto, 2003, Peretto, 2007, Peretto, 2011 and Ferraro et al. (2020), use the Schumpeterian growth model with endogenous market structure to explore the effects of tax policies on innovation-driven growth. This study builds on this literature by using a Schumpeterian growth model with endogenous market structure to explore how rent-seeking taxation affects the endogenous takeoff of an economy and its transition from stagnation to growth.====This study also builds on the literature on endogenous takeoff, in which the seminal study by Galor and Weil (2000) develops unified growth theory; see also Galor and Moav (2002), Galor and Mountford (2008) and Galor et al. (2009).==== Unified growth theory explores how an economy transits from a pre-industrial Malthusian trap to modern economic growth; see Galor, 2005, Galor, 2011 for a comprehensive review of this literature. This study also considers an economy’s endogenous transition from stagnation to growth but in a Schumpeterian model in which the endogenous activations of two dimensions of technological progress (i.e., the development of new goods and the quality improvement of goods) determine the takeoff.==== Therefore, this study contributes to a recent branch of this literature on endogenous takeoff in the Schumpeterian growth model developed in Peretto (2015) by deriving the entire transition dynamics of the economy and quantifying the effect of rent-seeking taxation on its takeoff; see also Iacopetta and Peretto (2021) on corporate governance, Chu et al. (2020a) on status-seeking culture, Chu et al. (2020b) on intellectual property rights, and Chu et al. (2020c) on agricultural technology.",Rent-seeking government and endogenous takeoff in a Schumpeterian economy,https://www.sciencedirect.com/science/article/pii/S0164070422000039,5 February 2022,2022,Research Article,77.0
"Gai Prasanna,Tong Eric","University of Auckland, New Zealand,Reserve Bank of New Zealand, New Zealand","Received 27 September 2021, Revised 29 January 2022, Accepted 1 February 2022, Available online 2 February 2022, Version of Record 7 February 2022.",https://doi.org/10.1016/j.jmacro.2022.103401,Cited by (3),"We examine the international impact of the information content in US monetary policy. When the Federal Reserve tightens monetary policy, it reveals not only a tightened monetary stance, but also optimism about the economy, raising global output and asset prices as a result. Using a panel dataset of fifty-eight countries over the period 1994–2020, we disentangle “information shocks” from pure monetary shocks and show that (i) while tightening shocks are contractionary for the global economy, the information inherent in a tightening announcement can have expansionary effects; (ii) monetary and information shocks tend to offset each other, softening the net ==== to the world economy; and (iii) the information inherent in monetary easing lowers output and asset prices. US monetary announcements can thus reveal optimism or pessimism with consequences for global economic activity.","Does a tightening announcement by the Federal Reserve have a contractionary or expansionary effect on other economies? While the majority of the literature points to the former (Dedola et al., 2017; Georgiadis 2016; Vicondoa 2019), recent findings suggest that Federal Reserve announcements contain an “information effect” that can be expansionary.==== When a decision to tighten monetary policy is announced, it conveys not only the monetary stance, but also strong economic fundamentals in the US (Nakamura and Steinsson 2018).==== This paper explores how this information effect spills over internationally.====Our paper makes use of state-of-the-art monetary policy shocks identified by Bu et al. (2021) to extract the “information shocks” embedded in the announcements of the Federal Open Market Committee (FOMC), and then propagates these shocks in panel local projections (LPs) to a sample of 58 countries over the period 1994–2020. We reason that, since the change in the US yields on the FOMC days is driven by a combination of monetary policy shocks, Federal Reserve information shocks and white noise, information shocks can be extracted by subtracting the pure monetary shocks identified by Bu et al. (2021) from the change in the US yields on the FOMC days—an approach similar to Albagli et al. (2019). To mitigate the residual noise embedded in the measurement, we combine a model-based bootstrapping procedure developed by Focarelli (2005) with economic reasoning to illustrate why our estimations are conservative. Following Nakamura and Steinsson (2018), we verify that the extracted information shocks are positively associated with surveys of expected output growth globally. We conduct a series of checks to ensure our results are consistent across different specifications.====Having established that information shocks increase expectations of future global activity, we proceed to document the benchmark result of the paper—pure monetary shocks and information shocks generate opposite impulse response functions (IRFs). While a tightening in US monetary shocks leads to a higher unemployment rate, lower industrial production and lower asset prices around the world, information shocks deliver the opposite—raising production, long-term government bond yields, stock prices, and lowering the unemployment rate.====Since the monetary and information effects are offsetting, we conclude that the net spillovers of US monetary policy are smaller than what the previous literature reports.==== Our conclusion that the global effects of US monetary policy are broadly neutral echoes Bernanke (2016), albeit for a different reason. Bernanke (2016) suggests that US monetary policy is neutral because the expenditure-switching and expenditure-augmenting effects offset each other.==== In this paper, the offset comes from the opposing effects of monetary and information shocks.====Our paper also contributes to the “trilemma” and “dilemma” debate. Rey (2018) and Miranda-Agrippino and Rey (2020) suggest that global financial conditions are dominated by a global factor underpinned by US monetary policy, such that floating and fixed exchange rate regimes are equally susceptible to the influence of the US. This hypothesis contrasts with the seminal work of Mundell (1963) and Fleming (1962), which holds that floating exchange rate regimes can exercise monetary autonomy to achieve macroeconomic stability (Obstfeld et al., 2019). To examine this issue, we split the sample into two groups: one with relatively stable exchange rates against the US dollar and another with predominantly floating exchange rates. When we re-estimate the regression for each subgroup, we find evidence that may offer a synthesis of the two perspectives. When monetary and information shocks are analysed in isolation, fixed exchange rate regimes exhibit more volatile IRFs than their counterparts, supporting the trilemma hypothesis. However, jointly analysing the shocks, we find that the net effect of US monetary policy is similar between the two regimes, lending support to the notion of a dilemma.====We repeat the state-dependant analysis by splitting the sample along multiple exogenous dimensions to assess the multifaceted nature of the information effect. We find supportive evidence of the financial channel of transmission of Federal Reserve information shocks. Following a tightening information shock, countries that are net US dollar creditors experience stronger output growth than dollar debtors. Potentially, the revelation of a stronger US economy anticipates higher US interest rates (10-year yield increases in the benchmark result) and a lower probability of default. These results complement Kearns et al. (2020), who establish a financial channel of US monetary policy spillovers.====We document two circumstances under which the nature of information changes significantly in the Federal Reserve announcements. First, the positive information effect disappears following the global financial crisis (GFC). Post-GFC, information in tightening Fed announcements turns contractionary and decreases output and asset prices. Instead of interpreting tightening announcements positively, it may be that investors and economic agents perceive the tightening of Federal Reserve as premature and, hence, contractionary. Second, when we separate episodes into easing and tightening rounds we find that, on average, information embedded in an easing of US monetary policy is contractionary. The information content seems to reverse in an easing shock: just as a surprise policy tightening may reveal strong fundamentals, a surprise policy easing may reveal pessimism that weighs adversely on economic activity in the US and globally.====The remainder of the paper is structured as follows. Section 2 describes the econometric specification, including the identification of US information shocks, and the data. In Section 3, we quantify and contrast the effects of US monetary shocks with information shocks on real and financial variables internationally, and document the nature of information in the FOMC announcements under different circumstances. Section 4 presents robustness checks. Section 5 concludes.",Information spillovers of US monetary policy,https://www.sciencedirect.com/science/article/pii/S0164070422000052,2 February 2022,2022,Research Article,78.0
"Davis Colin,Hashimoto Ken-ichi,Tabata Ken","The Institute for the Liberal Arts, Doshisha University, Japan,Graduate School of Economics, Kobe University, Japan,School of Economics, Kwansei Gakuin University, Japan","Received 31 August 2021, Revised 6 December 2021, Accepted 29 December 2021, Available online 14 January 2022, Version of Record 19 January 2022.",https://doi.org/10.1016/j.jmacro.2021.103396,Cited by (0),This paper considers how increasing longevity and declining birth rates affect market entry and endogenous productivity growth in a two-country model of ,"Over the last half century, a demographic transition characterized by rising longevity and falling fertility rates has lead to a drastic decline in the per-capita labor forces of most developed countries. As an example, Fig. 1 shows the UN projections for life expectancy at birth, the crude birth rate (births per 1000 people), and the resulting decrease in the working-age population share in the US and Western Europe (WE) between 2000 and 2050. Importantly, the decline in labor force participation has the potential to slow economic growth through contractions in both per capital labor forces and aggregate savings (Bloom et al., 2010). A less well understood implication of population aging for economic growth, however, is the shift in industry location patterns that follows the transition in the geographic distribution of demand (Sato and Yamamoto, 2005, Grafeneder-Weissteiner and Prettner, 2013). Indeed, there is now a well-developed literature stressing the importance of industry location patterns for innovation-based economic growth (Baldwin and Martin, 2004). But, further research is needed to understand how demographic transition affects long-run growth through adjustments in industry location patterns.====To this end, we develop an endogenous market structure and endogenous growth framework (Smulders and van de Klundert, 1995, Peretto, 1996, Aghion and Howitt, 1998, Laincz and Peretto, 2006, Peretto, 2018) to consider how population aging affects economic growth and market entry by inducing shifts in the geographic location of industry. In particular, we adapt the two-country model of Davis and Hashimoto (2015) to include individuals with heterogeneous ages, following Dinopoulos and Segerstrom (1999). National labor supply depends endogenously on the working-age share of the population, directly tying adjustments in market size with the dynamics of demographic transition. Monopolistically competitive firms assume a central role in the model, employing labor in the production of differentiated product varieties and in process innovation aimed at the reduction of future production costs. Each firm’s labor productivity in process innovation depends on a weighted average of the productivities of observable technologies, with a greater weighting applied to domestically employed technologies to capture the imperfect nature of international knowledge diffusion.====National rates of productivity growth converge in the long run, equalizing knowledge spillovers and firm-level employment in process innovation across countries. With common innovation costs, the country with the larger labor supply (i.e., the larger country) attracts a greater share of firms, each with a higher productivity level than the firms located in the country with the smaller labor supply (i.e., the smaller country). Accordingly, the model links industry location patterns with market entry and aggregate productivity growth; that is, an increase in the concentration of more productive firms in the larger country improves knowledge spillovers and raises firm-level employment in process innovation, thereby generating a faster rate of productivity growth. The increase in employment in innovation, however, raises firm-level costs inducing exit and reducing the level of market entry.====We use the framework to study how demographic transition affects market entry and productivity growth. First, population aging in the smaller country reduces its labor supply, and thus its market size, leading to an increase in the concentration of industry in the larger country. As a result, the rate of productivity growth accelerates. In addition, the contraction of the smaller country’s labor supply and the increased scale of employment in process innovation combine to ensure that the level of market entry falls. Second, population aging in the larger country causes a contraction in its market size that lowers the level of industry concentration and slows the rate of productivity growth. The level of market entry may rise or fall, however, depending on whether the negative effect of a smaller labor supply, or the positive effect of lower firm-level employment in process innovation dominates.====Retirement age extension is often suggested as a policy for mitigating the effects of population aging. We consider the implications of raising the retirement age proportionately with increases in life expectancy at birth. First, retirement age extension in the smaller country expands its labor force, reducing the concentration of industry in the larger country, and slowing productivity growth. With a larger labor force and lower firm-level employment in process innovation, the level of market entry rises. Second, retirement age extension in the larger country increases its labor force, raising the concentration of industry, and accelerating productivity growth. Once again, however, the level of market entry may rise or fall, depending on the balance of the positive effect of a greater labor supply and the negative effect of higher firm-level employment in process innovation.====To analyze the welfare effects of demographic transition, we complete a numerical analysis of our model using the UN projections for life expectancy at birth in the US and Western Europe (WE) from 2000 to 2050. In our analysis, a relatively large labor force supports a greater share of firms with higher productivity levels in the US. The UN projections suggest that life expectancy will rise at similar rates in each region, implying that the impacts of declining working-age population shares in the US and WE counterbalance. The result is a rather stable relative labor supply over the period of analysis, and accordingly, the US share of firms and the international productivity differential only exhibit small upward adjustments, resulting in a negligible increase in the rate of productivity growth. In contrast, as firm-level employment in innovation is for the most part constant, the direct effect of the decrease in the working-age population shares of the US and WE is a lower level of market entry. Consequently, our numerical analysis projects that the decline in labor income associated with a contraction in the working-age population share, and the reduction in the level of market entry, becomes the key driver of falling US and WE welfare levels.====We then introduce retirement age extension as a means of reversing the negative welfare effects of population aging.==== In particular, we consider a policy exercise where retirement age is raised proportionately with increases in expected lifespan in each region. Under such a policy setting, the comparable increases in the UN projections for the expected lifespans of the US and WE translate into similar patterns for retirement age extension in each region, with the resulting increases in the labor supplies counterbalancing another. As such, the overall impacts on the relative labor supply, the US share of firms, the international productivity differential, and the productivity growth rate are negligible. Our numerical analysis therefore suggests that a policy of retirement age extension would generate welfare improvements in the US and WE between 2000 and 2050 by raising labor income through an expansion of the working-age share of the population, and increasing the level of market entry.====The paper is organized as follows. In Section 2, we review the related literature. Section 3 introduces our single-sector model of trade and endogenous productivity growth. Section 4 provides a characterization of long-run equilibrium, and Section 5 investigates how changes in demographic structure and retirement age affect market entry and long-run productivity growth. The welfare implications of population aging and retirement age extension are considered in Section 6 through a numerical analysis. We conclude the paper in Section 7.","Demographic structure, knowledge diffusion, and endogenous productivity growth",https://www.sciencedirect.com/science/article/pii/S0164070421000926,14 January 2022,2022,Research Article,79.0
Marcolino Marcos,"Potsdam Institute for Climate Impact Research, Telegrafenberg A56, Potsdam, 14473, Germany","Received 30 January 2021, Revised 5 October 2021, Accepted 28 December 2021, Available online 13 January 2022, Version of Record 1 February 2022.",https://doi.org/10.1016/j.jmacro.2021.103394,Cited by (1),"I account for the sources of labor reallocation from the manufacturing sector towards services in the United States for the 1950 to 2010 period. I use a multi-sector model with sector-specific productivity growth and non-homothetic preferences to decompose the sources of labor reallocation into supply-side, demand-side, and wedge distortions. The decomposition is performed in the context of a competitive economy where the competitive equilibrium with wedges reproduces prices and quantities of the economy exactly. During the 1950–2010 period, the demand-side mechanism accounts for 57% of the reallocation of labor and the supply-side for 28%. Focusing only in the sub-period from 1950 to 1980, 70% of the reallocation is demand-driven. In the sub-period between 1980 and 2010, the three sources of labor reallocation are quantitatively important. Demand-side accounts for 47%, supply-side for 42% and wedge distortions for 10%.","Structural transformation is defined as the reallocation of economic activity and labor across broad sectors of the economy – here, manufacturing and services – that accompanies the process of economic growth. During the 1950–2010 period, the U.S. economy exhibited a persistent decline of the share of workers in the manufacturing sector and a persistent increase of the share of workers in the services sector.==== This structural transformation pattern is also observed in other economic variables such as the value-added shares, the consumption shares, and the expenditure shares.==== The economic literature has focused on supply-side and demand-side explanations of the structural transformation process. Supply-side explanations focus on the effects of unequal productivity growth between the sectors. Demand-side explanations focus on the changes of consumption patterns of individuals as income grows.==== As the individuals become richer, they spend a lower share of income on manufacturing goods and a larger share on services. Despite the extensive literature on the topic, there is still no consensus on the forces that drive the process.====In this paper, I use an accounting methodology to measure the importance of the different drivers of structural transformation in the U.S. economy during the 1950–2010 period. In addition to supply-side and demand-side drivers, I also consider that structural transformation may be driven by frictions in the economy. The analysis builds on the wedge accounting methodology of Chari et al. (2007), and I apply their insights to a standard structural transformation model with wedge distortions in both quantities and prices. The accounting exercise is performed in the context of a competitive economy where the competitive equilibrium with wedges reproduces prices and quantities of the U.S. economy exactly.====The analysis starts with a decomposition of the sectoral labor shares. The decomposition quantifies the labor required to produce goods used for demand purposes: consumption, investment, government spending, and net exports.==== For example, I calculate the share of labor necessary to produce manufacturing goods destined for final consumption. The decomposition establishes the existence of structural transformation within consumption, investment, and government spending. Putting it differently, the share of labor in manufacturing that produces goods used for final consumption has decreased over time, whereas the share in services has increased. This same pattern is also observed for investment and government spending. This decomposition allows the magnitude of the supply-side and the demand-side mechanisms to differ according to the final use of the good; i.e., if the good is used for consumption or investment. Throughout the paper, I call the share of labor necessary to produce goods used for consumption of consumption labor share and the share to produce investment of investment labor share.====This paper has two main findings. First, the three sources of labor reallocation – supply-side, demand-side, and wedge distortions – are quantitatively important. The demand-side effects account for 57% of the reallocation of labor during the 1950–2010 period. The supply-side effects account for 28% and the wedge distortions for 14%. The importance of each source of labor reallocation depends on the sub-period of analysis, though. Motivated by the relative productivity of manufacturing to services, I divide the 1950–2010 period into two sub-periods: 1950–1980 and 1980–2010. Between 1950 and 1980, manufacturing productivity relative to services displays a hump-shape,==== followed by a persistent increase afterward. In the first sub-period, the reallocation is mainly demand-side driven, accounting for 70%. The supply-side effects and wedge distortions account for 11% and 19%, respectively. In the second sub-period, the three sources of labor reallocation are quantitatively important. The demand-side effects are still the primary source of reallocation, accounting for 47%. The supply-side effects account for 42% and the wedge distortions for 10%. Second, the sources of labor reallocation differ between the consumption and the investment labor shares. The demand-side mechanism accounts for 45% of the reallocation of the consumption labor share, while it accounts for 40% of the investment labor share. On the other hand, the supply-side mechanism accounts for 45% of the reallocation of the investment labor share while it accounts for 35% of the consumption labor share.====The supply-side mechanism reflects the slow-growing productivity observed in the services sector and the small elasticity of substitution between manufacturing and services (complements). As the price of services increases, reflecting its slow productivity growth, the expenditure share of services increases, and labor moves towards services as a way to meet demand.==== The demand-side mechanism reflects the larger income elasticity of services. Following an increase in income, demand for services increases more than the demand for manufacturing goods, and labor moves towards services to satisfy demand.====The decline in U.S. manufacturing employment==== is a prominent topic of public debate. However, the appropriate policies to slow down or reverse this – supposedly detrimental – process depend crucially on the main driving forces. The supply-side mechanism indicates that labor reallocates towards services as a consequence of its slow-growing productivity. This phenomenon, known in the literature as Baumol’s cost disease of services,==== is of particular concern since resources are continuously reallocated towards the sector with little technological progress, slowing down aggregate GDP growth. On the other hand, the demand-side mechanism indicates that the decline reflects changes in consumption patterns, and implementing policies that readjust demand may actually reduce welfare.====I use a standard structural transformation model whose household preferences are non-homothetic. Non-homotheticity implies that changes in income lead to changes in the expenditure shares even if relative prices are constant.==== Household preferences are represented by the non-homothetic constant elasticity of substitution (CES), which dates back to Hanoch (1975) and Sato (1975), and was introduced in the structural transformation literature by Comin et al. (2021) (henceforth, CLM). This utility function is particularly suitable in the context of this paper because it completely separates the effects of higher income from the effects of relative price changes on household demand. This property delivers equilibrium conditions for which the different sources of labor reallocation are additive and independent of each other, a property key to the decomposition exercise.==== Moreover, I assume that investment production is also represented by a non-homothetic CES, with similar properties. As a result, the model provides a straightforward methodology to account for the labor reallocation sources and identify distortions that affect the structural transformation process.====The wedge accounting procedure essentially assumes an economic structure and uses the data together with the equilibrium conditions of the model to infer the distortions existent in the economy. In the model, these wedge distortions look like taxes, and I consider five different wedges: labor wedge, capital wedge, consumption wedge, investment demand wedge, and savings wedge.====The labor wedge accounts for frictions in the demand for labor faced by the firms of different sectors. This wedge is calculated as the ratio of the value of the marginal product of labor in the services sector relative to the value of the marginal product of labor in manufacturing. In a competitive equilibrium where labor can move freely across sectors, the value of the marginal product of labor should be equal in both sectors. I find that the labor demand wedge is lower than one during the entire 1950–2010 period.====The capital wedge follows a similar logic. It accounts for frictions on capital demand faced by the firms of the different sectors. It is calculated as the ratio of the value of the marginal product of capital in the services sector relative to the manufacturing and, under free mobility of capital, the value of the marginal product of capital should be the same in both sectors. The capital demand wedge is also lower than one throughout the entire 1950–2010 period. Even though the capital demand wedge has been lower than one, its value is increasing. In other words, the capital wedge distortions have decreased through time.====The consumption wedge accounts for restrictions on household demand for manufacturing and services goods. It reflects frictions of the goods market that limit household’s ability to equalize the marginal rate of substitution between manufacturing and services to their relative prices. In the model, it is calculated as a tax on the relative price of the goods. Despite its erratic behavior, the consumption wedge has a quantitatively small overall impact. The investment wedge is similar to the consumption wedge, accounting for restrictions in demand for manufacturing and services goods to produce capital. Its behavior is also erratic, with minimal impacts. The savings wedge accounts for distortions in the household’s inter-temporal decision, and it reflects frictions that limit the ability of the household to smooth consumption through time. This wedge is small throughout the entire 1950–2010 period, suggesting that frictions in the financial markets did not play an important role in the structural transformation of the U.S. economy.====This paper is related to an extensive economic literature on structural transformation. Baumol (1967) and Ngai and Pissarides (2007) focus exclusively on the supply-side mechanism characterized by different rates of sectoral productivity growth and their impact on relative prices and labor allocation. Another branch of the literature focuses on the changes of households’ expenditure patterns in income grows, such as Echevarria (1997) and Kongsamut et al. (2001). Nevertheless, most of the literature considers both supply-side and demand-side drivers on their frameworks, such as Duarte and Restuccia, 2010, Herrendorf et al., 2014 and Alder et al. (2022), to cite a few.====In particular, it contributes to the structural transformation literature that aims to quantify the sources of labor reallocation. Several papers, such as Buera and Kaboski, 2009, Dennis and Işcan, 2009, Alvarez-Cuadrado and Poschke, 2011, Herrendorf et al., 2013, and others, rely on preferences that are of the Stone-Geary form. The non-homothetic effects are generated by exogenous terms on the utility function representing subsistence requirements or consumption endowment. These preferences display only transitory income effects that impact the effects of price changes.==== Because of the vanishing income effects, the supply-side effects are the predominant driver of labor reallocation in these papers. Another influential reference, Boppart (2014), uses the Price-Independent Generalized Linear preferences whose income and price effects also interact with each other. Boppart finds that supply-side and demand-side sources contribute roughly the same to structural transformation.====CLM approaches the quantitative importance of each mechanism and, for a pool of countries, they find that about 75% of the reallocation of labor is demand-driven. Despite using the same utility representation, the accounting methodology followed in this paper is rather different from theirs. The authors infer the sources of labor reallocation by comparing the predicted equilibrium labor shares from the model to the data using econometric techniques. More specifically, the fit of the model is when it includes a different subset of regressors (capturing a different subset of reallocation mechanisms) in the estimation. In contrast, I derive optimal conditions for which the three mechanisms reproduce the observed labor reallocation (by construction) and account for the magnitude of each mechanism without relying on counterfactual simulations.====The works of Herrendorf et al. (2021) and García-Santana et al. (2021) provide theoretical and empirical analyzes of the change in the sectoral composition of consumption and investment on the sectoral shares of value-added. In this paper, instead of analyzing the impacts on value-added, I focus on how the sectoral composition of demand impacts the sectoral allocation of labor, including the importance of non-homothetic demand for the composition of investment.====This paper also relates to the literature that applies the wedge accounting method following the seminal work of Chari et al. (2007). The accounting methodology has been widely applied to different economic episodes and extended beyond the one-sector growth model. In the context of structural transformation, Cheremukhin et al., 2016, Cheremukhin et al., 2017 apply the wedge accounting methodology to study the Russian economy during the 1885–1940 period and the Chinese economy from 1953 to 1978. In both works, the economies were still in the early stages of economic growth and were influenced by large-scale economic policy programs. They focus on the reallocation of labor away from the agricultural sector towards the non-agricultural. Unlike them, I study a developed economy in the late stages of labor reallocation, moving from manufacturing towards services. Another close work is Świȩcki (2017), which investigates the determinants of structural transformation for 45 countries over the 1970–2005 period, and finds that the decline of manufacturing labor share and the corresponding growth in services in developed countries is mainly supply-side driven. Non-homotheticity is more relevant for economies in the early stages of economic development.==== The results stand for the particular case of the U.S., where sector-biased technological change explains most of the reallocation of labor and non-homothetic effects are much less important, which contrasts with the results I obtain. Moreover, both papers find a smaller role for wedge distortions.====An outline of the paper follows. In the next section, I describe the data. In Section 3, I describe the model and discuss household’s preferences. Section 4 characterizes the equilibrium with wedges of the model. In Section 5, I discuss the selection of the parameters of the model. Section 6 discusses the accounting results. Section 7 goes through the wedge distortions. Section 8 concludes.",Accounting for structural transformation in the U.S.,https://www.sciencedirect.com/science/article/pii/S0164070421000914,13 January 2022,2022,Research Article,80.0
"Cohen Gail,Jalles Joao Tovar,Loungani Prakash,Pizzuto Pietro","University of Lisbon, Portugal","Received 7 April 2021, Revised 18 December 2021, Accepted 5 January 2022, Available online 7 January 2022, Version of Record 12 January 2022.",https://doi.org/10.1016/j.jmacro.2022.103397,Cited by (2),"This paper provides cross-country evidence on the relationship between growth in CO==== emissions and real GDP growth from 1960 to 2018. The focus is on distinguishing longer-run trends in this relationship from short-run cyclical fluctuations, and on documenting changes in these relationships over time. Using two filtering techniques for separating trend and cycle, we find that long-run trends show evidence of decoupling in richer nations—particularly in European countries—but not yet in developing economies, and that there is stronger evidence of decoupling over the 1990 to 2018 sub-period than over the earlier 1960 to 1989 sub-period. There is also a strong cyclical relationship between emissions and real GDP growth in both advanced and developing economies, and the strength of this relationship has not weakened much over time. The cyclical relationship is largely symmetric: emissions fall about as much during recessions as they rise during booms. The transition to a low-carbon economy will thus require continued progress not only in bringing down trend emissions, particularly in developing economies, but also in taming the increase in emissions that occurs during the boom phase of the business cycle.","The steady rise in global average surface temperature and the severity of climate shocks—ranging from heatwaves and droughts to hurricanes and coastal flooding—have raised the urgency of national and international policy actions to accelerate the transition to a low-carbon economy (IPCC, 2018; OECD/IEA, 2018). Encouragingly, 196 countries signed the Paris Agreement in 2016 committing to domestic policy actions to limit the increase in global average temperature to 2 °C within this century (Peters et al., 2017). The United Nations 2030 Agenda for Sustainable Development also sets out several ambitious goals for environmental sustainability (TWI2050, 2018).====A key indicator of progress toward these goals is the extent of decoupling between the growth in greenhouse gas (GHG) or CO==== emissions and growth in economic activity, typically measured by real GDP growth. This decoupling can be relative or absolute: “GDP growth coinciding with absolute reductions in emissions or resource use is denoted as “absolute decoupling”, as opposed to “relative decoupling”, where resource use or emissions increase less so than does GDP” (Haberl et al., 2020; UNEP, 2011).====This paper provides estimates of the extent of decoupling between CO==== emissions and real GDP for a large group of countries—178 countries spanning the advanced, emerging market economies, and low-income country groups—for the period 1960–2018. The novelty of our work is to distinguish between trend and cyclical fluctuations in both emissions and output and to use this distinction to (i) provide estimates of both trend (or longer-run) decoupling and cyclical (or shorter-run) decoupling; and (ii) provide evidence on how this relationship has changed over time.====Our paper has two important sets of findings. First, the underlying trends reveal relative decoupling between emissions and real GDP for some of the richer nations, particularly in Europe, but not yet in developing countries; in a few countries, we encouragingly find evidence of absolute decoupling. Among the twenty major emitting countries in the world, there is an increase in the extent of relative decoupling in all but two cases.====At the same time, the cyclical relationship between emissions and real GDP is very strong for most countries across all income groups. The cyclical relationship is also symmetric: almost everywhere, emissions rise as much during booms as they fall during busts. Moreover, in contrast to the evidence on trend decoupling, the cyclical relationship has become weaker in only half the cases among the twenty major emitters.====These results are robust to two ways of decomposing trend and cycle, the popular HP filter (Hodrick and Prescott, 1981) as well as the newer filter suggested by Hamilton (2018). The results also hold up for a longer period starting in 1948; specifically, when comparing the post-WWII period (1948–1982) with the post-1983 period, we find that the trend elasticity has halved while the cyclical elasticity has nearly doubled (these results are available upon request from the authors). Progress towards a low-carbon economy thus requires progress on both further lowering the trend elasticity of emissions and taming the cyclical elasticity of emissions during booms.====Section 2 presents a review of the literature on decoupling and delineates the contribution of our paper. Section 3 lays out our econometric framework and describes the data sources used. Section 4 presents baseline estimates of both the trend and cyclical elasticities, using alternatively the top 20 emitting countries and selected countries from the “Global South” to illustrate the results and to show that they are robust to several checks. Section 5 summarizes the evidence on trend and cyclical elasticities for 178 countries, focusing on differences across country groups and also presents evidence on how these elasticities differ between booms and busts. The last section draws out the policy implications of our findings. An annex presents detailed country-by-country results.",Trends and cycles in CO,https://www.sciencedirect.com/science/article/pii/S0164070422000015,7 January 2022,2022,Research Article,81.0
Fegatelli Paolo,"Banque centrale du Luxembourg, Luxembourg.","Received 13 April 2021, Revised 14 December 2021, Accepted 16 December 2021, Available online 5 January 2022, Version of Record 18 January 2022.",https://doi.org/10.1016/j.jmacro.2021.103392,Cited by (8),"A major obstacle for the implementation of a general-purpose central bank digital currency (CBDC) is the risk of bank disintermediation, potentially jeopardizing financial stability and the ==== of monetary transmission. By adapting the theoretical framework of Dutkowsky and VanHoose (2018b, 2020) to the euro area, this study investigates and clarifies the conditions under which a digital euro could be introduced on a large scale without leading to bank disintermediation or a credit crunch. First, the central bank would require proper mechanisms to manage the volume and the user cost of CBDC in circulation. Second, the central bank should continue to facilitate access to its long-term lending facilities, to provide banks with a funding source alternative to client deposits at an equivalent cost. Depending on its design, a digital euro could improve bank profitability and competitiveness by absorbing large amounts of idle (and expensive) excess reserves without penalizing lending, while incentivizing bank digitalization.","The Covid-19 pandemic accelerated the process of digitalization, in particular for retail digital payments. As public concerns arose about virus transmission through cash use, consumers in many countries stepped up their use of contactless cards and other forms of digital payments, also related to expanding e-commerce activities (BIS, 2020a; Auer et al., 2020). This shift has revived the debate on access to different means of payments, and on the importance of resilience against a broad range of threats, leading to renewed interest in general-purpose central bank digital currencies (CBDCs).====At the Eurosystem level, in July 2021 the Governing Council of the European Central Bank (ECB) launched a two-year investigation phase of a digital euro project. A digital euro, i.e., a euro CBDC, would be a central bank liability offered in digital form to citizens and businesses for their retail payments, complementing the existence of cash and wholesale central bank deposits. The Eurosystem has identified a number of reasons for supplying a digital euro, ranging from the provision of a safe form of digital money (as opposed to risky cryptocurrencies and other similar forms of private money) to an alternative to foreign payment providers in Europe to a contingency solution if cash were to decline significantly or non-EU digital money were to largely displace payments in euro. By complementing private payment solutions, a digital euro could also stimulate retail payments innovation, while supporting financial inclusion. In addition, it could help reduce the overall costs and ecological footprint of the monetary and payment systems, and support the general economic policies of the European Union (ECB, 2020).====However, the costs and benefits of retail CBDCs should be carefully pondered. Allowing private individuals and non-bank institutions to directly access central bank balance-sheets could seriously destabilize the two-tier banking system. If central banks allowed private individuals and firms to exchange substantial parts of their bank deposits for retail CBDCs, this might facilitate bank runs. Moreover, commercial banks could be deprived of an important source of cheap funding, inducing them to reduce their lending and shrink their balance sheets, with negative repercussions on economic activity and output, unless the central bank itself takes on their role.==== Such an outcome would run counter to monetary policy since the Great Financial Crisis, which has consistently aimed to encourage banks’ credit provision to the real sector.====Nonetheless, the spillover effects of retail CBDC issuance on bank intermediation remains an open issue. A stream of literature on CBDCs does not believe that this should represent an insurmountable obstacle to implementation. Banks could find alternative funding sources in the market, without significant effects on credit granted to the real economy.==== Otherwise, governments or central banks could provide the necessary funding to banks, if and when required, without negative macroeconomic consequences.==== Still, the central bank could adopt direct or indirect methods to manage the volume of CBDC, avoiding excessive deposit losses that could undermine commercial banks’ funding and lending.====The last avenue seems particularly promising. Following this approach, the first objective of this study is to develop a realistic and comprehensive theoretical framework allowing us to explore the impact on banks from introducing a CBDC. In the euro area this task is complicated by the large heterogeneity across bank balance sheets. The results of the analysis are then used to clarify ==== the order of magnitude that retail CBDC issuance could assume before threatening bank lending at an aggregate level, and ==== which accompanying measures could preserve the bank lending channel and avoid financial stability disruptions.====Regarding point ====, Bindseil (2020) and Fegatelli (2019) already described how a central bank could manage the volume of CBDC demanded by the public without imposing a binding limit. Since the attractiveness of the CBDC as an asset would critically depend on the difference between its all-in remuneration and the average rate paid by commercial bank deposits, the central bank could fix the CBDC rate or fees to an appropriate level, roughly matching the desired demand. This could involve a single variable fee on CBDC deposits or a two-tier interest rate system. In addition, other monetary policy tools, such as reserve requirements, could also serve to manage liquidity flows between CBDC and bank deposits.====Setting up suitable mechanisms to steer the volume of CBDC in circulation is important if the central bank wants to ensure consistency with the current monetary policy stance. For example, CBDC issuance should not trigger the absorption of more excess reserves (ER) than those deemed unnecessary for the effectiveness of unconventional monetary policy. On the other hand, our analysis shows that a banking system with large amounts of negatively remunerated ER probably represents the best scenario to issue a CBDC, since retail deposits lost to CBDC will mostly reduce ER receiving a negative rate. A digital euro could then be configured as an additional tool for absorbing large amounts of idle and expensive ER without penalizing bank lending. This is important as euro area excess liquidity nearly doubled during 2020, reaching the unprecedented record of 3.3 trillion euro at year-end,==== with an implicit cost exceeding 13 billion euro per year for the banking sector. Thus, a digital euro could improve bank profitability while generating more homogeneous conditions across banks and countries of the monetary union.====Regarding the measures that could preserve the bank lending channel (point ==== above), the current volume of ER is rather heterogeneous across different types of banking institutions and different euro area countries. This heterogeneity implies that even a limited or ==== CBDC issuance might trigger different effects on bank balance sheets and lending, depending on bank business models and jurisdictions. In particular, problems might arise for illiquid banks that depend heavily on retail deposits for funding. CBDC issuance would then create stress in the weakest parts of the euro area banking system, with all the related implications for systemic risk on aggregate.====This study adapts the theoretical framework in Dutkowsky and VanHoose (2017, 2018a,b, 2020) to explain why banks in some euro area countries accumulated vast ER, while banks in others did not. This analysis then serves to identify monetary policy measures that could mitigate the impact of CBDC on weaker institutions who have limited excess liquidity and rely mainly on deposits for funding.====The next section surveys recent literature on the link between unconventional monetary policies and ER, including related effects on bank profitability and lending. Section 3 provides a theoretical framework to analyze how banks could be affected by the introduction of a retail CBDC competing with deposits. The results of the analysis are then used to identify possible accompanying measures that could preserve the integrity of the bank lending channel. Section 4 illustrates how CBDC introduction would affect the Eurosystem balance sheet, assuming that the central bank retains control on CBDC issuance. Section 5 presents some plausible macro implications for the functioning of the euro area, linked to its heterogeneity. Section 6 summarizes and concludes.",A central bank digital currency in a heterogeneous monetary union: Managing the effects on the bank lending channel,https://www.sciencedirect.com/science/article/pii/S0164070421000902,5 January 2022,2022,Research Article,82.0
"Ahmed M. Iqbal,Farah Quazi Fidia","Texas State University, USA","Received 19 March 2021, Revised 31 October 2021, Accepted 27 November 2021, Available online 31 December 2021, Version of Record 7 January 2022.",https://doi.org/10.1016/j.jmacro.2021.103389,Cited by (1),"Existing literature shows a strong relationship between innovations of information technology (IT) and their falling prices, which result in increased use of computer and information processing equipment, and related demand for investment and labor at the firm level. This paper investigates the macroeconomic effects of news about future IT innovations. Utilizing the maximum forecast error variance approach, we identify IT news shock from the real investment price on computers and related information processing equipment. We provide robust evidence that IT news shocks induce a co-movement in key ","With information technology (IT) as a driving force, rapid technological change that replaces the existing equipment at a faster pace has been the key driver of the US growth since the 1980s ((Jorgenson and Stiroh, 1999, Brynjolfsson and Hitt, 2000, Jorgenson, 2001, Jorgenson et al., 2011) and others).==== An impressive body of quantitative research has already made it clear that shifts in labor demand led by the skilled-biased technological change are due to the most significant and widespread technological change in the IT sector.==== The revolution in IT not only brings changes in occupational structure, productivity, and income, Bresnahan et al. (2002) show a strong relationship between IT innovations, organizational redesign, and new products and services. Most existing literature analyzes the relationships among IT innovations, declining IT prices, increased use of IT, and related demand for investment and labor and their effects on economic growth. This paper introduces how the news of future IT innovations might even affect a reduction in IT equipment prices and its implications to key macroeconomic aggregates.==== Intuitively, as firms anticipate favorable news about future technology, that anticipation coincides with high expectations about earnings growth, so positive news could induce organizational changes.==== Consequently, foresight into future IT is likely to increase investment demand, labor, and output. In particular, we investigate whether news shocks about future IT innovations generate co-movement in key macroeconomic aggregates, namely, output, consumption, hours, and investment. The macroeconomic connections of future IT innovations could be motivated by recent developments in news-driven business cycle literature that suggests that an agent’s foresight about future fundamentals induce positive co-movement among the major macroeconomic aggregates (Beaudry and Portier, 2006, Barsky and Sims, 2011, Jaimovich and Rebelo, 2009).====Since the seminal contribution of Greenwood et al. (1988), much of the investment-specific technology literature suggests that declines in the prices of IT equipment are likely associated with improved technology. For instance, improved integrated chips (ICs) and related innovations contributed to the rapid decline in IT and related equipment prices. Innovations in faster, better, and cheaper memory chips not only contemporaneously reduced the prices of IT equipment, but that reduction may also have contained information on future innovations, passing through media reports. We focus on IT because of the large difference between the rates of decline in the prices of IT equipment and other industrial equipment compared to prices of goods in other sectors.==== We assume that IT is an intermediate good, which could be a strategic complement in the production decisions of capital and consumption goods sectors.==== Although this is a restrictive assumption, we motivate it from the multisector models built on based on the degree of heterogeneity in changes in prices of sectoral goods. The empirical evidence shows significant heterogeneity in average frequencies with which firms in different sectors of the economy change their prices (Bils and Klenow, 2004, Nakamura and Steinsson, 2008). The literature also shows that accounting for this dimension of heterogeneity matters for the dynamic properties of New Keynesian economies (Carvalho and Nechio, 2018, Nakamura and Steinsson, 2010). Our baseline estimation measures IT innovations using the real price of information processing and computer-related equipment obtained from the National Income and Product Account (NIPA). Also, we use the prices of semiconductors and software to substantiate our baseline results.====We use maximum forecast error variance (MFEV) to identify news shocks from the real price of IT. Utilizing Uhlig’s (2003) MFEV approach, Barsky and Sims (2011) provide a nice econometric framework to identify the news shock of future total factor productivity (TFP).==== Relying on this framework, Kurmann and Otrok (2013) show that slope shock of the yield curve contains similar information to the TFP news shock. Later, Ben Zeev and Khan (2015) and Ben Zeev et al. (2017) extended the framework to identify the investment-specific technology news shock and the terms of trade news shock, respectively. This kind of identification requires adding a restriction to the MFEV optimization problem. In our case, we impose a constraint, which is based on two assumptions: (i) only limited numbers of shocks affect IT, i.e., IT’s own shocks that affect IT contemporaneously and news shocks that do not affect IT contemporaneously but rather predict future changes====; (ii) IT news shocks and IT contemporaneous shocks are orthogonal. Beaudry and Portier (2006) identify news shocks from the TFP and stock price data imposing the long-run restriction. Identification through the MFEV approach has advantages over the identification strategy through the long-run restriction. The MFEV approach does not rely on precise assumptions about the stochastic trend in the variable of interest, which is a subject of debate in VAR literature because the number of common stochastic trends among variables of interest can affect inferences about the importance of shocks.==== Also, Francis et al. (2014) show that the MFEV approach is better for finite samples than long-run restrictions because it gives consistent estimates of impulse responses.====Our baseline VAR specification includes six variables: IT, consumption, output, labor hours, and investment in the IT sector, and investment in non-IT sectors. We keep the IT sector and non-IT sector investment separate to understand how the news might affect the dynamics of investment in both sectors. Existing literature shows that investment in the IT sector complements the investment in other sectors (Brynjolfsson and Hitt, 2000, Bresnahan et al., 2002). The paper’s three main findings are: First, our identified IT news shocks derived from the price of information processing and computer-related equipment reflect the co-movement properties of business cycles. That is, on impact, consumption, output, and labor hours rise significantly. Investment in IT and non-IT sectors rises significantly following the news shock. Also, we find that contemporaneous IT shocks fail to generate co-movements in key macroeconomic variables, and their contribution to business cycle fluctuations is small. Second, we find that news shocks explain substantial business cycle fluctuations compared to contemporaneous or surprise shocks. Our identified news shocks explain all six recessions in our sample periods. News about IT innovations contributes more significantly to the forecast error variance of consumption, output, and labor hours. Third, our historical decomposition results indicate that adverse news shocks contributed to falling output in all six recessions. Our findings suggest that news about future IT could be important when firms go through substantial workplace organization changes, which drive up both IT and non-IT-related investment, employment, income, and consumption in aggregates.====Finally, we substantiate our baseline findings and conduct a battery of robustness checks to ensure that our baseline results hold. We conduct a comparative analysis to show the relative importance of hardware and software news components. We conduct the invertibility tests to check if the identification of news in our empirical specification suffers from the invertibility problem. We address the potential technological spillover effects. We find that an unanticipated positive news shock increases the capacity utilization rate and decreases the inflation rate. A positive IT news shock also causes the stock market returns to rise and the nominal interest rate and the spread of interest rates to fall. The increase in output and countercyclical movements in interest rate spread suggests that financial markets help propagate IT news shocks. Additionally, our results are consistent when we consider a subsample that excludes the Great Recession and use an alternative price deflator to measure the real price of IT and alternative lag lengths.====The literature considers TFP, investment-specific technology, and terms of trade, among others, as the key variables of interest in identifying news shocks.==== For example, Beaudry and Portier (2006) find that TFP news shocks generate co-movement among macroeconomic variables and account for more than two-thirds of the US business cycle fluctuations. Barsky and Sims (2011) find that positive TFP news shocks fail to produce co-movement on impact. Most recently, Ben Zeev and Khan (2015) considered identifying news shocks based on investment-specific technology. Ben Zeev et al. (2017) identified news augmented ToT news shock using the trade data. Unlike TFP, the investment-specific technology and terms of trade news shocks are identified using the related price data. We use the IT price data utilizing the idea provided by Greenwood et al. (1988), Greenwood et al. (2000) and Fisher (2006) that the fall in the relative price of IT is directly connected with its future innovations. Ben Zeev and Khan (2015) defined the aggregate measure of investment-specific technology using the investment price data of equipment, software, and durable consumption. Our measure is different from them as we take the investment price index of information processing equipment, including computers and peripheral equipment, which saw a significant decline in prices than other equipment (Nordhaus, 2021). Our identified shocks contain information about information processing equipment, including computer and related equipment’s future innovations, which are exogenous to the economy. It is more likely that innovations of information processing and computer-related equipment and semiconductors drive industrial and transport equipment innovation and consumer durable goods, which are highly dependent on innovations of ICs and semiconductors. An important difference between our findings and those in Ben Zeev and Khan (2015) is that our identified IT news shocks explain all the recessions, including 1981–82. However, news shocks based on the investment-specific technology identified in Ben Zeev and Khan (2015) do not explain the early 1980s’ recession. Moreover, we find that hardware news plays a more significant role in output fluctuations than the news of software components. Our paper is related to a recent development in news-driven business cycle literature; we show that news about IT innovations is important in driving US business cycles and induces a positive co-movement in key macroeconomic aggregates. Ben Zeev (2018) recently examined the macroeconomic effects of information and communication-related news and found that IT news is a major driver of US business cycles. However, our paper is different from that study as we use a different empirical strategy to identify the news shocks.====The rest of this paper is designed as follows. In Section 2, we present our empirical methodology. We discuss the data and illustrate our results in Section 3, followed by robustness in Section 4. Finally, we end to conclude in Section 5.",On the macroeconomic effects of news about innovations of information technology,https://www.sciencedirect.com/science/article/pii/S0164070421000884,31 December 2021,2021,Research Article,83.0
"Nakajima Tomoyuki,Takahashi Shuhei","Faculty of Economics, University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033, Japan,Canon Institute for Global Studies, Japan,Institute of Economic Research, Kyoto University, Yoshida-Honmachi, Sakyo-ku, Kyoto 606-8501, Japan","Received 10 May 2021, Revised 22 November 2021, Accepted 28 November 2021, Available online 17 December 2021, Version of Record 21 December 2021.",https://doi.org/10.1016/j.jmacro.2021.103391,Cited by (1),We show that uninsured idiosyncratic earnings risk dramatically influences the long-run government asset ,"We examine the long-run government asset Laffer curve, which shows the relationship between government asset income and government assets in the long run, under zero taxes.==== Our analysis is based on an Aiyagari (1994)-type model with heterogeneous households and incomplete asset markets. Households face uninsured idiosyncratic earnings risk and an exogenous borrowing limit. We also include endogenous labor supply, following previous works on fiscal policy using this framework.==== Firms produce output by renting capital and labor from households, whereas the government uses government asset income for government consumption and lump-sum transfers to households.====We first conduct a theoretical analysis of how uninsured idiosyncratic risk influences the long-run government asset Laffer curve under zero taxes. We characterize the government asset Laffer curve of the heterogeneous agent model with idiosyncratic risk and compare it with the curve of a representative agent model without such risk. Characterizing the Laffer curve for the heterogeneous agent model is not simple because we cannot generally establish the existence of an equilibrium. To overcome this difficulty, we rely on the seminal work by Zhu (2020), who establishes the existence of a steady-state equilibrium in the Aiyagari (1994) model with endogenous labor supply, but without fiscal policy and government assets. Zhu (2020) introduces supply and demand functions for the capital–labor ratio. The supply function shows how the household-sector choice of the capital–labor ratio varies with the interest rate, whereas the demand function shows how the firm-side choice of the capital–labor ratio changes with the interest rate. Zhu (2020) then reveals that the demand and supply curves intersect, implying the existence of an equilibrium.====We introduce government assets into the Zhu (2020) framework and define the supply and demand functions for the wealth–labor ratio. Due to the presence of government assets, the properties of the demand function in our setting differ from those in Zhu (2020). Nevertheless, we can concretely show that in the absence of distortionary taxes, increasing the government assets–output ratio eventually reduces the interest rate and the government asset income–output ratio to negative values. In contrast, in a representative agent model without such risk, the interest rate is invariant to the government assets–output ratio, which implies that the government asset income–output ratio increases linearly with the government assets–output ratio. Hence, uninsured idiosyncratic earnings risk dramatically changes the shape of the government asset Laffer curve.====We then conduct a numerical analysis on the long-run government asset Laffer curve under zero taxes. There are two objectives: to determine the shape of the Laffer curve for the heterogeneous agent model in more detail than that provided in the theoretical analysis and to quantify the impact of uninsured idiosyncratic earnings risk on government asset income. Calibrating the model to the US economy, we find an inverted U-shaped Laffer curve in the heterogeneous agent model. Specifically, as the government assets–output ratio increases, its asset income–output ratio initially increases, reaches a peak, and then decreases, eventually to negative values. Uninsured idiosyncratic risk also substantially reduces government asset income. Under the baseline calibration, the maximum government asset income–output ratio in the heterogeneous agent model is less than a quarter compared with that in the representative agent model.====The remainder of this paper is organized as follows. In the next part of this section, we summarize the literature. Section 2 explains the model. Section 3 theoretically examines the long-run government asset Laffer curves in the presence and absence of uninsured idiosyncratic earnings risk. Section 4 uses a numerical method to analyze them in more detail and quantify government asset income. Section 5 concludes.",Uninsured idiosyncratic risk and the government asset Laffer curve,https://www.sciencedirect.com/science/article/pii/S0164070421000896,17 December 2021,2021,Research Article,84.0
He Sicheng,"School of Finance, Nankai University, Tianjin, China","Received 1 August 2021, Revised 11 November 2021, Accepted 18 November 2021, Available online 7 December 2021, Version of Record 11 December 2021.",https://doi.org/10.1016/j.jmacro.2021.103388,Cited by (3),"This paper proposes an ==== to study whether innovation activities generate economic cycles. We find that the fishing-out and learning-by-doing effects, which are common properties in innovation activities, generate inverted U-shaped medium-term economic cycles. In this paper, primary and secondary innovations are conducted in two different sectors. The primary innovation sector provides new technologies that are used by the secondary innovation sector to create new blueprints. When a new technology is introduced in the secondary innovation sector, the learning-by-doing effect initially dominates, and the growth rate increases. Subsequently, the fishing-out effect dominates, and the growth rate starts to decrease. When the productivity of the old technology is sufficiently low, the primary innovation sector realizes the profitability of developing new technology. Thus, new technology is developed, and a new cycle begins.","An increasing number of studies establish that economic cycles cannot be separated from economic growth (Francois and Shi, 1999). As innovation drives modern economic growth, it is interesting to study how it can be related to economic cycles. Can innovation activities generate business cycles without any other common factors that cause business cycles? If the answer is affirmative, business cycles cannot be avoided in a growing economy driven by innovation. Herein, we propose a parsimonious endogenous growth model to show that the nature of innovation itself can generate economic cycles. Thus, economic cycles always accompany economic growth due to innovation activities, even without any other common sources of business cycles. However, these cycles are medium-term cycles rather than high-frequency cycles due to the nature of innovation. Our model shows an inverted U-shaped growth curve in every cycle. All the types of innovations in our model are endogenous.====Our model follows Romer’s (Romer, 1987, Romer, 1990) endogenous growth model with expanding varieties. Innovation is the engine of growth in the expanding variety of growth models. Unlike most other models following Romer, we have two innovation sectors: the secondary innovation sector (research and development (R&D) sector) and the primary innovation sector. The primary innovation sector provides technologies that can be utilized by R&D firms, which use certain technologies to create new ideas and develop them into blueprints that can be used by intermediate goods producers. Thus, varieties expand. This description may be somewhat abstract, thus, we use an example to explain it. Imagine that a primary innovation like artificial intelligence (AI) technology has been developed. Subsequently, an R&D firm in the secondary innovation sector uses the AI technology to develop image identification, which is subsequently used to create blueprints for self-driving cars. Separating primary innovation from secondary innovation ensures that the latter relies on the former. Meanwhile, a secondary innovation that relies on a particular primary innovation may not have a close relationship with other primary innovations. For example, image identification cannot directly benefit much from a primary innovation in mechanical manufacturing. Thus, introducing primary and secondary innovations facilitate our understanding of what happens in innovation activities. Notably, we also consider the indirect interactions between inventions using different technologies provided by the primary innovation sector. We comprehensively describe this indirect relationship later in the paper.====In this paper, we assume that there are infinite potential primary innovations. At the same time, there is a “fishing-out” effect observed in the secondary innovation sector when a certain technology is provided by the primary innovation sector. This is a property of innovation that has been introduced by other endogenous growth studies on innovation (Jones, 1995 and Segerstrom, 1995) and has been supported by recent empirical research (Bloom et al., 2020). The productivity of a new technology is initially high. Subsequently, productivity decreases as easy ideas are depleted first. Therefore, the growth rate of providing new blueprints and economic growth is reduced. At this time, switching from the old technology to a newer one is profitable. The primary innovation sector is willing to develop new technology, thus there is a new primary innovation, and a new cycle begins. This is the intuition behind our basic model.====Although the fishing-out effect in innovation activities is sufficient to generate cycles, it is unreasonable to assume that a certain technology reaches its highest productivity right after it is created. For example, the basic concept of blockchain technology had been described since the early 1990s but did not make much progress until Bitcoin’s creation in 2009. Thus, it is reasonable to assume that R&D firms become progressively skilled in a certain technology, as they continue to use it. Thus, we introduce the learning-by-doing effect in the full model. Unlike some famous growth studies that rely on the general learning-by-doing effect, which is an alternative to innovation in explaining economic growth (Romer, 1986 and Lucas, 1988), some recent studies find that the learning-by-doing effect also exists in the R&D sector, and they model it into R&D activities (Auerswald et al., 2000, Lee, 2012). Our model follows this approach: The learning-by-doing effect increases productivity when R&D firms develop new ideas and create new blueprints. We also assume a decreasing return of learning-by-doing, and an increase in productivity as learning-by-doing converges to a limit. Now, we have two effects working in different directions. Initially, learning-by-doing dominates, and thus, the growth rate of the economy increases. Thereafter, the benefit through learning-by-doing diminishes, thus the fishing-out effect dominates, and the growth rate of the economy decreases. The growth and innovation cycles show inverted U-shaped curves after we introduce the learning-by-doing effect.====That economic growth can generate cycles has long been of interest to macroeconomists. (Mitra et al., 2006) Amongst the papers studying how economic growth generates cycles, an important strand of literature pertains to innovation and cycles. Baumol and Wolff (1992) and Deneckere and Judd (1992) are pioneering studies about innovation cycles, despite their models not being endogenous growth models. Helpman and Trajtenberg (1994) build a model whereby general-purpose technology (GPT) can generate innovation cycles in a growth model. However, their introduction of new GPTs is exogenous. Evans et al. (1998) introduce rational expectation into a growth model, and they find that the economy switches stochastically between periods of low and high growth due to multiple equilibria. However, they only consider cycles caused by exogenous stochastic terms. Matsuyama (1999) generates cycles by introducing two growth resources: Solow’s capital accumulation and Romer’s innovation. Cycles are generated because of the dynamics of the model switching between the Solow and Romer regimes. Gardini et al. (2008) and Deng and Khan (2018) followed Matsuyama’s approach. However, the cycles in these papers are generated by switching between innovation-driven growth and capital accumulation-driven growth. They still cannot determine whether innovation activities can directly generate cycles or otherwise. Francois and Shi (1999) generate innovation cycles in a Schumpeterian model. In their model, however, innovations activities only happen in some time. Innovation firms choose a time to develop new technology when the cost of innovation is low. But the cost is low because all innovators start new research at the same time rather than it being a feature of innovation. Boldrin and Levine (2001) find that stock prices may increase when an old type of technology is abandoned.====Some recent papers, though not directly generating cycles in the whole economy, are closely related to our study. Jovanovic (2009) studied life cycles of technologies in an endogenous growth model Gersbach et al. (2018) divided research into two types: basic and applied. They find that basic research plays an important role in economic growth. Compared to previous models, our model is parsimonious and provides a good connection between innovation and cycles. Our model introduces the life cycle of a technology and also includes two types of research. Our paper does not rely on many restrictions on dynamics. Only common properties of innovation activity are sufficient to generate inverted U-shaped business cycles. Compared to Francois and Shi (1999), innovators in our model opt for major innovation when the old technology loses its potential and cannot sustain higher growth. Thus, this setting is closer to the findings of the life cycles of technologies (Jovanovic, 2009). Innovation activities are divided into two types: primary and secondary innovation. Innovations in our model happen in every period, which is more realistic. Notwithstanding this advantage in dynamics, our model shows that the decreasing return of innovation in a certain technology is a source of cycles. This finding is supported by a recent empirical study (Bloom et al., 2020). Thus, our model provides new insights into innovation cycles and is more consistent with the recent findings regarding the features of innovations.====Our paper is also related to the literature on endogenous growth with innovation. As we have mentioned before, both the fishing-out and learning-by-doing effects are not new in endogenous growth models.==== However, our model shows that these effects are more significant than reported in the existing literature by separating primary and secondary innovations. Recently, there have been some reports on the study of innovation cycles (Comin and Gertler, 2006, Ouyang, 2011). However, they either focus on how innovation changes through business cycles or have similar structures as those discussed in the previous paragraph. Our model provides a new mechanism that shows that an economy with innovation shall have cycles even if we do not consider other factors.====Some literature on innovation-driven cycles reports that innovations are clustered at certain points in time. For instance, Francois and Lloyd-Ellis (2003) contend that entrepreneurs optimally time the implementation of innovations, which makes all entrepreneurs implement their innovations at similar times. In this paper, innovations are divided into two types: secondary innovations show clustering, and the highest point of secondary innovations in every cycle is where many new blueprints are created in all industries. At the end of each cycle, fewer blueprints are created. However, primary innovations are not clustered. A new cycle originates from a single primary innovation. This setting is reasonable. Innovations are often led by a single primary innovation. For example, in the late 1700s and the early 1800s, it was the steam engine. However, in recent decades, information technology (IT) is the leading primary innovation. Almost all innovations in traditional industries benefit from IT. We also explain how primary innovation is connected to secondary innovations when we describe the model.====In Section 2, we introduce our basic model with only the fishing-out effect. This simplified model is easy to track, and we can focus on why business cycles are generated. In Section 3, we solve the basic model to determine the dynamics and reasons for the cycles. In Section 4, we introduce the learning-by-doing effect to obtain more realistic inverted U-shaped cycles. The conclusion is provided in Section 5.",Growing through endogenous innovation cycles,https://www.sciencedirect.com/science/article/pii/S0164070421000872,7 December 2021,2021,Research Article,85.0
"Dai Tiantian,Fan Hua,Liu Xiangbo,Ma Chao","China Economics Management Academy, Central University of Finance and Economics, No. 39 , Xueyuan South Road, Beijing, China,HKU Business School, The University of Hong Kong, Pokfulam Road, Hong Kong, China,School of Labor and Human Resources, IMI, Renmin University of China, No. 59, Zhongguancun Street, Beijing, China,School of Labor and Human Resources, Renmin University of China, No. 59, Zhongguancun Street, Beijing, China","Received 20 January 2021, Revised 8 November 2021, Accepted 18 November 2021, Available online 3 December 2021, Version of Record 15 December 2021.",https://doi.org/10.1016/j.jmacro.2021.103387,Cited by (0),"This paper examines the impact of retirement policy on the unemployment rates for both young and old workers. It employs a labor search framework with a ==== and cross-market matching to investigate the channels through which the delayed retirement policy has impacts. The findings show that through the cross-market matching channel, retirement policy increases the unemployment of young workers (it is ambiguous for old workers) and has a negative effect on the wages of cross-market matched workers. The latter effect is negative for young workers (positive for old workers) through the capital-skill complementarity. The paper calibrates the model to the U.S. data and quantifies the effects of retirement policy during the first decade of this century. Counterfactual experiments highlight the contribution of each channel.","Over the past few decades, there were two important eras of retirement policy in many developed countries, featuring contrasting policy targets. The first era arrived after the Great Depression and lasted until the oil shocks of the 1970s. The high unemployment rate, especially among the young, forced European and North American countries to introduce early retirement policies to create more vacancies for the young. This policy catered to the desire for longer retirement lives by the elderly at that time (see, for example Gruber and Wise, 1999, Mulligan and Sala-i Martin, 2004). Later, with the aging of baby boomers and declining fertility, the financial stability of social security was becoming a central concern. According to World Population Prospects 2017, Europe is the region with the most aging population globally. Europe’s rate of population growth of people over 60 is 24%, followed by 21% in North America and 16.5% in the Oceania region. The average for Asia and the world’ is around 12%. As a result, countries started to switch their retirement policies back and entered the second era of retirement policy, namely delayed retirement.====In the literature, researchers have examined the impact of delayed retirement on both the youth and elderly labor markets. On the one hand, we want to know the effectiveness of such policy. Gruber and Wise (2004) use Organisation for Economic Co-operation and Development (OECD) data and find that a delay of three years in eligibility for receiving retirement benefits would reduce the proportion of men ages 56 to 65 out of the labor force by 23% to 36% on average. Rust and Phelan (1997) and Panis et al. (2002) also find that the delayed retirement policy can significantly increase the labor force participation of older workers. Mastrobuoni (2009) finds that an increase in the normal retirement age by two months delays effective retirement by around one month. Staubli and Zweimuller (2013) empirically estimate the impact of early retirement age policy on elderly labor participation in Austria, where employment increases are accompanied by a substantial increase in registered unemployment for both genders.====On the other hand, since this is a completely contrasting policy target from the one in the first era, we want to know whether the delayed retirement policy squeezes the labor market for young workers. If the answer is yes, through what channels? According to OECD data, the average unemployment rates of young workers in European and OECD countries were 19.8% and 14.5% in 2005–2016, respectively. However, Carnevale et al. (2013) believe that delayed retirement does not squeeze the young labor market. Although many people retired in the past, a large number of people will retire in the future, and especially those born during the baby boom will eventually leave the labor market. Although the authors do not mention the impact of baby boomers on the young labor force, Munnell and Wu (2012) respond to this question by showing that the baby boomer labor force does not worsen the employment level of the young labor force, but rather prompts it, regardless of gender, education, and the period of the Great Recession. A popular explanation for these positive findings is that there is a certain degree of substitution between young and old workers. For example, Böheim (2014) uses OECD data to show that young and older workers are complements for each other rather than substitutes.====Most of the above papers are empirical and focus on changes in labor participation but not the unemployment rate. In this paper, we theoretically investigate the channels through which the delayed retirement policy has an impact on the unemployment rates of both young and old workers. In particular, we extend Shimer’s (2005) labor search model to study the interaction among firms as well as young and old workers. Our model has two important features. First, we consider a nested constant elasticity substitution (CES) production function following Jaimovich et al. (2013), in which old workers use capital to produce intermediate goods, and young workers use intermediate goods to produce final output. This assumption is used to capture the degree of complementarity between young and old workers. Second, we allow the old workers to do cross-market matching but not the reverse. This assumption is based on Chassamboulli and Palivos (2014) and is consistent with the reality that in some industries, old workers can do junior jobs if they want to, whereas young workers cannot. Furthermore, we do not restrict the old workers’ productivity to be smaller relative to the young. In the sensitivity test, we show that the unemployment rate of the old may reverse if the relative productivity is large enough.====We find that, in general, the effects of the delayed retirement policy are ambiguous on the unemployment rates for both young and old workers. We then provide three special cases to study each channel separately. First, we single out that through the cross-market matching channel, the delayed retirement policy has a negative effect on young workers’ market tightness and wages and a positive effect on their unemployment rate. Capital-skill complementarity works in the opposite way, through which the delayed retirement policy has a positive effect on young workers’ market tightness and a negative effect on their unemployment rate. Moreover, it has a direct negative effect on old workers’ market tightness and increases their unemployment rate. If we shut down the above two channels and keep the transition from the young to the old, the delayed retirement policy would have a negative effect on old workers’ market tightness and their wages by raising firms’ flow of value in hiring old workers. In particular, we find that there is a negative effect of the old workers’ market tightness on the young workers’ market tightness, which is due to the changes in firms’ flow of values. However, the overall effect on both unemployment rates is ambiguous through this channel, which suggests that allowing the transition from young to old itself might complicate the results.====In the recent literature, some papers attempt to study retirement policy using the labor search framework. Bhattacharya et al. (2004) study the optimal public policy using a one-period extension of the standard Mortensen (1982) and Pissarides (2000) model. They find that the optimal policy discourages retirement, because old workers receive much higher payments than the theory predicted. Bhattacharya and Reed (2006) extend the model to a dynamic Overlapping generations model to evaluate the effects of public pension policy on labor participation. The public pension program redistributes bargaining strength from young to old workers, raising the wage of the old and decreasing that of the young. The public pension program also induces unemployed old workers to retire and creates more vacancies for the young. Hairault et al. (2015) study the impacts of searching on retirement decisions. They find that search friction has an impact on the retirement decision of unemployed workers, but not on that of employed workers. As a result, they propose an unfair pension adjustment as an optimal policy. The above models focus on optimal public policies and are either partial equilibrium models or only consider one type of job. In contrast, we endogenize the price of final goods by modeling a CES production function. By considering the demand side, the price of final goods would be influenced by labor market tightness, which in turn affects wages and unemployment rates. The CES production function also provides a capital-skill complementarity channel through which the delayed retirement policy has a negative effect on the unemployment rate of the young. By adding two labor market interactions, the influx of old workers would not have an unambiguously positive effect on the unemployment rate of the young as what we would get in the case without the capital-skill complementarity channel.====Keuschnigg and Keuschnigg (2004) apply the labor search model to study the Austrian pension system. García-Pérez and Sánchez-Martín (2015) discuss the unemployment insurance system in the Spanish economy. Hairault et al. (2010) find that the search model with overlapping generations better explains the empirical evidence that the closer to mandatory retirement age an individual is, the less likely they are to be employed. Fisher and Keuschnigg (2011) extend the individual homogeneity setting to analyze the impact of pension reform on the unemployment rates of old or young workers. Lefébvre (2012) uses a similar framework to study the impact of the delayed retirement policy on youth unemployment. In his model, the unit output of the old labor force is the key to affecting the unemployment rate of the young labor force. Michaelis and Debus (2011) consider monopoly unions to show that more older workers have no effect on young and old unemployment, whereas, the unemployment rate of the old (young) will increase (decrease) if the union puts more weight on the old.====We calibrate our model to U.S. data to simulate the effects of the increase in retirement age during 2000–2009. We find that during the first decade of this century, the probability of retirement decreased 23.65%. We find significant positive overall effects on the number of employed old workers in both markers. However, the old earn more in the senior market and less in the junior market. Young workers earn more in the face of the aging population. The unemployment rates for both young and old decreases 2.42% and 3.91%, respectively, during this period. To highlight the contribution of each channel, we conduct three counterfactual experiments by removing the three channels one by one. We find that the aging population has positive effects on the unemployment rate of the young through the aging transition channel, and the cross-market matching channel and has a negative effect through the capital-skill complementarity channel.====The remainder of the paper is organized as follows. The general model is described in Section 2. The special cases are analyzed in Section 3. Calibration, counterfactual experiments, and sensitivity tests are detailed in Section 4. Section 5 concludes.",Delayed retirement policy and unemployment rates,https://www.sciencedirect.com/science/article/pii/S0164070421000860,3 December 2021,2021,Research Article,86.0
"Hunziker Hans-Ueli,Raggi Christian,Rosenblatt-Wisch Rina,Zanetti Attilio","Swiss National Bank, Börsenstrasse 15, CH-8022, Zürich, Switzerland","Received 7 June 2021, Revised 25 October 2021, Accepted 2 November 2021, Available online 27 November 2021, Version of Record 9 December 2021.",https://doi.org/10.1016/j.jmacro.2021.103380,Cited by (1),Long-term ,"Long-term inflation expectations are believed to be an essential element in the transmission of a central bank’s monetary policy. Expectations about future inflation affect consumption and savings of households, wage and price setting and investment and hiring decisions of firms and in turn actual inflation outcomes (see, e.g. Coibion et al., 2020a). Thus, central banks have a natural interest in understanding, monitoring and possibly managing long-term inflation expectations. However, the – possibly changing – behaviour of long-term inflation expectations is not yet fully understood. Unclear aspects include questions regarding how long-term inflation expectations – be they of households, professional forecasters or firms – are formed and how they are influenced by the monetary policy setup (for a recent overview, see, e.g. Coibion and Gorodnichenko, 2015).====There are basically two general views of what drives long-term inflation expectations. The first focuses on long-term factors, such as the monetary policy setup, the credibility and performance of the central bank and long-term average inflation. It suggests that long-term inflation expectations have become more stable and less sensitive to various forms of shocks as a result of the increased credibility of monetary policy. The adoption of explicit guidance in the form of a formal inflation target or an otherwise defined inflation objective is considered an essential element of these developments (see e.g. Mehrotra and Yetman, 2018, Davis, 2014, Mehra and Herrington, 2008, Guerkaynak et al., 2010). Recently, proposals to raise the inflation target to attenuate the zero-lower-bound problem have emerged (see e.g. Blanchard et al., 2010, Ball, 2014). The underlying assumption of these proposals is that inflation expectations would follow the adjustment of the target in the desired direction.====The second view focuses on the impact of short-term dynamics and/or individual characteristics as drivers of long-term inflation expectations. Kumar et al. (2015) argued that twenty-five years of inflation targeting have not allowed managers to form reasonable long-term inflation expectations. Trehan (2015) found evidence that, even in recent periods, survey participants (both households and professional forecasters) still place a disproportionately high weight on recent inflation data when forming their expectations, i.e., adopting a short-sighted, backward-looking behaviour. Nishiguchi et al. (2014) showed that the shift in the Bank of Japan’s inflation target has possibly reduced, but not eliminated, deflation expectations among households. They found that younger survey participants, who have essentially experienced only periods of negative inflation in their lifetimes, tend to have more persistent deflationary expectations than older respondents do. This finding is in line with the findings of Malmendier and Nagel (2015): using several decades of microdata, they found clear evidence that long-term inflation expectations vary across individuals depending on their personal experience of inflation.====In this paper, we find that, at least in the case of Switzerland, both views – the one that stresses the importance of long-term factors and the one that focuses on short-term dynamics and individual characteristics – are relevant for understanding the behaviour of the long-term inflation expectations of firms. To this end, we use the Swiss National Bank (SNB) regional network survey, which is conducted by the SNB on a quarterly basis among various companies from different industries and areas of the economy in Switzerland. The survey is based on the form of deep and detailed conversations with CEOs/CFOs. Our analysis spans Q3 2014 to Q4 2017.====With respect to the first view, the interaction of monetary policy with long-term inflation expectations, we proceed as follows. We test whether providing external guidance in the form of communicating information regarding the central bank’s objective, its past performance and long-term average inflation influences long-term inflation expectations and the uncertainty surrounding these expectations. To do so, we set up an experiment in the following manner. We randomly split the sample of interview partners into two equally large groups. Before answering the questions regarding long-term inflation expectations, half of the sample received an explicit reminder (which we call ====) regarding what the Swiss National Bank’s objective is, about its performance and about long-term average inflation. The second half of the sample did not receive any information (====). We find that: (a) guidance can influence the long-term inflation expectations of firms to a certain extent; and (b) guidance does, surprisingly, not have an impact on the uncertainty surrounding these forecasts. However, (c) the degree of uncertainty is positively correlated with the level of long-term inflation expectations, and respondents who are more uncertain place greater weight on the signals that they receive through guidance.====To address the second view, which focuses more on short-term dynamics and individual characteristics that help to explain long-term inflation expectations, we analyse whether other attributes covered in the SNB regional network survey possibly help to explain the firms’ long-term inflation expectations. We find that: (d) short-term inflation expectations; and (e) individual factors related to prices, such as the firms’ assessments of expected purchase prices, show a significant relationship with their long-term inflation expectations. We find in addition, that: (f) a large, unanticipated shock – in our case, a large shock to the exchange rate – moves long-term inflation expectations.====Firms’ inflation expectations found their way into macroeconomic theory and modelling a long time ago. As Candia et al. (2021) review and point out, the important role of firms’ inflation expectations was initially formalised in Friedman (1968) and Phelps (1968) and is usually presented in the form of an expectations-augmented Phillips curve. The expectations-augmented Phillips curve links inflation and the real side of the economy conditional on firms’ expectations of inflation. This relationship can be found in various classes of models such as, e.g., the sticky-price models with the New Keynesian Phillips Curve (see e.g. Galí and Gertler, 1999, Clarida et al., 1999), the sticky-information models (see e.g. Mankiw and Reis, 2002), in behavioural approaches to the New Keynesian model (see e.g. Gabaix, 2020), in models of dynamic rational inattention (see e.g. Afrouzi and Yang, 2021), etc.====Despite the importance of firms’ inflation expectations, only recently, research on the exact nature, formation and behaviour of firms’ inflation expectations has emerged. This is mainly due to the fact, as noted by Coibion et al. (2018), that information about firms’ expectations is scarce compared to information retrieved from surveys of households, from professional forecasters and from financial market participants. Thus, the inflation expectations of households, professional forecasters and financial market participants have been studied much more frequently. Our research and findings relate in particular to the following papers in which firms were analysed. Coibion et al. (2018) shed light on the behaviour of firms in New Zealand by implementing a quantitative survey conducted in four waves. With regard to inflation expectations, they focused on short-term inflation expectations covering a horizon of 12 months. Among many other things, they documented that errors in beliefs about recent inflation are biased towards the upside. They also found a positive correlation between uncertainty and inflation forecasts. When providing information about recent macroeconomic variables, as well as the forecasts of professional forecasters and the inflation target of the central bank, they found that firms adjusted their inflation forecasts consistently with models of Bayesian learning in response to new information. Additionally, firms with higher levels of uncertainty revised their expectations by more, i.e., placing greater weight on the signals that they receive. The authors also reported a time dimension (cyclicality) in how information is processed. While our findings are similar to those of Coibion et al. (2018), the setup of our survey and the experiment are nevertheless quite different. In addition, our paper focuses not on short- but long-term inflation expectations and has a cross-sectional time series rather than a panel structure. Coibion et al. (2020b) treat repeatedly Italian firms with information on recent inflation (year-on-year inflation in the previous month) and show that their short-term inflation expectations respond strongly, while the effect becomes less pronounced for expectations at longer horizons. We also find that guidance can affect long-term inflation expectations to a certain extent. Our approach is however different in that we do not provide information on recent inflation but our guidance is rather directed at emphasising the objective and performance of the central bank in a long-term perspective. Kumar et al. (2015) used the same database as Coibion et al. (2018) and complemented their findings by checking for anchoring of the firms’ inflation expectations in New Zealand and the influence of monetary policy on inflation expectations. To answer these questions, they concentrated on one wave of the survey, namely, that conducted in Q3 2014. They found that managers commonly report large revisions in their forecasts. In addition, those who report high short-term inflation forecasts also tend to expect higher long-term inflation. We also find this positive correlation in our study over a period of time.====The remainder of the paper is structured in the following way. Section 2 describes the survey and the data at hand in greater detail. It also provides a short overview of the inflation environment in Switzerland. Section 3 discusses the setup of our experiment. Section 4 presents visually some results regarding the impact of guidance on long-term inflation expectations and uncertainty. In Section 5, we estimate a model of the possible explanatory variables of long-term inflation expectations and discuss the results. Section 6 analyses how much weight the firms, on average, place on the signals that they receive through guidance. Section 7 concludes the study.","The impact of guidance, short-term dynamics and individual characteristics on firms’ long-term inflation expectations",https://www.sciencedirect.com/science/article/pii/S0164070421000793,27 November 2021,2021,Research Article,87.0
Elias Christopher J.,"Department of Economics, 703 Pray-Harrold, Eastern Michigan University, Ypsilanti, MI 48197, United States of America","Received 23 January 2021, Revised 21 October 2021, Accepted 28 October 2021, Available online 14 November 2021, Version of Record 20 November 2021.",https://doi.org/10.1016/j.jmacro.2021.103379,Cited by (1)," methods using post-Second World War U.S. data and results show that there is significant expectational heterogeneity in the data and that the heterogeneous expectations model performs better than a homogeneous expectations benchmark model. Furthermore, the model’s dynamics resulting from heterogeneous expectations are analyzed.","Representative agent rational expectations has been the dominant expectations modeling paradigm within macroeconomics for decades. However, macroeconomic models incorporating heterogeneous expectations are starting to become more commonplace, early examples of which are Evans and Ramey (1992) and Brock and Hommes (1997). Building on this early research, heterogeneous expectations were incorporated into a New Keynesian framework in Branch and McGough (2009) and Branch and McGough (2010), which further influenced a large line of literature.====The motivation for incorporating heterogeneous expectations into macroeconomic models comes from two areas. First, empirical evidence is consistent with heterogeneous expectations. For example, Mankiw et al. (2003) and Weber (2007) find disagreement among consumers and professional forecasters with respect to inflation expectations, while Carroll (2003) and Branch (2004) find evidence of time-varying expectations in inflation survey data. Second, lab experiments, for example in Hommes (2011), Assenza et al. (2013), and Pfajfar and Zakelj (2014) support the idea of heterogeneous expectations.====In light of these developments, this paper attempts to answer three main questions. First, is there evidence of expectational heterogeneity in macroeconomic data? An affirmative answer to this question will further bolster the empirical evidence supporting the incorporation of heterogeneous expectations into macroeconomic models. Second, does the inclusion of heterogeneous expectations into a standard macroeconomic model improve the performance of the model? An affirmative answer to this question will provide more evidence that heterogeneous expectations should be incorporated into a wider-range of macroeconomic models. Third, how do heterogeneous expectations alter the dynamics of a standard macroeconomic model? An answer to this question is important because if heterogeneous expectations are incorporated into macroeconomic models, it is beneficial to understand the mechanism at work.====To answer these questions, this paper builds on earlier work by augmenting a medium-scale New Keynesian model with heterogeneous expectations. The model includes sticky prices and wages, several structural shocks, and is very similar to those studied in Christiano et al. (2005) and Smets and Wouters (2007). Heterogeneous expectations are incorporated via the Euler equation adaptive learning mechanism of Evans and Honkapohja (2001) that treats agents like real-world econometricians. Two agent-types are differentiated by their forecasting model of choice; type-A agents use a model that is of the same form as the minimum state variable (MSV) rational expectations equilibrium (REE) solution, while type-B agents use a misspecified model that excludes a subset of variables that is part of the REE solution. One way to think about this formulation is that type-A agents are “nearly” rational, while type-B agents are subject to computational, cognitive, and/or cost constraints that oblige them to forecast with a misspecified model. This setup is designed to capture the idea that since real-world forecasters are subject to computational and cognitive limitations that may cause them to employ misspecified models, it is only natural to extend these same limitations to, at the very least, a subset of agents in the model.==== Furthermore, the specific heterogeneous expectations formulation used in this paper has a well-known equilibrium concept, referred to as a heterogeneous expectations equilibrium (HEE) and first studied extensively in Berardi (2007) and Berardi (2009), that is mathematically tractable, which allows for the demonstration that the estimated parameters generate an equilibrium that is stable under learning, and enables the model to be better put into the context of the literature.====The structural parameters of the model are estimated using Bayesian methods with post-Second World War U.S. data, with particular attention paid to the estimates of the proportion of type-A agents and the adaptive learning gains of both agent-types.==== The performance of the estimated model is then compared to that of an estimated homogeneous expectations model, where performance is assessed based on in-sample fit of the data and out-of-sample forecasting exercises.==== Lastly, to aid in understanding how heterogeneous expectations affects the dynamics of the model, the two models’ dynamics are compared by analyzing impulse response functions and forecast error variance decompositions.====The results support the conclusions that there is evidence of significant expectational heterogeneity in the macroeconomic data and that the heterogeneous expectations model outperforms the homogeneous expectations model. Furthermore, incorporating heterogeneous expectations into the model suggest five generalized observations. First, many model variables become more volatile, a fact which is at odds with the data and represents a noticeable weakness of the heterogeneous expectations model. Second, the productivity shock becomes relatively more important as a driver of economic activity. Third, the monetary policy shock becomes relatively less important as a driver of economic activity. Fourth, the price markup shock becomes relatively more important in explaining movements in inflation, and fifth, the monetary policy shock becomes relatively more important in explaining movements in the nominal interest rate.====This paper extends the work in Milani (2007), which was the first paper to use Bayesian methods to estimate a small-scale New Keynesian model with homogeneous expectations and adaptive learning, and Slobodyan and Wouters (2012), who use Bayesian methods to estimate a medium-scale New Keynesian model with homogeneous expectations and adaptive learning. The current paper is different from those papers, however, in that the model analyzed is one with heterogeneous expectations. Another related paper is Berardi (2009), which formulates the mathematical properties of the HEE concept in a generalized linear model. The current paper applies Bayesian estimation and empirical analysis tools to the theoretical results of that author’s work. Zhao (2017) uses Bayesian methods and data on the Chinese economy to estimate a medium-scale DSGE model with heterogeneous expectations in which some agents are fully rational and some are employing adaptive learning. However, the current paper analyzes a much larger medium-scale New Keynesian model, specifies the equilibrium properties of the model, employs agents who are all using adaptive learning (although some are using a misspecified forecasting model), estimates the model using U.S. data, compares the estimated model’s performance to that of a homogeneous expectations model, and analyzes the heterogeneous expectations mechanism in detail. The current paper is also related to Elias (2020), who estimates a small-scale New Keynesian model with heterogeneous expectations using Bayesian methods. The current paper significantly extends that work, however, by using a much broader data set and more comprehensive model that require more complex mathematical and computational methods, and quantitatively analyzing the heterogeneous expectations mechanism in detail. Moreover, the medium-scale model is more realistic and policy relevant, implying that the results of the current paper are more applicable to policy-making.====The main contribution of the paper is that it is the first, to my knowledge, that uses Bayesian methods to jointly estimate the structural and adaptive learning parameters of a medium-scale New Keynesian model with heterogeneous expectations, compares the model performance to a model with homogeneous expectations, and analyzes the dynamic properties that result directly from augmenting a standard macroeconomic model with heterogeneous expectations via adaptive learning. While there are many theoretical papers that examine heterogeneous expectations with adaptive learning in a New Keynesian setting, there is noticeable lack of empirical work in this area. This paper attempts to fill that gap.====The paper is organized as follows: Section 2 builds the medium-scale New Keynesian model with heterogeneous expectations and clarifies the HEE solution concept. Section 3 discusses the estimation methodology, Section 4 discusses the estimation results, Section 5 compares the performance of the heterogeneous and homogeneous expectations models, Section 6 analyzes the dynamics that result from the incorporation of heterogeneous expectations, and Section 7 concludes. Appendix A provides a detailed description of the medium-scale New Keynesian model, Appendix B describes the data, Appendix C derives the equations of the state-space representation of the model and describes the projection facility used in the estimation, and Appendix D conducts a robustness check.",Adaptive learning with heterogeneous expectations in an estimated medium-scale New Keynesian model,https://www.sciencedirect.com/science/article/pii/S0164070421000781,14 November 2021,2021,Research Article,88.0
Binder Carola Conces,"Haverford College, Department of Economics, United States of America","Received 8 August 2021, Revised 11 September 2021, Accepted 25 October 2021, Available online 10 November 2021, Version of Record 19 November 2021.",https://doi.org/10.1016/j.jmacro.2021.103378,Cited by (2),Social science research studies are frequently conducted on Amazon Mechanical Turk (MTurk). I use data from four previous surveys of ,"In recent years, clinical and social scientists have increasingly relied on crowdsourced convenience samples to quickly and affordably recruit study participants (Behrend et al., 2011, Horton et al., 2011, Chandler and Shapiro, 2016). Amazon Mechanical Turk (MTurk) is among the most popular platforms for crowdsourcing study participants in a range of fields (Paolacci et al., 2010, Arch and Carr, 2017, Miller et al., 2017, van Stolk-Cooke et al., 2018, Strickland and Stoops, 2019). On MTurk, “requesters” post small tasks called “Human Intelligence Tasks” (HITs) with a fixed payment. Requesters have the option of specifying eligibility criteria, such as age and location. MTurk workers can work whenever they choose, selecting from a list of HITs for which they are eligible. In research applications, the researcher posts a link to a survey or experiment as a HIT.====Given the proliferation of research relying on MTurk, a growing methodological literature investigates the participant characteristics, reliability, and validity of studies using the platform, with the aim of recommending best practices for researchers (Berinsky et al., 2012, Casler et al., 2013, Paolacci and Chandler, 2014, Peer et al., 2014, Levay et al., 2016, Mortensen and Hughes, 2018, Turner et al., 2021). One particular methodological issue that has received relatively less attention is the possibility of intertemporal variation in MTurk participation (Casey et al., 2017). That is, characteristics samples of MTurk workers may vary with the time of day or the day of the week in which the survey is taken. Such intertemporal effects could imply that a particular sample recruited from MTurk might not be representative of the population of MTurk workers, and could potentially threaten the validity and replicability of research findings. Awareness of intertemporal effects could also be beneficial to researchers hoping to study participants with particular characteristics, who might tend to be more active on certain days or times.====In this paper, I use data from four economics surveys conducted on MTurk from 2017 through 2021, with a total of 2780 observations, to study intertemporal effects on worker responses and behavior (Binder and Rodrigue, 2018, Binder, 2020, Binder, 2021b, Binder, 2021a). These four surveys, all using workers from the United States, include several questions in common, particularly about economic knowledge and inflation expectations, which facilitates the analysis of intertemporal effects. In particular, I test whether respondents’ demographic characteristics, responses to objective knowledge questions, inflation expectations, “don’t know” responses, news sources, responses to open-ended questions, and time spent on the survey vary by day of week and time of day. I also use data from several additional surveys from authors in other fields (Clifford et al., 2015, Necka et al., 2016, Almaatouq et al., 2020, Ahler et al., 2020, Brown and Pope, 2021) to demonstrate that the key results for demographics and time spent on survey are similar with a sample size of over 8000 respondents.====Several other papers study demographic variation in MTurk participants by day of the week or time of day (Lakkaraju, 2015, Casey et al., 2017). Some also document intertemporal differences in psychological characteristics and survey experience (Casey et al., 2017, Fordsham et al., 2019, Arechar et al., 2017). Section 2 provides an overview of this literature. This paper complements and extends the previous literature in several ways.====First, my relatively large and recent sample provides novel evidence on intertemporal variation in demographic composition and time spent on the survey. In some cases my results differ from previous findings. For example, like Casey et al. (2017), I find that Saturday participants are older than other participants; but I additionally find that Saturday participants are less likely to have a college degree and more likely to have low income. Since a large portion of my sample comes from a 2021 survey, I also am able to study how intertemporal effects have changed over time, potentially due to the COVID-19 pandemic.====Second, I test for intertemporal effects on a variety of outcomes not considered in the previous literature. My data includes objective measures of respondents’ knowledge, including two questions about basic statistics and questions about the Fed Chair and the Fed’s inflation target. Saturday respondents are most likely to answer these questions correctly, most likely to provide a reasonable (between −5% and 5%) inflation forecast, and least likely to provide a “don’t know” response about expected inflation. However, I do not detect intertemporal effects on mean inflation expectations. I hand-code responses to an open-ended question to identify respondents who make substantive comments and respondents who are likely to be bots. Substantive comments are slightly more likely from morning workers, while Saturday workers are less likely to appear to be bots. All of these results hold even when controlling for demographic characteristics. Third, I test whether typical data cleaning approaches can either alleviate or exacerbate these intertemporal effects. Dropping respondents who take the survey very quickly, who appear to be bots, or whose geographical coordinates are from outside of the United States (despite a requirement that workers be in the United States), or using sampling weights in the regressions to detect intertemporal effects, makes little difference in the effect estimates.====A growing macroeconomics literature on expectations formation increasingly uses survey-based experiments to test how various types of information treatments affect expectations, particularly of inflation (Armantier et al., 2015, Armantier et al., 2016, Coibion and Gorodnichenko, 2015, Kuchler and Zafar, 2019). Political scientists have also made recent contributions to this literature (Baerg et al., 2021). This literature has grown in response to monetary policymakers’ desire to understand how to better shape household inflation expectations as a policy tool, especially when nominal interest rates are constrained by the zero lower bound, and also how to build trust in the central bank (Coibion et al., 2020b). Crowdsourced survey experiments hold promise as a solution to the challenge of inferring the causal effects of central bank communications on beliefs and expectations using observational data. For example, researchers have conducted daily-frequency surveys of inflation expectations to identify effects of central bank announcements and other key events (Lamla and Vinogradov, 2019, Coibion et al., 2020a). As this literature progresses, it will be useful to have evidence on how intertemporal effects might affect these and other types of research designs.====More generally, the results suggest that some types of research may benefit from recruiting a larger share of participants on weekends, especially on Saturdays, when respondents are more demographically representative of the population along several dimensions and may put more effort into certain types of responses.",Time-of-day and day-of-week variations in Amazon Mechanical Turk survey responses,https://www.sciencedirect.com/science/article/pii/S016407042100077X,10 November 2021,2021,Research Article,89.0
"Salisu Afees A.,Gupta Rangan,Kim Won Joong","Centre for Econometric & Allied Research, University of Ibadan, Ibadan, Nigeria, Department of Economics, University of Pretoria, South Africa,Department of Economics, University of Pretoria, Pretoria, South Africa,Department of Economics, Konkuk University, Seoul, Republic of Korea","Received 1 March 2021, Revised 29 July 2021, Accepted 30 September 2021, Available online 3 October 2021, Version of Record 14 February 2022.",https://doi.org/10.1016/j.jmacro.2021.103374,Cited by (1),"This paper seeks to add to the literature on short-run exchange rate predictability by focusing on BRICS exchange rates. We utilize both time-varying and constant parameter models, and account for a variety of macro fundamentals, including those suggested by ","Given the importance of exchange rate in the economy, predicting exchange rate has become an important issue in international economics since Meese and Rogoff (1983). Specifically, they estimate whether macroeconomic fundamentals (such as the flexible-price monetary (Frenkel (1976)-Bilson (1978)) model, the sticky-price monetary (Dornbusch (1976)-Frankel (1979)) model, and a sticky-price asset (Hooper-Morton (1982)) model) can outperform the random walk model using out-of-sample method and fail to find evidence in favor of the models over the random walk model. Since then, many competing theoretical models are introduced. Among them, Rossi (2013) surveys the literature and summarizes that the Taylor rule fundamentals display significant out-of-sample forecasting ability at short horizons and panel monetary models display some forecasting ability at long horizons. In a later research by Byrne et al. (2016), the forecastability of Taylor rules-based exchange rate with Time-Varying Parameters produces better forecast outcomes over the random walk model particularly for the post Global Financial Crisis period. Engel et al. (2015) use the extracted factors from a cross-section of exchange rates together with fundamental models, such as Taylor rule model, monetary model, and PPP model, to forecast the exchange rates. They find predictive power for long horizon for later (1999–2007) periods. Byrne et al. (2018) also use factor models and other fundamental models, such as Taylor rule model, monetary model, PPP model and uncovered interest rate parity model, to forecast the exchange rate with time-varying parameter setup. They find superior forecast performance of the exchange rate models over the random walk model for most currencies.====Therefore, unlike the existing (prominent) studies on Taylor rule-based exchange rate predictability which majorly focus on advanced economies, our study fills the gap by examining the subject from the perspective of emerging economies of the BRICS countries. There are a number of attractions to these emerging economies. According to Deutsche Welle (2019), BRICS (Brazil, Russia, India, China, and South Africa) countries account for 42% of the global population, 23% of GDP, 30% of the territory and 18% of trade. Exchange rate is one of the most important macroeconomics variables for those countries and greatly affects the economy not only through the trade channel but also the financial channel. However, there is only a handful of research regarding the forecastability of the exchange rates for BRICS countries while there is none involving the Taylor rule fundamentals, to the best of our knowledge. Salisu et al. (2021a) estimate the oil-based model and find that oil-based model outperforms the random-walk model in predicting exchange rate. Salisu et al., 2021b), based on Uncovered Equity Parity, examine whether stock returns contain useful information in predicting exchange rate for BRICS countries. They find that stock returns improve the predictability of exchange rates for BRICS countries.====In addition, we extensively examine exchange rate predictability using time-varying and constant parameter models that are conditioned on three variants of conventional Taylor rules as well as six additional alternative models, namely monetary model (MM); purchasing power parity (PPP); uncovered interest rate parity (UIRP) and three different factor (F1, F2 and F3) models for BRICS countries. Among BRICS countries, three countries adopted floating exchange rate system since 1990s (Brazil (1999), India (1993), South Africa (1995)). Russia began adopting floating exchange rate system since 2014 and China started adopting managed floating exchange rate system since 2005. Therefore, in addition to finding the best model for exchange rate forecastability of BRICS countries in aggregate, comparing the performances for individual countries would give us an implication on whether the exchange rate regime affects the choice of exchange rate model.====Foreshadowing our results, we find that models conditioned on the Taylor rule-based model with homogeneous coefficients but without interest rate smoothing as well as other models like the PPP- and UIRP-based models offer better exchange rate predictability for the BRICS than the random walk model across the forecast horizons. In addition, constant parameter models offer superior forecasting ability relative to the time-varying parameter models. Two implications can be discerned from these findings. First, exchange rates of the BRICS are driven by the Taylor rule fundamentals and therefore monetary policy authorities in these countries can stabilize the movements in exchange rates by using relevant policy instruments, in which the policy rate comes in handy, to target these fundamentals such as the real level of economic activity and inflation, among others. Second, the relatively homogenous behavior of the BRICS's exchange rates to the Taylor rule fundamentals suggests the presence of some inherent similarities in their conduct of monetary policy. This is not far-fetched as these countries are more involved in inflation targeting which is a prominent feature of the Taylor rule with implications on other fundamentals like the real level of economic activity since the setting of the policy rate is directed towards meeting the inflation target.====The rest of the paper proceeds as follows. The next section reviews the literature and raises the issues that needs to be addressed in our research. Section 3 discusses estimation equations, forecasting evaluation strategy, and the data. Section 4 presents our main empirical findings. Section 5 provides concluding remarks.",Exchange rate predictability with nine alternative models for BRICS countries,https://www.sciencedirect.com/science/article/pii/S0164070421000732,3 October 2021,2021,Research Article,91.0
"Tran Chung,Wende Sebastian","Research School of Economics, Australian National University, Australia","Received 4 March 2020, Revised 5 October 2021, Accepted 8 October 2021, Available online 21 October 2021, Version of Record 27 October 2021.",https://doi.org/10.1016/j.jmacro.2021.103377,Cited by (2),"We extend marginal ====. The tax incidence analysis shows variation of tax burdens across households, depending on their age, income type and generation. In particular, older households with higher income bear the highest burden of company income tax; meanwhile, future born households bear the highest burden of ====. Hence, our MEB analysis demonstrates a fruitful approach to better understanding efficiency and incidence of tax reforms in one unified framework.","In the public finance literature, there are two main approaches to evaluating designs of tax system and policy. One is normative and studies the optimal tax system. The other is positive and studies excess burden of taxes at the point defined by an existing tax system. The latter approach has a long tradition in public finance and real-world policy making (e.g., see Auerbach and Hines (2002) for a review). According to the excess burden analysis, the efficiency of a tax system can be improved by relying more on taxes with low excess burden and less on taxes with high excess burden. This excess burden analysis has an intuitive appeal and is widely used in policy making as a comparative measure of the distortion from different taxes.====Previous studies on excess burden of taxes, including seminal works by Harberger, 1964, Ballard et al., 1985, Judd, 1987, rely on models with a representative agent. That modeling approach is useful in assessing relative efficiency of different taxes; however, it is not capable of assessing distributional consequences, i.e., tax incidence. In this paper, we extend the excess burden approach in the previous literature to a new analytical framework that accounts for household heterogeneity. This extension is essential for better understanding efficiency ranking and distributional effects of different taxes in one unified and more realistic framework.====We formulate a large-scale, dynamic general equilibrium, overlapping generations model that consists of heterogeneous households, a representative firm, and a government. Specifically, households are born with different skill types that determine their labor productivity over their life cycle, and make decisions on consumption, labor supply and saving to maximize their lifetime utility. Such household heterogeneity allows us to disaggregate marginal excess burden of taxes according to income type, age and generations. On the firm side, we deviate from the previous literature, which usually subsumes all taxes on capital in a single tax rate, and explicitly model the structure of corporate finance and taxes. The firm chooses investment to optimize its market value taking into account future profits and the structure of corporate finance and taxes. The firm relies on internal finance (retained profits) or external finance (debt or equity) to fund its investment plan. Having these new modeling features allows us to have a differentiated assessment of different capital taxes.====We first quantify the marginal excess burdens (MEB) of three main taxes: company income tax (CIT), personal income tax (PIT) and consumption tax (CT). Since the Harberger triangle is not well defined in a general equilibrium framework, we follow Judd (1987) and use Hicksian equivalent variation to measure marginal excess burden of taxes. We do so by computing the equivalent variation for all households who currently live in the initial steady state and who are born along the transition path to the new steady state after raising taxes. Only one tax rate is allowed to change at a time and the additional tax revenue is transferred back to all households via a uniform lump-sum transfer scheme. Excess burdens are welfare losses/gains that individuals bear after raising taxes. Marginal excess burden attached to each individual household is the net present value of the welfare losses normalized by the net present value of the change in net revenue. The metrics of marginal excess burdens (MEB) are calculated according to two measures: aggregate one for all households in the economy, and group-specific one for each household group conditioning on income type, age and generation. Our quantitative results, based on a calibration that matches the Australian economy, are as follows.====First, taxing capital income results in the largest welfare loss in terms of marginal excess burden at the point defined by the current policy settings. In particular, the aggregate marginal excess burden (MEB) for the company income tax is ==== cents per dollar of tax revenue collected, which is much higher than ==== cents and ==== cents found for the personal income tax and the consumption tax, respectively. The main reason is that an increase in the company income tax discourages investment and subsequently decreases the capital stock, productivity, and aggregate output. In the context of a small open economy model, the adverse effects of the company income tax are further amplified as foreign capital is highly mobile. Thus, our MEB analysis confirms a claim, frequently made by economic scholars and the popular press, that company income taxes are more distortionary than personal income and consumption taxes.==== According to the MEB metric, the efficiency of a tax system can be improved by relying more on taxes with low MEB and less on taxes with high MEB. The MEB analysis has an intuitive appeal and is widely used in policy making as a comparative measure of the distortion from different taxes. The influential nature of MEB analysis can be seen in policy making in many advanced economies (e.g., see Cao et al. (2015) for the case of tax reform in Australia).====More importantly, different households bear different burdens of taxes as the marginal excess burdens are distributed unequally across households and time. The group-specific marginal excess burdens vary significantly across skill types, ages, and generations. In particular, retirees are the biggest losers from a company tax increase; meanwhile, they suffer less from increases in the personal income tax or the consumption tax. Lower-income households bear, on average, minimal impacts from a company tax hike. This occurs as the baseline scenarios assume any extra revenue collected is redistributed uniformly to all households via the transfer system. The loss of income from lower wages is partly offset by higher transfers. The low-income households, in aggregate, would be only ==== cents worse off under an income tax rise and ==== cents worse off if the consumption tax increased again due to the additional transfers. Such unequal distribution of marginal excess burdens highlights the importance of accounting for household heterogeneity when conducting a marginal excess burden analysis.====We consider a broader set of taxes on capital and labor income separately, including investment tax credits (ITC), depreciation deductions (DD), personal labor income tax (PLT) and personal asset income tax (PAT). Our comparison indicates that the taxes which are broadly considered as capital taxes result in higher marginal excess burden than the taxes imposed on labor income and consumption in our framework. The overall ranking is that reducing investment tax credits results in the highest marginal excess burden, while raising consumption tax causes the lowest marginal excess burden. Interestingly, even though a reduction in the investment tax credits has the highest welfare cost at the aggregate level the households alive at the time of the policy change are better off.====Our findings indicate a disparity between overall efficiency ranking and incidence of different taxes. Raising capital taxes leads to relatively larger marginal excess burdens than raising labor and consumption taxes. According to this MEB ranking, the efficiency of a tax system can be improved by relying less on capital taxes. The importance of efficiency ranking can be seen as one of the driving factors behind tax reform proposals. However, our tax distributional analysis points out adverse distributional consequences as households at different stages of life, income type and time bear different excess burdens of taxes.====In robustness check, we consider different modeling assumptions and calibrations, including the openness of the economy, alternative methods to allocate additional tax revenue, foreign ownership, the role of fixed factors, franking credits, capital adjustment speed, economic growth, intertemporal elasticity of substitution and preferences, progressive taxes and intended bequests. We find that our main results are fairly robust. In extension, we study marginal excess burdens of taxes in a closed economy model that is calibrated to match the US economy. We find similar lessons from the US tax policy settings.====Our findings demonstrate the usefulness of marginal excess burden analysis for real-world policy making. The aggregate marginal excess burden is important to evaluating tax changes and aggregate welfare gains. In addition, the distribution of marginal excess burdens is useful to understand the incidence of tax changes and distributional consequences, which is important when designing a feasible tax reform package. Our marginal excess burden analysis contributes to bridging the gap between academic research and policy making.==== Our work is connected to several branches of the macroeconomic/public finance literature. First, our paper contributes directly to the large literature analyzing the excess burden of taxation dated back to Harberger (1964). The early work studies excess burden in a static partial equilibrium framework (e.g., see Feldstein (1978) , King, 1983, Feldstein, 1995, Fullerton and Henderson, 1989, and Hines (1999)). These early studies do not account for general equilibrium adjustments after tax change. Auerbach et al., 1983, Chamley, 1981 and Ballard et al. (1985) extend the excess burden approach to the general equilibrium framework, which then becomes very popular modeling tools in real-work policy making. Judd (1985a) and Judd (1987) further extend the excess burden analysis of taxes to a dynamic general equilibrium model with an infinitely-lived representative agent. These previous studies provide a wide range of estimated MEB of different taxes. In particular, Ballard et al. (1985) find MEB estimates in the range of ==== to ==== cents for industry level capital tax and a range of ==== to ==== cents for industry level labor taxes. Judd (1985b) finds a MEB of ==== cents for labor income tax, ==== cents for a tax on dividends and interest payments, and ==== dollars for an investment tax credit. Note that, all these numbers are the results of a steady state analysis. Our main contribution to this literature is an extension of marginal excess burden analysis of taxes to a dynamic general equilibrium model with heterogeneous agents. Our analysis accounts for long-run steady state and transitional dynamics. As a result, our estimates of aggregate MEB are relatively larger. Importantly, household heterogeneity enables us to map out the distribution of marginal excess burdens across households. We are able to show how marginal excess burdens of taxes vary across households and over time, which is missing in the previous studies based on a representative agent modeling approach.====The MEB analysis is related to the theory of optimal taxation (e.g. see Conesa et al. (2009) and Heathcote et al. (2017) ). However, it differs both in its approach and the underlying question it answers.==== The theory of optimal taxation aims to characterize which tax system should be chosen to maximize a social welfare function subject to a set of constraints. The optimal taxation literature usually makes a normative choice on a social welfare function, which aggregates the utility of individual households. The optimality of a particular tax system depends on this choice of social welfare function (e.g., see Fehr and Kindermann (2015)).==== The optimal tax analysis provides global information on the “best” tax system. This normative approach to studying a tax system is very popular in academic literature. However, the optimal tax analysis is rarely used in real-world policy making because it provides limited information on the local impacts of the current tax system. Instead, the MEB analysis measures the welfare loss from a tax increase at the point defined by the current tax system and can provide local information on the welfare cost of a marginal tax increase. Thus, the MEB and optimal tax analyses provide complementary information. A measure of the distortion of current policy settings can act as a motivator of policy reform while optimal analysis provides insights of what an overall reform package should look like.====Our work also links the literature on the marginal excess burdens of taxes to the literature on the dynamic effects of taxes. Since Auerbach and Kotlikoff (1987) there is a large literature analyzing aggregate and welfare effects of various tax schemes, using a large scale overlapping generations (OLG) models. Auerbach and Kotlikoff (1987) show that while replacing income tax with wage tax reduces efficiency, replacing income tax with consumption tax increases efficiency. Imrohoroglu (1998) finds that a positive capital income tax rate maximizes steady state welfare. In a similar environment, Ventura (1999) shows that the elimination of capital income taxation positively affects capital accumulation. Fuster et al. (2007) study the effects of different revenue-neutral tax reforms that eliminate capital income taxation and show that the majority of the population alive at the time of the reform benefit from it in the dynastic framework. Conesa et al. (2009) find that the optimal capital income tax rate is strictly positive at ==== percent in an overlapping generations economy with incomplete markets and heterogeneous agents. We use a similar modeling approach, but we rely on the excess burden analysis tradition to assess the efficiency and welfare effects of taxes. Our MEB analysis illuminates that the marginal analysis of tax changes at existing state of tax policy settings is important for understanding the efficiency and distributional effects at the margin.====Our paper is related to the literature on dynamic scoring of tax cuts. Mankiw and Weinzierl (2006) studies the extent to which a tax cut pays for itself through increased activity. Leeper and Yang (2008) emphasize the role of tax financing instruments when calculating the tax scoring. That literature is motivated by President Bush’s early 2000s tax cuts in the United States. In the dynamic scoring literature, the dynamic revenue estimate takes the behavioral responses and general equilibrium adjustments into account. Dynamic scoring of a tax rate computes the derivative of the dynamic tax revenue. MEB analysis is analogous to dynamic scoring analysis as they both study the effects of taxes at the point defined by the existing tax system. However, the former focuses on the welfare effects of raising taxes, while the later focuses on the revenue effects.====Finally, our paper contributes to the analysis of dynamic effects of fiscal policy in Australia (e.g. see Cao et al., 2015, Tran and Woodland, 2014, Kudrna et al., 2015 and Kudrna et al. (2019)). Our small open economy model is calibrated to match the Australian economy. Our quantitative assessment of the marginal excess burden of taxes has implications for policy making in Australia.====The paper is structured as follows. Section 2 describes the benchmark model and calibration. Section 3 presents the main quantitative analysis. Section 4 report robustness checks and extensions. Section 5 concludes. The Appendix A contains additional information, tables and figures. The detailed description of the key equations and the computational methods is available in an online technical appendix.",On the marginal excess burden of taxation in an overlapping generations model,https://www.sciencedirect.com/science/article/pii/S0164070421000768,21 October 2021,2021,Research Article,95.0
"Maih Junior,Mazelis Falk,Motto Roberto,Ristiniemi Annukka","Norges Bank, Norway,European Central Bank, Germany","Received 3 February 2021, Revised 28 September 2021, Accepted 4 October 2021, Available online 16 October 2021, Version of Record 29 October 2021.",https://doi.org/10.1016/j.jmacro.2021.103376,Cited by (8),We analyse the implications of asymmetric ==== rules by estimating Markov-switching ==== for the euro area (EA) and the US. The estimations show that until mid-2014 the ECB’s response to ,"The debate about positive and normative aspects of asymmetric monetary policy frameworks, both in an academic context as well as in central banks’ communication, has recently intensified. Have central banks so far responded (a)symmetrically to macroeconomic conditions? And, should central banks respond (a)symmetrically to macroeconomic conditions?====We contribute to the debate on asymmetric monetary policy by developing a Markov-switching DSGE model. We build on the DSGE model of Smets and Wouters (2007) and extend it to allow for asymmetry in the policy response to macroeconomic conditions. We introduce a second nonlinearity to also account for the lower bound on nominal interest rates.====In the euro area the debate on the degree of (a)symmetry of the ECB’s price stability framework goes back to the early days of monetary union and has recently re-emerged. Early academic contributions are, for instance,  [41], who emphasises that the ECB’s definition of price stability is “asymmetric”, and Begg et al. (2002), who state that the 2% inflation aim is a “ceiling” rather than a target for the ECB. More recently, Hartmann and Smets (2018) estimate policy rules for the ECB over the period 2000–2018, Rostagno et al. (2019) over the period 1999–2008 and Paloviita et al. (2017) over the period 1999–2014. They find evidence of asymmetric policy responses, but its specific form remains controversial. For instance, Hartmann and Smets (2018) find that the ECB tightened interest rates mainly in response to expected inflation above its inflation aim while it eased policy mainly in response to an expected slowdown in growth. Rostagno et al. (2019) and Paloviita et al. (2017) find more forceful reaction to inflation overshooting than undershooting of a 2% target. At the same time, the latter two contributions demonstrate that this is difficult to distinguish from alternative specifications in which the policy response to inflation is symmetric around a lower inflation target.====Rostagno et al. (2019) argue that an asymmetric response to overshooting and undershooting of inflation from target may be the direct result of the ECB’s quantitative approach to its price stability objective.==== The reason is that the ECB framework featured a definition of price stability in terms of an inflation range between 0% and 2% as well as an inflation aim below but close to 2%. As the inflation aim was not in the middle of the inflation range but close to its upper edge, this may have created an asymmetry, with the ECB responding more strongly to inflation above the aim than below it.==== They also argue that the estimated asymmetric response may have been beneficial in keeping inflation in check in the face of the prevailing inflationary pressures hitting the euro area in the first ten years of the ECB’s existence (1999–2008). But they emphasise that it may have contributed to persistently low inflation when shocks turned disinflationary after the global financial crisis and the sovereign debt crisis in some euro area countries.====The ECB officially communicated for the first time in July 2019 about whether its policy is symmetric. It stated that it follows a symmetric approach around its inflation aim: “the Governing Council is determined to act, in line with its commitment to symmetry in the inflation aim” (ECB, 2019). Some statements by the ECB’s former President Mario Draghi suggest that the symmetric approach may have been in place already earlier.==== Apart from the exact date in which the commitment to a symmetric approach may have started, it remains open whether symmetry was intended to characterise a symmetric response to inflation above and below its aim or the desire to achieve symmetric inflation outcomes. The latter may not necessarily require a symmetric policy response to inflation.====Bianchi et al. (2019) indeed point out that with a symmetric price stability objective, optimal policy in the presence of the lower bound on nominal interest rates and a low level of the natural real interest rate, ====, calls for a specific type of asymmetric policy rule: one in which policy responds less forcefully to inflation above its target than below target.====In the US, the 2012 FOMC’s Statement on Longer-Run Goals and Monetary Policy Strategy referred to a “symmetric inflation goal”, while the new policy framework announced in August 2020 consists of a makeup strategy defined by Vice Chair Clarida as temporary price-level targeting at the effective lower bound. He stated that “the new framework is asymmetric” (Clarida, 2020a), and explained that “In other words, the aim to achieve symmetric outcomes for inflation …requires an asymmetric monetary policy reaction function in a low ==== world with binding ELB constraints in economic downturns” (Clarida, 2020b).====We make three main contributions.====Our first contribution is to estimate the model on euro area and US data to assess whether, over the last two decades, there is evidence of nonlinearity in the ECB’s and the Fed’s policy response. For the ECB, we estimate the model allowing for endogenous switching of the coefficient on inflation in the policy rule depending on whether inflation is above or below target. We find that up until mid-2014 the policy response can be characterised as asymmetric by responding more strongly to inflation above 1.9% than below it. We show that such an asymmetric response to inflation can generate adverse interactions with the lower bound on nominal interest rates in a low ==== environment. When agents in the economy foresee the policy rate hitting the lower bound, they start saving in advance in order to smooth consumption through the lower bound period during which policy is not able to react. When monetary policy is asymmetric by responding less strongly to inflation below target than above target, the downward bias in inflation created by the lower bound is amplified. In our general equilibrium model with endogenous switching agents are aware of the possibility of regime changes and understand under what conditions the regime switches take place. As a result, they form expectations taking regime switches into account. By this mechanism, preemptive policies are effective (Davig and Leeper, 2006). For instance, if the central bank communicates that policy will tighten when inflation is above target, and this is credible, inflation can be stabilised with little movement of the policy rate. In addition, symmetric shocks can produce asymmetric effects. The asymmetry arises from the differential response of the monetary authority rather than nonlinearities in the structure of the economy.====We also find that since mid-2014, when the ECB started deploying a range of non-standard policy measures – which we capture by using a shadow interest rate – the ECB response is consistent with having become symmetric by responding to inflation below target as strongly as it responded to inflation above target prior to mid-2014. Our structural model allows us to provide a quantification of the macroeconomic relevance of this switch. We find that, had the ECB post-2014 kept following an asymmetric policy in line with the one estimated pre-2014, compared to their actual realisations over the period 2014–2019 inflation would have been up to 20 basis points lower and the output gap up to 70 basis points lower.====For the Federal Reserve, we take an agnostic approach, and let the regime switches be constant and not dependent on any endogenous variables. Specifically, we estimate the model over the last two decades allowing all policy parameters to switch and assume that the regime switching probabilities are constant and therefore that the economy switches exogenously between two regimes. This is motivated by the observation that, prior to August 2020, the Federal Reserve’s monetary policy strategy and communication have not emphasised asymmetric elements in the policy reaction. We find evidence of an asymmetric policy response, but of a different nature compared to the euro area. The Fed’s response to inflation is stronger in a regime that appears correlated with high financial stress. This suggests that the Fed has responded to deteriorating financial conditions over and above macroeconomic conditions perhaps out of fears of tail risks.====Our second contribution is to quantify the optimal degree of policy asymmetry in response to macroeconomic conditions using the estimated models for the euro area and the US. We assume a symmetric price stability objective and include the lower bound on nominal interest rates. Due to the lower bound, when the steady state level of the interest rate is low as in our exercise, monetary policy may not entirely counter disinflationary shocks with its policy rate. As a result, average inflation may be below the inflation target. We find that in such an environment the central bank should adopt an asymmetric response by responding more forcefully to inflation below target than above target. This type of inversely asymmetric policy rule is analysed also in Bianchi et al. (2019), who arrive at similar conclusions in a calibrated model. We contribute to this literature by providing a quantification of the optimised response to inflation, the output gap and the policy-rule persistence parameter depending on whether inflation is above or below target using a fully estimated model.====Our final contribution is to conduct counterfactual simulations to assess how, over the last two decades, the euro area and US economies would have performed had their respective central banks followed the optimal asymmetric policy response that we have derived: In both the euro area and the US, inflation and the output gap would have been better stabilised. For the euro area, we find that inflation would have been about 30 basis points higher after the 2008 crisis, while the output gap would have closed more quickly. For the US, we find that inflation would have been slightly higher after the burst of the dot-com bubble, and around 30–40 basis points higher after the global financial crisis. The output gap would have been better stabilised, falling only to around ====2% during the global financial crisis instead of ====.====The paper is organised as follows. In Section 2, we present the model and the regime switching monetary policy process. In Section 3, we report the estimation approach and data. In Section 4, we document the estimation results for the euro area and assess the macroeconomic implications of the estimated asymmetric policy response. We also derive optimal policy for the euro area. In Section 5, we present the estimation results for the US and derive optimal policy for the US. In the last section we offer conclusions.",Asymmetric monetary policy rules for the euro area and the US,https://www.sciencedirect.com/science/article/pii/S0164070421000756,16 October 2021,2021,Research Article,96.0
Sever Can,"International Monetary Fund, United States of America","Received 7 May 2021, Revised 19 September 2021, Accepted 27 September 2021, Available online 8 October 2021, Version of Record 13 October 2021.",https://doi.org/10.1016/j.jmacro.2021.103373,Cited by (0),"This paper shows evidence that political booms, defined as the rise in governments’ popularity, are associated with a higher likelihood of currency crises. The reasoning behind this finding is that prudent economic policies to address underlying weaknesses in the economy may be political costly for incumbent governments in the short-term. Hence, popularity-concerned governments may not have enough incentives to take such corrective actions in a timely manner. This approach, in turn, can deteriorate economic fundamentals and increase related risks in the economy which can eventually lead to crises. This paper sheds light on this phenomenon in the case of currency crises, suggesting that currency crises can be viewed as “political booms gone bust” events. Moreover, it finds that higher ====, higher exports, and a higher degree of financial openness alleviate the effect of political booms on currency crises.","Currency crises are associated with large and persistent economic losses (Cerra and Saxena, 2008).==== Hence, it is crucial to understand predictors of such damaging events to help avoid future crises and to provide cushion against their adverse consequences on the economy. The extant literature on early warning indicators documents a set of macroeconomic variables that help predict crises.==== However, the evidence on currency crisis predictors in the realm of political factors is scarce. This study focuses on the role of political booms, defined as the rise in governments‘ popularity, in predicting currency crises.====The evidence suggests that currency crises are generally “political booms gone bust” events. In particular, I find that political booms are associated with a higher probability of currency crises. The reasoning behind this is the following. Popularity-concerned governments may not take decisive corrective actions even though they may be aware of underlying weaknesses in the economy, since regulation may be politically costly in the short-term. Instead, they may lean towards a course of action (or inaction), which may not be ideal for the economy, but can help them boost their support in the short-term. This, in turn, can deteriorate economic fundamentals, and result in unintended consequences, i.e. higher risks that eventually lead to a country’s currency crash. I also show that international reserves, exports and financial openness can alleviate the effect of political booms in the likelihood of currency crises.====Although economic signals may occasionally be mixed, they are apparent in many cases, and can be well observed at least by policy makers who are more attentive to the economic fundamentals than the public.==== If there are early warning indicators of crises, why do policy makers not take decisive actions and corrective steps to minimize the risk of such damaging events in the first place? It is argued that generally the lack of political will, rather than the lack of information, prevents the implementation of responsible crisis prevention policies and corrective actions (Herrera et al., 2020).====Consider a scenario of an ongoing credit boom for which government can take credit and earn political dividend from it. It would be politically costly to smooth it with policy actions, since such response would prevent government from harvesting related political gains in the short-term. This may incentivize short-sighted behavior by incumbent politicians, which would be problematic for the economy under some conditions. Specifically, governments have better and more comprehensive information on economic fundamentals relative to the public, and therefore are likely know how sound an ongoing boom is. If booms signal deterioration of economic fundamentals (e.g. when driven by speculative events rather than an increase in productivity, as described by Gorton and Ordoñez, 2014), then the ideal option for the economy is to regulate those by taking decisive actions and applying corrective measures beforehand. By doing so, governments can curb the underlying weaknesses in the economy, and in turn prevent, or at least decrease the likelihood of, a future crisis. Such prudent policies and corrective measures, however, may not be optimal for government considering in its short-term reputation. For instance, they may disclose the public that the boom, for which government naturally takes credit to spur its own reputation, is not sound. Moreover, it can signal that incompetence of the government did generate an unsound credit boom in the first place, which would adversely affect public’s approval for incumbent politicians. Thus, instead of regulation, a reputation-concerned government may choose to ride booms in order to reap political benefits from it — even when they are unsound, at the costs of higher risks for unintended consequences, i.e. higher likelihood of a future crash.==== Governments’ actions (or lack of actions) leading to, or riding, unsound credit booms may trigger currency crises, since credit booms are found to be a good predictor of currency crises (e.g. Mendoza and Terrones, 2012).====There are other mechanisms through which (a mix of) policy choices driven by governments’ popularity concerns may weaken economic fundamentals eventually leading to currency crises. For instance, in an economy which imports final products and intermediate goods, interventions in the foreign exchange market using international reserves to avoid a depreciation can deplete reserves, lead to an overvalued currency, and in turn make the country more vulnerable to future shocks and speculative attacks. However, it can be politically advantageous in the short-term, since it can help governments keep inflation lower and reap political dividend from it, at the cost of increased risk of a crisis. Similarly, pro-cyclical fiscal policies to spur short-term growth and gain public’s approval can deteriorate the fiscal balance, increase sovereign debt and as a result, generate uncertainty about sustainability. It can also accelerate domestic inflation by increasing aggregate demand. Such exhaustion of domestic fundamentals can affect market expectations for forward economic outlook, as well as investor sentiment, and predispose the country to speculative capital outflows and sell-off in the foreign exchange market, which would trigger a crisis. These exemplify how governments’ actions to boost their popularity can lead to financial, fiscal or price instability that would put a pressure on a country’s currency. In this regard, whether political booms are indeed associated with a higher likelihood of currency crises is an important and open empirical question.====In this paper, using data from 99 advanced, emerging market and developing economies and 85 currency crises between 1984–2016, I shed light on the effect of political booms — defined as “the rise in governments’ popularity” on the likelihood of currency crises. The main challenge to study government popularity, or government support, in a cross-country setting is the lack of data dating back to earlier periods. If we have reliable data on public’s approval for governments, such as poll data, that would be the ideal source to measure government popularity. However, such data is available neither for a large set of countries, nor for long enough time period. To overcome this issue, I proxy changes in government popularity using the “index of government stability” from the International Country Risk Guide (ICRG) by following the innovative approach by Herrera et al. (2020).====I first document stylized facts on that public’s opinion on governments remarkably improves in the run-up to currency crashes. Empirical evidence similarly shows that political booms are a significant predictor of currency crises on top of well-known macroeconomic signals. Estimates suggest that one standard deviation increase in government popularity almost doubles the overall crisis probability in the sample. This result on the role of political booms as a crisis predictor survives through a large set of robustness checks. Importantly, I show that it is not driven by banking crises which may precede currency crises. Lastly, potential alternative explanations, such as political uncertainty, are not likely to drive the result.====Next, I explore the interplay between political booms and ex-ante macroeconomic factors in determining the likelihood of currency crises. Given that several macroeconomic variables can help predict currency crises as documented by the existing literature, it is interesting to shed light on how those predictors affect the likelihood of crises in light of political booms. I find that higher international reserves, higher exports and a higher degree of financial openness help alleviate the effect of political booms of the occurrence of crises. Thus, the strength of the relationship between political booms and the likelihood of crises can be influenced by several macroeconomic factors.",Political booms and currency crises,https://www.sciencedirect.com/science/article/pii/S0164070421000720,8 October 2021,2021,Research Article,97.0
Ozhan Galip Kemal,"Bank of Canada, 234 Wellington Street, Ottawa, ON K1A 0G9, Canada","Received 1 December 2020, Revised 23 August 2021, Accepted 16 September 2021, Available online 25 September 2021, Version of Record 30 September 2021.",https://doi.org/10.1016/j.jmacro.2021.103372,Cited by (1),"How does news about future economic fundamentals affect within-country and cross-country credit allocation? How effective is unconventional policy when financial crises are driven by unfulfilled favorable news? I study these questions by employing a two-sector, two-country ","The consensus narrative of the economic crisis in the eurozone in 2000s is that it was a “sudden stop” crisis (Baldwin and Giavazzi, 2015). Bank credit expanded massively in the European periphery with the announcement of the euro, which was fed by foreign borrowing of financial institutions (see Giavazzi and Spaventa (2010)). A major slice of these loans were toward the non-traded sectors of the economy (see Ozhan (2020)). The surge in capital flows ended with the Global Financial Crisis (GFC), which is linked with a loss of confidence. Capital flows reversed, bank balance sheets shrank, investment and output collapsed, and credit spreads of non-financial firms rose steeply.====Economists and policymakers often conjecture that the surge in borrowing and expansion of output in the aftermath of inauguration of the euro resulted from expectations of a future favorable state of the economy related with the entry in the eurozone (Blanchard, 2006, Blanchard and Giavazzi, 2002, Constâncio, 2005).==== Indeed, several empirical studies indicate that increase in firm valuations started after the European Council’s decision in 1998 with the announcement of the news about which countries were allowed to enter the final phase of Europe’s Economic and Monetary Union (EMU) (see Bris et al., 2009, Bris et al., 2014), before the inauguration of the euro. The results are stronger for firms in construction and service industries than for those in manufacturing. Regarding the divergence of valuations between sectors, Santos (2014) argues that the close link between politicians and financial institutions led to a belief in public that non-traded sectors will gain more from the benefits of the single currency as it is easier for politicians to be reelected by riding short-term prosperity delivered from non-traded sectors.====Against this background, I present an open economy model of financial intermediation in which I study financial crises associated with occasionally binding leverage constraints. The model closely follows the structure of Ozhan (2020). Financial intermediaries extend credit to domestic traded and non-traded non-financial sectors, and borrow from both domestic and international markets. The ability of intermediation is limited due to a moral hazard problem that places an endogenous restriction on bank leverage ratios. The key difference from Ozhan (2020) is the non-linear model solution that allows for the endogenous leverage constraints to be occasionally binding depending on the level of net worth of financial intermediaries. I also focus on news about economic fundamentals as the key source of fluctuations (in contrast with Ozhan (2020)). Specifically, optimistic news on the valuation of non-traded sector assets triggers the boom part of the cycle, and later unfulfillment of this optimism induces the bust. Hence, the boom–bust period is expectation-driven.====A calibrated version of the model reasonably captures the pattern of the changes in the current account and the asymmetric boom and the bust in bank credit in Spain between 2000–10. Following a positive news on the future value of non-traded sector assets, the model generates a skewed allocation of bank credit and investment toward non-traded sectors with a persistent and climbing current account deficit. When expectations are not met, private capital flows suddenly reverse, aggregate output and bank credit collapse with a jump in borrowing costs of non-financials. A huge correction in the current account takes place. The disproportionate allocation of foreign borrowing toward the non-traded sectors makes the external accounts more stringent, because past foreign liabilities do not match with expected surpluses.====To disentangle the contribution of bank balance sheets and cross-country capital flows, I further compare the baseline model with two other model versions: the version with perfect intermediation and the version with international financial autarky.==== Comparison of model versions with banks and lack thereof shows that inclusion of banks does not matter significantly in the boom regime. Bank net worth is sufficiently high in good times and the financial frictions do not apply in these periods. However, when expectations turn out to be incorrect, in the version with financial frictions, sudden reversal of capital flows brings the economy into the financial crisis regime (====, banks’ constraints start to bind). Under binding constraints, bank balance sheets transmit fluctuations across sectors and spread the recession to the overall economy, although the boom regime was mainly driven by the non-traded sector. I call this the ====-national spillover channel. When I compare the baseline model with the version under international financial autarky, I show that the cross-country capital flows contribute significantly to the dynamics both during the boom and the bust regimes. Bank balance sheets get larger in the case of international integration, and the leverage constraints are more effective in generating financial amplification and the intra-national spillover. It is also important to note that the deviation in the uncovered interest parity condition due to financial imperfections is key to generate data consistent dynamics in the baseline version.====I study unconventional policy in the form of direct asset purchases from non-financials and/or liquidity injections to the banking sector. First, the model does a quantitatively better job in capturing the bust periods when policy is in place, suggesting that the European Central Bank’s (ECB’s) response to the early phase of the eurozone crisis was effective. Under policy, the correction in the current account is milder because private outflows are replaced by public inflows. Second, the model also suggests that liquidity facilities directed toward the banking sector perform better at ameliorating the downturn than direct asset purchases. The reason that liquidity injections are more effective is due to the occasionally binding constraints that push the spreads between lending and borrowing rates up when there is disappointment in positive news. Positive spreads enable the government to lend to financial intermediaries at a penalty rate, and the proceedings from government lending are effectively distributed between the sectors (in contrast to purchasing assets only from the non-traded sector).====The main contributions of this paper are as follows. The first contribution is providing a quantitative, model-based analysis of a news-based assessment of the eurozone boom–bust cycle. In a concurrent work, Siena (2021) studies the role of news shocks in generating current account deficits abstracting from financial frictions. This paper also studies the regime switching between the build-up of imbalances and the crisis, instead of studying only the boom regime.====The second contribution of this paper is disentangling the role of bank balance sheets and international financial integration in transmission of news shocks under occasionally binding constraints. Görtz et al. (2021) highlight the amplified effects of news shocks by financial frictions. In this paper, I further study how unmaterialized news can bring the economy into the crisis regime and show the quantitative contribution of the international capital account in generating this outcome. The literature on news-driven business cycles also faced major challenges in generating empirically plausible co-movement of aggregate variables within the economy.==== In my model, the problem is overcome through the inclusion of balance-sheet constrained banks and news about future capital quality (or valuation), instead of future productivity.====The third contribution is studying unconventional policies in response to unfulfillment of favorable news. To the best of my knowledge, there is no paper that studies the effects of unconventional policies under such scenario.==== I also compare liquidity injections from direct asset purchases. Another paper which compares these two unconventional policies is Ozhan (2020). He finds that asset purchases are more favorable to liquidity injections when crises are associated with tightening of financial constraints. This paper shows that opposite results may emerge when crises are associated with occasionally binding constraints.====The rest of the paper is organized as follows. Section 2 presents the model. Section 3 discusses the outcome of the model without policy, when calibrated to a standard open economy. Section 4 describes the policy authority’s unconventional policy and discusses its results. Section 5 concludes.",News-driven international credit cycles,https://www.sciencedirect.com/science/article/pii/S0164070421000719,25 September 2021,2021,Research Article,98.0
Lee Jangyoun,"International Department, Bank of Korea, Seoul, 04531, South Korea","Received 24 March 2021, Revised 28 August 2021, Accepted 1 September 2021, Available online 20 September 2021, Version of Record 24 September 2021.",https://doi.org/10.1016/j.jmacro.2021.103371,Cited by (1),This paper investigates why the upsurge of top income shares has coincided with economic slowdowns in the US since the late 1970s. I argue that a fast-growing unearned income from ‘wealth residual’ – the unexplained increase in ,"In the middle of the twentieth century, the belief that economic growth would bring about an increase in aggregate wealth and higher living standards for society as a whole became widespread. Indeed, during the post-World War II period, there existed some evidence supporting this belief, especially for the US (this time is often referred to as ‘the Golden Age of American Capitalism Marglin and Schor, 1992’). After the late 1970s, even though aggregate wealth in the US has rapidly expanded, the prior trend towards greater income equality suddenly reversed (Atkinson, 2015). For instance, real gross domestic product (GDP) per capita in the US grew at an average annual rate of 1.7 percent during 1978–2015 – much lower than the 2.7 percent seen from 1959–1977 – while the share of the top 1% income was about 22.0 percent in 2015, compared to only 9.0 percent in 1977. The 1978–2015 period was also characterised by a slowdown in corporate investment. Net fixed capital formation in the non-financial corporate sector was, on average, 6.5 percent of gross value added and 4.5 percent of net capital stock from 1959–1977. These numbers fell to 5.1 percent and 3.2 percent, respectively, from 1978–2015.====Why has the upsurge of top income shares coincided with economic slowdowns in the US after the late 1970s? Wealth-to-income ratios, which have significantly risen in comparison to the capital-to-income ratio since 1978,==== provide a clue in this regard. As in Fig. 1, the wealth-to-income ratio remained stable at approximately 4.5 until 1978, but substantially increased up to around 6.3 in 2015, mainly due to increases in the price of equities and real estate. In contrast, capital-to-income ratio relatively remained flat. Concurrently, there has been an upsurge of income in the richest 1% group during the same period. This paper attempts to investigate the gap between wealth and capital, referred to as ‘wealth residual’ – the unexplained increase in wealth without a corresponding increase in investment (Stiglitz, 2015a).==== I then examine the role of wealth residual in the concurrence of rising top income inequality (hereafter, “inequality”) and falling growth rates of real income per capita (hereafter, “growth”) during the last four decades.====A growing body of recent studies (e.g., Stiglitz, 2015a, Rognlie, 2015, Brun and Gonzalez, 2017, Eggertsson et al., 2018, Fagereng et al., 2019) suggests that the decoupling of wealth and capital poses challenges for the general theory of an economy driven by the underlying laws of demand and supply, focusing on labour and capital only. In standard models, in which firms accumulate capital and households buy corporate securities, the aggregate market value of these two variables is expected to move in the same direction. Capital is the part of firms’ net worth that yields the flow of future dividends capitalised in the market value of corporate securities. So, in the absence of frictions, these two aggregates are equal and their ratio (the so-called “Tobin’s Q”) should equal one in equilibrium. However, historical values of Tobin’s Q during the post-1978 period have been neither constant nor equal to one, implying that wealth has been largely accumulated through ‘persistent capital gains’ in the stock and housing markets rather than through savings (Gutierrez and Philippon, 2017, Eggertsson et al., 2018, Fagereng et al., 2019, Fagereng et al., 2020). This general theory also struggles to generate results that fit the stylised macroeconomic facts of the post-1978 period (Atkinson, 2015, Stiglitz, 2015a, Eggertsson et al., 2018).====In a series of innovative papers, Stiglitz, 2015a, Stiglitz, 2015b, Stiglitz, 2015c and Stiglitz (2015a) tries to resolve this puzzle by introducing a new concept of wealth residual that is not related to the production of real output but is instead associated with asset ownership, giving rise to economic rents==== from land and monopoly. This paper is, I believe, the first to present the estimate of wealth residual for the US using the national accounts and the associated statistics: wealth residual increased by 59.3 times between 1978 and 2018, while capital increased by only 8.7 times. As a result, the ratio of wealth residual to total wealth in 2018 was 39.7 percent, much larger than the 8.8 percent figure in 1978. More importantly, due to a privileged access to wealth residual, the owners of wealth residual can obtain additional wealth gains at the expense of their counterparts through rentier premium — the returns over and above the returns on capital in a competitive market (Lee, 2021). As a result, while owners of equities and land benefit greatly from the rise in its value and from increasing returns on wealth, households whose income depends mostly on labour suffer from weak capital formation.====The important source of the disparity between the growth of wealth and that of capital is land (Piketty, 2014, Stiglitz, 2015a, Stiglitz, 2015b, Rognlie, 2015, Fagereng et al., 2019). More specifically, much of the increase in aggregate wealth is due to an increase in the ==== of land, which is only rarely related to an increase in the ==== of land. The increase in the value of land is generally not the result of the creation of more land but rather merely due to the increase in the ==== of existing land. Since the appreciation of land value reflects a desire for its positional value (e.g., access to high-quality education or scenic views), as first explored in Hirsch (1977), attempts to acquire land can only benefit one agent at the expense of another agent, resulting in a zero-sum game without any increase in the productive capacity of the economy — referred to as ‘scarcity rent’ that is evidently capitalised in non-financial assets (e.g., Stiglitz, 2015a, Rognlie, 2015). Another crucial source of wealth residual with a similar mechanism is rent-related finance – financial assets that do not contribute to the increase in (productive) capital or land – referred to as ‘monopoly rents’ that are unobtrusively capitalised in financial assets (e.g. Stiglitz, 2015a, Brun and Gonzalez, 2017, Lusardi et al., 2017, Philippon and Reshef, 2012).====Based on these theoretical and empirical backgrounds, this paper presents the wealth residual hypothesis that a fast-growing unearned income==== from wealth residual is associated with the concurrence of surging inequality and sluggish growth over the last four decades. To support this hypothesis, I assemble a much larger and more comprehensive panel dataset for the US at the state level on wealth residual, inequality, growth, and relevant environmental and policy variables, such as human capital, innovation, tax, regulations, labour bargaining power, and political regimes, than has hitherto been available. More importantly, the use of the US state-level data is much superior to the use of cross-country data in assessing the empirical relationship among economic variables because cross-state heterogeneity is much smaller than cross-country heterogeneity. Accordingly, this method allows us to avoid problems inherent in postulating a common parametric structure for cross-country samples that include countries at different stages of economic development and/or with different characteristics (e.g., size, specialisation). Finally, this method not only mitigates the quality and comparability issues inherent in the use of cross-country data but also helps us alleviate significant problems of endogeneity, omitted variables, and measurement errors often generated from cross-country analyses (Banerjee and Duflo, 2003).====This study reveals the existence of a wealth residual mechanism within the post-1978 growth-inequality relationship: after wealth residual increased, growth and inequality became worse at the same time. This is partly because while owners of wealth residual benefit greatly from the rise of its value and from increasing equity returns, households whose income depends mostly on labour suffer from the stagnation of capital formation. In addition, given the two-party political system of the US in reality, this study shows that the effect of wealth residual on top income inequality is dampened when labour-friendly governments assume state control. Finally, using discrete choice models, this study finds that a labour-friendly government tends to win elections over a rentier-friendly one when both wealth residual and inequality are maintained at low levels.====This paper contributes to the literature by measuring wealth residual and empirically examining the wealth residual hypothesis based on a set of panel regressions, controlling the state- and time-fixed effects, and the relevant control variables. Moreover, this paper has potential implications for future empirical work. Since the methodological framework adopted by this study is not sufficient to eliminate the omitted variable problem completely, the empirical results can be seen as conditional correlations rather than causal effects. It would therefore be interesting to test the wealth residual hypothesis by using a more compelling case for identification of causal effects (e.g., structural model, natural experiment, or instrumental variables).====Overall, this study concludes that the rapid increase in wealth residual has contributed to a co-evolution of fast-growing inequality and economic slowdown, as the increase in wealth residual has rarely contributed to the expansion of productive capacity of the economy, partly due to intensified positional competition. Attempts to acquire wealth residual can only benefit one agent at the expense of another agent, resulting in a zero-sum game without any increase in the productive capacity of the economy (the so-called “exploitation mechanism”– Stiglitz (2015a)). In other words, the force of the exploitation mechanism seems to have crowded out the accumulation of capital in the US since the late 1970s.====The rest of the paper is organised as follows. Section 2 presents the wealth residual hypothesis to spell out its empirical implications, Section 3 describes the data, and Section 4 presents the empirical results. The last section concludes the paper.==== The starting point of this paper is Piketty (2014), who concluded that the upsurge of wealth-to-income ratios was not accompanied by decreasing returns on capital, yielding the over-accumulation of capital and growing top income inequality. However, debates have arisen regarding several of his assumptions — in particular, the elasticity of substitution between capital and labour (e.g., Chirinko, 2008, Rowthorn, 2014, Chirinko and Mallick, 2017) and the equation of the concepts of capital and wealth (e.g., Stiglitz, 2015a, Stiglitz, 2015b, Eggertsson et al., 2018). In this respect, this paper is closely connected to Stiglitz (2015a), who argues that wealth and capital are distinctly different concepts. If wealth is equated with capital, the increase in capital should be associated with a decline in the return to capital and an increase in real wages. However, this hypothesis is contradicted by the macroeconomic data of the post-1978 US economy. To address this issue, Stiglitz, 2015a, Stiglitz, 2015b, Stiglitz, 2015c and Stiglitz (2015a) proposes a theory that the disparity between wealth and capital (i.e., wealth residual) might be partially due to an increase in scarcity rents, evidently capitalised in non-financial assets (e.g., land), and monopoly rents, unobtrusively capitalised in financial assets. I further develop his theory by empirically analysing the effect of wealth residual on the existing growth-inequality relationship, using the first-ever estimate of wealth residual of the US.====This study also resonates with the recent literature on puzzling trends in the post-1978 US macroeconomic data that has overturned Kaldor (1961)’s famous stylised facts, such as constant interest rates, a constant labour share, and a constant capital-to-income ratio. Notably, Piketty and Zucman (2014) document evidence that wealth-to-income ratios have risen over the past forty years, as Tobin’s Q has increased. They decompose the increase in wealth into two components – a saving component and a valuation component. The saving component can be described as the equation ====, where ==== is the ratio of the wealth-to-income ratio in the long run, ==== is the saving rate, and ==== is the growth rate. There can be valuation effects in the short run as the price of capital goods (i.e., Tobin’s Q) increases. In line with this argument, Fagereng et al. (2019), using Norwegian administrative panel data on income and wealth, show that saving rates including capital gains (“gross saving rates”) increase markedly with wealth, while saving rates net of capital gains (“net saving rates”) are approximately constant across the wealth distribution. This implies that wealthier households own assets that experience persistent capital gains, and that they hold on to these assets instead of selling them off to consume. They term this phenomenon “saving by holding”.====Meanwhile, Eggertsson et al. (2018) argue that an increase in the wealth-to-income ratio, an increase in Tobin’s Q, and a divergence between the marginal and the average returns on capital can be explained by an increase in monopoly rents. Similarly, Karabarbounis and Neiman (2013) and Barkai (2020) provide evidence that the capital share and the labour share for US income declined at the same time, while there is an increase in the mark-ups or what Karabarbounis and Neiman (2018) call factor-less income — income which accrues neither to labour nor to capital. In contrast, Rognlie (2015) demonstrates the dominant role of scarcity rents from real estate in explaining these post-Kaldor’s stylised facts.",Behind rising inequality and falling growth,https://www.sciencedirect.com/science/article/pii/S0164070421000707,20 September 2021,2021,Research Article,99.0
Bianco Timothy,"Department of Business and Economics, Allegheny College, Meadville, PA 16335, United States of America","Received 19 November 2020, Revised 11 August 2021, Accepted 14 August 2021, Available online 1 September 2021, Version of Record 8 September 2021.",https://doi.org/10.1016/j.jmacro.2021.103362,Cited by (2),The paper assesses the impact of ,"In neoclassical macroeconomic models without financial frictions, credit markets are an afterthought; banks act as inconsequential intermediaries between owners and ultimate users of capital (Jorgenson, 1963, Tobin, 1969). In such models, credit markets are assumed to be fully efficient and capital flows to its most productive use. Monetary policy impacts real economic activity through the “cost-of-capital” channel alone (Bernanke, 2007). In reality, credit markets are not frictionless and can severely affect economic activity. Modern models incorporate financial frictions and have expanded monetary policy channels beyond those influencing market interest rates and bank capital.==== That is, monetary policy impacts borrowing firms’ costs of external financing==== and lending institutions’ tolerance and perception of risk.====While there is a large body of literature investigating the effect of monetary policy on credit market outcomes,==== a less explored aspect is policy’s impact on the reallocation of credit among borrowing firms. Existing theory implies that the composition of credit will adjust following a macroeconomic shock.==== Because outstanding credit fluctuates and gets reshuffled to others, assessment of credit market conditions through inspection of aggregate net credit changes alone is incomplete and potentially misleading. For instance, if credit extended to one firm increases by 5%, but as the result of a 5% decrease in credit to another, then the net credit change is zero. Yet, credit has been reshuffled between these two firms. Such reallocation may have important implications for future capital formation and economic growth (Herrera et al., 2014, Bianco and Herrera, 2021). Thus, an assessment of monetary policy’s impact on credit markets ought to account for this reallocation.====The objective of this paper is to explore the effect of monetary policy shocks on the flow of credit. Following Herrera et al. (2011) – hereafter HKM –, I use Compustat North America data to compute credit flows for borrowing firms. Use of this data has several advantages. First, credit extended by nonbank financial institutions like private equity firms, investment banks, or insurance companies is included in the data.==== Second, Compustat data contains firm-level income statement and balance sheet information, which can be used to obtain relevant financial characteristics of the borrowing firm. This will aid in evaluating the importance of distinct monetary policy transmission mechanisms. For instance, credit flows may be larger and more fluid for firms that are highly dependent on external financing. Therefore, the reallocation of credit among these firms may be of a larger consequence for economic activity than those capable of generating cash flow internally. An additional benefit of Compustat data is that a firm’s total debt is classified into the short- and long-term components.==== Typically, short-term debt is used to provide cash flow for current expenses rather than to finance long-term investment projects. However, short-term, non-intermediated debt can be a relevant source of bridge financing for long-term projects (Kahl et al., 2015). Further, as Guedes and Opler (1996) document, borrowing (i.e. debt issuance) at both very short and long maturities are common features of large, highly rated firms, which account for a large portion of the U.S. output.====Nevertheless, there are disadvantages to using Compustat data. First, the data cannot distinguish between the type of borrowing, other than whether it is short- or long-term credit. Second, Compustat only includes information on publicly traded firms, which tend to be large and developed. With Compustat, credit movement among small, non-publicly traded firms is ultimately excluded. Over the period analyzed, Compustat contains an average of 45 percent of the U.S. non-financial sector credit, as reported by the Bank for International Settlements.==== Compustat’s coverage has increased over time, rising to an average of 64 percent since the first quarter of 2000. Even though a substantial portion of credit is absent from Compustat, this data provides a good testing ground for exploring the impact of monetary policy on credit flows.====The topic of credit reallocation was first addressed by Dell’Ariccia and Garibaldi (2005) using bank lending data. They find that credit expansion and contraction tend to co-move, specifically among banks of comparable size, loan type, and location. HKM draw similar conclusions by analyzing firm borrowing, rather than bank lending. They find that credit reallocation among borrowing firms is intense, volatile, procyclical, and highly concentrated among firms of comparable size, location, and industry. A connected study by Craig and Haubrich (2013) investigates the substantial bank consolidation in the 1990s and analyzes bank loan creation, destruction, and reallocation in and out of recessions using data from bank balance sheets. They observe that loan creation is higher during expansions and that loan destruction is higher during recessions.====These studies either do not consider the impact of monetary policy on the reallocation of credit or do so only in a tangential manner. In this paper, I show that monetary policy’s impact on the flow of credit is large, and that the impact depends on specific firm characteristics and the length of debt maturity. Following expansionary monetary policy, credit tends to flow to borrowers that are generally considered financially constrained or perceived as relatively risky to the lender. This is consistent with parts of the balance sheet and risk-taking channels of monetary policy.====This paper is organized as follows. Section 2 describes the construction of credit flows, as well as trends in credit flow and monetary policy measures. Section 3 summarizes the theoretical background for the expected response of credit flows to monetary policy and documents heterogeneity in credit flows. Section 4 outlines the empirical framework and discusses the impact of monetary policy’s influence on credit flows. Section 5 concludes.",Monetary policy and credit flows,https://www.sciencedirect.com/science/article/pii/S016407042100063X,1 September 2021,2021,Research Article,101.0
"Busetti Fabio,Neri Stefano,Notarpietro Alessandro,Pisani Massimiliano","Bank of Italy, DG for Economics, Statistics and Research, Italy","Received 1 December 2020, Revised 8 July 2021, Accepted 18 August 2021, Available online 1 September 2021, Version of Record 13 September 2021.",https://doi.org/10.1016/j.jmacro.2021.103366,Cited by (10)," and output and reducing the duration and frequency of ELB episodes. Temporary price level targeting is also effective in mitigating the ELB constraint, although its stabilization properties are inferior to those of price level targeting. Backward-looking average ==== targeting performs well and is preferable to inflation targeting. The effectiveness of these alternative strategies hinges upon the commitment of a central bank to keeping the policy rate “lower for longer” and is influenced by agents’ expectation formation mechanism.","An intense debate among academics and policy-makers is ongoing about monetary policy in the “new normal”, a macroeconomic environment in which the natural rate of interest may remain lower than its average value over the last five decades in the main advanced economies. The natural rate is the level of the real interest rate that is consistent with the economy growing at its potential rate and inflation being at the central bank’s target. Possible factors behind the new normal include a slower rate of technological progress, the demographic transitions associated with population aging, excess savings in emerging market economies, and an increased demand for safe and liquid assets.==== The low level of the natural rate limits the space available to the central bank for reducing the policy rate to stabilize the economy. In this context, periods in which the policy rate is at its effective lower bound (ELB) may be frequent and long-lasting, amplifying the negative effects of recessionary shocks.====Several proposals for conducting monetary policy in the new normal have been put forward in the literature. Some economists have suggested raising the inflation target (Ball 2013 and Blanchard et al. 2010). According to others, central banks should and could rely more extensively on non-standard monetary policy measures, such as forward guidance on the policy rate and the purchases of private and public securities to overcome the limitations posed by the ELB (Bernanke 2017). Another option that has been discussed is to change the monetary policy strategy, involving commitments akin to price level targeting.====In this paper, we simulate a medium-scale New Keynesia model calibrated to the euro area (EA henceforth) to evaluate the stabilization properties of monetary policy strategies alternative to inflation targeting when the natural rate is low and the probability of reaching the ELB is non-negligible. The model, similar to Smets and Wouters (2003), has been extensively used for analyzing monetary policy at central banks and by academic economists. We assume, consistent with the existing empirical evidence, that the natural rate in the EA is low.==== We use stochastic simulations to feed the model with demand and supply shocks calibrated to generate inflation and output volatilities in line with those of the corresponding EA data over the 1985–2014 period (i.e., before the ECB’s policy rate reached levels close the ELB).==== The benchmark monetary policy rule is consistent with flexible inflation targeting (IT, henceforth) and it is in line with the existing literature (see, for example, Smets and Wouters 2003 and Warne et al. 2008). The central bank adjusts the policy rate in a systematic way by responding gradually to deviations of inflation from the target and to output growth.====We compare the IT framework to the following alternative monetary policy strategies: (i) price level targeting (PLT); (ii) temporary price level targeting (TPLT); (iii) average inflation targeting (AIT). We evaluate these strategies along the following dimensions: the probability and duration of an ELB episode; the mean and the volatility of inflation and output; a quadratic loss function.====Under a PLT strategy, the central bank aims at keeping the price level aligned with a target path. The slope of this path is equal to the steady-state gross inflation rate, which is set to the central bank’s target. The key difference between PLT and IT is that, by ignoring past misses of the target, an IT central bank lets “bygones be bygones”. To the opposite, a central bank following a PLT strategy commits to reversing temporary deviations of the price level from the target path: a period in which inflation is above its target is expected to be followed by a period in which inflation is below target, and vice versa. Following Bernanke (2017) and Bernanke et al. (2019), we also consider a TPLT strategy according to which the policy rule changes from IT to PLT when the ELB becomes binding.==== Finally, we modify the benchmark IT rule by allowing the policy rate to react to the average of past inflation rates (AIT).====In the benchmark simulations, we assume that households and firms have model-consistent expectations and, thus, fully understand the monetary policy strategy the central bank is following. However, the effectiveness of PLT, AIT, and TPLT in stabilizing inflation strongly depends on the degree of forward-lookingness of inflation expectations. For example, following a disinflationary shock, the anticipation of a future temporary overshooting of the target and, thus, of lower future real interest rate, contributes to stabilizing current inflation. If this anticipation effect weakens, the effectiveness of the strategies is reduced. To assess the role of forward-looking inflation expectations, in a robustness analysis we decrease their degree of forward-lookingness by increasing the degree of indexation of current-period inflation to previous-period inflation and, simultaneously, decreasing the indexation to the inflation target. Moreover, Bernanke et al. (2019) suggest that the assumption of full credibility of the monetary policy strategy can be too strong during the transition to a new regime. If public expectations do not respond to central bank announcements, then all monetary policy strategies could be less effective during ELB periods. For this reason, in addition to the case of fully rational (i.e., model-consistent) expectations, we also consider, as a robustness, the case of “hybrid” inflation expectations, i.e. a weighted average of adaptive and rational expectations.====Our results are as follows. First, PLT is the most effective strategy in terms of stabilizing inflation and output and, thus, reducing the duration and frequency of ELB episodes. Second, a TPLT strategy is also effective in terms of likelihood and duration of ELB episodes, although its macroeconomic stabilization properties are inferior to those of PLT. Third, AIT reduces the frequency and duration of ELB episodes as PLT and TPLT and stabilizes inflation and output less than PLT and TPLT but more than IT. The effectiveness of PLT, TPLT, and AIT hinges upon the commitment of the central bank to keeping the policy rate “lower for longer” and it is influenced by agents’ expectations formation mechanism.====The literature on monetary policy strategies to deal with the ELB in the new normal has grown significantly over the last five years. Bernanke et al. (2019) simulate the FRB/US model to study the effectiveness of lower-for-longer policies when some agents are not forward-looking. The authors find that this feature reduces but does not eliminate the advantages of these policies. Kiley and Roberts (2017) find that, with a policy rule estimated on US data and under the assumption that the steady-state nominal policy rate is 3%, monetary policy may be constrained by the ELB as much as one-third of the time. This constraint in turn leads to inferior macroeconomic performance, as the central bank’s ability to hit its inflation target or to keep output near potential would be impaired. Schmidt et al. (2016) simulate a DSGE model of the U.S. economy and find that the tail risk induced by the ELB causes inflation to undershoot the target. The authors also show that achieving the target may be more difficult in the future, if the decline in natural rate has led households and firms to revise up their estimate of the frequency of future ELB events. Billi and Galí (2018) analyze the impact of the ELB constraint on the welfare effects of greater nominal wage flexibility under alternative monetary policy regimes (Taylor rule vs. optimal policy) using the canonical three-equation New Keynesian model and relying on a second-order approximation of the welfare function. Nakata et al. (2020) show, in a stylized New Keynesian model with an occasionally binding borrowing constraint, that a discretionary central bank that stabilizes an average inflation rate, rather than a period-by-period inflation rate, increases welfare. Under rational expectations AIT with a finite, but sufficiently long, averaging window can attain most of the welfare gain generated by PLT. Under boundedly-rational expectations, if cognitive limitations are sufficiently strong, the optimal averaging window is finite, and the welfare gain of adopting AIT can be small. Amano et al. (2020) simulate a two-agent New Keynesian (TANK) model in which a fraction of firms have adaptive expectations. They examine the optimal degree of history dependence under AIT and find it to be relatively short for business cycle shocks of standard magnitude and duration. In this case, the properties of the economy are quantitatively similar to those under PLT.====Our contribution to the literature on monetary policy strategies is twofold. First, compared with the existing studies, which are all based on the US, we study the EA case. The latter is an interesting case-study because of the very low level of the natural rate and the persistently below-target inflation. Second, we use a medium-scale New Keynesian model suitable for policy analysis taking into account the ELB constraint to systematically compare alternative monetary policy strategies. To the best of our knowledge, ours is the first paper to provide these two contributions.====The remainder of the paper is organized as follows. Section 2 describes the model and the different monetary policy strategies. Section 3 reports the calibration. Section 4 presents and discusses the results of the analysis. Section 5 reports the sensitivity analysis. Section 6 draws the conclusions.",Monetary policy strategies in the New Normal: A model-based analysis for the euro area,https://www.sciencedirect.com/science/article/pii/S0164070421000665,1 September 2021,2021,Research Article,102.0
"Parello Carmelo Pierpaolo,Ikhenaode Bright Isaac","Sapienza University of Rome, Department of Economics and Law, Via del Castro Laurenziano, 9, I -00161, Rome, Italy","Received 29 December 2020, Revised 19 August 2021, Accepted 24 August 2021, Available online 31 August 2021, Version of Record 5 September 2021.",https://doi.org/10.1016/j.jmacro.2021.103369,Cited by (1),"This paper proposes a Ramsey-Like model of growth with endogenous migration to study the effects of migration networks on the macroeconomy and welfare of hosting economies. In the model, migration is assumed to be made of two different components: a first, forward-looking component in which the rate of net migration depends on the wage gap between countries; a second, backward-looking component in which in-migration depends on the immigration history of the destination country through the formation of immigrant networking. We find that the model exhibits a unique saddle-path steady-state equilibrium and that introducing pro-immigration policies aimed at enhancing community networks have asymmetric impacts on the welfare level of natives and immigrants that hinge on the relative size of immigrant communities.","In the last decades, a large body of literature has established that community networks play a key role in affecting migrants’ ability to move abroad, find a job or start a business, and integrate into the host society. Yet, very little is known about the mechanisms through which immigrant networks can affect the macroeconomy and welfare of hosting economies. Ranging from universities promoting international programs to family reunification schemes that allow close relatives to live together within the same country, several institutions are nowadays in place to support organizational ties within migrant networks. A close examination of how migrants’ networks impact on the receiving economies is then crucial to design migration policies able to foster immigrants integration and promote economic growth.====In this paper, we propose a way to introduce migration networks into one-sector neoclassical growth models based on Bauer et al. (2007) and Dolfin and Genicot (2010), according to which community networks have a quadratic effect on the likelihood of migrating. Here our objective is twofold. On the one hand, we aim at testing whether immigrants’ networks are able to affect the macroeconomic dynamics of the host countries. On the other, we investigate to what extent community networks can increase natives’ and immigrants’ welfare. In this respect, our paper differs from the main body of literature on migration and growth in that we consider the eventuality that past migration can produce network externalities to future migration.====The addition of network externalities from past migration to our extended version of the Ramsey–Cass–Koopmans model of Neoclassical growth with international labor mobility has important implications for the stability and welfare properties of the host economies. In particular, the paper shows that network externalities and past immigration history are fundamental to shape the future streams of migrants and identify some threshold sizes of migrant communities. In our opinion, our model represents an enrichment for the literature since it allows to plug into one theoretical framework two different motivations for migration: ==== an economic motivation linked to migrants’ need to exploit all of the income opportunities that might appear at both the home and the foreign countries; ==== a non-economic motivation including social and personal reasons to migrate such as, for instance, family reunification, cultural proximity, etc.====So far the bulk of migration literature focused on only international wage differentials as the main motivation to migrate (Harris and Todaro, 1970; Hart, 1975, Borjas, 1990, Borjas, 1994, Borjas, 2001, Dustmann, 2003, Parello, 2019, Parello, 2021), and when it addressed migration networking issues, the approach adopted is basically empirical or, at best, based on partial-equilibrium frameworks (Boyd, 1989, Bauer and Zimmermann, 1997, Winters et al., 2001, Bauer et al., 2002, Bauer et al., 2007). To the best of our knowledge, this paper is the first study that plugs immigration networks into a fully-fledged dynamic general-equilibrium model of Neoclassical growth.====The findings of the paper are as following. First, we find that the unique steady-state equilibrium of the model is always determined and saddle-path stable. This result is different from that previously reported by Parello, 2019, Parello, 2021 for the case of Neoclassical growth models without migrants networking, where the long-run equilibrium of the model is found to be always locally indeterminate. Second, strengthening community networks have asymmetric effects on natives’ and immigrants’ welfare. In particular, if the initial size the local community of immigrants is relatively small with respect to the native population, we find that every migration policy aiming at strengthening immigrants’ networks is welfare enhancing for immigrants but not for natives. Yet, if the relative size of the local community of immigrants is higher than a certain threshold, we find that the opposite occurs and hence that strengthening immigrants’ networks is welfare enhancing only for natives. Lastly, upon calibrating the model to match the observed data for Canada, Japan and USA, we find that promoting immigration networks improves immigrants’ welfare but hurts natives in all of the three countries considered. On the contrary, the introduction of a tax on immigration is able to improve welfare of all agent types in the population, though at the expense of migrant labor and capital accumulation in the domestic economy.====The paper relates to at least two different streams of economic research. Firstly, it relates to the strand of literature that analyzes the role of community networks in affecting future migration. This literature includes, among others, papers by Munshi (2003), Moretto and Vergalli (2008), Vergalli (2008), Dolfin and Genicot (2010), Mckenzie and Rapoport (2007), Beine et al. (2015) and Comola and Mendola (2015). In particular, our study closely relates to Moretto and Vergalli (2008), who consider a model in which migration depends on the existence two driving forces: the presence of a positive wage differential between the receiving and sending country; the existence of positive network externalities emanating from the previous waves of immigrants. However, while Moretto and Vergalli’s study builds up a real option model to study how immigration networking might affect future migration, our contribution proposes a fully-fledged dynamic model of migration and growth, and focuses on macroeconomic dynamics and social welfare of the receiving economy.====Secondly, our study relates to the strand of literature that analyzes the long-run implications of migration on macroeconomic dynamics and economic growth. This literature comprises, among others, contributions by Ben Gad, 2003, Ben Gad, 2004, Ben Gad, 2008, Klein and Ventura, 2007, Klein and Ventura, 2009, Benhabib and Jovanovic (2012), Khraiche (2015) and Parello, 2019, Parello, 2021. Our paper adds to this strand of research by introducing network externalities in a standard model of Neoclassical growth and by providing a formal analysis demonstrating how immigrant networking can fix extrinsic uncertainty and indeterminacy problems induced by migration. Indeed, in two recent studies, Parello, 2019, Parello, 2021 shows that a standard one-sector Neoclassical growth model, extended to include temporary and circular migration, can predict the emergence of local indeterminacy regardless of whether the capital account of the receiving economy is either open or closed. Our study adds to this analysis in the following ways. First, we find that the addition of permanent migration and immigrant networking can prevent Neoclassical growth models from generating multiple equilibria. Second, we demonstrate that natives’ and immigrants’ welfare is very sensitive to immigrant networking and past migration, and also that the way network externalities affect consumers’ welfare is dependant upon the size of the local communities of immigrants.====The outline of the paper is the following. Section 2 presents the baseline model and characterizes the dynamic properties of the equilibrium. Section 3 analyzes the welfare implications of immigrants’ networks under different assumptions regarding the size and strength of network externalities. Section 4 calibrates the model and applies it to assess the long-run effects of two different types of migration policies: a pro-immigration policy aiming at strengthening immigrants’ networking and a anti-immigration, protectionist policy aiming at reducing in-migration. Finally, Section 5 concludes.","Migration, community networks and welfare in neoclassical growth models",https://www.sciencedirect.com/science/article/pii/S0164070421000689,31 August 2021,2021,Research Article,103.0
"Nunes Ricardo,Park Donghyun,Rondina Luca","University of Surrey, United Kingdom of Great Britain and Northern Ireland,CIMS, United Kingdom of Great Britain and Northern Ireland,University of Sussex, United Kingdom of Great Britain and Northern Ireland","Received 24 November 2020, Revised 7 July 2021, Accepted 18 August 2021, Available online 30 August 2021, Version of Record 9 September 2021.",https://doi.org/10.1016/j.jmacro.2021.103363,Cited by (1),This paper studies optimal ,"In seminal contributions, Kydland and Prescott (1977) and Barro and Gordon (1983) showed that optimal policy plans can be subject to time-inconsistency. In a model with rational forward-looking agents, the optimal policy plan prescribes that a central bank should manage expectations of future outcomes. Nevertheless, honouring past promises becomes sub-optimal from the perspective of a future central bank, hence the time-inconsistency of the originally formulated policy plan.====In order to address this issue, the literature has focused on two main models of central bank behaviour: commitment and discretion. Under commitment, the policymaker has access to a ==== that allows her to fully deliver on past promises at any point in the future, therefore bypassing the time-inconsistency problem. The temptation to renege is still present but the planner does not deviate from previously announced plans. In contrast, under discretion the policymaker can never fulfil a previous promise and, instead, always chooses her actions based on current circumstances. Discretion is therefore time-consistent by construction.====Despite the valuable insights produced using either framework, both models of optimal policy contrast with some stylised facts. For instance, it is impossible to commit based on unforeseen contingencies, also the composition of policy committees changes over time and so does the understanding of economic models and data, which determines that policy strategies must be reformulated and previous commitments may be occasionally reneged on. On the other hand, central banks actively try to influence expectations through policy statements and forward guidance, which suggests that central banks retain some imperfect credibility to announce future policy actions and fulfil past promises.====In this paper we consider a monetary policy setting that spans the limiting cases of full commitment and full discretion. This is known in the literature as ==== or ====.==== Under loose commitment, the policymaker can credibly commit to an announced policy plan and honour past promises; however, with an exogenous probability she will renege on previous plans and reoptimise. The central bank is aware that it may be given the opportunity to revise its plans later on, and formulates optimal policy accordingly. Moreover, the probability of reneging is common knowledge to all agents, which internalise this information when forming expectations. As a result, the credibility of the monetary authority is imperfect.====We study optimal monetary policy under imperfect credibility in a baseline New Keynesian model with sticky prices and sticky wages. This canonical model has the key advantage that analytical expressions for welfare can be used, which provides a more transparent and rigorous framework for optimal policy evaluation. In addition, the presence of both price and wage stickiness means that this model retains key frictions for evaluating the trade-offs of optimal monetary policy and the effects of wage flexibility on welfare.==== In this paper, we argue that the interplay between these two elements of the model, imperfect credibility and wage stickiness, is key in providing a comprehensive understanding of welfare losses.====The loose commitment paradigm allows us to investigate interesting normative and positive aspects of monetary policy and credibility. On the normative side, the relationship between credibility and welfare can be studied and the welfare gains from increasing commitment can be measured. On the positive side, solving the optimal policy under loose commitment allows us to evaluate the transmission of monetary policy under different credibility levels and the associated business cycle properties.====After examining the main features of the model, the paper examines the debate of whether wage flexibility is welfare improving. The model features several frictions and shocks; by the theory of the second best, it is not necessarily the case that decreasing the degree of wage rigidity leads to higher welfare. In fact, Galí (2013) and Galí and Monacelli (2016) discuss that increasing wage flexibility may reduce welfare. We show that the degree of credibility can be important in this discussion. If credibility is low, monetary policy may not counteract the potential feedback loop between wage and price inflation. Consequently, when monetary policy is not very credible, wage flexibility can be welfare detrimental for a wider range of cases. However, we also incorporate wage markup shocks in the model. These shocks are important in the empirical literature, and we find that wage flexibility is key to dampen the welfare losses of wage markup shocks.====This paper is related to the literature on optimal monetary policy and loose commitment settings.==== In particular, this work is related to Schaumburg and Tambalotti (2007) and Debortoli et al. (2014). Unlike the latter, we use a microfounded loss function, which is key to have an accurate computation of welfare. Unlike Schaumburg and Tambalotti (2007), we consider a model with sticky prices and sticky wages, which introduces additional trade-offs for monetary policy. Our framework allows us to check accurately the effects of wage rigidities on welfare. On this front we extend Galí (2013) by incorporating additional shocks and examining imperfect credibility settings.====The paper is organised as follows. Section 2 describes the model. Section 3 investigates the relationship between central bank credibility and welfare losses. Section 4 examines the transmission mechanism and business cycle properties under different degrees of commitment. Section 5 examines the effects of wage rigidities on welfare. Section 6 concludes.","Imperfect credibility, sticky wages, and welfare",https://www.sciencedirect.com/science/article/pii/S0164070421000641,30 August 2021,2021,Research Article,104.0
"Ouliaris Sam,Rochon Celine","Former IMF Staff,IMF Staff, International Monetary Fund, Washington DC","Received 3 December 2020, Revised 20 August 2021, Accepted 24 August 2021, Available online 28 August 2021, Version of Record 21 September 2021.",https://doi.org/10.1016/j.jmacro.2021.103370,Cited by (3),"This paper estimates the change in policy multipliers in the U.S. relative to their pre-2008 financial crisis levels. It also estimates the likely impact of the 2020 stimulus packages implemented to address COVID-19. The analysis is based on an augmented Blanchard-Perotti model that allows for the dynamic effects of shocks to the central bank balance sheet, real interest rates and debt levels on economic activity. The results suggest that expenditure multipliers have fallen post-2008, mostly because of higher government debt, implying that the effectiveness of fiscal policy has declined. They also suggest that the impact of quantitative easing is beneficial, but requires sizable interventions to have noticeable effects on real GDP. Because of rising debt stocks, dealing with a crisis is becoming more and more costly despite the current low ==== environment.","G7 countries responded promptly to the 2008 Global Financial Crisis (GFC) by using a mix of economic policies, particularly expansionary fiscal and accommodative monetary policies. They complemented these measures with structural policies designed to address the slowdown in productivity and other macro critical issues, including climate change. Countries facing the COVID-19 pandemic have implemented sizable fiscal and monetary policy measures to support the real economy, including transfer schemes to households and corporates, coordinated central bank actions via swap lines arrangements, liquidity facilities, and actions to support lending to households and businesses.====While expansionary policies helped to resolve the 2008 financial crisis, they may have also reduced the ability of G7 countries to address future slowdowns using fiscal and monetary policies. Specifically, the size and effectiveness of future countercyclical fiscal measures could be constrained by elevated debt levels that, with only one exception among G7 countries, increased substantially during the last cyclical upswing (Figure 1). The fiscal constraint could also be compounded by structural developments such as ageing populations. Likewise, future monetary policy could be constrained by the relatively low level of nominal interest rates and the sheer size of central bank balance sheets (Figure 2). Though aggressive fiscal and monetary policies allowed for a consistent and effective response to the 2008 financial crisis, they also increased external imbalances within the G7. Together with the rise in public debt, this has increased the risk of sudden and uncoordinated relative price adjustments, particularly through the exchange rate channel.====In this paper, we study the effects of higher debt levels, quantitative easing, and lower interest rates on real GDP by estimating pre- and post-GFC policy multipliers in the case of the U.S. We assess whether the expenditure and tax multipliers for the U.S. have fallen since the start of the GFC in late 2007. We then use the estimated post-GFC multipliers to gauge the impact of the COVID-19 stimulus packages adopted in 2020.====There is considerable theoretical and empirical research on time-varying fiscal multipliers that are dependent on some attribute of the macroeconomy. The fiscal position, for example, can affect the size of fiscal multipliers through various channels. One channel is the perceived negative wealth effect that leads households to cut consumption and increase savings, and thus weakens the impact of the stimulus on output. Ilzetzki, Mendoza, and Vegh (2013), Nickel and Tudyka (2014) and Auerbach and Gorodnichenko, 2012, Auerbach and Gorodnichenko, 2013 estimate multipliers that depend on the fiscal position and find that weaker fiscal positions are associated with smaller fiscal multipliers. Alternatively, when the fiscal position is weak, a fiscal expansion can increase lenders’ concerns about sovereign credit risk, raising bond yields and borrowing costs, potentially crowding-out both private investment and consumption, and reducing the size of the fiscal multiplier. Papers that study this second channel include Corsetti et al. (2013) using a DSGE application, Bocola (2016) using a structural model, and Auerbach and Gorodnichenko (2017) using a local projections model. The two channels are studied jointly in Huidrom et al. (2019) using a panel data set and a time-varying parameter vector autoregressive model in which parameters evolve endogenously according to the fiscal position. Other econometric approaches used to study time-varying fiscal multipliers include smooth transition vector autoregressive models (Auerbach and Gorodnichenko (2012), Hernandez de Cos and Moral-Benito (2016)), threshold autoregressive models (Baum et al., 2012) that allow the fiscal multiplier to depend on the business cycle, and rolling regressions used to investigate structural changes (Cimadomo and Benassy-Quere, 2012). Local projections methods have also been used to identify time variation in fiscal multipliers (Jorda (2005) and Ramey and Zubairy (2018)).====The empirical approach used in this paper draws on Blanchard and Perotti (2002) to assess whether fiscal multipliers for the U.S. have changed since the global financial crisis. Throughout this paper, the comparison of the multipliers pre- and post-crisis are not the result of a formal time-varying parameter VAR model but rather a structural VAR with constant parameters. Structural changes in the unknown parameters are allowed by changing the sample period over which the VAR is estimated.====We first estimate the dynamic effects of debt-financed shocks in government spending and taxes, using the standard Blanchard and Perotti model involving per capita government expenditure, taxation and real gross domestic product. Even though the standard model abstracts from government debt dynamics and monetary policy, we find evidence that the fiscal multipliers have fallen after the start of the crisis.====To better understand the reasons for the change in the fiscal multipliers, we augment the Blanchard and Perotti model to allow for the dynamic effects of shocks in the central bank's balance sheet, real interest rates and debt levels on economic activity. Given the elevated debt levels and significantly larger balance sheet, how much larger, if at all, do fiscal and monetary policy measures need to be to deliver the desired support to the real economy?====As in Blanchard and Perotti (2002), this paper uses a mixed structural VAR/event study approach. Identification relies on institutional information about the tax and transfer systems, central bank communication about their quantitative easing (QE) and normalization process, and debt trajectory to infer the impact of policy shocks.====The empirical results show (positive) government spending shocks as having a positive effect on output, and (positive) tax shocks as having a negative effect. However, the estimated multipliers for spending and taxation shocks using the enhanced model have declined appreciably since the 2008 global financial crisis. The decline can be attributed in part to the increased debt burden accumulated since the GFC. Our paper offers two complementary angles: i) an interest rate angle, which in the short run, given the substantial decline in real interest rates since the end of 2007, has probably increased the fiscal multiplier; ii) a debt angle, which likely created a negative drag on real growth and hence reduced the multiplier. Despite the presumption that policy multipliers are likely to increase in a low interest rate environment, this view is not supported by our model because of the negative role played by debt accumulation. There is also a risk that interest rates may not remain low for an extended period, which would ultimately increase the debt burden.====Many papers====, both theoretical and empirical, provide evidence that debt has a negative impact on the macro economy in the long run, especially when the debt to GDP ratio exceeds a threshold. The main channels that explain this relationship include private saving (via the impact on households’ consumption and saving behavior of higher taxes to finance the interest payments on the government debt), public investment (via the debt overhang), total factor productivity (via incentives for work and the use of capital and labor) and long-term interest rates (via crowding out of private investment).====Our analysis also suggests that quantitative easing has a relatively small effect on the real economy compared to fiscal measures, and that future quantitative easing will need to be substantial to achieve a given policy target.",Pre- and Post-Global Financial Crisis Policy Multipliers,https://www.sciencedirect.com/science/article/pii/S0164070421000690,28 August 2021,2021,Research Article,105.0
"Li Bing,Pei Pei,Tan Fei","Academic Center for Chinese Economic Practice and Thinking, Tsinghua University, China,Chinese Academy of Finance and Development, Central University of Finance and Economics, China,Department of Economics, Chaifetz School of Business, Saint Louis University and Center for Economic Behavior and Decision-Making, Zhejiang University of Finance and Economics, China","Received 13 April 2021, Revised 1 August 2021, Accepted 3 August 2021, Available online 14 August 2021, Version of Record 20 August 2021.",https://doi.org/10.1016/j.jmacro.2021.103353,Cited by (0),Is ,"In any dynamic model with nominal government debt, there are two regions of the policy parameter space in which monetary and fiscal policies ==== determine inflation and stabilize debt. One region produces active monetary and passive fiscal policy or regime M, yielding the conventional monetarist/Wicksellian paradigm of inflation determination. Regime M assigns monetary policy to control inflation by raising the nominal interest rate aggressively with inflation and fiscal policy to stabilize debt by adjusting taxes or spending. The second region consists of passive monetary and active fiscal policy or regime F, producing the fiscal theory of the price level (Leeper, 1991, Woodford, 1995, Cochrane, 1999, Davig and Leeper, 2006, Sims, 2013). Under regime F, policy roles are reversed, with monetary policy responding weakly to inflation and fiscal instruments adjusting weakly to government debt.==== Because these two policy regimes imply completely different mechanisms for price level determination and, therefore, starkly different policy advice, identifying the prevailing regime is a prerequisite to understanding the macroeconomy and to making good policy choices.====While the popular surplus-debt regressions are subject to potential simultaneity bias that may produce misleading inferences about the nature of fiscal behavior, testing endeavors based on general equilibrium models with fixed policy rules, on the other hand, find nearly uniform statistical support for regime M over various subsamples of the precrisis U.S. data (Traum and Yang, 2011, Leeper and Li, 2017, Leeper et al., 2017). This consensus emerged even from periods of marked increases in fiscal stress (measured by deficit-to-debt ratio in Fig. 1), such as the mid-1970s, during which monetary policy appears to lose control over inflation.====The literature on regime-switching monetary and fiscal policy constitutes a notable exception. By embedding the possibility of recurrent regime shifts in the formation of agents’ expectations, this class of models uncovers alternating periods of regime M, the benign policy mix that many believe contributed to the steady decline in inflation in the 1980s, and regime F, the policy mix that some believe produced the run-ups in inflation in the 1970s, together with episodes of active (passive) monetary and active (passive) fiscal policy (Davig and Leeper, 2006, Bianchi, 2012, Bianchi, 2013, Bianchi and Ilut, 2017).====Scant attention, however, has been paid to the empirical relevance of financial frictions in discerning the underlying policy regime despite its growing importance in macroeconomic modeling of business cycle fluctuations [Bernanke et al., 1999, Christiano et al., 2003, Christiano et al., 2014, Del Negro et al., 2015, among others]. Such neglect is also surprising in light of the comovements between inflation and measures of financial and fiscal stress, as shown in Fig. 1—the Great Inflation from 1965 to 1982 concurred with several spikes in the credit spread and deficit-to-debt ratio, while the subsequent disinflation was associated with a downward trend in these variables, at least until the late 1990s. If inflation depends importantly on fiscal behavior and credit market conditions, a natural conjecture is that introducing financial frictions to the model and adding financial variables to the dataset may generate fresh identifying restrictions for the underlying policy regime.====This paper assesses the role of credit market imperfections in the identification of policy regimes. To that end, we extend a standard medium-scale dynamic stochastic general equilibrium (DSGE) model in two aspects. In particular, we follow Leeper et al. (2017) to fill in details of fiscal policy and incorporate the Bernanke et al. (1999) (henceforth, BGG) type of credit market frictions, as in Christiano et al. (2014). These ingredients allow for a comprehensive study of monetary and fiscal policy interactions in a financially constrained environment.====Our key findings are threefold. First and foremost, adding financial frictions to the model and financial variables to the data improves the relative statistical fit of regime F. A rich set of model comparison exercises indicates that this result remains robust with respect to the sample period and fiscal details of the model and data. Surprisingly, regimes M and F become ‘nearly’ observationally equivalent in some cases, thereby overturning the clear-cut regime ranking found in the literature. As a byproduct of our model comparison exercises, we find that adding the deficit-to-debt ratio to the observables also improves the relative statistical fit of regime F, to the extent that it can fundamentally alter the regime ranking. This result largely corroborates that of Kliem et al., 2016a, Kliem et al., 2016b, who find that the low-frequency relationship between inflation and fiscal stance (defined as primary deficit over one-period lagged debt) plays a prominent role in discerning the underlying policy regime.====Second, the two policy regimes produce strikingly different inflation dynamics following a credit crunch. Contrary to regime M, which underlies the analysis of Christiano et al. (2014) and many others, elevated financial distress brings forth heightened fiscal uncertainty and inflation through the mechanism that regime F emphasizes. The implied comovement patterns between inflation, credit spread, and deficit-to-debt ratio under regime F turn out to be consistent with their major trends that Fig. 1 displays. More broadly, any exogenous disturbance that triggers different comovements in these variables across regimes, e.g., monetary and fiscal policy shocks, could potentially help to distinguish which policy regime generated the observed data.====Last, we consider an incomplete model space to confront policy regime uncertainty, where neither regime M nor regime F necessarily corresponds to the true model. By estimating a linear prediction pool model that dynamically combines both policy regimes, we find a strong cyclical pattern of transitions across regimes. In particular, despite the differences in model specification and dataset, the estimated historical regime weights exhibit pronounced cyclical fluctuations in all cases, with marked decreases in the relevance of regime M (or equivalently, sharp increases in the importance of regime F) accompanying the economic recessions.====Our paper complements the recent literature on the joint study of monetary and fiscal policy in the presence of financial frictions. For example, Cui (2016) develops a tractable macro model with endogenous asset liquidity to understand monetary–fiscal policy interactions with liquidity frictions. Xu and Serletis (2017) take a first pass at providing a theoretical demonstration of the fiscal theory of the price level in a world with borrowing constraints. Gomes and Seoane (2018) study the macroeconomic implications of each regime in a financially constrained environment and attribute the distinct postcrisis dynamics of the U.S. and Euro area to different policy arrangements. Li et al. (2018) show that a new Keynesian model with financial intermediaries and monetary–fiscal regime shifts can explain the time-varying correlation between returns on the market portfolio and Treasury bonds. To the best of our knowledge, this paper is the first to systematically assess the empirical relevance of financial and fiscal stress in the identification of policy regimes. While it is almost impossible to explore the entire model space, we perform extensive regime comparisons by estimating the marginal likelihoods for a total of 24 relevant models. Indeed, our analysis is rooted in the spirit of the Bayesian model scan framework proposed by Chib and Zeng (2019).====The rest of the paper is structured as follows. Section 2 outlines a standard medium-scale DSGE model augmented with financial frictions and a rich set of fiscal details for the empirical analysis. Section 3 reports the estimation results and explores the mechanism at work. Section 4 estimates a dynamic prediction pool model to confront policy regime uncertainty. Section 5 concludes.",Financial distress and fiscal inflation,https://www.sciencedirect.com/science/article/pii/S0164070421000549,14 August 2021,2021,Research Article,106.0
"Chipeniuk Karsten O.,Walker Todd B.","Economics Department, Reserve Bank of New Zealand, New Zealand,Department of Economics, Indiana University, United States of America","Received 15 July 2021, Accepted 17 July 2021, Available online 9 August 2021, Version of Record 13 August 2021.",https://doi.org/10.1016/j.jmacro.2021.103348,Cited by (0),"%), we can confirm the direction of change at a daily frequency. Moreover, we can assign nearly all substantial movements to specific events, documenting asymmetry in the behavior of inflation expectations. We provide a structural interpretation of our findings, emphasizing the importance of a zero-lower bound over this time period. We also connect the dispersion in inflation expectations to newly developed measures of uncertainty [Jurado et al. (2015), Baker et al. (2016)]. Innovations to financial and ==== uncertainty increase the dispersion in inflation floors, while decreasing the dispersion in inflation caps.","The market for direct hedges of the inflation rate has matured since its inception, with $1.2 trillion worth of treasury inflation protected securities (TIPS) issued in 2017==== and over $2 billion of notional principle cleared daily in the inflation swap market.==== Alongside the TIPS and swap markets, cap and floor contracts have been written over-the-counter since late 2009. A question of continuing interest to financial market participants and central banks is the extent to which activity in the market for inflation options provides information with respect to investor beliefs about future changes in the rate of inflation.====We analyze daily Bloomberg composite prices for zero-coupon caps and floors over the period starting in January 2012 until May 2017. We argue the substantial increase in volume over the initial years of our sample period is sufficient for identifying changes in investor sentiment (Section 2). To back out implied probability densities, we use the “canonical valuation” method introduced by Buchen and Kelly (1996) and Stutzer (1996). This approach finds the forward densities that correctly price the inflation option (no arbitrage) and are the closest to the empirical distribution of inflation, where “closest” is measured according to the Kullback–Leibler Information Criterion (KLIC). The advantage of the canonical method for our purposes here is that we can isolate the density associated with specific options; that is, we do not need the entire set of option prices to form a forward distribution that is necessary of the “derivative method” developed by Ross, 1976, Breeden and Litzenberger, 1978 and Aït-Sahalia and Duarte (2003). This allows us to examine dispersion measures associated with inflation caps and floors separately. More importantly, because the canonical valuation methodology is not reliant on having a full set of option prices, we can exclude specific prices that suffer from potential illiquidity concerns.====Section 3 contains our main results. We find that dispersion in the forward inflation distribution exhibits clear breaks between regimes following major economic developments. Transitions in dispersion typically take place over a period of one day up to a week, and display high inertia. The high stability and rapid transitions allow us to sidestep the critique of Fair (2002), and we find that breaks are typically accompanied by either statements from the Federal Reserve and its Chairperson, or a shift in uncertainty around a major economic events such as the European debt crisis, US debt-ceiling crises, and the Greek bailouts. In particular, dispersion in the forward measure due to movements in the prices of ==== is associated with statements by the Board of Governors of the Federal Reserve or its Chair, while changes in dispersion due to movements in the prices of ==== are typically attributable to structural economic performance (e.g., labor markets, oil prices). Sections Section 3.2 conducts a narrative view of changes in regime and we count only three exceptions to this stylized fact over the entire sample period. Our approach demonstrates the benefit of high-frequency methods to identify monetary policy following Gertler and Karadi, 2015, Hanson and Stein, 2015 and Nakamura and Steinsson (2018).====The asymmetry in inflation caps and floors is our primary finding and we offer a structural interpretation in Section 3.3. The model is a relatively standard New Keynesian model with households that are subject to discount factor shocks and a monetary policy authority constrained by the zero-lower bound (ZLB). Using the nonlinear solution method and results developed in Richter et al., 2014, Gavin et al., 2015 and Plante et al. (2016), we show that technology shocks are qualitatively different under the ZLB. Weakness in the labor market is correlated with much lower expected inflation. This is consistent with our empirical finding that statistics portending structural weakness in the economy (e.g., a weak employment report) almost always caused an increase in the dispersion of inflation floors. The model can also generate substantial inflation uncertainty, defined as time-varying second moments, which is consistent with our regime-switching specification.====Section 3.4 documents that our measures of expected inflation dispersion are significantly predicted by changes in recently developed metrics of uncertainty and compares our metric to other measures of it (e.g., VIX). Specifically, we regress dispersion on the financial and macroeconomic uncertainty of Jurado et al. (2015) and policy uncertainty of Baker et al. (2016). All measures of uncertainty are significant predictors of dispersion but the uncertainty of Jurado et al. (2015) enters negatively for forward dispersion due to inflation cap price movements and positively for floor price movements. Thus, an increase in financial uncertainty decreases (increases) the dispersion in caps (floors). This asymmetric response is consistent with the deflationary concerns revealed in our regime-switching results, and explains why innovations to financial time series, on average, consolidated expectations in inflation cap markets while unanchoring them in floor markets.",Forward inflation expectations: Evidence from inflation caps and floors,https://www.sciencedirect.com/science/article/pii/S0164070421000501,9 August 2021,2021,Research Article,107.0
"Neyer Ulrike,Stempel Daniel","Heinrich Heine University Düsseldorf, Universitätsstraße 1, 40225 Düsseldorf, Germany","Received 9 November 2020, Revised 23 July 2021, Accepted 30 July 2021, Available online 5 August 2021, Version of Record 7 August 2021.",https://doi.org/10.1016/j.jmacro.2021.103352,Cited by (2),Empirical evidence suggests that women are discriminated against in the labor market. We analyze the effects of taste-based and statistical gender discrimination on business cycle and ,"Discrimination in the labor market has been at the forefront of economic research for decades. Starting with Becker (1971), who analyzes the various consequences of racial discrimination in firms, many more scholars have theoretically and empirically examined the extent and economic effects of discrimination against different groups. A considerable portion of this literature addresses gender discrimination in the labor market, captured by significant adjusted gender wage gaps (i.e., gender wage gaps controlling for productivity measures), for instance. Furthermore, systematic gender differences with respect to the time spent in paid and unpaid work can be observed: OECD (2020) data shows that women spend about 18.2% of a 24-hour day performing unpaid work while men spend half that time (9.4%). Conversely, women spend 15.1% working in the paid labor market, men 22%.==== In principle, these differences could be explained by the preferences of women and men with regards to household and labor market work. However, studies indicate that these differences are not purely preference-driven.====Against this background, our paper analyzes the effects of gender discrimination on business cycle fluctuations and inflation. In particular, we investigate how gender discrimination distorts the transmission of shocks into an economy by extending conventional New Keynesian models. At the household level, we introduce a female and a male agent. Furthermore, we include unpaid household work in addition to paid labor market work in order to account for the differences in the working time allocation between women and men. On the firms’ side, we introduce gender discrimination into our framework, accounting for the adjusted gender wage gap. We differentiate between two types of gender discrimination: taste-based and statistical discrimination. Following Becker (1971), we conceptualize taste-based gender discrimination against women as a preference among firms to hire men over equally productive women. Moreover, we implement statistical gender discrimination as an information asymmetry between households and firms, thereby following the literature brought forward by Phelps (1972) and Arrow (1973). In particular, we specify statistical discrimination in our model by assuming that firms face greater uncertainty with respect to the productivity of women than of men: we assume that there are two types of women with different productivity levels, while there is only one type of men. However, firms cannot observe the individual productivities of women but rather base their decisions on average female productivity. This implies statistical discrimination against women with higher productivity levels. These extensions of common dynamic stochastic general equilibrium (DSGE) models allow us to analyze and compare the effects of different types of discriminatory behavior by firms on business cycle and inflation dynamics after demand (discount rate) as well as monetary policy shocks.====We find that in response to a negative demand shock, both taste-based and statistical gender discrimination lead to a more severe economic downturn due to an increase in the inefficient utilization of female and male productivity. The working time allocation of women and men between labor market and household production becomes even more inefficient. Quantitatively, the economy suffers more from taste-based discrimination than it does from statistical gender discrimination. However, statistical discrimination leads to quantitatively larger intra-household distortions than taste-based discrimination after negative discount rate shocks. Furthermore, we find that both types of gender discrimination weaken the transmission of expansionary monetary policy shocks on inflation. Overall, female and male wages increase too little in response to the shock, since women are discriminated against and female and male productivity is therefore utilized inefficiently. Thus, firms’ marginal costs (wages) increase less compared to the non-discriminatory case. Furthermore, the adjusted gender wage gap increases after expansionary monetary policy shocks. Quantitatively, taste-based discrimination has a more dampening effect on the transmission of expansionary monetary policy shocks on inflation than statistical discrimination. However, statistical discrimination implies larger effects on the adjusted gender wage gap and on the inefficiency of the working time allocations of the households.====In our analysis, we use the adjusted gender wage gap as a proxy for discrimination against women in the labor market. Various existing studies investigate the extent of gender discrimination captured by the gender wage gap. For instance, Blau and Kahn (2017) show that the raw, unadjusted female to male wage ratio ranged from 62.1% in 1980 to 79.3% in 2010 in the United States.==== However, it is usually argued that the unadjusted gap is not a sufficient measure for potential discrimination because factors such as education or work experience may explain at least a part of the gap. Therefore, most studies report an adjusted gender wage gap, thereby taking into account productivity measures such as experience, hours worked, education, industry, occupation, or union status.==== Blau and Kahn (2017) find an adjusted gender wage gap of 20.6% in 1980 in the United States. They show that this gap closed to 7.6% in 1989, however this trend did not continue in the following 20 years: in 1998 the adjusted gender pay gap was still 8.6%, in 2010 8.4%. It is argued that these adjusted wage differences can at least partly be ascribed to gender discrimination.====Our paper relates to the literature in the following ways. Most importantly, we contribute to the strand of literature that analyzes gender differences on a macroeconomic level. This includes the analysis of Morchio and Moser (2020) who provide estimates of output and utility gains from closing gender gaps. While static effects form the focus of their model, we specifically take a dynamic perspective. In the same vein, our paper complements studies that analyze the effects of gender discrimination on labor market outcomes==== as well as work that considers the effects of female empowerment and employment on macroeconomic outcomes.==== Furthermore, our paper relates to work that analyzes heterogeneity across agents. This heterogeneity has been introduced into New Keynesian frameworks in recent years.==== However, approaches to studying gender-related topics within these frameworks are rare (exceptions include Khera, 2016, Albanesi, 2019), and scant attention has been paid to an analysis of the effects of gender discrimination on the business cycle and inflation, which is a main focus of our model.==== Our paper also contributes to the literature that examines the effects of monetary policy on inequality. The studies conducted by Doepke et al. (2015), Coibon et al. (2017), Ampudia et al. (2018), or Furceri et al. (2018) analyze the effects of conventional and unconventional monetary policy shocks on household inequality. However, only little attention has been paid to the effects on women or minorities==== and, to the best of our knowledge, none paid to the impact on the adjusted gender wage gap and the effects of gender discrimination.====The paper is organized as follows: Section 2 states the model before Section 3 analyzes the results. Section 4 concludes.","Gender discrimination, inflation, and the business cycle",https://www.sciencedirect.com/science/article/pii/S0164070421000537,5 August 2021,2021,Research Article,108.0
Lin Yi Chun,"Department of Economics, Washington University in St. Louis, One Brookings Drive, St. Louis, MO 63130, USA","Received 12 March 2021, Revised 26 May 2021, Accepted 14 July 2021, Available online 27 July 2021, Version of Record 30 July 2021.",https://doi.org/10.1016/j.jmacro.2021.103349,Cited by (0),This paper examines the Taiwanese economy in a ,"This paper studies the sources of fluctuations of the business cycle in Taiwan during 1999–2019. It is the first research to examine the Taiwanese economy through the lens of a small open economy DSGE model in which the volatility of the structural innovations changes over time. The model extends the small open economy model of Justiniano and Preston (2010) by incorporating capital, and it is estimated by the tailored random block Metropolis–Hastings (TaRB-MH) algorithm of Chib and Ramamurthy (2010). This algorithm was proven efficient to estimate large-scale DSGE models with Bayesian methods. The combination of an open economy DSGE model with the TaRB-MH algorithm provides a brand new framework to examine the Taiwanese economy with a focus on the fluctuations of key macroeconomic variables.====The model constructed in this paper is a modification of Justiniano and Preston (2010) with capital accumulation. By having physical capital and investment in the structure, the model is better suited for depicting the well-developed capitalist economy of Taiwan. Moreover, investment shocks are important for business cycle fluctuations as Greenwood et al. (1988) and Justiniano et al. (2010) claim, so incorporating capital helps examine how investment shocks affect the volatility of the Taiwanese business cycle. Since the 1970s, international trade has been an important factor in Taiwan’s economic growth. The degree of openness==== has increased from 60% in 1970 to 118% in 2019. Taiwan’s biggest trading partners include China, the USA, Japan, the European Union, and Hong Kong. It is of interest to study how economic shocks in these big economies are affecting Taiwan as a small open economy.====There are two economies in the model: one small open home economy, and the other big foreign economy. The sizes of the two economies differ to the extent that the home economy is influenced by the foreign economy, while the foreign economy is big enough to be considered as an individual closed economy. This matches the situation between Taiwan and the US. Although China has become the biggest trading partner of Taiwan in the past decade, the US is a better choice as the foreign economy in this model for two reasons. First, the US is considered a relatively closed economy in the literature, comparing to the recent Chinese economy. Second, the model assumes that the home country imports a portion of its consumption basket from the foreign country, while in reality, most imports from China to Taiwan are intermediate goods, not final goods. From this perspective, the US fits the role of the foreign economy in this model more. To sum up, the two-country DSGE model is estimated with Taiwan and US data using Bayesian methods.====For the past two decades, the development of Bayesian estimation of DSGE models has flourished both in macroeconomic research and in policy analysis. In the literature, most DSGE models are estimated and analyzed using Dynare, but there are some restrictions about it. Dynare can only solve DSGE models with shock innovations having constant volatility. Since the goal of this paper is to identify the sources of fluctuations in the Taiwanese business cycle, fewer limitations to the shocks help in extracting more information from the data. Thus, assuming constant volatility for shock innovations is not the first choice. To overcome the limitations of Dynare, the model in this paper is analyzed by the ‘DSGE-SVt’ Matlab package.==== This package is capable of estimating high-dimensional DSGE models with shock innovations having stochastic volatility. This paper compares three types of shock distributions. The first two are Normal and Student-t distributions, both with constant volatility, and the third type is a Student-t distribution with stochastic volatility. Examples of having Student-t type of shocks in DSGE models can be seen in Chib and Ramamurthy (2014). Log-marginal likelihoods of three models are calculated to determine which type of distribution of shock innovations best explains the characteristics of the business cycle in Taiwan.====The rest of the paper is organized as follows. Section 2 presents the two-country DSGE model. Section 3 brings the model to the Taiwan/US data. Section 4 illustrates the Bayesian analysis, including the prior distribution assumed, and the posterior estimation results. Section 5 concludes.",Business cycle fluctuations in Taiwan — A Bayesian DSGE analysis,https://www.sciencedirect.com/science/article/pii/S0164070421000513,27 July 2021,2021,Research Article,109.0
"Antón Arturo,Hernández-Trillo Fausto,Ventosa-Santaulària Daniel","Banco de México, Mexico City, Mexico,Department of Economics, CIDE, Mexico City, Mexico","Received 20 April 2020, Revised 16 July 2021, Accepted 20 July 2021, Available online 24 July 2021, Version of Record 29 July 2021.",https://doi.org/10.1016/j.jmacro.2021.103350,Cited by (3),Cash holdings have increased worldwide in recent years. It is well known that a higher ,"Despite the availability and expansion of different payment methods, cash is widely used among advanced countries (Amromin and Chakravorti, 2009; Bagnall et al., 2016). Contrary to claims about its death, cash holdings in some developed countries, such as the United States and Europe, have increased as a share of GDP in recent years (Rogoff, 2016; Jobst and Stix, 2017). Evidence in developing economies is mixed. Fig. 1 reports cash holdings as a share of GDP for selected emerging economies from 2000 to 2018. There are countries where such a ratio has remained relatively stable or diminished (China, Nigeria and South Africa). In others, it has increased over time. Mexico's cash-to-GDP ratio, which was 2.2% in 2000, increased by a factor of 2.7 in a period of just 18 years. As a result, Mexico's cash-to-GDP ratio surpassed Chile, Nigeria, and South Africa but is still well below China, India, and Thailand. However, Mexico's growth in cash holdings is the highest among the countries in the sample.====This paper identifies the factors behind significant increases in cash holdings in a large developing country such as Mexico. For that purpose, we exploit meaningful fiscal changes in the country to quantify the effect of stricter tax enforcement to explain large increases in cash holdings. In 2013, the Mexican government implemented two important fiscal actions aimed at increasing enforcement. First, it announced that electronic invoices would be mandatory for all taxpayers to counter a secondary market of illegal invoices used by firms, small entrepreneurs, and own-account workers to reduce taxable earnings.==== Likewise, it started a registration program to increase the number of taxpayers. These actions may have induced people to circumvent the regulation by conducting their transactions in cash. The argument about circumvention is old: regulations intended to bar one path to a goal, legitimate or not, create incentives to find alternate routes (Kane, 1981). Such is the case with fiscal measures in developing economies with weak institutions, which often causes a further jump in informal activities in the economy. As a result, demand for cash may rise as monetary transactions become the way around fiscal structures. This study evaluates whether that phenomenon accounted for large increases in cash holdings in Mexico.====We propose a transaction-based model of money demand with taxes as in Rogoff (1998) to allow for the possibility of tax enforcement (this model is presented in Appendix A). In this framework, agents may use money to decrease their tax burden. However, the effectiveness to conceal income from authorities by using cash depends on the quality of tax enforcement. Under high-quality institutions, cash is useless at curbing tax enforcement and thus an increase in enforcement is followed by a fall in cash. But if institutions are inefficient and enforcement is raised, agents can effectively increase their use of cash to avoid taxes. Therefore, the model predicts an ambiguous effect of tougher tax enforcement on money holdings. Based on this finding, we proceed to estimate an otherwise standard money demand equation wherein real cash holdings per capita are a function of the nominal interest rate and a measure of economic activity.==== The novelty is that our empirical model considers the tougher tax enforcement faced by agents in 2013. Based on the previous discussion, one might expect stricter tax enforcement to stimulate demand for cash more in economies with low-quality institutions than in developed economies. In this regard, Mexico spurs interest because it is frequently identified as having weak rule of law.==== It is important to emphasize at the outset that authors such as Cagan (1958), Tanzi (1980, 1983) and Rogoff (1998) are interested in capturing the effect of a higher ==== on cash holdings. Therefore, our focus on ==== distinguishes our paper from previous studies.====One of our measures of tax enforcement is the number of wage earners as a percentage of the workforce that are registered by the tax authority (“number of taxpayers” henceforth). Arguably, this measure is misguided for examining developed economies, where substantially all workers are registered taxpayers, but not for developing economies. In January 2010, registered salaried workers represented only 35% of Mexico's workforce (and 56% of total salaried workers), reflecting the country's weak institutions for tax collection and its inability to reduce high levels of informality. As a result of the enforcement policies implemented, this ratio increased to 79% in December 2019. Our second measure of tax enforcement is the mandatory introduction of electronic invoices, which guarantees that all (commercial and personal services) transactions are registered before the tax authority (more on this system later).====We use a vector error correction (VEC) specification to test the implications of the theoretical model, especially that of the relationship between tax enforcement and cash holdings. VEC has advantages over a standard regression analysis. First, obtaining evidence of a cointegrated relationship diminishes the risk of erroneous inference in the presence of nonstationarity, as is the case for the variables we study (see the discussion below). Second, VEC distinguishes between short- and long-run relationships among variables, unlike standard regression analysis. This distinction is important for understanding whether tax enforcement can explain cash holdings in the short run. Third, VEC allows practitioners to infer which variable(s) adjust(s) whenever there is disequilibrium in a long-run relationship.====Our findings suggest that Mexico's significant increase in cash holdings may be explained partly by the heavier tax enforcement measures imposed in 2013. Our results indicate that a 10% increase in tax enforcement (whether it is proxied by the number of taxpayers or electronic invoices) is followed by a 2% increase in cash holdings during the period studied. On the other hand, a forecast error variance decomposition analysis shows that the relevance of the number of taxpayers for explaining cash holdings is significantly greater than that of electronic invoices. We test the robustness of our main model with alternative specifications and different variable definitions for output, interest rate, and tax enforcement. We also include informality, nonperforming loan rate and bank systemic risk measures as additional robustness exercises. In all cases, we confirm that the estimated coefficients under the baseline model do not vary significantly.====Our paper is related to the literature on cash demand, taxation, and tax evasion. Since the seminal work by Cagan (1958), the relation between taxes and cash holdings has been a subject of interest for economists. Cagan (1958) and Tanzi (1980, 1983), who study the relation between the underground economy and the use of cash, posit that high taxes prompt underground transactions that occur strictly in cash. In turn, demand for currency rises. Gordon (1990) argues that cash sales provide a link between the evasion of direct and indirect taxes, which in turn underlines many informal transactions. Tanzi (1980, 1983) uses the tax rate to proxy changes in the size of the underground economy. Adopting further assumptions, he uses the estimated equation as an input to calculate its size.==== Our paper differs from the approach of Tanzi (1980, 1983) and his followers in the sense that we only want to understand how tax enforcement affects cash holdings in an environment of weak institutional framework. We do not seek to measure either the effects of a higher tax burden on cash holdings or the size of the underground economy.====This paper is also related to the works of Rogoff (1998), Stix (2013), Herwartz et al. (2016), and Jobst and Stix (2017). Rogoff (1998) estimates determinants of currency velocity for 16 OECD countries using annual data spanning 1980–1994 and finds that the ratio of taxes to GDP relates negatively to currency velocity in 14 OECD countries. In contrast, Herwartz et al. (2016) report that the share of direct and indirect taxes plus social security contributions to GDP did not affect demand for currency in 11 OECD economies from 1970–2012. These results portray the effects of taxation on demand for currency across economies ====. Therefore, country-specific studies such as ours add insight into the relations between tax policies and demand for cash. Jobst and Stix (2017) evaluate whether increased cash holdings are attributable to changes in the magnitude of the shadow economy in a panel of 70 countries for 2001–2014 and find no significant effects. The results in such a framework portray the effects of the shadow economy on demand for currency across economies. However, none of these studies examines the effect of tax enforcement on cash holdings. In relation to the connection between these two variables, Stix (2013) uses data for 10 Central, Eastern, and Southeastern European countries and finds that households in environments with weak tax enforcement have larger-than-average cash holdings. Importantly, the data do not include changes in tax enforcement policies in any country during the period of analysis.====Our paper is also linked to a literature exploring the relationship between the use of electronic payments and tax revenue (Immordino and Russo, 2018a; Alognon et al., 2020; Hondroyiannis and Papaoikonomou, 2020; and Madzharova, 2020). The idea behind is that electronic payments act as a deterrent to tax evasion because they build a trail for transactions, which may be avoided by using cash. In such a context, we should expect a negative relationship between the use of cashless payments and tax evasion. To reduce evasion and impose a cost on cash users, Immordino and Russo (2018b) propose a tax on cash withdrawals and a tax rebate conditional on having a receipt in a bargaining model of tax evasion. These papers are related to ours in terms of the importance of electronic means (in our case, electronic invoices) to deter tax evasion.==== However, we do not explore the role of electronic means on either tax revenue or welfare.====The welfare effects of suppressing cash have been studied by Alvarez and Lippi (2017), Hendrickson and Park (2021), and Garin et al. (2021). Alvarez and Lippi (2017) report small welfare costs of abolishing cash in a partial equilibrium framework. In contrast, Hendrickson and Park (2021) find that eliminating paper currency can be welfare improving if the negative externality of cash-intensive activities related to crime is large. Under a general equilibrium setting where cash facilitates tax evasion, Garin et al. (2021) show that suppressing cash combined with decreasing distortionary taxes to balance the government's budget can enhance welfare. In a general sense, our findings support the premise of Garin et al. (2021) that the demand for cash is motivated for tax evasion purposes. In relation to cash-abolishing policies, the Indian demonetization of 2016 is worth mentioning. Lahiri (2020) observes that it is hard to argue that such policy induced an increase in the collection of tax revenues, highlighting that India is a very informal economy where 44 million people are registered before fiscal authorities out of a labor force of 600 million. However, formal testing of the relationship between tax enforcement and cash holdings is missing. To proceed, in the next section we focus our attention on the effects of tax enforcement policies on cash demand under weak rule of law.",(In)Effective tax enforcement and demand for cash,https://www.sciencedirect.com/science/article/pii/S0164070421000525,24 July 2021,2021,Research Article,110.0
"Walerych Małgorzata,Wesołowski Grzegorz","Institute of Economics, Polish Academy of Sciences, Poland","Received 6 October 2020, Revised 3 April 2021, Accepted 28 June 2021, Available online 16 July 2021, Version of Record 29 July 2021.",https://doi.org/10.1016/j.jmacro.2021.103345,Cited by (4),"This paper presents evidence that the international ==== of both Fed and ECB conventional ==== to Emerging Market Economies (EMEs) are global. The result comes from the panel ==== Vector Autoregressive (BVAR) model estimated for EMEs in which we control i.a. for foreign central banks’ policy shocks. Furthermore, in the separate BVAR model for Central and Eastern European (CEE) countries we show that the ECB is the main foreign central bank for these economies — after controlling for its shocks, their Fed counterparts play a very moderate role in driving GDP and prices in CEE.","Monetary policy in the United States (US) and the euro area (EA) is believed to impact not only domestic economies but also other countries. Since many of them are open to capital flows and their exchange rates float, changes in short-term interest rates by major central banks affect prices and trade globally, see, e.g. [14] or [11]. This phenomenon has already received well-deserved attention separately in case of Fed and ECB policy spillovers to Emerging Market Economies (EMEs). Much less is known, however, about the relative importance of these two for international business cycles.====This paper aims to fill this gap and investigate conventional monetary policy spillovers of both the ECB and the Fed to EMEs. We focus on conventional monetary policy since, even though major central banks engaged in non-standard measures such as quantitative easing and forward guidance, they maintained the short-term interest rate as their main instrument, ready to be used when times normalize. In fact, by the end of 2018, Fed had already increased short-term rates in several steps. Therefore, it is important to understand how the main policy instruments of two major central banks spill over the boundaries. It is also worth to note that in our analysis we explicitly account for the role of expected policy changes by distinguishing between the impact of monetary policy shocks that have been anticipated by the markets and unanticipated monetary surprises. Thus, we go much beyond the standard approach that focuses solely on the latter.====The paper addresses two questions. First, given a well-established role of the Fed monetary policy for global business cycle, we investigate whether ECB shocks similarly spill over to EMEs. Second, given strong trade and financial linkages between Central and Eastern Europe (CEE) and the EA, we verify whether in this particular group of countries ECB spillovers are stronger than their Fed counterparts.====To this end we first derive unanticipated and anticipated policy changes in short-term interest rates in the EA and the US. Then, we estimate Fed and ECB monetary policy rules and use them to split fluctuations in unanticipated and anticipated policy rates into their systematic and shock components. We interpret the latter as the conventional monetary policy shocks and show that they intuitively and significantly affect their economies of origin. As the final step we estimate the panel Bayesian Vector Autoregressive (BVAR) models separately for two groups of countries: nine EMEs excluding CEE economies and three CEE countries. These two groups are homogeneous in a number of respects: they are small open economies, their trade is strongly linked to one of the advanced economies (either the US or the EA) and they have substantial financial linkages to the world financial system.====Our main finding is that the ECB conventional monetary policy spills over to EMEs even after excluding countries that are closely tied to the EA, i.e. CEE economies. This suggests that not only the Fed but also the ECB plays an important role for global business cycle. Second, we show that the ECB is the main foreign central bank for CEE — after controlling for its shocks, their Fed counterparts play a very moderate role in driving GDP and prices in economies that are tightly linked to the EA.====The paper is closely related to [23] who investigated the propagation of Fed news (anticipated) and surprise (unanticipated) shocks to a set of EMEs. We add to the literature by focusing on the relative importance of Fed and ECB conventional policy spillovers. Furthermore, we propose the shock identification procedure that explicitly accounts for the effective lower bounds episodes. Finally, differently from the Vicondoa’s 2019 work, we estimate panel BVAR models with hierarchical priors inspired from [15] instead of the classical panel model. Using Bayesian approach mitigates the problem of short data samples available for the analysed countries.====Our paper relates also to other works deriving and investigating monetary policy shocks. The first strand of research applies VAR models to identify structural monetary policy shocks with various properties, see, e.g. [3] and [15]. Other papers, notably Altavilla et al., 2019, Gertler and Karadi, 2015 and [22], use high-frequency data on asset prices to derive policy surprise components around central banks’ announcements of their decisions. [16] combine the high-frequency identification with sign restrictions and show that news about central bank lowering rates may be contractionary information shock by affecting agents expectations about worsening economic outlook. The importance of central bank information shock is also documented by [18]. Finally, [20] is a seminal contribution to the narrative approach to monetary policy shock identification.====Moving to spillover aspect of our research, we build on the rich literature that points to a strong impact of US short-term rates on the global economy, see e.g. Dedola et al., 2017, Georgiadis, 2016, di Giovanni and Shambaugh, 2008, Hanisch, 2019, Maćkowiak, 2007, Vicondoa, 2019 or [4]. There is also a number of papers that address ECB policy spillovers, such as, e.g. Babecká Kucharčuková et al., 2016, Potjagailo, 2017 or [12]. In case of CEE economies considered in some of these articles, we add to literature by investigating whether the ECB’s policy spills over similarly to the Fed’s. Our paper addresses this issue utilizing a consistent framework that accounts for both anticipated and unanticipated policy shocks of these two main central banks.====The rest of the paper is structured as follows. Section 2 explains how we model conventional policy in the US and the EA and derives monetary policy shocks. Section 3 describes the empirical model which we use to investigate policy spillovers to emerging countries and presents the main results of the paper. Section 4 concludes. The Appendix checks the robustness of our main findings to a number of specific modelling assumptions.",Fed and ECB monetary policy spillovers to Emerging Market Economies,https://www.sciencedirect.com/science/article/pii/S0164070421000483,16 July 2021,2021,Research Article,111.0
"Damjanovic Tatiana,Damjanovic Vladislav,Nolan Charles","Durham University, United Kingdom of Great Britain and Northern Ireland,University of Glasgow, United Kingdom of Great Britain and Northern Ireland","Received 26 November 2020, Revised 18 May 2021, Accepted 28 June 2021, Available online 16 July 2021, Version of Record 20 July 2021.",https://doi.org/10.1016/j.jmacro.2021.103346,Cited by (1),"We discuss a time invariant policy which delivers the unconditionally optimal outcomes in purely forward-looking models and Ramsey outcomes in purely backward-looking models. This policy is a product of interaction between two institutions with distinct responsibilities. Motivated by Brendon and Ellison (2015), we think of them as arms of government. One institution is responsible for ‘forward guidance’, setting rules which are necessary and sufficient to determine private expectations. The second institution implements optimal policy taking expectations as given. The forward guidance rules are designed to maximise the unconditional expectation of the social objectives.","Ramsey policy is time inconsistent in models with forward-looking behaviour (Kydland and Prescott, 1977). That is because the government can affect the economy via its current and future actions. Future policy influences current outcomes via an expectations channel, whilst current policy only affects the economy contemporaneously. Therefore the optimal policy at ‘period zero’ is different from future optimal policy. That property of optimal Ramsey rules is known as time inconsistency and typically implies that it will not be optimal for policymakers to make good on policy promises when the time arrives to deliver on those promises.====To deal with time inconsistency, two types of time invariant rules are often considered in the literature. The first analyses policies which maximise the unconditional expectation of the social objective (unconditionally optimal, or UO, policy) proposed by Taylor (1979).==== The second analyses policies that are optimal from a Timeless Perspective (TP policy) introduced by Giannoni and Woodford (2002), which assumes commitment to Ramsey policy designed many periods ago. Commitment to these invariant rules is tantamount to assuming that institutions have been devised which deliver the outcomes associated with these rules although that assumption is rarely, if ever, made explicit. We discuss this issue in a little more detail below.====In many cases UO and TP policies perform very similarly. Indeed, Blake (2001) and Damjanovic et al. (2008) proved that TP can be converted to UO policy if the government were accounting for all generations equally: that is, if it sets the social discount rate equal to zero. Nevertheless, TP and UO policies have important theoretical and sometimes quantitative differences and neither obviously dominates the other.====On the one hand, TP policy can lead to non-stationary outcomes in models with forward-looking constraints (Blake and Kirsanova, 2004, Benigno and Woodford, 2012). In particular, Schmitt-Grohe and Uribe (2004) demonstrate that TP policy results in non-stationary dynamics of government debt. Unlike the TP, the UO policy implies stationarity by design, since any non-stationarity would result in infinitely large expected value of the loss function.==== Moreover, TP policy may put an unreasonably large weight on a relatively distant event in the past, which is not the case for UO policy (Jensen and McCallum, 2010).====On the other hand, TP policy has a number of attractive features in models with only backward looking constraints. Thus, it coincides with Ramsey optimal discretionary policy. That is not the case for UO policy which requires a commitment device even in purely backward looking models. And whilst it is true that the UO policy would dominate TP policy conditional on the fact that all generations had followed it in the past, since the current generation would have had a better start in terms of economic environment,==== even in this case, it is still optimal for the current generation to deviate towards TP. Therefore, there is a sense in which TP policy is more stable and could be preferable in models with backward-looking constraints.====As UO policy performs better in forward-looking models whilst TP/Ramsey policy is sometimes more desirable in backward-looking models, the question is which policy to use if an economy has both types of constraints. In this paper we discuss a time invariant policy which inherits properties of the TP policy in backward-looking models and UO in forward-looking models. We will call this policy UO-Ramsey. Our paper shows an easy, intuitive and transparent way to design such a policy.====Following Brendon and Ellison (2015) we consider a little more explicitly the issue of institutional design. In particularly, policies are designed by two authorities with distinct and distinctive responsibilities–we think of these two authorities as arms of government. One arm of government (“====” government in Brendon and Ellison), is responsible for forward guidance.==== The outer government makes promises and determines private expectations about future policy outcomes. The second arm of government (“====” government) implements policy taking promises and corresponding private expectations as given. In this framework, expectations are taken as exogenous and therefore cannot be changed by the inner government. As the inner government cannot use the expectations channel to affect the economy, the inner policy maker does not face any problem related to time consistency.====Our main contribution compared to Brendon and Ellison (2015) is that we propose an alternative way to design the problem of the forward-guiding, outer government which is responsible for expectations formation. In this paper we show that the outer government maximises the unconditional expectations of social objectives. Hence, although the policy outcome is the same as Brendon and Ellison (2015), our approach serves to illuminate the fundamental objectives of the outer government in an intuitive and transparent way.====After presenting the design of UO-Ramsey policy an interesting application is pursued. We design UO-Ramsey policy in a linear-quadratic model with a so-called hybrid Phillips curve. This case can easily be nested to either the purely forward-looking new Keynesian model or to the purely backward-looking model where Ramsey policy is time consistent. Our policy will deliver UO policy in the first case and Ramsey policy in the second.====Finally we evaluate the welfare gain. The policy delivers a negligible welfare gain in a time of low volatility. However, the welfare gain can be significant in a time of higher uncertainty, high private discounting, and when the government increases the weight of the output gap in the loss function.====The paper is structured as follows. Section 2 describes and explains the design of UO Ramsey policy. Section 3 applies the UO-Ramsey policy to a linear-quadratic model with hybrid Phillips curve. Section 4 concludes.",Unconditionally optimal Ramsey policy,https://www.sciencedirect.com/science/article/pii/S0164070421000495,16 July 2021,2021,Research Article,112.0
"Cho Daeha,Han Yoonshin,Oh Joonseok,Rogantini Picco Anna","Department of Economics, University of Melbourne, 111 Barry St, Carlton VIC 3053, Australia,Korea Deposit Insurance Corporation, Cheonggyecheon-ro 30, Jung-gu, 04521 Seoul, Republic of Korea,Chair of Macroeconomics, School of Business and Economics, Freie Universität Berlin, Boltzmannstrasse 20, 14195 Berlin, Germany,Monetary Policy Department, Research Division, Sveriges Riksbank, SE 103 37, Stockholm, Sweden","Received 19 October 2020, Revised 24 March 2021, Accepted 28 June 2021, Available online 15 July 2021, Version of Record 24 July 2021.",https://doi.org/10.1016/j.jmacro.2021.103343,Cited by (8),", implying no policy trade-offs. Our result suggests that precautionary pricing matters only insofar as ==== is volatile. Thus, a simple ","Time-varying uncertainty has recently received considerable attention from policymakers and academics, spurring the burgeoning literature on identifying transmission mechanisms of uncertainty shocks. It has been shown that a precautionary pricing motive is an important mechanism that amplifies uncertainty shocks. This mechanism is present in New Keynesian models when sticky prices are modeled according to Calvo (1983) and monetary policy follows an empirical Taylor rule. Due to the presence of a precautionary pricing motive, uncertainty shocks behave like cost-push shocks; a rise in uncertainty causes an increase in inflation but a fall in the output gap. A classic and important question for policymakers is whether these shocks generate the well-known output–inflation trade-off that emerges in response to cost-push shocks.====This paper studies optimal monetary policy when the precautionary pricing channel is present. Our main result is that the output gap and inflation are both stabilized under optimal monetary policy, meaning that policy trade-offs do not emerge. Our finding suggests that the precautionary pricing channel is operative only in an environment in which inflation is volatile. Therefore, monetary policy that fully stabilizes inflation eliminates the inefficiencies related to the precautionary pricing channel, thus allowing policymakers to attain efficient allocation.====Our conclusion is drawn from comparing allocations under optimal monetary policy in two popular price-setting approaches. The first is Calvo (1983) pricing, under which firms face a constant probability of not being allowed to reoptimize their price in every period. The second is Rotemberg (1982) pricing, under which firms can always adjust their price upon payment of a quadratic price adjustment cost. While the precautionary saving motive is operative under both Calvo and Rotemberg pricing, precautionary pricing is only operative with Calvo pricing (Oh, 2020). Accordingly, comparing allocations under the optimal monetary policy in Calvo and Rotemberg allows us to evaluate the extent to which precautionary pricing matters for a monetary policy prescription.====Specifically, under Rotemberg pricing, uncertainty shocks act as negative demand shocks; a rise in uncertainty increases the households’ precautionary savings motive, which causes a decrease in both inflation and the output gap. In contrast, under Calvo pricing, a rise in uncertainty triggers firms’ precautionary pricing motives along with households’ precautionary saving motives. A precautionary pricing motive stems from firms’ exposure to the risk of not being able to set their desired price level in the future. Under Calvo, as long as the future expected inflation is volatile, this risk is always present. Price-resetting firms that are exposed to such risk raise prices today to hedge against an uncertain future profit stream. This causes a rise in inflation and a sharper fall in the output gap, as the resulting inflation increase further compresses aggregate demand. Therefore, because of the precautionary pricing channel, uncertainty shocks are more amplified in Calvo than in Rotemberg. We find that, under optimal monetary policy, the differences between allocations in Calvo and Rotemberg disappear. This implies that the precautionary pricing channel is not operative under optimal monetary policy. Moreover, the joint stabilization of the output gap and inflation in Rotemberg suggests that a precautionary saving motive does not pose any policy trade-off, which is consistent with the property of demand shocks in textbook New Keynesian models.====The joint stabilization of the output gap and inflation under optimal monetary policy suggests that the divine coincidence holds in the case of uncertainty shocks; inflation stabilization also brings about the output gap stabilization. Thus, a simple rule that places extremely high weight on inflation (i.e., the strict inflation targeting rule) closes the output gap. It is worth noting that the divine coincidence does not emerge in response to uncertainty shocks in all models. As discussed in Blanchard and Galí (2007), the divine coincidence emerges only in the absence of nontrivial real imperfections. We confirm their results by showing that a trade-off between the output gap and inflation arises in response to uncertainty shocks when real wage rigidities are introduced.","Uncertainty shocks, precautionary pricing, and optimal monetary policy",https://www.sciencedirect.com/science/article/pii/S016407042100046X,15 July 2021,2021,Research Article,113.0
"Ruan Xinfeng,Zhang Jin E.","Department of Accountancy and Finance, Otago Business School, University of Otago, Dunedin 9054, New Zealand","Received 28 January 2021, Revised 27 June 2021, Accepted 30 June 2021, Available online 6 July 2021, Version of Record 12 July 2021.",https://doi.org/10.1016/j.jmacro.2021.103347,Cited by (1),"This paper extends the AK production model in Pindyck and Wang (2013) into a more general setting in which the volatility of capital stock is stochastic and driven by shocks. After solving the equilibrium, the fundamental shocks are embedded into the stock price and the leverage effect is contributed from three distinct channels. As an application, we employ our extended AK production model to match well the negative variance risk premium.","How do macro fundamental risks affect the stock price? How do we derive a realistic model for the stock price from the fundamentals in order to explain several financial phenomena (e.g., the negative variance risk premium)? This paper extends Pindyck and Wang’s (2013) AK production model to answer these two questions. Pindyck and Wang (2013) propose an equilibrium model with an AK production technology, adjustment costs, and the downward jumps (or “shocks”) to capital stock.==== In this paper, we further extend this to the volatility of capital stock that is stochastic and driven by shocks. Through the equilibrium, all fundamental risks are embedded into the price of the stock, so that it follows a stochastic volatility with contemporaneous jumps (SVCJ) model, which is in line with Broadie et al. (2007) and Neumann et al. (2016) for option pricing. The leverage effect embedded in our SVCJ model is contributed from three distinct channels: the market’s aggregate financial leverage, a systematic negative correlation between capital stock and its volatility, and the negative correlated jumps in the stock’s returns and its volatility. As an application, we derive the model-based variance risk premium (VRP) and then find that our extended AK production model captures well the high negative VRP.====The AK production model studied by Pindyck and Wang (2013) is developed from the production-based asset pricing model in Cox et al., 1985, Bates, 1991, Bates, 1996, Bates, 2000 and neoclassical investment theory (e.g., q theory) in Hayashi (1982). This helps us to understand the financial market from the angle of the firm’s financing instead of the angle of dividend cash flows in the consumption-based asset pricing model (e.g., Lucas, 1978; Duffie and Zame, 1989). Recently, Baker et al. (2016) extend Pindyck and Wang’s (2013) AK model by including investors’ disagreement on the dynamics of capital stock. Xu (2017) considers the volatility risk in capital stock and studies the effects of time-varying volatility on welfare. Unlike these extensions, this paper develops Pindyck and Wang’s (2013) model in a different way; that is, we allow that the volatility of capital stock is stochastic and contains occurrences of rare events. Based on that, we derive a more realistic model for the stock price in order to explain the negative VRP and the Black–Scholes implied volatility surface.====In our production economy, the stochasticity of capital-stock volatility captures the changes in uncertainty. Our extension is consistent with the literature on the impact of uncertainty on economic activity, for example, Bloom, 2009, Fernández-Villaverde et al., 2011, Christiano et al., 2014, Fernández-Villaverde et al., 2015, Leduc and Liu, 2016, and Xu (2017), which highlight time-varying volatility (i.e., volatility shock) as an important driver of business cycles. The literature has so far mainly focused on the effects of volatility shocks on economic activity. This paper aims to fill part of the gap by analyzing the relationship between volatility risk and its risk premium. The constant-volatility AK production model in Pindyck and Wang (2013) fails to capture volatility risk and its risk premium.====We are interested in using our extended AK production model to explain the high negative VRP, as it is very important for not only researchers but also practitioners. Practitioners pay a fee to hold variance swaps in order to eliminate the downside risks in the underlying assets. This is because of the negative correlation between the underlying and its volatility products (which is due to the leveraged effect, according to Carr and Wu, 2017). Recently, Bollerslev et al., 2009, Drechsler and Yaron, 2011, Bollerslev et al., 2012, Drechsler, 2013, and Jin (2015) have adopted long-run risks models to explain the large average VRP and Buraschi et al. (2014) use a two-tree (Lucas, 1978) economy with two heterogeneous investors to explain it. In contrast, we employ our production-based asset pricing model to investigate this crucial phenomenon and find that it works well. This paper complements the literature.====The paper closest in spirit to our study is Ruan and Zhang (2018), who extend the cost-free economy in Zhang et al. (2012) in order to explain the VRP. In Ruan and Zhang’s (2018) cost-free economy, there is no cost of installing capital, and the firm’s productivity and the Tobin’s q both are assumed to be a unity. This leads to the stock price being the same as the capital stock. In contrast to Ruan and Zhang (2018), the economy in this paper has an AK production technology and installing capital costs. Furthermore, the stock price in this paper is endogenously solved in equilibrium, which further helps us to understand the three channels contributing the leverage effect, that is, the generally negative correlation between an asset’s return and its changes of volatility.====The main contribution of this paper is to extend the AK production model in Pindyck and Wang (2013) and then use it to dissect the leverage effect and successfully explain the negative VRP.====The remainder of the paper is organized as follows. Section 2 presents the framework. Numerical results are given in Section 3 and model limitations and intuitions are discussed in Section 4. Section 5 concludes.",Time-varying uncertainty and variance risk premium,https://www.sciencedirect.com/science/article/pii/S0164070421000471,6 July 2021,2021,Research Article,114.0
"Rossi Lorenza,Zanetti Chini Emilio","University of Pavia, Department in Economics and Management, Via San Felice 5, 27100 Pavia, Italy,Sapienza University of Rome, Department in Economics and Law Via del Castro Laurenziano 9, 00161 Roma, Italy","Received 9 July 2020, Revised 27 May 2021, Accepted 3 June 2021, Available online 23 June 2021, Version of Record 25 June 2021.",https://doi.org/10.1016/j.jmacro.2021.103337,Cited by (0),"We provide new disaggregated data and stylized facts on firm dynamics ==== at establishment level by using a state-space method to transform Census yearly data of entry and exit from 1977 to 2013 into quarterly frequency. We select the most significant determinants of these variables by matching Census data with a new database by Federal Reserve Bank. These determinants are extrapolated by using an unobserved factor model whose loadings are estimated via ====. Alternative sources and their data are also investigated and discussed. We find that (i) Entry is pro-cyclical, coincident and symmetric; (ii) Exit is lagging with a maximum positive correlation with RGDP at lag 5 and asymmetric along the business cycle; (iii) the standard ==== estimated on our disaggregated series support the recent theoretical literature.","Firms are one of the pillars of any economic system. Their dynamics is an important indicator of the status of the economic activity. Having a clear view of their creation and destruction is fundamental in order to understand the business cycle phase and address a properly calibrated policy. Despite the relevance of this issue, however, there is a lack of long-span quarterly dataset on firms dynamics, even for the U.S. economy. In fact, the dataset currently published by Census Bureau is yearly and constitutes the only long-span data source on the argument.==== On the other side, the quarterly time series published by the Bureau of Labor Statistics (BLS, henceforth), start in the second quarter of 1993, while data provided by the Bureau of Economic Analysis (BEA, henceforth) are monthly and span up to 1996. Hence, the only time series suitable of immediate application in macroeconometric models faces with limited – and limiting – frequency. Moreover, the starting date of BLS series does not allow a comprehensive long-span analysis as required by modern macroeconometric science. This scarcity of data at high frequency motivates this paper, which contributes to the macroeconomic literature in the provision of new disaggregated series on counts of entering and exiting establishments for the U.S. economy. These are supposed to complement the yearly Census data using a state space form (SSF, henceforth)-based model. Our data are used to investigate (i) the business cycle properties of the these variables; (ii) their reaction to a productivity shock; (iii) finally, the asymmetry of firm dynamics.====For what concerns the contribution, for the first time in the literature, we disaggregate the yearly data on ENTRY and EXIT in levels currently in use by relying on the FRED-MD, a new, large macroeconomic database describing all sectors of the U.S. economy published by Federal Reserve Bank of St. Louis. Moreover, to the best of our knowledge, this is the first paper that identifies the determinants of the dynamics of the new disaggregated time series among the large number of possible variables in the same FRED-MD. This requires an empirical exercise of variable selection and allows economists to avoid to infer on (qualitatively very different) proxies like creation and destructions of jobs adopted by recent empirical literature; see, among the others, Davis and Haltiwanger, 1992, Davis and Haltiwanger, 1999 and Davis et al. (2012).====Concerning the use of the disaggregated data, we carry out a descriptive analysis on the business cycle properties of our disaggregated series, jointly with a Structural Vector AutoRegression (SVAR, henceforth) analysis to test their dynamic performances in response to a positive productivity shock in order to test whether the implied responses of ENTRY and EXIT at quarterly frequency are able to confirm some recent theoretical findings and namely: (i) the pro-cyclicality of firm creation; (ii) the overshooting of firms destruction described in Rossi (2019) and also found in Hamano and Zanetti (2017).==== Further, we test a peculiar kind of asymmetry in our disaggregated series. In all these exercises we compare the performance of our disaggregated series with two quarterly series of establishment Births and Death for total private sector (henceforth, ‘Births’ and ‘Deaths’) released by the Bureau of Labor Statistics (BLS) through the Business Employment Dynamics (BED) database.====Concerning the business cycle analysis, we extract the trend and cycle components via the standard Hodrick and Prescott (1997, HP) and Baxter and King (1999, BK) filters and, subsequently, we classify them as leading or lagging indicators of the business cycle by looking at the maximum absolute value of cross-correlations between the cyclical components of ENTRY and EXIT and the one of real GDP (RGDP, henceforth). According to our results, while ENTRY is pro-cyclical and coincident indicator, EXIT can be classified as a lagging indicator, showing a maximum and positive correlation with RGDP at lag 5. In this respect, this paper complements the empirical investigation by Tian (2018), with respect to whom we deepen the economic analysis on the establishment-level data====Overall, our analysis shows that the two types of indicators considered in this paper (Deaths and EXIT) are very similar, though coming from a different data source.====Secondly, we perform a test to understand if EXIT (ENTRY) is (a)symmetric around the mean, that is to investigate whether their short-run patterns change along different business cycle phases of boom and downturn.==== To this aim, we assume that the same time series follows a generalized smooth transition autoregression (GSTAR) introduced by Zanetti Chini (2018) because it allows the asymmetric cycles by construction. Subsequently, we provide a test for the null hypothesis that our disaggregated data present the same rate of acceleration in the cycle either in recession either in expansion. This hypothesis is rejected in many cases of EXIT and often not rejected in ENTRY, meaning that the latter is symmetric, while the former is asymmetric.====Finally, the last part of the paper shows a validation exercise of our disaggregated method. We replicate our disaggregation exercise to data that are disposable at both annual and quarterly frequency. For this reason we disaggregate data on Birth and Death by using our disaggregation method and indices. Then, we compare the disaggregated series with the correspondent official quarterly series of Births and Deaths. The comparison is done in terms of correlations, business cycle analysis (cross correlations at different leads and lags) and asymmetry test. Our results shows that the above mentioned findings do not depend on the disaggregation method used and that the business cycle properties of the disaggregated series are therefore intrinsic characteristics of the series.====At the current state, the theoretical contributions on this topic are mainly focused on firms’ entry.==== Fewer papers model firms exit in such a peculiar framework.==== For what concern the literature of temporal disaggregation, since the seminal contribution by Friedman (1962), this literature has considerably evolved in several directions: Denton (1971) consider the disaggregation problem via constrained maximization; Chow and Lin, 1971, Fernandez, 1981 and Litterman (1983) via static regression models; the SSF method capable to encompass many of the previously mentioned models estimated by Augmented Kalman Filter (AKF), see Quilis (2018) for an up-to-date survey.====The paper is so organized: after a comprehensive discussion of data in Section 2, we describe the statistical methodology in Section 3. Section 4 provides the illustration of our econometric results. Finally, Section 5 concludes. A separate Supplement gives additional results and details on statistical methods here adopted.",Temporal disaggregation of business dynamics: New evidence for U.S. economy,https://www.sciencedirect.com/science/article/pii/S0164070421000422,23 June 2021,2021,Research Article,115.0
Lu Chia-Hui,"Department of Economics, National Taipei University, 51, University Rd., San-Shia, 23741 New Taipei, Taiwan","Received 20 January 2021, Revised 14 June 2021, Accepted 16 June 2021, Available online 20 June 2021, Version of Record 22 June 2021.",https://doi.org/10.1016/j.jmacro.2021.103342,Cited by (12),"Focusing on the self-accumulation ability and the nonrival characteristic of artificial intelligence (AI), this paper develops a three-sector ==== and investigates the impact of the development of AI along the transitional dynamics path and the balanced growth path. The development of AI can increase economic growth along the transitional dynamics path, and can increase household short-run utility if an increase in the accumulation of AI is due to the rising productivity in the goods or AI sector, but can be detrimental to household short-run utility if an increase in the accumulation of AI is because firms use more AI to replace human labor. In addition, the development of AI is not necessarily beneficial to household welfare in the long run. The main results are unaffected when considering the case where AI can improve the accumulation of human capital, the traditional research and development model, and different kinds of physical capital.","The purpose of this paper is to construct a basic endogenous growth model with artificial intelligence (AI), and to explore the impact of the development of AI on economic growth and welfare. The innovation of this paper is that we incorporate the self-accumulation ability and the nonrival characteristic of AI into the model. In addition, we also consider its ability to replace human labor. The contribution of this paper is that we can investigate the impact of AI accumulation along the transitional dynamics path and in the long run.====AI is considered to have the potential to instigate a fourth industrial revolution,==== and is dramatically changing people’s patterns of interaction and economic activities. The traditional factors of production and physical capital and labor may no longer promote substantial economic growth. It is generally believed that AI will be one of the most important factors determining future economic growth.==== However, unlike traditional machines, which replaced the use of human and animal labor for simple manual work and heavy or dangerous activities, AI-related inputs may change the type of human work in a comprehensive way.====In contrast to previous industrial revolutions, AI does not simply involve the invention of a new machine or technology. Instead, AI has similarities to the accumulation of human capital, as it can learn and accumulate knowledge by itself. For example, AlphaGo, a computer program that plays the board game Go, which brought widespread attention to AI, can beat human professional Go players. The new successor, AlphaGo Zero, is a version that has been created without using data collected from games played by humans. The programmers of this new version of the game did not use the available accumulated human intelligence used by Alpha Go. Instead, they programmed only the most basic rules of Go into AlphaGo Zero and then allowed it to play games against itself. Everything started from scratch. Only relying on the deep learning and reinforcement of the machine, AlphaGo Zero exceeded the ability of all previous versions of the game in only 40 days.====The above application of AI implies that, by using machines (physical capital, such as computers), human labor (human capital, such as programmers), and AI, the AI can learn by itself. This is the first characteristic of AI that we explore and the innovation in this paper: the self-accumulation ability of AI and its required elements. In addition, AI, as ideas, is a nonrival input. It can be used for production, for automation, and for generating new AI at the same time. AI is similar to a certain technology or knowledge. When this technology is used in the production of final goods, it does not detract from its ability to accumulate AI. This makes AI a different type of capital from physical and human capital, and is the second characteristic of AI that we explore and the innovation in this paper: the nonrival feature of AI.====Nowadays, AI is an important input to production in many industries and is expected to eventually replace human resources. Morikawa (2017) collected original survey data from more than 3000 Japanese firms operating in both the manufacturing and service sectors to investigate their views about the impacts that AI may have on business and employment in the future, and found that firms expect favorable impacts of AI and robotics on their businesses and generally think upgrading human capital is important. That is, the pattern of employment will change in the future. In addition, Acemoglu and Restrepo (2018a) considered the situation in which AI replaces labor, and discussed the impact of automation on labor demand, wages, and employment. They further examined the mismatch between skills and technologies, which will reduce the productivity gains from new technologies. Furthermore, Berg et al. (2018) built models with different elasticities of substitution between robots and labor, and analyzed how automation may transform the labor market in different situations. The above studies suggested that AI may replace human labor, and we thus further consider this feature in this article.====Existing macroeconomic models, however, seldom discuss AI, and therefore, they ignore its self-accumulation ability, which occurs as the machine learns or as people input information into the AI. The endogenous growth models with research and development (R&D) pioneered by Romer (1990) and Aghion and Howitt (1992) usually treat such innovation as an intermediate good produced by a monopolist. However, the characteristics of machines with AI are quite different. They can directly enter the production process and can even replace human labor. In addition, AI, like human capital, can learn and accumulate by itself. Therefore, the traditional R&D model may not fully explain the impact of AI.====Moreover, AI is generally considered important for future economic growth. The growth accounting conducted for the US economy in Fernald and Jones (2014) showed that R&D contribute most to the economy. The authors considered that the possibility of AI allowing machines to replace workers, to some extent, could lead to higher growth in the future. Recent studies, including Acemoglu and Restrepo (2018b), Hémous and Olsen (2016), and Aghion et al. (2017), have discussed the potential implications of AI for the growth process. However, those papers modeled AI in terms of new ideas, as in Romer (1990) and Aghion and Howitt (1992), and they did not take into account the simultaneous contributions to human and physical capital. In addition, Acemoglu and Restrepo (2017) considered low-skill and high-skill automation and discussed wage inequality.====In contrast to the above studies, we introduce AI’s ability of self-accumulation and its nonrival characteristic into the (Lucas, 1988) endogenous growth model. We treat AI as an input like human and physical capital, which can directly enter the goods production process and replace workers. People can distribute resources to develop AI, just as in the process of the accumulation of human capital. The innovation contribution of this paper is that we introduce the self-accumulation ability and the nonrival characteristic of AI into an endogenous growth model and explore the impact of the development of AI on macroeconomic performance. In this paper, we investigate the impact of the development of AI using transitional dynamics and the balanced growth path. Exploring the impact along the transitional dynamics path is especially important because it helps us understand and predict the future impact of AI.====The structure of the paper is as follows. In Section 2, we construct a benchmark model and prove the existence of the long-run equilibrium. Section 3 analyzes the impact of AI along the transitional dynamics path and provides some comparative statics analysis in the long run. Section 4 considers the case in which AI can improve the accumulation of human capital. Section 5 compares our benchmark model with the traditional R&D model. In Section 6, we consider different kinds of physical capital in our model. Section 7 offers some concluding remarks.",The impact of artificial intelligence on economic growth and welfare,https://www.sciencedirect.com/science/article/pii/S0164070421000458,20 June 2021,2021,Research Article,116.0
"Tayler William J.,Zilberman Roy","Lancaster University Management School, Department of Economics, LA14YX, United Kingdom","Received 19 January 2021, Revised 16 April 2021, Accepted 7 June 2021, Available online 18 June 2021, Version of Record 23 June 2021.",https://doi.org/10.1016/j.jmacro.2021.103338,Cited by (0),"We study the welfare implications of optimal loan loss provisions in a ==== featuring endogenous default risk and inflationary credit spreads. A unique link between provisions, credit spreads and ","In the aftermath of the financial crisis and the Great Recession, the Basel Committee on Banking Supervision together with the International Accounting Standards Board (IASB) have called for the transition from ==== loan loss provisioning systems (incurred-loss approach) towards a more ==== provisioning regime (expected-loss approach). Until January 2018, the provisioning system in most advanced economies was specific and tied by the International Accounting Standards (IAS) 39, which required banks to set specific provisions related to identified credit losses, such as past due payments (usually 90 days) or other default-like events. Critics of the IAS 39 have argued that setting provisions in an ex-post fashion, after the identification of nonperforming loans, comes too late in the cycle, with the additional provisioning expenses in banks income statements exacerbating the procyclical tendencies of the financial system (see also Jiménez et al. (2017)).====In contrast, according to the new International Financial Reporting Standards (IFRS) 9, which have replaced the IAS 39 as of January 2018, dynamic provisions should be set in a timely manner before credit risk materializes, and allow the financial sector to better absorb losses by drawing upon these provisions in the wake of a negative credit cycle. In this way, provisions are established according to the expected-loss impairment model, potentially curbing procyclicality in the credit markets and providing a more accurate profit and loss account. More specifically, expected provisioning ought to smooth out the evolution of total loan loss provisions, and reduce the need of financial intermediaries to increase costly loan loss reserves during financial downturns. Prior to the IFRS 9, and already back in 2001, the Bank of Spain was one of the first central banks to introduce elements of the expected-loss provisioning system in an effort to dampen excessive credit growth. Under the expected-loss approach, banks must make provisions according to the latent risk over the business cycle, or based on the historical information regarding credit losses for different types of loans. By anticipating better the expected losses lurking in a loan portfolio over some specific horizons, IFRS 9-type provisions aim to promote financial stability.====Despite the recent adjustments in global financial accounting standards and their significance in driving credit cycles, the theoretical literature on the macroeconomic stabilization roles of loan loss provisions and their welfare optimizing behavior is rather limited. Motivated by the recent implementation of more prudential-type provisions, and the limitations of standard monetary policy as a tool for macroeconomic stabilization, as has been evident over more than a decade, in this paper we address the following interrelated questions: (i) what should be the ==== design of loan loss provisions in response to financial market disruptions associated with inflationary and volatile credit spreads; (ii) what are the macroeconomic and welfare implications of optimally varying provisions against the backdrop of state-contingent liquidity traps?====We answer these questions by providing closed-form analytical and quantitative results in a small-scale workhorse New Keynesian model à la Woodford (2003). The basic framework is augmented for a supply-side collateral constraint that gives rise to endogenous default risk and inflationary credit spreads. Default risk is determined by the borrowers’ leverage, measured in terms of the debt to collateralized output ratio, while the credit spread is shown to be endogenous with respect to risk and costly loan loss provisions. This supply-side financial market friction results in a distorted steady state allocation and in inefficient economic dynamics, both of which call for the implementation of corrective credit policies. Such policies in this paper take the form of state-contingent ==== loan loss provisions that are primarily designed to ==== business and credit cycles that result in occasional liquidity traps.====As in Ravenna and Walsh (2006), firms in our model must borrow from a bank in order to finance their labor costs.==== Therefore, monetary policy, credit risk and loan loss provisions, all of which dictate the loan rate behavior, translate into changes in the firms’ marginal costs, price inflation and output through the credit cost channel.==== Importantly, part of this cost-push conduit is driven by the ==== relating provisions directly to inflation and the real economic activity. Such unique association has important and novel implications for the design of optimal financial policies aimed at enhancing overall welfare. We argue that optimal provisioning practices are most effective when dealing with inflationary financial shocks that result in high borrowing costs and occasional supply-side-driven liquidity traps. Optimal loan loss provisions can consistently achieve the first-best allocation regardless of whether the nominal interest rate is constrained by the zero lower bound (ZLB) or not. In a demand-side-driven liquidity trap instigated by adverse shocks to the natural rate of interest, prudential regulation policies like the ones proposed by the IFRS 9 bring about a variant of the ==== (as popularized by Eggertsson (2010)), wherein otherwise expansionary supply-side measures can paradoxically lead to lower welfare. In our paper, this paradox is highlighted through the provisioning cost channel.====Supporting Gilchrist et al. (2017), adverse financial shocks that raise the lending rate produce inflationary pressures, and result in an output-inflation trade-off. This upshot motivates us to examine optimal financial policies beyond traditional interest rate policy. Following a cost-push credit disturbance, optimal monetary policy under commitment sends the nominal policy rate to its zero bound due to the large inefficient contraction in output that is amplified by the enforcement of IAS 39 specific provisions. We demonstrate that ==== provisioning requirements, even ==== the full-smoothing of provisions as proposed by the new IFRS 9 regulation, can be a very effective way to restore complete price and output stability by directly minimizing variations in borrowing costs and eliminating the zero bound problem. In particular, a mildly countercyclical response of loan loss provisions solves the standard output-inflation trade-off induced by the financial disruption and the credit cost channel. These results hold regardless of how the policy maker implements monetary policy, with loan loss provisions removing the necessity for any costly and time-inconsistent monetary policy commitments. In our model, loan loss provisions stand out as a sole and natural policy reaction against financial shocks inherent in high and volatile borrowing costs that result in occasional supply-side-driven liquidity traps.====Adverse deflationary demand shocks warrant an optimal ==== in provisions and a prolonged ZLB interest rate policy. Intuitively, raising provisions provides a cost-push effect that lifts the credit spread and increases prices through the credit and provisioning cost channels. We find that higher provisions result in a mild short-run contraction in output, which is optimal when the public authority places a much higher weight on price stability than on output stability in the micro-founded welfare function. At the same time, dynamic and optimal loan loss provisions yield minimal welfare gains relative to the more constrained optimal monetary policy under commitment. These quantitative results prevail despite the qualitatively different impulse response functions. In other words, stabilization following negative shocks to the natural rate of interest is mainly attributed to the optimal and loose monetary policy. From a purely welfare perspective, whether to increase, flatten or lower provisions over the cycle depends largely on what triggers the business cycle and pressingly on the output-inflation dynamics that arise.====Much of the earlier literature on loan loss provisions has empirically examined the role of provisioning practices in shaping the financial cycle (see Laeven and Majnoni, 2003, Bikker and Metzemakers, 2005 and Jiménez and Saurina (2006), among others). Moreover, in a recent contribution, Jiménez et al. (2017) provide an exhaustive empirical study on the impact of dynamic provisions on the Spanish credit cycle. These authors show that dynamic provisioning smooths credit supply cycles and supports firm performance in bad times. From a theoretical standpoint, we highlight the importance of provisions in also explaining the behavior of real business cycles and inflation, as well as their optimal dynamics and welfare implications.====Our findings also extend the theoretical literature on the effectiveness of loan loss provisioning regimes. Bouvatier and Lepetit (2012a) develop an analytical partial equilibrium model, and show that dynamic provisions, defined by accounting rules to cover for expected losses, can eliminate procyclicality in lending standards induced largely by specific provisions. Agénor and Zilberman (2015) use a standard medium-scale calibrated New Keynesian model and illustrate that dynamic provisions can help mitigate financial and real sector volatility, even more so when implemented together with a credit-augmented monetary policy rule. Unlike these papers, we instead derive analytically and examine the fully optimal behavior of loan loss provisions, as well as quantify the welfare gains from state-contingent optimal provisioning policies following inflationary (deflationary) financial (demand) shocks in a tractable small-scale New Keynesian model. Furthermore, we show that varying provisions alone can completely circumvent adverse financial shocks and importantly liquidity traps generated by such disturbances. This desirable feature of optimal provisions holds regardless of any potential interactions between provisioning rules and monetary policy and/or reserve requirements (as is the case in Agénor and Zilberman (2015) and Agénor and Pereira da Silva (2017), respectively).====In terms of methodology, our paper is related to those of Demirel (2009) and De Fiore and Tristani (2013), who also derive a micro-founded risk premium in a New Keynesian model augmented for the credit cost channel. However, these papers focus solely on optimal monetary policy away from liquidity traps, whereas the aim of our paper is to calculate optimal provisioning policies in response to credit and demand shocks, accounting for the ZLB. Correia et al. (2021) show within a classic monetary economy framework that credit subsidies to firms can overcome the zero bound constraint on nominal interest rates following shocks that raise credit spreads. Our paper supports this result in the context of a New Keynesian model where the financial policy is conducted via loan loss provisions that directly affect the financial intermediation pricing decisions. Put differently, provisions in our model affect bank profits by acting similarly to a macroprudential tax/subsidy, which feeds into the bank’s optimal loan rate determination.====The rest of the paper is structured as follows: Section 2 describes the model and the market clearing conditions. Section 3 derives the steady state and the short-run equilibrium properties. Section 4 details the parameterization of the model. Section 5 examines the dynamics and welfare implications of state-dependent optimal provisioning policies. Section 6 concludes.",Optimal Loan Loss Provisions and Welfare,https://www.sciencedirect.com/science/article/pii/S0164070421000434,18 June 2021,2021,Research Article,117.0
Jermann Urban J.,"Wharton School of the University of Pennsylvania, 3620 Locust Walk, Philadelphia, PA 19104, United States of America,NBER, USA","Received 24 May 2021, Revised 7 June 2021, Accepted 9 June 2021, Available online 15 June 2021, Version of Record 29 June 2021.",https://doi.org/10.1016/j.jmacro.2021.103340,Cited by (3),The drivers of the prices of ,"Bitcoin and Ethereum are used as investment assets and mediums of exchange. Ethereum is also widely used to pay fees for applications on its network. For both, there is a lot of uncertainty about how widely they will be used in the future. Both have been subject to dramatic price fluctuations. The goal of this paper is to better understand these price fluctuations and to account for the fundamental drivers.====Cagan’s (1956) model has been a popular tool to study episodes of hyperinflation. In Cagan’s model individuals use money for transactions but are concerned about its rapid loss of purchasing power. As they expect inflation to increase, they reduce cash holdings. The velocity of money increases, and this further contributes to inflation. Bitcoin and Ethererum have experienced extended periods of dramatic increases in their values (as well as some periods with declines). To the extent that individuals have anticipated these hyperdeflations, they have held on to their coins for capital gains. This has lowered the velocity and further driven up prices. Given the parallels, Cagan’s model has the potential to offer some perspectives on these cryptocurrencies. The parsimony of Cagan’s model also make this an attractive framework to account for the drivers of their prices.====In this paper, I start from the Cagan model with rational expectations (Sargent and Wallace (1973) and Sargent (1977)) and augment it to focus on the uncertainty in the transaction volumes and velocities. Empirical analyses of hyperinflations focus on the increase and fluctuations in the supply of money. Money supplies of cryptocurrencies grow at steady and very predictable rates. However, the adoption of cryptocurrencies has increased over time and at varying rates, and there is a lot of uncertainty about how widely they will be used in the future. In the model, current and anticipated transaction volumes are driving the demand and the price. In the model, prices are also driven by shocks to the velocity. Cryptocurrencies are undergoing rapid technological change and this could be one source for velocity shocks. As Bitcoin adoption has grown, difficulties in handling increased transaction volumes have lead to changes in its payment technology (for instance, initiatives such as Segwit or attempts to increase the block size). Changes in the payments technology affect the transaction velocity and Bitcoin’s price. They may also influence adoption. Cagan’s model is augmented to allow for these effects and estimated with state-of-the-art Bayesian likelihood methods using data on cryptocurrencies prices, transaction volumes, and money supplies.====Based on the estimated models, innovations to transaction volumes are the predominant driver of the prices of Bitcoin and Ethereum. More than 90% of the variances of their price changes is attributed to innovations in the transaction volumes, with the remaining due to velocity shocks. Despite that, velocity shocks are estimated to be volatile. However, they trigger partially offsetting changes in the endogenous component of the velocity, as they forecast future changes in the prices of the cryptocurrencies. As another key finding, the estimated demand sensitivity to expected price changes is smaller for Ethereum than for Bitcoin, which seems consistent with Ethereum’s wide usage for paying fees to execute applications on its network. Compared to fiat currencies in past episodes of hyperinflation, the demands of the cryptocurrencies are more sensitive.====The quantity equation of money is popular in the crytpocurrency space, see for instance, Buterin (2017). My analysis characterizes key quantitative properties implied by this equation when some basic economic structure is added. There is a growing body of research on modelling prices of cryptocurrencies. Earlier papers include, for instance, Athey et al. (2016), Bolt and van Oordt (2018), Prat and Walter (2018). Contemporary studies include Cong et al. (2018), Sockin and Xiong (2020), Biais et al. (2018), and Schilling and Uhlig (2019). Different from these papers, the modelling approach used here is focused on quantitatively accounting for the macro-fundamentals driving prices, and except for the last paper listed, my model is the only one that is formally estimated. Ciaian et al. (2016) study empirical properties of bitcoin prices using the quantity equation of money but without Cagan’s model of velocity. On other features of cryptocurrencies, see, for instance, Satoshi (2008), Chiu and Koeppl (2017), Fernandes-Vilaverde and Sanches (2016), Gandal et al. (2018), Huberman et al. (2017), and Saleh (2017). The model used here shares the linear rational expectations structure popular for studying exchange rates, see, for instance, Engel and West (2005), and Chinn (2012).",Cryptocurrencies and Cagan’s model of hyperinflation,https://www.sciencedirect.com/science/article/pii/S0164070421000446,15 June 2021,2021,Research Article,118.0
"Gallen Trevor S.,Winston Clifford","Purdue University, Krannert School of Business, United States of America,Brookings Institution, United States of America","Received 15 October 2020, Revised 13 May 2021, Accepted 17 May 2021, Available online 5 June 2021, Version of Record 12 June 2021.",https://doi.org/10.1016/j.jmacro.2021.103334,Cited by (3),"We analyze the effect of the US transportation system on economic activity by building a quantitative dynamic general equilibrium model with a taxpayer-funded transportation capital stock. We highlight stark differences between the positive welfare effects of additional infrastructure spending in the long run, and its potentially negative effects when we account for the large transition (time and delay) costs to build. We also quantify large differences between the effects of additional infrastructure spending and efficient transportation policies, such as congestion pricing and eliminating laws that artificially inflate input prices, concluding that taxpayer-funded transportation improvements that increase GDP significantly may produce smaller welfare gains than efficient policies that increase GDP modestly.","The efficiency of a nation’s transportation system can significantly affect the essential inputs and outputs of an economy, including individuals’ accessibility to jobs and firms’ accessibility to workers, the availability, price, quality, and variety of consumer goods and services, the intensity of competition among and the productivity of firms, and economic growth attributable to agglomeration economies. It is therefore not surprising that many countries have tried to improve their standard of living by spending enormous sums of money on their transportation systems. The United States, for example, spends more than $5 trillion annually in both money and time on freight and passenger transport services, and has invested more than $4 trillion in highway, rail, aviation, pipeline, and water infrastructure (Winston, 2013).====In light of those enormous expenditures, it is surprising we have little knowledge about the transportation system’s effect on other sectors, its overall effect on the economy, and the benefits from improving the system’s efficiency. Transportation economists have closely studied the individual components of a transportation system, such as passenger airline service and the federal highway network, but they have rarely studied the interrelationships between transportation and other sectors of the economy. Urban and regional economists have estimated, for example, the effect of airports on metropolitan growth, but they have taken the efficiency of the transportation system as given.==== Trade theorists (Dixit and Stiglitz, 1977, Krugman, 1979, Krugman, 1980) developed general equilibrium models, but those models incorporated transportation improvements only as a source of lower trade barriers.==== Macroeconomists have estimated the returns from investments in public infrastructure capital,==== and have used DSGE models to examine productive government investment in a manner broadly similar to this paper (Leeper et al., 2010). We differ from Leeper, Walker, and Yang by focusing on permanent changes and economic welfare and by incorporating shopping and commuting time wedges, the costly time to build infrastructure, and efficient government pricing and investment policy, which we find are important for assessing the effects of infrastructure spending on welfare and GDP.====In this paper, we develop an applied general equilibrium model that includes the capital stock of the U.S. transportation system and we quantify how improving the stock by increasing government investment in it or by reforming policy to increase the stock’s efficiency enhances the nation’s welfare. Our model incorporates four direct effects of increased transportation spending, which: (1) directly increases firm productivity, (2) decreases time spent traveling to shop for consumption goods, (3) decreases time spent traveling to work, and (4) increases distortionary taxes to pay for spending. Our application of applied general equilibrium modeling, in the tradition of Shoven and Whalley (1984) and Kehoe and Kehoe (1994), first focuses on long-run outcomes. We then consider the dynamics of transition, which accounts for factors that increase the time to build infrastructure and for travelers’ delay costs during construction. We find that the transition costs that are associated with building additional infrastructure are important, and that a failure to include them by simply comparing steady states may cause the benefits from increasing infrastructure spending to be seriously overstated.====Investments in transportation infrastructure differ from other forms of government expenditures, such as military spending, foreign aid, and transfers, because they do not directly enter a household’s utility function or budget constraint; instead, they indirectly affect a household’s utility or budget constraint by affecting the performance, especially the service quality, of the transportation system. Thus, the welfare effects of infrastructure spending are different from its effect on GDP and the relative effects are ambiguous a priori. For example, spending may improve welfare by reducing commute travel time but may not increase GDP. In addition, spending is financed by taxation that reduces welfare, but may not reduce GDP. We find that government expenditures that improve transportation capital increase GDP far more than they increase economic welfare. In our case, the subsequent reductions in commuting and shopping travel times cause households to substitute the additional leisure time for additional time working and traveling to increase consumption, which increases GDP, but produces a smaller gain in welfare.====Our model yields important insights. First, we conclude that endogenous responses make up more than half of the overall change in GDP, exceeding the direct effects of transportation infrastructure on GDP though productivity gains. Consequently, we obtain a larger change in GDP than we would obtain by using a partial equilibrium model, although the change in welfare is smaller than the change in GDP may suggest. Second, our model, which was initially calibrated to the relatively large U.S. GDP multipliers, calls for much smaller GDP multipliers when it is calibrated for countries, such as Japan, with a transportation capital stock that constitutes a high share of GDP.==== We argue that this finding is consistent with the evidence that additional units of transportation infrastructure have higher costs (or lower productivity). Third, we find that delays in building transportation infrastructure may enhance welfare by giving private capital more time to respond (for example, by building new gas stations and restaurants along a new highway), thereby improving the effectiveness of the new capital stock.====We find that efficient transportation policy reforms that improve the system’s efficiency produce larger welfare gains than increasing government expenditures because they do not require additional distortionary taxes to finance the increase in effective infrastructure. For example, efficient pricing of trucks to reduce pavement and vehicle damage and efficient investment in highway durability that optimally trades off up-front capital costs for reductions in long-run maintenance costs could generate billions of dollars of annual welfare gains (Small et al., 1989, Winston, 2013). We then examine the economy’s transition path and find that the gulf between the welfare effects of efficient policies and additional spending expands because increased spending entails large up-front costs that may lead to a welfare loss.====Our analysis questions the social desirability of spending large sums of money on public infrastructure to increase GDP, as many economists suggested in the wake of the 2008–2009 recession and its sluggish recovery and continue to advocate in the Biden administration as we struggle to recover from the global pandemic.==== Indeed, an efficient policy improvement that increases GDP only a small amount, or even decreases it, may raise national welfare more than would an increase in public investment that increases GDP by a large amount.",Transportation capital and its effects on the U.S. economy: A general equilibrium approach,https://www.sciencedirect.com/science/article/pii/S0164070421000392,5 June 2021,2021,Research Article,119.0
Chen Huiying,"Department of Economics, 222 Thatcher Hall, College of Business, University of Central Oklahoma, 100 N University Dr, Edmond, OK 73034, United States of America","Received 20 May 2020, Revised 17 May 2021, Accepted 19 May 2021, Available online 2 June 2021, Version of Record 15 June 2021.",https://doi.org/10.1016/j.jmacro.2021.103336,Cited by (0),"This paper examines the welfare implications of a nominal GDP growth targeting rule, a nominal GDP level targeting rule, and ","Prior to the normalization of the federal funds rate, the past economic crisis and the nominal interest rate at the zero lower bound revived economists’ interest in the targeting of nominal GDP (or nominal income) as an attractive policy option. Since 2016, the Federal Reserve has been gradually raising the federal funds rate from the zero lower bound, with the expectation of restoring it to the normal range when the unprecedented COVID-19 pandemic broke the pace. Lawmakers passed the relief package of 2.2 trillion dollars, financed by a new round of quantitative easing — open market purchase of treasury bonds. The federal funds rate was brought back to the zero lower bound. In an environment with constrained policy interest rates and a weak global economy, a natural question on what monetary policy a central bank should adopt rises. The current study is motivated by the constraint of the traditional monetary policy rules in the foreseeable future due to the narrow operation room at the zero lower bound, the normalization of the federal rates, and the traditional Taylor rule’s congenital defects in requiring the measurement of the potential output, real economic activity and core inflation.====This paper examines a nominal GDP growth targeting (NGDP-GT) rule, a nominal GDP level targeting (NGDP-LT) rule, and inflation targeting regime in a New Keynesian model with the features of trend growth of productivity, partial indexation to inflation and a positive rate of trend inflation. The simulation results provide evidence to U.S. policy makers that nominal GDP growth targeting is desirable relative to alternative regimes with changes of all dimension standards. The paper contributes to the literature on nominal GDP targeting in the following aspects. First, this is the first paper that adopts two welfare measures to comprehensively examine welfare properties of nominal GDP targeting. Although the paper presents different policy rankings in terms of welfare measures, in general, nominal GDP growth targeting produces less welfare loss. Garín et al. (2016) adopt consumption equivalence as the welfare measure and focuses on the desirability of nominal GDP level targeting rule in a New Keynesian model.====Second, this is the first paper to evaluate both the nominal GDP growth and level targeting rules in the same framework with the features of positive trend inflation and trend growth of productivity, and it finds that nominal GDP growth targeting dominates level targeting for most scenarios. The paper shows that although the consumption-equivalence-welfare loss associated with the level targeting is smaller conditional on low productivity growth and partial inflation indexation with non-separable utility function, nominal GDP growth targeting outperforms level targeting for all other scenarios. The above conclusion is contingent upon applying consumption equivalence as the welfare measure. When using the weighted sum of variances of inflation and output gap as the welfare standard, nominal GDP growth targeting is always preferable to the level targeting rule.====There is research focusing on either nominal GDP growth targeting rule or level targeting rule in the literature. Beckworth and Hendrickson (2015) amend a standard New Keynesian model and their simulations show that a nominal GDP growth targeting rule produces lower volatility in both inflation and output gap in comparison with the Taylor rule under imperfect information. Chen (2020) examines a nominal GDP growth targeting rule in a New Keynesian model and finds that nominal GDP growth targeting either outperforms Taylor type of rules and inflation targeting or is weakly dominated by a desirable policy. Billi (2017) compares nominal GDP level targeting with strict price level targeting in a small New Keynesian model, with the central bank operating under optimal discretion and facing the zero lower bound on nominal interest rates, and shows that, if the economy is only buffeted by temporary inflation shocks, nominal GDP level targeting may be preferable.====Third, the model of the paper incorporates both demand and supply shocks to thoroughly examine policy performance. When trend growth is greater than one or equal to one, the most likely scenario for the U.S., nominal GDP growth targeting regime is the most desirable framework, generating the least consumption variation while level targeting produces the most consumption variation; when both the trend growth and inflation indexation are less than one with the non-separable utility function, nominal GDP level targeting yields the least consumption-equivalence-welfare loss, regardless of the type of shock, while nominal GDP growth targeting, takes the second place. When using the weighted sum of variances of inflation and output gap as the welfare standard, the paper draws consistent conclusions for both the productivity shock and government spending shock and demonstrates that nominal GDP growth targeting rule dominates all other policy regimes, creating the least fluctuations, and nominal GDP level targeting policy is proved to be the second preferable regime. By adopting nominal GDP growth rate targeting, not only the growth rate of output is stabilized but so is the inflation rate.====Fourth, inflation targeting generally performs better with consumption-equivalence criteria. On average, inflation targeting is superior to the nominal GDP level targeting framework, but to minimize short-run fluctuations, nominal GDP level targeting shows its advantage.====The essential intuition for why nominal GDP growth targeting is desirable is that it puts equal weight on inflation and output growth path, effectively weighting real economic activity more than alternative rules. Nominal GDP growth targeting allows output growth and inflation to accommodate a shock and assists in finding the point where the relative levels of output growth and inflation lead to lower economic fluctuations. Thus, this regime generates higher output and consumption levels and lower volatilities in output and inflation, resulting in relatively low utility loss.====The baseline setup of this paper is characterized by three sectors of the economy: households, monopolistically-competitive firms that face adjustment costs, and a monetary authority. The private-sector equilibrium is constituted by optimal paths of consumption, labor, interest rate, real marginal cost, output, and inflation. For the nominal GDP growth targeting rule, I keep the growth rate of nominal output between two consecutive periods constant. In the benchmark model, the growth rate of nominal GDP is set to the U.S. historical level.====The rest of this paper is organized in the following structure: Section 2 outlines the model and defines the private-sector equilibrium, Section 3 provides a description of the policy rules, Section 4 defines equilibrium, Section 5 presents the quantitative results with consumption equivalence as the welfare measure, Section 6 shows the policy rankings with weighted sum of variances of inflation and output gap as the welfare measure and compares findings under the two measures. Section 7 presents some robustness with a separable utility function. Section 8 concludes.",On the welfare implications of nominal GDP targeting,https://www.sciencedirect.com/science/article/pii/S0164070421000410,2 June 2021,2021,Research Article,120.0
"Mazzoli Marco,Lombardini Simone","Department of Economics - DIEC - Genoa University, Via Vivaldi 5, 16126 Genova, Italy","Received 30 March 2020, Revised 16 May 2021, Accepted 17 May 2021, Available online 29 May 2021, Version of Record 5 June 2021.",https://doi.org/10.1016/j.jmacro.2021.103335,Cited by (0),In this paper we introduce a microfounded ,"This paper introduces a new macromodel where entry, exit and strategic interactions among large oligopolistic firms are explicitly formalized and may generate macroeconomic fluctuations. In dynamic stochastic general equilibrium models (henceforth DSGE) with monopolistic competition, the firms as a whole are commonly modeled as a continuum on a unit segment. In our model any significant change in the number of operating firms is a measurable phenomenon, potentially carrying macroeconomic implications. Our work ideally links the literature on the aggregate effects of large firms’ decisions to the recent studies on the macroeconomic impact of business formation: the former introduced by Gabaix (2011), with his “granular hypothesis”, the latter initially studied by Jaimovich (2007). As in Mazzoli et al. (2019), the firms produce a homogeneous good in an oligopolistic market, but contrary to what was stated in that case, here we assume that the law of one price applies. For the microfoundation of the aggregate demand, we refer to Mazzoli et al. (2019), where the agents have the same utility function but could differ in their budget constraints and may possibly change their social status because of entry and exit or as a consequence of the firms’ decisions to hire new workers or fire some of their existing workers. Although conventional technology shocks could be easily formalized in our model, in this study the focus is on entry and exit, and their ability to generate aggregate fluctuations.====Assuming quantity (Cournot) competition in a macro-model might raise the problem of how prices are determined without referring to an auctioneer or, similarly, what prevents the firms from implementing price undercutting. A possible solution could lie in assuming an oligopolistic industrial sector with quantity precommitment à la Kreps and Scheinkman (1983), with the modified and extended assumptions as provided by Madden (1998). In particular, as stated in Madden (1998), we assume that at the beginning of each period the oligopolistic firms enter a two-stage game with quantity precommitment where:====– at stage one they decide to enter, bear the entry cost, and hire workers (which constitutes a production capacity precommitment);====– at stage two they set prices and profits.====We share, to some extent, Ghironi’s (2018, p. 208) view that “The benchmark macro model with monopolistic competition assumes a continuum of measure-zero producers that interact with each other in non-strategic fashion” and “once we begin entertaining the idea that firms in our macro models should no longer be measure-zero entities, the assumption of non-strategic monopolistic competition becomes less and less tenable”. Therefore, “it is time for macroeconomics to move beyond the representation of firm behavior in terms of production by a constant number of symmetric firms that produce either the same good under perfect competition or a fixed range of goods under monopolistic competition between a continuum of firms” (Ghironi, 2018 p. 196).====Section 2 briefly discusses the literature on the aggregate effects of business formation, while in Section 3 we look at some recent US data explaining the premises for our model. Section 4 discusses the determination of aggregate demand, Section 5 the firms’ sector and labor market equilibrium, and Section 6 analyzes the macroeconomic implications of the Cournot–Nash equilibrium in mixed strategies among oligopolistic firms. In Section 7, we show that our model may qualitatively reproduce the business cycle in a large economy and the expected effects of changes in monetary policy regime by introducing some numerical simulations. Finally, Section 8 contains a few concluding remarks.",Business cycle in an oligopolistic economy with entry and exit,https://www.sciencedirect.com/science/article/pii/S0164070421000409,29 May 2021,2021,Research Article,121.0
"Lin Hsuan-Chih,Tanaka Atsuko,Wu Po-Shyan","Institute of Economics, Academia Sinica, 128 Academia Road, Section 2, Nankang, Taipei 115, Taiwan, ROC,Department of Economics, University of Calgary, 2500 University Dr NW, Calgary, AB, Canada,Department of Economics, Indiana University, 100 South Woodlawn Avenue, Bloomington, IN 47405, USA","Received 24 October 2019, Revised 13 May 2021, Accepted 13 May 2021, Available online 27 May 2021, Version of Record 1 June 2021.",https://doi.org/10.1016/j.jmacro.2021.103329,Cited by (3),"With aging demographics and generous pension programs, the sustainability of the pay-as-you-go (PAYG) public pension system has been often questioned and has motivated policymakers to enact reforms in many countries. Although mandatory funded Individual Retirement Accounts (IRAs) appear to be a solution to this unsustainable system, existing reforms usually take place within the PAYG system by reducing pension benefits. This paper evaluates the effects of PAYG reforms as well as reforms that switch to the IRA system. Our analysis shows that PAYG reforms outperform IRA reforms in many aspects. In fact, PAYG reforms achieve higher GDP and yield higher welfare in the long run. The transition to the steady state is also found to be less volatile for PAYG reforms. While PAYG generally places a larger burden on future generations, the positive welfare effect of cross-subsidization dominates the welfare loss. Our findings may explain why pension reform is a controversial issue in most countries and why we rarely observe a shift to the IRA system.","The financial issue of pensions has received great attention in many countries over the past decade. The pay-as-you-go (PAYG) pension system is the most common pension system, and its enrollment is often mandatory for workers. Among the commonly mentioned disadvantages of PAYG are (i) financial unsustainability, (ii) burdens on future generations, (iii) distortion in the labor market, and (iv) implicit pension debt and policy uncertainty (source: OECD Pensions at a Glance). Many of these concerns are not a problem if an individual retirement account (IRA) system is implemented. Although IRAs appear to solve the unsustainability problem, replacing PAYG with IRAs has not been commonly considered as an alternative. In fact, reforms actually implemented during the past five years in OECD countries include changes in retirement age, coverage, pension benefits, and contributions, but rarely indicate switching to an IRA system.====Why do we focus on the differences between the PAYG and IRA systems? The two systems greatly differ in their expected effects on inter-generational/cross-sectional equity; whereas the PAYG system collects premiums through payroll taxation from the working population and immediately re-distributes them to retirees as pension benefits (insurance and redistribution channels), the IRA system incentivizes workers to grow their own retirement accounts (usually tax-free) so they will have enough savings as self-insurance against health/productivity shocks in their later years (tax distortion channel). Although both reforms have often been discussed in recent policy debates, few studies assess the welfare implications of reforms and the consequences of real implementation. Focusing on the aging population and transition dynamics, we provide a comprehensive evaluation of how the economy benefits from a reform both in the short run and over the long run.====This paper analyzes the transition dynamics and welfare implications of different policy reforms (PAYG or IRA). Specifically, we extend the model developed byImrohoroglu and Kitao (2012) andKitao (2015) and build a general equilibrium model of overlapping generations of individuals who make decisions on consumption, savings, pension benefit claims, and labor supply on both extensive and intensive margins. We then quantify the responses of macroeconomic outcomes and individuals’ behavior when the government treats the new cohort and existing cohorts differently. For example, the government could implement the IRA system for new cohorts and maintain the current PAYG system for current cohorts. By analyzing different policy reforms, we investigate the implications through the insurance, tax distortion, and inter-generational redistribution channels.====To conduct the welfare analysis, Taiwan is selected as the target economy for the following reasons. First, Taiwan has experienced a drastic change in its demographics mainly due to a low fertility rate. In fact, it has been reported that Taiwan has almost the lowest fertility rate of all the developed countries. Fig.1 shows the rate of natural increase and the old-age dependency ratio over time: we observe that the absolute value of the slope for Taiwan is the steepest, implying a drastic change in demographics. Second, related to the first point, since current pension spending poses a serious threat to the Taiwanese government, in-depth pension reform is direly needed and inevitable. Based on the current estimates of pension expenditure growth,==== pension expenditure could account for almost 50% of government spending by the year 2050. A recent article in The Economist (====) estimates that the predicted bankruptcy date for Taiwan is 2027, which seems to make the budget problem for Taiwan the most urgent compared to other countries (2035 for China, 2035 for the USA, 2052 for Japan, and 2029 for Spain).==== Third, as shown in Fig.2, the pension replacement ratio in Taiwan (58%) is comparable to the other OECD countries. Thus, by investigating the welfare analysis of Taiwan pension reform, important insights can be provided to countries facing pension problems that are similar to Taiwan’s.====The findings of the paper consist of three parts. First, we analyze the final steady state outcomes and find that within-PAYG reforms and IRA reforms both achieve a higher GDP compared to the baseline case of no reform. The simulation results show that without any reform, the output decreases by 30.26% due to the aging population. With reforms, the negative effects of the aging population on the output are mitigated: for within-PAYG reform, GDP decreases to a lesser extent by 19.84%, and for IRA reform it does so by 29.38%. When we look at the financial burden, measured by the income tax rates required to balance the government budget, the effectiveness of reforms is more remarkable. Whereas the case without reform requires the highest average income tax rates of 11.77%, within-PAYG reform requires 4.63% and IRA reform requires 1.75%. Thus, this analysis indicates that reforms perform better than the case without reform in that they mitigate the output reductions and ease the tax burdens of the aging economy to a significant extent.====We next analyze the transition dynamics and the impacts on individual welfare when a given reform is implemented differently. Specifically, we consider alternative transition paths to the final steady states in the following four policy reforms: (i) IRA reform applied only to new cohorts, (ii) within-PAYG reform applied only to new cohorts, (iii) IRA reform applied to new cohorts and PAYG payments reduced to the minimum for old cohorts, and (iv) PAYG pension benefits reduced and applied to all cohorts immediately. We evaluate the transition paths of each reform in terms of both aggregate outcomes and individual welfare. Compared to the baseline transition, the output levels are found to be lower during the early transition period for reform (i), but they start to increase 60years after implementation. The decline in GDP can be partially explained by the higher financial burdens on the younger cohorts. Under reform (i), which leaves old cohorts with the old generous PAYG system, the average income tax rate must increase from 14% to 30%. In contrast, reform (iii) has no problem of increased taxes. However, reform (iii) substantially lowers the welfare of individuals in current generations, indicating the potential difficulties in obtaining the consent to implement the reform. Furthermore, with respect to individual welfare, the within-PAYG reforms, (ii) and (iv), yield the highest long-term welfare whereas the IRA reforms, (i) and (iii), yield lower long-term welfare than the baseline scenario. This finding seems to indicate the importance of cross-subsidization from the rich to the poor offered by the PAYG system, despite its inter-generational burdens.====Lastly, we investigate the mechanism behind these welfare changes by decomposing the effects into three channels: the insurance channel, the inter-generational redistribution channel, and the tax distortion channel. The analysis shows that the insurance channel offered by the PAYG system plays an important role in the Taiwanese economy. We also find that the inter-generational redistribution channel is especially large in reform (i), which causes the welfare of future generations to be higher by 18% than the baseline transition at the beginning of the transition periods. For reform (ii), the welfare of future generations is only reduced by less than 5% than the baseline transition for the first ten years, and is higher for most of the rest of the transition period. Finally, reform (iv) yields positive welfare for all future generations, but the current generations sacrifice considerably to a great extent. Our findings imply that policymakers must take into consideration the transition process as well as the inter-generational trade-off in deciding how to improve the fiscal balance of the pension system.====This paper contributes to the literature on pension reform using a large-scale overlapping generation model. The literature in macroeconomics that uses large-scale, general equilibrium models can be traced back toAuerbach and Kotlikoff (1987) and also includesConesa and Krueger, 1999, Conesa and Garriga, 2008,Imrohoroglu and Kitao (2012),Kitao (2015),Kudrna and Woodland (2011),Demange and Laroque (1999),Razin et al. (2002),Abel (2003),Boldrin and Montes (2005),DeNardi (2004) andSong et al. (2015), where the particular focus is on the general equilibrium effects of pension reforms in different countries.====Another strand of literature focuses on the OLG model with different angles. For example,Huang et al. (1997),Storesletten (2000),Rogerson and Wallenius (2009),Prescott et al. (2009), andKitao et al. (2017) uses multi-period OLG models quantitatively address retirement decisions, welfare implications, and decisions on labor force participation.Iacoviello and Pavanb (2013) focus on the relationship between pension and the housing debt, whileMakarski et al. (2017) study the effect on how the pension is funded and calculated.Heijdra and E. (2009) implement mortality process into the model, andJacobs and Schindlerbc (2013) study the effect of tax reform and pension. Although we calibrate the model to the current Taiwanese system, the implications can be generalized to any country with an aging population and a generous pension system.====In particular, our paper is closely related toImrohoroglu and Kitao (2012),Kitao (2015), andSong et al. (2015).Imrohoroglu and Kitao (2012) build a general equilibrium model and focus on the interaction between work incentive and the decisions to claim Social Security in order to evaluate the effectiveness of Social Security reforms in the U.S in terms of fiscal sustainability. They simulate a reform that increases the retirement age by two years and find that such a reform significantly increases capital stock and labor supply while improving the government budget.Kitao (2015) analyzes the effects of introducing IRAs in Japan, which has an employer-based PAYG public pension system. We simulate the IRA reform in a similar fashion toKitao (2015), but we allow individuals’ retirement decisions to be endogenous and we focus particularly on the welfare implications through different channels.Song et al. (2015) focus on different sustainable reforms in China and on demographic transitions. Our analysis diverts from existing papers in that we endogenize the timing of pension benefit claims, and estimate the cost of various kinds of pension reforms, including the reforms within PAYG system. By comparing within-PAYG reform to IRA reform, we are able to illustrate different paths of transitions and analyze the costs and benefits of each reform.====The rest of the paper proceeds as follows. Section2 describes the model. Section3 introduces the data and how we calibrate the model. Section4 presents a counterfactual policy analysis of sustainable pension reforms and Section5 concludes.",Shifting from pay-as-you-go to individual retirement accounts: A path to a sustainable pension system,https://www.sciencedirect.com/science/article/pii/S0164070421000355,27 May 2021,2021,Research Article,122.0
"Nalban Valeriu,Smădu Andra","Independent Researcher,De Nederlandsche Bank, University of Groningen, The Netherlands","Received 7 December 2020, Revised 5 May 2021, Accepted 16 May 2021, Available online 24 May 2021, Version of Record 1 June 2021.",https://doi.org/10.1016/j.jmacro.2021.103331,Cited by (6),"We examine whether the response of the euro area economy to uncertainty shocks depends on financial conditions. We find strong evidence that uncertainty shocks have much more powerful effects on key ==== in episodes marked by financial distress than in normal times. We also document that the recovery of economic activity following an adverse uncertainty shock is state dependent: it is gradual in normal times, but displays a more accelerated rebound when the shock hits during financial distress, reflecting monetary accommodation provided by the central bank. These findings are based on a non-linear data-driven model that accounts for regime switching and time-varying volatility. Our findings imply that whether financial markets are calm or distressed matters when it comes to the appropriate policy responses to uncertainty shocks.","The interaction between financial market disruptions and heightened economic uncertainty is frequently identified among the culprits and amplifying factors of the Great Recession. At the current juncture, marked by elevated uncertainty with respect to the macroeconomic implications of the ongoing COVID-19 pandemic, the link between the two has re-emerged at the forefront of policy and research debates. Therefore, we seek to contribute to this discussion and aim at answering the following question: ====We find strong empirical evidence that uncertainty shocks have much more powerful effects on key macroeconomic variables in episodes marked by financial distress than in normal times. We use a pre-pandemic sample and employ a non-linear data-driven model that accounts for regime switching and time-varying volatility. The latter feature allows us to estimate aggregate economic uncertainty as a common component of all the modeled structural shocks’ volatilities. Our results show that the effects of a change in uncertainty depend on the size of the shock. In tranquil episodes responses are essentially linear in the size of the shock, while in financial turmoil uncertainty perturbations give rise to amplification effects. We also document sign asymmetries in the case of large uncertainty shocks that hit during financial distress: an adverse perturbation leads to a larger decline in industrial production compared to mild positive effects following a decrease in uncertainty (of the same magnitude). Furthermore, we find that the recovery of economic activity is state-dependent after an adverse uncertainty shock. More precisely, it is gradual in normal times, but displays a more accelerated rebound when the shock hits during financial turmoil. Finally, we argue that whether financial markets are calm or distressed matters when it comes to the appropriate monetary policy responses to uncertainty shocks.====Our motivation is based on the recent re-enforcement of financial distress and heightened uncertainty in the context of the COVID-19 pandemic. Measures of uncertainty (such as equity market volatility) have increased sharply in countries around the world (including the United States, the euro area, Japan), amid concerns about the economic impact of the pandemic. These fast-moving measures reacted swiftly to developments in the health sector, anticipating declines in conventional economic indicators. At the onset of the health crisis, stock markets in major economies plummeted and witnessed a surge in implied volatility, reflecting a major repricing of risk. With the spike in volatility, market liquidity has deteriorated significantly, and credit spreads have widened broadly across markets, as investors have started to reallocate from relatively risky to safer assets (“flight to safety”). As a result, emerging markets have experienced unprecedented capital outflows. Therefore, a sharp rise in uncertainty coupled with financial turmoil can put both economic growth and financial stability at risk. Our ultimate aim is to better understand this link and to assess how instrumental financial markets are to the transmission of uncertainty shocks.====Financial conditions have tightened markedly as the economic disruption caused by the pandemic outbreak intensified. This translated in higher funding costs for firms when they needed to tap equity and bond markets to finance their working capital. A sharp tightening of financial conditions and heightened uncertainty can have powerful effects on economic activity, as companies delay investment projects and individuals postpone consumption due to precautionary motives or binding financial constraints. Nevertheless, major central banks acted quickly and in a coordinated manner by injecting liquidity and/or cutting policy interest rates. These measures improved overall market functioning, eased financial conditions and supported the flow of credit to businesses and households.====There is widespread agreement in both academic and policy circles that uncertainty shocks have powerful recessionary effects, with financial frictions at the core of the propagation of the shocks to the real economy. The conjecture here is that when financial contracts are subject to agency or moral hazard problems, a rise in uncertainty increases the external finance premium, leading to a surge in the cost of capital and a fall in firms’ investment, in line with the financial accelerator mechanism proposed by Bernanke et al. (1999).====Against this background, we analyze the nexus between credit market disruptions and uncertainty for the euro area economy. To the best of our knowledge, there is no empirical evidence to document the macroeconomic implications of uncertainty shocks for the eurozone studied in the context of non-linearities. To explore the state-dependent link between financial conditions and uncertainty we employ a non-linear vector autoregressive (VAR) model. While our main findings are based on a pre-pandemic sample, we discuss the implications of extending the sample to include COVID-related data in Section 4. In this setting, uncertainty impacts macroeconomic developments directly and it is captured by the overall volatility of the economy’s structural shocks. We document that uncertainty has different macroeconomic implications depending on whether financial markets are calm or in distress when the shock hits the economy. Furthermore, it is essential to better understand how long it takes for the uncertainty shocks to fully dissipate and for the economy to recover (return to “normal”), depending on the prevailing financial stress level and the size of the shock.==== Next, we briefly review some of the contributions to the ongoing assessment of the macroeconomic implications of uncertainty shocks in an environment characterized by financial frictions. The theoretical underpinning of why financial distortions should be incorporated in macroeconomic models stems from the feedback loop between credit constraints and economic activity. More precisely, financial constraints may lead to amplification and persistence of (standard) shocks, which in turn shape and magnify macroeconomic cycles. In conjunction with this, financial factors can influence the transmission mechanism of macroeconomic policies and shocks.====The global financial crisis (GFC) has ignited extensive discussions on the impact of macroeconomic shocks and how they shape the financial and business cycles. Considerations on the interactions between uncertainty and financial frictions have emerged at the forefront of the policy discussion. Broad-based and heightened uncertainty is seen by both researchers and policymakers as one of the main factors behind the depth and duration of the GFC. Frequently, the explanation is linked to two transmission channels. First is the risk premium, which is often invoked, as uncertainty typically leads to an increase in the cost of external financing. Second, risk aversion is advocated to have also been at play because uncertainty undermines consumer and business confidence, which hold back spending and investment (due to precautionary motives). At the current juncture, the unfolding coronavirus pandemic appears to illustrate that financial markets are critical to the propagation of uncertainty shocks to the real economy.====Theoretical studies document that elevated uncertainty typically generates a decline in economic activity. A frequently cited channel that links uncertainty to the real economy relates to firms’ investment and hiring irreversibility, following the lead of Bernanke (1983) and Bloom (2009). A more recent stream of research argues that financial frictions are at the core of the transmission mechanism of uncertainty shocks to the real economy. In other words, this view advocates that credit markets are the crucial link in the propagation of uncertainty shocks (for example, Christiano et al., 2014, Gilchrist et al., 2014 among others). The reasoning is the following: when financial contracts are subject to agency or moral hazard problems, a rise in uncertainty increases the external finance premium, leading to a higher cost of capital and a fall in firms’ investment. More precisely, the impact of heightened uncertainty on financing conditions and on market expectations may lead to adjustments in asset prices (e.g. equities, high-yield bonds, commodities). This, in turn, has adverse implications for consumption and investment via wealth effects. For instance, when equity prices fall, individuals might reduce consumption to make-up for the loss in their net wealth. Furthermore, abrupt asset price movements can also impact aggregate demand via the value of collateral, which typically allows borrowers to gain access to credit and receive funding. Finally, broad-based uncertainty might also prompt households to increase their precautionary savings. For a recent review of the literature on uncertainty shocks and business cycle research see Fernández-Villaverde and Guerrón-Quintana (2020). They also discuss the main mechanisms – precautionary behavior, the Oi–Hartman–Abel effect and real rigidities caused by financial frictions – linking uncertainty and its variation over time to aggregate fluctuations in economic models. Another key contribution to the literature is the work by Fajgelbaum et al. (2017), who propose a theory of endogenous uncertainty and business cycles. In their model, heightened uncertainty about fundamentals discourages investment (via information frictions) and the economy displays uncertainty traps, defined as self-reinforcing episodes of elevated uncertainty and low activity.====On the empirical front, the VAR literature has largely abstracted from taking into account the interaction between uncertainty and financial conditions. Only in the aftermath of the GFC a few studies have focused on this link. For instance, Popescu and Smets (2010) analyze the interaction between perceived uncertainty==== and financial risk premia for Germany. Their results point to a modest and temporary impact on economic activity of an exogenous uncertainty shock. However, they emphasize the identification issues arising in a VAR setting when simultaneously incorporating a credit spread measure and a proxy to capture aggregate uncertainty. Benati (2013) investigates the role of policy uncertainty shocks in the United States, the euro area, the United Kingdom, and Canada using a Bayesian time-varying parameters structural VAR framework with stochastic volatility.==== The author finds that, depending on the identification strategy, these shocks have either a marginal or a non-negligible contribution in explaining macroeconomic fluctuations. Caldara et al. (2016) show that uncertainty shocks have important macroeconomic implications when operating through the financial channel, that is, when allowing financial conditions to respond immediately to changes in uncertainty.==== Otherwise, these have a more muted effect. A first attempt to offer some empirical evidence on the global impact of COVID-19-induced uncertainty is provided by Caggiano et al. (2020). Their empirical framework relies on VIX as a proxy for global financial uncertainty.====Our work is closely related to the novel approach proposed by Alessandri and Mumtaz (2019). They incorporate a regime-switching structure in a VAR model where the structural shocks have time-varying volatilities, which influence the first-moment dynamics of the system (see also Mumtaz and Surico, 2018, Mumtaz and Zanetti, 2013 for instance). Based on this non-linear VAR model, they document that in the USA, uncertainty shocks have recessionary effects at all times, but their impact on economic activity is significantly larger when the economy is experiencing financial turmoil. Our paper is also related to previous contributions in the literature as proposed by Caggiano et al. (2014), which study the effects of uncertainty shocks contingent on the state of unemployment, and by Mumtaz and Theodoridis (2018), which examine the effects of uncertainty shocks contingent on the monetary policy regime.====In a broader context, our paper contributes to the literature analyzing the state dependent nature of business cycle fluctuations and policy interventions. With regards to fiscal policy, Auerbach and Gorodnichenko (2013) estimate that government purchases are more effective in recessions than in expansions in OECD countries, with significantly stronger impact on GDP, private consumption, investment, labor market and prices. Ghassibe and Zanetti (2021) provide both theoretical and empirical support for state-dependent fiscal multipliers as determined by the fundamental source of business cycle fluctuations: spending multipliers are substantially higher in demand-side recessions than in supply-side recessions, while the opposite is true for tax cut multipliers. In a theoretical framework with search complementarities – whereby aggregate output depends on the intensity with which firms match – and multiple equilibria, Fernández-Villaverde et al. (2019) show that the volatility of the shocks determines the duration of each equilibrium state, thus making the policy support particularly effective when implemented during the passive equilibrium (when output is low and unemployment is high) if it manages to push the economy on the path towards the active equilibrium.====In the case of labor market fluctuations and the associated policy interventions, Pizzinelli et al. (2020) also document important state dependencies. They use a threshold VAR estimated for the USA to show that productivity shocks lead to larger effects on unemployment, job findings and job separations in states of low aggregate productivity (despite a similar magnitude of the underlying shock across regimes).==== These empirical findings are then replicated using a search-and-matching model with on-the-job search and idiosyncratic productivity. Ferraro (2018) also shows that the USA unemployment rate fluctuates asymmetrically, with sharp and deep declines during recessions but slow and gradual recoveries during expansions; in contrast, aggregate output asymmetries are much milder. A general equilibrium model with endogenous separations and skill heterogeneity is then developed to account for these empirical regularities, highlighting the effectiveness of economic policies targeting the composition of labor force and worker–firm relations.====Overall, considering that financial distress and economic uncertainty are countercyclical, our findings complement existing evidence that both the effects of shocks and efficiency of policy responses are larger in recessions. Accordingly, the results highlight the importance of asymmetries in business cycle fluctuations and provide support for more policy activism during periods of depressed economic activity.====The remainder of the paper proceeds as follows. Section 2 describes the model used to conduct our empirical analysis. Section 3 presents the quantitative exercises, which deliver our results. Section 4 shows how this framework can be used as a device to track the state of financial markets in real time. Finally, Section 5 provides a set of robustness checks, and Section 6 concludes.",Asymmetric effects of uncertainty shocks: Normal times and financial disruptions are different,https://www.sciencedirect.com/science/article/pii/S0164070421000379,24 May 2021,2021,Research Article,123.0
"Eichengreen Barry,Park Donghyun,Shin Kwanho","Department of Economics, University of California, Berkeley USA,Economics Research and Regional Cooperation Department, Asian Development Bank,Department of Economics, Korea University, South Korea","Received 9 February 2021, Revised 4 May 2021, Accepted 16 May 2021, Available online 23 May 2021, Version of Record 31 May 2021.",https://doi.org/10.1016/j.jmacro.2021.103330,Cited by (11),"In this paper we seek to make headway on the question of what recovery from Covid-19 recession may look like, focusing on the duration of the recovery – that is, how long it will take to re-attain the levels of output and employment reached at the prior business cycle peak. We start by categorizing all post-1960 recessions in advanced countries and emerging markets into supply-shock, demand-shock and both-shock induced recessions. We measure recovery duration as the number of years required to re-attain pre-recession levels of output or employment. We then rely on the earlier literature on business cycle dynamics to identify candidate variables that can help to account for variations in recovery duration following different kinds of shocks. By asking which of these variables are operative in the Covid-19 recession, we can then draw inferences about the duration of the recovery under different scenarios. A number of our statistical results point in the direction of lengthy recoveries.","Attention turned next to the prospects for recovery. Here the outlook was (and, at the time of writing, remains) uncertain. On one side were those who forecast a quick V-shaped recovery. They argued that the Covid recession was not the result of the sorts of factors that had led to extended recessions in the past. It was not the result of an increase in global ====, a financial crisis, or a sharp rise in global energy costs. It was not the legacy of a credit boom that caused banks to become overextended, requiring them to deleverage and leading to an extended period of depressed investment. Rather, it reflected the decision of governments to lock down populations and put the economy into suspended animation. It was argued that, consequently, once the spread of the virus had been contained by the lockdown, it would be possible for economies to pick up where they had left off. There was no reason why activity couldn't resume immediately, in other words, and rebound quickly to the same levels as before.====, if left unaddressed, could impair bank balance sheets, adding a layer of financial stress and disfunction. Pre-existing connections between workers and employers would be lost, destroying the value of firm-specific skills and requiring time for new connections to be formed. Recovery, in this view, was not as simple as flipping a switch.====Superimposed on these scenarios were two further complications. First and foremost, there was the epidemiology of COVID-19. This Coronavirus being novel, no one knew how it would be affected by temperature and humidity, how easily it would be spread, or whether contagious new vaccine-resist variants would appear. It was uncertain whether there would be a second or third wave, transforming a U- or V-shaped recovery into a W.====Second, it is not clear whether to conceptualize the COVID-19 recession as a supply shock, a demand shock, or a combination of the two. Advocates of the V-shaped recession hypothesis tended to think of it as a temporary negative supply shock. Since the lockdowns that gave rise to the recession were temporary and could be removed abruptly, recovery was likely to be quick, in this view. But there are also those who point to much longer lived supply-side effects, due to disruptions to global supply chains, resurgent protectionism, and the loss of age-critical education and ==== due to school closures. Negative supply shocks of this sort are not easily offset, of course, by conventional demand-management policies.====Others emphasize the reduction in demand as incomes are lost in COVID-19 lockdowns. Here demand-management policies helped to mitigate the effects of the shock, but to differing extents in different economies. In advanced countries, central banks have undertaken asset purchases on a scale that dwarfs even those of 2009–10, while governments have adopted fiscal initiatives that replace – in some cases more than replace – the decline in disposable incomes. Emerging markets and developing countries possess less monetary and fiscal policy space, but they have nonetheless used monetary and fiscal policies more aggressively than in previous downturns, reflecting their progress in building up monetary-policy credibility and fiscal capacity. Nonetheless, these considerations highlight the high uncertainty surrounding the scope for policy to mitigate the downturn and sustain any subsequent recovery.====A final complication is that there is no direct historical precedent or available data set on earlier recessions taking place as a result of similar shocks to which to look. The 1918-19 Spanish flu pandemic and recession is a parallel (====, ====, ====), but it occurred in a very different institutional setting. Other recessions feature the negative aggregate-supply and aggregate-demand shocks characteristic of COVID-19 in very different combinations.====In this paper we seek to make headway on the question of what recovery from the COVID-19 recession will look like, focusing on the duration of the recovery – that is, how long it will take to re-attain the levels of output and employment reached at the prior business cycle peak. Focusing on the duration of the recovery rather than the shape (V, U or W) allows us to circumvent epidemiological uncertainties. To be sure, epidemiological issues such as whether or not there is a second wave are also likely to affect the duration of recovery, but it is possible, as we show below, to analyze and identify the determinants of recovery duration following various categories of shocks independent of epidemiology.====We start by categorizing all post-1960 recessions in advanced countries and emerging markets into supply-shock, demand-shock and both-shock induced recessions. We measure recovery duration as the number of years required to re-attain pre-recession levels of output or employment. We then rely on the literature on business cycle dynamics to identify candidate variables that can help to account for variations in recovery duration following different kinds of shocks. By asking which of these variables are operative in the COVID-19 recession, we can then draw inferences about the duration of the recovery under different scenarios.====, ====, ====Second, there is the literature directly concerned with the length of recoveries. ====, focusing on the United States, suggest that a slowing trend rate of growth has been associated with slower recovery (longer recovery duration) over the course of the 20==== century. ==== also focus on the US and document that there has been a tendency over time for recoveries to take longer. They associate this with the rise of the service sector, arguing that services, unlike goods, cannot be produced ahead of demand, so there is less scope for inventory accumulation to initiate early recovery in service-heavy economies, and that most services can't be exported, so there is less scope in service-based economies for exports to spur recovery. A possible implication is that recovery should take longer in advanced economies than emerging markets and developing countries where employment is less service dependent.====Other authors (viz. ====, ====) suggest that recoveries take longer when the preceding recession is marked by a financial crisis. Similarly, ==== point to credit booms in the preceding expansion as auguring more gradual recoveries. Kannan, Scott and Terrones find that recoveries from global recessions (when many economies are in recession simultaneously) tend to be lengthy and weak.====Third, there is the literature concerned with changes in the effectiveness of demand-management policies. ==== describes how interest rates have declined secularly across a range of advanced countries and emerging markets, and suggests that this has left less scope for conventional monetary policies to support recovery. Others point to the diminished capacity of expansionary fiscal policy to stimulate recovery when governments are already heavily indebted (see ==== and ====).==== compare recoveries in countries with pegged and flexible exchange rates. They find little difference by regime for the entire period they consider but some evidence that countries with flexible rates recovered more quickly starting with the Global Financial Crisis. Whether this is a result specific to euro area countries (which dominate their sample of post Global Financial Crisis recoveries) or a more general finding is unclear, however. ==== looks specifically on emerging markets in the Global Financial Crisis, and reports limited evidence of faster recoveries in economies with flexible rates. ==== focuses on recoveries from global recessions, finding that countries with flexible exchange rates recover more quickly from global downturns. ==== find that a flexible exchange rate is associated with stronger growth during recoveries.==== Related to this, ==== finds that a range of export-related variables have a strong impact in shaping the duration of recoveries from currency crises: these variables include higher output growth in trade partners, a well-diversified export base and improvements in export competitiveness.====Finally, there is literature concerned with the relative speed of recovery of output and employment (both of which we consider here). ==== focus on jobless recoveries – the tendency for output to recover more quickly than employment – which they attribute to a decline in middle-skill jobs. ==== attribute the slow recovery of employment compared to output to the extent of the structural changes associated with recession – to the permanent contraction of some sectors and lags with which other sectors take on those redundant workers – a scenario that is directly relevant to the aftermath of the Covid-19 downturn.",The shape of recovery: Implications of past experience for the duration of the COVID-19 recession,https://www.sciencedirect.com/science/article/pii/S0164070421000367,23 May 2021,2021,Research Article,124.0
Dzhambova Krastina,"Department of Economics, University of Wisconsin - Whitewater, United States of America","Received 20 May 2020, Revised 24 April 2021, Accepted 8 May 2021, Available online 21 May 2021, Version of Record 25 May 2021.",https://doi.org/10.1016/j.jmacro.2021.103319,Cited by (1),"Fiscal procyclicality, meaning co-movement between ","A striking stylized fact of cross-country data comparisons is the tendency of fiscal policy to be procyclical in emerging and poor economies relative to developed economies. Kaminsky et al. (2004) among others==== document that the cyclicality of fiscal policy variables differs between the developing and developed country groups. In particular, developing economies exhibit fiscal procyclicality, which is difficult to justify as a policy prescription. Fiscal procyclicality is defined as a positive comovement between government expenditure and GDP.==== Procyclicality implies that governments in emerging and poor economies tend to reinforce recessions rather than counteract them. Fittingly, Kaminsky et al. (2004) dub this pattern of fiscal procyclicality “when it rains, it pours”.====This paper employs a structural VAR analysis to uncover the reasons for fiscal procyclicality. The theoretical literature has pointed to two broad sources for fiscal procyclicality: (1) political frictions and (2) international financial conditions. Woo (2009) and Ilzetzki (2011) motivate procyclicality in government expenditure with political and social polarization. In these papers, political under-representation or institutional infighting about public good provision makes government expenditure more sensitive to revenue fluctuations. Alesina and Tabellini (2008) propose a model of political agency to explain procyclicality in government spending. Frankel et al. (2012) relates government expenditure procyclicality to lack of good institutions and find empirical evidence that improving the soundness of institutions helps the procyclicality issue. Overall, models based on political frictions predict that government expenditure is more sensitive to output and revenue fluctuations relative to the efficient benchmark.====Another strand of literature highlights the relevance of international credit markets and financial frictions for procyclicality. Several papers provide justification why developing governments face lower debt limits: see Mendoza and Oviedo (2006) and Bi (2012). Riascos and Vegh (2003) and Cuadra et al. (2010) propose models to justify why taxes on consumption in the former case and labor income taxes in the latter case might behave procyclically when international asset markets are incomplete or when premiums on government debt are determined a la Arellano (2008) respectively.====To understand the causes behind procyclicality, I estimate a panel SVAR on a group of developed and emerging economies. My findings lend strong support for international borrowing costs being key for fiscal procyclicality. The evidence on political frictions is less conclusive. While political distortions predict that government spending increases in response to an increase in output, my results suggest that there is no systematic difference in the response to output fluctuations between emerging and developed governments. Emerging governments are however more sensitive to changes in international borrowing costs. These countries lower spending in response to a hike in the cost of borrowing. Higher output relaxes international borrowing constraints and lowers the cost of borrowing for the emerging group. The sensitivity of emerging governments’ consumption spending to sovereign rate fluctuations coupled with the procyclicality of sovereign rates leads to the comovement between government consumption and output. This finding suggests that increases in government consumption expenditure during good times are credited to emerging governments being more sensitive to international borrowing conditions. I employ counterfactual experiments based on the VAR estimates for the emerging and developed economies to illustrate these channels, which shape the pattern of procyclicality.====I investigate the differential role of international financial conditions for policy and business cycle outcomes of emerging and developed economies along two dimensions: shocks to the international safe rate and shocks to the price of a country’s government debt on international markets. Fluctuations in the safe rate have a negligible impact on the business cycle and the fiscal stance in either group of countries. Shocks to the government borrowing rate have different implications for the developed versus the emerging group. An exogenous increase in the sovereign rate is contractionary in both economies. In the emerging group, these shocks are more important for output fluctuations, while in the developed group they matter relatively more for investment dynamics.====The analysis overcomes the issues of endogeneity between (1) fiscal policy and fundamentals (2) international borrowing costs and fundamentals and (3) international borrowing costs and fiscal policy. A number of papers have emphasized the feedback between fundamentals and borrowing costs for emerging economies.==== Fiscal policy variables also both respond to and influence the state of the economy. Finally, fluctuations in the price of externally traded sovereign debt put a constraint on the policymakers’ ability to use fiscal policy for stabilization.====There are several pass-through channels for a sovereign interest rate shock to influence the cycle. First, if government spending contracts in response to increases in the borrowing cost, there is a standard multiplier effect that decreases output and affects investment and consumption contingent on wealth effects and the strength of Ricardian equivalence. Second, the government might shift part of the debt burden domestically and crowd out investment and consumption in this way. Finally, rising sovereign rates can affect the private economy if interest rate shocks pass through to households and firms via the banking sector.==== To shed light on these questions, the analysis needs to isolate orthogonal shocks to policy variables, fundamentals, and international borrowing costs. In contrast to much of the literature, this paper studies both developed and emerging economies. I compare the behavior and the effect of government consumption spending across the two groups. I also characterize the effect of sovereign rate shocks on government consumption spending as well as their direct and indirect effect on private sector variables.====My identification strategy uses the Blanchard and Perotti (2002) SVAR approach to identify fiscal shocks. I use the Uribe and Yue (2006) identification of external borrowing cost shocks and output shocks. The methodology proposed by Blanchard and Perotti (2002) uses institutional knowledge to put constraints on the impact matrix in a VAR analysis. In particular, the identification scheme assumes that discretionary fiscal responses take place with one period delay. To identify external borrowing costs, Uribe and Yue (2006) propose an ordering of the impact matrix, which leads to impulse responses consistent with the standard small open economy real business cycle model (SOE RBC). The current study offers a unified framework for considering the interactions among fundamentals, fiscal policy shocks, and external financial conditions.==== Apart from the variables commonly included in the analysis, I also consider the interplay between the government response function and the dependence of spreads on both aggregate conditions and the government policy stance. This approach allows me to retrieve estimates of the policy response function to output and international borrowing costs as well as the coefficients on the government debt pricing function. I also trace out the dynamic effect of orthogonal shocks to income, sovereign yields, and government spending.====Understanding the reasons behind fiscal procyclicality is important. Policymakers in the developing and emerging world face constraints, which impede their role as insurers against macroeconomic volatility. That is why it is important to investigate if procyclicality is mostly due to political distortions or the result of tighter financial constraints faced by emerging governments.====The paper is organized as follows. Section 2 illustrates the pattern of procyclicality based on the unconditional correlation between government expenditure and GDP. In Section 3, I present a stylized theoretical model to illustrate how both political distortions and the government’s access to international financing relate to procyclicality. In Section 4, I lay out the empirical model and identification. Section 5 describes the data, I estimate the empirical model on. Section 6 reports the results from estimating the empirical model and highlights the key differences between emerging and developed economies. In 7, I employ counterfactual analysis to illustrate which of the estimated differences are important for the pattern of procyclicality. In Section 8, I check whether results are robust to alternative specifications. Section 9 concludes.","“When it rains, it pours”: Fiscal policy, credit constraints and business cycles in emerging and developed economies",https://www.sciencedirect.com/science/article/pii/S0164070421000264,21 May 2021,2021,Research Article,125.0
Hogan Thomas L.,"American Institute for Economic Research, 250 Division St., Great Barrington, MA 01230, United States of America","Received 9 August 2020, Revised 15 May 2021, Accepted 17 May 2021, Available online 21 May 2021, Version of Record 25 May 2021.",https://doi.org/10.1016/j.jmacro.2021.103333,Cited by (4),"This paper econometrically tests for effects on bank lending of the Federal Reserve’s policy of paying interest on excess reserves (IOER). Following the 2008 financial crisis, US banks decreased their loan allocations and increased holdings of excess reserves. A model of bank asset allocation shows that when the rate of IOER is higher than other short-term rates, banks will switch from zero excess reserves to a regime with higher excess reserves and lower lending. Using a sample of panel data on US banks from 2000 through 2018, we find evidence of a switch to a positive excess reserve regime in the post-crisis period. Controlling for market ====, loan demand, and economic activity, we find that IOER accounts for the majority of the decline in bank lending after the financial crisis.","During the financial crisis of 2008, the Federal Reserve initiated a variety of new and unprecedented policies. One such change transformed the Fed’s monetary policy framework by allowing it to pay interest on the excess reserves (IOER) that banks hold at the Fed. Though garnering fewer headlines than its quantitative easing (QE) program and its ad hoc last-resort lending facilities,==== the Fed’s IOER policy and its effect on bank reserves is “one of the most notable and important policy issues in U.S. banking” (Dutkowsky and VanHoose, 2017, p.1). There is, however, little research on the empirical effects of IOER policy and in particular “little analysis of how reserves affect bank lending when interest is paid on reserves” (Martin et al., 2016, pp. 216–217).====Building on a model of bank asset allocation from Dutkowsky and VanHoose (hereafter “DV” Dutkowsky and VanHoose, 2017, Dutkowsky and VanHoose, 2018a, Dutkowsky and VanHoose, 2018b, Dutkowsky and VanHoose, 2020), we find that low rates of IOER increase lending but that banks reduce their loan allocations when the rate of IOER is greater than rates on other short-term assets. A Fed balance sheet expansion that increases total reserves will increase lending if the rate of IOER is below short-term interest rates, as in a corridor system of monetary policy. When the rate of IOER is above other short-term interest rates, as it was following the financial crisis, an increase in total reserves leads to a decrease banks’ loan allocations since excess reserves become a profitable alternative to loans.====This study provides an empirical investigation of the effects of IOER on bank lending. In Q4 of 2008, the Fed began paying a rate of IOER that was higher than other short-term rates, which our model indicates may have caused banks to increase their excess reserve holding and decrease their loan allocations. Using regression analysis, we test for the effects of IOER on bank loan allocations in a sample of panel data on US banks from Q1 of 2000 through Q4 of 2018. We find evidence of a change to a regime of positive excess reserves following the 2008 financial crisis. In the post-crisis regime, IOER has large negative effects on bank lending. Controlling for economic factors such as GDP growth, unemployment, uncertainty, and loan demand, we find that the rate of IOER relative to other short-term interest rates accounts for the majority of the post-crisis decline in lending as a percentage of bank assets.",Bank lending and interest on excess reserves: An empirical investigation,https://www.sciencedirect.com/science/article/pii/S0164070421000380,21 May 2021,2021,Research Article,126.0
Biolsi Christopher,"Department of Economics, Western Kentucky University, Bowling Green, KY 42101, USA","Received 18 August 2020, Revised 7 May 2021, Accepted 11 May 2021, Available online 20 May 2021, Version of Record 25 May 2021.",https://doi.org/10.1016/j.jmacro.2021.103321,Cited by (0),"In this paper, I assess the evidence for a structural break in labor productivity growth in the years before the ","In recent years, much ink has been spilled attempting to explain the causes of or proposing solutions for slowing growth in labor productivity in the United States. Gordon (2016), for example, posits that recent rates of innovation do not begin to approximate those experienced in the late 19th and early 20th centuries, an argument backed up by the findings of Bloom et al. (2017) that it currently requires greater research and development inputs to achieve the same level of innovation relative to the past. Fernald (2014), Cette et al. (2016), and Fernald et al. (2017) also point to a decline in the growth rate of total factor productivity (a major ingredient of labor productivity) as being a major cause of slow GDP growth in the wake of the Great Recession, citing Oliner et al. (2007) as finding evidence for a structural break in productivity growth before the recession even started.==== Although some have argued that the productivity slowdown might be chalked up to measurement issues (see, e.g., Feldstein, 2017), others, such as Syverson (2017), suggest that such explanations do not sufficiently account for the observed low readings of recent years.====Another dimension of the slowdown story draws from reasoning expressed in Delong and Summers (2012). The idea is that persistently suppressed aggregate demand that causes extended bouts of unemployment could cause an erosion of human capital and, consequently, reduced rates of labor productivity growth (the “secular stagnation” hypothesis).====These kinds of arguments have found their way into policy making circles. In an essay in ====, former President Barack Obama pointed to declining productivity growth as one of the major challenges facing his successors.==== In a speech, his final Chairman of the Council of Economic Advisers, Jason Furman, cited Fernald (2014) in taking declines in labor productivity growth as a given to be addressed with better policy (Furman, 2015).====In this paper, I will attempt to assess whether incorporating such a hypothesized structural slowdown into forecasting models can improve forecasting performance of labor productivity growth, in the root mean squared error sense.==== Furman (2015) points out that forecasting labor productivity is especially difficult and that using the longest possible estimation window often produces the lowest mean squared error forecast. This, however, is not surprising in light of work by, among others, Pesaran and Timmermann (2007) and Pesaran et al. (2013), that suggests that only using post-structural break observations (or post-regime change observations) to forecast future realizations in a time series is made more difficult by the fact that there tends to be a great deal of uncertainty around both the precise timing and the actual magnitude of any supposed structural break.====With these concerns in mind, I will look to prepare forecasts with the help of the Beveridge–Nelson filter, recently developed in Kamber et al. (2018). In their study introducing the filter, these authors show that it helps provide improvements in forecasting future dynamics of a given time series. I will apply the Beveridge–Nelson filter to the process for labor productivity, allowing for structural breaks in the mean growth rate of the series. Then, I will compare the forecast performance of the cyclical deviations produced by the filter with and without structural breaks to determine if including structural breaks makes for substantially smaller root mean squared forecast errors. In addition, I will compare the forecast performance to a naive random-walk-with-drift model.====I prepare forecasting models making use of more than two dozen specifications of the Beveridge–Nelson filter, as well as one based on only past observations of labor productivity growth. I also consider both rolling and recursive forecasting techniques, for AR(1) and higher order autoregressive models, and for both one-step-ahead forecasts and longer-horizon forecasts. For a recursive methodology, I find that, in the period since the end of the Great Recession, the best forecasting performance at both the one-step-ahead and two-year-ahead horizons is provided by a forecasting model that includes one lag of the Beveridge–Nelson filter with no structural breaks. This model significantly outperforms the random walk with drift at the one percent level, consistent with evidence reported in Kamber et al. (2018). Like the other models, however, even with smaller forecast errors, this best-performing model still proved to be too optimistic about labor productivity growth in the years following the end of the Great Recession.====This study does not claim that there was no structural break in productivity growth ahead of the financial crisis. Rather, it only argues that there is significant estimation uncertainty surrounding both the timing and the magnitude of such a break or the data-generating process behind labor productivity growth. This notion is supported by results using the AveW estimator of Pesaran and Pick (2011) and the structural break test of Boot and Pick (2020). In turn, this uncertainty makes incorporation of supposed structural breaks into forecasting models more difficult and potentially biases the forecasts.====These findings reinforce the need for caution in making declarations about structural changes in macroeconomic variables, especially one as important as labor productivity growth. This variable is an essential ingredient in the preparation of forecasts for a number of policy bodies, including the federal government and the Federal Reserve, and given recent low readings, it is understandable that policy makers would want to be vigilant and adjust forecasts nimbly to take account of a “new normal”, not least to come up with more appropriate policy prescriptions. The results of my study, however, imply that over-zealous corrections to views about mean productivity growth could also be problematic.====The rest of this paper proceeds as follows. In Section 2, I briefly discuss the Beveridge–Nelson filter developed in Kamber et al. (2018) and report the estimated cyclical deviations it implies for labor productivity growth. Section 3 examines the evidence for a pre-Great Recession structural break using conventional tests developed in the literature. Section 4 gives the results of the various forecasting exercises, and Section 5 turns to a Monte Carlo exercise to attempt to assess why the model with no structural breaks seems to outperform those that include breaks. Section 6 offers concluding remarks, as well as paths for future research.",Labor productivity forecasts based on a Beveridge–Nelson filter: Is there statistical evidence for a slowdown?,https://www.sciencedirect.com/science/article/pii/S0164070421000276,20 May 2021,2021,Research Article,127.0
Cavenaile Laurent,"Department of Management, University of Toronto Scarborough, Canada,Rotman School of Management, University of Toronto, Canada","Received 21 May 2020, Revised 27 April 2021, Accepted 30 April 2021, Available online 6 May 2021, Version of Record 7 May 2021.",https://doi.org/10.1016/j.jmacro.2021.103317,Cited by (2),"This paper proposes a model of ==== with heterogeneous agents in terms of human capital to quantify the role of offshoring and computerization in labor market polarization and increased top income inequality. We find that both offshoring and computerization played a major role regarding labor market polarization in the US over the period 1975–2008. We further show that the last decades can be decomposed into two subperiods. Computerization is the main driver of labor market polarization from 1975 to the mid 1990s, after which globalization (through decreased costs of offshoring) explains more than 70% of job and wage polarization. Our model can also explain around 40% of the observed increase in top income inequality since 1975.","Since the early 1980s, the US labor market has experienced several major transformations in terms of income distribution. A large body of literature documents an increase in income inequality in the US. Katz and Murphy (1992) and Autor et al. (1998) (among others) show evidence of a steady increase in the college wage premium. One common explanation for this phenomenon is skill-biased technical change (Acemoglu, 1998, Acemoglu, 2002, Krusell et al., 2000). However, models of skill-biased technical change cannot generally account for two key characteristics of the evolution of the job and wage distributions over the last decades: job and wage polarization. Autor et al. (2003) and Autor et al. (2006) document job polarization in the US making a distinction between skills and tasks, and between routine and non-routine tasks. Routine tasks can be substituted by computers.==== Hence, cognitive routine tasks (typically in the middle of the wage/skill distribution) can potentially be replaced by computers while non-routine manual (low wages/human capital) and non-routine cognitive tasks (high wages) cannot. Substitution of routine manual (middle-skill) tasks by computers leads to a U-shaped relationship between the change in employment shares of occupations and their skill-content.====Beside computerization, offshoring has also been proposed as a competing explanation for the hollowing out of middle-wage jobs (Firpo et al., 2011) as routine manual jobs are potentially subject to relocation to lower-wage countries. Over the same period, wages also started to follow the same pattern of polarization, i.e. wage increase was relatively higher for occupations with low- and high-skill content than for occupations in the middle of the wage distribution (Goldin and Katz, 2007, Autor et al., 2008, Antonczyk et al., 2010, Autor and Dorn, 2013). On the other hand, the early 1980s marks the beginning of an increase in top income inequality in the US (see Piketty and Saez, 2003, Piketty, 2005). Atkinson et al. (2011) document that the increase in the share of top income is mainly due to labor income increase for top earners. The share of US income going to the top 10% of income earners grew over the period 1975–2013, going from around 32% to about 47%.====In this context, the main contribution of this paper to the existing literature is to propose a new structural quantitative framework to jointly assess the impact of both offshoring and automation on the polarization of the US labor market as well as on top income inequality. This new framework allows us to quantify the relative role played by both technological change and offshoring in explaining labor market polarization. While we find that both forces played a significant role, we also highlight a shift from automation to offshoring as the main driver of labor market polarization during the 1990s.====Whereas computerization is a result of technological improvements, offshoring is mainly a result of globalization and is related to several factors such as reductions in formal barriers (e.g., improvement in property rights or removal of restriction on foreign investments), trade liberalization or decreased communication costs. Over the last few decades, the stock of computer capital held by US companies increased significantly. This period has also been marked by increased economic globalization and offshoring of production. For instance, trade as a share of GDP and foreign direct investments have been increasing since the late 1970s.====While there exists an empirical literature trying to estimate the role of offshoring and computerization on labor market polarization, there is no structural quantitative assessment of the relative role of each of these factors on both job and wage polarization, and top income inequality within a single framework. Our model aims at filling this gap focusing on annual data instead of long-term patterns. We build a general equilibrium model of occupational choice with heterogeneous agents in terms of human capital (or skills) and four different occupations. The model is an extension of Eeckhout and Jovanovic (2012) to four occupations.==== We also follow Autor and Dorn (2013) in assuming that there are two sectors in the economy respectively producing goods and services but we do not restrict skills to belong to only two skill levels. Instead, we use a continuous distribution of human capital. The allocation of skills to different occupations is endogenously determined. Jung and Mercenier (2014) also propose a general equilibrium model with a continuum of skills to study the effect of globalization and routinization-biased technical change. Their model differs from ours by assuming three different types of occupations and not allowing for a direct interaction between different skills in production. In addition, they do not use their model to quantify the role of offshoring and computerization in explaining labor market polarization. A similar model can be found in Cortes (2016) who focuses on the role of technical change on polarization. Costinot and Vogel (2010) study the effect of technical change and globalization on income distribution in a worker assignment model and assume perfect substitutability between workers of different skill levels in the production of tasks. We show that allowing for some interaction between different levels of skills through the span of control of high-skill agents can have implications for the distribution of income. In addition, we use our model to perform a quantitative assessment of the role of offshoring and computerization in explaining labor market polarization. Using a quantitative model, Vom Lehn (2019) finds that technical change explains a large share of the observed polarization of the labor market until 2000 but that it cannot explain the subsequent trends. By also allowing for globalization to affect polarization in our quantitative framework, we first confirm the major role that computerization played until at least the mid-1990s and the fact that automation cannot explain the trends afterwards as globalization becomes the main driver of polarization in the 2000s.====In our model, the goods industry is assumed to be more human capital intensive than the service industry and jobs in this industry can either be offshored or automated. Production of either goods or services is performed by firms or production units. Each of these units combines one “team leader” and workers. A “team leader” can be seen as a manager who is in charge of a team of workers or, more generally, as any other occupation which involves “creative” abstract thinking by one agent and a subsequent implementation by a group of workers. In the rest of the paper, we refer to these “team leaders” as managers. Managerial jobs in both industries are assumed to be human capital intensive. In equilibrium, managers are high skill agents and managers with higher human capital work in larger production units.====Agents can choose between working in the service or goods industry or performing managerial tasks in either of the two production sectors. Agents endogenously sort themselves and work in the occupation which offers the highest wage given their level of skills. A decrease in the price of computers or in the cost of offshoring leads to a reallocation of agents across industries and to changes in relative wages. The demand for middle-skill workers falls leading to a decrease in the share of employment and relative wages in middle-skill occupations. At the same time, managers see their relative wage increase as they benefit from the decreased costs of production.====Our calibrated model closely matches the evolution of employment shares by occupations and of relative wages over the period 1975–2008. It shows that both globalization (through offshoring) and computerization had a significant impact on labor market polarization. Interestingly, our results show that the relative role of computerization and offshoring in explaining labor market polarization changes significantly over time. In particular, we find that offshoring became the main driving force over the most recent years.====Our model and quantitative analysis build on the evidence in Autor and Dorn (2013). They document that polarization of employment and wages in the US can be related to the growth of low-skill services. These services, which are at the bottom of the wage/skill distribution, experienced an increase in employment share and relative wages which account for most of the observed labor market polarization in the US. Beyond low-skill services, they identify four routine/middle-skill occupation groups as well as non-routine managerial activities (at the top of the skill distribution). We further merge the four middle-skill occupation groups giving us three groups in total: low-skill services, middle-skill occupations and managerial jobs.====
 Fig. 1 shows the evolution of employment shares and real wages for these three occupation groups.==== The plot shows the polarization of both employment and wages starting in 1980. The employment share in low- and high-skill occupations increases over the period, consistent with a U-shaped relationship between change in employment and skill content. At the same time, the wage of low- and high-skill groups increased relatively faster than in the middle-skill group.====Computerization and offshoring may have also played a role in the increase in top income inequality over the past decades. Technological change, which decreases the cost of computer capital, and cheaper offshoring opportunities can boost firms’ size and profit.==== The increase in firm size and profit can further be accompanied by increased salaries for managerial occupations which are typically at the top of the wage distribution.==== In our model, globalization and automation reduce the production cost of firms raising their profits which leads to an increase in managerial compensation. In this sense, our model captures the idea that globalization and computerization could increase the return to superstars as in Rosen (1981).==== Our model can explain around 40% of the observed increase in top income inequality.====The rest of the paper is organized as follows. First, we present the occupational choice model in Section 2 and illustrate its comparative statics. We then turn to the quantitative analysis in Section 3 where we describe the data, our calibration and the predictions of the model. Finally, Section 4 concludes.","Offshoring, computerization, labor market polarization and top income inequality",https://www.sciencedirect.com/science/article/pii/S0164070421000252,6 May 2021,2021,Research Article,128.0
"Uchida Yuki,Ono Tetsuo","Seikei University, Japan,Osaka University, Japan","Received 15 October 2020, Revised 26 March 2021, Accepted 22 April 2021, Available online 27 April 2021, Version of Record 5 May 2021.",https://doi.org/10.1016/j.jmacro.2021.103315,Cited by (1),"This study considers the politics of public education and its impact on economic growth and welfare across generations. We employ probabilistic voting to demonstrate the generational conflict regarding ==== and spending and show that aging shifts the tax burden from the retired to the working generation, reduces public education spending, and ultimately slows economic growth. We subsequently consider a legal constraint that aims to boost education spending: a spending floor for education. This constraint stimulates economic growth but creates a trade-off between current and future generations’ welfare. Finally, the quantitative implications of our results are explored by calibrating the model to the Japanese economy.","When looking at how population aging affects public education expenditure, median voter theory suggests that the composition of government spending is biased toward goods and services that benefit the elderly because a decisive voter becomes older as the population ages. This implies that with aging, public education expenditure, which provides fewer benefits to the elderly, is expected to decrease. Indeed, the Organisation for Economic Co-operation and Development (OECD) reports that Japan, which has been experiencing rapid population aging over the past two decades, had the lowest ratio of public education expenditure to GDP among the 34 comparable OECD member countries in 2014 and 2015 (OECD, 2017, OECD, 2018). This negative association between aging and public education expenditure is also reported by Ohtake and Sano (2010), who analyze the effects of population aging on compulsory public education expenditure in Japan using prefectural panel data from 1975 to 2005.====Given this background, the present study proposes a political economy theory that demonstrates the mechanism behind the reductions in public education expenditure and economic growth. Specifically, we propose a three-period-lived overlapping-generations model with physical and human capital accumulation (e.g., Kunze, 2014, Lambrecht et al., 2005, Ono and Uchida, 2016). Public education contributes to human capital formation and is funded by taxing the labor income of the working generation (i.e., the middle-aged) and capital income of the retired generation (i.e., the old) (e.g., Boldrin, 2005, Soares, 2003, Soares, 2006). An increase in the labor income tax rate to finance education expenditure lowers savings and raises the interest rate in the next period. Therefore, the middle-aged enjoy public education expenditure through the general equilibrium effect, whereas the old do not.====To present this generational conflict over education expenditure and taxation, we employ probabilistic voting à la Lindbeck and Weibull (1987). Within this voting framework, the government, representing the middle-aged and old populations, chooses expenditure and taxes to maximize the weighted sum of the utility of the middle-aged and old. Based on this voting mechanism, we demonstrate the political determinants of both expenditure and taxes and their impacts on economic growth and welfare over time and across generations.====Within this framework, we show that population aging lowers public education expenditure and shifts the tax burden from the old to the middle-aged because the government reflects the preference of the old in policies. A decrease in public education expenditure discourages human capital accumulation; an increase in the labor income tax rate reduces savings and thus slows physical capital accumulation. Therefore, population aging has a negative impact on economic growth.====A reduction in public education expenditure, stemming from population aging, reduces human capital accumulation. This implies a negative impact on economic growth via the choice of fiscal policy, as reported by Cattaneo and Wolter (2009) and the references therein. One way to tackle this negative impact is to introduce a spending floor constraint that supports spending on public education, which is being or has been implemented in some East Asian countries such as Indonesia (OECD, 2010), Malaysia (OECD, 2016), South Korea, and Taiwan (Ho, 2006).====We examine the effect of a spending floor constraint on public education and show that the constraint spurs education spending and promotes human capital accumulation. At the same time, it induces a rise in the labor income tax rate and thus reduces physical capital accumulation. Overall, the spending floor promotes economic growth and thus benefits future generations. However, it forces the government to increase the capital income tax rate to finance its increased expenditure and thus worsens the welfare of the current old generation. Therefore, the constraint creates a trade-off between current and future generations in terms of welfare.====We calibrate the model to the Japanese economy to quantitatively explore the implications of the spending floor constraint for Japan. The sample period for estimating the model parameters is 1995–2014. During this period, the average ratio of public education expenditure to GDP is 0.0324. However, the average ratio during 1987–1989, or the “bubble years”, is 0.0551, which is the highest over the past 40 years (see Panel (a) in Fig. 1). Furthermore, the annual growth rate of GDP per capita during this period is also the highest in the past 40 years (see Panel (b) in Fig. 1). The data suggest that if the Japanese government had maintained the education-to-GDP ratio of the bubble years, it might have avoided the economic stagnation that followed. Thus, from the perspective of maintaining economic growth, we take the ratio 0.0551 as a target and analyze how the growth and distribution of utility over time and across generations would have changed if Japan had increased the ratio and followed the policy during the bubble years.====To further motivate the analysis of the spending floor constraint, we evaluate the optimality of the political equilibrium from the viewpoint of the benevolent planner who can commit to all his or her choices at the beginning of the period, subject to the resource constraint. Assuming such a planner, we compare the political equilibrium in the presence of the spending floor constraint with the planner’s allocation. This comparison enables us to evaluate short-sighted politicians’ decisions, corrected by the spending floor constraint, from the far-sighted planner’s viewpoint.====We show that the welfare of the old in the presence of the spending floor constraint is lower than their welfare that would have been obtained in the planner’s allocation. This is because the constraint increases the tax burden on the old and reduces their consumption. We also show that the consumption and welfare allocations of some successive generations in the presence of the constraint deviate from those that would have been obtained in the planner’s allocation. This deviation occurs because compared with the planner’s allocation, the constraint stimulates human capital accumulation, while inhibiting physical capital accumulation. These results are obtained by assuming that the planner’s discount factor equals the individual one. We also verify the robustness of the results by considering alternative discount factor cases for the planner and show that the choice of the planner’s discount factor matters as to who benefits or loses from the spending floor constraint.====With these results, we can draw policy implications for Japan, which, as noted earlier, ranked last among the 34 comparable OECD member countries in public spending on education in 2014 and 2015. The introduction of a spending floor is a way to increase public education expenditure and, in turn, economic growth in Japan. However, at the same time, such benefits accrue to future generations at the expense of the currently living old. Thus, when policymakers decide on a spending floor or similar policies, they should keep in mind that a trade-off between generations in terms of utility is created. The same argument applies to other countries with low education spending-to-GDP ratios and growth rates, such as Greece and Italy.====The rest of this paper is organized as follows. We first review the relevant literature in Section 1.1. Thereafter, Section 2 presents the model and characterizes the economic equilibrium. Section 3 characterizes the political equilibrium and investigates the effects of generational conflict on economic growth. Then, we introduce the spending floor for education and evaluate it in terms of growth and welfare. Section 4 calibrates the model to the Japanese economy and compares the political equilibrium in the presence of the spending floor with the planner’s allocation. Section 5 concludes.",Generational conflict and education politics: Implications for growth and welfare,https://www.sciencedirect.com/science/article/pii/S0164070421000240,27 April 2021,2021,Research Article,129.0
"Strong Christine,Yayi Constant","Department of Economics, Old Dominion University, 2021 Constant Hall, Norfolk, VA 23529, United States,A.R. Sanchez School of Business, Texas A&M International University, 5201 University Boulevard, Laredo, TX 78041, United States","Received 17 November 2020, Revised 9 April 2021, Accepted 12 April 2021, Available online 16 April 2021, Version of Record 26 April 2021.",https://doi.org/10.1016/j.jmacro.2021.103313,Cited by (0),"Can ==== (CBI) help to reduce fiscal balances? In this paper, we answer this question using novel measures of CBI based on the turnover rate of central bank governors (TOR) and the Garriga measure of legal independence for 30 African countries for the period 1990–2017. Our novel measures of CBI capture the degree of alliance between the fiscal authority and the monetary authority which can potentially lead to debt monetization and higher fiscal balances. Thus, we classify central bank governor changes into ally changes or non-ally changes; in addition to that, we decompose our full sample into CFA zone countries and non-CFA zone countries to capture the effect of currency union membership. Our results show that for CFA zone countries, central bank autonomy, when proxied by the turnover rate of central bank governors, is associated with a decrease in fiscal balances and replacing a central banker with a non-ally, is negatively and significantly associated with fiscal balances.","Most economists agree that the separation of the fiscal authority from the monetary authority is essential to economic stability (Barro and Gordon (1983) or Sikken and De Haan (1998)) and have considered fiscal dominance or a situation in which the fiscal authority dominates the monetary authority, an idea of the past. But events, such as the financial crisis of 2008 or the current covid-19 pandemic, that have resulted in a massive increase in many central banks’ sheets and higher debt to GDP ratios, are bringing this idea back to life. Indeed, in today's world, with interest rates at an all-time low and negative in some European countries, central banks, while becoming more creative, are still limited in the tools they can use to respond to future recessions. This has led to an increased reliance on fiscal policy that most likely will translate into higher government spending. As governments rack up more debt to curtail the negative impacts of recessions, the pressure on central banks to accommodate governments will become more significant; hence, the resurgence of fiscal dominance.====For many African countries, fiscal dominance has been an ongoing issue that has threatened the balance of power between the fiscal authority and the monetary authority. In Africa, fiscal dominance takes many forms, and range from direct financing of government debt by central banks to pressure on the central banks to keep interest rates low or to intervene in the foreign markets to limit currency depreciation and lower debt servicing costs. Between 2013 and 2018, the average debt of African countries has risen by almost 20 percentage points of GDP==== and countries that are commodity and oil exporters such as Nigeria or Cameroon are particularly vulnerable, given that they tend to run procyclical fiscal policies, which leaves them unprotected when face with negative commodity price shocks. Konuki and Villafuerte (2016) estimate that between 2005 and 2008 (a period of high oil prices), 83% of oil-exporting African countries ran expansionary fiscal policies compared to only 56% of Sub-Saharan African countries. This higher level of debt is especially concerning because African countries’ ability to repay their debt has deteriorated significantly over the years.====In this paper, we ask whether an increase in central bank independence (CBI) can curtail monetary accommodation or the need of the central bank to accommodate the fiscal authority. We contend that the more independent a central bank is, the higher the fiscal balances (or the lower the fiscal deficits) are likely to be since an autonomous central bank can force governments to become fiscally disciplined and prevent them from overlying on debt monetization. If this assumption holds, then we expect a positive relationship between CBI and our dependent variable of interest, each country's fiscal balance.====To answer this question, we start by following conventional practice by measuring de facto CBI using the turnover rate of central bank governors (TOR) and construct a dataset for 30 African countries for the period 1990–2017, where we code each central bank governor change as either an ally change or non-ally. TOR relies on the assumption that above a certain threshold, a higher TOR is synonymous with a lower level of independence (Cukierman et al., 1992). This is because early and frequent removals of central bank governors might be an indication that the fiscal authority is seeking to appoint a subservient central bank governor whose monetary responses match the government's agenda. In essence, the executive branch is looking to appoint an ally. A certain degree of alliance between the fiscal authority and the monetary authority can exacerbate the time-inconsistency problem, i.e. a central bank reneges on its commitment to price stability and instead surprises economic agents with an expansion to stimulate the economy (Barro and Gordon (1983); Kydland and Prescott (1977)). Presumably, a certain degree of autonomy can insulate the central bank from any political pressure and improve fiscal discipline by allowing a central banker to focus on the primary goal of price stability.====While many economists agree that higher central bank independence yields lower inflation (Cukierman, Web, and Neyapti, 1992; Cukierman et al., 2002; Crowe and Meade 2008), and/or that CBI can help to reduce political business cycle shocks (Nordhaus, 1975); there is a lack of consensus in the literature concerning the relationship between fiscal deficits and CBI. For instance, in developed countries, studies have found a negative relationship between CBI and fiscal deficits in the case of OECD countries (See Burdekin and Laney (1988), Grilli et al. (1991), or De Haan and Sturm (1992) whereas in developing countries, similar studies have yielded ambiguous and statistically insignificant results. (See Sikken and De Haan (1998) or Bodea (2013)).====The ambiguity of the results obtained in the case of developing countries we argue is due to the following reasons: first, the studies examining the relationship between CBI and fiscal balances in developing countries have primarily relied on TOR. But using the turnover rate of central bank governors as a proxy for de facto CBI in developing countries and in Africa, in particular, poses some issues. First, while the consensus in the literature is that a high TOR is a signal of low CBI it is also possible that a low TOR can be an indication of low CBI, since an obedient central banker can simply remain as head of the central bank. Second, TOR fails to explicitly capture the de facto institutional environment, in which African central banks operate; specifically, they have failed to explicitly model how fiscal dominance affects the de facto degree of CBI for African countries. Moreover, the literature on CBI has, for the most part, focused on the relationship between CBI and inflation; consequently, there has not been as many empirical studies on the relationship between fiscal deficits and CBI. Finally, in the case of developing countries, many studies have not accounted for the fact that some African countries belong to a currency union.====Therefore, this paper addresses these shortcomings and contributes to the literature in the following ways: first, we devise a novel measure of CBI that accounts for the presence of fiscal dominance in African countries, this is necessary for an optimal and efficient measure of the autonomy of African central banks. Thus, we decompose the turnover rate of central bank governors to better capture the level of alliance that is persistent between African governments and their central banks, and in doing so, we classify a central bank governor as an ally change if he or she falls in at least one of the following categories: 1) he or she occupied an important position related to the economy such as minister of finance or minister of the economy in the executive branch before his or her nomination as a central bank governor====; 2) he or she occupies an important position in the ruling party, i.e. the political party of the head of the executive branch; 3) he or she shares the same ethnic background as the head of the executive branch.====This new measure of TOR is then used to assess the effect of independence on fiscal balances. In a nutshell, we ask the following question: are lower fiscal balances more likely when central bankers are an ally of the executive branch?====Second, Africa is home to one of the two largest currency unions in the world: the CFA zone, thus our sample consists of 16 non-CFA countries and 14 CFA countries. This allows us to ask another key question: Does the effect of CBI on fiscal balances vary with currency union membership? In a currency union, countries share a common central bank, whose monetary objectives consist of implementing optimal policy responses than ensure the economic well-being of the union as a whole. Thus, a common central bank should have ==== more autonomy than a national central bank and be less likely to cave to political pressures. Consequently, we hypothesize that if CBI, proxied by TOR, reduces fiscal balances, then this effect should be smaller for countries that belong to a currency union.====Third, we further test the effect of CBI on fiscal balances by using another measure of CBI, the Garriga index of legal independence (see Garriga (2016)), to test the robustness of our results. The Garriga index extends the Cukierman measure by first including central bank reforms that both decrease or increase CBI and second, by including a larger dataset consisting of 182 countries. Finally, we test whether the effect of CBI on fiscal balances is conditional on the level of financial development, and whether or not a country is going to an electoral cycle, or periods of stress, which we proxy by political instability.====Our key findings, based on our GMM results, can be summarized as follows:====The paper is structured as follows. Section 2 explores the evolution of CBI and fiscal balances in Africa, Section 3 describes the related literature review. Section 4 describes the estimation method and the data. Section 5 presents the results. Section 6 focuses on sensitivity analysis and Section 7 concludes.","Central bank independence, fiscal deficits and currency union: Lessons from Africa",https://www.sciencedirect.com/science/article/pii/S0164070421000239,16 April 2021,2021,Research Article,130.0
Sorge Marco M.,"University of Salerno, University of Göttingen and CSEF, Italy","Received 5 October 2020, Revised 25 March 2021, Accepted 29 March 2021, Available online 3 April 2021, Version of Record 9 April 2021.",https://doi.org/10.1016/j.jmacro.2021.103312,Cited by (0),"I revisit the stabilizing and determinacy properties of Taylor-type policy rules in the canonical ==== when allowing for a unit root in the supply shock process. While able to offset inflationary pressure from non-stationary disturbances, interest-rate feedback rules that are unresponsive to fluctuations in the output gap necessarily produce unstable dynamics and explosive volatility for the latter. Specifically, rules fulfilling the Taylor principle are found to enforce the unique (non-stationary) equilibrium featuring well-anchored ==== and immunity to sunspots; yet there exists no equilibrium predicting stationary behavior for both the ","Since Taylor (1993), simple reactive rules stipulating how changing measures of macroeconomic activity feed back into interest rate policy have provided a useful framework for the analysis of monetary policy, from both a positive and a normative perspective. While able to capture historical patterns of policy behavior, especially in the U.S., Taylor rules have also been advocated as a functional guidance in policy decisions, on account of their empirically documented good stabilization performance vis-à-vis alternative proposals (e.g. Taylor, 1999).====The adoption of rule-based interest rate policies embeds a stabilization logic insofar as they conform to the so-called ====, i.e. when they respond more than one-for-one with inflation. Intuitively, when inflation is expected to rise, abiding by the Taylor principle forces real interest rates to surge, so as to depress aggregate demand and generate disinflationary pressure. When framed in the context of simple general equilibrium models of the monetary business cycle, the Taylor principle has also been shown to impart uniqueness of the dynamically stable rational expectations (RE) equilibrium, i.e. a collection of bounded equilibrium responses of endogenous variables to fundamental macroeconomic disturbances that ==== converge back to the model’s non-stochastic steady state (e.g. McCallum, 1981, Clarida et al., 2000). This equilibrium is said to be ====, for it delivers a unique value for each of the endogenous variables of the system conditional on the structure of the economy and the realizations of the fundamental shocks hitting it (e.g. Benhabib and Farmer, 1999, Lubik and Schorfheide, 2003, Lubik and Schorfheide, 2004, Farmer et al., 2015).====How monetary authorities react to such shocks based on their actual (or perceived) persistence is of course key to evaluating the effectiveness of monetary policy against its stabilization goals. Recent evidence suggests that supply disturbances are a key driver of inflation and output dynamics, and that their conventional measures tend to exhibit near unit root behavior. Permanent supply shocks have been documented to largely contribute to variance and autocorrelation patterns for output growth in major European countries and the US (e.g. Hartley and Whitt Jr, 2003). Inspection of high frequency data from the crude oil market reveals that unanticipated exogenous changes in the oil supply are best approximated by a random walk process (e.g. Maslyuk and Smyth, 2008).====System-based inference on the persistence properties of identified supply shocks in structural models of the US economy is, by contrast, somewhat inconclusive. For example, Del Negro and Schorfheide (2008) report evidence of mildly persistent cost-push shocks, whereas both Lubik and Schorfheide (2004) and Benati and Surico (2009) offer maximum likelihood estimates of the persistence parameter ranging from 0.418 to 0.85. Other studies conducted within a Bayesian framework rather point to highly persistent (close to random walk) processes for supply shocks: using data from the postwar United States and different model specifications, Ireland (2004) estimates the persistence of the autoregressive cost-push shock process to be 0.9907 over the post-1980 sample (the Great Moderation period), whereas Smets and Wouters (2007) report a posterior mode for the autocorrelation coefficient of the identified wage markup process of 0.97.====In light of such mixed evidence on the time series properties of supply shocks, and given their role in shaping fundamental policy trade-offs in New Keynesian (NK) environments (e.g. Galí, 2008), the question of how monetary authorities should design policies in response to the destabilizing effects on macroeconomic volatility of non-stationary supply-side disturbances is a sensible one. The main contribution of the present study is to re-expose and further explore the stabilizing and determinacy properties of Taylor rules in the prototypical NK model, with an explicit focus on unit root behavior for the supply (here, cost-push) shock process. Specifically, I analyze what NK theory has to say about the ability of rule-based interest rate policies to insulate the economy from a non-stationary cost-push shock as well as from non-fundamental (sunspot) noise. I show that Taylor-type rules fail to prevent emergence of stationary equilibrium representations for the endogenous variables, no matter whether the policy stance on inflation stabilization induces determinacy or indeterminacy, or whether a forward-looking rather than contemporaneous rule specification is adopted. What is more, a Taylor-type rule that is unresponsive to output gap fluctuations and yet satisfies the Taylor principle is unable to replicate the optimal discretionary policy targeting micro-founded objective functions, i.e. those encompassing welfare-relevant measures of economic activity other than inflation. These core findings qualify previous results in the literature, while also extending them along several dimensions, all of which appear to be relevant for the economic insights of the model and its policy implications.====Yao (2014) is the first to explore the consequences of permanent cost-push shocks for inflation and output gap stabilization, when a Taylor-type instrument rule describes monetary policy behavior. He establishes that (i) stabilizing inflation requires the Taylor rule not to depend on the contemporaneous output-gap measure, no matter whether the model features backward dependence (inertia) or not; and that (ii) the Taylor principle suffices to warrant a determinate RE equilibrium, in the sense of being the unique dynamically stable solution to the NK model.====I first argue that Yao (2014)’s findings need a qualification. While the Taylor principle enforces a uniquely determined stable path for inflation as a function of fundamental shocks, stabilizing inflation ==== that the output gap follow a non-stationary process. In fact, the model under scrutiny fails to admit a (stable) saddle-path representation. As a result, invoking a zero response of the nominal interest rate to changes in the output gap ==== forces a unit root in the output gap dynamics. More specifically, I establish that a muted response of the interest-rate rule to output gap fluctuations and fulfillment of the Taylor principle jointly manage to stabilize inflation dynamics, and yet produce unbounded growth in the unconditional variance of the output gap series (Proposition 1).====I then go a step further and show that interest-rate feedback rules that violate the Taylor principle (and thus open room to equilibrium indeterminacy) ==== safeguard against time-varying volatility originating from non-stationary shocks, no matter whether output gap fluctuations are targeted or not; that is to say, even when considering model’s parameter configurations that support existence of multiple (indeterminate) equilibria, there exists no equilibrium in which both inflation and output gap exhibit stationary behavior (Proposition 2). Being grounded in the theory of linear stochastic recursions with martingale difference errors (e.g. Broze and Szafarz, 1991), this finding holds true ==== the method used to represent and/or estimate the full set of equilibria under indeterminacy (e.g. Lubik and Schorfheide, 2003, Fanelli, 2012, Farmer et al., 2015, Bianchi and Nicolò, 2017).====As mentioned, I also investigate whether these insights extend to NK environments where the interest-rate feedback rule reacts to forecasts of future rather than contemporaneous output and inflation deviations, and thus is forward-looking in nature (e.g. Bullard and Mitra, 2002); and explore the relationship between Taylor rules that fully stabilize inflation in the presence of unit root supply shocks and optimal policies derived under discretion when monetary policy objectives reflect society’s welfare losses. I find that (i) similarly to the contemporaneous data specification, forecast-based rules are unable to impart stationary dynamics for equilibrium inflation and output gap in the presence of a permanent supply shock process; and that (ii) the optimal discretionary policy targeting a welfare-theoretic loss function cannot be implemented via a Taylor-type rule that aggressively targets the inflation rate and stipulates no response to fluctuations in real economic activity.====The remainder of the paper is as follows. Section 2 lays out the model, while Section 3 is devoted to the analysis of the model’s equilibrium. Section 4 discusses two variants of the benchmark model under investigation. Section 5 offers concluding remarks.",Stabilizing Taylor rules and determinacy under unit root supply shocks: A re-examination,https://www.sciencedirect.com/science/article/pii/S0164070421000227,3 April 2021,2021,Research Article,131.0
"Huh Sungjun,Kim Insu","Department of Economics, College of Business Administration, Marquette University, David Straz Hall #414, P.O. Box 1881, Milwaukee, WI 53201, USA,Department of Economics, Jeonbuk National University, 567 Baekje-daero, Deokjin-gu, Jeonju-si, Jeollabuk-do 54896, Republic of Korea","Received 17 September 2020, Revised 21 March 2021, Accepted 23 March 2021, Available online 1 April 2021, Version of Record 7 April 2021.",https://doi.org/10.1016/j.jmacro.2021.103310,Cited by (2),"This paper investigates how real estate ==== lowers the profit of the firm that uses real estate as a factor of production, induces a decline in the ","Macroeconomic literature emphasizes the real estate market as an important source of business cycle fluctuations (e.g., Iacoviello and Neri, 2010, Liu et al., 2013). The interaction of the real estate market with the macroeconomy has received substantial attention since the financial crisis of 2007–2009. Disruptions in real estate prices significantly alter the household’s optimal choices, such as those for consumption, saving, and labor. In the finance literature, as pointed out by Piazzesi and Schneider (2016), real estate has two characteristics: a consumption good and an asset in household portfolios. Real estate is undoubtedly the largest share of household portfolios (Eiling et al., 2019). Despite this fact, it is surprising that real estate has received much less attention than stocks in the asset pricing literature.====The objective of this paper is to investigate how real estate affects the household’s attitude toward risk in a production economy when households are endowed with generalized recursive preference. To do so, we rigorously derive the closed-form expressions of risk aversion with period utility including real estate. Specifically, the assumption of additive separability between consumption and real estate is not imposed in our derivation, and therefore the expressions are generally applicable in many types of utility specifications.====We find three channels through which real estate affects the household’s attitude toward risk. First, compared to other assets, holding and trading real estate entail large transaction and repairing costs that are proportional to real estate prices.==== These costs are likely to reduce trading volume of real estate, making it illiquid. Fluctuations in real estate value also impose additional risk on real estate owners. Our closed-form expressions reveal that these risks increase the household’s relative risk aversion. We refer to this channel as the real estate risk channel. It predicts a positive relationship between relative risk aversion and real estate prices since both the size of the gamble and the costs increase with real estate prices. Second, households are able to absorb negative shocks to the economy by liquidating real estate wealth. The cushioning channel, in which real estate acts as a cushion to negative shocks, reduces relative risk aversion. This channel implies a negative relationship between relative risk aversion and real estate prices. Finally, the third channel is associated with competition between firms and households to purchase real estate, which consists of land or the buildings on it. As in Liu et al. (2013), our model implies that a rise in the share of real estate owned by households leads to a decline in the remaining share for firms. A rise in real estate demand by households drives up real estate prices, reducing firms’ real estate holdings. The profit of firms declines since real estate as a factor of production becomes expensive. A decline in profit lowers the real wage, consumption, and production. We find that this crowding-out channel, associated with general equilibrium effects, increases relative risk aversion as it generates consumption risk, and the impact on relative risk aversion rises with the curvature parameter of generalized recursive preferences. This channel thus predicts a positive link between relative risk aversion and real estate prices. We also find that the crowding-out channel disappears when households are characterized by expected utility preferences. It is worth emphasizing that these channels are not captured by the conventional measure of risk aversion by Arrow (1965) and Pratt (1964). Our measure of relative risk aversion implies that the conventional measure could be misleading if real estate is ignored.====In order to comprehensively understand the effects of real estate on relative risk aversion, we use a simple real business cycle model. In our baseline calibration, the crowding-out channel and the real estate risk channel are more important than the cushioning channel. Accordingly, the model predicts a positive relationship between real estate prices and relative risk aversion. Interestingly, risk aversion is still positively related to real estate prices even when households are endowed with expected utility preferences. This is because the real estate risk channel dominates the cushioning channel.====To explore the implications of real estate on risk aversion in light of asset pricing, we examine the effect of real estate on both the equity premium and the real estate risk premium. The real business cycle model with generalized recursive preferences implies that the household’s and the firm’s holding of real estate significantly affects not only relative risk aversion but also the risk premia on equity and real estate. Generalized recursive preferences are essential for the model to generate sizable risk premia.====Arrow (1965) and Pratt (1964) define the coefficient of relative risk aversion using a static model that abstracts labor and real estate. Swanson (2012) shows that a household’s attitude toward risk can be different when labor is included in period utility. Swanson (2018) extends his previous study with generalized recursive preferences. These papers show that labor adjustments to a negative income shock reduce relative risk aversion. A main difference between our paper and these studies is that our model incorporates real estate in period utility as well as labor. While Swanson, 2012, Swanson, 2018 studies the implications of labor adjustments on relative risk aversion, this paper studies how real estate, in its role as an asset, a consumption good, and as a production factor affects relative risk aversion. Zanetti (2014) derives the close-form expression of relative risk aversion when households receive utility from real estate. He finds that real estate wealth can reduce risk aversion due to the cushioning channel. This paper differs from Zanetti (2014) in at least four respects. First, Zanetti (2014) is necessarily silent about the remaining two channels since his model assumes that the real price of real estate is constant and households are characterized by expected utility preferences instead of generalized recursive preferences. Second, Zanetti (2014) treats real estate as liquid assets, while this paper does not. Third, Zanetti (2014) considers an endowment economy, while this paper incorporates a production economy. As shown in this paper, the production sector has a substantial impact on risk aversion and risk premiums. Finally, we analyze the impact of real estate on the equity and real estate risk premiums, whereas Zanetti (2014) does not study the implications of real estate on risk premiums.====Piazzesi et al. (2007) studies issues related to returns on stocks and housing using a consumption-based asset pricing model with housing and expected utility preferences. Their model shows that introducing housing into the model increases the equity premium and that the relative share of housing in the consumption basket helps forecast excess stock returns. Jaccard (2011) uses a production-based asset pricing model with habit formation and building restrictions to account for the equity premium and the housing risk premium. This paper is different from these papers in that our main objectives are to derive the coefficient of relative risk aversion and then to study how the firm’s and the household’s real estate holding influences relative risk aversion. Our model also differs from Piazzesi et al. (2007) and Jaccard (2011) in that our model assumes firms use real estate for production, firms and households face liquidity risk of real estate, and the household has generalized recursive preferences. We show that real estate as a factor of production has a substantial impact on relative risk aversion and the risk premiums. Illiquidity of real estate is also an important determinant of relative risk aversion and risk premiums.====The organization of the paper is as follows. Section 2 describes the household’s problem. Section 3 shows derivation of risk aversion with generalized recursive preferences. Section 4 studies how risk aversion, the equity premium, and the real estate premium are determined in a simple real business cycle model. Section 5 concludes.",Real estate and relative risk aversion with generalized recursive preferences,https://www.sciencedirect.com/science/article/pii/S0164070421000215,1 April 2021,2021,Research Article,132.0
"Hendrickson Joshua R.,Park Jaevin","Department of Economics, The University of Mississippi, United States,Department of Economics, Soongsil University, South Korea","Received 29 July 2020, Revised 13 March 2021, Accepted 17 March 2021, Available online 31 March 2021, Version of Record 8 April 2021.",https://doi.org/10.1016/j.jmacro.2021.103308,Cited by (2),"When large denomination bills are preferred in illegal activities, what is the optimal policy response? We construct a dual currency model where illegal activity can be reduced by modifying the payment environment. In our model, legal (goods) traders are indifferent between small and large bills, but illegal (goods) traders face a lower transaction cost of using large bills in comparison to small bills because it is easier to conceal. We show that eliminating large bills can reduce illegal trade and its associated social cost. However, this pooling equilibrium is sub-optimal because the government can collect more ==== by allowing illegal traders to use large bills with a lower rate of return. When the transaction cost of using small bills for illegal traders is sufficiently large, a separating equilibrium, where legal traders use small bills and illegal traders use large bills, can maximize welfare by making an implicit transfer from the illegal traders to the legal traders.","Despite the existence of alternative means of payment, including those that earn a higher rate of return, cash is still used in many transactions. One reason individuals might choose to use cash rather than other assets is that cash is free of record-keeping. For example, two parties to a transaction might prefer to exchange cash for goods and services to avoid the costs associated with third-party processing. Alternatively, individuals who are engaged in illegal activity might also prefer cash because of the lack of record-keeping involved.====If cash is used for illegal activity, and we want to discourage this activity, then a natural question is whether such activity can be reduced or eliminated by modifying the payment environment. Rogoff (2016) claims that large denomination bills are widely used in the underground economy such as the drug trade, bribes, crime, and money laundering. He argues that policymakers should eliminate large denomination bills in order to reduce illegal transactions. Some countries have recently ceased issuing large denomination bills.==== Williamson (2017) agrees that the gain from reforming the currency system could be significant, but argues that the elimination of a specific bill could result in a reduction of seigniorage to the central bank.==== The central bank can earn seigniorage by generating a strictly positive inflation rate. Moreover, raising the inflation rate reduces cash transactions as the rate of return on cash decreases. So eliminating cash or large denomination bills, instead of raising the inflation rate, may not be the optimal solution when we consider the benefit of seigniorage to the government or central bank. Therefore, even if this sort of demonetization policy is effective in reducing illegal activities, it is not obvious that this is the welfare-maximizing policy.====In this paper, we consider the role of seigniorage in determining whether eliminating large denomination bills can be the welfare-maximizing policy. For example, if the behavior of illegal traders creates a social cost, a transfer from the illegal traders to legal traders financed by a Pigouvian tax on illegal traders might be the most efficient solution. However, since those trading illegal goods have an incentive to keep their identity private, this would seem to imply that an explicit tax and transfer is infeasible. Nonetheless, if one could design a policy that would differentiate individuals by their types, then such a tax or a transfer could occur. One way to produce this type of policy would be to have a dual currency system in which the exchange rate between the currencies is determined by the market. Suppose that one type of currency is preferred by illegal traders. Policymakers could vary the rates of return between the two currencies such that illegal traders choose to hold one of the currencies whereas legal traders choose to hold the alternative. By doing so, policymakers are able to treat traders differently even under private information and can use this relative difference in the rates of return to generate a transfer via seigniorage from illegal traders to legal traders.====When it comes to illegal trade, however, differences in the choice of currency are often choices between different denominations of the same currency. For example, those engaged in illegal trade tend to prefer large denomination bills. As a result, the sort of policy we just described might be difficult to implement since the two currencies in question are actually just different denominations of the same currency. In other words, it is difficult to imagine that one would be able to allow the exchange rate to fluctuate between $10 bills and $100 bills, for example. Nonetheless, as we demonstrate in this paper, all that is required for this type of policy is to have different rates of return on small denominations of currency and large denominations of currency. In short, one could facilitate the sort of transfer described above by paying interest on small denominations, but not large denominations.====Since this type of policy engineers the sort of transfer by which the social cost can be paid by the creator, it might be preferable to eliminating the currency used by illegal traders. In other words, by eliminating large denomination bills, policymakers can reduce illegal activity, but this might come at the cost of losing the policy outlined in our example. It is important to consider whether eliminating large denomination bills is a desirable policy and whether designing policy to engineer a transfer from those engaged in illegal activity to those engaged in legal activity is the most preferable solution.====The purpose of this paper is to use a model, in which money is essential, to examine payment choices when some trades are illegal. To do so, we use a modified version of the monetary search model of Rocheteau and Wright (2005), in which ==== different individuals trade in the decentralized places. We assume that some fraction of agents engage in illegal trade, which generates a social cost. Since those who advocate eliminating large denomination bills argue that one of the primary benefits to doing so would be to reduce the social cost of illegal trade, our decision to simply assume that illegal trade produces a social cost is deliberate. Our objective is to accept the premise that illegal trade produces a social cost and then ask what type of policy would be optimal, given that premise.====The denomination structure of a currency system is constructed to trade efficiently by using various denomination bills together.==== However, since specific denominations are preferred in some types of transactions, we can also consider the various denomination bills as different types of payment methods.==== In this respect our model addresses the issue related to denomination size by considering issues of hiding illegal transactions. In the model, we assume that small denomination bills are more costly than large denomination bills for illegal traders to hide their transactions.==== Furthermore, we allow the government to adjust the rate of return on each denomination bill by providing a nominal interest on it.====Our model is therefore capable of answering an important policy question. Suppose that there are conditions in which people engaged in illegal trade prefer to use large denomination bills in equilibrium. What is the optimal policy response? Should we eliminate large denomination bills? Or is there a better solution?====The model produces a couple of important results. First, the model shows that eliminating large bills cannot maximize welfare as long as there is an additional transaction cost for illegal traders when using small bills. Second, if the transaction cost associated with using large bills for illegal traders is sufficiently small, welfare is maximized in a separating equilibrium in which illegal traders use large bills and legal traders use small bills. Since illegal trade creates a social cost, the government can generate seigniorage from the illegal traders by setting a low rate of return on large bills and providing implicit transfers to legal traders by setting a high rate of return on small bills.==== Then the amount of illegal trade is reduced while the amount of legal trade increases. This is a basic application of microeconomics in which activity that has a social cost above its private cost is taxed to produce the socially desirable outcome.====These results provide us with an implication for optimal monetary policy associated with the denomination structure when illegal activity is pervasive. When one type of denomination of bills is preferred in illegal transactions that create a negative externality, the monetary authority can internalize the social cost by reducing the rate of return on that denomination of currency as long as the other alternative bills are costly for these transactions.====Finally, our main result requires implementing a separating equilibrium with different rates of return on small and large bills. The government can implement it by providing strictly positive interest on small denomination bills or allowing the exchange rate between the small and large denomination bills to fluctuate over time. It is important to note that paying interest on small denomination bills is not without precedent. An example is the patacón, which is a small denomination bond issued by a local government in Argentina in 2001 that circulates in the economy at the same time as pesos and pays an interest annually.",The case against eliminating large denomination bills,https://www.sciencedirect.com/science/article/pii/S0164070421000203,31 March 2021,2021,Research Article,133.0
Darougheh Saman,"Danmarks Nationalbank, Denmark","Received 24 September 2020, Revised 9 February 2021, Accepted 16 February 2021, Available online 17 March 2021, Version of Record 21 April 2021.",https://doi.org/10.1016/j.jmacro.2021.103302,Cited by (5),"I process credit-card consumption data through an input–output model of sectoral linkages to impute the sector-level output responses to the Covid-19 pandemic. The sector-level consumption responses are highly dispersed and even positive for some. Yet, all sectors suffer from output losses. Production of ==== stabilises output. Consequently, the sectoral dispersion of final consumption is higher than the sectoral dispersion of output produced. Sectors that provide intermediate goods are affected less by the pandemic. Many service sectors face the largest losses in output since they depend the most on final consumption.","Several authors have reported changes in consumer spending, mostly using transaction data similar to that used here. Measuring these changes in household consumption is useful for policymakers to understand how interventions on the household-side would transmit to the economy. There, it is important to understand how much of a stimulus cheque would be spent by households, and what types of goods they would spend it on. A second type of government interventions are on the business side. It is vital to know which sectors are affected most by the pandemic to guide ==== or liquidity provisions. Yet, it is not sufficient to observe the households’ sector-specific consumption response to understand how severely a sector is affected. This is because government demand and business-to-business relationships form an important part of total demand for many businesses.==== illustrates this point. I have chosen three sectors that I will be measuring in my empirical exercise, and have plotted the supply relationships between the various sectors. In this chart, final demand is split into government and household demand. All arrows illustrate a supply relationship and are proportional in size to the importance of the relationship to the originating sector: the more important a purchaser is for a sector, the larger the arrow. For example, health services depend heavily on household demand. Business services depend more on government than on household demand. Relationships that comprise less than ==== per cent of the originating sector’s total demand are not illustrated for clarity. The colours of the arrows illustrate whether the purchasers increased or decreased their demand in response to the pandemic. Importantly, households decreased their demand for both types of services, and increased their demand for retail trade.====Judging purely from the households’ response, this would imply that both service sectors are severely hit, while trade is actually profiting from the recession. However, there are two additional factors at play. First, government demand presumably remained stable, stabilising the demand response to the business services sector. Second, sectors that were doing relatively well – in this example, Retail trade – used goods from other sectors as inputs. The business services sector provides some services that are being used in retail trade, which also stabilised its output in the recent period. The health services sector does not provide many services to other sectors, and hence was much more affected by the households’ change in consumption pattern.====In this paper, I will use the sectoral supply chain to estimate how a change in household consumption patterns transmits through the network. This provides us with a first estimate on how each sector was affected. For this purpose, we need to build a simple model that allows us to integrate the supply chain into our analysis. I introduce this model in the next section.",Dispersed consumption versus compressed output: Assessing the sectoral effects of a pandemic,https://www.sciencedirect.com/science/article/pii/S016407042100015X,17 March 2021,2021,Research Article,134.0
"Cavusoglu Nevin,Goldberg Michael D.,Stillwagon Josh","Department of Economics, James Madison University, Harrisonburg, VA 22807, United States of America,Department of Economics, University of New Hampshire, Durham, NH 03820, United States of America,Economics Division, Babson College, Babson Park, MA 02457, United States of America,Institute for New Economic Thinking (INET) Program on Imperfect Knowledge Economics (IKE), United States","Received 16 September 2020, Revised 21 January 2021, Accepted 16 February 2021, Available online 9 March 2021, Version of Record 22 March 2021.",https://doi.org/10.1016/j.jmacro.2021.103304,Cited by (0),"The paper considers competing portfolio-balance specifications of currency returns, including one based on expected utility theory and another on prospect theory. The prospect theory specification relates downside risk to the gap between the exchange rate and its benchmark value. The empirical analysis uses survey data on exchange rate expectations to test directly the models’ predictions concerning ","The difficulty of conventional risk premium models to account for excess returns in asset markets is well known.==== Studies find that expected returns are much too volatile to be explained with a plausible degree of risk aversion. Researchers have considered alternative preference specifications to increase the predicted volatility, but with results that are mixed at best. In addition, Mark and Wu (1998) show that the consumption capital asset pricing model (CAPM) has difficulty along another dimension in currency markets: it is grossly inconsistent with the tendency of excess returns to undergo sign reversals. Early evidence on the older portfolio balance models also shows an inability to explain volatility and sign reversals in currency markets.====In this paper, we re-examine the empirical performance of several portfolio balance models of currency returns. Interest in these models has witnessed a notable resurgence. For example, Obstfeld (2004) appeals to portfolio balance models in understanding external adjustment. Blanchard et al. (2005) use a portfolio balance model to explain current account and exchange rate movements and remark that these models have been “unjustly forgotten”. More recently, researchers have found that the risk factors of portfolio balance models have explanatory power. Menkhoff et al. (2012) report that exchange rate volatility risk accounts for the cross section of carry trade returns in currency markets.====
 Gourinchas and Tornell (2004) and Della Corte et al. (2016) find that the cyclical components of net foreign assets and net exports can predict exchange rate returns. Della Corte et al. (2016) report that countries’ international financial positions help explain the cross section of currency excess returns, implying that debtor countries are characterized by currency risk premia.====Our investigation differs from other studies in several significant ways. First, we test several competing portfolio balance models, rather than just one model or one risk factor. We consider international CAPMs (ICAPM) due to Dornbusch (1983) and Frydman and Goldberg, 2007, Frydman and Goldberg, 2013). The latter model replaces expected utility theory (EUT) with Kahneman and Tversky (1979) prospect theory (PT). We also consider a hybrid model that combines the conventional and PT specifications. Researchers have found that alternatives to expected utility theory improve the consumption CAPM’s empirical performance.==== Such alternatives may also help portfolio balance models. The prospect theory specification is particularly promising because it offers a way to model downside or crash risk, which researchers find is important for explaining carry trade returns.====The competing portfolio balance models imply different risk factors. All the models relate the risk premium on foreign currency to the country’s bilateral international debt position (IDP). With expected utility theory, the risk premium also depends on the conditional volatility of returns. But, with prospect theory, the premium depends on downside risk, which the model relates positively to the gap between the exchange rate and market participants’ assessments of its benchmark value. The “gap effect” is intuitive: the more over- or undervalued a currency becomes, the riskier it is for market participants who speculate on a further over- or undervaluation. We find strong support for this prediction.====The models’ predictions for sign reversals also differ. Both models relate the risk premium’s sign to the sign of the international debt position. But, with the prospect theory model, the sign of the risk premium also depends on the risk assessments of the bulls (who take long positions in foreign exchange) relative to those of the bears (who take short positions). This additional factor gives the model greater potential to explain sign reversals over the conventional ICAPM.====Another key distinction of our study is that we estimate models of ==== rather than ==== currency returns using survey data on exchange rate expectations. Other risk premium studies estimate models of ==== returns, drawing inference under the rational expectations hypothesis (REH). Inference in these studies, therefore, involves joint tests of the models’ predictions concerning expected excess returns and REH’s prediction that ==== and ==== outcomes differ by white noise errors. There is considerable evidence against REH’s white-noise-error prediction, suggesting that the negative results of earlier studies may arise in part from a failure of REH.==== Indeed, Chinn and Frankel (2016) speculate that the “stylized fact that the exchange risk premium is unrelated to macroeconomic variables is in fact an artifact of the questionable methodology of rational expectations.” Our use of ==== returns enables us to test directly the competing implications of the portfolio balance models without the joint hypothesis problem.====There is considerable evidence that currency excess returns are highly persistent and possibly nonstationary.====
 Frydman and Goldberg (2007) address this problem with single-equation error correction models. In this paper, we rely on the ==== cointegrated vector autoregression (CVAR) framework (Johansen, 1996, Juselius, 2006), which is better suited for handling persistent variables.==== We find that expected excess returns and other variables in the information set are best characterized with unit roots. A familiar critique of cointegration analysis is that it has low power in limited samples to reject the unit-root null against stationary, but slow mean-reverting, near-unit root alternatives. This problem does not arise in our samples, however. We find quick error-correction to the equilibrium relationships that are implied by the portfolio-balance models. Our over-identifying restrictions are thus able to establish stationary relationships at high significance levels.====We also extend earlier studies by including measures of exchange rate volatility and bilateral debt in our information set.==== This enables us to consider all of the predictions of the conventional and prospect theory ICAPMs, and those of a hybrid model. This is the first paper to compare or combine the two specifications of risk. We test the models’ competing predictions with alternative over-identifying, long-run restrictions on the VAR.====To preview our results, we find little support for the conventional ICAPM. We reject the model’s main prediction – that the expected excess return moves positively with conditional volatility and IDP – in two of the three currency markets examined. By contrast, we find a positive gap effect in all three currency markets, as predicted by the PT model.==== In two of these markets, IDP enters the cointegrating relationship as predicted. We also find that the PT model accounts for sign reversals better than the conventional ICAPM. Interestingly, the CVAR results show the strongest evidence for the hybrid model in which the gap, conditional volatility, and IDP drive expected excess returns.====The remainder of the paper is structured as follows. Section 2 reviews the competing predictions of the portfolio models. Section 3 discusses the information set and data selections, whereas Section 4 outlines the CVAR restrictions that are implied by the risk premium models. Section 5 presents the empirical results. We offer concluding remarks in Section 6.","Currency returns and downside risk: Debt, volatility, and the gap from benchmark values",https://www.sciencedirect.com/science/article/pii/S0164070421000161,9 March 2021,2021,Research Article,135.0
"Nam Eun-Young,Lee Kiryoung,Jeon Yoontae","Sejong University, Department of Business Administration, 209, Neungdong-ro, Gwangjin-gu, Seoul, Republic of Korea,Cheongju University, 298, Daeseong-ro, Cheongwon-gu, Cheongju-si, Chungcheongbuk-do, Republic of Korea,Ted Rogers School of Management, Ryerson University, 350 Victoria Street, Toronto, Ontario, Canada, M5B 2K3","Received 6 September 2020, Revised 20 February 2021, Accepted 22 February 2021, Available online 2 March 2021, Version of Record 5 March 2021.",https://doi.org/10.1016/j.jmacro.2021.103306,Cited by (6), model.,"Time-varying macroeconomic uncertainty is one of the main variables affecting the economic decision makings of firms and individuals in modern economics and finance theories. (e.g., Bansal and Yaron, 2004, Bloom, 2009, Bhamra et al., 2010, Chen, 2010, Bansal et al., 2014, Bloom et al., 2018). Recent studies have developed indices of macroeconomic uncertainty (e.g., Jurado et al., 2015; Baker et al., 2016; Bekaert et al., 2020) that offer benchmark indices for evaluating economic theories where uncertainty plays a significant role. A large body of following studies shows the significance of these indices for various economic outcomes. This includes the study at the macro-level (e.g., Jurado et al., 2015; Baker et al., 2016; Bekaert et al., 2020), firm-level (e.g., Jens, 2016, Colak et al., 2017, Nguyen and Phan, 2017, Bonaime et al., 2018, Atanassov et al., 2019), and also household-level (e.g., Gábor-Tóth and Georgarakos, 2019).====Modern macroeconomic models emphasize the importance of an individual’s consumption smoothing as a tool to maximize social welfare. As a result, it is important to empirically analyze economic variables that influence household consumption levels, which directly affect optimal consumption smoothing. However, despite the importance of understanding variables related to household consumption, there is no empirical evidence on the relation between household-level consumption choices and recently developed economic uncertainty indices. We fill this gap in the literature and ask the following question: How do different macroeconomic uncertainty indices affect households’ consumption choices, and which index is the most relevant? Answers to these questions are important for the following reasons. First, from the policymakers’ perspective, stabilizing consumption growth is one of the most important objectives. By identifying the most important index, policymakers are better able to monitor the type of uncertainty that is relevant. Second, we expand our understanding of the economic channel in which macroeconomic uncertainty affects economic decision makings.====Why would the economic uncertainty indices be relevant for the consumption choice of individual households? Theoretical models imply that in response to an increase in uncertainty about variables that impact future consumption, economic agents should take on prudent behaviors such as increasing precautionary savings, lowering consumption, and risky investment (e.g., Gollier and Pratt, 1996, Zeldes, 1989, Kimball, 1990, Carroll, 1997, Bertola et al., 2005). Therefore, to the extent that economic uncertainty indices capture uncertainty on consumption faced by individuals, we expect a negative impact of economic uncertainty indices on consumption. There are two potential economic channels through which economic uncertainty indices reflect important information about individuals’ consumption uncertainty. First, an individual’s labor income uncertainty is one important source of consumption uncertainty (e.g., Guiso et al., 1992, Feigenbaum and Li, 2015, Wang et al., 2016), which could be largely explained by economic uncertainty indices. Second, for stockholders, financial uncertainty could mainly drive their consumption uncertainty (e.g., Mankiw and Zeldes, 1991, Attanasio et al., 2002). Thus, economic uncertainty indices based on financial markets could play a role in explaining stockholders’ consumption uncertainty. Motivated by these, we hypothesize that shocks to economic uncertainty indices depress households’ future consumption expenditure.====A novelty of our study is to consider widely used multiple uncertainty indices in our unified framework, instead of arbitrarily choosing a few indices. This is important because it is not clear which uncertainty index is more important for households’ consumption choice than others. If the uncertainty about future economic policy is closely related to households’ consumption uncertainty, the economic policy uncertainty index may have a significant effect. Alternatively, financial uncertainty could be important as well for overall households because financial variables contain important forward-looking information that reflects future economic uncertainty. Therefore, in our study, we consider commonly used multiple economic uncertainty indices in the literature in examining their explanatory power for households’ consumption decisions.====Specifically, we consider the following widely used economic uncertainty indices: financial, real, and macro uncertainty index by Jurado et al. (2015) (JLN, hereafter),==== economic policy uncertainty (EPU) index by Baker et al. (2016) (BBD , hereafter), and economic uncertainty index by Bekaert et al. (2020) (BEX, hereafter). We also consider the uncertainties of the financial market (VIX and realized variance of S&P 500 index). To observe households’ consumption choice, we exploit micro-level survey data: the Consumer Expenditure Survey (CE) from October 1986 to February 2019. We collect 455,991 household-month observations of detailed consumption information as well as demographics and other financial information. We adopt a panel regression setting where unobserved individual heterogeneity, as well as a wide set of time-varying household characteristics, are controlled to mitigate endogeneity concern.====We first examine the cross-sectional variation in different economic uncertainty indices. To this end, we compute the cross-correlation of economic uncertainty indices. We find that recently developed uncertainty indices show a quite independent variation. For instance, the correlation coefficient between changes in economic policy uncertainty of BBD and changes in the real uncertainty index of JLN is only 0.146. This finding indicates that each index captures different types of economic uncertainty, which highlights the importance of considering multiple indices as in our empirical setting.====Next, we test whether shocks to economic uncertainty indices depress future household consumption, consistent with the theory. We run a predictive panel regression of consumption growth on one month lagged shocks to economic uncertainty index with households fixed effects and household-level controls. In doing so, to avoid potential multicollinearity problems, we use one uncertainty index at a time. First, in terms of the sign, we find that all uncertainty indices deliver the theoretically consistent negative sign. Second, when it comes to the statistical significance, we find that changes in financial, real, and macro uncertainty indices by JLN and VIX have a statistically significant negative relationship with households’ consumption. One standard deviation increase in financial uncertainty by JLN is associated with 0.73% points decrease in consumption growth, which compares to 0.65% points for macro uncertainty by JLN and VIX, and 0.50% points for real uncertainty by JLN. However, EPU measures, BEX, and realized variance are not significant. This result suggests that some of the recently developed uncertainty indices are related to households’ consumption decisions in a theoretically consistent way. Furthermore, our result shows that not all uncertainty indices are statistically relevant to households’ consumption choice, which confirms the importance of considering different types of economic uncertainty given different information each index carries.====To investigate the economic mechanism behind the negative impact of economic uncertainty indices on consumption, we further study the cross-sectional variation in the effect of the uncertainty. We find that overall, households with high consumption volatility more sensitively react to shocks to uncertainty indices. This finding provides supporting evidence that households adjust their consumption faced with economic uncertainty shocks because economic uncertainty could translate into households’ consumption uncertainty. Therefore, households that are highly uncertain about their future consumption are more likely to care about aggregate economic uncertainty.====In order to examine whether uncertainty indices have a long-lasting impact on consumption choice, we also study the impact of uncertainty on longer horizon future consumption up to six months. Our test shows that the real and macro uncertainty index by JLN is significantly associated with up to five months ahead of consumption, whereas financial uncertainty by JLN is significant for a one-month horizon and VIX is significant up to two months’ horizon. This implies that real and macro indices by JLN, in particular, have longer-lasting negative effects on household consumption than other indices. This finding can be rationalized by the habit-formation model (e.g., Campbell and Cochrane, 1999) where unwelcome news about an economic condition affects agents’ behaviors in a persistent way due to time-varying risk preferences.====Finally, we conduct the sub-sample analysis by splitting our entire sample into two sub-periods: 1986–1999 and 2000–2019. We find that finance, real, and macro uncertainty indices by JLN are more significant in the later period which includes the dot-com bubble and the Great Recession. For VIX, it is not significant for both sub-periods.====Our paper belongs to the literature that studies the impact of broad uncertainty on consumption (e.g., Bertola et al., 2005, Mian et al., 2013, Aaberge et al., 2017, Ben-David et al., 2018, Mian et al., 2018, Kalcheva et al., 2019). We contribute to the literature in the following ways. First, to the best of our knowledge, this is the first paper that uses recently developed various economic indices (e.g., Jurado et al., 2015; Baker et al., 2016; Bekaert et al., 2020) to study whether and how U.S. households adjust their consumption in response to uncertainty shocks. Despite the economic importance, there has not been an attempt to test how households respond to increased economic uncertainty using widely available indices. Our test provides the first evidence that shocks to recently developed uncertainty affect households’ consumption negatively and also it is consumption uncertainty concern that leads to this relationship.====Second, in evaluating the relationship between economic uncertainty and households’ consumption, our study provides a comprehensive examination by considering multiple indices without making an assumption on which index well captures the degree of economic uncertainty. This is important because there is no existing study that shows which type of uncertainty is particularly important for households’ consumption choices out of macroeconomic, financial, and economic policy uncertainty. Our study provides novel empirical evidence that financial, real, and macro uncertainty indices by Jurado et al. (2015) and VIX are the most important indices that drive households’ consumption choice.====Our work is related to Kalcheva et al. (2019). However, our study is different from their study, in that instead of studying the EPU index only, we consider multiple economic indices and test which index is more important than others. Moreover, while Kalcheva et al. (2019) investigate a particular type of consumption, we study broad nondurable and services consumption.====This paper proceeds as follows. Section 2 describes the data and variables used in our empirical analyses. Section 3 discusses our empirical specification and methodology. Section 4 presents the empirical results. Section 5 concludes the paper.",Macroeconomic uncertainty shocks and households’ consumption choice,https://www.sciencedirect.com/science/article/pii/S0164070421000185,2 March 2021,2021,Research Article,136.0
"Ilabaca Francisco,Milani Fabio","University of California, Irvine, United States of America,Office of Financial Research, United States of America","Received 20 March 2020, Revised 30 December 2020, Accepted 22 February 2021, Available online 2 March 2021, Version of Record 8 March 2021.",https://doi.org/10.1016/j.jmacro.2021.103307,Cited by (4),".====The model is estimated with ==== techniques, using rolling windows and allowing the parameters to fall both in the determinacy and indeterminacy regions. The estimates reveal large shares of agents who depart from rational expectations. Heterogeneous expectations are decisively preferred by the data everywhere in the sample.====Finally, the paper revisits the narrative that sees postwar US macroeconomic data as consistent with indeterminacy in the pre-1979 sample, with a switch to determinacy starting in the early 1980s, and it shows that it is overall robust to inclusion of heterogeneous expectations.","A generally accepted account of US postwar macroeconomic history views output and inflation fluctuations in the 1960–1970s as the outcome of passive monetary policy and indeterminacy, followed by a shift to active monetary policy after 1979, which induced stability in the macroeconomic environment by ensuring a determinate equilibrium.====Empirical evidence in favor of this narrative is provided, for example, in the influential papers by Clarida et al. (2000), and Lubik and Schorfheide (2004). Their works, as well as the related literature, use New Keynesian frameworks that particularly highlight the role of monetary policy in managing private-sector expectations. A weak monetary policy response may cause inflation and output expectations to become unhinged, steering the economy away from the rational expectations equilibrium. Expectations may coordinate, instead, around a more volatile equilibrium, where realizations of a sunspot variable affect agents’ beliefs and, ultimately, the dynamics of macroeconomic variables.====The results on the transition from indeterminacy to determinacy are, however, obtained in models that impose two strong expectational assumptions: first, that expectations are formed according to the rational expectations hypothesis, and, second, that expectations are homogeneous. All households and firms in the model are assumed to form identical expectations conditioned on the same, correct, model of the economy.====However, recent studies have provided growing evidence in favor of heterogeneous expectations. Branch (2004) uses individual-level data on inflation expectations from the Michigan Survey of Consumers to show that different shares of consumers form expectations based on alternative forecasting models, with different levels of complexity. Mankiw et al. (2004) and Branch (2014) document a significant extent of disagreement among professional forecasters. The experimental evidence has also strongly stressed the importance of heterogeneous expectations: Hommes, 2011, Hommes, 2013 show that expectations cluster around different groups, with some characterized by adaptive, and others by trend-following, behavior.====Branch and McGough (2009) introduce heterogeneous expectations in a New Keynesian model and analyze their microfoundations. The model retains a share of agents who form conventional rational expectations, but they allow for a share of agents whose expectations deviate from rational (RE) and can be adaptive, naïve, or extrapolative. As they stress, the inclusion of heterogeneity alters the determinacy properties of the system, and may be stabilizing or destabilizing depending on the nature of the expectations. If expectations have an adaptive component, that is they respond less than one-to-one with respect to the lagged value of the endogenous variable, a larger share of non-RE agents will be stabilizing. The determinacy region is expanded, and the equilibrium can be determinate even with weak responses to inflation. On the other hand, if expectations are extrapolative, by responding more than one-to-one to past values, indeterminacy will be more likely. For large enough shares of non-RE agents, conventional active monetary policies become powerless in trying to steer the economy toward a stable equilibrium. Hence, the Taylor principle is neither necessary nor sufficient for equilibrium uniqueness. As a consequence, it is possible that determinacy exists in the 1970s in conjunction with passive monetary policy, or that indeterminacy continues to provide a better explanation even of the post-Volcker data, as long as beliefs are extrapolative. The details of expectations, overall, may be more important than monetary policy for determinacy of the equilibrium. The traditional narrative of postwar shifts from indeterminacy to determinacy, therefore, may not be robust to expectational heterogeneity.====In this paper, we estimate the New Keynesian model with heterogeneous expectations and allow for both determinacy and indeterminacy. We use the techniques developed in Bianchi and Nicoló (2019) to solve the model and obtain the likelihood under indeterminacy. We also recognize that monetary policy coefficients and the formation of expectations may have changed over time. Therefore, we adopt a rolling-window estimation from 1954 to the end of the sample. We use different window sizes: in the benchmark estimation, our rolling window includes twenty years of observations, but we also repeat the estimation with a ten-year window. The model is re-estimated every year, and we calculate how the probability of determinacy versus indeterminacy varies over time. We use a rolling sample because, while there may be evidence of a structural break for monetary policy coefficients in correspondence of the appointment of Paul Volcker as the Federal Reserve’s Chairman in 1979, we cannot realistically assume a one-time structural break in the expectation parameters, or in other structural coefficients.==== The estimation results strongly indicate that heterogeneous expectations provide a better fit of the data than homogeneous expectations do, as measured by the marginal likelihoods. Significantly, heterogeneous expectations outperform homogeneous expectations decisively in every rolling sample from 1954 to 2007. We estimate large shares of boundedly rational agents: at points in the sample, they reach up to 80% for output expectations and more than 60% for inflation expectations.====We document ample evidence of time-variation in the structural and expectation parameters. Monetary policy is passive toward inflation in the earlier samples, as expected, and it becomes more reactive to the output gap for windows that are centered around the 1970s. The shares of boundedly rational agents vary over time. They are low in the earlier windows and for some of the samples that begin in the second half of the 1970s, but, in general, they indicate that the vast majority of agents do not conform with rational expectations. We also document that expectations about the output gap display backward-looking coefficients close to one over most of the sample. In contrast, inflation expectations are typically formed adaptively, with coefficients ranging from below 0.5 to 0.7, but with the exception of windows that start in the late 1970s, when the coefficients rise above one and expectations display extrapolative or trend-chasing behavior.====Our approach allows us to test whether the conventional narrative about monetary policy and indeterminacy, based on Clarida et al. (2000) and Lubik and Schorfheide (2004), is robust to more realistic assumptions about expectations. The conventional story regarding post-war switches from indeterminacy to determinacy is largely confirmed, and is, therefore, lent additional credibility. The early years in our sample are best explained as coming from a data-generating process characterized by passive monetary policy and multiple equilibria. For samples starting after the early 1980s, there is a definite shift to determinacy. But there are other periods in which the Taylor principle is usually considered as satisfied in the literature, but where we find expectations that become extrapolative. In these periods, the probability of indeterminacy rises considerably, up to 70%. These additional episodes of potential indeterminacy are usually absent in estimations under homogeneous expectations. Under indeterminacy, the economy is susceptible to the action of sunspots. Based on our estimates, sunspot shocks typically do not explain a large share of fluctuations in output and inflation, but they can account for up to 20% in some episodes. Sunspots also account for a similar share of the variability in inflation in those samples where expectations become extrapolative.==== This paper aims to contribute to multiple literatures. More directly, it builds empirical evidence in favor of modeling heterogeneous expectations in the New Keynesian model. The microfoundations of heterogeneous expectations were developed in Branch and McGough (2009), who also document the conditions for equilibrium determinacy and show that heterogeneity can expand or shrink the region of determinacy depending on the nature of non-RE expectations. Massaro (2013) adds heterogeneous expectations in the microfoundations of a New Keynesian model with long-horizon expectations instead. Gasteiger (2017) develops alternative microfoundations in a stochastic version of the New Keynesian model. Beqiraj et al. (2018) estimate shares of agents that depart from rational expectations ranging from 13% to 46%; they do not consider the implications of heterogeneous expectations for indeterminacy in the estimation, and do not allow for time variation in the parameters. Our empirical estimates suggest that non-RE shares are even larger. Elias (2021) estimates a New Keynesian model with heterogeneous expectations, where different shares of agents use correctly-specified or misspecified adaptive learning models. He finds large shares of agents that form expectations from the more parsimonious, misspecified, models, which could be suggestive of cognitive or computational limitations on their part. Other papers have studied the implications of heterogeneous expectations on the amplification of technology shocks on output (Branch and McGough, 2011), optimal monetary policy (Gasteiger, 2014, Di Bartolomeo et al., 2016, Beqiraj et al., 2019), and monetary–fiscal policy interactions (Gasteiger, 2018). While we provide evidence on the importance of expectation heterogeneity at the macro level, other papers have documented its importance on micro data (Branch, 2004, Branch, 2014, Pesaran and Weale, 2006, Dovern et al., 2012, Cole and Milani, 2020), or in laboratory experiments (Hommes, 2011, Hommes, 2013). Cole and Milani (2019) use a DSGE-VAR approach and reveal that a major misspecification in the New Keynesian model lies in the way expectations are modeled: when heterogeneous expectations, chosen to mirror lab evidence, replace homogeneous rational expectations, the data become more supportive of DSGE restrictions. Calvert Jump and Levine (2019) show that the model with heterogeneous expectations can account for some empirical regularities (excess kurtosis, stochastic volatility, and departures from rational expectations) that cannot be explained by the benchmark New Keynesian model.====The paper then adds to the literature on changes in monetary policy and indeterminacy to explain postwar US economic history. The paper generally confirms the shift from indeterminacy to determinacy over the sample found in Clarida et al. (2000) and Lubik and Schorfheide (2004), but it uncovers additional periods when indeterminacy may arise because of extrapolative expectations. Other papers relax rational expectations or add behavioral elements to revisit the empirical evidence in favor of indeterminacy, but they impose homogeneous expectations (Milani, 2008, Ilabaca et al., 2020).====Benhabib and Farmer (1999) and Farmer (2019) review the broader literature on indeterminacy in macroeconomics. Empirical studies have been few in the past due to difficulties in solving and estimating models with multiple equilibria. Lubik and Schorfheide (2004), Farmer et al. (2015), and Bianchi and Nicoló (2019) provide key methodological contributions that make it easier to take these models to the data. Our paper adds to the literature by revisiting the empirical evidence for indeterminacy versus determinacy in a model that studies the interaction between monetary policy and expectations, by allowing for heterogeneous expectations and parameter variation.====Furthermore, the paper is connected to the literature that deals with parameter instability in DSGE models. Fernández-Villaverde and Rubio-Ramírez (2007) estimate a model with time-varying parameters to judge whether parameters can be interpreted as ‘structural’. Canova (2009), Canova and Ferroni (2012), and Castelnuovo (2012), use a closer approach to ours, by employing a rolling window estimation. Compared to models with Markov-Switching, time-varying parameters, or stochastic volatilities, a rolling Bayesian estimation allows us to consider time variation in all parameters at the same time in a computationally tractable way (the other approaches typically only allow a minor subset of parameters to shift, while forcing others to remain constant). It is particularly suitable for a situation in which there is uncertainty over which parameters are subject to instability, and what form that instability takes. Our rolling-window approach does not require the specification of evolution equations for each parameter that is assumed to be time-varying, and it does not require the estimation of its corresponding autoregressive and variance coefficients. Moreover, it allows us to retain the simplicity of estimating a model in linearized form, rather than dealing with nonlinearities and higher-order approximations. The previous papers still impose determinacy and work with models with homogeneous expectations, while we emphasize parameter variation related to heterogeneous expectations, and we show how parameters may fall in regions that are conducive to indeterminacy.====Finally, the paper is more broadly connected to the literatures that model deviations from rational expectations, such as the literature on adaptive learning in macroeconomics (e.g., Evans and Honkapohja, 2001). We do not directly model agents’ learning here, but we show how the parameters related to expectations change over time through our rolling window estimation. Both under adaptive learning and under our framework, expectations have backward-looking components that can induce additional persistence in the economy, amplify the effects of shocks, and improve the fit of the model to the data. Under learning, agents can adopt a perceived law of motion to form expectations that includes the same variables that would appear in the solution under rational expectations (a correctly-specified model). But they would not know the reduced-form parameters of the rational expectations solution and, therefore, they would attempt to learn them over time using the most updated sample of data they have available. An alternative would be to assume that agents use underspecified models, which include only some of the relevant variables. As discussed before, Elias (2021) estimates a model that merges these approaches. Our heterogeneous expectations framework assumes, instead, that agents form backward-looking expectations based on the lagged value of the variable they are trying to forecast. They correspond, therefore, to a stronger departure from rationality.","Heterogeneous expectations, indeterminacy, and postwar US business cycles",https://www.sciencedirect.com/science/article/pii/S0164070421000197,2 March 2021,2021,Research Article,137.0
"Gross Jonas,Zahner Johannes","University of Bayreuth, Universitätsstraße 30, 95447 Bayreuth, Germany,Marburg Centre for Institutional Economics (MACIE), Philipps-University Marburg, Germany","Received 16 August 2020, Revised 19 January 2021, Accepted 1 February 2021, Available online 11 February 2021, Version of Record 1 March 2021.",https://doi.org/10.1016/j.jmacro.2021.103292,Cited by (6),This paper analyzes the ,"However, there are various reasons why applying a standard Taylor rule yields misleading policy implications. First, it seems far-fetched to restrict decision-makers to a monetary policy rule that captures sufficient information about the real economy in only two variables. Vítor Constâncio, former vice president of the ECB, argues that ====Third, in reality, central bankers might have more than one model in mind of how the economy functions. The interest rate setting body of the ECB, the Governing Council, consists of the presidents of the euro area national central banks and the Executive Board. These central bankers have different backgrounds that might affect their attitudes towards relevance of certain variables such as inflation and economic activity and the importance of indicators such as bond yields to affect those variables. Although we remain agnostic about the source of heterogeneity, central bankers differ with respect to their social, political, and academic backgrounds. However, aside from potential heterogeneity between the ==== members of the Governing Council, it can further be assumed that heterogeneity exists within ==== central banker, i.e., that more than one reaction function is in her mind. Such heterogeneity leads to different concepts about the transmission of shocks and the interaction of economic agents that might yield deviating policy implications and interest rate recommendations. Therefore, we argue that the standard Taylor rule should be extended to draw more precise inferences for monetary policy.====We contribute to the current monetary policy literature by shedding light on the ECB’s monetary policy decisions and analyze the potential shift of priorities due to the global financial crisis in 2008. Our analysis focuses on the following two key factors: Firstly, uncertainty about the form of the central bankers’ reaction functions, and secondly, uncertainty about the magnitude of the coefficients included in the specific reaction functions. We base our analysis on real-time data and insights derived from textual analysis of ECB press conference statements.====Employing an empirical Bayesian model averaging (BMA) approach allows us to consider variables besides the ones included in the classical Taylor rule, and to evaluate ==== model combinations of potential monetary policy determinants. We consider a variety of variables and evaluate all model combinations with respect to the observed data to determine the most likely models. Thereby, we derive the ECB’s most likely interest rate determinants. Applying BMA in the context of the ECB’s monetary policy is – to the best of our knowledge – a novel approach and addresses a gap in the current literature.====Our key findings are as follows: First, the ECB focuses its decisions mainly on the inflation rate measured by the Harmonized Index of Consumer Prices (HICP). Second, economic activity seems to be a key priority for the central bank before the financial crisis. Third, our results suggest that the importance of economic activity for the ECB’s monetary policy decisions decreased over the last decade. Fourth, when setting interest rates, central bankers from the ECB tend to consider more than one model. This finding supports the necessity to use model averaging techniques in order to take model uncertainties in the context of monetary policy into account.====Our paper is structured as follows: In section two, we identify the variables potentially influencing the ECB’s monetary policy by a literature review and a textual analysis of ECB communication. Furthermore, we motivate why one should consider model uncertainty in the context of monetary policy. In section three, the BMA approach is discussed. In section four the data used is discussed. In sections five and six, the estimation results are explained in detail, and robustness checks are conducted. The last section concludes the paper.",What is on the ECB’s mind? Monetary policy before and after the global financial crisis,https://www.sciencedirect.com/science/article/pii/S0164070421000057,11 February 2021,2021,Research Article,138.0
Walheer Barnabé,"HEC Liège, Université de Liège, Belgium","Received 27 July 2020, Revised 21 January 2021, Accepted 28 January 2021, Available online 3 February 2021, Version of Record 6 February 2021.",https://doi.org/10.1016/j.jmacro.2021.103290,Cited by (10),"This paper investigates the role of technology club heterogeneity in economic growth and convergence. To do so, we break up labor productivity change into three factors – efficiency, technological, and capital–labor ratio changes – while distinguishing the impact of technology club heterogeneity respectively. This allows us to observe what is happening within and between clubs; as well as between the world and club technologies. Our labor productivity decomposition is nonparametric in nature and thus overcomes the issue of specifying functional forms for the club technologies. Our results reveal the existence of technology heterogeneity and divergence: the world technology is defined by advanced and rich countries; there exists intra-convergence phenomena (mostly due to capital–labor ratio change), but inter-convergences (owning to capital–labor ratio and technological changes) are not found. Finally, we argue that follower and marginalized countries have adopted imitating strategies, but with respect to different dimensions, namely technological change or capital–labor ratio.","There has been considerable empirical research on countries’ economic growth with the view of answering two questions: first, whether there is a tendency for their respective growth to converge over time, and second, how the sources of economic growth contribute to the convergence process. Besides shedding light on the economic growth process, these questions are relevant to policy targeting. This paper mainly contributes to answering the latter question.====There exist two main conceptual approaches to these questions: exogenous growth theory, initiated by Solow (1956), points out that technological progress is the source of growth, while endogenous growth theory, initiated by Romer (1990) and Lucas (1988), emphasizes differences in technology across countries and over time. Practically, exogenous growth models are easier to deal with as they consider a linear growth process, whereas endogenous growth models consider non-linearities and multiple regimes or clubs. Unsurprisingly, these approaches have contrary answers to the above questions: the former sees convergence resulting from capital accumulation, whereas the latter sees lack of convergence owing to technology heterogeneity.====Endogenous growth theory, however, acknowledges that convergence of countries within technology regime or club is possible (Azariadis and Drazen, 1990, Durlauf and Johnson, 1995, Bernard and Durlauf, 1996, Galor, 1996). Club convergence theory is founded on strong empirical evidence rather than theoretical models as is the case of exogenous growth theory. This strong empirical evidence, starting with Quah (1993), and espoused by Galor (1996), Jones (1997), and Johnson (2005), showed that labor productivity distribution is bimodal, and hence suggested that the world is divided into a poor and a rich ‘club’. Yet, since Quah’s contribution, advanced and tailored analyses have claimed evidence of more ‘clubs’.==== Whatever, while it is generally admitted that there is a club of rich countries, poor countries would seem to be categorized into sub-groups, namely marginalized, followers, and emerging countries, on the basis of additional variables, such as human capital (Durlauf and Johnson, 1995, Kalaitzidakis et al., 2001, Castellacci and Archibugi, 2008), institutional factors (Alfo et al., 2008), geographical characteristics (Bloom et al., 2003), trade status (Rodrik et al., 2004), and ownership (Maasoumi et al., 2007, He and Walheer, 2019). The goal is to choose variables that capture initial structures since countries with similar structural characteristics could be expected to converge to a similar steady state equilibrium, despite different initial conditions (Kormendi and Meguire, 1985, Grier and Tullock, 1989).====In many cases, econometric and statistical methods based on the first (or second) moment are used to determine whether countries tend to converge over time, and the relative contribution of economic growth sources to the growth process. Typical examples thereof are cross-section and panel regressions, used by Baumol (1986) and Barro (1991), and followed by increasingly more advanced (and complex but tailored) statistical and econometric tools (e.g. Magnus et al., 2010, Mirestean and Tsangarides, 2016, MoralBenito, 2016). In the context of club technology heterogeneity, plenty of empirical works have shown that there is strong parameter heterogeneity in cross-country or panel type growth regressions (Durlauf and Johnson, 1995, Desdoigts, 1999, Masanjala and Papageorgiou, 2004, Owen et al., 2009). Advanced and tailored methods have also been used in the context of club heterogeneity (Bloom et al., 2003, Canova, 2004, Alfo et al., 2008). A broad overview of econometric and statistical methods for empirical growth can be found in Durlauf et al. (2005).====In practice, econometric and statistical methods require to specify particular assumptions about the growth process. It is necessary to choose a particular functional form for the technology captured by a specific production function; often, though, more assumptions are needed, such as assumptions about technological change, market structure, market imperfections, returns-to-scale nature. Choosing a functional form for the technology is not insidious and may have important impacts on the empirical analysis. Moreover, empirical evidence has shown that the growth process may be too complex to be captured by methods focusing on the first (or second) moment. This evidence dates to Quah’s critique pointing out that empirical works should focus on the entire distribution. Also, the use of more sophisticated statistical methods often requires relatively large samples and, given the limited number of countries in the world, such techniques can ‘ask a lot of the available’.==== In the context of technology heterogeneity, this problem becomes more complex since several production functions have to be specified. Moreover, even when strong arguments are found to choose specific functional forms, it could be computationally cumbersome when the sample size is small and/or the number of parameters is large.====An increasingly popular alternative is to adopt a non-parametric model.==== This was suggested by Kumar and Russell (2002), who promoted the Farrell’s (1957) deterministic production-frontier method to analyze the economic growth process. In practice, data are used to reconstruct a world frontier that may be different over time. These authors’ approach is linked with the exogenous growth theory for considering a common world technology, but also with the endogenous one since the world technology differs over time. They obtain a tripartite decomposition of labor productivity into efficiency change, technological change, and capital–labor accumulation without assuming any particular structure for the growth process. Simple as it is, this approach has yielded interesting results: technological catch-up has not contributed to convergence; technological change is decidedly non-neutral; and both growth and bimodal polarization are driven primarily by capital deepening.====Kumar and Russell’s initial work has received some attention in the literature and has been extended to include more components in the decomposition: human capital (Henderson and Russell, 2005), financial development (Badunenko and Romero-Avila, 2013), region heterogeneity (Filippetti and Peyrache, 2015), sector heterogeneity (Walheer, 2016), and energy (Walheer, 2018b). These extended specifications have confirmed Kumar and Russell’s initial findings, while highlighting the relative contribution of their added component(s) to the economic growth and convergence questions. At this point, it is important to notice that these extended models require specific assumption in order to include their additional variable, such as modeling human capital as an augmented factor of labor, financial inclusion as an augmented factor of physical capital, or energy as a production factor. These additional assumptions often lack theoretical foundation. The initial and extended models have been used, for instance, in Enflo and Hjertstr (2009), Badunenko and Romero-Avila (2013), Badunenko et al. (2013), and Walheer (2016).====In this paper, we introduce technology heterogeneity in Kumar and Russell’s production-frontier approach. That is, instead of reconstructing a common world technology, we recognize that countries may have access to different technology over time. Our extension is based on the argument that technology difference is a major factor of growth differences across countries (Bernard and Jones, 1996, Prescott, 1998, Hall and Jones, 1999, Gong and Keller, 2003). The argument was taken over in Kumar and Russel’s initial work, admitting that their method based on a common world technology fails to capture the true technology for low level of capital per worker. Moreover, we consider the standard production process with labor and capital used to generate output, and recognize the importance of additional variables by using them when defining the technology clubs. By doing so, we follow the common practice in macroeconomic empirics while acknowledging the indirect impact of other important variables. This also avoids making additional (unverifiable) assumption(s) about the growth process.====We obtain a new decomposition of labor productivity distinguishing changes in efficiency, technological, and capital–labor ratio inside the clubs, and the gaps between the clubs and the world technology for these three dimensions.==== In other words, the new decomposition offers the advantage of making out what is happening within and between clubs. Using our new decomposition enables us to answer several additional empirical questions. First, we can verify each component’s effect on economic growth and contribution to the convergence/divergence process within and between clubs. It has been acknowledged that potential economic growth can be drastically different inside each club (Kormendi and Meguire, 1985, Grier and Tullock, 1989). Second, we can investigate whether one club dominates in terms of technology, and whether the other clubs exploit their backwardness position by imitating new technologies produced in the dominant club. It is often argued that advanced countries are the leader in terms of technology, while developing countries with enough technology capabilities adopt imitating strategies. Next, our paper gives more clues about the theoretical foundations of the existence of technology clubs. That is, whether it is capital accumulation, efficiency, or technological change that explains the existence of clubs, and how these dimensions evolve over time (Azariadis and Drazen, 1990, Wang et al., 2018).====The rest of the paper is structured as follows. In Section 2, we decompose labor productivity change into several explanatory components, and investigate the role of each component in the convergence/divergence of countries. In Section 3, we summarize our main findings and present our conclusions.",Labor productivity and technology heterogeneity,https://www.sciencedirect.com/science/article/pii/S0164070421000033,3 February 2021,2021,Research Article,139.0
"Nikolsko-Rzhevskyy Alex,Papell David H.,Prodan Ruxandra","Department of Economics, Lehigh University, Bethlehem, PA 18015.,Department of Economics, University of Houston, Houston, TX 77204-5882.,Department of Economics, University of Houston, Houston, TX 77204-5882.","Received 3 September 2020, Revised 26 January 2021, Accepted 1 February 2021, Available online 3 February 2021, Version of Record 12 February 2021.",https://doi.org/10.1016/j.jmacro.2021.103291,Cited by (2),"Debates about the conduct of ==== gap than on the output gap are preferred to rules with larger coefficients on the output gap than on the ==== gap. These results are robust to policy lags between one and two years, different weights on inflation loss than on unemployment loss, various definitions of high and low deviations periods, fixed and time varying neutral real interest rates, fixed and time-varying inflation targets, and measuring economic slack by either the output gap or the unemployment gap. We conclude that (1) the Fed should “constrain” constrained discretion by following a rule that responds more strongly to inflation gaps than to output gaps and (2) this type of rule should be added to the Fed's semi-annual Monetary Policy Report.","Is economic performance better under rules-based or discretionary monetary policy? This has been a central question in macroeconomics from the “rules versus discretion” debate among Friedman (1960), Council of Economic Advisors (1962), and Kydland and Prescott (1977) to the “policy rules versus constrained discretion” debate among Bernanke (2003), Mishkin (2017), and Taylor (2017). While money supply rules in the 1960s and 1970s had little effect on Fed policymaking, interest rate feedback rules following Taylor (1993a) have been much more influential. Interest rate rules have been presented to the Federal Open Market Committee (FOMC) since 2004 and have been included in the Federal Reserve Board's semi-annual Monetary Policy Report since 2017. Variants of Taylor rules have been used by Kohn (2007) and Bernanke (2010) to justify Fed behavior between 2003 and 2006 and by Yellen (2012, 2015a, 2017) to explain Fed behavior following the financial crisis.====What is the relation between policy rules and constrained discretion? Consider a class of Taylor rules where the federal funds rate equals the inflation rate plus alpha times the inflation gap, the difference between the inflation rate and the target inflation rate, plus gamma times the output gap, the percentage deviation of Gross Domestic Product (GDP) from potential GDP, plus the neutral real interest rate that is consistent with the inflation and output gaps equal to zero.==== The Taylor (1993a) rule, where alpha and gamma equal one-half and the inflation target and the neutral real interest rate equal two, is an example of a “balanced” rule where equal changes to the inflation and output gaps cause equal changes in the real interest rate. Constrained discretion, as in Bernanke (2003), specifies that the Fed has an inflation target and respects the dual mandate, which restricts policy rules to those where alpha and gamma are both positive. While the Taylor (1993a) rule is consistent with constrained discretion, so is any rule with positive coefficients on both gaps.====Current Fed policy can be characterized as a combination of policy rules and constrained discretion. The February 2020 Monetary Policy Report describes rules as providing “helpful guidance” for the FOMC. The Report goes on, however, to caution “against mechanically following the prescriptions of any specific rule” because of uncertainty regarding estimates of resource slack and the longer-run neutral real interest rate.==== The rules considered in the Report are all examples of “simple” rules with coefficients on only the two gaps.====Suppose that the Fed were to use a policy rule as a benchmark. Which rule should it choose? The standard way to answer this question is to estimate a model, simulate the model using the estimated coefficients and disturbances, and calculate the optimal policy rule that minimizes a loss function that includes inflation, the output gap, and the change of the nominal interest rate. Several leading macro models for the U.S. are the Christiano, Eichenbaum, and Evans (2005) model (CEE), the Smets and Wouters (2007) model (SW), the Taylor (1993b) model, and the Federal Reserve Board/United States model (FRB/US). When the three variances are equally weighted, Taylor and Wieland (2012) show that the optimal policy rule in the CEE, SW, and Taylor models is “inflation gap tilting” with alpha > gamma and Tetlow (2015) shows that the optimal policy rule for the October 2007 vintage of the FRB/US model is “output gap tilting” with gamma > alpha. The choice of a policy rule cannot be definitively answered by models, as the values of alpha in just these four models range from 0.53 to 2.00 and the values of gamma range from 0.26 to 1.17.====We propose an alternative method to evaluate monetary policy rules that are consistent with constrained discretion by comparing economic performance. Since the loss over the full sample is invariant to the policy rule, we calculate quadratic loss ratios, the loss in periods of high deviations between the actual federal funds rate and the rate prescribed by a particular rule divided by the loss in periods of low deviations from the same rule. The minimum criterion for a good policy rule is that the loss ratio be greater than one, so that economic performance is worse in high deviations periods than in low deviations periods. Otherwise, economic performance would improve by not adhering to the rule. More generally, rules with higher loss ratios are preferred to rules with lower loss ratios because economic performance is relatively worse in high deviations periods than in low deviations periods. Based on values for optimal policy rules, estimates of Taylor rules, and congruence with Fed behavior, we calculate loss ratios for both the 100 rules with alpha and gamma ranging from 0.1 to 1.0 and the 400 rules with alpha and gamma ranging from 0.1 to 2.0 in increments of 0.1.====Our method has both advantages and disadvantages compared to the standard practice. The most important advantage is that it does not require the use of a particular model. This is not just a hypothetical issue since, as described above, different well-known models provide very different optimal policy rules. The most important disadvantage is that it does not produce counterfactual simulation results. While we can provide an answer to the question of whether economic performance has been better during periods when the Fed adhered more closely to a rule, we cannot answer the question of whether economic performance would have been better if the Fed had always followed a rule.====The central results of the paper are (1) economic performance is better in low deviations periods than in high deviations periods for the vast majority of rules, and (2) rules with larger coefficients on the inflation gap than on the output gap have higher loss ratios, and are therefore preferred to, rules with larger coefficients on the output gap than on the inflation gap. Policy rules with larger coefficients on the inflation gap than on the output gap are preferable even if society places greater weight on unemployment loss than on inflation loss. The results are stronger for the larger set of rules.====We start with a variant of the Taylor (1993a) rule that is consistent with the rules posted on the Fed's Monetary Policy Principles and Practices web page, with real economic activity measured by the output gap, a two percent inflation target, and a time-varying neutral real interest rate, and proceed to consider three additional specifications with an unemployment gap, a two percent neutral real interest rate, or a time-varying inflation target.==== The benchmark model for all specifications has equal weight on inflation and unemployment loss, high (low) deviations periods defined by the absolute value of the deviation between the prescribed and actual federal funds rate being greater (less) than two percent, and the loss function calculated six quarters after the classification between low and high deviations periods to account for the possibility of reverse causation from economic performance to policy rule deviations.====Economic performance is better in low deviations periods than in high deviations periods. For the four benchmark specifications, the average loss ratio for the 100 policy rules is between 1.74 and 2.53, the loss ratios are greater than one for between 90 and 100 policy rules, and the loss ratios are greater than one for all 80 rules with a coefficient on the inflation gap of 0.3 and above. The Taylor principle that the coefficient on the inflation gap should be positive, so the federal funds rate is increased more than point-for point with inflation, is necessary, but not sufficient, for good policy. The strongest results are for the specification with an output gap, a time-varying neutral real interest rate, and a two percent inflation target. The results are even stronger when we consider all 400 policy rules.====Inflation gap tilting rules with larger coefficients on the inflation gap than on the output gap are preferred to output gap tilting rules with larger coefficients on the output gap than on the inflation gap. We divide the 100 policy rules into five quintiles of 20 rules, and the preponderance of rules in the top two quintiles, where the loss ratios are largest, are inflation gap tilting rules. The “relative loss ratio” of the inflation gap tilting rules to the output gap tilting rules is between 1.30 and 1.83 and significantly greater than 1.0 at the one percent level for all four benchmark specifications, showing statistical as well as economic significance. These results are robust to higher weight on inflation loss than on unemployment loss, higher weight on unemployment loss than on inflation loss, high and low deviations periods defined by 1.5 and 2.5 percent thresholds, and policy lags of four to eight quarters. The strongest results are for the specification in the Monetary Policy Report with an unemployment gap, a time-varying neutral real interest rate, and a two percent inflation target. The results are again stronger when we consider all 400 policy rules. We conclude that the Fed should “constrain” constrained direction by responding more strongly to inflation gaps than to output gaps.====We proceed to interpret our results in the context of other metrics. The preference for inflation gap tilting rules is consistent with theoretical results in Woodford (2003) when the coefficient on expected inflation is greater than the coefficient on the output gap in the New Keynesian Phillips curve. Orphanides and Williams (2007) and Laubach and Williams (2016) discuss the policy implications of uncertainty in real-time measures of the natural rate of unemployment and the neutral real interest rate, respectively. They advocate a strong response to inflation in order to reduce the importance of accurately measuring the natural rate of unemployment and the neutral real interest rate. Our result that inflation gap tilting rules are preferable to output gap tilting rules is consistent with their policy prescriptions.====Balanced and output gap tilting Taylor rules have been presented to the FOMC since 2004 and included in the Monetary Policy Report since 2017. While the balanced rule has a coefficient on both gaps of one-half, the output gap tilting rule has a coefficient on the inflation gap of one-half and a coefficient on the output gap of one. Our results suggest that the Fed should add an inflation gap tilting rule to the Taylor rules presented to the FOMC and included in the Monetary Policy Report. Based on the rules that are currently reported, an obvious choice would be a rule with a coefficient of one on the inflation gap and a coefficient of one-half on the output gap. In comparison with the two rules in the Report, this rule would prescribe both a higher interest rate when inflation is above target and a lower interest rate when inflation is below target.",Policy Rules and Economic Performance,https://www.sciencedirect.com/science/article/pii/S0164070421000045,3 February 2021,2021,Research Article,140.0
"Novta Natalija,Pugacheva Evgenia","International Monetary Fund, 700 19th St NW, Washington, DC 20431, United States","Received 2 July 2020, Revised 29 December 2020, Accepted 22 January 2021, Available online 28 January 2021, Version of Record 10 February 2021.",https://doi.org/10.1016/j.jmacro.2021.103286,Cited by (8), and show that similar results are obtained with and without pre-conflict GDP forecasts.,"The total number of conflict-related deaths has been on the rise since the early 2000s, reflecting the very deadly conflicts in Afghanistan, Iraq, and Syria. Conflict is a key factor that can hold back economic development (Rodrik, 1999; Hoeffler and Reynal-Querol, 2003; Besley and Persson, 2008; Esteban and Ray, 2017). Conflict leads to economic losses that can persist for years (Cerra and Saxena, 2008), dramatic consumption losses (Barro and Ursua, 2008), and immeasurable humanitarian suffering. Conflict can also ignite large refugee flows and may affect the economies of countries, near and far, for an extended period of time. Even though the number of countries in conflict has fallen since the 1990s (see Fig. 1), the rise in violent conflict across the world since the 2000s has weighed on global and regional GDP, given the number of relatively large economies experiencing strife and the severe effect of some of these episodes on economic activity.====In this paper, we focus on the macroeconomic costs of conflict. We first show examples of conflict outbreak and subsequent growth collapses, as measured by the decline in real GDP relative to the pre-conflict growth forecast made by the International Monetary Fund, published in the ====. Defining growth collapses in this way is similar in spirit to the synthetic control method of Abadie and Gardeazabal (2003). The key difference is that each country's counterfactual GDP path is based on its own pre-conflict forecasts, rather than a statistical weighting procedure based on other countries’ subsequent growth. We also investigate what share of global GDP is attributed to countries in conflict, which has been shown to help explain errors in global GDP forecasting (Celasun et al., 2021).====We then conduct a comprehensive empirical analysis of the dynamic effect of conflict on real GDP per capita, with and without controls for pre-conflict GDP forecasts. Further, we examine the effect of conflict onset on each of the key components of GDP by expenditure (private consumption, government spending, investment, and trade in goods) and sectoral value added (manufacturing, services, agriculture) to determine the channels through which conflict affects aggregate GDP. Lastly, we consider the impact of conflict on the number of refugees seeking shelter in neighboring countries and in advanced economies, which are typically located farther away from the epicenter of conflict. Throughout, we focus on a ten-year horizon after conflict outbreak.====We contribute to the literature in two ways. First, by using pre-conflict GDP forecasts as controls when estimating the effect of conflict on per capita GDP growth in a panel of countries, we offer new evidence that the long-standing concern about reverse causality does not appear to be large in this context. The possibility of reverse causality between conflict and economic conditions has long been recognized (Alesina et al., 1996; Collier and Hoeffler, 1998) and modeled (Garfinkel and Skaperdas, 2000; Chassang and Padro i Miquel, 2009; Dal Bo and Dal Bo, 2011). Within-country and within-region studies have shown that poverty and lack of employment can cause conflict (e.g. Miguel et al., 2004; Dube and Vargas, 2013; Do and Iyer, 2010). Yet in cross-country panel studies it has been difficult to move past correlations primarily because there is no counterfactual data on GDP, i.e. what economic conditions would have been (or expected to have been) in the absence of conflict. We fill this long-standing gap with the use of pre-conflict GDP forecasts.====Second, we present a comprehensive examination of the relationship between conflict and all the expenditure components of GDP, allowing us to infer their relative importance in explaining growth declines over the medium-term. Micro-level studies have shown that conflict affects all aspects of economic life – consumption (Justino, 2011), investment and firm production (Eckstein and Tsiddon, 2004; Amodio and Di Maio, 2018), international trade (Martin et al., 2008; Calì, 2014). Our analysis suggests that private consumption—typically the largest component of GDP—tends to be the primary driver behind the negative relationship between conflict and income per capita. From the value-added side, to the extent that sectoral data are available, we find that conflict reduces output in all sectors: agriculture, industry and services.====Lastly, we define conflict onset and incidence based on the share of population killed, as in Mueller (2016). The standard definition of conflict in the literature is based on an absolute number of people killed (usually 1,000) to identify a major conflict. A conflict with a thousand deaths could have almost no macroeconomic impact in a very large country yet be a major destabilizing force in a small country. Hence, from a macroeconomists’ perspective at least, conflict should be defined in terms of the percentage of the country's total population that was killed in conflict. Our analysis indicates that the effect of conflict tends to be underestimated if the definition of 1,000 deaths is used to define a conflict episode.====The rest of the paper is organized as follows: in Section II we present our definitions of conflict onset and incidence; in Section III we look at conflict-related growth collapses and the effect on global GDP; in Section IV we quantify the macroeconomic costs of conflict; Section V concludes.",The macroeconomic costs of conflict,https://www.sciencedirect.com/science/article/pii/S016407042100001X,28 January 2021,2021,Research Article,141.0
"Honda Jiro,Miyamoto Hiroaki","International Monetary Fund, United States of America,Tokyo Metropolitan University, Japan","Received 15 June 2020, Revised 3 January 2021, Accepted 22 January 2021, Available online 27 January 2021, Version of Record 29 January 2021.",https://doi.org/10.1016/j.jmacro.2021.103288,Cited by (4),What is the impact of population aging on the effectiveness of ,"The topic of population aging and its impact is gaining increased attention. Due to steady declines in fertility rates and increasing longevity, many countries – particularly those with advanced economies – are facing rapid population aging.==== ====
 Fig. 1 shows that the old-age dependency ratios have been rising for several decades and are projected to increase further.==== ==== Facing such stark demographic changes, an increasing number of studies have examined the macroeconomic implications of population aging.==== ====Recently, a few studies examine how population aging affects the effectiveness of fiscal policy (Basso and Rachedi, 2020, Honda and Miyamoto, 2020, Miyamoto and Yoshino, 2020). However, these studies estimate fiscal multipliers without considering the state of the business cycle while the recent literature finds that the impact of fiscal spending shocks depends crucially on the state of the business cycle (Auerbach and Gorodnichenko, 2012, Auerbach and Gorodnichenko, 2013, Blanchard and Leigh, 2013, Dell’Erba et al., 2014).====The purpose of this paper is to examine how population aging affects the effectiveness of fiscal policy over the business cycle. For this purpose, we estimate the state-dependent fiscal multipliers by using the local projection method of Jordà (2005) for aging and non-aging economies.==== ==== Our empirical analysis employs a panel data of OECD countries. We use the old-age dependency ratio to identify the aging state of an economy and identify the government spending shocks as forecast errors of government spending as in Auerbach and Gorodnichenko, 2012, Auerbach and Gorodnichenko, 2013.====We find that the output-boosting effect of fiscal stimulus is weakened as population aging proceeds. We first confirm that population aging weakens the effectiveness of fiscal policy in boosting an economy without considering the state of the business cycle. This is consistent with findings in recent studies (Yoshino and Miyamoto, 2017, Basso and Rachedi, 2020, Miyamoto and Yoshino, 2020). We then examine how population aging affects the state-dependent fiscal multipliers. We find that there is no effect of population aging on output effects of fiscal spending shocks in expansionary times, whereas in recessions the output effects of fiscal spending shocks are weakened as population ages.====Our analysis also suggests a few possible areas in aging economies by which the output effect of fiscal stimulus would be weakened. Specifically, we find that the effects of government spending shocks on private consumption and investment, as well as employment during recessions, are all weaker in aging economies. These results could be due to concerns about future tax burdens and uncertainties stemming from population aging, which would lower consumer confidence and prospective investment returns. Our results are also consistent with the mechanism found by Yoshino and Miyamoto (2017), which shows that population aging could weaken the multiplier effects of fiscal policy since there would be more retirees.====Our results have important policy implications. In aging economies, policymakers should account for the weaker demand-supporting effects of fiscal policy. During recessions, a larger fiscal stimulus would thus be called for to support aggregate demand, which would require maintaining a larger fiscal space to allow for wider swings in the fiscal position. Furthermore, given the weak effect of fiscal stimulus to boost output even during recessions, other macroeconomic policies or structural reform measures would have to play a more important role. Our analysis also suggests that policy measures promoting labor supply could help increase the output effect of fiscal stimulus in aging economies.====To our knowledge, this is the first empirical paper that attempts to assess the impact of population aging on fiscal policy effectiveness over the business cycle. Basso and Rachedi (2020) find that the age structure of the population affects local fiscal multipliers (such that fiscal multipliers are larger in economies with higher shares of young people in the total population) by using the state-level data of the United States. By using a method similar to this paper, Miyamoto and Yoshino (2020) estimate fiscal multipliers at the country level. They find that in non-aging economies, government spending shocks increase output, while output responses are not statistically significant in aging economies. However, these studies do not control the state of the economy. Yoshino and Miyamoto (2017) show that, by using a tractable dynamic stochastic general equilibrium (DSGE) model with heterogeneous agents, the effectiveness of fiscal and monetary policies is weakened with population aging over business cycles. They find that reduced labor supply due to population aging weakens the effectiveness of fiscal policy. However, they are theoretical rather than empirical.==== ====The rest of the paper is organized as follows: Section 2 presents the empirical methodology. Section 3 presents the main findings. Section 4 discusses possible channels through which population aging affects the effectiveness of fiscal stimulus. Finally, Section 5 provides the conclusion and reveals policy implications.",How does population aging affect the effectiveness of fiscal stimulus over the business cycle?,https://www.sciencedirect.com/science/article/pii/S0164070421000021,27 January 2021,2021,Research Article,142.0
"Jiang Dou,Weder Mark","Nanjing University of Finance and Economics, China,Aarhus University, Denmark and CAMA, Australia","Received 26 April 2020, Revised 14 December 2020, Accepted 18 December 2020, Available online 24 December 2020, Version of Record 29 December 2020.",https://doi.org/10.1016/j.jmacro.2020.103285,Cited by (0),"This paper quantitatively investigates the Depression of the 1890s and the 1907 recession in the United States. Business Cycle Accounting decomposes economic fluctuations into their contributing factors. The results suggest that both the 1890s and the 1907 recessions were primarily caused by factors that affect the efficiency wedge, i.e. slumps in the economy’s factor productivity. Distortions to the labor wedge played a less important role. Models with financial market frictions that translate into the efficiency wedge are the most promising candidates for explaining the recessionary episodes.","The string of economic slumps in the United States, beginning in 1890 and leading up all the way to World War I, has received limited attention from macroeconomists, likely because of being overshadowed by the Great Depression and, as of late, the Great Recession. However, several striking similarities stand out between these earlier slumps and the Great Depression and Recession. To start with, in all cases, the crises appeared to have originated within the financial sector that then spread through the rest of the economy. Also, all slumps coincide with panics on financial markets associated with rapid price declines on asset markets. Lastly, during all these episodes, the recoveries were exceptionally tepid.====The period from 1863 to 1914, the National Banking Era, was characterized by the absence of a central bank. While financial regulation existed and clearinghouses took on roles later fulfilled by the Federal Reserve, these regulations and arrangements were uneven across States and often inadequate and ineffective in dealing with crisis events.==== ==== Paired with the limited role played by fiscal policies the National Banking Era provides an useful laboratory for studying recessions. Although the various specifics of the economic environment and circumstances differ from today’s economy, we believe that macroeconomics can potentially learn valuable lessons from studying these historic slumps. In particular, we concentrate on two macroeconomic events: the panics and slumps of 1893 and 1907 which together with the Great Depression and the 1920–21 recession make up the list of the largest recessions in the United States. The current paper is a first approach towards these early recessions working with quantitative dynamic general equilibrium theory and by conducting Business Cycle Accounting (BCA)measurements it will provide directions for future research.==== ==== BCA (a) isolates promising classes of models and theories and (b) guides the development of theory. The approach sets up a prototype model with time-varying wedges for which, as Chari et al. (2007) and Brinca et al. (2016) demonstrate, mappings exists between the prototype and detailed models with frictions. The core is the accounting procedure involves data to measure wedges, to estimate stochastic processes and to indicate how much of macroeconomic fluctuations are accounted for by each wedge.",American business cycles 1889–1913: An accounting approach,https://www.sciencedirect.com/science/article/pii/S0164070420302068,24 December 2020,2020,Research Article,143.0
"Crawley Andrew,Welch Sarah,Yung Julieta","School of Economics, University of Maine, United States of America,Department of Economics, Bates College, United States of America","Received 29 July 2020, Revised 1 December 2020, Accepted 11 December 2020, Available online 23 December 2020, Version of Record 23 December 2020.",https://doi.org/10.1016/j.jmacro.2020.103282,Cited by (3),"Traditional measures of unemployment can mask important changes in the labor market across time. We therefore use broader definitions of unemployment to estimate time-varying job-matching efficiency rates that are consistent with vacancies and hiring activity data for the U.S. Our efficiency rates are then modelled along with employment data to study their dynamic, non-linear relationship. We find that including marginally attached workers and part-time workers for economic reasons helps explain the changes in employment patterns observed after the global financial crisis. This finding emphasizes the importance of accounting for labor underutilization, particularly during the latest economic recovery.","Following the 2008 global financial crisis, the United States experienced the longest expansion on record, marked with a sluggish recovery of the labor market. Notably, while the unemployment rate hovered around an all time low, businesses seeking to hire workers were struggling to find qualified applicants, resulting in inefficiencies that over a prolonged period of time can be detrimental to the economy.==== ====
 Fig. 1(a) shows the official unemployment rate, known as u3, along with the job vacancy rate from 1967 to 2018. During expansionary periods, both series move closer to one another, quickly diverging as the economy enters into a recession. For the first time since the 1970s, the U.S. job vacancy rate surpassed u3 in early 2018, prompting concerns about the overall health of the labor market that arise from pronounced inefficiencies in the job matching process. This period of unprecedented expansion was brought to an abrupt end in March 2020 as the U.S., like many other nations, was forced to shut down its economy in response to the COVID-19 Pandemic. What followed was the largest increase in unemployment the U.S. had ever recorded, with the unemployment rate standing at 14.7% in April 2020. The magnitude of this rise illustrates the challenges that lie ahead for an economic recovery, with labor market dynamics taking unprecedented centre stage in both academic and policy debates.====While u3 represents the number of unemployed people as a percentage of the labor force, in this paper, we utilize broader measures of unemployment to estimate the rate at which job matching efficiency has changed over time. In particular, we consider definitions of unemployment provided by the ==== that incorporate additional groups of people not included in the official rate, offering alternative measures of labor underutilization. The first measure considered is u4, which adds ‘discouraged’ workers to the total number of unemployed people; not classified as unemployed under u3 because they have not actively searched for work in the last 4 weeks due to discouragement about their job prospects.==== ====
 Fig. 1(b) shows the difference between u4 and u3 (dash-and-dot line), effectively representing the set of discouraged workers, which remains relatively flat prior to the 2008 global financial crisis, showing an increase that reverses only slowly in its aftermath.====The second measure we consider is u5, which adds all people who are marginally attached to the labor force (including discouraged workers) to the total number of unemployed people. This accounts for those who want but have not searched for work during the most recent 4 weeks, either because of discouragement about their job prospects or other reasons such as childcare problems, family responsibilities, and ill health or disability. Fig. 1(b) indicates that the set of marginally attached workers for reasons other than discouragement (u5 minus u4, represented with the dotted line), has remained relatively stable during the 1994–2018 period.====The final and most encompassing measure of labor underutilization is u6, adding to u5 people working part time for economic reasons, also called involuntary part-time workers. Part-time workers for economic reasons are shown in Fig. 1(b) by subtracting u5 from u6 (dashed line), revealing a pronounced increase during the 2008 global financial crisis, not reflected in the other measures of unemployment. Altogether, the difference between u6 and u3 (solid line) indicates that broader definitions of unemployment might be particularly important when accounting for changes in the labor market, especially post 2008.====We study how all these measures of unemployment yield different estimates of job matching efficiency and then explore their ability to empirically describe employment patterns pre and post the 2008 crisis. To this end, we begin by investigating changes in the degree of labor market tightness, relating unemployment to vacancy rates through the Beveridge curve in Section 2. We show how broader definitions of unemployment amplify well-known properties of the Beveridge curve previously identified in the literature, such as time variation and non linearities, as well as the pronounced contrast in labor market dynamics, pre and post the global financial crisis. Then, we utilize the foundational labor market framework of Blanchard and Diamond (1989) to obtain monthly matching efficiency rates using different definitions of unemployment, consistent with data on vacancies and hiring activities in Section 3. Finally, we estimate the cyclical properties of employment and matching efficiency rates in Section 4.1, and obtain the dynamic correlation between different job matching efficiency rates and employment pre and post the global financial crisis in Section 4.2.====We find that when including marginally attached people and part-time workers for economic reasons, the estimated job matching efficiency rates can better describe the labor market patterns observed in the data during the 2008–2018 period. Importantly, we find that the rate at which job vacancies can be filled has been hindered by labor market frictions that are present in alternative measures of labor underutilization, and that accounting for part-time workers for economic reasons in particular yields job matching efficiency estimates that are more correlated with employment during the latest expansion.====Our work relates to the strand of the literature that has emphasized the need to account for broader definitions of unemployment. Kingdon and Knight (2006), for example, focused on the inclusion of discouraged workers making up a large proportion of those out of the employment pool. Their work indicates that potential structural issues that can amplify the mismatch in the labor market may not be captured by the traditional measure of unemployment. More recently, Feng and Hu (2013) provided evidence that the unemployment rate is not an accurate and reliable indicator of labor market health, also advocating for the need of a more comprehensive measure. There is a long-standing literature studying job matching inefficiencies in the labor market in other countries (e.g. Mumford and Smith, 1999 in Australia) its relationship to vacancies and wages (e.g. Diamond, 2011, Mavromaras et al., 2015) and to employment dynamics (e.g. Barnichon and Figura, 2015, Elsby et al., 2015, Sengul and Tasci, 2020); as well as its impact on changes in the labor force participation rate (e.g. Krueger, 2017).====Our paper also relates to studies of Beveridge curve dynamics after the great recession (e.g. Hobijn and Şahin, 2013), including Klinger and Weber (2016), who investigated the structural or cyclical nature of Beveridge curve dynamics in Germany, finding that matching efficiency accounted for about half of the inward shift that followed institutional reforms during the great recession. Additionally, we describe the cyclical properties of employment along with our job matching efficiency rates by identifying peaks and troughs in the time series (e.g. Claessens et al., 2012 utilized this algorithm to estimate linkages between different phases of business and financial cycles for 44 countries) and account for heteroskedasticity and volatility clustering in the data by implementing a GARCH model to estimate dynamic correlations (see Bauwens et al., 2006 for a survey of different applications).====We conclude that incorporating a broad definition of unemployment is particularly important for describing labor market dynamics following the global financial crisis, with notable policy implications for upcoming recovery periods. The current crisis affecting the U.S. labor market might be better assessed through this broader definition of unemployment rather than the narrower scope of u3.",Improving estimates of job matching efficiency with different measures of unemployment,https://www.sciencedirect.com/science/article/pii/S0164070420302032,23 December 2020,2020,Research Article,144.0
"Ahmed Rashad,Aizenman Joshua,Jinjarak Yothin","USC,USC & the NBER,VUW","Received 24 June 2020, Revised 2 October 2020, Accepted 11 December 2020, Available online 18 December 2020, Version of Record 25 December 2020.",https://doi.org/10.1016/j.jmacro.2020.103281,Cited by (8)," volatility, and underlying commodity exposure exhibit stronger associations between public debt and policy interest rates.","A notable outcome of the Global Financial Crisis (GFC) has been the search for yields by OECD investors, manifested by their growing demand for debt issued by Emerging Market Economies (EMEs). The QE policies adopted by the US and the Eurozone in the aftermath of the GFC induced a sharp decline of interest rates and risk premia, propagating ‘yield chasing’ by institutional investors, increasing thereby the demand for EMEs’ hard and local currency sovereign debt. These developments mitigated the ‘original sin’ concerns identified by Eichengreen, Hausmann, and Panizza (2007) – the inability of most EMEs to borrow abroad in their currency.==== The resultant rise of the external debt of EMEs led to an unprecedented increase in their debt/GDP, putting to the fore concerns about growing debt overhang and fragility, including the possibility of fiscal dominance. This possibility is the case when growing debt/GDP constrains the conduct of monetary policy, inducing the central bank to pay growing attention to reducing the costs of serving the public debt, and the country's external debt [Blanchard (2004)].==== Our paper investigates fiscal dominance channels, with a particular focus on EMEs and Developing Countries before and after the GFC.====Research on the interaction of fiscal and monetary policy has a long history. Sargent and Wallace (1981) reason that a fiscal policy that fixes an exogenous path for the real primary government deficit may make inflation inevitable, regardless of the choice of monetary policy. This would be the case in circumstances leading the public debt to reach a limit, beyond which further government borrowing is impossible, implying that monetary policy will be induced to creation of sufficient seignorage revenues to finance the deficit, as may be the case if a central bank's independence is in doubt. This configuration leads to ‘fiscal dominance,’ where the central bank is induced to accommodate the fiscal pressure.====A follow up perspective on these issues is provide by Woodford's contributions. Woodford (1998) outlined an example where even an independent central bank will choose to react to fiscal news. Specifically, there are plausible circumstances where the fiscal authority would change its policy when a country's “debt limit” were reached. A central bank charged to maintain price stability opt to be concerned about the conduct of fiscal policy as well. The central bank might seek to ensure that the fiscal authority is committed to a Ricardian policy, without then feeling any further need to participate in the year-to-year conduct of fiscal policy. Alternatively, a central bank operating in a country where the fiscal policy is non-Ricardian would be aware that unexpected changes in government's budget would affect the equilibrium price level. This in turn may induce the central bank to influence fiscal policy decisions on an ongoing basis.====Woodford also shows that a policy deemed as achieving price stability in an environment of Ricardian fiscal policy, might be disastrous in the case of a non-Ricardian fiscal policy. In these circumstances, a conclusion that fiscal policy is non-Ricardian would concern the Central Bank with debt management issues since the composition of the public debt matters for the behavior of equilibrium inflation, as Cochrane (1996) illustrated. Specifically, the longer the duration of the nominal bonds, the more sharply its value will decline with increases in inflation. Thereby, inflation is unambiguously less affected by fiscal shocks when the public debt is of longer maturity. In the same vain, higher indexation of government debt affects inflation variability in the case of non-Ricardian fiscal policy.====The considerations outlined in Woodford (1998) led him to conclude that “rather than simply implementing an interest-rate rule and letting the government budget evolve as it may, it would be appropriate for the central bank to play an active role in commenting upon the inflationary consequences of proposed changes in fiscal policy, and similarly in ensuring that the composition of the public debt favors price stability, by maximizing the extent to which the adjustment in response to fiscal shocks occurs through changes in long bond yields rather than through variations in the inflation rate. Through such recognition on the part of the fiscal and monetary authorities of the interconnectedness of their respective concerns, it may be possible to enjoy the benefits of a tax policy aimed at microeconomic efficiency rather than budget balance, without sacrificing the advantages that flow from stable prices.”==== Against this background, our paper investigates the degree to which the observed patterns of central bank's policies are affected by country's debt profile and find evidence that are consistent with conditional fiscal dominance, akin to Woodford's interpretations.====Fiscal dominance concerns have been manifested across several channels. The most transparent one is when the government funds domestic debt by using the printing press, leading to accelerated inflation, and possibly hyperinflation. The more nuanced manifestation may take place in in the open economy with hard and soft currency debt when monetary policy focuses on keeping a government solvent as opposed to hitting the designated domestic policy targets of Taylor rule - targeting inflation, unemployment and/or growth. Instead, monetary policy is prioritizing sovereign debt management, like minimizing debt service costs by keeping interest rates low and reassuring financial markets of the country's creditworthiness. A clear example of these challenges include Inflation Targeting (IT) regimes in countries with large hard currency external debt/GDP, possibly Turkey in recent years, and a fair share of Latin American economies in past decades. Their policymakers may face growing ‘fear of floating’ (Calvo & Reinhart, 2002). Specifically, real exchange rate depreciation increases the costs of serving their hard currency external debt by the debt/GDP times the depreciation rate (the cost measured as a fraction of the country's GDP). This condition in turn may induce the central bank to put a higher weight on stabilizing the real exchange rate, or to set interest rates in such a way that lower the costs of servicing the debt, hence deviating from prescribed targets. With the exception of Argentina, during our sample period, 2000-2017, inflating the debt is less widespread concern among emerging markets than successful sovereign debt management. Thereby, this paper focuses on tracing fiscal dominance in circumstance when the authorities prioritizing sovereign debt management, attempting to reassure financial markets of the country's creditworthiness.====While the original inflation targeting and Taylor Rule ignored the real exchange rate as a policy goal in OECD countries, the research dealing with Emerging Market Economies put it to the fore (Aizenman, Hutchison, Noy (2011); Berganza and Broto (2012); Ghosh, Ostry, Chamon (2016)). Indeed, exchange rate targeting (aka exchange rate stabilization) may be accomplished in a hybrid Inflation Targeting regime by putting higher policy weight on stabilizing the real exchange rate, possibly by proactive management of sizable buffers of international reserves (IR) and sovereign wealth funds (SWFs).====The impact of growing local currency debt overhang on Inflation Targeting countries and managed flexible exchange rate regimes may be more intricate. One expects their policy interest rates to go up with increasing debt overhang, reflecting higher risk premia. The higher interest rate may also be associated with nominal depreciation and more significant inflationary pressure. Countries with sizable IR may also opt to mitigate these effects by selling IR to lean against the currency depreciation.====We conclude this section with a road map of the main results. In section 2, using de jure Inflation Targeting classification, we find evidence of fiscal dominance among Developed Market Economies (DMEs) under IT, where interest rates tend to be negatively associated with rising public debt levels. However, amid a battery of robustness tests, we find that this negative association is primarily driven by the secular decline in risk-free rates. Among EMEs under non-IT regimes, we find a negative interest rate effect accounted for by a higher foreign currency public debt/GDP ratio. Additionally, for EMEs under non-IT regimes, the composition of public debt matters, as larger proportions of debt denominated in foreign currency are associated with higher interest rates: a risk premium effect. These offsetting effects imply that the fiscal dominance effect among EMEs following non-IT regimes is non-linear and depends on both debt composition and the hard currency public debt-to-GDP ratio.====In Section 3, under a simple de facto classification binning groups of EMEs by their nominal exchange rate volatility into low, moderate and high exchange rate volatility groups, we highlight that interest rates under high NEER volatility regimes are more sensitive to the fiscal position of the country. This is at odds with the de jure analysis until we show that high NEER volatility countries are also those with high inflation volatility. In Section 4, we further show that the debt-interest rate relationship among EMEs is largely coming from EMEs with significant commodity exposure. This highlights a key role of commodity volatility in inducing potential fiscal dominance among EMEs.====Section 5 focuses on analysis and discussion regarding the non-linear association between public debt and policy rates in Emerging Markets. In Section 6, we provide a battery of robustness tests. The final section concludes. We also provide additional results and analysis in the Online Appendix. Section B of the Online Appendix provides a case study specific to the Euro Zone.",Inflation and Exchange Rate Targeting Challenges Under Fiscal Dominance,https://www.sciencedirect.com/science/article/pii/S0164070420302044,18 December 2020,2020,Research Article,145.0
Stähler Nikolai,"Deutsche Bundesbank, Wilhelm-Epstein-Str. 14, 60431 Frankfurt, Germany","Received 12 October 2020, Revised 16 November 2020, Accepted 19 November 2020, Available online 24 November 2020, Version of Record 28 November 2020.",https://doi.org/10.1016/j.jmacro.2020.103278,Cited by (5)," and consumption rise. Although expected advances in automation technologies are able to mitigate or even circumvent output losses in the aggregate and improve consumption possibilities for everyone, this comes at the cost of increased inequality because non-routine workers benefit disproportionately.","Population aging and technological progress in automated production processes (using artificial intelligence, AI henceforth, or robots, for example) are two major trends that are going to affect all developed economies in the future. Technological advances are likely to make our lives easier and more convenient in many ways. Nevertheless, there are fears that working life could undergo radical change as a result. Some researchers believe that there is a significant chance of AI outperforming humans in many tasks within a time span that most of us will live to see (see Grace et al., 2018). For example, driverless cars could pose a threat to the jobs of millions of bus and taxi drivers, robots could replace cashiers as well as staff in the medical, the legal and the educational profession or in translation services (The Economist, 2019); (Chui et al., 2015); (Dengler and Matthes, 2018); (Ford, 2015); (Brynjolfsson and McAfee, 2014). It is also discussed already whether or not AI can replace (computational) economists some time soon (Maliar et al., 2019). Pessimistic forecasts suggest that around half of existing jobs in the United States will be particularly affected by the expected technological progress, and some fear that 83% of the jobs in the low-wage sector could be scrapped (see Frey, Osborne, 2017, Acemoglu, Restrepo, 2018, Acemoglu, Restrepo, 2019c). Similar, slightly lower numbers are found for Europe, see Chiacchio et al. (2018).====On the other side of the spectrum are “technology optimists”, who stress that the rise in productivity implied by the technological progress will lead to higher aggregate income and new lines of work being opened up in the medium term (see Autor, 2014). This is especially relevant when taking into account the second major trend: population aging. Increased longevity and low fertility is said to reduce per-capita output, output growth, investment and real interest rates (Carvalho, Ferrero, Nechio, 2016, Aksoy, Basso, Smith, 2017, Aksoy, Basso, Smith, Grasl, 2019, Papetti, 2019, Sudo, Takizuka, 2019). The negative effects can become stronger when aging contributes to a reduction in innovation and/or business churn (Hopenhayn et al., 2018); (Liang et al., 2018); (Ouimet and Zarutskie, 2014); (Pugsley and Sahin, 2018); (Röhe and Stähler, 2020). Technological progress, it is therefore argued, can mitigate or even circumvent these negative aging-induced effects (Acemoglu and Restrepo, 2017); (Graetz and Michaels, 2018). In this paper, we contribute to this discussion by analyzing====We address these questions by means of an extended life-cycle model along the lines of Gertler (1999) and Carvalho et al. (2016). We extend their model by assuming the existence of two types of labor, namely routine and non-routine workers, and two types of capital, traditional and automation capital. Individuals are born as a routine worker. With a given probability, they either retire (as routine worker) or become a non-routine worker. A non-routine worker also retires someday, and all retirees pass away eventually.==== A representative firm produces a unique final good. Robots and routine workers are substitutes. Traditional capital and labor as well as the two labor types are complements. The remaining model elements are standard.====We find that population aging fosters the increased use of robotics in production. As life expectancy increases, individuals increase their savings when young(er) to consume when old(er). The savings glut reduces the real interest rate which, in the end, reduces capital interest/costs (see Carvalho et al., 2016, as well as Baldanzi et al., 2019, and Gehringer and Prettner, 2019). In addition, lower fertility rates eventually generate a scarcity of labor, which increases wages. Firms substitute (more) expensive workers by (now cheaper) robots, and the labor share of income falls (see also Acemoglu, Restrepo, 2018, Basso, Jimeno, 2020, and Irmen, 2020). Abeliansky and Prettner (2020) show empirically that, indeed, the incentive to automatize increases with population aging. We can show that the reduction in the labor share of income is entirely borne by routine workers, who are the ones that can be substituted for. Non-routine workers actually gain because, due to complementarity, the increased use of robotics fosters marginal productivity of non-routine workers. It also seems worth highlighting that, quantitatively, the majority of these effects is driven by an increase in longevity while fertility changes are doing little. The reason is that the former reduces the interest rate – and, thereby, capital costs – significantly more.====Higher productivity of automation technologies itself also fosters the increased use of robotics in production. For given wages and capital interest rates, it becomes more attractive to now use (more productive) robots relative to routine labor services. This drives down wages for routine workers relative to wages for non-routine workers and reduces the labor share of income, which is in line with findings by Acemoglu, Restrepo, 2018, Acemoglu, Restrepo, 2018, Acemoglu, Restrepo, 2019, Acemoglu, Restrepo, 2019c, Karabarbounis and Neiman (2014), and Prettner (2019), among others. Brynjolfsson and McAfee (2014) provide a very comprehensive overview. Due to the same mechanism described above already, this is shouldered by routine workers; see also Autor (2014), Berg et al. (2018), Eden and Gaggl (2018), Dauth et al. (2017), Hemous and Olsen (2020) and Kharlamova et al. (2018).====Population aging per se has negative effects on per-capita output (see Aksoy et al., 2019, and the other literature mentioned above). If population aging and progress in automation technologies occur at the same time, however, these effects are mitigated and can even be overturned when the technological progress is sufficiently strong (Acemoglu and Restrepo, 2017, and Graetz and Michaels, 2018). Yet, this comes at the cost of a further decline in the labor share of income, borne by routine workers, and further (relative) wage losses of routine workers. The different developments of (expected life-time) labor income imply an increase in wealth inequality. Relatively lower labor income and capital income, in turn, imply that consumption inequality also increases as a result.====Our findings suggest that advances in automation technologies can help to reduce aggregate income losses resulting from population aging. However, to reap the full benefits from technological progress and to avoid “social unrest” (as it has, for example, taken place during the Industrial Revolution; see Allen (2009), and Katz and Margo (2014), for a discussion), promoting inclusion and participation of those who are likely to lose (at least in relative terms) is important. We think that our model can, also because of its tractability (relative to the literature discussed above), serve as a suitable laboratory for analyzing costs and benefits of upcoming policy suggestions in this direction in future research (see, among others, Prettner and Strulik, 2019 and Gasteiger and Prettner, 2020, for a discussion).====The rest of the paper is structured as follows. The model and its calibration is described in Section 2. In Section 3, we describe the simulation experiments and show the results. Section 4 concludes.",The Impact of Aging and Automation on the Macroeconomy and Inequality,https://www.sciencedirect.com/science/article/pii/S0164070420302020,24 November 2020,2020,Research Article,146.0
Couture Cody,"Department of Economics, 3151 Social Science Plaza, University of California, Irvine, CA 92697, United States of America","Received 5 April 2020, Revised 18 November 2020, Accepted 19 November 2020, Available online 23 November 2020, Version of Record 17 December 2020.",https://doi.org/10.1016/j.jmacro.2020.103279,Cited by (3),"I examine the impact of the forecasts released by the Federal Open Market Committee (FOMC) in the Summary of Economic Projections over the period of April 2011 through March 2019. I find that changes in the median FOMC federal funds rate forecast did impact asset prices, but forecasts of output and ","Forward guidance, that is, communication about the future path of monetary policy, has long been one of the tools available to the Federal Open Market Committee (FOMC). However, following the financial crisis in 2008, it became more important as the federal funds rate – the typical tool for monetary policy – was rendered ineffective by the zero-lower bound (ZLB). Thus, the FOMC turned to unconventional monetary policy, which consisted primarily of forward guidance and large-scale asset purchases (LSAPs).====There is a large literature showing that forward guidance has been effective in influencing asset prices (e.g.,Gürkaynak et al., 2005) as well as stimulating output (e.g.,Bundick and Smith, 2016). However, there is still much that is not understood about the channels through which forward guidance is communicated. Likewise, it is unclear precisely how various forms of FOMC communication are received by market participants. To better understand both these questions, I consider the financial market response to the Summary of Economic Projections.====Beginning in October 2007, the FOMC began to provide a Summary of Economic Projections (SEP), which summarized FOMC participants’ forecasts of key economic variables – real GDP growth, the unemployment rate, personal consumer expenditures (PCE) inflation, and core PCE inflation – for the current year and the following two to three years. In 2012, forecasts of the federal funds rate, often called the dot plots, were added to the SEP. According to the FOMC, the SEP was implemented to “improve the accountability and public understanding of monetary policy making”.==== ==== These forecasts not only provided investors with a deeper insight into policymakers’ outlook on the economy, but they also allowed policymakers to communicate their anticipated path of macroeconomic variables, especially that of the federal funds rates. As such, the SEP functions as one of the channels for the FOMC’s forward guidance.====Among the various forms of forward guidance, the SEP is unusual in that it is purely quantitative. As such, the communication is much less ambiguous than other forms of forward guidance such as the FOMC policy statement or speeches by FOMC members. This has led to close market scrutiny of the SEP, particularly the federal funds rate forecasts. Consequently, it appears that changes in the SEP can influence financial markets.====For instance, on March 16, 2016, the FOMC did not change the current federal funds rate and there were not any significant large-scale asset purchases or any clear verbal communication indicating forward guidance. Instead, much of the new information that day came from the SEP. In particular, the median of the dot plot for both the current year and following year dropped by 50 basis points. On that same day, the surprise change to the current federal funds rate was a 1 basis point decrease, indicating that keeping the federal funds rate unchanged was largely expected. Despite this, expectations of the federal funds rate at longer horizons had a surprise decrease of 9 basis points. Treasury yields, particularly those with medium-length maturities, declined by a similar magnitude. Thus, it would appear that the expected path of interest rates and treasury yields moved almost entirely due to the SEP.====Despite this scrutiny – or perhaps because of it – James Bullard, the President of the St. Louis Federal Reserve, has suggested removing the federal funds rate forecast from the SEP due to fears that the dot plots are de-emphasizing the FOMC’s continued data dependence(Bullard, 2018).==== ==== Despite the importance of the SEP, there are, to the best of my knowledge, no other studies examining the impact that it has on asset prices. I attempt to fill this gap by examining the relationship between SEP forecasts, private sector expectations, and asset prices between April 2011 and March 2019. Quantifying this relationship is important to understanding both how forward guidance is communicated and the effectiveness of the SEP in influencing private sector expectations.====To answer these questions, I use an event-study framework to study the financial market’s response in the 30-minute window surrounding FOMC announcements. I find that a change in the FOMC’s median federal funds rate forecast for the end of the current or following year had a significant impact on treasury yields. Surprisingly, a change in the median federal funds rate forecast for two years ahead did not have any impact. Further, changes in the median forecast of other variables did not have any effect on asset prices. This would suggest that the primary mechanism for the SEP is influencing beliefs regarding future monetary policy rather than providing information about the future state of the economy. I also consider the effect of unpublished projections in the years prior to the release of the SEP and find that over this period, asset prices did not respond to a change in the federal funds rate forecast, which suggests that it is the publication of the interest rate forecasts that causes the response in asset prices rather than other variables that may be correlated.====I also consider the dispersion of the dot plots and find that this has no direct effect on asset prices. I interpret this as treasury risk premiums not responding to the distribution of the federal funds rates forecasts. However, multiple measures of dispersion are highly correlated with the uncertainty surrounding monetary policy over the next year. More importantly, changes in the dispersion can impact the degree of monetary policy uncertainty. In this sense, there may be instances in which the dot plots are creating more confusion than clarity. Finally, I find that most of forward guidance can be summarized through the forecast revision of the median federal funds rate forecast for the end of the following year.====The rest of the paper is as follows. In Section2, I detail the data relating to both FOMC projections and federal funds rate futures. Section3 estimates the relationship between the median federal funds rate projections and asset prices. I then show that this effect is caused by shifting investor expectations. Finally, I explore the relationship between the median forecast of the remaining variables and asset prices but find no effect. Section4 explores whether there is any impact of the dispersion of the forecasts. Section5 relates the FOMC projections to the more general concept of forward guidance. Section6 concludes.",Financial market effects of FOMC projections,https://www.sciencedirect.com/science/article/pii/S0164070420302019,23 November 2020,2020,Research Article,147.0
Yukhov Alexey,"Weber State University, Department of Economics, Ogden, 84408 Utah","Received 10 August 2018, Revised 31 August 2020, Accepted 15 November 2020, Available online 18 November 2020, Version of Record 1 December 2020.",https://doi.org/10.1016/j.jmacro.2020.103267,Cited by (0),"Possession and production of oil reserves affects the host country’s current account. Throughout the history of the North Sea oil, Norway ran persistent current account surpluses and accumulated public “oil funds.” The other major producer, the United Kingdom did not establish an oil fund. This work models how oil discoveries impact the current account. A ","Developments in the oil and gas sector change the country’s economy in a number of ways. Typically, consumption grows, work hours shorten, and some of the oil wealth is saved for the future. Often, oil-rich countries save by running a current account surplus.====To evaluate the effect of these developments on the international savings of an oil-producing country, using the examples of Norway and the United Kingdom, this paper models the current account as a function of oil reserves. This is in contrast to current open-economy macroeconomic literature that focuses on the effects of price shocks on countries’ international finance. The model parses out the implementation of an oil fund, the home country bias, and rigidity of oil production as the factors that shape the relationship. This is to reflect that oil-rich countries often establish oil funds, such as Norway’s pension fund, to prevent squandering of the national oil wealth. They maintain long positions in international asset markets and take steps to ensure the profitability of their investments. Countries produce oil by licensing oilfields to international oil companies that possess adequate oilfield technology.====This paper interprets an oil fund as a manifestation of the country’s intertemporal preferences. A simple guiding principle of many oil funds is to save the oil export revenue and consume only the interest accrued on the savings. In terms of intertemporal choice, this implies that consuming today is no more preferable than tomorrow. Such an oil fund puts downward pressure on the overall intertemporal discount rate of the economy. Then, the country’s population can offset this pressure to an extent by dissaving privately, a behavior known as the Ricardo-Barro effect. To see if this happens, the estimation of the intertemporal discount rate is done for a country with an oil fund (Norway) and, separately, a country that produces a comparable amount of oil, but does not have an oil fund (The United Kingdom).====Equity home bias is the tendency of asset managers to weigh their portfolios toward assets located in their home country. This paper estimates the home bias for Norway, a country that produces and exports oil and runs current account surplus, and for the UK, which produces a similar amount of oil, but mostly for domestic use, and does not maintain a current account surplus. This analysis helps determine whether the equity home bias is the reason why Norway had been producing and exporting oil since the late 1970s, but began accumulating claims on foreign assets only in the 1990s.====The lag between the discovery and the accumulation of wealth can be due to the failure of the oil firms to instantly begin producing the discovered oil. To see how this affects the current account, the model includes a Calvo-style process of oil production in which only a fraction of firms are able to increase oil production in response to a discovery shock at any given time period.====Estimating the effects of policy and technological factors responsible for the formation of the net international investment position (NIIP) is instructive for predicting which set of policies to adopt for a desired effect: to maximize the size of the surplus, maximize its longevity, or achieve the benefits of resource wealth sooner. Other considerations include modifications of the Taylor rule and mitigation of external imbalances.====This is the first paper to estimate the effect of an oil discovery within a dynamic stochastic general equilibrium (DSGE) framework. In a small open economy (SOE) DSGE model with resources, discovery shocks create a long-term cycle of external borrowing, repayment, and savings. Existing macroeconomic literature regularly covers macroeconomic effects of oil price shocks (Kilian et al. (2009); Backus and Crucini (2000); Bodenstein et al. (2011) and, more recently, Baas and Belke (2018)), and this work complements it by analyzing both the discovery and price shocks.====This work is likely the first to implement endogenous optimal production of a finite oil reserve. Oil discoveries cause decades-long changes in macroeconomic behavior. An endogenous profit-maximizing oil sector allows an otherwise basic DSGE model to reproduce the direction, timing, and magnitude of the current account response to a discovery. This is radically different from an oil price spike, which creates an uptick in export revenue and only has short-term implications for the general economy.====Both the oil price shocks and discoveries continue to influence major economies. The ongoing US tight oil boom and the Canadian tar sands in 2000s are the recent examples. The case of the North Sea oil discoveries, starting in the 1970s, makes available forty-three years of annual oil sector and macroeconomic data for Norway and the UK, which split the bulk of the discovery roughly equally. The North Sea oil had starkly contrasting effects on the two countries: Norway has accumulated an oil fund of claims on foreign assets, while the UK’s international investment is roughly balanced. However, the model shows that both of these outcomes are consistent with the predictions of a SOE DSGE model with a resource sector. The difference is primarily due to Norway exporting most of its oil during the period of high oil prices, in contrast to the UK using most of its oil in domestic production.====The remainder of the paper has the following structure. Section 2 lays out the DSGE model and the estimation procedure. Section 3 uses the North Sea data and results of the estimation to identify the main factors shaping the dynamics of the current account after an oil discovery. Conclusion is in Section 4.",Long-term implications of oil discoveries for international saving in a DSGE model,https://www.sciencedirect.com/science/article/pii/S0164070420301919,18 November 2020,2020,Research Article,148.0
"Cusato Novelli Antonio,Barcia Giancarlo","Department of Economics, Universidad del Pacífico, Sánchez Cerro 2050, Jesús María, Lima, Perú,Department of Economics, University of California, Santa Cruz, USA","Received 28 February 2020, Revised 15 August 2020, Accepted 12 October 2020, Available online 12 November 2020, Version of Record 9 December 2020.",https://doi.org/10.1016/j.jmacro.2020.103263,Cited by (1),"Of the different types of government outlays, since the 2000s public investment has been the main variable of adjustment during recessions in advanced and emerging economies. These contractions (expansions) have been associated with relatively medium-high (low) sovereign spreads, especially in advanced economies. To rationalize these issues, we develop a model of fiscal policy and ====, with corporate default risk. Policymakers must decide between the provision of an unproductive public good and public investment, weighting their respective net benefits in terms of short-term stabilization and debt sustainability. In our model, investment follows a countercyclical stance only in the case of low levels of debt and moderate negative shocks, and otherwise contracts during recessions. The policy stance, along with the mix between different outlays, is determined by how sovereign risk responds to adverse economic shocks.","In the context of financial crises, the burden of sovereign debt affects macroeconomic outcomes and policy responses. Romer and Romer (2019) show that OECD countries with lower debt-to-GDP ratios present smaller recessions than countries with higher leverage ratios, thanks to a more aggresive countercyclical fiscal response.==== In this paper, we analyze the behavior of different types of government expenditures around the time of economic downturns, since the begining of the century. In advanced and emerging economies, public investment is the main variable of adjustment, while other types of outlays do not conform to the same procyclical pattern. Then, we provide evidence of public investment contractions when sovereign spreads are relatively high, and of expansions under the opposite scenario, especially in advanced economies. To shed light on these issues, we use a fiscal policy model of sovereign debt and default in which credit to firms is risky, and study how the response of public investment during difficult times depends on the size of the debt-to-GDP ratio and the severity of the shock. In our model, sovereign risk plays a key role in determining a procyclical or countercyclical public investment stance.====In the first section of the paper we look at the evolution of different categories of public expenditures (over output) around recession windows (centered on output troughs). The sample includes a group of advanced and emerging markets, with data availability for public investment and sovereign spreads. Our exploration of recession windows shows that at the output trough, on average, the ratio of public investment over output falls by 0.32 percentage points; resources allocated to social benefits and subsidies rise by 1.35 percentage points; and we do not find conclusive results in the case of compensation to employees. These three categories account for approximately two-thirds of public outlays.====The contraction in public investment occurs between one year before the output trough and at the trough itself. On this basis, we calculate the change in government outlays between two points in time: two years before the trough, and at the trough. Then we analyze whether these variations were correlated with a proxy of the initial fiscal space (we use five-year sovereign CDS spreads at two periods preceding the trough). We find that before economies hit the bottom of the recession: (i) government spending cuts (as a percentage of output) are implemented mainly via public investment (reductions in the ratios of social benefits-subsidies and wages are unusual events); (ii) public investment cuts are associated with relatively high sovereign risk (limited or no fiscal space), while public investment expansions are implemented when sovereign risk is low (which indicates the existance of fiscal space); and (iii) in advanced economies with little fiscal space, investment cuts give room for the expansion of social benefits-subsidies. Our results for advanced economies are clearly driven by the stimulus experience during the Great Recession, while the contractionary public investment episodes correspond to recessions postdating 2009. This change of policy stance has been widely discussed; see for example Alesina et al. (2015) or Callegari et al. (2017).====In the following sections, we build a model of fiscal policy and sovereign default to account for these facts. A first key element of the model is that sovereign spreads are endogenous, since governments can choose to repay or default on their previous obligations and international investors price long-term sovereign bonds based on their expectations about repayment (Eaton, Gersovitz, 1981, Aguiar, Gopinath, 2006, Arellano, 2008, Chatterjee, Eyigungor, 2012). In fiscal policy models of sovereign default (Cuadra, Sanchez, Sapriza, 2010, Hatchondo, Martinez, Roch, 2015, Arellano, Bai, 2017, Roch, Uhlig, 2018) governments pursue austerity when things turn bad, via cuts in the provision of a public good. The second key element, and the main difference between our model and the literature on sovereign default, is the introduction of public capital and financial frictions in the private sector of the economy.====The inclusion of public capital, in addition to the standard element of public consumption, affects the set of goals that policymakers might pursue. Now, governments face the tradeoff between the provision of an unproductive public good (short-term welfare stabilization) and productive public spending (with potential medium run benefits in terms of debt stabilization). Since public capital is an input of production for private firms, investing more today raises future private production and future tax collection, thus increasing the chances of repayment. However, the decision to implement a countercyclical fiscal policy must account for the negative effects of the recession, which, in addition to the fact that more investment might be debt-financed, increases the burden of future repayment obligations, reducing the chances of repayment. Hence, in the model, the reaction of sovereign spreads reveals whether the potential debt stabilization benefits outweight the associated costs of the increase in public investment. Asonuma and Joo (2020) also include public capital in a fiscal policy model of default with debt restructurings, but they do not aim to understand the heterogeneity of the policy stance, or its relation with spreads.====The aforementioned government tradeoff must also account for the potential effects on the private sector. In our model, the private sector side takes into account not only public capital, but also the possibility that heterogeneous firms borrow in international markets to finance the wage bill. Firms differ because they are subject to an idiosyncratic productivity shock. Following the current literature on sovereign and corporate debt, a group of firms with very low shock realizations default on their obligations. Hence, the model allows for a negative feedback loop between sovereign and corporate risk, as in de Ferra (2018) and Kaas et al. (2020). In these two papers the link between spreads is attributable to taxation (with an increase in tax rates during downturns), while in our setup it is public investment that plays the major role.====The results of the model show that governments can implement a countercyclical fiscal policy if the burden of debt is not high and negative shocks are not that strong. Because initial debt levels are low, there is fiscal space to increase government debt and finance more investment and the public good as a response to a negative shock, without increasing sovereign spreads to prohibited levels. More capital tomorrow yields more production and tax collection, and this effect outweighs the burden of higher repayment obligations tomorrow. A different scenario arises if initial debt levels are higher or the recession is too deep; when this occurs, a deterioration in economic conditions triggers a strong increase in spreads. The optimal government response is to aggresively cut public investment, in order to smooth out the drop in public consumption. These two scenarios provide a rationale for the behavior of public investment and social benefit-subsidy expenditures in advanced economies during the recession of 2009 (with fiscal space) and the subsequent economic contractions (around 2013 and with less fiscal space).====The third scenario in the model corresponds to an economy with relatively large repayment obligations subject to a significant unexpected negative shock. As in any model of sovereign default, the government ends up defaulting on its obligations. Under any of the previous scenarios, the model yields a positive association between sovereign spreads and corporate risk premium. This is consistent with the evidence related to both types of yields presented by Agca and Celasun (2012), Klein and Stellner (2014), and Augustin et al. (2018), among others. Moreover, as Kaas et al. (2020) shows, the feedback mechanism brings the model statistics closer to the data.====A question worth highlighting is why the proposed mechanism is not present in most of the fiscal policy models of sovereign default (Cuadra, Sanchez, Sapriza, 2010, Hatchondo, Martinez, Roch, 2015, Arellano, Bai, 2017, Roch, Uhlig, 2018). Intuitively, this is simply because increasing public consumption in this setup does not yield important benefits in terms of debt stabilization (see Bianchi et al. (2019) for a model in which this statement does not hold); or public consumption mainly affects the expenditure side, but not the revenue side, of the government’s budget constraint. This point is relevant because the tradeoff between short-term stabilization and debt sustainability has been present in the recent fiscal policy discussions for the Euro area (E.Commission, 2016, ECB, 2016, Ferdinandusse, Checherita-Westphal, Attinasi, Lalouette, Bańkowski, Palaiodimos, Campos, 2017, IMF, 2017, IMF, 2019, Campos, Checherita-Westphal, Jacquinot, Burriel, Caprioli, 2019). Our contribution takes one step toward understanding this tradeoff, since we abstract from an important closely related channel: the effect on private investment and growth.====We contribute to the sovereign debt literature by showing the debt dependence of the fiscal policy stance, via different public investment behaviors. Our work is along similar lines to two preceding papers. Bianchi et al. (2019) rationalize the debt dependence of the fiscal policy stance through a sovereign default model with heterogenous households, downward nominal wage rigidities, a fixed exchange rate, and optimal fiscal policy. They show that for low debt, a debt-financed stimulus via public consumption yields positive aggregate demand effects (reduction in unemployment and inequality) that outweigh the costs in terms of sovereign risk. One difference between Bianchi et al. (2019) and our paper, is that public consumption stabilization benefits capture the textbook Keynesian mechanism, while we aim to model a more direct channel in terms of debt stabilization. Anzoategui (2019) does not model optimal fiscal policy, and estimates fiscal rules using historical data in the context of a sovereign default model. The author establish a set of conditions under which fiscal austerity policies can worsen recessions.====Moreover, our paper follows the contribution of Gordon and Guerron-Quintana (2018), who develop a sovereign default model for a small open economy with aggregate capital (using a planner’s version). In their setup, fiscal policy is absent and the main focus is on the business cycle properties. In addition, they discuss how aggregate capital accumulation yields counteracting forces that affect the default decision. In a paper closely related to ours, Asonuma and Joo (2020) study the evolution of public investment and consumption around debt settlements. Their model captures the sovereign debt overhang in terms of public investment, showing its severe decline and slow recovery up to the point of restructurings. When it comes to their empirical facts, they observe that recessions associated with default episodes present deeper and more protracted declines in public investment than non-crisis recessions—although they do not present evidence on sovereign spreads.====In addition to sovereign default papers, our work is closely related to the literature on fiscal procyclicality in emerging markets (Gavin, Perotti, 1997, Talvi, Vegh, 2005, Kaminsky, Reinhart, Végh, 2004, Ilzetzki, Vegh, 2008), providing a mechanism that explains the heterogenity of fiscal responses during recent decades (Frankel et al. 2013). Naturally, there is also a connection with the empirical literature on fiscal multipliers (see Ramey (2019) for a survey), and in particular with those recent contributions that stress the heterogeneity of government outlays and public investment (for example, see Alloza et al. (2019) for the Euro Area case and Izquierdo et al. (2019) for emerging markets).====This paper is organized as follows. In Section 2 we discuss a set of empirical regularities about the evolution of public outlays during recessions and the initial fiscal space. In Section 3 we describe the model. In Section 4 we present the quantitative analysis of the model, as well as evaluating the main trade-offs and debt dependence of fiscal policy. In Section 5 we conclude.","Sovereign Risk, Public Investment and the Fiscal Policy Stance",https://www.sciencedirect.com/science/article/pii/S0164070420301877,12 November 2020,2020,Research Article,149.0
"Li You,Tay Anthony","School of Business, Macau University of Science and Technology, Avenida Wai Long Taipa, Macau, China,School of Economics, Singapore Management University, 90 Stamford Road, Singapore 178903, Singapore","Received 3 September 2019, Revised 28 October 2020, Accepted 5 November 2020, Available online 9 November 2020, Version of Record 17 November 2020.",https://doi.org/10.1016/j.jmacro.2020.103266,Cited by (2),"We explore empirically the role of ==== and policy uncertainty in explaining dispersion in professional forecasters’ density forecasts, and in explaining individual forecaster uncertainty (defined as the uncertainty expressed by individual forecasters in their density forecasts). We focus on US real output growth and ====, using data from the Philadelphia Fed's quarterly Survey of Professional Forecasters (SPF), 1992-2016. We find that dispersion in individual density forecasts is related to ==== uncertainty, especially in longer horizon forecasts, but not policy or forecaster uncertainty. There is also little evidence that forecaster uncertainty reflects macroeconomic or policy uncertainty.","Surveys of professional macroeconomic forecasters show that forecasters generally disagree with each other. This is true for both point and density forecasts, both of which are typically dispersed. Pervasive evidence of forecast dispersion, even among a group of economic agents who should be reasonably homogeneous in terms of ability and motivation, suggests that expectations may in general be heterogeneous. Empirical investigations into point and density forecast dispersion can help us better understand the formation of expectations and the underlying reasons for heterogeneity, with follow-on implications for how expectations can affect business cycles, and the effectiveness of policy. Dispersion in point forecasts has been well-studied, but dispersion in density forecasts less so. Studying dispersion in density forecasts, in particular, may help us better understand the connections and relationships between individual forecaster uncertainty and uncertainty in the macroeconomic environment, and how they relate to disagreement among forecasters in particular. This is the main focus of our study.====Dispersion of (point) forecasts is often used as a proxy for uncertainty, whereas density forecasts appear to offer a direct measure of forecaster uncertainty. An important part of understanding forecast dispersion is to see how it differs from individual forecaster uncertainty, as expressed in their density forecasts. This a second focal point of our study.====As an example of dispersion in point forecasts, we show in Fig. 1 point forecasts for annual 1996 US PGDP (GDP price index) inflation made by economic forecasters surveyed by the Philadelphia Fed in their quarterly Survey of Professional Forecasters (SPF) over the period 1995Q1 to 1996Q4, i.e., these are forecasts made at horizons of 7 quarters down to 0 quarters. The point forecasts are dispersed at long horizons, with dispersion falling as the forecasters approach the full realization of the forecast event. Some persistence in the forecasts can be observed, with relatively optimistic and pessimistic forecasts tending to remain so. Similar patterns can be observed in many other similar datasets. Explanations put forward for these and other patterns in dispersion of point forecasts include the use of different information sets by forecasters, perhaps due to different degrees of information rigidities among them (Mankiw et al., 2003), different interpretation of information by forecasters (Kandel and Zilberfarb, 1999; Manzan, 2011), different loss functions among forecasters (Capistran and Timmermann, 2009), and different priors held by forecasters regarding the unconditional distribution of the variables being forecasted (Patton and Timmermann, 2010).====The objective of this paper is to study patterns of dispersion in density forecasts. In addition to point forecasts, forecasters participating in the SPF are also asked to provide density forecasts in the form of histograms. In each survey, forecasters are given a set of intervals and asked to provide for each interval an estimate of the probability with which the variable's realization is expected to appear in that interval. Fig. 2 displays an individual forecaster's density forecast of growth in PGDP inflation for the year 1996, taken from the 1996Q2 survey.====In order to show dispersion in density forecasts over forecasters, we derive, for each individual density forecast, the median, central 0.90 probability interval, and a measure of skewness, and we study dispersion in these descriptive statistics. The construction of these statistics (or ‘characteristics’) is illustrated in Fig. 2, and discussed in more detail in later sections. The panels in the right column of Fig. 1 plot these statistics for individually reported density forecasts of 1996 annual PGDP inflation, taken from the same set of surveys as the point forecasts in the left panel. The dispersion patterns in the medians match those of the point forecasts. The dispersion patterns in the range and asymmetry of the density forecasts are less clear, but there is certainly dispersion. Density forecasts, of course, offer us the potential of observing a forecaster's expectations in a more complete form. The spread of a density forecast might be considered a possible direct measure of the level of individual uncertainty perceived by the forecaster. Asymmetries in the density forecasts, for a given level of uncertainty, may indicate some degree of optimism or pessimism.====The plots in Fig. 1 focus on the change in dispersion with respect to forecast horizon, but there is also considerable variation in dispersion over time, as can be observed in Fig. 3, which plots the medians of all of the density forecasts of PGDP inflation in our chosen sample period.====The primary objective in this paper is to understand the time variation in dispersion of the medians, spread, and skewness of the density forecasts reported by forecasters. In particular, we are interested in the role of forecaster uncertainty, and uncertainty in the macroeconomic and policy environment on dispersion among individual density forecasts. Throughout we make a clear distinction between forecaster uncertainty versus uncertainty in the macroeconomic environment. It is generally accepted that there is time-variation in the volatility of macroeconomic variables, so that these variables are easier to predict in some periods, but harder to predict in others. However, it is not difficult to imagine an overconfident forecaster who always issues very narrow density forecasts. As part of the broader concept of uncertainty in the macroeconomic environment, one might also include policy uncertainty, which again is not the same as forecaster uncertainty or variations in predictability. We use density forecasts to construct a measure of forecaster uncertainty, and take advantage of recently developed indices of macroeconomic uncertainty, which focus on whether the economy has become more or less predictable (Jurado et al., 2015), and of policy uncertainty, that rely on the prevalence of ‘uncertainty’ keywords in news articles (Baker et al., 2016). We correlate our measures of dispersion in density forecasts with these direct measures of uncertainty.====Our work differs from much of the forecast dispersion literature in that we explore dispersion of density forecasts, and not point forecasts. While there are several papers that have documented dispersion of density forecasts (Boero et al., 2008), the literature has in general focused on explaining the behavior of individual uncertainty (Lahiri and Liu, 2006). Our interest is in the behavior of dispersion in the location, spread, and skewness in individual density forecasts, using the average levels of these as well as indices of macroeconomic and policy uncertainty as explanatory variables. Whereas most studies focus on inflation expectations, we also study output growth expectations; it turns out that there are some interesting differences in the behavior of the two. This paper is also closely related to research aimed at establishing whether or not point forecast dispersion is a good proxy for forecaster uncertainty (Zarnowitz and Lambros, 1987; Giordani and Soderlind, 2003; Rich and Tracy, 2010; Boero et al., 2008, 2015), which boils down to asking if dispersion can explain individual uncertainty (the general consensus appears to be, mostly, ‘no’.) The objective of our exercise, on the other hand, is to see if various forms of uncertainty can explain forecast dispersion, which we take to be a reflection of heterogeneity in expectations. We find, in general, that dispersion is correlated with macroeconomic uncertainty, less so with policy uncertainty, and not correlated with forecaster uncertainty.====In the next section, we describe the SPF survey dataset briefly, focusing on the density forecasts and the percentile-based summary statistics that we use to characterize them. We also describe the patterns of dispersion in these summary statistics. We take a closer look at forecaster uncertainty in Section 3, and its relationship to the macroeconomic uncertainty and policy uncertainty indices. Our main results regarding dispersion are reported in Section 4, and Section 5 concludes.",The role of macroeconomic and policy uncertainty in density forecast dispersion,https://www.sciencedirect.com/science/article/pii/S0164070420301907,9 November 2020,2020,Research Article,150.0
"Abildgren Kim,Kuchler Andreas","Danmarks Nationalbank, Langelinie Allé 47, DK-2100 Copenhagen Ø, Denmark","Received 2 April 2020, Revised 5 October 2020, Accepted 26 October 2020, Available online 2 November 2020, Version of Record 7 November 2020.",https://doi.org/10.1016/j.jmacro.2020.103264,Cited by (10),"Households' inflation perceptions and expectations play a key role in many dynamic macroeconomic and monetary models and are important for the ability of central banks to reach their objective of price stability. This paper revisits the issue of overestimation bias in inflation perceptions and expectations based on new and unique microdata from the Danish part of the EU-Harmonised Consumer Expectations Survey linked to rich household-level data from administrative registers. The analysis shows that accounting for even several of the household characteristics and social gradients usually addressed in the literature is far from sufficient to explain the inflation perception bias. Furthermore, we find that respondents participating in the survey more than once tend to be persistent in their degree of perception bias and that overpessimistic households have larger perception bias than other households. This indicates that inflation perception bias is related to fundamental personality traits. Finally, households' expectations of the future inflation level tend to be mean reverting and associated with the same types of bias as inflation perceptions.","Inflation is a complicated social phenomenon, and households' inflation perceptions and expectations play a key role in state-of-the-art dynamic macroeconomic and monetary models. In representative agent models, households' consumption expenditures are typically determined by nominal income, nominal interest rate, price level and inflation expectations via an Euler equation. Inflation perceptions and expectations are also of crucial importance for households' investment decisions, and both consumption and investment are central to the working of the monetary-policy transmission channel as well as the size of automatic stabilizers. Although still mostly on the ""to-do list"" within the frontier of macroeconomic modelling, heterogeneity in inflation perceptions and expectations might also turn out to matter significantly for the outcome in heterogeneous-agent models (Woodford, 2003; Lein and Maag, 2011; Galí, 2014; Coibion et al., 2018; Kaplan and Violante, 2018; Falck et al., 2019; Acharya and Dogra, 2020).====Most macroeconomic models still rely on the assumption of ""rational"" expectations where the households know the ""true"" economic model and don't make any systematic expectation errors. However, a broad range of survey-based evidence indicates that households' inflation perceptions and expectations persistently exceed the inflation level measured by official consumer price indices. Households' inflation perceptions and expectations are not only of interest for a general understanding of the working of the economy but also for the fundamental objective of price stability pursued by central banks. If there is a wide gap between perceived and actual inflation, it might affect the credibility of a central bank and thereby its ability to reach its monetary-policy objectives (Nautz et al., 2017; Buono and Formai, 2018). The ""inflation perception conundrum"" is for example of key importance to the discussion regarding whether central banks should increase their inflation targets to reduce the risk of hitting the effective lower bound for policy interest rates in future recessions. If there is a substantial upward bias in households' inflation perceptions and expectations, there might not have been a real risk of a deflationary spiral during the downturn in the wake of the recent global financial crisis. This issue also relates to the more fundamental question: Whose inflation expectations matters for monetary-policy transmission and the reaction function of central banks? The households' (and firms') or the professional forecasters' (Ascari and Sbordone, 2014; Afrouzi et al., 2015; Coibion et al., 2020)?====Since 2003, the European Commission has collected data on consumers' quantitative estimates and expectations of the past and future inflation level as part of the EU-Harmonised Consumer Expectations Survey. However, it was only in the first part of 2019 that the European Commission began to publish the results at a regular basis, cf. European Commission (2019). In line with other surveys, the results of the survey show that the level of inflation perceived by European households persistently exceed the observed inflation level measured by the official Harmonised Index of Consumer Prices (HICP). For the Euro area, the year-over-year HICP inflation rate was 1.5% in the third quarter of 2019 whereas consumers on average perceived year-over-year inflation to be 7.2%.====Data on quantitative inflation perception and expectations from the EU-Harmonised Consumer Expectations Survey have previously been studied by Lindén (2005), Biau et al. (2010), European Commission (2014) and Arioli et al. (2017). In this paper, we revisit the issue of overestimation bias in inflation perceptions and expectations based on confidential microdata from the Danish part the EU-Harmonised Consumer Expectations Survey 2007–2016 merged with household-level information from a wide range of Danish administrative registers.====Our study is the first analysis of the European inflation perception survey data which utilise the possibility of linking the individual respondents' answers in the survey to rich microdata from administrative registers. We have therefore not to rely on e.g. respondents' self-reported information on income, etc. The Danish register data are primarily based on third-party reporting and therefore commonly believed to be of a very high quality (Kleven et al., 2011). Furthermore, the link to register data provides a comprehensive and previously unexplored household-level data set that allows us to simultaneously study many of the drivers of inflation perception and expectation bias usually put forward in existing literature as well as new factors and dimensions not previously analysed due to data limitations.====Our analysis confirms the importance for the size of the overestimation bias in inflation perceptions of a range of well-established factors such as income, age, education and gender. We also confirm that part of the bias reflects that the respondents may have another price concept in mind (food prices) rather than consumer prices in general when interviewed about the level of inflation. However, accounting for even several of the above mentioned factors simultaneously is far from sufficient to explain the inflation perception bias. Furthermore, house price inflation does not contribute to explaining the level of perceived inflation.====We are also able to use the time series dimension of our household-level register data to explore the significance of overpessimism for the inflation perception bias which – in contrast to ""pure"" optimism or pessimism – has not previously been examined in the literature. We find that overpessimistic households have a significantly larger perception bias than other households. This indicates that the inflation perception bias is related to fundamental personality traits. This conclusion is further underlined by our finding that respondents participating in the survey more than once tend to be persistent in their degree of perception bias.====We find a strong correlation between inflation perception bias and inflation expectation bias at a household level. The bias in inflation expectations is much smaller for households' expectations regarding the future change in inflation than for households' inflation expectations in levels, even if one adjusts the latter by a constant reflecting the general overestimation of the level of inflation. These findings suggest that the expected change in inflation might be a better measure of households' inflation expectations than other simple measures when using indicators from the EU-Harmonised Consumer Expectations Survey in empirical works.====Consequently, we proceed with an assessment of the anchoring of inflation expectations in the household sector based on expectations regarding future changes in inflation. We find that if inflation has recently increased, households expect this trend to reverse in the future, and if inflation has recently decreased, households to a larger degree than in other periods expect that inflation will increase in the future. Similar results have previously been found for gas prices (Binder, 2018). Although we find some degree of heterogeneity in the extent to which different household groups have mean reverting inflation expectations, the mean reversion indicates that households' inflation expectations are well-anchored despite the bias in expectations of the future inflation level.====However, even when expected changes in inflation are considered, there are still substantial outliers that need to be addressed, and changes to the survey design might be a way forward in this area. Our empirical analysis shows that households with a relatively accurate perception of the current level of inflation do not overestimate future inflation to the same extent as households with less accurate perceptions of current inflation. This suggests that knowledge of current inflation is important for the accuracy of expectations regarding inflation in the future. Implementing some form of guidance on the current or typical rate of inflation in survey questions might therefore reduce the widely observed positive bias in inflation expectations.====The remainder of this paper is organised as follows. In Section 2, we offer an overview of our findings in relation to earlier research followed by a description of our data set in Section 3. In Section 4, we explore the importance of price concept, household characteristics and social gradients and show that accounting for such factors is far from sufficient to explain the inflation perception bias. In Section 5, we turn our attention to personality traits and quantify the significance of overpessimism in relation to the perception bias followed by an examination of the link between bias in inflation perception and bias in inflation expectations in Section 6. Section 7 concludes and offers some thoughts on further research.",Revisiting the inflation perception conundrum,https://www.sciencedirect.com/science/article/pii/S0164070420301889,2 November 2020,2020,Research Article,151.0
"Akcay Mustafa,Elyasiani Elyas","Citigroup, New York, NY,Fox School of Business and Management, Temple University, Philadelphia, PA 19122","Received 15 July 2018, Revised 24 October 2020, Accepted 26 October 2020, Available online 31 October 2020, Version of Record 12 November 2020.",https://doi.org/10.1016/j.jmacro.2020.103265,Cited by (0)," risk and credit risk, without increasing systemic risk.","Since the financial crisis of 2007–2009, academics and practitioners have sought to define and measure “systemic risk” and to identify the factors that contribute to it, in order to better assess the vulnerabilities of the financial system.==== Monetary policy affects the banking sector by encouraging or discouraging risk-taking by banks and alters borrower behavior by making it harder or easier to refinance or pay back existing loans. We investigate the impact of deviations of the primary monetary policy interest rate, the target federal funds rate (FFR), from its benchmark rate, the Taylor rule rate (TRR), on banking system distress. The target FFR is set by the Federal Open Market Committee (FOMC). The Fed used open market operations to drive the FFR towards the target rate before it began paying interest on excess reserves in 2008Q4. The Taylor rule, proposed by John Taylor (1993, 1999) to serve as a benchmark rate for monetary policy, prescribes how a central bank should adjust its interest-rate policy in response to ==== and macroeconomic ==== to achieve price stability and full employment.====The recession of 2007–2009 marked the steepest downturn in the US economy since the Great Depression, according to several indicators.==== Policy responses were also extraordinary. The Fed used the FFR as a primary policy tool until FFR hit the zero lower-bound (ZLB) in 2008-Q4. It then introduced “Quantitative Easing (QE)”, and forward guidance programs to add liquidity to the market, to reduce long-term rates in order to spur loan demand, and to, thereby, stimulate the economy.==== It is claimed that QE implies further cuts in the FFR (Bernanke 2011) and variants of ==== interest rates have been proposed to account for cuts below the ZLB (Bauer and Rudebusch, 2013; Christensen and Rudebusch, 2014; Lombardi and Zhu, 2014; Wu and Xia, 2016). The concept of shadow rate emerged as a powerful tool to summarize useful information at the ZLB and to describe the monetary policy stance below the ZLB (detailed later).====While the Fed implements the FFR as the main policy rate, debate around the “normative” policy rate has continued. No central bank follows the Taylor rule strictly, but a variant of the Taylor rule provides guidance for many policymakers. Thus, Taylor rule rates (TRR) are often used to analyze actions of the central banks (Bernanke, 2015; Neely, 2015; Dudley, 2010; Kahn, 2010; Lubik and Schorfheide, 2007; Clarida et al., 1998). Policy rate deviations from the benchmark (PRD), measured by the spread between the FFR and the TRR, are described as either implementing a tight or a loose monetary policy depending on whether the former exceeds or falls short of the latter.====To gage the impact of the policy stance on the banking system distress, we develop a systemic risk indicator which follows an Expected Shortfall (ES) concept, similar to the one proposed by Acharya et al. (2010, 2017). The ES concept uses conditional expectations of losses due to bank default under extreme conditions, where the shortfall refers to the shortage of capital (or a hypothetical insurance premium) needed to offset the losses. Our systemic risk indicator is the ratio of total bank losses to banking system liabilities. Since these liabilities are the assets of households and firms, the systemic risk indicator shows the share of total households’ and firms’ assets lost in case of a systemic event.====We use a ==== FFR proposed by Wu and Xia (2016) as the primary policy rate to expand the FFR data to the period of ZLB. We use the original Taylor rule rate (1993) and five variants of the rule as a benchmark for the monetary policy rate. Our model creates impulse response functions (IRF) by using the local projection method proposed by Jorda (2005), to be detailed later. Our base model includes a ==== and the spread between FFR and the variants of the TRR, as the deviation of the policy rate from the benchmark and examines the association between these deviations and systemic risk between 2001 and 2017.",The link between the federal funds rate and banking system distress: An empirical investigation,https://www.sciencedirect.com/science/article/pii/S0164070420301890,31 October 2020,2020,Research Article,152.0
"de Groot Oliver,Motto Roberto","University of Liverpool Management School & CEPR, Liverpool, UK,European Central Bank, Frankfurt, Germany","Received 1 October 2021, Revised 4 October 2021, Accepted 4 October 2021, Available online 19 October 2021, Version of Record 26 October 2021.",https://doi.org/10.1016/j.jmacro.2021.103375,Cited by (0),None,"While optimal control theory in the academic study of monetary policy can be traced back at least to the 1970s (Peterson and Lerner, 1971), it was not until 2012 that Janet Yellen—the then Vice-Chair of the Federal Reserve Board (henceforth, the Fed)—began to popularize in speeches the use of optimal control in the practice of monetary policy (Yellen, 2012).====While optimal policy simulations have the potential to provide valuable insights for policymakers regarding the trade-offs faced in the conduct of monetary policy, at least four significant conceptual and methodological challenges exist. The first is the introduction of new policy instruments to the central bank’s toolkit. In addition to the short-term policy rate, central banks in the past decade have added quantitative easing, forward guidance, and macroprudential policy to their toolkits, to name but a few. Yet the optimal mix of instruments remains uncertain.====Second, the frequent visits of nominal policy interest rates to their effective lower bound (ELB) has introduced unavoidable nonlinearities, making the traditional linear–quadratic approach to the study of optimal policy less appropriate for addressing contemporary monetary policy problems.====Third, forward guidance as a policy instrument has highlighted the fact that the canonical New-Keynesian model, and many of its DSGE offspring, suffer from a puzzle—that forward guidance is unconvincingly powerful in this class of models, where monetary policy can control aggregate demand today using small promises about policy very far in the future. This puzzle requires a rethinking of many of the cornerstones of the canonical model including the representative agent, rational expectations, and credibility.====Finally, the persistence of low interest rates and the aggressive use of quantitative easing in the UK, euro area (EA) and the US has placed the potential consequences of monetary policy for redistribution and inequality into sharp public focus. Heterogeneous-agent models are rapidly advancing to address these issues but remain fledgling, with serious computational challenges hindering the way.====This special issue brings together research at the forefront of this new agenda, addressing some of the significant challenges outlined above.==== In particular, it brings together work from 23 international experts from North America to Europe spanning academia and central banking into 8 original research articles.",Guest editors’ introduction: Optimal monetary policy: Theory and practice,https://www.sciencedirect.com/science/article/pii/S0164070421000744,19 October 2021,2021,Research Article,154.0
"Miller Stephen Matteo,Ndhlela Thandinkosi","Mercatus Center, George Mason University, 3434 Washington Blvd., 4th Floor, Arlington, VA 22201,Monash University Wellington Road Victoria, 3145 Australia","Available online 6 September 2021, Version of Record 6 September 2021.",https://doi.org/10.1016/j.jmacro.2021.103367,Cited by (0),None,None,Corrigendum to “Money Demand and Seignorage Maximization Before the End of the Zimbabwean Dollar”,https://www.sciencedirect.com/science/article/pii/S0164070421000677,6 September 2021,2021,Research Article,155.0
"Adler Gustavo,Mano Rui C.","International Monetary Fund, 700 19th street N.W., Washington, DC 20431, United States","Received 25 January 2018, Revised 11 May 2018, Accepted 5 July 2018, Available online 2 August 2018, Version of Record 23 February 2021.",https://doi.org/10.1016/j.jmacro.2018.07.001,Cited by (10),"Foreign exchange intervention led to a sizable expansion of many central banks’ balance sheets over the last decade, raising questions about the associated fiscal costs. This paper clarifies conceptual issues about how to measure these costs both from an ex-post and an ex-ante (relevant for decision making) perspective, and estimates both marginal and total costs for 73 countries over the period 2002–13. Averaging across various estimation methods, we find ex-ante ==== costs in the inter-quartile range of 2.0–5.5% per year for emerging market economies. Reflecting differences in the accumulation of foreign exchange, ex-ante ==== costs amounted to 0.2–0.7% of GDP per year in countries with limited intervention, while reaching 0.3–1.2% of GDP per year in heavy-intervening economies. These estimates indicate that fiscal costs of sustained FX intervention are not negligible.","Over the last decade, many central banks both in emerging market economies (EME) and advanced economies (AE) resorted to sizeable foreign exchange intervention (FXI) operations in an effort to cope with the effects of large capital inflows or sizable terms-of-trade shocks, while trying to maintain monetary policy independence. Despite its widespread use, however, the merits of using FXI, including in comparison to other policy instruments, remain a matter of significant debate. Several recent studies have delved on the use of FXI as an additional policy instrument, focusing primarily on the effects on the exchange rate (e.g., Adler et al., 2015, Blanchard, 2015a, Daude et al., 2014, Fratzscher et al., 2015, Adler and Tovar, 2014) and the macro-economy (Ostry et al., 2011, Ostry et al., 2016, Blanchard et al., 2015b, Cavallino, 2016). There is also a vast literature on the benefits of conducting FXI intervention, both for mercantilist and precautionary motives. Yet, little attention has been given to the quasi-fiscal costs associated with these policies, despite the fact that gross positions in central banks’ balance sheets (Fig. 1) have increased rapidly as a by-product of FX intervention.====For the most part, there is a belief in policy circles that costs of holding FX positions (often mistakenly equated to the cost of holding international reserves, as discussed below) can be significant; while the academic literature has rarely focused on this aspect arguably because such costs are thought to be of second order importance. Surprisingly, none of these views is based on thorough empirical analysis of the fiscal costs of FXI, since this remains largely undone. As a result, these costs are poorly understood, particularly from an ex-ante perspective, which is the relevant point of view for decision making, as discussed further below.====Conceptually, quasi-fiscal costs were discussed in the early literature on FX intervention. Friedman (1953) argued that interventions could only be effective if they were profitable for the central bank, involving an arbitrage opportunity for the latter at the expense of private speculators. However, the subsequent development of the portfolio balance theory—pioneered by Henderson and Rogoff, 1982, Kouri, 1983 and Branson and Henderson (1985), and further developed recently by Kumhof (2010) and Gabaix and Maggiori (2015)—challenged this view by introducing the notion of a risk premium associated with imperfect asset substitutability and incomplete asset markets.====The empirical evidence, on the other hand, remains very scarce and mostly outdated. Early evidence (see Taylor, 1982, Sweeney, 1997, Neely, 1998) focused only on specific advanced economies, and showed that profit and losses varied widely across countries and time periods.==== More recent studies have documented these quasi-fiscal costs (International Monetary Fund 2011, World Bank 2013) but only for a limited group of countries and focusing solely on ex-post costs.====This paper attempts to fill this gap in the literature by clarifying key concepts related to the measurement of the costs of FXI, and documenting them for a large set of countries. In particular, we focus on:====Following the discussion of these conceptual issues, we document the marginal (per US$) and total costs of FXI (i.e., of rolling over the overall FX position), both from an ex-post and ex-ante perspective, for a set of 73 emerging and advanced economies over the period 2002–13.====We find that ==== costs have been relatively large due to sizeable deviations from uncovered interest rate parity (UIP) and elevated central bank FX positions during the sample period.====More importantly, from an ==== perspective, our estimates indicate non-negligible marginal and total costs. We find ex-ante ==== costs averaged across estimation methods in the inter-quartile range of 2.0–5.5% per year for the median EME and full sample period; while estimated ex-ante ==== costs (of sustaining FX positions) amount to 0.3–0.9% of GDP per year for the same sample. Nearly one fifth of the countries in the sample incurred in costs above 1% of GDP per year over the sample period; and more in recent years, as such costs increased significantly along with foreign asset holdings during the period of analysis. Heavy interveners (i.e., countries with a heavy degree of exchange rate management) incurred in ex-ante total costs of about 0.3–1.2% of GDP per year, compared to 0.3–0.7% of GDP by light interveners.====Marginal costs are also found to be significantly larger in EMEs than in AEs, suggesting that the conjectured greater effectiveness of FXI in EMEs is inherently associated with higher quasi-fiscal costs.====Overall, our estimates indicate that fiscal costs of sustained FX intervention are not negligible, and thus should be a factor to consider when assessing the desirability of these policies. This is, of course, only one of multiple aspects to take into account when conducting FXI (for example, the potential conflict with other policy objectives, the implications for private sector balance sheets and resulting vulnerabilities, etc.) which are beyond the scope of this paper.====The rest of the paper is organized as follows: Section 2 discusses key concepts and the definition of the cost of FXI. Section 3 documents these costs using various approaches. Section 4 concludes with the key takeaways.",The Cost of Foreign Exchange Intervention: Concepts and Measurement,https://www.sciencedirect.com/science/article/pii/S0164070418300387,2 August 2018,2018,Research Article,159.0
"Castillo Paul,Montoro Carlos,Tuesta Vicente","Pontificia Universidad Católica del Perú, Lima, Perú,Banco Central de Reserva del Perú, Peru,CENTRUM Católica Graduate Business School, Lima, Perú","Received 19 July 2019, Revised 8 August 2020, Accepted 29 September 2020, Available online 7 October 2020, Version of Record 26 October 2020.",https://doi.org/10.1016/j.jmacro.2020.103259,Cited by (9),"In a fully micro-founded New Keynesian framework, we characterize an analytical relationship between average ","The past fifty years were characterized by many periods of oil price shocks with different implications on both economic activity and inflation. For instance, during the 1970s following the oil price shock (1973 and 1979) inflation peaked substantially and GDP declined as well.==== Nevertheless, in the 2002 - 2008 period, the American economy has experienced an oil price shock of similar magnitude comparable to that of the 1970s, however, in distinction with the first episode both GDP growth and inflation have remained relatively stable. Similarly, if we further explore the link between oil price volatility (measured as the standard deviation of oil price) and both average inflation and output, we attain that large oil price volatility was associated to high levels of inflation and low levels of output during the 1970s, whereas this link seems to be broken down in the 2000s.==== These different episodes pose some questions regarding the link between oil price volatility and inflation: Does oil price volatility matters for the level of inflation?, what is the role of monetary policy considering the importance of the oil price volatility?====Blanchard and Gali (2008), hereinafter BG, addressed some of the above questions using a log-linear New Keynesian model that includes the role of oil as both a production factor and a component of the consumer price index. BG showed that a monetary policy improvement, good luck, more flexible labor markets and smaller share of oil in production have had an important role in explaining the different macroeconomic performance between the 1970s and 2000s. The log-linear approximation, however, might offer an incomplete solution when shocks are relatively large, substantially increasing the approximation error of the model’s solution. Indeed, as it is confimed by the empirical evidence, oil price shocks are hefty compared to other shocks usually appended in traditional new-Keynesian models (like monetary and productivity shocks). More importantly, the log-linear solution does not consider crucial channels through which oil price affects inflation, such as its own volatility, the precautionary behavior of price setters and the convexity of the Phillips Curve. Therefore, a log-linear solution can not tell us something about a link between oil price volatility and average inflation.====Our paper deals with the afore-mentioned limitations arising from log-linear approximations and provides a tractable and unified framework to analyze the effects of oil price volatility over average inflation. Our framework allows to shed additional light beyond those offered by log-linear solutions on the nature of the apparent changes in the macroeconomic effects of oil prices. We establish the link between oil price volatility and average inflation. In doing so, we use a standard New Keynesian micro-founded model where the central bank implements its policy following a Taylor rule. We modify this simple framework by considering oil as a production input for intermediate good which is difficult to substitute in production. Then, we analytically solve up to second order of accuracy the rational expectations equilibrium of this model using the perturbation method.====The second order solution has the advantage of incorporating the effects of shocks volatility in the equilibrium, which are absent in the linear solution. We implement this method both analytically and numerically. As part of our contribution, we use an original strategy to obtain an analytical solution. Different from other papers in the literature such as Aruoba et al. (2006) and Schmitt-Grohé and Uribe (2004) in which the perturbation method is applied directly to the non-linear system of equations, we first approximate the model up to second order and then apply the perturbation method to the approximated model. This strategy allows us to disentangle the key determinants of the relationship between oil price volatility and inflation, and to quantify the importance of each determinant in general equilibrium.====This paper offers a closed-form solution for the link between average inflation and oil price volatility: higher oil price volatility induces higher levels of average inflation. This link shifts with several factors. We show that when oil has low substitutability and monetary policy reacts more aggressively towards output fluctuations, oil price volatility has a larger impact over average inflation. Thus, both monetary policy and the properties of oil prices have important implications on the determination of the link between oil price volatility and average inflation.====Our basic finding indicates that the second order analytical solution allows us to establishing a link between oil price volatility and average inflation. Indeed, the solution up to second order shows that oil price volatility produces an extra level of inflation by altering the way in which forward-looking firms set their prices. This mechanism is absent in log-linear solutions and more importantly the link arises from the forward-looking behavior of the optimizing agents.====Regarding the sources of the link between oil price volatility and inflation, the analytical solution shows the following: first, when oil has low substitutability, marginal costs are convex in oil price; hence, its price volatility raises the expected value of marginal costs. Second, oil price volatility generates inflation volatility, thus inducing price setters to be more cautious to future expected marginal costs changes. In fact, producer relative prices become more sensitive to marginal costs, amplifying the previous channel. Third, relative price dispersion, by increasing the amount of labor required to produce a given level of output, increases average wages, thus amplifying the effect of expected marginal costs over average inflation. Fourth, in general equilibrium, the weights that the central bank assigns on inflation and output are key determinants of the level of inflation induced by oil price volatility. As a result, the model predicts that the smaller (larger) the endogenous responses of a central bank to inflation (output) fluctuations are, the greater the average inflation (smaller average output) is. Thus, changes in the way monetary policy is conducted may explain different responses of the economy to oil price volatility and oil shocks. This finding is consistent with the fact that, in the model, oil price generates a monetary policy trade-off between stabilizing inflation and output, which also implies a trade-off in average levels.====The simulated moments of both inflation and the real price of oil, in line with the analytical result, delivers a positive level of average inflation from non-demand oil shocks, which is also increasing with the volatility of the latter. Also, the decline in the oil price volatility during the second sub-period helps to explain the drop in average inflation compared to the previous sub-period. However, it fails to deliver the lower level of inflation observed during 2003:2008 in comparison with 1983:2002. This evidence suggests that other factors, such as monetary policy, could have been playing a role. For example, as reported by different authors such as Clarida et al. (2000), the larger response of monetary policy to higher expected inflation reduces the impact of oil price shocks on average inflation.====An implication of our results is that oil price volatility can shift the Phillips curve affecting the real cost of stabilizing inflation. In periods where oil prices volatility is low, it is less costly for central banks to stabilize inflation since for the same level of output gap inflation is lower than when oil price volatility is high. This result is similar to the one reported by Benigno and Ricci (2011). They found that when nominal wages exhibit downward rigidity and there exist idiosyncratic shocks in the labor market, output volatility shifts the Phillips curve and makes more costly wage inflation stabilization.====Other authors introduced the second order approach in closed and open economies; however, most of the studies have mainly focused on welfare evaluations across different environments. For example, Benigno and Woodford (2003) implemented the second order solution to evaluate optimal monetary and fiscal policy in a closed economy. Nakov and Pescatori (2010) proposed an extension of the standard NK model in which the presence of a dominant oil supplier (OPEC) leads to inefficient fluctuations in the oil price markup, reflecting a dynamic distortion of the economy’s production process. As a result, in the face of oil sector shocks, stabilizing inflation does not automatically stabilize the distance of output from first best, and monetary policymakers face a trade-off between the two goals. Montoro (2012) and Natal (2012) assume intermediate degrees of substitution between oil and labor to generate an endogenous trade-off between output gap and inflation. Benigno and Benigno (2003) used the second order approach to evaluate the optimal policy in a two-country model with complete markets. A study closer to our work, that uses a similar methodology, is the one of Obstfeld and Rogoff (1998) who developed an explicit stochastic NOEM model relaxing the assumption of certainty equivalence. These authors, based on simplified assumptions, obtained an analytical solution for the level nominal exchange rate premium. As opposed to the afore-mentioned authors, in this paper, we obtain an analytical solution for the extra level of inflation arising from the second order approximation using a fully micro-founded New Keynesian model with oil prices.====Our work is also related to an extensive literature that studies the macroeconomic effects of oil price shocks. Barsky, Kilian, 2002, Barsky, Kilian, 2004 and Kilian, 2008, Kilian, 2009 emphasized the importance of separating exogenous oil shocks to properly identify their effects on the US economy. Rotemberg and Woodford (1996) pointed out that oil shocks can generate recessions, because an increase in oil prices reduces the demand for factors of production, not only for oil but also for labor and capital services. Hamilton (1988) highlighted that an increase in the price of oil can generate recessions because it generates a reallocation of work between productive sectors. If this process is expensive, it can generate significant contractions in value added. Also, and related to this paper, Barsky and Kilian (2002) found evidence of significant increases in consumer prices after significant increases in oil prices occurred.====The rest of the paper is organized as follows. Section 2 provides an empirical estimation that motivates the paper. Section 3 presents the model. Section 4 explains the mechanism at work in generating the link between oil price volatility and average inflation. Section 5 uses the model to analyze the role of oil price volatility and monetary policy for the level of average inflation. Finally, conclusions are presented in Section 6.","Inflation, oil price volatility and monetary policy",https://www.sciencedirect.com/science/article/pii/S0164070420301841,7 October 2020,2020,Research Article,160.0
"Borio Claudio,Drehmann Mathias,Xia Fan Dora","Bank for International Settlements, Centralbahnplatz 2, 4002 Basel, Switzerland","Received 30 October 2019, Revised 31 July 2020, Accepted 22 September 2020, Available online 6 October 2020, Version of Record 15 October 2020.",https://doi.org/10.1016/j.jmacro.2020.103258,Cited by (11),"Financial cycles can be important drivers of real activity, but there is scant evidence about how well they signal recession risks. We address this question, using a range of financial cycle measures. Unlike most papers, ours assesses forecasting performance not just for the United States but also for a panel of advanced and emerging market economies. We find that financial cycle measures have significant forecasting power both in and out of sample, even for a three-year horizon. Moreover, they outperform the term spread - the most widely used indicator in the literature - in nearly all specifications. These results are robust to different recession specifications.","Predicting recessions has been a long-standing quest for market practitioners, policymakers and academics alike. As early as the first volume of ====, Irving Fisher (1911) looked at developments in the “nation's bank book” to forecast the likelihood of a contraction in the United States.==== Following Estrella and Mishkin (1998), a consensus has emerged that the inverted yield curve, i.e. long-term bond yields below short-term interest rates, is among the best signals of impending recessions, if not the best one.====The notion that finance matters for the real economy has regained significant attention, especially after the Great Financial Crisis.==== Some authors even argue that all recessions in the United States since 1985 had financial origins (Ng and Wright (2013)).==== Despite this, research exploring how financial developments affect recession risks, i.e. the likelihood that a recession will occur in the near future, and whether they outperform the term spread as recession signal is scant. We fill this gap.====We explore in particular the importance of the financial cycle for predicting recession risks. This is not only of interest academically, but also from a policy perspective as the concept of the financial cycle is underpinning financial stability analyses in many policy institutions.====In the spirit of Minsky (1982) and Kindleberger (2000), the term “financial cycle” refers to the self-reinforcing interactions between asset prices, risk, risk taking, and financing constraints (Borio (2014)).==== To measure the financial cycle empirically, the literature has emphasised the role of joint, medium-term fluctuations - typically in the range of 10-20 years – in credit and asset prices as important interactions between credit, asset prices and the real economy play out at these frequencies along two dimensions. First, medium-term cycles of credit and asset prices are especially highly correlated with the medium-term GDP cycle (e.g. Runstler and Vlekke (2018) or de Winter et al. (2017)). Second, peaks in the (medium-term) financial cycle coincide with considerable financial stress or banking crises (e.g. Drehmann et al. (2012)), which in turn usher in deep and protracted recessions (e.g. Jordà et al. (2013)).====In this paper, we look at two financial cycle measures specifically. First, the “composite” financial cycle proxy as in Drehmann et al. (2012). This combines medium-term cycles in real credit, the credit-to-GDP ratio and real residential property prices. Second, the debt service ratio (DSR), defined as interest payments plus amortisation divided by GDP. Juselius and Drehmann (20202) show that the financial cycle can be described by the joint behaviour of leverage and the DSR. The DSR, in turn, is the key link to the real economy as it has a large negative impact on GDP growth (see also Drehmann et al. (2018)).====By focussing on the financial cycle, we expand on the few papers in the literature that assess the predictive power of individual variables that proxy some aspects of the financial cycle. For instance, the literature has found balance sheet conditions, property prices, credit growth or corporate credit spreads to be important (e.g. Ng (2012), Liu and Mönch (2016), Ponka (2017), Guender (2018) or Hwang (2019)).====As our aim is to better understand how financial cycles affect recession risks, we do not to try maximise overall forecast performance. We run simple panel probit models and do not do a horse race across all possible explanatory variables.==== But it is clear from the literature that using more elaborate methods such as time varying coefficient models, Bayesian model averaging or machine learning would boost forecast performance even further (e.g. Berge (2015) or Hwang (2019)).====We assess predictive performance of the financial cycle both in and out of sample. We rely on the area under the receiver operating characteristic (ROC) curve (AUC) to judge forecast performance (Berge and Jordà (2011)). Throughout, we compare the performance of the financial cycle with that of the term spread in light of the importance of the term spread as a benchmark even though its performance has shown to change over time (e.g. Chauvet and Potter (2002) or Hwang (2019)).====And in contrast to much of the literature, we assess the signalling power of financial cycle measures and the yield curve not only for the United States but also for an unbalanced panel of 16 advanced economies and nine emerging market economies (EMEs) from 1985 to 2017. This makes our results significantly more general. For instance, of the above-mentioned papers that look at variables related to the financial cycle, only Guender (2018) goes beyond the United States and considers a panel of European countries. And even the literature on the information content of the yield curve barely looks at EMEs; the exceptions are Mehl (2009), who examines the effect of the yield curve on growth; Öztürk and Pereira (2013), whose panel analysis includes EMEs; and several country-specific studies (e.g. Grabowski (2009) for Poland, Zulkhibri and Rani (2016) for Malaysia, and Paweenawat (2017) for Thailand)).====We find that financial cycle measures are very useful for gauging recession risks. They perform generally better than the term spread. When the competing variables are considered on a standalone basis, AUCs for financial cycle measures are statistically significantly higher and are significant even for a three-year horizon, for which the term spread is uninformative. When financial cycle measures and the term spread are included jointly in a probit model, they all remain statistically significant up to a two-year horizon. But combining information from the spread and financial cycle measures improves only marginally, and not significantly, AUCs and other evaluation metrics relative to specifications that include just financial cycle measures.====The high forecasting performance of the financial cycle measures also applies to the out-of-sample tests. Here, we carry out two exercises to assess the indicators’ predictive content in (quasi) real time. We examine first the effect of real-time data==== and fixed model parameters estimated using our first 10 years of data. We then assess the combined effect of real-time data and model parameters estimated recursively. In both exercises, the financial cycle measures retain statistically significant forecasting power.====The results apply to both advanced economies and EMEs. That said, the forecast performance of both the term spread and the financial cycle is generally weaker in EMEs, possibly reflecting much more volatile business cycles there. Across countries, a reason for the disappointing performance of the term spread of is that for several economies the variable is “contaminated” by credit risk premia. As a result, in some episodes, the yield curve steepened rather than flattened ahead of recessions, including in some periphery countries ahead of the 2011–12 euro sovereign debt crisis. By contrast, the financial cycle measures are immune to this problem.==== Consistently with this explanation, the term spread retains forecasting information for the United States as shown by the literature.====The performance of financial cycle proxies is robust regardless of whether we forecast the likelihood that the economy will be in a recession at a ==== point in time several periods ahead – the literature standard – or whether the business cycle will turn ==== the next few periods. Following much of the literature (e.g. Rudebusch and Williams (2009)), in the main text we focus on the first approach – in our case one, two or three years ahead. As an alternative, we estimate the risk that the business cycle will turn from boom to bust within the next one, two or three years. This second type of exercise is much closer to the spirit of Irving Fisher, who had observed in 1911: “The exact date of such crisis (recession in his context), of course, it would be foolish to predict, but it if it occurs it would seem likely to occur between, say 1913 and 1916” (Fisher (1911), p 304).====The rest of the paper is structured as follows. In the second section, we briefly introduce the notion of the financial cycle and document how the nature of the business cycle, and its link with the financial cycle, have changed in the past 50 years. In the third section, we explain our methodology. In the fourth, we evaluate the performance of financial cycle proxies and compare it with that of the term spread based on full-sample information, i.e. ex post. In the fifth, we consider out-of-sample exercises, seeking to mimic the information policymakers have when assessing risks in real time, i.e. ex ante. In the sixth, we consider a different definition of recession risk as robustness check. Then we conclude.",Forecasting recessions: the importance of the financial cycle,https://www.sciencedirect.com/science/article/pii/S016407042030183X,6 October 2020,2020,Research Article,161.0
Troug Haytem,"Central Bank of Libya, Department of Research and Statistics, Tripoli, Libya","Received 16 March 2020, Revised 14 July 2020, Accepted 29 September 2020, Available online 2 October 2020, Version of Record 10 October 2020.",https://doi.org/10.1016/j.jmacro.2020.103260,Cited by (0),"Commodity-rich economies share many common factors, which resulted in the generalization of any findings obtained from a single commodity-rich economy. This paper proposes a ","There exists a long and growing literature that investigates the effect of commodities on commodity-rich economies. This literature suggests that as much as commodity windfalls present an opportunity for development and prosperity, the volatility of its windfalls poses a threat to the stability and growth of commodity-rich economies. Nevertheless, several papers suggest some “prototypical” countries that have escaped the “resource curse,” and were able to harness commodity windfalls====, advising that other countries should follow the footsteps of these success stories, under the belief of a “one-size-fits-all” assumption. In this regard, the literature has yet to investigate the existence of heterogeneity among commodity-rich economies, and this paper seeks to detect some sources of heterogeneity between this group of countries based on the type of commodity they possess.====This paper proposes a small open economy model for a commodity-rich country to quantitatively study the triggers of business cycles in different commodity-rich economies. This paper extends the theoretical model of Galí and Monacelli (2005) by adding some features to make it more relevant for a commodity-rich economy. The model contains five key features. First, the supply of commodities is exogenous, and it is affected by political, geographical, and technical factors, i.e., non-economic factors. Second, the government is the sole owner of commodities and it collects the windfalls of selling them to the rest of the world====. Third, following Schmitt-Grohe and Uribe (2017), this paper deviates from the assumption that a terms of trade shock solely represents external shocks, rather we model the rest of the world economy and allow developments in the rest of the world economy to affect commodity prices. Fourth, government consumption is added to the utility function in a non-separable form to deliver business cycle behavior akin to those observed in commodity-rich economies. Lastly, this paper imposes the same commodity-price shock on all commodity-rich economies to extract away the differences in the magnitude of price volatility of different commodities.====The principal behavioral parameters that the paper focuses on are the elasticity of substitution between government consumption and private consumption and the response of government consumption to fluctuations in the commodity prices. The former parameter is an indicator of the efficiency of government consumption and its effect on private consumption (crowding-in versus crowding-out), while the latter captures the behavior and the stance of fiscal policy during booms and busts of commodity prices, along with the size of the commodity windfalls in the government’s revenue (composition effect). The analysis of this paper proceeds in four steps. First, an empirical analysis is conducted on key variables in commodity-rich economies. Second, we generate the impulse response of the data using a structural VAR model. Third, we present the full structure of our DSGE model. The model generates extra sources of stochastic processes proposed by the existing literature. Also, the calibration of the parameters for our DSGE model is made for all our economies of interest, based on the empirical findings of this paper and the long-term averages found in the data. Fourth, we use Bayesian estimation techniques to calculate the variance decomposition of our variables of interest.====The results of this paper show that external shocks explain about 30 percent of variations in key macroeconomic variables in commodity-rich economies, which is consistent with the empirical findings of Fernández et al. (2017). Nevertheless, the results show that the variations caused by external shocks explain about 60 percent in oil-rich economies, more than double the impact observed in other commodity-rich economies. These results show that the source of this heterogeneity lies in the performance of fiscal policy, a principal variable that was not, as highlighted above, included in Fernández et al. (2018), and was highlighted earlier in Pieschacon (2012) and Arezki and Ismail (2013). Thus, this paper is in line with and supports the findings of Mehlum et al. (2006) who showed that institutions are a vital factor for the effect of natural resources on economic performance, as the results of this paper calls for greater fiscal discipline in oil-rich countries where the share of commodity windfalls exceeds their counterparts in other commodity-rich economies, which, in return, refute the argument by Sachs and Warner (1995) who argue that institutions play no role in harnessing commodity proceeds.====The empirical and theoretical findings of this paper show that consumption is excessively volatile relative to output, which is consistent with the findings of the previous literature==== However, our findings show that this might also be the case for developed countries that are rich with natural resources, as in the case of Australia. The results also show that, once we control for the commodity prices, there is heterogeneity in the forces driving the business cycle within commodity-rich economies. The fiscal sectors in these economies drive these forces, along with institutional factors and the share of commodity windfalls in the economy. The results show the existence of a procyclical fiscal stance in developing, commodity-rich countries. This is consistent with the findings of Kaminsky et al. (2005), Frankel (2011), and Bastourre et al. (2012). Nevertheless, we find that adopting the fiscal rule, as in the case of Chile and Australia, reverses this behavior, supporting the findings of Cespedes and Velasco (2014). This paper adds to this literature and shows that countries with significant shares of commodity windfalls must take more prudential fiscal measures than their other commodity-rich counterparts. Our results also support the findings of Leite and Weidmann (1999) and Isham et al. (2005) of how the abundance of commodities erodes institutions, and that, in return, will affect how economies react to commodity shocks. The results of this paper, at least regarding commodity-rich economies, support the findings of Gali et al. (2007) and Bouakez and Rebei (2007), who show that government consumption has a crowding-in effect on private consumption.====The organization of the remainder of this paper is as follows. In the second chapter, we illustrate our stylized facts and empirical findings for our economies of interest. In the third chapter, we build a DSGE model for a commodity-rich small open economy. We add some structural shocks that were suggested by the previous literature and calibrate the model based on our empirical findings and the long-term parameters found in the data. In the fourth chapter, we estimate the model using Bayesian estimation techniques. Chapter five concludes.",The heterogeneity among commodity-rich economies: Beyond the prices of commodities,https://www.sciencedirect.com/science/article/pii/S0164070420301853,2 October 2020,2020,Research Article,162.0
"Kang Hyunju,Park Jaevin,Suh Hyunduk","Korea Capital Market Institute, Korea,Department of Economics, University of Mississippi, United States,Department of Economics, Inha University, Korea","Received 6 April 2020, Revised 10 September 2020, Accepted 18 September 2020, Available online 25 September 2020, Version of Record 27 October 2020.",https://doi.org/10.1016/j.jmacro.2020.103257,Cited by (4),"During the ==== (firms’ efficiency to use part-time labor). As for the ====, the initial increase in part-time employment at the outset of the financial crisis is mostly explained by the rise of the risk premia; the persistently high level of part-time employment in the later period is mainly explained by an exogenous increase in part-time labor supply. A part-time labor supply shock also explains a significant portion of slow recovery in the gross wage during the recession, as the shock lowers the part-time wage and the proportion of full-time workers in total employment. Notably, the results from our model suggest that though the transition from full-time to part-time jobs contributed to mitigating the sharp contraction in total employment and labor force during the Great Recession, it played only a limited role in relieving recessionary pressure.","During the Great Recession, the U.S. economy experienced a substantial rise in part-time employment==== for a sustained period. Historically, this is not new, because, in the U.S., the part-time share of employment increases during recessions, but decreases during expansions, as shown in Fig. 1 (a). This counter-cyclical movement in part-time jobs has remained unnoticed despite its non-negligible size.==== Yet, recently, policymakers have paid attention to the rise of part-time jobs, as this phenomenon may indicate an additional slackness in the labor market. For example, Yellen (2014) claims that the current unemployment rate may not fully capture the extent of the slackness in the labor market, considering the high percentage of involuntary part-time jobs. As Fig. 1 (b) shows, the proportion of part-time workers in the labor force increased during this recession and has only slowly declined, even though the unemployment rate has come down to its pre-crisis level. This could signal that the economy fails to employ at full capacity even with the low unemployment rates.====In addition, the prevalence of part-time jobs during the Great Recession can be related to another recent puzzle in the U.S. labor market: the “flattening” of the wage Phillips curve (WPC hereafter), which refers to a situation wherein wages move up slowly despite the falling unemployment. Indeed, the unemployment rate in the U.S. decreased by a remarkable extent, that is, by 5.5 percentage points from 2010:1Q to 2017:2Q. However, during the same period, the median usual weekly nominal earning of full-time workers increased by only 1.9% annually and that of part-time workers increased by an even less degree, 1.8%.==== Considering that the wages in part-time jobs are lower than those in full-time jobs on average, the fact that the part-time share in total employment remains relatively high compared with previous recoveries could be associated with this flattening of WPC (a composition effect). Such complications call for a deeper understanding of what caused part-time jobs to rise and how it affected the overall economy during the recession and its recovery.====In this respect, our main research question can be summarized as follows: How does part-time employment react to economic shocks? Especially, what are the main causes of the transition to part-time employment during the Great Recession? Is it driven by household supply or firm demand? Why did part-time employment remain high until recently, whereas full-time jobs seem to have recovered? How do part-time jobs affect the labor force, unemployment, and the other macroeconomic variables such as output and inflation? Is there a connection between part-time jobs and the slow wage recovery?====To capture the dynamics of part-time jobs along the business cycle, we extend the New Keynesian dynamic stochastic general equilibrium (DSGE) model by Galí et al. (2012) (GSW hereafter), which incorporates the unemployment into the benchmark medium-sized DSGE model of Smets and Wouters (2007) (SW hereafter). Our extension of the model mainly involves introducing the additional building block of labor, in which agents work part time. Then we take a number of actual macroeconomic data series to identify the structural shocks, and explain the interaction between the macroeconomic variables and the labor type transitions. By maintaining the basic structure of SW and GSW, still considered as workhorse models by many central banks for forecast and policy analyses, we can practically focus on the dynamics of part-time jobs by using the standard analytical methods.====In our model, a large household has full-time and part-time available populations. It optimally determines consumption and the labor participation of each type, considering wage conditions in both types of jobs. The household can effectively substitute labor types by reducing one type of participation and increasing the other. We assume that a fraction of the part-time available population exhibits hand-to-mouth (HTM hereafter) behavior. These agents cannot access the financial markets for consumption smoothing, and thus their labor supply is likely to increase more aggressively during the recessions. Firms’ production technology includes both full-time and part-time labor; and the firms choose the level of employment in each type, given the wage conditions. Once the agents are employed—either full or part time—their labor is indivisible in the sense that the labor hour per person is constant (no change in the intensive margin for each labor type).====We define “part-time labor supply shock” as a disturbance term that governs the relative disutility between full-time and part-time labor. This term reflects how each labor type has different utility implications in various aspects, arising from the differences in average working hour, degree of stress, and preference of demographic groups toward part-time jobs.==== In addition, it captures how the relative preference between the two labor types can change over time, either because the preference itself or a population share of certain demographic groups that favor part-time jobs can change. On the other hand, we define “part-time labor demand shock” as a disturbance that affects the firms’ relative productivity of part-time labor. This term captures, as Valletta et al. (2020) suggest, the improvements in monitoring and scheduling technology that help schedule the part-time workers more efficiently, or the change in the share of certain industry groups that rely more heavily on part-time jobs. Moreover, it might reflect the firm’s cost concerns that are not captured in the usual concept of wages.==== Following GSW, the frictions in the labor market arise from a nominal wage rigidity and the monopolistic power of labor unions reflected in the wage markup. This wage markup leads to unemployment, that is, the household is willing to supply a certain level of labor at a given wage, but firms cannot hire all of them because of the wage markup.====Our estimated model does capture some key characteristics of the part-time variables in the data along the business cycle. For example, the model exhibits high volatilities of part-time employment and wage, a negative correlation between full-time and part-time employment, and a positive correlation between unemployment and part-time employment.====Shocks in the part-time labor supply, risk premium, monetary policy, and full-time wage markup mainly explain the rise of part-time employment during the Great Recession. They are able to cause substitutions between the labor types and were realized in ways to increase part-time employment during the Great Recession. The initial increase in part-time employment at the outset of the financial crisis is mostly explained by the rise of the risk premia. A large share of persistently high part-time employment in the later period is explained by an exogenous increase in the part-time labor supply. In addition, the contractionary monetary policy induced by the zero lower bound (ZLB), and a positive full-time wage markup, contribute to a lesser, but still significant proportion of the rise of part-time employment. On the other hand, a reduction in full-time employment is jointly attributable to the risk premium, investment technology, and monetary policy shocks. Increased supply of part-time labor also mitigates the sharp contraction of the labor force, and the long-lasting effect of this shock keeps the part-time employment at a high level even after the full-time employment recovers.====Although an exogenous increase in part-time labor supply and demand during the Great Recession functioned to mitigate the contraction of the output, its positive influence on the output is modest in quantitative terms. After removing the trend, the output in 2010–2014 is about 6% below the level at the end of 2007, but the combined positive effect of part-time shocks on the output is about 0.6%. Though there are also the endogenous reactions of firms and households to switch from full-time to part-time employment in our model, a counterfactual analysis suggests that these endogenous reactions do not play a significant role in relieving the recessionary pressure either.====Our estimated WPC for full-time jobs has the parameters similar to those estimated from GSW, whereas WPC for part-time jobs shows a higher wage flexibility. Regarding the recent flattening of the WPC, the part-time labor supply shock explains a significant proportion of slow recovery in the gross wage by lowering the part-time wage as well as the proportion of full-time workers in the total employment. The part-time labor demand shock has a significant positive effect on the part-time wage, but its effect on the gross wage is largely reduced, as the shock also lowers the proportion of the better-paid full-time workers.====The results from our general equilibrium model suggest that the demand of firms for part-time jobs is less critical for the rise of part-time jobs than supply factors, which offers a different view from the hypothesis that Canon et al. (2014) and Borowczyk-Martins and Lalé (2019) suggest. Meanwhile, the result that the transition to part-time jobs does not prevent well the contraction of output during the Great Recession is consistent with the analysis by Farber (2017), who reports that the earnings of the full-time job losers significantly decrease due to the lower wage levels and fewer working hours.====Although recent studies in the literature emphasize the behavior of involuntary part-time employment during the Great Recession, we focus on total part-time employment for the following reasons. Most importantly, analyzing involuntary part-time employment in our framework is difficult because CPS dataset does not provide the wage and unemployment rate separately for voluntary and involuntary workers. Also, focusing only on involuntary part-time employment can be subject to the following limitations. First, voluntary part-time employment accounts for a large fraction (approximately 80%) of total part-time employment. Second, voluntary part-time employment contains valuable information. For example, although the inflow to part-time employment at the onset of the Great Recession is mostly explained by involuntary part-timers, the increase of voluntary part-timers in the later period, which coincides with the increase of the part-time labor supply shock in our model, can explain why part-time employment share remained high for a sustained time. In this respect, involuntary part-time employment alone cannot explain why wage recovery was slow even when the economy seems to recover, while that can be better explained by total part-time employment.",The rise of part-time employment in the great recession: Its causes and macroeconomic effects,https://www.sciencedirect.com/science/article/pii/S0164070420301828,25 September 2020,2020,Research Article,163.0
Caines Colin,"Federal Reserve Board, United States","Received 9 January 2020, Revised 9 July 2020, Accepted 8 September 2020, Available online 21 September 2020, Version of Record 17 October 2020.",https://doi.org/10.1016/j.jmacro.2020.103256,Cited by (2), from observed fundamentals whilst replicating key moments of housing market variables at business cycle frequencies.,"Following the financial crisis of 2008 there has been an intense focus on the tendency of markets to generate boom-bust patterns in asset prices. Explaining these episodes poses a difficult question for researchers: how can large and persistent price growth be explained in the absence of large and persistent exogenous variation? Across a wide range of settings it has proven difficult to identify market fundamentals or frictions that can explain price booms as well as asset price volatility more generally. This paper argues that boom-bust behavior in asset prices can be explained by a model in which boundedly rational agents learn the process for prices. The key feature of the model is that agents on both the asset demand and credit supply sides of the market are information-constrained and both are learning. Propagation comes from the interaction between the two sets of agents in the model, which creates complementarities in their respective beliefs. The quantitative performance of the model is evaluated in the context of recent experiences in US housing markets. A single unanticipated interest rate drop, consistent with that observed in the early 2000s, generates 20 quarters of house price growth whilst capturing the full appreciation in US housing in the early 2000s.====The novel feature of this work is that it allows for learning in the credit supply problem facing lenders. This is in contrast to canonical asset pricing models with learning that restrict subjective beliefs to the demand side of the market. Models of bounded rationality allow for the possibility of feedback loops to exist between subjective beliefs and observed outcomes. In order for such environments to generate large and persistent asset price growth in response to a small set of shocks, beliefs need to exhibit two properties. First, subjective beliefs must be highly responsive to observed shocks. The response of outcomes to shifts in beliefs must be of sufficient size to generate subsequent belief shifts. Second, the belief process itself must be sufficiently persistent to prevent the episode from dying out quickly. A contribution of this paper is to show that in models with only demand side learning, there exists a trade-off between these two properties. In other words, increasing the elasticity of beliefs with respect to shocks comes at the cost of decreasing the persistence of these beliefs. As a result of this trade-off such models struggle to explain asset price booms.====Next, the paper shows that this trade-off in the formation of the belief process can be broken by extending bounded rationality to credit suppliers. When learning about prices operates in both the demand side and credit supply side of the market there are complementarities in the beliefs of buyers and lenders. An increase in buyers’ price forecasts increases the capital gains they expect to receive on their assets, driving up demand. An increase in lenders’ price forecasts increases the expected value of their claims to collateral in the event of default, leading to relaxed lending conditions. Each of these actions drive up prices, and through the learning mechanism further increase the price forecasts of each type of agent. The paper shows that this complementarity loosens the trade-off between generating beliefs that are both persistent and responsive.====Finally, the paper shows that this mechanism can quantitatively capture many properties of US housing markets. The full appreciation in US housing seen in the early 2000s can be explained with observed interest rate movements. The calibrated model is also shown to replicate key volatilities of housing market variables at business cycle frequencies. Furthermore, the paper explains observed comovements in house prices and household leverage. The model developed here is able to endogenously generate substantial liberalizations in households’ borrowing environment concurrent with periods of prolonged price growth.====The paper is structured as follows. Section 2 provides an overview of the literature in which this work is placed as well as a discussion of the recent experience in US housing markets. Section 3 presents the main model, outlining the microfoundations of agents’ beliefs, discussing the decision problem and optimality conditions of households and lenders, and finally providing an equilibrium for the model under learning. A discussion of the model’s calibration is to be found in Section 4. Section 5 presents the analytical findings of the paper and demonstrates how the presence of bounded rationality in both the demand for housing and the supply of credit breaks a trade-off between the persistence of beliefs and the sensitivity of beliefs to shocks that exists in traditional learning models. Empirical findings are discussed in Sections 6 and 6.1 examines the effect of observed lending rate drops and highlights the model’s ability to capture much of the observed experience in the US housing market post-2000, while Section 6.2 shows the model’s performance in attempting to match business cycle moments of the US housing market.",Can learning explain boom-bust cycles in asset prices? An application to the US housing boom,https://www.sciencedirect.com/science/article/pii/S0164070420301816,21 September 2020,2020,Research Article,164.0
"Perez-Laborda Alejandro,Perez-Sebastian Fidel","Departamento de Fundamentos del Análisis Económico (FAE), Universidad de Alicante, Spain,Hull University Business School, UK","Received 8 March 2019, Revised 8 September 2020, Accepted 8 September 2020, Available online 21 September 2020, Version of Record 25 September 2020.",https://doi.org/10.1016/j.jmacro.2020.103255,Cited by (4),"The goal of this paper is two-fold. First, we reexamine the evidence for the capital-skill complementarity (CSC) and the skill-biased technological change (SBTC) hypotheses at the sectoral level in the US economy for the period 1970–2005. Second, we quantify their effect on the evolution of the wage skill premium. To do so, we estimate a translog model with three production factors (skilled labor, unskilled labor, and capital) for different sets of ==== aggregates suggested by the literature. At the aggregated level, we find that both CSC and SBTC explain a substantial part of the observed change in the skill premium. The CSC hypothesis also receives support across sectors, although SBTC often explains a larger part of the premium change. We also find that the relevance of CSC increases with the level of aggregation of the data. Besides, when we disaggregate capital into ICT and non-ICT, our results suggest that often ICT capital is not the primary source of CSC. However, ICT-CSC is the most important driver of the skill premium in specific sectors, such as financial and business services.","Over the last 40 years, most developed economies have experienced a steady increase in the relative supply of skilled workers, which has not been accompanied by a deterioration of the skill premium. The wage gap between skilled and unskilled workers has even increased in some countries, such as the US.====Several works have postulated the existence of a latent technological change that is favoring skilled workers over the unskilled to explain this apparent contradiction. This skill-biased technical change (SBTC) has raised the productivity of the skilled over the unskilled, driving up the premium. Papers that defend this thesis include Bound and Johnson (1992) and Katz and Murphy (1992).====An alternative explanation is related to the decline of the relative price of the investment in equipment during the same years, which has led to a substantial increase in the use of capital in the workplace. If the Griliches (1969) capital-skill complementarity hypothesis (CSC) holds, and capital is more complementary to skilled than to unskilled labor, the observed increase in the quantity of capital may have raised the relative productivity of skilled labor and, thus, its relative wage (see, e.g., Krusell et al., 2000).====CSC and SBTC represent different characteristics of the production function. While CSC is related to the curvature of the isoquants, and hence, to the ease with which an input can be substituted by other input without changing the amount of output produced, SBTC originates from non-parallel shifts in those isoquants, which modify the relative productivity of the inputs. The validity of these hypotheses has core implications in growth, trade, and development, as argued, e.g., in Stokey (1996).==== Consequently, a battery of studies has looked for their empirical support (Acemoglu, 2002, makes an excellent review). While many works stress the importance of SBTC to explain skill premium changes, the evidence in favor of the CSC hypothesis has been much more elusive. For example, Fallon and Layard (1975), Krusell et al. (2000) and Bergstrom and Panas (1992) document that capital is more complementary to skilled than to unskilled. Other papers, like Duffy et al. (2004) and Papageorgiou and Chmelarova (2005), find null or weak support for the CSC. On the contrary, SBTC has found plenty of support in the literature (see, e.g., Bound, Johnson, 1992, Katz, Murphy, 1992; Berman et al., 1994; Balleer and van Rens, 2013).====Our work contributes to this literature by rigorously assessing the evidence for both the SBTC and the CSC hypotheses in several literature-suggested sets of industry aggregates of the US economy within an integrated empirical framework. Previous work generally focuses only on one of these two hypotheses, employing either manufacturing or completely aggregated data. The studies based on aggregated data, on the one hand, omit the large differences in input structures and technological progress of the different industries stressed, for example, by Jorgenson and Timmer (2011) and Herrendorf et al. (2015). This omission may mask meaningful relationships among inputs present at a lower aggregated level. On the other hand, studies based only on manufacturing data neglect the important role of the different service industries, which account for about three-quarters of the total value-added and hours worked in many developed economies. Our results also provide information about certain features of the production functions, such as the elasticity of input substitution and the importance of input-biased technological change, which are necessary for multi-sectoral models, nowadays employed to understand many economic issues of interest.====To assess the strength of the CSC and SBTC conjectures at the sector level, we use data from the EU KLEMS dataset and estimate a translog model specification for a set of representative sectors of the US economy suggested by Jorgenson and Timmer (2011). Following Valentinyi and Herrendorf (2008), we also investigate their empirical support in two-sector splits of the aggregate economy suggested by theoretical models. The great flexibility of the translog model allows for a more consistent characterization of the SBTC and CSC hypotheses in terms of the estimated parameters than alternative specifications like the CES or Cobb-Douglas production functions. Moreover, as shown in Ruiz-Arranz (2002), the translog can be employed to gauge the relative contribution of the CSC and SBTC to the observed growth in the skill premium, which is essential because both hypotheses may found support in the data.====Our work is closely related to Fallon and Layard (1975) and Richter (2013). Fallon and Layard use cross-sectional data to estimate reduced form equations derived from a two-level CES production function for a single year. They find mild (though statistically not significant) evidence in favor of the CSC hypothesis at the aggregate level but strong evidence at sector level. However, the authors do not have capital data for the different sectors and must rely on cross-sector efficiency. The EU KLEMS database contains detailed capital and labor numbers at a sector level; so, we can directly evaluate the CSC hypothesis allowing as well for differences in efficiency across sectors. Besides, the time dimension of the data gives us the possibility to accommodate SBTC, which is not considered in Fallon and Layard (1975). Richter (2013), in turn, carries out a translog analysis of the evolution of the wage skill premium in different 2-digit US industries. However, she neither tests for the CSC and SBTC hypothesis nor quantifies the role of other factors than ICT and technology on the evolution of the skill premium.====Our results can be summarized as follows:====The paper is organized in the following way. Section 2 provides a theoretical example to understand the importance of sectoral analysis. Section 3 briefly describes the translog model. Section 4 discusses the empirical framework and describes the data. Section 5 presents the estimation results for a baseline translog specification with three inputs (skilled and unskilled labor, and capital) for the set of sectors discussed in Jorgenson and Timmer (2011). These results are extended in Section 6 by explicitly distinguishing between ICT and non-ICT capital, and also by considering two-sector splits of the aggregate economy common in the macroeconomic literature. Section 7 makes some concluding remarks.",Capital-skill complementarity and biased technical change across US sectors,https://www.sciencedirect.com/science/article/pii/S0164070420301804,21 September 2020,2020,Research Article,165.0
Guisinger Amy Y.,"Department of Economics, Lafayette College, 213 Simon Center, Easton, PA 18042, United States of America","Received 20 April 2020, Revised 25 August 2020, Accepted 29 August 2020, Available online 14 September 2020, Version of Record 22 September 2020.",https://doi.org/10.1016/j.jmacro.2020.103254,Cited by (2)," due to their different complementarities with capital. These estimated elasticities find that women are more complementary to capital. The calibrated model generates the cyclical volatility of work hours by gender and for the total hours worked that matches the U.S. data better than the traditional representative agent model. I then explore other extensions to this model including investigating the stability of the estimated labor demand elasticities and allowing for various Frisch elasticities of labor supply. This paper demonstrates that allowing for even broad levels of heterogeneity in a simple framework can increase the model’s tractability with the data. Since gender is important to explain U.S. business cycle dynamics, we need to carefully consider heterogeneity when analyzing counter-cyclical economic policy, as it may not have symmetric effects across assorted groups.","The 20th Century saw a large influx of women into the labor force for both supply and demand reasons. There is a deep literature identifying the causes for this change in trend including lack of male employees during WWII (====), decrease child care costs and decrease of the gender wage gap (====), and increase of schooling and wages (====), women facing child care and home production tradeoffs (====), and two working member households facing an additional budget constraint (====There are a variety of reasons why firms may view male and female workers as different components in the production function with differing labor demand elasticities. Men and women vary in occupations (====, ====) and levels of education (====, ====, ====).====  Additionally, recent research has shown that the elasticity of labor demand has changed in the U.S. as jobs and employer requirements have evolved (====) due to shifts from manufacturing to services (====), increase in computerization (====), change in society’s attitudes (====), and a rise in female educational attainment (====). ==== found that women have seen a relative increase in nonroutine analytic and interactive tasks due to technological change. This is similar to ==== who make the argument for female-biased technological change, as work requirements have shifted from physical to intellectual requirements (====). Even at midcentury, ==== found that women were more substitutable for high school educated men than for lower skilled men.====. I find that the volatility of the cyclical component is greater for men than women using a sample from 1976Q3 to 2015Q2; this result is robust to different filtering methods. This is consistent with recent research that has found larger recessionary effects on men than women, which is not only a feature of the most recent recession (====, ====).====Motivated by this empirical result, I set up a model with two different agents that compose a representative household, as specified by ====. Since these agents have identical utility functions and additive separability, consumption is the same among household members. These agents are only different in the demand for their labor in the baseline model. Motivated by the capital and skill complementarity literature (====, ==== who finds that the elasticity of labor demand has shifted in favor of women as employers require more office work than manual labor. Additionally, allowing for different elasticities of men and women workers has been used in an overlapping generations model by ==== to explain fertility and growth within a country.====These labor demand elasticities are the key parameters in the model. Using annual micro-level income data from the March supplement of the CPS and national data from the Bureau of Economic Analysis from 1964 to 2014, I am able to estimate the elasticities directly from the first order conditions of the representative firm’s problem. I find that the elasticity of substitution between female workers and capital is greater than the elasticity between female workers and male workers, which is consistent with this explanation that women are more complementary with capital than men. Additionally, I explore the stability and sensitivity of these estimated elasticities. Using standard values for all other parameters, I compare the performance of the model with HP filtered U.S. data statistics, which is standard in the literature. I find that the relative volatility of hours for males and females generated by the model (1.34 and 1.19) closely matches the U.S. data (1.32 and 0.96). As a consequence of matching the volatilities of the gender groups, the model (1.24) generated relative volatility of aggregate hours that matches the data (1.12), as well. This result is much improved over the classical RBC model, which finds the volatility of hours to be around half the volatility of output (====). Understanding that men and women can differ on the labor supply side as well, I then explore the concurrence of my baseline results to the estimated elasticities and to various Frisch elasticities taken from the literature and find that my results are largely robust to differing labor supply elasticities. Therefore, accounting for difference in the labor demand for the two groups, specifically defining female labor as being more complementary to capital, resolves the classical RBC’s inability to generate sufficient hours worked volatility.====The paper is organized as follows: Section ==== explores the gender differences of work hours using three different filtering methods. Section ==== discusses the estimation and sensitivity of the elasticities of substitution of labor demand. Section ==== outlines the results of the baseline model and extensions of the model. Finally, Section ==== concludes.",Gender differences in the volatility of work hours and labor demand,https://www.sciencedirect.com/science/article/pii/S0164070420301798,14 September 2020,2020,Research Article,166.0
"Mbara Gilbert,Tyrowicz Joanna,Kokoszczynski Ryszard","Faculty of Economic Sciences, University of Warsaw, Poland,Group for Research in Applied Economics (GRAPE), Faculty of Management, University of Warsaw IZA, Poland,National Bank of Poland, Poland","Received 12 March 2020, Revised 21 August 2020, Accepted 24 August 2020, Available online 29 August 2020, Version of Record 3 September 2020.",https://doi.org/10.1016/j.jmacro.2020.103245,Cited by (1),"” employment that are consistent with survey evidence for the EU14 and United States. We investigate the fiscal and welfare effects of varying the avoidable and unavoidable shares of labor income tax while keeping the total wedge constant, and find that increasing the employer component raises hours worked, output, and welfare. Partial labor tax evasion makes tax revenues more elastic, but full tax compliance need not be a welfare enhancing policy mix.",None,Striking a balance: Optimal tax policy with labor market duality,https://www.sciencedirect.com/science/article/pii/S0164070420301713,29 August 2020,2020,Research Article,167.0
"Martin Christopher,Wang Bingsong","Department of Economics, University of Bath, Bath BA2 7AY UK,Department of Economics, University of Warwick, Coventry CV4 7AL, UK","Received 12 April 2019, Revised 20 June 2020, Accepted 23 August 2020, Available online 29 August 2020, Version of Record 12 September 2020.",https://doi.org/10.1016/j.jmacro.2020.103243,Cited by (1),"This paper proposes a modified version of the standard search and matching model of the labour market that includes a shirking mechanism. We show that our model delivers a close match to the simulated volatilities, correlations and autocorrelations of unemployment, vacancies, labour market tightness and the job finding rate with values observed in US data. In doing so, it outperforms prominent alternative models. Our model also has novel policy implications for the impact of income ","Although the currently dominant approach to modelling labour markets, the search frictions model pioneered by, among others, Diamond (1982); Mortensen and Pissarides, 1994 and Pissarides (2000), provides a simple framework for the analysis of the labour market and associated policy issues, it has some well-known difficulties. In the most prominent statement of these difficulties, Shimer (2005) compares the volatilities, autocorrelations and correlations of unemployment, vacancies, labour market tightness and the job finding rate from a calibrated and simulated version of the search frictions model with US data for 1951–2003. Two findings stand out. The simulated volatilities of these key labour market variables are much lower than those observed in the data. And the model cannot match the autocorrelation of vacancies or the co-movements of vacancies with other labour market variables. The inability of the search frictions model to match the cyclical behaviour of labour market variables, the unemployment volatility puzzle, has been extensively analysed by a large and growing literature (e.g. Christiano, Eichenbaum, Trabandt, 2016, Gertler, Trigari, 2009, Hagedorn, Manovskii, 2008, Hall, 2005, Hall and Milgrom, 2008, Pissarides, 2009, Rogerson, Shimer, 2010) and Ljungqvist and Sargent (2017). The particular weakness of the search frictions model in explaining vacancies has received less attention.====This paper argues that these weaknesses can be addressed by a modified version of the search frictions model that incorporates a shirking mechanism. Shirking arises because firms have imperfect information about the effort exerted by workers; we model this by assuming firms have an exogenous probability of detecting and firing a shirking worker. This implies that workers must balance the utility benefits of shirking against the costs, in the form of a higher probability of becoming unemployed. In our baseline model, we follow the extensive literature on shirking originated by Shapiro and Stiglitz (1984) and assume that wages are posted by firms. Firms will set the wage at the lowest value that induces the worker to choose not to shirk; the no-shirking constraint. Calibrating and simulating this model using, where possible, standard values from the literature, we can closely match the volatilities of unemployment, vacancies and labour market tightness and can also match the autocorrelation of vacancies and the correlations between vacancies and the other variables. Our calibration assumes a small shirking effect as the utility benefit of shirking is small and the probability of detection is high. This small friction is sufficient to generate a large volatility of unemployment and other labour market variables.====The model generates a large unemployment volatility through a mechanism similar to that outlined by Hall (2005). Firms respond to a positive productivity shock by posting more vacancies, leading to a fall in unemployment and an increase in labour market tightness. In the standard search frictions model, as Shimer (2005) argued, a highly pro-cyclical wage will absorb part of an increase in productivity, reducing the incentive for firms to post vacancies and so dampening cyclical movements in unemployment and vacancies. This does not occur in our model since the wage offered by firms to workers has a low volatility across the business cycle. We can also use the concept of the “fundamental surplus”, introduced by Ljungqvist and Sargent (2017), to explore the underlying forces that enable our model to generate a large unemployment volatility. We argue that the fundamental surplus is lower when workers are able to shirk, since firms must pay a wage premium to deter them from doing so. A smaller fundamental surplus implies that the “invisible hand” can allocate fewer resources to vacancy creation, leading, as Ljungqvist and Sargent (2017) argue, to a larger volatility of unemployment.====Our findings are robust to extending our simple baseline model. In Section 4, we analyse the case where wages are set through worker-firm bargaining, rather than through wage posting by firms, focusing on the credible bargaining approach of Hall and Milgrom, 2008. We show that this version of the model also generates a large volatility of unemployment, since the bargained wage has a low volatility across the business cycle. We also consider the impact of three policy measures: income taxes paid by workers, hiring subsidies paid to firms and payroll taxes. We show that our main results also hold in these extensions of our model.====We also show that our model has novel implications for policy. The literature on labour market policy using the standard search frictions model (e.g. Pissarides, 2000) argues that the marginal tax rate on income has no direct impact on the cyclicality of wages and unemployment. We show that this is not the case in our model, as an increased marginal rate of income tax increases the volatility of wages and reduces the volatility of unemployment. We also show that an increased hiring subsidy or a payroll tax cut leads to reduced and less volatile unemployment.====There is a robust body of evidence supporting the existence of shirking effects (Wolfers and Zilinsky, 2015). Burda et al. (2015) analyse empirical measures of shirking and argue that the data are consistent with a model in which “workers are paid efficiency wages to refrain from loafing on the job.” Groshen and Krueger (1990) and Rebitzer (1995) find an inverse relationship between wages and monitoring costs. Cappelli and Chauvin (1991) and Reich et al. (2003) find that firms take less disciplinary action against workers in workplaces where relative wages are higher. Pfeifer (2010) and Zhang et al. (2013) find that absenteeism is inversely related to wages. Malcomson (2010) estimate aggregate wage equations on U.S time series data and argue that their estimates are consistent with the shirking model. The potential of a shirking mechanism in explaining the unemployment volatility puzzle has been raised previously in the literature, for example by Rogerson and Shimer (2010). Costain and Jansen (2009) develop a model with a shirking mechanism but, in contrast to our approach, assume that wages are determined through bargaining. They find that this model does not help address the unemployment volatility puzzle. Uhlig and Xu (1996) assess the ability of a different type of efficiency wage model==== to explain large cyclical movements in unemployment using a real business cycle model without search frictions.==== A related strand of the literature (e.g. Danthine, Donaldson, 1990, Danthine, Kurmann, 2010) incorporates efficiency wage effects into DSGE models but does not investigate the unemployment volatility puzzle.====The remainder of the paper is structured as follows. We outline our model in Section 2. We discuss calibration and present our results in Section 3. We analyse the impact of wage bargaining on our results in Section 4; in Section 5, we discuss the underlying causes of unemployment volatility and relate our analysis to the “fundamental surplus” of Ljungqvist and Sargent (2017). We discuss the policy implications of our model in Section 6. Finally, Section 7 sumarises our work and raises issues for subsequent research.","Search, shirking and labor market volatility",https://www.sciencedirect.com/science/article/pii/S0164070420301695,29 August 2020,2020,Research Article,168.0
"Erauskin Iñaki,Turnovsky Stephen J.","Deusto Business School, University of Deusto, P° Mundaiz 50, 20012 Donostia-San Sebastián, Spain,Department of Economics, University of Washington, Seattle, WA 98195, United States","Received 3 June 2020, Revised 21 August 2020, Accepted 21 August 2020, Available online 26 August 2020, Version of Record 16 September 2020.",https://doi.org/10.1016/j.jmacro.2020.103244,Cited by (0),This paper employs a ,"What is the optimal size of government? This question has consistently generated a lively discussion among policy makers and academic economists, as well as the general public. Empirical evidence suggests that the size of government consumption has increased over the last 45 years. Between 1970 and 2015 average government consumption as a fraction of GDP increased significantly from around 14.5% to 16.5%. Over the same period, cross border holdings of gross financial assets and liabilities have also increased dramatically. For example, the stock of gross external assets and liabilities as a ratio of ==== in the world increased from around 45% in 1970 to 100% in 1987. It then accelerated to around 200% in 1998 and 400% by 2007 Lane and Milesi-Ferretti, 2007, 2018).====  These contemporaneous developments raise the obvious question: Can one can establish a positive causal relationship between financial globalization and the size of government consumption expenditures, or does their observed co-movement simply reflect their mutual response to some other common influence?==== This issue has been addressed both from a theoretical standpoint, as well as empirically, in a number of studies to which we shall refer below.====Most governments typically allocate around 75% of their expenditures to consumption, and the balance of 25% to productive causes. As can be seen in Table 1, the latter appear to be quite variable and exhibit a generally secular ==== over the period 1970–2015. At the same time, the fraction of GDP allocated to productive government expenditure has a ==== correlation of −0.65 with the measure of total gross external assets and liabilities, and a ==== correlation of 0.66 with net financial assets.==== These strong correlations with conventional measures of international financial integration suggest that productive government expenditure has indeed been influenced by the recent increase in globalization. But the negative correlation with total gross external assets contrasts sharply with the corresponding positive correlation for government consumption expenditure. This suggests that globalization impacts the two components of government expenditure in very different ways.==== Given that productive government expenditure (a) is a significant fraction of total government expenditure, and (b) has a profound impact on economic performance, it is important to see how it is impacted by globalization, especially in light of the contrast with government consumption expenditure.====Accordingly, our objective is to analyze the impact of financial globalization on the size of ==== government expenditure, as measured by its share of GDP.==== The basic framework we employ is an extension of the small open economy stochastic general equilibrium growth model developed by Turnovsky (1999a), and modified by Erauskin and Turnovsky (2019b) to include financial frictions. This provides a convenient framework for interpreting the subsequent empirical evidence undertaken.====As we have noted previously, there are several reasons why employing a stochastic framework is desirable. First, the essential element of international financial liberalization is its impact on investors’ portfolio choices as they balance off the risks and returns associated with domestic and foreign investments. Related to this, increased international financial integration alters the relative importance of internal versus external sources of volatility on the stability and welfare of a small open economy. In this regard, as Rodrik (1998) suggested, the government may play a risk-reducing role in economies exposed to a significant amount of external risk. The observation that the two forms of government expenditure appear to be very differently linked to international financial conditions suggests that productive government expenditure may play a very different stabilizing role than does the more extensively studied government consumption.====A key feature of the model we employ is that the economy has restricted access to international financial markets. Although it can lend and borrow abroad simultaneously, in either case it is constrained by financial frictions that are reflected in costs that increase with its position in the relevant market. Financial liberalization is specified as a reduction in the marginal costs associated with trading in international financial markets. Introducing independent frictions (and corresponding financial liberalization) associated with foreign lending and borrowing respectively, reflects the reality that the controls countries impose on capital outflows (lending abroad) are essentially independent of those they impose on capital inflows (borrowing abroad).====The main insight provided by our theoretical setup is to identify the channels through which financial liberalization influences the size of productive government expenditure. As a general characteristic we find that its optimal size, as measured by the share of productive government expenditure in GDP, is a modification of the familiar Barro (1990) expression derived for the canonical endogenous growth model. In the absence of domestic production risk, the optimal size of productive government expenditure in the stochastic open economy reduces to the elasticity of government expenditure in production, precisely as in the benchmark deterministic closed economy model.====However, the presence of domestic production risk, alters things fundamentally. First, its direct effect is to reduce the welfare-maximizing share of productive government expenditure. This is because production risk raises the variance of the return to domestic capital. To maximize welfare, the government must offset that increased risk by reducing its level of expenditure.====In addition, the equilibrium portfolio share of domestic capital becomes relevant, providing a channel through which other factors will now influence the optimal degree of productive government expenditure. Through this channel the two forms of financial liberalization – reducing the costs of lending abroad vs. reducing the costs of borrowing from abroad – are shown to have sharply contrasting effects on the portfolio share of domestic capital, and consequently on the country's net foreign asset position. Reducing the cost of investing abroad tends to divert productive resources away from the domestic economy, reducing the allocation of wealth to domestic capital, and ==== its net foreign asset position. The reduction in domestic capital reduces domestic output, causing the government to compensate by ==== its level of productive expenditure to maintain the welfare-maximizing level of output. In contrast, reducing the cost of foreign borrowing stimulates the domestic economy, increasing the portfolio share of domestic capital, and ==== its net foreign asset position. Domestic output increases, causing the government to compensate by ==== its productive expenditure. Consequently, whether financial liberalization is associated with more, or less, productive government expenditure depends upon whether the financial liberalization is oriented toward reducing the costs of foreign lending or borrowing. The key observation is that in either case, the fraction of the economy's GDP allocated to the government's productive expenditure is positively associated with the economy's net asset position.====The impact of increased volatility operates in a similar way. Thus, more domestic volatility, which increases the risk associated with domestic investment, reduces the portfolio share of domestic capital, leading to an increase in productive government expenditure, partially offsetting the decrease due to the direct effect, alluded to above. A similar effect applies to an increase in the risk associated with borrowing from abroad. But, in contrast, an increase in the risk of lending abroad leads to an increase in domestic investment and a reduction in productive government expenditure.====It is important to emphasize that these government responses to foreign source risk operate if and only if there is domestic production risk. Moreover, this overall mechanism determining the optimal productive government expenditure contrasts sharply with the determination of optimal government consumption discussed in our previous work (Erauskin and Turnovsky, 2019b) and elsewhere, which does not depend upon the existence (or otherwise) of domestic production risk.====In the latter part of the paper we perform and report comprehensive empirical testing of the underlying model and its implications, using recent data for a sample of 97 countries over the period 1970–2015. In doing so, we are guided by the insights offered by the theoretical framework employed. This involves two main issues. First, our representation of financial liberalization in terms of reduced marginal borrowing and lending costs is not directly observable. Accordingly we use the theoretical implications to infer the relationship between productive government expenditure and observable quantities pertaining to international financial assets. Specifically, we use the result that the optimal productive government expenditure is positively related to the country's ==== asset position. We also use a similar, but weaker, link between government expenditure and the measure of ==== international financial integration.====The second issue pertains to the assertion of the optimality condition that globalization impacts the degree of productive government expenditure only in the presence of domestic production risk (volatility). Unfortunately, due to data limitations, explained below, we are unable to integrate the impact of globalization and volatility measures into the same regression and examine their interdependence in a unified relationship. Rather, we proceed in the following sequential way. Using all available data, we first investigate the impact of financial globalization, on the assumption that domestic volatility is present. Then, using the reduced data set, brought about by the need to calculate volatilities, we then investigate the impact of volatility.====Incorporating these two elements, we find that the main theoretical results for the model are convincingly supported by the empirical evidence. Using alternative estimation procedures, we find compelling support for a robust positive relationship between the extent of productive government expenditure and the economy's net foreign asset position. We also find a weaker negative relationship between productive government expenditure and the gross measure of international financial integration. These latter results are consistent with the informal correlations mentioned at the outset, as well as with the predictions of the formal model.====The regressions pertaining to volatility are also consistent with the theoretical predictions of the underlying model. Specifically, domestic volatility is highly nonlinear. A modest amount may increase productive government expenditure, but once it reaches a mild threshold it will reduce government expenditure, consistent with the model. In contrast, external volatility has an insignificant effect on government expenditure. This would appear to reflect two effects. First, we are unable to distinguish between volatility associated with respect to foreign lending costs versus volatility with respect to foreign borrowing costs, which impact productive government spending in offsetting ways. In addition, in contrast to the direct effect of domestic volatility, these effects operate only indirectly through the impact on portfolio adjustments.====This paper is related to two bodies of literature. First, beginning with the seminal paper by Rodrik (1998), a substantial body of evidence has evolved suggesting a positive association between ==== openness and government size.====  In contrast, despite the recent wave of financial globalization, the consequences of ==== openness for the size of government has received much less attention. Moreover, the existing empirical evidence is inconclusive. For example, using data for 20 developed countries over the period 1967 to 2003, Liberati (2007) found empirically that capital openness and the size of government are negatively associated. In contrast, for a larger sample of 87 countries from 1976 to 2003, Kimakova (2009) suggests a positive association between financial (and trade) openness, and the size of government. Erauskin (2013) found that the size of government should be greater in an open economy because of higher productivity and/or less volatility (through risk sharing) and provided supporting evidence for 49 countries from 1970 to 2009. More recently, Kim et al. (2017) have found that government size increases with trade openness but decreases with financial, social, and political globalization.====Our analysis differs from the existing literature, (including our own recent paper, Erauskin and Turnovsky, 2019b) in several important respects. Beginning with Rodrik (1998), most studies focus either on government ==== expenditure, or on an aggregate measure of federal government expenditure that comprises a mix between the two types of government expenditure. In contrast, we focus exclusively on ==== government expenditure, which we define more explicitly in Section 5 below. With evidence suggesting that the two forms of government expenditure respond in markedly different ways to globalization, it is important to disaggregate them, as we have done. In addition, incorporating the reality that the form taken by financial liberalization – whether it favors foreign lending or borrowing – has significant consequences for wealth allocation is important. Finally, by focusing on the impact of financial liberalization on optimal productive government spending, our study contributes to the extensive literature spawned by Barro (1990), relating to this issue.====The remainder of the paper proceeds as follows. Section 2 sets out the basic components of the model, with the macroeconomic equilibrium being summarized in Section 3. Section 4 derives and discuss the main analytical results. Section 5 describes the data, paying particular attention to the measurement of productive government expenditure.==== The empirical implementation of the model is described in Section 6, while Section 7 concludes. The Appendix provides many of the technical details and other supportive information.",Financial globalization and its consequences for productive government expenditure,https://www.sciencedirect.com/science/article/pii/S0164070420301701,26 August 2020,2020,Research Article,169.0
Fair Ray C.,"Cowles Foundation, Department of Economics, Yale University, New Haven, CT 06520–8281, USA","Received 28 July 2020, Revised 9 August 2020, Accepted 11 August 2020, Available online 13 August 2020, Version of Record 19 August 2020.",https://doi.org/10.1016/j.jmacro.2020.103242,Cited by (3),This comment points out mismeasurement of variables in the ==== in Smets and Wouters (2007) and in models that follow the Smets-Wouters measurement procedures. The mismeasurement errors appear to be large.,"The DSGE model in Smets and Wouters (2007) (SW) has had an important influence on macroeconomic research. In particular, the variable measurements introduced in SW have been used in much of the research that followed. The following nine studies are in this category: Edge and Gürkaynak (2010); Kolasa et al. (2012); Wolters (2013); Del Negro et al. (2015); Kaplan et al. (2018); Wolters (2018); Anzoategui et al. (2019); Beraja et al. (2019), and Auclert et al. (2020).====There are seven observable variables in the SW model: consumption, investment, output, hours, inflation, real wage, and interest rate. This comment points out that three of these variables—consumption, investment, and hours—are mismeasured.",Variable mismeasurement in a class of DSGE models: Comment,https://www.sciencedirect.com/science/article/pii/S0164070420301683,13 August 2020,2020,Research Article,170.0
"Coulibaly Dramane,Gnimassoun Blaise,Mignon Valérie","EconomiX-CNRS, University of Paris Nanterre, France,BETA-CNRS, University of Lorraine, France,CEPII, Paris, France","Received 23 January 2020, Revised 4 May 2020, Accepted 29 July 2020, Available online 4 August 2020, Version of Record 11 August 2020.",https://doi.org/10.1016/j.jmacro.2020.103241,Cited by (1),"Following the dynamics of globalization, ","The sustained dynamics of globalization since the 1990s has been accompanied by marked global imbalances and a dramatic increase in international migration. These two phenomena are probably among the most complex topics of contemporary international economics faced by economists and decision-makers. Several recent contributions have been devoted to analyzing both international migration (see, e.g., Ortega, Peri, 2014, Bosetti, Cattaneo, Verdolini, 2015, Aubry, Burzyński, Docquier, 2016) and global imbalances (Dong, 2012, Lane, Milesi-Ferretti, 2012, Barattieri, 2014, Chinn, Eichengreen, Ito, 2014, Eugeni, 2015). Surprisingly, these dynamics have been investigated separately, and the link between international migration and global imbalances has received no particular attention in the literature. However, the analysis of global imbalances cannot obscure the issue of international migration, which could play a crucial role as a factor amplifying or alleviating these discrepancies.====Indeed, life cycle theory allows us to conjecture the existence of a direct link between the saving and investment rates of a country and its demographic structure. This relationship has been widely investigated in the literature,==== and empirical studies on the medium- and long-term determinants of current accounts emphasize the importance of demographic factors in explaining their dynamics.==== As an example, Cooper (2008) argues that the large US current account deficit at play from the early 1990s to the 2008 financial crisis is the natural result of two major forces in the world economy, namely the globalization of financial markets and the demographic evolution—two factors that could maintain these imbalances over a long period of time. Using a multi-country overlapping generations model, Backus et al. (2014) show that demographic differences between countries, affecting both individual saving decisions and the age composition of the population, can have a significant impact on capital flows around the world. Calibrating a standard neoclassical model consistent with life cycle theory, Domeij and Flodén (2006) also highlight the role of the age distribution (population aging) in capital flows between OECD countries.====As is well known, the population age distribution has profoundly changed in most countries due to demographic shifts, namely a decrease in fertility and mortality rates combined with increased longevity. As recalled by Curtis et al. (2017), these demographic evolutions cause changes in both the ratio of savers to non-savers and household size, which in turn affect the aggregate saving rate. Specifically, a demographic composition effect is at play: whereas aggregate saving decreases when the number of retirees reduces, it rises when the number of persons in their prime-earning years grows. Moreover, increased longevity leads agents to save more for their longer expected retirement. Finally, declining fertility also tends to act positively on the saving rate. Indeed, as noted by Curtis et al. (2017), prime-age agents expect less retirement support from the forthcoming smaller cohorts and are thus incited to increase their saving rate. Similarly (see, e.g., Krueger, Ludwig, 2007, Backus, Cooley, Henriksen, 2014), demographic changes affect investment. Declining fertility rates lower investment needs, in particular the demand for schools and housing, and less new capital is required to equip the new members of the labor force (Cooper, 2008). Overall, the current account dynamic is impacted.====If demographic changes are important in explaining the dynamics of current accounts and, in turn, global imbalances, international migration should play a leading role. Indeed, international migration has a structural or permanent component that contributes to changing the usual pattern of demographic structure in both emigration and immigration countries.==== In general, high-income countries are characterized by increasing immigration, while low-income countries are marked by emigration of the same trend. This decomposition of demography in the world can exacerbate or alleviate global imbalances by altering the demographic structure and, consequently, the age dependency ratios.==== Indeed, to a greater extent than the world population, international migration mainly consists of working-age persons—the latter amounting to about 77% in 2015 (see Fig. 1). Through its impact on the demographic structure of countries, international migration can influence the medium- and long-term evolution of their current accounts and, in turn, the dynamics of global imbalances. Fig. 2 clearly suggests the existence of such a link, highlighting a positive nexus between migration and current account which mainly operates through the saving rate. The role of international migration in the path followed by global imbalances is all the more likely as its evolution is heterogeneous, in both countries of emigration and countries of immigration.====Several notions have been suggested in the literature to explain global imbalances. Among them, the saving-glut hypothesis was widely shared (Bernanke, Clarida, 2005, Gruber, Kamin, 2007), but other explanations exist such as the twin deficit hypothesis (Chinn, Erceg, Guerrieri, Gust, 2005), the role of exchange rates and exchange-rate regimes (Obstfeld, Rogoff, 2005, Taylor, Gnimassoun, Mignon, 2014), and the importance of valuation effects in net foreign asset positions (Lane, Milesi-Ferretti, 2007b, Gourinchas, Rey, 2007, Devereux, Sutherland). But even in the forensic investigation of global imbalances conducted by Chinn et al. (2014), the path of international migration has not been explored. In the best-case scenario, the influence of international migration is treated indiscriminately from that of the natural demographic factors of countries.====This paper aims to fill this gap by determining the role played by international migration in the dynamics of current accounts and, in turn, in the evolution of global imbalances. To this end, we rely on an overlapping generations model to derive the theoretical relationship between migration and the current account in the context of an open economy with mobility of goods, capital, and people. This theoretical framework gives us the legitimacy to then carry out a series of robust econometric investigations to deeply analyze and assess the influence of international migration on global imbalances. Relying on a large panel of developed and developing countries over the period 1990–2014, our findings corroborate the theoretical prediction that migration improves the current account position in the host country, while exerting the opposite effect in the home country. Specifically, we find a positive (negative) impact of net immigration (emigration) on the host (home) country’s current account position which mirrors the positive (negative) effect of immigration (emigration) on the saving rate of the host (home) country. This result confirms the theoretical prediction that international migration—mainly consisting of working-age persons—leads to an increase in the saving rate in the destination country by raising its support ratio, and exerts the opposite effect in the origin country. We also find a mixed impact of migration on the investment rate, reflecting the compensatory effect between (i) the negative impact of emigration on the investment rate of the home country through labor force emigration, and (ii) the positive influence of emigration on the home country’s investment rate through remittances. Finally, we underline that the impact of net immigration on the current account balance and savings is particularly acute for developing countries compared to developed economies and is attenuated by remittances. Our findings hold after various sensitivity analyses.====Our contribution not only provides a key piece in the puzzle on world disequilibria but also delivers a more global dimension to the geography of current account imbalances. Indeed, in the previous literature, global imbalances are often presented as coming from some surplus countries—mainly the Asian emerging economies, Germany, Japan and the oil countries—and some large deficit countries—in particular the United States and the United Kingdom. Although this assertion is correct, it ==== excludes the role played by developing countries, which are nevertheless characterized by increasing structural deficits. Given that international migration is a phenomenon that links both developing and developed countries with a certain degree of heterogeneity, accounting for it provides a more global dimension to the analysis of world imbalances. It also helps in explaining the chronic deficits experienced by developing countries, whose evolution questions the principle of external sustainability. Moreover, disregarding international migration, despite its key role in the evolution of current accounts, is likely to lead to an erroneous assessment of the magnitude of global imbalances and, most importantly, to a distortion of the diagnosis whereby the influence of migration is confused with that of the natural demographic dynamics of countries.====The rest of the paper is organized as follows. Section 2 sets out the theoretical framework used to derive the relationship between international migration and the current account balance. Section 3 describes our empirical strategy and the data. We present and discuss our main results in Section 4, and provide several robustness checks and sensitivity analyses in Section 5. Finally, Section 6 concludes the paper.",The tale of two international phenomena: Migration and global imbalances,https://www.sciencedirect.com/science/article/pii/S0164070420301671,4 August 2020,2020,Research Article,171.0
"Aguirre Idoia,Vázquez Jesús","University of the Basque Country (UPV/EHU), Spain","Received 22 January 2020, Revised 19 June 2020, Accepted 27 July 2020, Available online 30 July 2020, Version of Record 13 August 2020.",https://doi.org/10.1016/j.jmacro.2020.103240,Cited by (5),Recent studies show that the estimated parameters of rational expectations ,"Recent literature shows evidence that the parameters in the DSGE models used to study aggregate fluctuations are in general time-varying (see, among others, Inoue, Rossi, 2011, Canova, Ferroni, 2012, Castelnuovo, 2012a, Castelnuovo, 2012b, Hurtado, 2014, Casares, Vázquez, 2018, Castelnuovo, Pellegrino, 2018, Canova, Ferroni, 2020). These findings have important implications. On the one hand, the instability of structural parameters somewhat weakens the ability of DSGE models to assess policies reliably (Fernández-Villaverde and Rubio-Ramírez, 2007). On the other hand, parameter variability may be an important source for explaining the swings in macroeconomic dynamics observed during the post-WWII era in the US.====This paper analyzes parameter variability by estimating the canonical medium-scale DSGE model suggested by Smets and Wouters (2007) under both rational expectations (RE) and adaptive learning (AL), following the approach of Slobodyan, Wouters, 2012, Slobodyan, Wouters, 2012a to AL. Both versions of the model are estimated first for the whole sample and then using a 20-year rolling-window approach. Our estimation results show that learning dynamics help to explain a large proportion of the parameter variability observed under the standard RE hypothesis typically assumed in macroeconomic modeling. The intuition is simple: Under RE, a time-invariant relationship links the endogenous variables to the (predetermined and exogenous) state variables of the economy whenever the structural parameters of the model are constant. Therefore, parameter variability becomes the only source of swings in macroeconomic dynamics under RE other than those generated from the exogenous shocks of the model. By contrast, the relationship linking endogenous variables with state variables becomes time varying under AL even when the structural parameters are time invariant, which might result in much richer macroeconomic dynamics.====We also assess the potential roles of both AL and time-varying parameters in explaining the large swings in the comovements between real and nominal variables. The estimation strategy outlined above enables us to distinguish four alternative scenarios by estimating the model alternatively under the AL and RE hypotheses and then estimating the model using the whole sample and the rolling-window samples for each expectations hypothesis:====Our estimation results show that assuming AL reduces parameter variability by roughly 75% on average (across all parameters estimated), as measured by a quadratic distance function defined below. The reduction in parameter variability induced by AL is much stronger for the subsets of parameters that control nominal price and wage rigidities and the policy rule parameters (at 98% and 83%, respectively). Moreover, the AL version provides a better model fit than the RE version. These findings certainly strengthen the potential of DSGE models under AL for implementing policy assessment, showing that AL helps to resolve the issue of parameter instability in RE versions of DSGE models raised by Fernández-Villaverde and Rubio-Ramírez (2007). Furthermore, our estimation results also show that the AL model estimated does a better job in reproducing the dramatic swings in the correlations between real and nominal variables observed in US data than the RE version. Those correlations are found to be moderately negative in the 1960’s and 1970’s, but they have become strongly positive more recently. Nonetheless, the fall in the extent of supply shocks relative to demand shocks (due to the fall in both persistence and size of innovations of price- and wage-markup shocks as shown below) seems to be the most important driving force in explaining these dramatic swings in the comovement between real and nominal variables. In spite of the relative success of AL in explaining these comovements, AL still falls short in explaining the recent fall in inflation persistence.====The rest of the paper is structured as follows. Section 2 discusses the related literature. Section 3 briefly describes the main features of the model. Section 4 discusses the empirical results from both the whole sample and the rolling-window estimation approaches under RE and AL. The same section also quantifies the variability of the model parameters and the differences in the transmission mechanism of shocks under the two hypotheses on expectation formation. Section 5 analyzes the ability of the alternative model specifications to account for the recent US macroeconomic swings. Section 6 concludes.","Learning, parameter variability, and swings in US macroeconomic dynamics",https://www.sciencedirect.com/science/article/pii/S016407042030166X,30 July 2020,2020,Research Article,172.0
"Anzuini Alessio,Rossi Luca,Tommasino Pietro","Banca d’Italia, Italy,Universitat Pompeu Fabra, Spain","Received 29 January 2020, Revised 8 July 2020, Accepted 13 July 2020, Available online 25 July 2020, Version of Record 4 August 2020.",https://doi.org/10.1016/j.jmacro.2020.103238,Cited by (20),Economic uncertainty is an important factor behind ,"Economic theory suggests that, under certain conditions, uncertainty shocks may be important in explaining economic fluctuations: firms may react to an increasingly uncertain environment by reducing hiring and investment; financial intermediaries may become more reluctant to lend; households may increase their propensity to save, as supported by the evidence in the empirical literature (Bloom, 2014).====Economic uncertainty can take many forms, and may originate from several sources. In the current paper we focus on fiscal policy uncertainty (FPU). Fiscal policy may represent a source of uncertainty for economic agents for several reasons. In countries with unsustainable public finances, households and firms may expect changes in future tax rates and/or expenditure programs (and therefore in crucial variables such as net profits, disposable income, etc.), but they may be unsure of the timing as well as of the magnitude of these changes.==== Even in countries where public finances are sustainable, FPU may be high if the political process is polarized and fiscal frameworks are weak (Kontopoulos, Perotti, 2002, Roubini, Sachs, 1989). In those countries, political uncertainty translates into FPU, because changes in government coalitions can lead to unpredictable or erratic changes in fiscal policy.====In the present paper we propose a new measure of FPU, and study its effects on the macroeconomic situation. For most of its recent history Italy has been characterised by fragile public finances and by a highly partisan and often fragmented political landscape. It is therefore an extremely appropriate laboratory to study FPU and its consequences.====In particular, first we estimate a fiscal reaction function in order to capture how the fiscal stance reacts to economic developments. The key difference with respect to previous empirical exercises (for a review, see Golinelli and Momigliano, 2009) is that the fiscal rule incorporates an innovation not only to the level, but also to the volatility of the fiscal stance (technically, we adopt a stochastic volatility model).====As a second step, we feed a VAR model with the two series of innovations - i.e. innovations to the ==== and to the ==== of the fiscal variables of interest - and analyze how they impact the macro-economy.====We find that the effects of a level fiscal shock are quite standard and in line with the previous VAR literature - i.e. we find positive multipliers. More interestingly, we also find that an increase in FPU has a negative impact on the economy.====Our paper contributes to two different streams of the macroeconomic literature.====First, the recent empirical research on the macroeconomics of uncertainty. As we already mentioned, uncertainty stems from several sources. Some papers have focused on stock-market induced uncertainty, such as Bloom (2009), which uses peaks in stock market volatility (captured by a dummy variable equal to one in selected dates) as a measure of uncertainty (see also the early paper by Romer, 1990). Policy may be clearly another relevant source of macroeconomic uncertainty.==== Baker et al. (2016) propose a broad policy uncertainty index based on the frequency of references to economic policy uncertainty in the news. More specific indicators are those related to trade policy and monetary policy, developed respectively by Handley (2014) and Creal and Wu (2017).====The only two papers that look at fiscal uncertainty shocks (both for the U.S.) are Born and Pfeifer (2014) and Fernández-Villaverde et al. (2015). We follow their econometric methodology, and proxy FPU with the time-varying volatility of the innovation of a fiscal reaction function.==== However, differently from Born and Pfeifer (2014) and Fernández-Villaverde et al. (2015), we look at the overall (cyclically-adjusted) primary deficit (CAPB) and not just to some of its components. This more encompassing variable is the most used indicator of the government’s fiscal stance (incidentally, the CAPB also plays a relevant role in the context of the fiscal framework of the European Union).====Given our focus on the CAPB, our paper is directly relevant for a second stream of literature, namely that concerned with the macroeconomic effects of discretionary fiscal policy.==== A review of that field is clearly outside the scope of this introduction, but it is well-known that there is no consensus about the size - and even the sign - of fiscal multipliers. On one side, studies like Blanchard and Perotti (2002) and Romer and Romer (2010) find standard demand-driven Keynesian effects; on the other side, starting from Giavazzi and Pagano (1990), other authors have argued that the effects of a fiscal change can be non-Keynesian, with the possibility of expansionary fiscal consolidations and contractionary fiscal expansions (a recent example is Alesina and Ardagna, 2013). Our main contribution to this debate is to show that fiscal policy-makers can influence the economy not only by changing the level of the budget deficit, but also by affecting its volatility. As a consequence, the same change in the government budget (say a budgetary expansion) can have different effects depending on whether it is associated with a reduction or an increase in the FPU. From an econometric viewpoint, this implies that a proper assessment of the impact of changes in the fiscal policy stance should correctly identify both the level and the uncertainty shock. From a policy perspective, our findings highlight the importance for policy-makers of being credible, and avoid policies that are unsustainable in the long run.====The remainder of the paper is organised as follows. Section 2 describes how we measure FPU: we outline our methodology, our data, and present the results; in Section 3 we present a battery of VAR estimates to show the effects of the fiscal shocks on macroeconomic variables. Section 4 concludes.",Fiscal policy uncertainty and the business cycle: Time series evidence from Italy,https://www.sciencedirect.com/science/article/pii/S0164070420301646,25 July 2020,2020,Research Article,173.0
"Chen Been-Lon,Hu Yunfang,Mino Kazuo","Institute of Economics, Academia Sinica, 128 Academia Road, Section 2, Taipei, Taiwan,Graduate School of Economics, Kobe University, 2-1 Rokodai-cho, Nada-ku, Kobe 657-8501, Japan,Institute of Economic Research, Kyoto University, Yoshida Honmachi, Sakyo-ku, Kyoto, 606-850 Japan","Received 14 March 2019, Revised 15 June 2020, Accepted 12 July 2020, Available online 17 July 2020, Version of Record 25 July 2020.",https://doi.org/10.1016/j.jmacro.2020.103236,Cited by (4),This paper examines the stability of a ,"Does the income taxation rule act as a built-in stabilizer? This long-standing question has attracted a renewed interest in macroeconomics research, ever since Guo and Lansing (1998) revealed that progressive income taxation contributes to stabilizing an economy in the presence of sunspot-driven business fluctuations. Using a one-sector real business cycle model with external increasing returns, Guo and Lansing (1998) demonstrate that progressive income taxation narrows the parameter space in which equilibrium indeterminacy emerges. They also confirm that regressive income taxation enhances the possibility of equilibrium indeterminacy. Subsequent studies have reconsidered Guo and Lansing’s finding in alternative settings such as two-sector real business cycle models, models with productive public investment, models with utility-enhancing public spending, and models of endogenous growth====. Those studies have shown that the taxation rule may play a decisive role in stabilizing the economy in various settings.====So far, the research on the stabilization effect of income taxation rules has focused on closed economies, and the role of taxation schemes for the stabilization effect in open economies has not yet been explored well. The purpose of this paper is to investigate the relationship between income tax schedules and the stability of a small open economy. We introduce the nonlinear taxation rule formulated by Guo and Lansing (1998) into a prototype model of a one-sector, small open economy with free capital mobility. Based on this analytical framework, we investigate which type of taxation rule contributes to stabilizing the small open economy.====We obtain two main findings. First, if the income taxation schedule is linear, then the small open economy will not yield equilibrium indeterminacy. However, it is also shown that, under certain conditions, the equilibrium path of the economy diverges from the steady-state equilibrium, so that there is no feasible, perfect-foresight equilibrium. In this case, an appropriate choice of nonlinear tax schedule on the factor income may recover the saddle-point stability of the economy. Therefore, the income taxation rule stabilizes the economy in the old sense rather than in the modern sense, that is, ruling out sunspot-driven fluctuations.====Our second finding is that nonlinear taxation on the interest income on foreign bonds may generate equilibrium indeterminacy. In particular, our numerical analysis reveals that under plausible parameter values, sunspot-driven business cycles arise in the small open economy, if the tax schedule on the interest income is regressive, while progressive taxation may establish equilibrium determinacy. This means that, as far as the taxation on the interest income is concerned, progressive taxation would act as a built-in stabilizer in the sense that it may eliminate sunspot-driven fluctuations. Therefore, our paper shows that the main conclusion of Guo and Lansing (1998) generally holds in the open economy counterpart as well.",Income Taxation Rules and Stability of a Small Open Economy,https://www.sciencedirect.com/science/article/pii/S0164070420301622,17 July 2020,2020,Research Article,174.0
Iyer Tara,"International Monetary Fund, Washington DC 20037, United States","Received 21 June 2018, Revised 23 June 2020, Accepted 25 June 2020, Available online 16 July 2020, Version of Record 6 August 2020.",https://doi.org/10.1016/j.jmacro.2020.103233,Cited by (0)," and imperfect asset markets, the benefits of exchange rate flexibility are shown to depend on the extent of labor and product market development. With developed markets, flexible exchange rates are preferred as they allow for greater relative price fluctuations, which amplify the transmission mechanism of labor re-allocation upon commodity price volatility. When labor and product markets are not well-developed, however, international relative price adjustments exacerbate currency and factor misalignments. A nominal exchange rate peg, by mitigating relative wage and price fluctuations, increases welfare relative to a float. Given the current low level of labor and product market development across most agricultural commodity exporters, the study provides a novel rationale as to why exchange rate targeting is implemented in many developing agricultural economies.","This paper analyzes the appropriate choice of an exchange rate regime in developing economies that are significantly dependent on labor-intensive agricultural commodity exports.==== In practice, the majority of such economies target their exchange rates.==== This poses a puzzle in light of the conventional macroeconomic policy advice since Friedman (1953) that flexible exchange rate have superior stabilization properties. Falling agricultural commodity prices since 2012 (Fig. 1), alongside persistent volatility in such prices (World Bank, 2017) have re-ignited discussions across international policy circles on exchange rate choices in commodity exporters (IMF, 2016). The policy recommendations to float have not taken into account the low level of development that influences monetary policy in labor-intensive commodity exporters. This paper seeks to reconcile the exchange rate puzzle in the data and contribute to the ongoing policy debate by providing benchmark theoretical results on the appropriate choice of an exchange rate regime in agricultural commodity-exporting economies.====I develop an open economy dynamic stochastic general equilibrium (DSGE) framework that incorporates key structural characteristics of agricultural commodity-exporting economies. While quarterly time series are not available for many of these economies, which tend to be low-income and data-scarce, the full DSGE model is estimated using Bayesian methods for an agricultural commodity exporter in Africa with data. The model is also able to match stylized facts on the macroeconomic response to commodity export price volatility (discussed further in Section 2) as well as approximate second moments in the data (model estimation is discussed in Sections 4 and 5). The economy has a heterogeneous production structure with commodity and non-commodity firms. Each sector employs workers to capture the labor-intensive production structure in agricultural commodity-exporting economies (UNCTAD, 2013). Labour market rigidity exists in the form of imperfect labour mobility across sectors, implying that wage rates are different across the economy. Both sectors are tradable, with commodities being fully exported and non-commodity goods being supplied to domestic and foreign agents. The commodity sector takes the world price of agricultural goods as given. The non-commodity sector is monopolistic with sticky prices, implying a role for monetary policy. There is limited financial integration with the world and international risk-sharing is imperfect, as representative of developing economies (Kose et al., 2006). ====In a novel set of results, I show that the appropriate choice of an exchange rate regime in agricultural commodity exporters depends on the degree of development of domestic labour and product markets. Inflexible labour markets in the model prevent workers from re-allocating hours worked across sectors in response to wage differentials. In developing countries, labour markets are fairly rigid due to factors including sector-specific skills and institutional regulations (Fields, 2009, Artuc, Lederman, Porto, 2013). Under-developed domestic product markets in the model are represented by a low Armington (1969) trade elasticity, or the elasticity of substitution between domestic and imported goods. The Armington elasticity is considered low in developing economies as in contrast to industrialized nations, domestic products are typically of poorer quality than imports (Hummels, Klenow, 2005, Henn, Papageorgiou, Spatafora, 2013). With inflexible labour markets and underdeveloped domestic product markets, a fixed exchange rate leads to higher welfare than a float. Intuitively, with inflexible real markets, and limited financial opportunities to hedge against country-specific risk, relative wage and price fluctuations exacerbate currency and factor misalignments. Instead of allowing for more efficient economic adjustment, price fluctuations in inflexible markets lead to the misallocation of domestic resources. The Central Bank prefers to mitigate international relative price fluctuations, and correspondingly relative wage fluctuations through labour re-allocation, by targeting the exchange rate.====As domestic labour and product markets become more developed, agents can more efficiently respond to wage and price differentials. Flexible labour markets allow households to re-allocate hours worked across sectors worked more efficiently, and thus enjoy higher wages for any given amount of labour effort. Better-developed domestic product markets increase the availability of substitutes for imports, and allow households to re-allocate consumption expenditure toward relatively cheaper domestic or foreign goods. In fact, while it is easier said than done in developing economies, when either domestic labour ==== product markets become more developed, relative price fluctuations become desirable. An exchange rate float leads to higher welfare than a peg in this case. Intuitively, upon a negative shock to the commodity sector, flexible exchange rates allow for greater real depreciation. This increases aggregate demand for tradable non-commodity goods, amplifying the macroeconomic adjustment mechanism of greater wage differentials through labour re-allocation away from the lagging commodity sector. Table 1 summarizes these results, which (i) reconcile the exchange rate puzzle by offering a theory as to why less-developed agricultural commodity exporters target their exchange rates in practice and (ii) these economies should transition to flexible exchange rates, as per conventional wisdom, as they develop over time.==== This paper contributes to three strands of literature.====First, several studies have analyzed macroeconomic policy options to deal with commodity revenue volatility. The literature has tended to focus on the design of fiscal frameworks that promote macro-fiscal stability, fiscal sustainability, and the accumulation of precautionary savings (reviewed in Baunsgaard et al., 2012), and a countercyclical fiscal stance (Frankel et al., 2013).==== Other papers have studied the stabilization properties of various short- and medium-term fiscal rules as an alternative to discretionary fiscal policy (Bi, Kumhof, 2011, C. Garcia, Tanner, 2011, Kumhof, Laxton, 2013, Snudden, 2016). Monetary policy choices have been analyzed in single-sector models with frictionless complete financial markets (Wills, 2013, Ferrero, Seneca, 2015).==== This does not account for the several distinguishing factors that influence monetary policy in commodity exporters, including factor re-allocation dynamics or that their financial markets are not deep (UNCTAD, IMF, 2016). I advance the research agenda by incorporating heterogeneous production, factor re-allocation, and financial market incompleteness. In contrast to the previous literature’s prescription of inflation targeting that relies on complete markets, I show that exchange rate targeting, instituted in practice by close to 80% of commodity exporters, is preferred with inflexible real and financial markets.====My work is also related to the open economy literature on incomplete financial markets. This literature refutes the existence of complete financial markets in the data, and shows that imperfect risk-sharing in open economy models is generally required to match relative price dynamics (Backus, Smith, 1993, Chari, Kehoe, McGrattan, 2002, Corsetti, Dedola, Leduc, 2008). While the majority of New Keynesian models nevertheless continue to use complete markets, a few studies have analyzed the appropriate choice of monetary policy with imperfect risk-sharing, albeit in single-sector models (Benigno, 2009, De Paoli, 2009, Corsetti, Dedola, Leduc, 2010). These papers find a case for exchange targeting under certain parameter configurations, as it can redress inefficient cross-country demand imbalances. I take this literature a step ahead by incorporating dual labour markets in an incomplete financial market world, and showing that the desirability of exchange rate targeting is contingent on the efficiency of factor re-allocation. My results are relevant to developing economies with export-intensive structures and inflexible markets. Finally, my paper adds to the burgeoning literature on monetary policy and factor re-allocation, which has previously focused on closed economy models with labour (Petrella and Santoro, 2011) and capital (Bouakez et al., 2009). I advance this research agenda by incorporating an open economy dimension with exchange rates, as well as incomplete financial markets.====The rest of the paper proceeds as follows. Section 2 provides stylized facts that motivate the theoretical analysis. Section 3 develops an open economy DSGE model that incorporates key structural features of agricultural commodity exporters. Section 4 calibrates the framework and estimates key parameters using a Bayesian maximum likelihood approach. Section 5 assesses the role of labour market inflexibility and product market development in influencing the appropriate choice of an exchange rate regime, and conducts welfare analysis. Section 6 concludes, discusses extensions, and offers policy recommendations.",The welfare implications of exchange rate choices in developing agricultural economies,https://www.sciencedirect.com/science/article/pii/S0164070420301592,16 July 2020,2020,Research Article,175.0
Unel Bulent,"Department of Economics, Louisiana State University, Baton Rouge, LA 70803 United States","Received 10 April 2020, Revised 8 July 2020, Accepted 13 July 2020, Available online 16 July 2020, Version of Record 21 July 2020.",https://doi.org/10.1016/j.jmacro.2020.103237,Cited by (0),"I use state-level ==== deregulation in the U.S. to study the causal impact of credit expansion on unemployment through its effects on the average monthly job-finding and job-losing rates. State-level analysis shows that deregulation increased the average job-finding rate and decreased the job-losing rate, and thus led to a lower unemployment rate. I also find that deregulation decreased the average unemployment duration. Extending the analysis to industry-state level, I find that the impact of deregulation on the job-finding rate is positive, but does not show any pattern across industries with respect to their needs for external ====. However, deregulation reduced the average job-losing rate, and the reduction monotonically increases with industries’ dependence on external ====.","This paper investigates how changes in credit constraints affect unemployment dynamics in the United States. Changes in credit constraints are identified by state-level deregulation policies that allowed expansion of banks within and across states. Exploiting the variation in the timing of banking deregulation in a difference-in-differences (DD) setting, I examine the causal impact of deregulation on average monthly job-finding and job-losing rates over the 1977–1999 period. I then extend my analysis to examine how banking deregulation affected unemployment dynamics in industries with respect to their dependence on external finance.====Results of this study can be summarized as follows. My state-level analysis shows that deregulation lowered the unemployment rate by increasing the average job-finding rate and decreasing the average job-losing rate. Specifically, the entry rate increased by 9 percent and the exit rate decreased by 12 percent (relative to the sample mean) following deregulation, resulting in about a 14 percent reduction in the unemployment rate. I also find that unemployment duration declined by 18 percent following the reforms. Extending my analysis to the industry-level, I find that the impact of deregulation on the average job-finding rate is positive, but does not show any pattern across industries with respect to their needs for external finance. However, deregulation reduced the average job-losing rate, and the reduction monotonically increases with industries’ dependence on external finance.====Before discussing related literature and the paper’s contribution to them, I want to discuss why banking deregulation can be used to identify the causal impact of credit constraints on economic outcomes. The U.S. banking industry was subject to extensive regulation throughout much of the twentieth century. States adopted policies to limit the geographic expansion of banks both within and across state boundaries. In 1970 there were only twelve states that allowed expansion of branches within their borders, and no states allowed cross-state bank ownership. These restrictive policies naturally created local monopolies from which state authorities extracted some rents (Kroszner and Strahan, 2014). However, as advances in technology and finance changed the role of banks in the economy, many banks began to lobby for the removal of these geographic restrictions. Starting in early 1970s states began lifting banking restrictions in two main ways. First, they allowed banks to open new branches anywhere within state borders. Second, they permitted out-of-state banks to acquire/open banks within their home state, and the passage of the Interstate Banking and Branching Efficiency Act of 1994 let states complete this process by 1997.====Reforms led to rapid expansion of new banks and branches and increased efficiency by reducing banking costs (Stiroh and Strahan, 2003), which in turn increased credit supply, especially for previously excluded businesses and households (Dick and Lehnert, 2010), (Krishnamurthy, 2013), (Favara and Imbs, 2015). In addition, as Jayaratne and Strahan (1996) argue, states did not pass these reforms in anticipation of future economic performance. Kroszner and Strahan (2014) give a detailed discussion about financial and political factors leading to the banking deregulation. Therefore, the banking deregulation has provided a natural experiment for researchers to investigate causal effects of credit constraints on various economic outcomes, including growth, income distribution, entrepreneurship, and so on.====There is now a large literature that investigates the implications of US banking deregulation for state economies. Previous studies have examined implications of deregulation for various economic outcomes, including per capita income and output growth (Jayaratne and Strahan, 1996), reallocation of labor and productivity gains (Bai et al., 2018), income and wage inequality (Beck et al., 2010), entrepreneurship and self-employment (Black and Strahan, 2002; Kerr and Nanda, 2009; Sarker and Unel, 2017), volatility in business cycles (Morgan et al., 2004), credit expansion and personal bankruptcy (Dick and Lehnert, 2010), mortgage loans (Tewari, 2014; Favara and Imbs, 2015), educational attainment (Sun and Yannelis, 2016), and foreign investment (Kandilov et al., 2016). However, limited attention is paid to its impact on unemployment dynamics.====My paper is closely related to Cetorelli and Strahan (2006), Beck et al. (2010), and Bostanifar (2014). Cetorelli and Strahan (2006) investigate the impact of banking deregulation on industry structure in U.S. manufacturing sectors. They find that banking deregulation led to more establishments and a smaller average establishment size in sectors with high external financial needs. Beck et al. (2010) examine the impact of branch deregulation on income distribution, and find that it substantially tightened income inequality (measured by Gini coefficient or Theil index) by raising incomes in the lower half of the distribution. They also consider the impact of deregulation on unemployment, and their event-study analysis shows that deregulation lowered the unemployment rate. Bostanifar (2014) also examines the impact of banking deregulation on employment, and shows that it had a substantially positive impact on employment growth in industries with higher labor intensity. He also shows that deregulation lowered the unemployment rate by nine percent.====My paper differs from Beck et al. (2010) and Bostanifar (2014) in two key aspects. First, they consider the impact of deregulation on the unemployment rate (a stock variable), whereas I investigate its impact on job-finding and job-losing rates (flow variables). Examining the impact on entry and exit rates helps us to better understand why unemployment changed. Second, I also investigate whether the impact differs across industries based on their needs for external finance.====This paper also relates to the literature that emphasizes the impact of credit constraints (especially during the recessions) on employment and other macroeconomic variables. This literature is vast and its full review is beyond the purpose of this paper. In this literature, my paper relates to two recent papers, Duygan-Bump et al. (2015) and Siemer (2019), which investigate the impact of the Great Recession of 2007–2009 on (un)employment dynamics in the United States. Duygan-Bump et al. (2015) show that during the recession, workers in industries with high external financial dependence were more likely to become unemployed, and the effect was stronger among small firms in these sectors. Siemer (2019) investigates the impact of the Great Recession on firm entry and employment dynamics. Using firm-level data, he shows that financial constraints considerably reduced employment growth in small relative to large firms. These papers examine the impact of credit constraints created during the Great Recession on employment dynamics, whereas I investigate the impact of credit expansion through banking deregulation on unemployment.====The paper is organized as follows. The next section discusses the data, explains the construction of key variables and presents summary statistics. Section 3 investigates the impact of banking deregulation on employment dynamics at the state level. Section 4 extends the analysis to the industry-state level, and explores possible explanations for why deregulation affected unemployment dynamics. Section 5 concludes the paper.",Effects of U.S. Banking Deregulation on Unemployment Dynamics,https://www.sciencedirect.com/science/article/pii/S0164070420301634,16 July 2020,2020,Research Article,176.0
"Dovern Jonas,Zuber Christopher","Friedrich-Alexander University Erlangen-Nürnberg and CESifo Germany,Alfred-Weber-Institute for Economics, Heidelberg University","Received 6 November 2019, Revised 7 July 2020, Accepted 13 July 2020, Available online 16 July 2020, Version of Record 1 August 2020.",https://doi.org/10.1016/j.jmacro.2020.103239,Cited by (4),"Economic crises lead to lower potential output (PO) estimates, but little is known about which components of PO are revised. Our paper answers the questions of how much, how fast, and how persistently estimates of the capital stock, of trend labor, and of trend total factor productivity are revised downwards after major economic crises. It shows that revisions to different components of PO contributed equally to the substantial overall decline in estimated PO levels. Revisions of trend labor are predominantly driven by revisions of the NAWRU. The heterogeneity of revisions across EU countries after the ==== is large, suggesting that different policies are needed to bring countries back to their previous growth paths.","The Great Recession of 2008/09 had long lasting effects on most major economies. In particular, estimates of potential output (PO) for many countries are substantially lower now than the corresponding pre-recession projections. It is well established that this is a common observation in the aftermath of major economic crises (Cerra and Saxena, 2008) and also that after 2008/09 many EU countries were among those countries which suffered most heavily following the Great Recession (Ball, 2014). It is less clear, in contrast, how large the relative importance of revisions to different components of PO has been for the overall decline of PO levels.==== This, however, is of importance for identifying the mechanisms via which deep recessions lead to long-run economic damage and for identifying suitable policies that might help pushing economies back to their previous growth paths.====Our paper answers the questions of how much, how fast, and how persistently estimates of the capital stock, of trend labor, and of trend total factor productivity (TFP) are revised downwards after major economic crises. We consider the Great Recession and the European sovereign debt crisis as two prominent examples of major economic crises. Our newly compiled real-time data set contains data for EU member states. Our research focus differs from other related papers that only look at overall revisions of PO (or gross domestic product (GDP) for that matter) and do not look at the components (e.g., Reinhart, Rogoff, 2009, Reinhart, Rogoff, 2014, Ball, 2014, Hosseinkouchack, Wolters, 2013, Blanchard, Cerutti, Summers, 2015). Our approach also differs because most previous papers consider PO growth rates and derive revisions to the level of PO without taking revisions to pre-recession potential growth rates into account (e.g., Benati, 2012, Haltmaier, 2012, Martin, Munyan, Wilson, 2015, Coibion, Gorodnichenko, Ulate, 2018) and/or—with the exception of Furceri, Mourougane (2012) and Dovern and Zuber (forthcoming)—do not carefully look at the timing of PO revisions due to the lack of real-time data.====Our paper shows that revisions to different components of PO contributed equally to the substantial overall decline in estimated PO levels after the Great Recession. It is the first paper that systematically analyzes the contributions of the revisions of components to the overall PO revision for a comprehensive real-time data set and a large country sample. By tracking revisions in real time, we provide information on the relationship of component revisions across time and on the dynamics of PO revisions made by experts over time. Our work contributes to a limited number of studies that focus on revisions to individual PO components after economic crises. Using ex-post data, Furceri, Mourougane (2012) show for 30 countries in the Organization for Economic Co-operation and Development (OECD) between 1960 and 2008 that the reductions of capital stock estimates account for most of the permanent decrease of PO after financial crises. Similarly, Haltmaier (2012) provides evidence that the capital-output ratio is the single most important contributor to lower trend output per capita after recessions for a sample of ten OECD countries while declines in trend employment and participation rates contributed to a fall of trend output only in some of the countries. With special focus on the Great Recession in the United States, Hall (2014) documents that a deterioration of the capital stock and lower TFP were the main drivers of permanently lowered trend GDP. Fernald (2015) confirms this finding over a larger sample period and argues that PO returned to a lower path after the exceptionally high TFP growth during the 2000s ended.====We find that the revision process after the European sovereign debt crisis was totally different. The European Commission (EC) seems to have fully incorporated lower PO paths in its estimates by 2011 and did not substantially lower its estimates further afterwards.====A methodological contribution of our paper is that we conduct the empirical analysis using a comprehensive real-time data set of estimates for PO levels by the EC. Our data cover 27 real-time vintages from autumn 2005 to autumn 2018 and 27 EU countries. The data set contains information about the contribution of the capital stock, trend labor, and trend TFP, respectively, to PO growth that allows us to derive the trend levels of these components for those vintages in which component levels are not readily available. This allows us to focus on revisions to the ==== of PO and its components. This is important because PO is often revised also for past time periods so that looking only at revisions of PO ====, as it has been done in most papers in this literature (e.g., Benati, 2012, Coibion, Gorodnichenko, Ulate, 2018, Furceri, Mourougane, 2012, Martin, Munyan, Wilson, 2015), systematically underestimates the true size of PO revisions.====PO is a measure that is commonly used to quantify the long-run production capacities of an economy (Okun, 1962, Havik, McMorrow, Orlandi, Planas, Raciborski, Röger, Rossi, Thum-Thysen, Vandermeulen, 2014). In modern macroeconomics, it is an important factor when deciding on monetary or fiscal policy. Central banks, for instance, evaluate how far the PO level is below (above) GDP to infer future inflationary (disinflationary) pressure when deciding on the appropriate monetary policy (e.g., Draghi, 2017, Yellen, 2017). Fiscal policy and, in particular, commonly used fiscal rules depend on the PO level. For example, government spending that increases productivity “raises [...] thereby future potential output, which increases fiscal space today” (Draghi, 2019). Thus, the PO level is an important indicator for designing and evaluating appropriate macroeconomic stabilization policies.====We show how sensitive PO estimates are in response to economic crises, providing evidence that policy makers should be careful to use pre-recession PO estimates when deciding on appropriate (monetary and fiscal) stabilization policies. Measurement error can lead to poor policy decisions. To mitigate the policy errors induced by a strong reliance on current PO estimation methods, policy makers could focus on observables (e.g., inflation and wage dynamics or survey data on capacity utilization), account for additional variables like the evolution of the current account or the credit growth when estimating PO (as suggested by Dovern and Zuber forthcoming), and/or account for model uncertainty by building a range of estimates (as, for example, recently done by González-Astudillo, 2019a, González-Astudillo, 2019b). In any case, PO and the output gap remain reasonable complementary measures for decision makers (e.g., Edge and Rudd, 2016, Guisinger, Owyang, Shell, 2018).====Since PO is defined to proxy the sustainable long-run level of output, it should in principle be independent of cyclical (temporary) fluctuations. However, it reacts to supply shocks that lead to a reassessment of trend TFP. Moreover, PO revisions could be driven by demand shocks in the presence of hysteresis (e.g., Blanchard, Summers, 1986, Blanchard, Summers, 1987, Lindbeck, Snower, 1986, Stadler, 1986, Stadler, 1990). Such hysteresis effects can potentially work through the different components of PO. If the hysteresis effects caused structural changes in the labor market, we would expect revisions of trend labor. If they worked through a depreciation of the capital stock or a decline of investment in physical capital, we would expect estimates of the capital stock to be revised. Finally, if they caused a decline in R&D investment, we would expect revisions of trend TFP. Thus, it is of great interest to investigate which components of PO experts revise downwards after economic crises. A third explanation for PO revisions is that previous estimation errors are simply corrected. Our results on the importance of the revisions to those three components of PO address the fact that empirically little is known about the relevance of the three channels.====The remainder of this paper is structured as follows. We present our data and explain the methodology that lies behind the PO estimates published by the EC in Section 2. Section 3 contains our empirical results. We first focus on revisions of PO estimates and then analyze the contributions of the component revisions to the revisions of overall PO. Section 4 concludes.",How economic crises damage potential output – Evidence from the Great Recession,https://www.sciencedirect.com/science/article/pii/S0164070420301658,16 July 2020,2020,Research Article,177.0
Frohm Erik,"Sveriges Riksbank, Stockholm SE-103 37, Sweden","Received 3 February 2020, Revised 3 July 2020, Accepted 8 July 2020, Available online 11 July 2020, Version of Record 21 July 2020.",https://doi.org/10.1016/j.jmacro.2020.103235,Cited by (3),"Economic slack plays an important role for ==== dynamics in conventional ====. Some have argued that this relationship, known as the Phillips curve, has broken down in recent years. However, due to the endogenous response of ","”The Phillips curve may be broken for good.” This sentence headlined ==== magazine Daily Chart section of the November 2017 issue==== and echoes a recent discussion among economists and policymakers on the validity of conventional macroeconomic models to describe the drivers of inflation.====At the core of the debate is the observation that the relationship between economic slack and inflation - the Phillips curve - has weakened (Borio, Filardo, 2007, Blanchard, Cerutti, Summers, 2015 and Auer et al. 2017) or even disappeared (Borio et al. 2018). The critics often point to the fact that product and labor markets have become increasingly integrated as a result of globalization trends and technological advancements, which could reduce cost increases and diminish the scope for firms to raise prices. Sweden has not been insulated from the debate and some have argued that the Swedish Phillips curve is ”disconnected”.==== Others (Ciccarelli et al., 2017) argue ==== or that the Phillips curve is alive and well (Gordon 2013 and Coibion and Gorodnichenko 2015).====The outcome of the debate has important policy implications. If the relationship has indeed broken down, policymakers at central banks might be forced to re-evaluate their main tools and rethink their prime objectives.====Common for most empirical studies of the Phillips curve relationship is reliance on aggregate data. But as discussed by McLeay and Tenreyro (2019) the aggregate Phillips curve relationship may indeed ==== to break down if a central bank is successful in conducting monetary policy to minimize welfare losses and achieve its inflation target. A structural Phillips curve relationship should still be present in more granular data if there are enough idiosyncratic demand shocks.====This paper explores the relationship between economic slack and price-setting of firms in Sweden by utilizing a novel firm-level data set from a large, reoccurring and representative business survey, the Economic Tendency Survey.==== The survey collects information about the essential ingredients in a New Keynesian Phillips curve framework: the respondents answer questions about their selling price changes, expectations of inflation in general and their economic slack. The micro data constitutes the backbone of this paper and contains around 600 quarterly observations of firms in the retail trade sector from the second quarter of 2010 to the first quarter of 2018.====Swedish data are particularly interesting when assessing the existence of a underlying Phillips curve. It is a small open economy highly reliant on global trade, with exports and imports accounting for more than 80 percent of GDP. Firms in Sweden are also deeply integrated in global supply chains (Tillväxtanalys, 2015) and subject to high price competition from digital platforms such as e-Commerce (Breman and Felländer, 2014). The share of e-Commerce in total retail sales in Sweden rose from about 4 percent in 2010 to around 10 percent in 2018.==== If the Phillips curve relationship has indeed broken down due to globalization trends or technological advances, it should certainly be visible in Swedish estimates.====Utilizing survey data to estimate a firm-level Phillips curve has precedents in the literature. Gaiotti (2010) utilizes the Bank of Italy’s Survey of Investment in Manufacturing to gauge the relationship between firm-level capacity utilization, annual price changes and firms’ exposure to international trade. Bryan et al. (2014) use the Federal Reserve Bank of Atlanta Business Inflation Expectations Survey to estimate a cross-sectional Phillips curve in the United States and Boneva et al. (2016) finds a firm-level New Keynesian Phillips curve in the United Kingdom for a panel of manufacturing firms from the Industrial Trends Survey performed by the Confederation of British Industry. These studies rely on data for manufacturing firms, or a mix of firms in industry and services. The price changes are therefore either producer prices or a mix of producer and consumer prices. Differently, this paper focuses on domestic prices set towards consumers, which are normally the target for monetary policy.====The analysis controls for firm-level inflation expectations (as in Bryan et al. 2014 and Boneva et al. 2016), heterogeneity (Gaiotti 2010 and Boneva et al. 2016) and different measures of economic slack (Bryan et al., 2014). Moreover, this paper deals with some of the endogeneity issues in the previous literature by utilizing ==== of firms’ responses to questions relating to economic slack and ==== price realizations. It also controls for time fixed-effects that capture aggregate developments affecting the price-setting of firms (monetary policy, the overall business cycle or inflation rates).====The results suggest that firms’ past assessments of their sales situation are highly correlated with current selling price decisions. Business survey respondents that perceived the sales situation to have been good in the previous quarter are more likely to respond that selling prices have increased, than firms with a satisfactory or poor sales situation. This finding is robust to the inclusion of firm-level inflation expectations, firm fixed-effects and year-quarter fixed-effects as well as different specifications of the firm-level Phillips curve. Firms assessments of their profitability, labor shortages and goods in stock tend to be insignificant when fixed-effects are added. The evidence in this paper thus suggests that a firm’s sales situation is a better direct indicator of economic slack with implications for pricing decisions, than other survey-based measures.====In addition to establishing a relationship between firms’ economic slack and their selling prices to consumers, this paper adds to our understanding of how qualitative responses in large business surveys relate to economic theory and opens up the door to researchers to address other topics using survey data (see for example, Coibion et al. 2018).",Price-setting and economic slack: Evidence from firm-level survey data,https://www.sciencedirect.com/science/article/pii/S0164070420301610,11 July 2020,2020,Research Article,178.0
Chen Shu-Hua,"Department of Economics, National Taipei University, 151 University Road, San Shia District, New Taipei City, 23741 Taiwan","Received 2 December 2019, Revised 27 June 2020, Accepted 29 June 2020, Available online 30 June 2020, Version of Record 3 July 2020.",https://doi.org/10.1016/j.jmacro.2020.103234,Cited by (6),Within heterogeneous-household extensions of Romer’s (1986) one-sector representative agent model of ,"High economic growth and a more equal income distribution are two main policy goals of government authorities. Whether there is a trade-off between these two policy goals, or that they go hand in hand with each other, is thus a crucial issue to explore. Empirical works studying this topic mainly have run panel regressions of real GDP per capita growth on the Gini coefficient (or other measures of income inequality) and other independent variables, offering mixed conclusion.==== In the theoretical literature that employs general equilibrium macroeconomic models, on the other hand, output growth and income equality are both endogenous variables. The theoretical correlation between these two variables thus can be positive or negative, depending on the underlying influencing policy or structural parameters.====In this paper, I focus on analyzing the macroeconomic consequences of progressive income taxation in heterogeneous-household extensions of Romer’s (1986) one-sector representative agent model of endogenous growth with elastic labor supply. To facilitate comparison with previous works, the benchmark specification postulates, as in Guo and Lansing (1998), Li and Sarte (2004), and Koyuncu and Turnovsky (2016), among many others, that a single-tax scheme is imposed on the household’s total income. The non-linear taxation formulation contains two tax-code parameters, which respectively characterize tax progressivity and the base tax rate, where the latter is the tax rate levied on households whose income equals the economy-wide average level. I introduce household time preference heterogeneity (Sarte, 1997). To simplify matters and without loss of generality, I further follow Li and Sarte (2004), Section II.A.) and Koyuncu and Turnovsky (2016), Section 5) in considering a two-group economy, wherein group 1 (2) contains impatient/patient households that possess a higher/lower rate of time preference, thereby becoming Poor/Rich households.====The analysis of the benchmark single-tax economy shows that, regardless of the model analyzed, the impatient households always earn less income, own less capital stock, consume more, and work more than the patient households. When income tax becomes more progressive, both income inequality and asset inequality improve, consumption inequality continuously deteriorates, while labor hour inequality and welfare inequality first improve and then deteriorate. Moreover, the impatient households enjoy a higher/lower level of welfare than the patient households under lower/higher degrees of tax progressivity, implying the existence of an interior tax progressivity that attains perfect welfare equality. I then show that an increase in the base tax rate deteriorates inequality in terms of income earned, capital stock owned, consumption, and hours worked; whereas, welfare inequality first improves and then deteriorates. Hence, there also exists an interior base tax rate that attains perfect welfare equality.====The above results imply that the income inequality-growth relation is ceteris paribus positive (negative) when tax progressivity (base tax rate) unilaterally changes.==== When implementing a tax reform, however, the fiscal authority may simultaneously change both tax progressivity and the base tax rate. The theoretical prediction would be that the resultant inequality-growth nexus can be positively- or negatively-sloped, depending on the relative size of the changes in tax progressivity and the base tax rate.====To obtain further insights, I carry out quantitative analysis on the effects of changes made to the U.S. statutory income tax code within a calibrated version of the model economy such that the combination of parameter values is consistent with post-war U.S. data. With regard to calibrating the tax-code parameters, I adopt Chen and Guo’s (2013) year-by-year non-linear least squares estimates from the U.S. federal individual income tax schedule for the 1966–2005 pre-crisis period. Several important findings are obtained from observing the model-simulated artificial time series of output growth and rich people’s relative income, where a larger value of the latter represents a higher degree of income inequality.====First, the model-simulated income inequality exhibits a discernible structural break upon implementation of the Tax Reform Act of 1986 (TRA-86). In particular, rich people’s relative income was below 1.24 during the pre-reform period between 1966 and 1986, and rose above 1.355 from 1987 onwards. This theoretical prediction turns out to be consistent with the data, whereby the U.S. Gini coefficient slightly fluctuated below 50% prior to 1986, and exhibited a noticeable rising trend and became higher than 50% after 1986. Hence, as with the theoretical findings of Altig and Carlstrom (1999) and Li and Sarte’s (2004) as well as the empirical evidence of Feenberg and Coutts (1993), Feenberg and Poterba (1993) and Feldstein (1995), this paper finds that TRA-86 ==== has resulted in a considerable increase in U.S. pre-tax income inequality.====Second, the artificial time series display positive long-run relationships between output growth and income inequality during both the pre- and post-reform periods. These positively-sloped theoretical long-run inequality-growth correlations emerge, because in both periods tax progressivity was way more volatile than the base tax rate. I further find that the slope of the theoretical inequality-growth nexus drastically declined by 44.06 percent between the pre- and post-reform periods. This implies that the post-reform economy ceteris paribus exhibits less sacrifice of real GDP per capita growth for a more equal income distribution.==== The above theoretical predictions are shown to be consistent with the observed positive long-run correlations between U.S. real GDP per capita growth and Gini coefficient over both the pre- and post-reform periods,==== as well as the sharp decline in the slope of the data’s inequality-growth nexus between the pre- and post-reform periods.====I next examine the growth and inequality effects of separate progressive tax schemes for labor income and capital income. The calibrated version of the model generates artificial time series of the effective marginal tax rates on labor and capital incomes that exhibit quite similar evolutionary patterns as the weighted average marginal tax rates on wages and interest received calculated according to NBER’s TAXSIM model. I find that while both tax progressivity and the base tax rate for both labor and capital incomes generate negative growth-rate effects, a more progressive labor/capital income tax aggravates/alleviates income inequality, and a higher labor/capital income base tax rate improves/deteriorates income inequality. I then show that the single-tax economy’s predictions on the impacts TRA-86 may have on the U.S. income inequality and inequality-growth nexus are robust to separation of labor and capital income taxes. Specifically, the separate-tax economy produces artificial time series that display a drastic structural break and deterioration in income inequality upon the implementation of TRA-86, as well as a sharp decline (by 31.48 percent) in the slope of the inequality-growth nexus between the pre- and post-reform periods.====The remainder of this paper is organized as follows. Section 2 describes the benchmark model with a single tax rate imposed on the household’s total income, and analyzes its equilibrium conditions. Section 3 explores the growth and inequality effects of the progressive tax structure, and investigates the extent to which the model accounts for the U.S. income inequality and inequality-growth nexus. Section 4 examines the robustness of Section’s 3 results to the separation of labor and capital income taxes. Section 5 concludes.",Inequality-growth nexus under progressive income taxation,https://www.sciencedirect.com/science/article/pii/S0164070420301609,30 June 2020,2020,Research Article,179.0
"Agénor Pierre-Richard,Jia Pengfei","University of Manchester, United Kingdom,Nanjing University, China","Received 15 April 2019, Revised 13 February 2020, Accepted 20 May 2020, Available online 23 June 2020, Version of Record 27 June 2020.",https://doi.org/10.1016/j.jmacro.2020.103220,Cited by (5),"This performance of time-varying ==== on cross-border bank borrowing is studied in an open-economy, ==== with financial frictions and imperfect ","A large body of evidence has shown that surges in capital inflows and outflows can contribute to financial instability—in the form of excessive credit growth, asset price pressures and, in some extreme cases, banking crises—even in countries with a floating exchange rate and an independent monetary policy. Temporary capital controls have been increasingly viewed by some economists and policymakers (especially in middle-income countries), as well as international financial institutions like the International Monetary Fund (2012), as a useful instrument—alongside monetary and macroprudential policies—for managing the risks associated with large swings in capital flows.====The case for imposing capital controls is often made on second-best grounds (see Dooley (1996)). Distortions in the domestic financial system, for instance, may cause resources borrowed from abroad to be allocated in socially unproductive ways in the domestic economy. In the absence of a well developed regulatory framework or adequate risk management practices in the financial sector, overborrowing can also increase (as noted earlier) financial vulnerability. If the distortion causing the problem cannot be removed, a second-best option may be indeed to limit foreign borrowing by the financial and nonfinancial sectors.====More recent analytical contributions have focused on the role of capital controls as a prudential instrument that can help to mitigate consumption volatility and increase welfare, or as a tool to reduce the probability of financial crises. These contributions include Bianchi (2011); Farhi and Werning (2012); Schmitt-Grohé and Uribe (2012); De Fiore and Tristani (2013); Bengui and Bianchi (2014); Costinot et al. (2014); Brunnermeier and Sannikov (2015); Heathcote and Perri (2016); Korinek and Sandri (2016); Davis and Presno (2017); Chang et al. (2015); Benigno et al. (2016); Kitano, Takaku, 2017, Kitano, Takaku, 2018, and Devereux and Yu (2019).====One strand of this literature motivates capital controls based on aggregate demand externalities in the presence of nominal frictions on the use of monetary policy. Farhi and Werning (2012), for instance, argued that a countercyclical capital controls policy can play an important stabilization role in a small open economy with a fixed exchange rate. They also argued that capital controls can mitigate the effects of excess international capital movements caused by risk premium shocks. Using a two-country model, De Fiore and Tristani (2013) showed that restricting international capital flows through capital controls can be beneficial for individual countries, although it would limit the ability to engage in risk sharing internationally. Devereux and Yetman (2014) considered the desirability of capital controls for an economy when its trading partner is in a liquidity trap. They found that capital controls can enhance the scope for monetary policy independence and improve welfare in the face of external shocks.====Another strand of this literature motivates capital controls, and more generally taxes on foreign borrowing, on the basis of the existence of pecuniary externalities. Benigno et al. (2016) developed models of foreign borrowing subject to collateral constraints and pecuniary externalities associated with the exchange rate. They showed that a credible commitment to a price support policy in the event of a financial crisis always welfare-dominates prudential capital controls, because it can achieve the unconstrained allocation. Bengui and Bianchi (2014) considered the implication of an environment in which the ability to enforce capital controls is limited. They showed that while leakages create distortions that make capital controls undesirable, to better stabilize the economy it may be optimal for the social planner to tighten restrictions on regulated households. They argued as well that, despite the presence of leakages, there are important gains from capital controls. Brunnermeier and Sannikov (2015) also studied the implications of pecuniary externalities in a two-country growth model with incomplete markets. In their model, short-term capital flows can be excessive because individual firms do not internalize that an increase in production capacity undermines their output price, worsening their terms of trade. In such conditions, capital controls aimed at limiting short-term borrowing can improve social welfare. Devereux and Yu (2019) also studied the performance of capital controls, as well as monetary policy, in an economy subject to abrupt shifts in capital flows and pecuniary externalities associated with collateral constraints.====Yet another strand characterizes capital controls as a tool to manage the international terms of trade. De Paoli and Lipińska (2013) described a model in which import and export taxes and subsidies are not available, and capital controls are instead tightened and loosened as competing concerns about output fluctuations gain and lose importance over the business cycle. Costinot et al. (2014) studied an infinite-horizon endowment economy with two countries in which one of them chooses optimal taxes on capital flows while the other remains passive. They showed that it is optimal for the strategic country to tax capital ==== if it grows faster than the rest of the world and to tax capital ==== if it grows more slowly. Finally, Heathcote and Perri (2016) considered a two-country, two-good world in which international financial markets are incomplete, in the sense that the only asset traded internationally is a non-contingent bond. This creates ==== a potential role for policy intervention. The intervention that they consider is an extreme form of restrictions on capital flows, in which asset trade is ruled out altogether. Their results show that capital controls are desirable if, in response to country-specific shocks, fluctuations in the terms of trade help to improve consumption risk sharing across countries.====This brief overview suggests that, by and large, the recent literature on capital controls has provided a number of channels through which such controls can improve welfare.==== However, the analysis in this paper differs from existing studies in several important ways. First, as in Escudé (2014); Chang et al. (2015); Davis and Presno (2017), and Devereux and Yu (2019), for instance, we use an open-economy stochastic general equilibrium model to study the benefits of time-varying controls on capital inflows. However, unlike most of these contributions, but in line with Benigno et al. (2016) and Kitano, Takaku, 2017, Kitano, Takaku, 2018, we do so in a model with banks and financial frictions—two features that are important to understand some of the negative externalities associated with capital flows from the perspective of financial volatility, such as excessive credit growth or asset price pressures. Second, again in contrast to most existing contributions, which tend to focus on controls on households or the nonfinancial sector, we focus on capital controls on bank-related short-term capital flows. Such flows have been an important component of cross-border capital flows in recent years, despite the partial international deleveraging process that followed the global financial crisis.==== And because in our base experiment capital controls are related to changes in (aggregate) bank foreign borrowing, they are tantamount to a macroprudential instrument. Third, we solve for the optimal, welfare-maximizing capital controls simple rule, and compare its performance to the Ramsey policy.====In our setting, the distortion that capital controls are aimed at correcting relates fundamentally to the procyclicality of the financial system. When world interest rates fall, domestic bank borrowing abroad increases; all else equal, banks borrow less from the central bank, thereby reducing the rate at which they refinance themselves at home. This lowers the rate at which domestic firms borrow and induces a credit expansion. Thus, lower world interest rates are associated not only with a standard reallocation of household portfolios but also an expansion in domestic lending, which—in line with the evidence relating credit growth and financial crises, reviewed by Agénor and Silva (2019), for instance—may exacerbate systemic financial risks. Capital controls, which essentially operate as a tax on bank foreign borrowing, help therefore to mitigate these risks. Moreover, by doing so the volatility of interest rates, and thus the volatility of consumption of employment, is mitigated, thereby implying that capital controls may also enhance welfare. Reserve requirements, as discussed in related closed- and open-economy contributions (see Agénor et al. (2018)), play fundamentally the same role—hence the issue of substitutability between these instruments, which is also addressed in the paper.====To study the performance of time-varying capital controls on cross-border bank borrowing, the model is parameterized for a middle-income country and is shown to replicate in the base experiment the main stylized facts associated with a shock to world interest rates: capital inflows, a real appreciation, a credit boom, asset price pressures, and an expansion in economic activity. A simple, implementable capital controls rule, based on changes in bank borrowing abroad, is first specified. Because its goal is to mitigate the volatility of (bank-related) capital flows, and thus indirectly financial volatility, the rule is fundamentally macroprudential in nature. We then study the joint optimal determination of simple, implementable countercyclical rules in terms of both controls on capital inflows and reserve requirements.====Our analysis shows that countercyclical capital controls designed to dampen bank foreign borrowing can indeed lead to significant welfare gains (relative to no intervention) in response to external financial shocks. In line with other studies, we also find that the optimal simple rule—as is often the case in the literature—does not perform particularly well relative to the Ramsey policy, either in terms of welfare or the volatility of core macroeconomic and financial variables. However, the volatility of the instrument under the Ramsey policy is also significantly higher, which could matter in practice if policymakers are concerned that abrupt changes in policy tools can destabilize markets. We also show that, in general, it is optimal to tax banks on both components of their market funding sources to maximize welfare. In addition, the implementation of a reserve requirement rule (which responds to credit growth) requires less reliance on capital controls. Thus, the two instruments are partial substitutes—at least in response to the type of external financial shocks considered in this paper. This is important because a common criticism of capital controls (especially when they begin to take a more permanent form) is that domestic agents eventually find ways to evade them. At the same time, it may be more difficult to do so for reserve requirements. Consequently, beyond the specific tools considered here, our results have broader implications for the ongoing debate regarding the extent to which countercyclical macroprudential instruments should be combined to promote financial stability.====The remainder of the paper is organized as follows. As background to the analysis, Section 2 outlines the nature of the financial frictions that are captured in the model and how the treatment of these frictions compares with the recent literature. Section 3 describes the model, which is a simplified version of the model in Agénor et al. (2018). In addition to accounting for capital controls on bank borrowing abroad, the model features imperfect capital mobility, a two-level financial intermediation system, and perfect exchange rate flexibility. It also accounts for the fact that the rate at which banks can borrow from the monetary authority incorporates a penalty rate (above and beyond a base policy rate, determined through a Taylor rule), which depends on the ratio of central bank borrowing to deposits. This assumption is essential for reserve requirements to play a countercyclical role.====The equilibrium and some key features of the steady state are briefly discussed in Section 4, and an illustrative calibration (designed to reproduce the main stylized facts associated with episodes of large capital inflows, as stated earlier) is presented in Section 5. The results of a temporary drop in the world risk-free interest rate are discussed in Section 6 As documented in a number of studies, shocks to world interest rates have often been a key impulse factor in explaining capital inflows (a “sudden flood,” in the terminology of Agénor, Alper, Silva, 2014, Agénor, Alper, Silva, 2018) to some of the larger middle-income countries in Asia, Latin America, and the Middle East. At the same time, these shocks have created significant challenges for policymakers. Following a drop in world interest rates, for instance, the scope for responding to the risk of macroeconomic and financial instability through monetary policy is limited, because higher domestic interest rates may exacerbate capital inflows. As an alternative option, a countercyclical capital controls rule is defined in Section 7 and its performance—in terms of maximizing welfare and mitigating volatility of key macroeconomic and financial variables—is evaluated in Section 8. A comparison with the Ramsey policy is also conducted. The joint determination of countercyclical reserve requirements and capital controls rules is discussed in Section 9. The last section provides some concluding remarks and discusses some potentially fruitful directions for future research.",Capital controls and welfare with cross-border bank capital flows,https://www.sciencedirect.com/science/article/pii/S0164070420301464,23 June 2020,2020,Research Article,180.0
Duarte Margarida,"Department of Economics, University of Toronto, 150 St. George Street, Toronto, ON M5S 3G7, Canada","Received 5 January 2020, Revised 7 June 2020, Accepted 10 June 2020, Available online 12 June 2020, Version of Record 24 June 2020.",https://doi.org/10.1016/j.jmacro.2020.103232,Cited by (0),"I study the patterns of manufacturing consumption expenditures across a broad set of countries that differ in their level of development using disaggregated expenditure and price data. The relative price of manufacturing tends to decline with income and the real share rises with income, particularly for countries in the top half of the income distribution. I find that the nominal expenditure share of manufacturing displays a hump-shape pattern with respect to the level of income per capita. I document that the income elasticities of the relative price of individual manufacturing categories lie in a wide range. However, since most categories have a negative elasticity, the average elasticity for manufacturing is negative. In addition, most aggregations of individual categories, regardless of the criteria used, yield manufacturing sub-sectors that feature a negative income elasticity of its relative price and the variation across income in nominal expenditure shares tends to mask a larger variation across income in real shares. Using a standard development accounting framework, I report large differences in productivity across countries for manufacturing categories. I also find some differences in productivity across countries for manufacturing sub-sectors, but these differences are smaller than the differences between manufacturing and services and considerably smaller than the differences across individual manufacturing categories.","One of the key stylized facts of economic development is the change in the economic structure as economies grow. Kuznets (1966) listed the patterns in the share of employment across broad sectors of agriculture, manufacturing, and services as one of the key development facts in modern economic growth. More recently, Herrendorf et al. (2014) document these facts using recent and historical data for a wide set of countries and long time periods showing the systematic pattern observed across countries and over time whereby the share of employment in agriculture falls systematically with income and the share of employment in services increases systematically with income, whereas the share of employment in manufacturing features a hump-shaped pattern with income (it first rises with income and then falls). Associated with structural transformation is the pattern of consumption across different expenditure categories; for instance, the falling importance of agriculture as income rises is reflected in a fall in the consumption expenditure share of food. Similarly, the rise in services with income is reflected in an increase in the expenditure share in services as countries develop.====In this paper, I document patterns in consumption expenditures and relative prices of manufactured goods and its components across a broad set of countries that differ in their level of development. This focus on patterns of manufacturing consumption follows a growing interest in understanding industrialization patterns across countries.==== While a growing body of literature has explored the extent and importance of patterns within the service sector, comparatively little is known about the manufacturing sector.==== This paper aims to provide evidence on the patterns of manufacturing expenditures and relative price across countries. I use detailed data on individual consumption expenditures by households from the International Comparisons Program (ICP) for 2011 to construct measures of consumption expenditures measured in domestic prices (nominal expenditures), measures of consumption expenditures in international prices common across all countries (real expenditures), and relative prices for different consumption aggregates for a large number of countries at a point in time.==== I first characterize consumption expenditures and relative prices for the three standard broad classes of consumption categories: food, manufacturing, and services. I find that the behavior of the nominal and real shares of food and services and the behavior of their relative price in this dataset is consistent with other data sources. In particular, the nominal and real shares of food decline with income as does its relative price, while the nominal and real shares of services rise with income as its relative price also rises. For manufacturing consumption, I find that the nominal share rises with income up to the eighth decile of the income distribution and declines after that. That is, the nominal share exhibits a hump-shape pattern with respect to the level of income per capita, similar to the well-documented behavior of production-based measures of economic activity in the manufacturing sector. The relative price of manufacturing tends to decline with income and the real share rises with income, particularly for countries in the top half of the income distribution. For manufacturing, and in contrast to food and services, variation across income in nominal expenditure shares masks a larger variation across income in real shares.====I then look at the behavior of individual manufacturing categories. I find that the behavior of the relative price of manufacturing reflects the behavior of the relative price of most of its individual categories. The range of the income elasticity of the relative price of individual categories is large but most categories feature a negative or mildly positive elasticity. This finding is in sharp contrast with evidence for the service sector which documents that the rising relative price of aggregate services with income is mostly driven by a few large expenditure categories (such as housing and health) while many individual service categories feature a relative price that declines with income (Duarte and Restuccia, 2020).====I further characterize expenditures and relative price patterns in manufacturing consumption across countries by grouping individual manufacturing categories according to different criteria. I find that expenditures in non-durable goods fall systematically with income and account for the observed decline in manufacturing expenditures at the top of the income distribution. For aggregations based on the purpose of use of goods, I find that expenditures in “Transport” and “Recreation and Culture” feature nominal and real shares that rise strongly with income. Finally, for aggregations based on industrial classification, I find that expenditures in goods from industries associated with the manufacture of machinery rise strongly with income. Overall, I find that, regardless of the criteria used, most aggregations yield sub-sectors of manufacturing that feature a negative income elasticity of its relative price. This finding reflects the fact that the income elasticity of the relative price of most individual categories is negative or mildly positive. In addition, I also find that for most sub-sectors and regardless of the criteria used, variation across income in nominal expenditure shares masks a larger variation across income in real shares. This finding suggests the importance of real measures of consumption expenditures and its drivers in analyzing cross-country manufacturing consumption expenditures.====I then study the productivity implications associated with the ICP expenditure and price data using a standard accounting framework with minimal structure as in Herrendorf and Valentinyi (2012) and Duarte and Restuccia (2020). I find that the magnitude of productivity differences across countries varies substantially across individual manufacturing categories. For instance, the productivity ratio between the richest and poorest 10 percent of countries is as high as 130-fold for some categories and 35-fold for some others, reflecting the wide range of income elasticities of the relative price of individual categories. I also find some differences in productivity across countries for manufacturing sub-sectors, but these differences are smaller than the differences between manufacturing and services and considerably smaller than the differences across individual manufacturing categories. This result largely reflects the fact that, for manufacturing sub-sectors, the behavior of relative price with income is relatively homogeneous. This result is, in fact, largely robust to alternative criteria for the aggregation of individual manufacturing categories that do not rely on the behavior of the relative price itself since the elasticity of the relative price of most individual categories is negative or mildly positive.====These results suggest that there is substantial scope in terms of aggregate productivity implications of pursuing disaggregated policies in the manufacturing sector, but these gains are reduced substantially among manufacturing sub-sectors that have similar composition of disaggregated manufacturing categories. Nevertheless, aggregate industrial-level policies or institutional reforms can be successful in closing the substantial productivity gap in this sector across countries.====This paper is related to the literature on structural transformation such as Baumol (1967), Echevarria (1997), Kongsamut et al. (2001), Ngai and Pissarides (2007), and Duarte and Restuccia (2010).==== It is also closely related to the literature characterizing expenditure patterns in the data, such as Herrendorf and Valentinyi (2012) and Duarte and Restuccia (2020) using cross-country data, and Atkenson and Ogaki (1996) across households and over time using micro data from India.====The paper is organized as follows. Section 2 describes the data used in this paper and Section 3 presents the patterns of consumption expenditures and relative price for manufactured goods. Section 4 explores the connection between the results in Section 3 and productivity. Section 5 concludes.","Manufacturing consumption, relative prices, and productivity",https://www.sciencedirect.com/science/article/pii/S0164070420301580,12 June 2020,2020,Research Article,181.0
"Chen Yunmin,Guo Jang-Ting,Krause Alan","School of Economics, Shandong University, No. 27 ShandaNanlu, Jinan City, 250100 Shandong Province, P.R. China,Department of Economics, University of California, Riverside, CA, 92521, USA,Department of Economics and Related Studies, University of York, Heslington, York, YO10 5DD, UK","Received 5 February 2020, Revised 1 June 2020, Accepted 2 June 2020, Available online 12 June 2020, Version of Record 3 July 2020.",https://doi.org/10.1016/j.jmacro.2020.103231,Cited by (0),Previous studies that examine optimal nonlinear taxation of savings/capital have assumed either full-commitment or no-commitment by the government. This raises the question as to whether the results under full-commitment and no-commitment provide upper and lower bounds on the optimal marginal ,"Previous studies that examine optimal nonlinear taxation of savings/capital have typically assumed full commitment by the government, while a few studies have considered no commitment. Under full-commitment, the government announces its tax policies for the present and future, and then simply implements those polices. Importantly, it is implicit that agents completely believe that the government will implement its announced policies. Under no-commitment, the government re-optimizes its tax policies period by period, irrespective of any previous promises or announcements. In this environment, individuals are aware that the government will re-optimize each period. It follows that under full-commitment, it is common knowledge among agents and the government that the probability of commitment is one, whereas this probability is zero under no-commitment.====In this paper, our aim is to examine a setting that falls between the aforementioned polar cases of full-commitment and no-commitment, and that better reflects realistic behavior and beliefs. To illustrate our thinking, consider for example corporate tax competition among governments. In order to entice a company to locate in its country, a government may promise to levy a low corporate tax rate. However, if the company does undertake operations in its country, the government will be tempted to raise its corporate tax rate. This temptation exists because the company is now a resident, and moving is costly. Moreover, when making its location decision, the company is aware of this temptation and the possibility that it may face higher taxation in the future. The government will then be aware that the company is aware of this temptation, and so on. If one were to assume full-commitment, it is common knowledge that the government will never succumb to this temptation. On the other hand, it is common knowledge that the government will always succumb to this temptation under no-commitment. In our view, a more realistic setting is one we call ‘commitment without credibility’. In this case, the government sets taxes as it would under full-commitment, but it is aware that individuals attach some probability to re-optimization. Therefore, the government’s promise to commit is not completely credible.====As in the related literature, we adopt the Mirrlees (1971) information-constrained approach to analyze optimal nonlinear taxation of labor income and savings under commitment-without-credibility in an infinite-horizon overlapping generations (OLG) model that is inhabited by two-ability-type (high-skill and low-skill) workers ==== Stiglitz’s (Stiglitz).==== Each agent is postulated to live for two periods: working in the first period and living-off savings in the second period. These features in turn create a redistributive role for taxation, as well as the possibility that individuals have doubt about the government’s commitment to its savings tax policy. In addition, based on the empirical evidence that citizens who receive higher (lower) education degrees and/or earn higher (lower) levels of income are more (less) likely to participate in public affairs, we first postulate that high-skill agents are sophisticated and can have heterogeneous beliefs about the government’s credibility. Given the redistributive objective associated with the tax policy, low-skill individuals will never want to choose the high-skill type’s allocation, whereas high-skill individuals may want to mimic low-skill workers by choosing their allocation. The government will deter such mimicking behavior by making sure that the allocations it offers satisfy the high-skill type’s incentive-compatibility constraints. We also postulate that low-skill agents are naive and have a common belief regarding the probability of commitment; but we do not need to specify its notation because the incentive-compatibility constraints for low-skill individuals are always slack and can be omitted. It follows that this commonly-shared belief does not play any role in deriving the optimal tax system.====In the context of a standard Mirrlees-style model in which agents differ only by their abilities or skills, the existing literature has found that zero marginal savings taxation is optimal under full-commitment as in Atkinson and Stiglitz (1976). But savings taxation will be progressive under no-commitment, in that the optimal marginal savings tax rates are increasing with respect to individuals’ skills (see, ====. Farhi, Sleet, Werning, Yeltekin, 2012, Brett, Weymark, 2019). As a result, it is tempting to view the results under no-commitment as providing the upper-bound on the size of the optimal marginal savings tax distortions. In the model economy that we consider, it is shown that if high-skill agents have the same beliefs regarding the probability of commitment, then the optimal marginal savings tax rates do always fall between those under full-commitment and no-commitment. Intuitively, high-skill individuals know that if the government re-optimizes the savings tax in the second period of their lives, it will redistribute some of their savings toward low-skill workers. This creates an incentive for high-skill agents to pretend as low-skill individuals. It follows that in order to deter mimicking, the government brings forward consumption by high-skill workers (through taxing their savings) and delays consumption by low-skill individuals (through subsidizing their savings). The intuition for the non-zero marginal savings tax rates under commitment-without-credibility is qualitatively identical to that under no-commitment discussed above, but they are closer to zero because there is some chance of commitment.====We also find that when high-skill agents have different beliefs, optimal nonlinear savings tax rates do not simply fall between those under full-commitment and no-commitment – this is our main result. Specifically, some high-skill workers may face larger marginal savings tax distortions than they would under no-commitment, even though all individuals believe that there is some probability of commitment. The explanation for this counter-intuitive finding follows from the possibility that high-skill agents may disagree ==== the probability of commitment. Under full-commitment and no-commitment, all workers know and agree that the probability of commitment is one and zero, respectively. Therefore, individuals differ only by their skills. Under commitment-without-credibility, agents differ by their skills as well as their beliefs. This makes the optimal tax problem more complicated, as it must now take into account this second source of heterogeneity, i.e. belief heterogeneity itself calls for marginal savings tax distortions. It is this new rationale on taxing savings that makes it theoretically possible for some high-skill individuals to face larger marginal savings tax distortions under commitment-without-credibility than they would under no-commitment. To gain further insights, we numerically show that this theoretical possibility can take place under a baseline calibration with empirically plausible values of model parameters, and that it remains qualitatively robust with respect to various parametric changes. In sum, the general point of our analysis is to illustrate that a minor relaxation of the full-commitment assumption, particularly when agents do not completely believe the government’s credibility to commit, may yield substantive effects on the optimal marginal savings tax rates.====In terms of policy relevance, we have developed an analytical framework in which the commitment-without-credibility government should impose a higher marginal savings tax rate on certain high-skill individuals than that under the no-commitment regime. Moreover, our numerical experiments find that this highest marginal tax rate on savings will become higher when (i) the degree of belief heterogeneity among high-skill workers rises, or (ii) there in an increase in the proportion of high-skill agents who believe that the government’s credibility is strong, or (iii) the ability/productivity gap between high-skill versus low-skill individuals is enlarged. It can be shown that any of the above three scenarios increases the information rent that high-skill workers may receive from their mimicking behavior. In order to induce truth-telling, the government will front-load these agents’ consumption through raising the top marginal tax rate on savings. These results are valuable not only for their theoretical insights to the academic literature, but also for their broad implications about the design and implementation of optimal nonlinear savings taxation within an infinite-horizon OLG model.====As it turns out, heterogeneous beliefs have been incorporated into some recent research in macroeconomics and finance; see, for example, David (2008) and Cogley et al. (2014), among others. However, this feature has not been extensively considered in the field of public finance theory. A notable exception is Blume and Easley (2006) that analyze the asymptotic properties of Pareto optimal allocations when heterogenous consumers differ in their beliefs about the state of the economy. In addition, our work is closely related to Farhi and Gabaix (2020) who examine nonlinear optimal taxation with behavioral individuals that exhibit heterogeneous beliefs. These authors show that in the presence of agents’ misperception, the properties of optimal nonlinear taxation stand in sharp contrast to the traditional results. Specifically, if poor agents do not fully recognize the future benefits of work, the optimal marginal tax rate of labor income for these individuals should be negative. Since Farhi and Gabaix (2020) study a static Mirrlees-style model, the implications of the intertemporal tax wedge (or the marginal savings tax rates) are not investigated. On the contrary, this paper examines constrained-efficient allocations in a dynamic commitment-without-credibility setting with unobservable skill as well as belief heterogeneities.====The remainder of this paper is organized as follows. Section 2 outlines our modelling framework, Section 2.1 analytically as well as numerically examines optimal nonlinear taxation of savings under commitment-without-credibility, and Section 4 concludes. A number of mathematical derivations are contained in three appendices.",The credibility of commitment and optimal nonlinear savings taxation,https://www.sciencedirect.com/science/article/pii/S0164070420301579,12 June 2020,2020,Research Article,182.0
Ng Wung Lik,"Department of Economics, National Cheng Kung University, Taiwan","Received 16 May 2020, Revised 26 May 2020, Accepted 2 June 2020, Available online 10 June 2020, Version of Record 15 June 2020.",https://doi.org/10.1016/j.jmacro.2020.103230,Cited by (23),"In this paper, we construct an extended SIR model with agents optimally choosing outdoor activities. We calibrate the model and match it to the data from the United States. The model predicts the epidemic in the United States very well. Without government intervention, our simulation shows that the epidemic peaks on 22 March, 2020 and ends on 29 August, 2022. By the end of the epidemic, more than 21 million people will be infected, and the death toll is close to 3.8 million. We further conduct counterfactual experiments to evaluate the effectiveness of different polices against this pandemic. We find that no single policy can effectively suppress the epidemic, and the most effective policy is a hybrid policy with lockdown and broadening testing. Lockdown policy alone is ineffective in controlling the epidemic as agents would have optimally stayed at home anyway if the infection risk is high even without a lockdown. Broadening testing solely will accelerate the return to normal life as there are fewer infected people hanging around. However, as people do not internalize the social costs of returning to normal life, the epidemic could get even worse. Increasing medical capacity without any other measures only has temporary effects on reducing the death toll. We also find that random testing is too inefficient unless a majority of population is infected.","Life is no longer the same for an American since the first case of COVID-19 recorded on 22 January, 2020. The confirmed cases have skyrocketed with more than a million American tested positive and tens of thousands of lives being taken away in about three months. States after states have issued the state of emergency order. Following the footsteps of many Asian and European countries, cities are locked down to tackle the virus. With such a strong measure on limiting the freedom of people, a question naturally arises: Is the measure effective and necessary?====One challenge for tackling COVID-19 is the high proportion of no symptoms or mild cases (or long incubation periods) which could continue spreading the virus. Broadening testing will keep people informed with their truth health status. This works together with an effective quarantine order could effectively lower the unaware infected people hanging around in the community and slowdown the spread. The strategy has been used by many countries without a lockdown.====The COVID-19 has already become a pandemic, but countries around the world are posing substantially different death rates. The case fatality rates vary from more than 10% in Italy and the United Kingdom to less than 1% in Singapore. The number is about 6% in the United States as of 8 May, 2020. One possible explanation to the large discrepancies in death rates is the collapse of healthcare systems in many countries. Increasing medical capacities, such as building temporary hospitals and producing emergency ventilators, are particularly important in lowering the death rates. The overall effect of medical capacity to the epidemic is another issue worth studying.====In this paper, we construct an extended SIR model and match the model to the available data in the United States. We simulate the benchmark model under the situation of no government intervention. By 29 August, 2022, more than 21 million of people will be infected in the whole epidemic and the death toll is close to 3.8 million. We then perform experiments to investigate the effects of different policies against the epidemic, including lockdown, broadening testing, increasing medical capacity and a hybrid policy combining lockdown and broadening testing. We find that no policies alone can successfully tackle the epidemic. Locking down only results in delaying the peak and the end of the epidemic because a second wave of infection will quickly emerge after reopening. Broadening testing solely lowers the risk of infection because fewer infected individuals are hanging around. However, people do not internalize the social costs of spreading the virus, and will choose to return to normal life too soon, leading to a worsened epidemic. Increasing medical capacity alone lowers the costs of infection initially. However, similar to broadening testing, more people will hang around, and the resulted increases in infection and medical demand will erase the gain of an enlarged medical capacity. In the end, we experiment on a combination of policies with lockdown and broadening testing and find that such policy successfully limits the spread of the virus. This is because when people do not internalize the social costs of spreading the virus, lockdown can effectively prevent unconfirmed infected cases from returning to normal life and spreading the virus further. Our simulation shows that the hybrid policy stops the epidemic completely.====As the pandemic of COVID-19 is still ongoing, the literature is rapidly growing. This paper is closely related to papers using a standard SIR model to study COVID-19. For example, ==== study the optimal lockdown policy. ==== extend the standard SIR model to study the economic interactions with the spread of virus. ==== focus on the matching mechanism in the SIR model. ==== extend an SIR model to SEIR in order to study the effect of testing and quarantine. Other papers include ====, ====, ====, ====, ====, ====. Lockdowns around the world also raises lots of research interests in working from home and people being affected by the stay-at-home order. ==== estimate that 34% of US jobs can be performed at home. Using American Time Use Survey, ==== analyze the sectors and groups of people that are most affected by the stay-at-home order. Other works examining different countries include ====, ====, ====.",To lockdown? When to peak? Will there be an end? A macroeconomic analysis on COVID-19 epidemic in the United States,https://www.sciencedirect.com/science/article/pii/S0164070420301567,10 June 2020,2020,Research Article,183.0
"Caraiani Petre,Luik Marc-André,Wesselbaum Dennis","Institute for Economic Forecasting, Romanian Academy, Romania,Bucharest University of Economic Studies, Romania,Helmudt Schmidt University, Germany,University of Otago, New Zealand","Received 17 October 2019, Revised 29 May 2020, Accepted 1 June 2020, Available online 9 June 2020, Version of Record 16 June 2020.",https://doi.org/10.1016/j.jmacro.2020.103229,Cited by (1),"In this paper, we study the effectiveness of ==== to stabilize the economy after a bursting asset price bubble. We estimate a ==== with an asset price bubble for the United States. We find that ==== does stabilize the economy in response to a bursting asset price bubble. However, credit policy is less efficient in response to the bubble compared to a capital quality shock. The stabilizing effect on output is only roughly 30% for a bubble shock compared to a capital quality shock. Further, while credit policy increases the recovery speed for a capital quality shock it does not affect the recovery speed after a bursting bubble. We also find different dynamics under a binding zero-lower bound, but our previous qualitative findings remain unchanged.","The global financial crisis (GFC, for short) has generated interest by policy makers and academics into the link between financial markets and the real economy. In particular, the effects of asset price bubbles have drawn increased attention Galí (2014); Galí and Gambetti (2015), and Martin and Ventura (2016).==== While bubbles are not a new phenomenon, the GFC has shown that conventional monetary policy (setting the short-term interest rate) has limits, especially when (bubble) shocks are large.==== During the GFC unconventional monetary policies have been developed to counter the adverse effects.====Asset price bubbles can have significant adverse effects on the real economy. Along this line, the role of monetary policy has been debated, for example in the lean (Cecchetti et al., 2000) and Farhi and Tirole (2012) vs. clean Mishkin (2017) debate. Galí (2014) and Galí and Gambetti (2015) investigate, theoretically and empirically, the link between monetary policy and bubbles. They show that monetary policy has a positive impact on bubble formation. Caraiani and Calin (2018) revisit the results accounting for the zero-lower bound and find that monetary policy shocks have a much smaller positive effect on bubbles.====New policy tools including macroprudential policy (Basel II and III) have been suggested as a way to enhance financial stability (Borio, 2003) and forward guidance is argued to be able to stimulate the economy, especially at the zero-lower bound Eggertsson and Woodford (2003) and Campbell et al. (2012). Most importantly, quantitative easing (QE, for short), i.e. unconventional monetary policy, has been extensively used by many central banks around the world.==== While the effects of QE are still debated (see Bhattarai and Neely, 2016 for a survey of the empirical literature), it has become a standard instrument in the tool kit of central banks. In this paper, we provide novel insights into the effectiveness of credit policy for stabilizing the economy after the burst of an asset price bubble. In addition, we also consider the additional restriction on monetary policy of a binding zero-lower bound constraint, as the GFC has shown that QE is used when conventional tools have been exhausted. Therefore, we combine the literature on unconventional monetary policy and the literature on asset price bubbles.====In order to answer our research question we take a well-established DSGE model of unconventional monetary policy (Gertler and Karadi, 2011) and introduce an asset price bubble process Galí (2014) and Galí and Gambetti (2015). We then estimate this model on data for the United States and discuss the stability of parameter estimates and the main driving forces of business cycles for the model with and without the bubble. Then, we simulate the model with and without the binding zero-lower bound constraint and consider unconventional monetary policy to stabilize the economy after a negative capital quality shock and a bursting asset price bubble.====In the following, we model an asset price bubble rather than a house price bubble for several reasons. First, the general specification of an ”asset” can also include houses. Second, while we do not dispute that the GFC started in the housing sector, we argue that the crisis reached the entire financial system because of the combination of subprime crisis, the downgrading of mortgage-backed assets, and mutual mistrust in the banking system. We believe that the latter stage is more similar to an asset price bubble than a housing price bubble in our model.====Our analysis reveals several new insights relevant for policy makers. For a negative capital quality shock, we find that unconventional monetary policy generates a sizable stabilization of the economy and prevents a deep recession. The reason is that it limits the increase in the financing premium and, therefore, stabilizes investment. However, for the asset price bubble we find that the stabilization effect is very small (around ten percent of output, roughly a third of the stabilization for a capital quality shock). In this case, unconventional monetary policy is not suitable to address this variable. It can only address the symptoms but not the cause of the recession. This is because for the asset price bubble shock, in contrast to the capital quality shock, the factor driving down investment is not related to the bank lending channel but is related to the price of capital, which is only indirectly affect by unconventional monetary policy. While the speed of recovery is faster with credit policy for the capital quality shock, it is unaffected for the bubble shock.====We find that the zero-lower bound constraint leads to different dynamics but does not change our main finding that unconventional monetary policy stabilizes the economy much more for the capital quality shock compared to the asset price bubble.====The paper relates to several streams in the literature. First, it adds to the literature estimating DSGE models with financial frictions. Sanjani (2014), Bekiros et al. (2016), and Villa (2016) estimate the Gertler and Karadi (2011) (GK, for short) model for the United States and Villa and Yang (2011) estimate this model for the UK. An alternative to the modeling by Gertler and Karadi (2011) is the approach by Bernanke et al. (1999) (BGG, for short).==== The key difference is that GK explicitly model the source of the financial friction emerging from within the banking sector using a moral hazard problem. Villa (2016) finds that the GK model empirically outperforms the BGG framework, as the financial accelerator effect in the GK model is stronger than in the BGG model. Miao and Wang (Forthcoming) develop a new theory of rational stock market bubbles by using an endogenously derived credit constraints that is identical to the one derived from the incentive constraint in GK. They investigate the existence of deterministic and stochastic bubbly and bubbleless equilibria. Second, we add to the growing literature on bubbles in DSGE models. Papers include Christiano et al. (2010), Wang and Wen (2012), Liu et al. (2013), and Galí (2014). Third, we add to the literature on the question whether monetary policy should respond to bubbles. Gruen et al. (2005), by building a stylized macroeconomic model, find that the optimal respond by the central bank depend on the information about the stochastic properties of the bubble. Robinson and Stone (2006) find inconclusive theoretical predictions about whether monetary policy should operate differently, when they detect a bubble in the presence of a ZLB constraint.====The closest paper to ours is the work by Miao and Wang (2015). They build a continuous-time model, similar to GK, with a banking sector and show that bubbleless and bubbly equilibria can, under certain assumptions, exist. Then, they explore how credit policies (direct lending, discount window lending, and equity injections) affect the economy after a bursting, stochastic bubble. They find credit policy can stabilize the economy after a bursting bubble. The largest stabilization is found for the equity injection policy scenario. This paper differs from ours in many aspects. First, we develop a full-blown, discrete-time DSGE. Second, they analyze banking bubbles while the specification used in this paper allows to capture bubbles more broadly. Third, they do not consider dynamics at the zero-lower bound. Finally, while we estimate our model, Miao and Wang (2015) calibrate their model to match stylized facts of the U.S. economy.====The paper is structured as follows. Section 2 develops our DSGE model. Section 3 discusses the details of the estimation strategy and presents the estimation results. Section 4 performs the policy exercises and Section 5 concludes.",Credit policy and asset price bubbles,https://www.sciencedirect.com/science/article/pii/S0164070420301555,9 June 2020,2020,Research Article,184.0
Sawadogo Pegdéwendé Nestor,"Université Clermont Auvergne, CNRS, IRD, CERDI, F-63000 Clermont-Ferrand, France","Received 19 October 2019, Revised 15 May 2020, Accepted 20 May 2020, Available online 4 June 2020, Version of Record 17 June 2020.",https://doi.org/10.1016/j.jmacro.2020.103214,Cited by (6),"A number of countries have introduced fiscal rules to deter fiscal profligacy, enhance the credibility of fiscal policy, and reduce borrowing costs. In this paper, we examine the outcome of fiscal rules in terms of improving financial market access for developing countries. We use entropy balancing and various propensity score matching. We find that the adoption of fiscal rules reduces sovereign bond spreads and increases sovereign debt ratings for a sample of 36 developing countries, which are part of the JP Morgan Emerging Markets Bond Index Global (EMBIG), for the period 1993-2014. We explain this finding by the effect of fiscal rules on the credibility of a country's fiscal policy: more credible governments are rewarded in the ==== by low sovereign bond spreads and high sovereign debt ratings. These results are robust to a wide set of alternative specifications. We also show that this favorable effect is sensitive to several country structural characteristics. Our findings confirm that the adoption and sound implementation of fiscal rules is an instrument for policy makers to improve developing countries’ financial market access.","Fiscal policy is an important instrument which Developing Countries (DCs) can utilize to accelerate their development process by reducing inequalities (Azzimonti et al., 2014; Larch and Turrini, 2010; Milasi, 2013), improving economic growth (Stiglitz, 2015; Summers, 2014), and improving well-being (Bom and Ligthart, 2014; Ganelli and Tervala, 2016). To be more effective in addressing these development challenges, any fiscal policy must be sound (Dabla-Norris et al., 2010; Hameed, 2005; Prakash and Cabezon, 2008, etc.). Mastering debt and sound public finances are key factors in mobilizing financial resources in developing countries (Reinhart et al., 2003; Reinhart and Rogoff, 2010, etc.).====The role of fiscal rules in improving fiscal outcomes has been stressed in the literature (Corbacho and Schwartz, 2007; Debrun et al., 2008; Debrun and Kumar, 2007; Deroose et al., 2006; Guerguil et al., 2017; Kopits, 2004; Schaechter et al., 2012; Tapsoba, 2012). However, few studies have shed light on the link between fiscal rules and financial market access in DCs: examples include Afonso and Jalles (2013), and Thornton and Vasilakis (2017) who investigate the effects of fiscal rules on risk premiums in a mixed sample of advanced and developing countries. The effects of fiscal rules might be different depending on the type of economy. The originality of our paper is that it extends the literature by exploring both the heterogeneity and the interactive effects of various types of fiscal rules on financial market access in developing countries. It then shows the differences between balanced budget rules, debt rules, expenditure rules, and shows their interactions. It also tackles the self-selection problem by using an effective empirical methodology, namely entropy balancing, and alternative matching.====We consider two measures of financial market access in this paper - sovereign bond spread and sovereign debt rating. Sovereign debt rating is an assessment of credit risk i.e. the possibility that the debtor will not fulfil its obligations in full and on time (Ferrucci, 2003). For sovereign debt, the risk of default depends on the fundamental characteristics of the issuer, and the ability of the lender to enforce the contract. Bond spread reflects market risk (the possibility that secondary market bond prices may move against the bondholder), and liquidity risk (the risk that investors will not be able to liquidate their portfolios without depressing secondary market prices). The proponents of the efficient market hypothesis argue that investors are rational and able to exploit all the available information to discriminate among borrowers. (Edwards, 1984) highlights that asset prices always reflect the information publicly available, as evidenced by the yield differential on bonds issued by sovereign borrowers with different credit ratings and macroeconomic characteristics. If the efficient market hypothesis holds, investors and rating agencies share the same interpretation of the body of public information pertaining to sovereign risks (Cantor and Packer, 1996). However, the opponents of this hypothesis emphasize that market failures and imperfect information lead to distortions in asset pricing (Calvo and Mendoza, 1996; Chari and Kehoe, 1997).====Better financial market access leads to lower bond spreads and higher sovereign debt rating. Fig. 1 illustrates the change in the average bond spreads and debt ratings, for countries which have adopted FR compared to Non-FR==== countries. The evidence is clear, adopting fiscal rules is associated with lower bond spreads and higher debt rating in developing countries.====Our estimates for a panel of 36 emerging markets economies for the period 1993 to 2014 show that the adoption of FR matters for financial market access in DCs. Indeed, countries which have implemented FR show a lower sovereign bond spread and a higher sovereign debt rating. Regarding the types of fiscal rules, we find that Budget Balanced Rules (BBR) and Debt Rules (DR) significantly improve financial market access, but Expenditures Rules (ER) do not improve this access. However, the combination of ER and multi-year expenditure ceilings (MEC) improve financial market access. These results are robust to a wide set of alternative specifications of the entropy balancing method, and the alternative matching method.====Our findings suggest that DCs could improve their financial market access by adopting fiscal rules. More specifically they should give more importance to BBR and DR because they are valued by financial markets in terms of lower bond spreads and higher debt rating. For countries aiming at adopting ER, sound PFM systems (including multi-year expenditure ceilings) are needed to enhance their effectiveness and improve financial market access.====The remainder of the paper is structured as followed. Section 2 discusses the related literature. Section 3 describes the data, provides some stylized facts and details the underlying method. Section 4 summarizes the main econometric results. Section 5 explores their sensitivity. Section 6 concludes with some policy recommendations.",Can fiscal rules improve financial market access for developing countries?,https://www.sciencedirect.com/science/article/pii/S0164070420301403,4 June 2020,2020,Research Article,185.0
"Almosova Anna,Burda Michael C.,Voigts Simon","Technische Universität Berlin,Humboldt-Universität zu Berlin, CEPR and IZA,International Monetary Fund","Received 7 July 2019, Revised 29 February 2020, Accepted 15 April 2020, Available online 2 June 2020, Version of Record 14 July 2020.",https://doi.org/10.1016/j.jmacro.2020.103209,Cited by (0),"We investigate business cycle dynamics of ==== (SSC), by far the largest labor ==== in the ","Throughout the developed world, social insurance programs – including unemployment benefits, work disability insurance programs, health insurance, old-age pensions as well as other programs aimed at social inclusion – redistribute a significant share of national income. Most financing for this redistribution comes from dedicated payroll taxes, also known as social security contributions (SSC). SSC represent more than half of total labor income taxation in OECD member states, and thus dominate the difference between employers’ costs of labor and monetary benefit received directly by workers. In 2017, total SSC in OECD countries amounted to about $4.4 trillion, or 9.2% of GDP; in many economies, social security contributions represent more than a third of total labor compensation. Even if workers perceive these contributions as payment for social benefits, SSC represent a significant potential distortion in the worker-firm relationship.====This paper investigates the cyclical behavior of social security contributions in 25 OECD countries over the period 1960-2015. To our knowledge, the dynamics of SSC over the business cycle have yet to be systematically studied.==== We document that for a majority of countries and time intervals, average SSC rates (defined as the total SSC divided by total gross labor compensation) vary counter-cyclically with respect to growth and output, especially at business cycle frequencies, declining in booms and rising in recessions. This feature is not shared by all countries, especially where social security budgets are cross-subsidized by other revenue sources. Because SSC represents a large part of the average burden of a worker-job match in a frictional labor market, this cyclicality has potentially important implications for labor market dynamics over the business cycle. Below, we show that this distortion co-moves with output and employment movements at business cycle frequencies as a “labor wedge” (Chari et al. 2007).====Average SSC tax rates can vary over time for two reasons. First, holding tax schedules constant, cyclical shifts in the distribution of gross labor earnings can increase or decrease the SSC burden relative to the wage bill, due to different tax rates applying to different income brackets. Second, social security systems are mandated in many countries to run balanced or near-balanced budgets, requiring tax schedule adjustments to meet revenue shortfalls in recessions or to trim surpluses in periods of economic expansion. This “Bismarckian principle” originates in political constraints to regulate, or even minimize the degree of redistribution between capital and labor. In contrast, balanced budget requirements are typically absent in social insurance systems in which redistribution plays a larger role and cross-subsidy of social security funds is more common, in the spirit of Beveridge (Beveridge, 1944). Esping-Andersen (1990) distinguishes between corporatist welfare systems that dominate Continental Europe, reinforce traditional society and protect individuals from the “logic of class opposition” from those that emphasize the “decommodification of labor,” either in a liberal laissez-faire or social welfare state context (Esping-Andersen 1990, p.40).====To quantify the relative importance of exogenous changes in SSC tax rates versus endogenous changes in the structure of earnings, we propose an accounting framework for decomposing movements in average SSC rates. We find that the lion's share of observed changes in annual average contribution rates originates in adjustments to statutory tax schedules. In the spirit of Barro and Redlick (2011), we show that these average tax burdens co-vary with average marginal tax rates, with the latter exceeding the former in almost all cases. Our estimates of average SSC rates are also correlated in many countries with measures of the “labor wedge,” the reduced form labor market distortion described by Chari et al. (2007) and Brinca et al. (2016) in their business cycle accounting framework.====In Section 2 we present the data and descriptive statistics that motivate our interest in SSC. Section 3 discusses long-run features of social security contributions in our sample. Section 4 presents more empirical findings on cyclical properties of contribution rates and evaluates sources of this behavior at cyclical and lower frequencies. Section 5 examines the behavior of the SSC tax burden in the context of the business cycle accounting framework of Chari et al. (2007). Section 6 concludes.",Social Security Contributions and the Business Cycle,https://www.sciencedirect.com/science/article/pii/S016407042030135X,2 June 2020,2020,Research Article,186.0
"Ouerk Salima,Boucher Christophe,Lubochinsky Catherine","Department of Finance, ESCP Business School, Paris, France,Economix, University Paris-Nanterre, Nanterre, France,Economix, University Paris 2 ASSAS, Paris, France","Received 7 May 2019, Revised 17 April 2020, Accepted 20 May 2020, Available online 2 June 2020, Version of Record 9 June 2020.",https://doi.org/10.1016/j.jmacro.2020.103219,Cited by (7),We assess transmission mechanisms and the macroeconomic impact of ,"Unconventional monetary policies (UMP) at the zero lower bound (ZLB) have attracted various criticisms, from increasing wealth inequalities by stimulating asset prices (see e.g. Domanski et al., 2016) to having no effect on the real economy, as well as being complex to communicate for the public====. These negative spillovers have reinforced the debate on the utility and effectiveness of the monetary policies themselves.====We contribute to this debate by assessing transmission mechanisms and the macroeconomic impact of the unconventional tools used by the European Central Bank (ECB) since 2009. Our empirical approach combines shadow short rate (SSR) estimates from Wu and Xia (2017) to gauge the stance of the monetary policy at the ZLB and a Factor-Augmented Vector Autoregressive (FAVAR) econometric model that incorporates a larger amount of economic information.====For decades, the monetary policy instrument of leading central banks has been the nominal short-term interest rate. Since its creation in 1998, the ECB has used its set of key interest rates as the primary instrument of monetary policy, lowering the rates to provide more stimulus and raising them to slow economic activity and control inflation, through open-market operations and standing facilities. The drawback of this instrument has long been recognized: the existence of a ZLB. If money is stored without cost, then the rate of interest can never fall below zero. With this ZLB on the short-term nominal interest rate, when inflation is low, real rates cannot fall further to help reduce high debt burdens and support aggregate demand. While some central banks have adopted a negative interest rate policy to counter low inflation (ECB, Bank of Japan, Sveriges Riksbank), demonstrating that the ZLB is slightly less binding than previously thought, several UMP have also been applied to support aggregate demand, such as balance sheet operations or quantitative easing and forward guidance.====More precisely, our paper has three distinct but complementary objectives. First, we analyse the transmission mechanisms of monetary policy at the ZLB in the Euro Area as a whole and quantify the macroeconomic impact of unconventional monetary policies. Second, we assess cross-country differences in monetary policy transmission across Euro Area countries. Finally, we gauge the stability of transmission mechanisms before and during the ZLB.====Our empirical strategy is explained hereafter. To assess the effectiveness of UMP in the Euro Area, we compute impulse responses using a FAVAR that incorporates 40 Euro Area variables over the period 2009M1-2016M9.We also estimate the model over a longer period (2004M1-2016M9) in order to produce a counterfactual analysis comparing scenarios with/without unconventional policies at the ZLB. We follow Giannone et al. (2012), Altavilla et al. (2014) and Wu and Xia (2016) and established two scenarios. For scenario (1), we take the realized values of the selected variables. For scenario (2), we choose a SSR, which remains above the ZLB.====Note that we not only use the traditional macroeconomic variables taken from Boivin et al. (2008), but we also dis-aggregate the rate and volume of loans granted by banks, variables commonly used in empirical studies, into a range of variables such as short term (ST) and long term (LT) consumer loans, corporate loans and real estate loans. This choice is justified by the fact that these variables may react differently to a monetary policy shock.====To assess the potential cross-country heterogeneity of the monetary policy transmission at the ZLB, our second objective, we estimate a country-specific FAVAR for some selected countries (Germany, France, Italy, Spain, Portugal, and Greece) over two sub-samples, before and during the ZLB (2004M1-2008M12 and 2009M1-2016M9), while computing and comparing impulse responses for each sub-sample. The selection of countries for the sample was driven by different considerations. We selected major core and peripheral countries of the Euro Area with comprehensive and available monthly data on economic and financial conditions while excluding countries not in the Euro Area over the entire period of the study====.====Empirical results suggest that: (1) the transmission takes place mainly through the interest rate channel and the exchange rate channel at the ZLB, while the credit channel comes partially into play. This confirms that injections of liquidity into the banking system by ECB have not been used enough to finance new corporate investment. More importantly, (2) UMP does not influence inflation, given that the reaction of consumer and industrial prices to an UMP shock are not significant. Results obtained from the counterfactual exercise show that(3) monetary policies implemented at the ZLB revived the economy as a whole since there was a positive impact on investment and consumption which respectively increased by 9% and 2% while the impact on employment came later (in 2015). Moreover (4) the impact of the monetary policy on interest rates and the real economy was higher and more persistent in the Euro Area before the ZLB. Finally, (5) the difference in the transmission of monetary policy between countries of the Euro Area was more pronounced during the 2009-2016 period. Spain and Greece appeared as the least sensitive to monetary policy shocks at the ZLB.====Our key contribution is the identification of the mains transmission channels operating across the Euro Area before and during the ZLB period and the highlighting of the cross-countries heterogeneity, which allow us to compare and explain the macroeconomic effects of ECB's conventional and unconventional monetary policies. In particular, our paper completes different literatures, as specified in the next section.====The rest of the article is organized as follows. Section 2 presents the related literature and justify our approach. Section 3 describes the econometric model, data used and preferred specifications. Sections 4, 5 and 6 summarize empirical results about respectively the effectiveness of the UMP in the Euro Area, the heterogeneous transmission across countries and the stability of monetary policy at the ZLB. The robustness tests and the conclusion appear in sections 7 and 8.",Unconventional monetary policy in the Euro Area: Shadow rate and light effets,https://www.sciencedirect.com/science/article/pii/S0164070420301452,2 June 2020,2020,Research Article,187.0
"Acevedo Sebastian,Mrkaic Mico,Novta Natalija,Pugacheva Evgenia,Topalova Petia","International Monetary Fund 700 19th St NW, Washington, DC 20431","Received 16 April 2019, Revised 30 March 2020, Accepted 3 April 2020, Available online 1 June 2020, Version of Record 20 June 2020.",https://doi.org/10.1016/j.jmacro.2020.103207,Cited by (39),"Global temperatures have increased at a historically unprecedented pace. This paper finds that the negative effect of temperature on output in countries with hot climates runs through reduced investment, depressed labor productivity, poorer human health, and lower agricultural and industrial output. We find that hot low-income countries suffer the largest costs. In a median low-income country, aggregate output is about 2 percent lower and investment is about 10 percent lower seven years after a 1 degree increase in average annual temperature. We also find that economic development, in general, helps to shield countries from temperature shocks, with hot regions in high-income countries on average sustaining less economic damage from rising temperatures than hot regions in low-income countries.","Since the turn of the 20th century, the Earth's average surface temperature has increased at a speed that is unprecedented for at least the past 20,000 years (Figure 1).==== Most scientists agree that global temperatures are set to rise further. A rise in average temperatures by 4°C or more is projected by the end of the century in the absence of further action to restrain greenhouse gas (GHG) emissions. To limit warming to less than 2°C, dramatic cuts to current emissions would be needed (IPCC, 2014). In either case, both the speed and the eventual magnitude of the increase in average temperature will be historic.====The pioneering work of Dell, Jones, and Olken (2012) and Burke et al. (2015a) offered evidence that higher temperatures significantly reduce economic growth in low income and warm countries.==== But less is known about the specific channels through which growth is affected at the aggregate level. Having a detailed understanding of the main channels of impact—on a macroeconomic level, both empirically and theoretically—is necessary as governments and multilateral institutions seek a robust defense system against global warming over the next 100 years. Yet, the vast majority of prior empirical studies that consider various channels of the economic impact are done at the micro-level (see reviews in Dell, Jones and Olken, 2014; Carleton and Hsiang, 2016).====We provide new evidence on the effects of weather shocks on economic activity, the persistence of these effects, and the channels through which they operate. We offer a unified empirical framework, with a more flexible specification than Dell, Jones and Olken (2012) or Burke et al. (2015a), and use an expanded dataset from more than 180 economies over 1950–2015. We exploit the annual variation in temperature and precipitation to estimate their causal effect on aggregate economic activity, sectoral output and each of the key inputs of the aggregate production function: productivity, capital investment, and labor supply.====We contribute to the literature in several ways. To the best of our knowledge, we are the first to demonstrate that temperature shocks negatively impact investment globally, in both the short and long run. Since investment drives capital deepening and economic growth in the long run, understanding how climate change affects investment is vital.==== We show that in a median low-income country, seven years after a 1 degree increase in average annual temperature, investment is 10 percent lower, an economically large impact. For comparison, seven years after the same temperature shock, aggregate output is about 2 percent lower.====For productivity and labor supply, prior studies documented the negative effect of temperature increases only in specific micro settings, such as experiments, or within individual countries.==== Similarly, for sectoral output—agriculture and industry—the prior literature tended to focus on narrower samples, considering regions, countries, or specific crops.==== The likelihood of finding statistically significant results within micro settings is much greater than in our macro case, and may leave room for skeptics to question the generalizability of the findings. The strong negative findings we report, taken together, dispel any notion that these effects might be localized, sporadic, or economically small.====Second, our findings demonstrate that hot countries, which are overwhelmingly low-income, suffer the most from an increase in temperature. We thus confirm the findings by Dell, Jones and Olken (2012) and Burke et al. (2015a) of the uneven effects of temperature increases. But we are also able to demonstrate that the large and long-lasting aggregate damages are due to reduced capital accumulation, depressed labor productivity in heat-exposed sectors, poorer human health, and lower agricultural and industrial output.====Finally, we also shed light on the debate whether economic development shields countries from the negative effects of climate change (Burke and Tanutama, 2019). Using subnational data, we find that hot regions in high-income countries on average sustain less economic damage than hot regions in low-income countries. This is an important finding, as it suggests that general economic development policies could complement any climate adaptation strategy.====The rest of the papers is organized as follows. Section 2 presents some key stylized facts about historical patterns of temperature and precipitations as well as scientific projections of future changes. Section 3describes the data and lays out the empirical strategy used to assess the macroeconomic effect of weather shocks. Section 4 presents the main findings and several robustness checks of the empirical results, while Section 5 looks at the channels through which aggregate economic output is affected. Section 6 presents results on the role of development based on subnational data. Section 7 concludes.",The Effects of Weather Shocks on Economic Activity: What are the Channels of Impact?,https://www.sciencedirect.com/science/article/pii/S0164070420301336,1 June 2020,2020,Research Article,188.0
"Fisher Jonathan D.,Johnson David S.,Smeeding Timothy M.,Thompson Jeffrey P.","Stanford University,University of Michigan, 426 Thompson St, ISR 3234, Ann Arbor, MI 48106, USA,University of Wisconsin,Federal Reserve Bank of Boston","Received 6 June 2019, Revised 28 April 2020, Accepted 20 May 2020, Available online 30 May 2020, Version of Record 9 June 2020.",https://doi.org/10.1016/j.jmacro.2020.103218,Cited by (18),"Studies of economic ==== almost always separately examine income, consumption, and ==== distribution. This heterogeneity in the response to income changes can have significant impact on the effectiveness of government fiscal policy. Using the Panel Study of Income Dynamics from 1999–2013, we examine how the marginal propensity to consume (MPC) differs across the wealth distribution. We find that the MPC is lower at higher wealth quintiles, indicating that low wealth households cannot smooth consumption as much as other households. This implies that increasing wealth inequality likely reduces aggregate consumption, which, in turn, could limit economic growth.",None,"Estimating the marginal propensity to consume using the distributions of income, consumption, and wealth",https://www.sciencedirect.com/science/article/pii/S0164070420301440,30 May 2020,2020,Research Article,189.0
Ida Daisuke,"Department of Economics, Momoyama Gakuin University, Japan,Research Fellow, Graduate School of Economics, Kobe University, Japan","Received 9 October 2019, Revised 10 April 2020, Accepted 20 May 2020, Available online 26 May 2020, Version of Record 2 June 2020.",https://doi.org/10.1016/j.jmacro.2020.103215,Cited by (3),This paper examines optimal ==== persistence. It focuses on the welfare differential between timeless perspective (TP) and a purely ,"The objective of this paper is to examine optimal monetary policy in a new Keynesian (NK) model with sectoral inflation persistence. In particular, this paper focuses on the welfare differential between timeless perspective (TP) and a purely discretionary monetary policy.==== The NK model has been used as a standard framework for the analysis of monetary policy. While the standard NK model is characterized by a purely forward-looking component, such a model structure has often been criticized because it might generate imprecise predictions of the new Keynesian Phillips curve (NKPC). The hybrid NKPC that incorporates lagged inflation is considered one solution to this concern. In addition, several studies have examined the role of lagged inflation in the NKPC in terms of determining optimal monetary policy. On the other hand, while some papers argue the importance of sectoral heterogeneities in inflation persistence, it seems that there are few papers that investigate the specific effect of sectoral inflation persistence on optimal monetary policy.====This paper investigates how the presence of sector inflation persistence affects the gain from a TP policy, as suggested by Woodford (2003), over a discretionary regime. In a purely forward-looking NK model, the issue of whether a TP policy outperforms a purely discretionary policy has been debated in previous studies.==== Steinsson (2003) showed that putting a larger weight on lagged inflation in the NKPC causes greater welfare losses in a one-sector NK model, and that the gain from a commitment policy almost vanishes when backward-looking inflation dominates inflation dynamics. We might simply ask the following question: How should the central bank conduct its monetary policy in the context of significant sectoral inflation persistence? Despite the importance of this question, to the best of our knowledge, it still remains unclear how sectoral inflation persistence affects welfare gains from a TP policy.====To provide the theoretical foundation of sectoral inflation persistence in a two-sector NK model, the present study incorporates the generalized rule-of-thumb hypothesis assumed by Steinsson (2003) into a two-sector NK model developed by Woodford, 2003, Woodford, 2010 who uses the constant elasticity of substitution (CES) aggregate consumption index.==== Under the rule-of-thumb hypothesis, firms use information from past aggregate price levels and inflation to set (or re-set) current prices. According to Steinsson (2003), a generalized rule-of-thumb model contains the past price level, past inflation rate and a lagged output gap.==== This enables the derivation of a generalized NKPC in a two-sector economy characterized by inflation persistence. Furthermore, this study derives the central bank’s loss function that corresponds to the second-order approximation of the household utility function. In this model, inflation persistence in each sector significantly changes the shape of the central bank’s objective function. Indeed, we demonstrate that the shape of the loss function is more complicated than that in a one-sector model. Consequently, the presence of sectoral inflation persistence makes the central bank’s optimization problem more complicated.====The main results of this paper are summarized as follows. First, a TP policy creates non-negligible welfare gains as long as inflation in either sector is not extremely backward-looking. The key mechanism that determines whether the central bank can obtain the welfare gains from a TP policy depends on the power with which it manipulates inflation expectations in each sector. Second, the specification of rule-of-thumb price setting crucially influences the gain derived from a TP policy. In particular, when moderate inflation persistence exists in either sector, the presence of the lagged output gap using the rule-of-thumb rule can create larger gains from a TP policy. Third, we show that higher substitutability of goods produces larger welfare gains than the complementarity of goods. Fourth, this paper demonstrates that an increase in inflation persistence in either sector makes the gains from a TP policy larger—as long as price stickiness is not insignificant in both sectors.====This study is organized as follows. Section 2 briefly discuss the related literature. Section 3 briefly describes an NK model with sectoral inflation persistence. Section 4 presents the derivation of the central bank’s loss function in this model, and explains the properties of optimal monetary policy in both TP policy and purely discretionary regimes. Section 5 calibrates the deep parameters used in this model. Section 6 provides the gain from a TP policy in an NK model with sectoral inflation persistence. Section 7 concludes and provides some possible future extensions for this methodology.",Sectoral inflation persistence and optimal monetary policy,https://www.sciencedirect.com/science/article/pii/S0164070420301415,26 May 2020,2020,Research Article,190.0
"Magkonis Georgios,Zekente Kalliopi-Maria","Economics and Finance Subject Group, Portsmouth Business School, University of Portsmouth United Kingdom,Alpha Bank, Economic Research Division, 11 Sophocleous St., Athens, 10559, Greece","Received 23 September 2019, Revised 29 April 2020, Accepted 20 May 2020, Available online 26 May 2020, Version of Record 30 May 2020.",https://doi.org/10.1016/j.jmacro.2020.103217,Cited by (0),"This study analyses the determinants of the sacrifice ratio; i.e., the output cost of disinflation. The empirical literature so far has used several model specifications, indicating the degree of model uncertainty. Even for those factors where consensus on their significance has been reached, such as ","During the 1980s, price stability became the primary concern for monetary policy in developed countries. The adoption of such a policy was a key element of what is now known as Great Moderation (Bernanke, 2004). At the same time, a new wave of academic research emerged, trying to explain on theoretical and empirical grounds the benefits of such a policy. One key aspect of the empirical literature was the short-run output losses that one economy may experience in the effort to reduce inflation. These effects were quantified by the ratio of cumulative output losses from a reduction of 1% of inflation. The magnitude of the sacrifice ratio, as well as, its determinants were an important piece of information for central banks. After the global financial crisis of 2008 and the recession that followed, many economies faced disinflation periods with associated output losses, while central banks of many developed economies significantly lowered their policy rates. New research appeared emphasizing the conditions of zero lower bound (Gust et al., 2017). Measuring the sacrifice ratio in such an environment became once again an important indication of the welfare effects of a recessionary period (Ascari and Ropele, 2012).====Across the literature, three different approaches are typically followed to estimate the sacrifice ratio. The first approach is based on the estimation of a Phillips curve. The slope of the Phillips curve measures the response of output to changes in inflation and therefore, this estimate is used as a proxy of the inflation-output trade-off (Gordon, King, Modigliani, 1982, Gordon, 2013). The second method is the usage of structural VAR model (Cecchetti, Rich, 2001, Huh, Jang, 2007). In this setting, the sacrifice ratio is defined as the ratio of the cumulative response of output due to a monetary shock over the cumulative response of the inflation rate. The last method is based on the identification of actual disinflation episodes and the calculation of the associated losses in real output as a result. The exact calculation procedure is discussed in Ball (1994). Due to its analytic simplicity, this method proves to be the most popular across the empirical literature.====The contribution of the present study is summarized as follows. Firstly, we estimate the sacrifice ratios for a panel of 42 countries. For each examined economy, we collect data for 13 variables that have been identified in the prior literature as potential drivers of the sacrifice ratio. These variables capture a range of macroeconomic, monetary and institutional features. Secondly, we identify the most significant drivers through a method that takes into account model uncertainty. Our results prove to be quite robust in a number of alternative assumptions and specifications. The remaining paper is structured as follows. Section 2 provides an overview of prior studies, while Section 3 describes the data collection process. The methodology is discussed in Section 4. Section 5 presents the main set of results, while Section 6 includes a series of robustness checks. Section 7 concludes.","Inflation-output trade-off: Old measures, new determinants?",https://www.sciencedirect.com/science/article/pii/S0164070420301439,26 May 2020,2020,Research Article,191.0
Kanazawa Nobuyuki,"Department of Economics, Soka University, 1-236, Tangi-machi, Hachioji-shi, Tokyo, 192-8577, Japan","Received 24 December 2019, Revised 17 March 2020, Accepted 30 April 2020, Available online 23 May 2020, Version of Record 23 May 2020.",https://doi.org/10.1016/j.jmacro.2020.103210,Cited by (5),I propose a flexible Radial Basis Functions (RBFs) Artificial Neural Networks method for studying the time series properties of ,"In this paper, I use the Radial Basis Function (RBF), which is a class of Artificial Neural Network (ANN), as a nonlinear Vector Autoregression (VAR) estimator and examine its applicability to macroeconomic time series analysis. The RBF has been studied extensively in fields of computer science and neural networks, and it has been shown that the RBF can approximate any continuous functions on a compact domain (known as the universal approximation property).==== The use of RBF in the macroeconomic time series analysis is motivated by its ability to flexibly estimate a nonlinear data-generating process without prior modeling of the type of nonlinearity. Despite its flexibility, however, the RBF can be efficiently estimated by following a simple two-step procedure commonly adopted in the neural network community that essentially breaks down a costly nonlinear estimation into a fast clustering method and a linear estimation.==== The benefit of low computational costs allows researchers to estimate nonlinear VAR models having a typical amount of linear VAR dimensions. In this paper, I introduce the RBF application to macroeconometrics and show that the RBF time series model estimated via two-step estimation can be a valid alternative to other nonlinear estimators for macroeconomic time series analysis.====To validate the use of the RBF estimator for macroeconomic analysis, we must first test to see if the estimator can correctly capture the structure of the aggregate economy with a number of observations that is usually available to a macro-econometrician. This is because, even though the RBF estimator has the universal approximation property, its rate of convergence generally depends on the smoothness of the function to be approximated and the number of available observations. To this end, I conduct a Monte Carlo experiment using simulated time series data generated from a medium-scale nonlinear New Keynesian (NK) model, which economists and policy makers frequently view as a representation of the aggregate economy. Nonlinearity in this NK model comes from a kink in the central bank’s monetary policy rule, wherein the interest rate is bounded from below at 0%. I then calculate the within-sample Mean-Squared-Errors (MSEs), which are defined by the distance between the true impulse responses from the NK model and the impulse responses estimated by the RBF estimator. For comparison, I also compute the MSEs using linear- and threshold-VAR (TAR) estimators. I find that the RBF estimator produces smaller MSEs than linear VARs and TARs, especially in middle and long horizons, even when the sample size is limited to 300 periods. The result suggests that the use of the RBF estimator for macroeconomic time series analysis may be appropriate.====I finally use the RBF estimator to understand the dynamic relationship between seven US macroeconomic variables from 1978 to 2016. Using the utilization-adjusted total factor productivity (TFP) of Fernald (2014) as a series of exogenous shocks, I estimate the impulse responses of output, consumption, investment, hours worked, expected inflation rate, and nominal interest rate. Through this exercise, I test the prediction of the textbook NK model, which states that the expansionary effects of a positive supply shock are weaker under passive monetary policy regimes, such as the zero-lower-bound (zlb) periods. Consistent with the NK model’s prediction, I find that the effects of the positive TFP shock are considerably weaker during periods of zlb.====Furthermore, the output responses to the supply shocks are found to be similarly small between 2003 and 2004. During these periods, the federal funds rate was very low at around 1%. In fact, the estimated nominal interest rate response suggests that the Fed was not actively responding to supply shocks during these periods. Owning to the unresponsive nominal interest rate and the supply shocks’ deflationary effects, I find that the real interest rate rises after the positive supply shocks during these periods, which discourages consumption, investment, and output. The finding highlights the critical role that the response of the real interest rate plays in shaping the responses of other macroeconomic variables. The study also suggests that the estimated responses of macroeconomic variables of 2003—2004 are remarkably similar to those during the zlb periods of 2008—2015.====My results contrast with those of Garín et al. (2019) and Wieland (2019), who found the strong expansionary effects of positive supply shocks during the periods of zlb. I briefly discuss potential explanations for the differences in the findings, and I show that these differences are a consequence of different data specifications rather than different methodologies. Specifically, I estimate the output response using the state-dependent local projection (LP) method and show that the effects of the supply shocks are considerably weaker during the zlb periods when the growth rate specifications are used and an outlier (2008Q4) is excluded. This exercise reiterates the point that macroeconomic dynamics during passive monetary policy regimes differ markedly from the macroeconomic dynamics typical of active monetary policy regimes, as prescribed by the basic NK model.====The rest of the paper is organized as follows: Section 2 reviews the literature on the RBF. Section 3 goes over the literature about time series analysis. Section 4 describes the Monte Carlo simulation exercise. Section 5 illustrates an empirical application using the US data. Section 6 discusses the results, and Section 7 concludes.",Radial basis functions neural networks for nonlinear time series analysis and time-varying effects of supply shocks,https://www.sciencedirect.com/science/article/pii/S0164070420301361,23 May 2020,2020,Research Article,192.0
"Hashimoto Ken-ichi,Im Ryonghun,Kunieda Takuma","Graduate School of Economics, Kobe University, Rokko-dai 2-1, Kobe 657-8501, Japan,Institute of Economic Research, Kyoto University, Yoshida-honmachi, Sakyo-ku, Kyoto 606-8501, Japan,School of Economics, Kwansei Gakuin University, 1-155 Uegahara Ichiban-cho, Nishinomiya 662-8501, Hyogo, Japan","Received 9 January 2020, Revised 9 April 2020, Accepted 20 May 2020, Available online 23 May 2020, Version of Record 23 June 2020.",https://doi.org/10.1016/j.jmacro.2020.103212,Cited by (4),"A tractable growth model with asset bubbles is presented to demonstrate that a financial crisis caused by a bubble bursting increases unemployment rates. A bubbly asset, which is intrinsically useless, has a positive market value because purchasing the asset is a sole saving method for agents who draw insufficiently low productivity, whereas selling the asset is a fund-raising method for agents who draw high productivity to initiate an investment project. The presence of asset bubbles corrects allocative inefficiency regarding production resources, relocating investment resources from low-productivity agents to high-productivity agents. Accordingly, the presence of asset bubbles can promote capital accumulation. As capital accumulates and output increases, the number of vacant positions increases because firms acquire more funds to cover a search cost. As a result, firms are incentivized to increase employment. However, extrinsic uncertainty may burst asset bubbles and cause a self-fulfilling financial crisis, which is followed by increased unemployment.","An asset bubble is defined as a deviation of the asset’s market value from its fundamental value. Economic history has repeatedly witnessed severe financial crises accompanied by the collapse of asset prices in modern monetary and financial systems. Before a financial crisis, asset prices are often greater than their fundamental values and possibly lead to higher output, thus stimulating employment. When asset prices collapse, however, output suddenly declines, the economy goes into a depression, and unemployment increases.==== Therefore, a bubble bursting arguably causes a higher unemployment rate. Figure 1 indicates that as the Japanese asset price bubble collapsed in 1991, the unemployment rate in Japan increased from 2.1% in 1991 to 4.8% in 2000. Additionally, when the US subprime loan crisis occurred in 2009, the unemployment rate in the US increased from 4.7% in 2007 to 9.7% in 2010.==== Despite the historical observations related to the collapse of asset prices and depressions, the impact of a bubble bursting on unemployment has not been sufficiently investigated in macroeconomics, although a few studies have addressed it (e.g., Kocherlakota (2011); Miao et al. (2016); Hashimoto and Im (2016), Hashimoto and Im (2019)). In this paper, we present a tractable overlapping-generations model with asset bubbles to demonstrate that a financial crisis triggered by a bubble bursting depresses an economy and increases unemployment.====In our model, a bubbly asset has a positive market value because selling the asset is a fund-raising method for those who draw sufficiently high productivity to initiate an investment project, and purchasing the asset is a sole saving method for those who draw insufficient productivity to initiate a project. Our model is closely related to that of Martin and Ventura (2012), who develop a tool to investigate how the occurrence of asset bubbles promotes capital accumulation and the bursting of bubbles causes depressions. As in the model by Martin and Ventura, the young generation can issue new bubbly assets to raise funds. Once they sell the new bubbly assets in the asset market, they do not have to purchase them back from the market. Accordingly, the young generation always has incentives to issue new bubbly assets and to obtain more funds that cannot otherwise be acquired because of borrowing constraints.====Although the central role of asset bubbles in our model is similar to that of Martin and Ventura (2012), our model departs from theirs in some respects.==== First, we introduce the labor market frictions. The investigation of the relationship between a bubble bursting and unemployment is a main theme in this paper. By introducing the labor market matching frictions in our model, we can demonstrate that a bubble bursting increases unemployment under mild parameter conditions.==== Second, we employ a continuous distribution with respect to idiosyncratic productivity shocks, whereas Martin and Ventura apply a binary distribution. The use of a continuous productivity distribution significantly simplifies the analysis. In particular, one can derive the productivity cutoff that divides agents into bubbly-asset holders and investors. Those who draw productivity shocks smaller than the cutoff purchase bubbly assets, and those who draw productivity shocks greater than the cutoff become investors. Our model obtains a simple two-dimensional dynamical system with respect to capital and the cutoff, through which one can easily investigate the dynamic behavior of the system.====The presence of asset bubbles corrects allocative inefficiency by relocating investment resources from low-productivity agents to high-productivity agents, and it promotes capital accumulation if the bubbles’ crowding-out effect á la Tirole (1985) is relatively weak. Whether the presence of asset bubbles promotes capital accumulation is contingent on the labor market conditions because agents’ investment activities are affected by their labor market status (i.e., employed or unemployed) through labor market imperfections. In our framework, agents and firms face search matching frictions. If a match with a firm is successful, the agent can work and earn the wage. After their employment status is determined, she issues new bubbly assets to raise additional funds, and then, draw productivity shocks. She becomes an investor or a bubbly asset holder depending upon her productivity. From a microeconomic perspective, her employment status affects the amount of her investment. Therefore, when the unemployment rate is low, the effect through which the presence of asset bubbles corrects allocative inefficiency is strong, and thus, capital accumulation is promoted. In turn, if the presence of asset bubbles promotes capital accumulation, output increases and the number of vacant positions increases because each firm can acquire more funds to cover a fixed search cost. As a result, the unemployment rate decreases.====The literature on asset bubbles and economic growth has recently experienced a resurgence.==== In this stream of the literature, financial market imperfections and the productivity differences across agents are key factors in producing a situation in which asset bubbles á la Tirole (1985) enhance capital accumulation.==== In contrast to the existing literature, whether capital accumulation is enhanced by the presence of asset bubbles is subject to not only financial frictions but also labor market frictions. Furthermore, we investigate how unemployment rates are affected by the presence and bursting of asset bubbles in our model.====There are related studies to ours. Miao et al. (2016) investigate the relationship between unemployment and stock market bubbles in an economy with labor and financial market frictions. In their model, as a firm’s fundamental value increases depending on extrinsic uncertainty, the collateral constraint is relaxed. As a result, the firm increases its production and employment. In contrast, in our model, which is based on á la Tirole (1985), the new issuance of intrinsically useless assets by investors to raise their funds corrects the allocative inefficiency regarding production resources. As a result, capital accumulation is promoted, and firms are incentivized to increase employment. Although Kocherlakota (2011) investigates the impact of the occurrence of asset bubbles on unemployment, he does not consider capital accumulation. Therefore, the mechanism in which the presence of asset bubbles enhances capital accumulation and capital accumulation increases employment cannot be investigated in Kocherlakota’s model. Hashimoto, Im, 2016, Hashimoto, Im, 2019 also study the relationship between bubbles and unemployment. In contrast to our paper, however, they do not consider financial frictions in obtaining the bubbly steady state. Without financial frictions, the employment-enhancing effect of asset bubbles through capital accumulation cannot be derived. However, the employment-enhancing effect of asset bubbles is the main issue to be addressed in the current paper relative to historical observations.====The remainder of this paper is organized as follows. Section 2 develops the model, and section 3 investigates the dynamic behavior in equilibrium and derives the relationship between the unemployment rate and capital accumulation. In section 4, the growth-promoting effect of asset bubbles is analyzed, and policy implications focusing on the unemployment benefit and the search cost subsidy are discussed. Section 5 derives a self-fulfilling financial crisis as a rational expectations equilibrium. Section 6 concludes the paper. The proofs of propositions and lemmata are collected in the appendix.","Asset Bubbles, Unemployment, and a Financial Crisis",https://www.sciencedirect.com/science/article/pii/S0164070420301385,23 May 2020,2020,Research Article,193.0
Cole Stephen J.,"Department of Economics, Marquette University, P.O. Box 1881, Milwaukee, WI 53201, United States","Received 22 November 2019, Revised 28 April 2020, Accepted 20 May 2020, Available online 23 May 2020, Version of Record 30 May 2020.",https://doi.org/10.1016/j.jmacro.2020.103213,Cited by (1),"This paper examines how the effectiveness of central bank forward guidance depends on two key channels: the expectations formation process and the ==== regime. The results show that rational expectations relative to an adaptive learning rule amplifies the positive benefits a price-level targeting central bank creates for forward guidance. Specifically, forward guidance generates greater amounts of output and ==== under a price-level than ","The unconventional monetary policy of forward guidance–communication to the public about the future course of policy–has constituted the toolkit of central banks around the world since the aftermath of the Great Recession. For instance, the U.S. Federal Open Market Committee (FOMC) conveyed guidance on the future course of the interest rate in their December 2008 statement to the public when short-term interest rates hit the zero lower bound (ZLB). The Federal Reserve also implemented forward guidance by communicating information about the path of its long-run inflation target. In 2012, the Federal Reserve stated that “The Committee judges that inflation at the rate of 2 percent ... is most consistent over the longer run with the Federal Reserve’s statutory mandate” (Federal Reserve, 2012). In addition, communication about the future course of policy will likely be a part of the Federal Reserve’s monetary toolkit in the future. Forward guidance could be used again when ZLB episodes constrain conventional monetary policy. During periods away from the ZLB, forward guidance can be employed to provide greater clarification and transparency to the public (see Williams et al., 2013).====The efficacy of forward guidance hinges on two key assumptions–the monetary policy regime and expectations formation process. As of this writing, the Federal Reserve implements an inflation targeting regime. However, the U.S. inflation rate has generally been below its Federal Reserve target of 2% over the period May 2012 through May 2019.==== In response, an alternative framework that has been debated is a price-level targeting monetary policy (see Bullard, 2018 and Clarida, 2019). Since inflation and price-level central bank policies target different variables, the type of monetary regime can influence the efficacy of forward guidance statements about future policy.==== Moreover, the effect a monetary policy regime has on forward guidance is influenced by the expectations of agents. Since forward guidance concerns statements about the future course of policy, the way in which expectations of private sector agents are formed affects the potency of forward guidance. Full-information rational expectations (FIRE) is the standard way to model agents’ expectations in macroeconomic models. While a reasonable starting point for analysis, FIRE may not be best suited for modeling the economy’s expectations as Bernanke (2007), (para. 6) states “The traditional rational-expectations model of inflation and inflation expectations... is less helpful for thinking about economies in which (1) the structure of the economy is constantly evolving in ways that are imperfectly understood by both the public and policymakers.” Therefore, it is natural to investigate the effectiveness of forward guidance under different central bank policies when FIRE is no longer assumed.====This paper examines how the effects of central bank forward guidance depend on the monetary policy regime when an adaptive learning rule replaces the standard FIRE hypothesis. A benchmark New Keynesian model with a Taylor (1993) rule and time-varying inflation target is augmented with anticipated or forward guidance shocks on the inflation target. These shocks can be thought of as communication about the Federal Reserve’s long-run inflation target. The model is solved under both inflation and price-level targeting central banks. The results are then examined under FIRE and adaptive learning expectations formation processes.====The two types of monetary policy regimes studied in this paper are inflation and price-level targeting. Under both types of central banks, the monetary authority is assumed to adjust its short-term policy rate following a Taylor (1993) rule that depends on lagged interest rates, output gap, and an unanticipated monetary policy shock. However, an inflation targeting central bank includes deviations of the inflation rate from its target in its decision to adjust interest rates. Alternatively, a price-level targeting monetary authority responds to deviations of the price level from its target. This latter central bank regime can be beneficial as it is inherently history dependent.==== Unlike an inflation targeting regime that lets “bygones be bygones,” a price-level targeting monetary authority reacts to past inflation shocks. If there exists a decrease in the price level and inflation in the past, a price-level targeting central bank will use its policy tool to elevate the price level back to its target causing a period of above average inflation. Any previous decreases in inflation below target are met with subsequent increases in inflation above target so that the growth in the price level achieves its target on average (e.g. 2%). This feature can bring more positive benefits to the economy (see Giannoni (2014)).====The standard New Keynesian model examined in this paper shows that output and inflation depend on expectations of future macroeconomic variables. The conventional method to evaluate expectations is via FIRE. When constructing forecasts about future macroeconomic variables, FIRE assumes agents know the true structure of the economy, the exact relationship between variables (i.e. value of the deep parameters), beliefs of other agents, and distribution of the structural shocks. In contrast, the adaptive learning approach to evaluating expectations does not make as strong assumptions. This approach starts with viewing agents as real-life econometricians (see Evans and Honkapohja, 2013). In particular, when constructing forecasts about future economic variables (e.g. GDP or inflation), he or she might rely on a linear econometric model of the economy. The econometrician would estimate the parameters of this model using Ordinary Least Squares (OLS), and produce forecasts using this ==== version of the economy. He or she would reestimate the model’s parameters and update forecasts when new data about the economy are released. Similarly, adaptive learning agents in this paper proceed in an analogous fashion. They build a linear model based on a VAR(1) in three variables (output gap, inflation, and interest rates) as well as central bank announcements of the inflation target and forward guidance included as additional regressors. As new data arrive, adaptive learning agents estimate the coefficients of their model using OLS and update their beliefs each period. Thus, in the preceding description, it is apparent that both real-life econometricians and adaptive learning agents engage in a similar process of “learning” about the economy.====The results show that rational expectations relative to adaptive learning amplifies the positive benefits a price-level targeting regime creates for the effectiveness of forward guidance. This outcome is shown via the monetary authority communicating forward guidance about its long-run constant inflation target. The benchmark rational expectations results display that the effects of forward guidance on the economy are boosted under a price-level targeting central bank relative to an inflation targeting central bank. However, FIRE overstates these benefits compared to adaptive learning. When an adaptive learning rule replaces the rational expectations assumption, the reactions of output and inflation to forward guidance under a price-level targeting rule are dampened versus rational expectations. In addition, the different responses of expectations to forward guidance are driving the results. Forecasts of output and inflation react more favorably to forward guidance under rational expectations than under adaptive learning. The reason is that the former type of agents base expectations on the true model of the economy. They know precisely how future statements about the inflation target will affect future variables, and consequently, influence the values of current output and inflation. In contrast, the reaction of expectations to forward guidance under adaptive learning agents is dampened. The reason is that these agents have to estimate the effects of forward guidance statements on the economy, which leads to a performance gap between the two expectations formation processes.====The paper then examines two approaches that influence the performance gap between the two expectations formation processes. If the linear forecasting model of adaptive learning agents separates more from FIRE, the response of macroeconomic variables under adaptive learning is more diminished relative to the baseline scenario. The value of the central bank’s long-run inflation target also influences the performance gap. Higher (lower) values of the inflation target lead to increased (decreased) differences between the responses of rational expectations and adaptive learning to forward guidance announcements under a price-level targeting regime.====Additional extensions are also performed to examine the main results of the paper. If an inflation targeting central bank places more weight on deviations of inflation from its target, the performance gap between rational expectations and adaptive learning agents still exists. However, the better outcomes of a price-level targeting regime relative to inflation targeting regime dampen. The duration of the forward guidance horizon also does not overall influence the baseline results. The length of the forward guidance horizon affects the amount of time rational expectations amplifies the positive effects a price-level targeting regime creates for forward guidance relative to adaptive learning. In addition, the amount of weight in which adaptive learning agents place on new information (i.e. constant gain parameter) is varied. If agents place more weight on recent forecasts errors, output and inflation are overall farther away from their rational expectations counterparts than under the baseline adaptive learning scenario. However, a lower value of the constant gain parameter implies agents’ coefficient estimates do not vary as much from the previous period. Thus, their reactions to forward guidance are closer to their rational expectations counterparts. Moreover, the results of the paper are robust to alternative measures of the forward guidance shocks.====Overall, the results suggest essential takeaways for policymakers. A price-level targeting central bank can increase the effectiveness of forward guidance on the economy relative to inflation targeting. However, FIRE overstates these effects relative to a more realistic adaptive learning rule. Thus, policymakers should consider how expectations are modeled if forward guidance and price-level targeting are implemented in an economy.",The influence of learning and price-level targeting on central bank forward guidance,https://www.sciencedirect.com/science/article/pii/S0164070420301397,23 May 2020,2020,Research Article,194.0
"Servén Luis,Abate Girum Dagnachew","CEMFI, 28014 Madrid, Spain,The World Bank Group, Washington, DC, 20433 USA","Received 14 April 2019, Revised 21 February 2020, Accepted 30 April 2020, Available online 16 May 2020, Version of Record 23 June 2020.",https://doi.org/10.1016/j.jmacro.2020.103211,Cited by (5),"Growth fluctuations exhibit substantial synchronization across countries, which has been viewed as reflecting a global business cycle driven by shocks with worldwide reach, or ==== resulting from local real and/or financial linkages between countries. This paper brings these two perspectives together by analyzing international growth fluctuations in a setting that allows for both global shocks and spatial dependence. Using annual data for 117 countries over 1970–2016, the paper finds that the cross-country dependence of aggregate growth is the combined result of global shocks summarized by a latent common factor and ==== accruing through the growth of nearby countries – with proximity measured by bilateral trade linkages or geographic distance. The latent global factor shows a strong positive correlation with worldwide TFP growth. Countries’ exposure to global shocks is positively related to their openness to trade and the degree of commodity specialization of their economies, and negatively to their financial depth. Despite its simplicity, the empirical model fits the data well. Ignoring the cross-country dependence of growth, by omitting ==== or common shocks (or both) from the analysis, leads to a marked deterioration of the empirical model’s in-sample explanatory power and out-of-sample forecasting performance.","The international synchronization of business cycles has long attracted academic and policy interest. From the academic viewpoint, understanding the factors behind the cross-country comovement of output can help shed light on the empirical validity of different classes of theoretical models. From the policy perspective, quantifying the degree of business cycle commonality is a primary consideration from the point of view of optimal currency areas and, more broadly, to assess the merits of independent stabilization policies.====An extensive empirical literature views the international comovement of growth as the reflection of a global business cycle driven by shocks affecting a multitude of countries. Following the contribution of Kose et al. (2003), a number of studies model the cycle as the combined effect of a set of global and region (and, in some cases, sector-specific) latent common factors; see e.g., Kose et al. (2012), Crucini et al. (2011), Mumtaz et al. (2011), and Karadimitropoulou and Leon-Ledesma (2013). This literature finds that the international business cycle can account for a major portion of cyclical GDP fluctuations – as much as 40 percent of their variance in the case of G7 countries, according to the results of Kose et al. (2003).====Another strand of empirical literature stresses growth interdependence arising from economic linkages between countries or regions. This is the approach taken by the extensive Global VAR (GVAR) literature pioneered by Pesaran et al. (2004), and recently surveyed by Chudik and Pesaran (2016), which underscores the real and financial dependence across countries that arises from their bilateral goods and assets trade. The same basic mechanisms feature in several papers taking a spatial perspective on growth empirics. Thus, Ho et al. (2013) find evidence of growth spillovers due to bilateral trade linkages between OECD countries. In the context of a Solow model, they conclude that the estimated rate of convergence is significantly higher once those spatial links are taken into account. Likewise, Wang et al. (2015) find that the comovement of growth across countries is well explained by the geographic distance between them.====These two empirical literatures share a common concern, namely the dependence of economic growth across countries and regions. But methodologically they take very different views. The first literature stresses shocks with global reach, affecting all countries or regions under consideration. The second literature puts the emphasis on the linkages generating dependence between particular countries or regions. The two views roughly correspond to the distinction between strong and weak cross-sectional dependence, respectively. Strong dependence arises from pervasive common shocks. In turn, weak dependence between specific countries or regions primarily reflects their economic and/or geographic proximity.==== Strong dependence is typically analyzed with factor models (as done, for example, by Kose et al. (2003)), while weak dependence is typically analyzed with spatial models highlighting geographic or economic distance (as in, e.g., Ho et al. (2013)).====So far, the empirical literature on growth interdependence and international business cycles has taken into account one or the other form of dependence – but not both. However, identifying correctly the type of cross-sectional dependence at work can be quite important for estimation of and inference on empirical growth models. While details may depend on the specific model under consideration, ignoring strong dependence in the estimation when it is present will generally lead to inconsistent estimates if the omitted common shocks are correlated with the model’s regressors (Pesaran and Tosetti (2011)). Conversely, introducing common factors in the estimation when only weak dependence is at play may similarly yield inconsistent estimates (Onatski (2012)). In turn, the consequences of neglecting spatial dependence when it is present hinge on its precise form. If spatial dependence accrues through the model’s error term, ignoring it will only cause loss of efficiency. However, ignoring spatial dependence in the dependent variable and/or the independent variables may yield biased and inconsistent estimates of the parameters of the remaining variables (LeSage and Pace (2009)).====In reality, however, the two forms of dependence are likely to be simultaneously present. Indeed, growth in a given country is likely to be affected by both global shocks and shocks to economically nearby countries – with closeness defined by bilateral trade intensity, financial linkages, and so on. The main contribution of this paper is to bring both perspectives together. We analyze the international comovement of GDP growth in a sample comprising over a hundred advanced and emerging countries, using an encompassing empirical framework including both spatial effects and common factors. This allows us to assess quantitatively the respective roles of strong and weak cross-sectional dependence in the observed patterns of GDP growth across the world, and to illustrate the consequences of ignoring either (or both) of them. To our knowledge, only Bailey et al. (2016), who examine the patterns of house prices across U.S. metropolitan areas, and Vega and Elhorst (2016), who study regional unemployment trends across the Netherlands, have employed a similarly encompassing approach.====We assume that spatial interactions between countries occur through growth itself. This seems a natural way to model the linkages between national economies, and is the same approach followed by Ho et al. (2013), as well as the GVAR literature on global business cycle dynamics. However, it also implies that the two-step estimation methods employed by Bailey et al. (2016), who assume that the interaction occurs through the spatial error, are not applicable. Instead, we use the quasi-maximum likelihood (QML) estimator recently introduced by Shi and Lee (2017), which permits joint consideration of common factors and spatial dependence in a dynamic framework. Because the factors and their loadings are treated as parameters, and their number grows with sample size, they pose an incidental parameter problem that introduces asymptotic bias in the QML estimator. The bias correction developed by Shi and Lee (2017) addresses this issue.====In light of the earlier literature, we experiment with two alternative specifications of the spatial weight matrix that summarizes interactions between countries. We use both a bilateral trade weight matrix, as done by Ho et al. (2013), and a bilateral geographical distance weight matrix, as done by Wang et al. (2015).====Our country sample contains both advanced and developing economies. The former are likely to be more deeply integrated than the latter in the international economy, and thus more exposed to the international business cycle. Hence we also estimate the empirical growth model on a subsample of 21 advanced countries. This allows us to assess differences between these countries and the rest regarding the extent and nature of cross-sectional dependence.====Estimation results using the two alternative specifications of the spatial weight matrix show that growth exhibits significant inertia, somewhat higher in the advanced country subsample than in the full sample. There is strong evidence of spatial effects, summarized by a significantly positive contemporaneous spatial lag, and a negative spatial-time lag, significant in the advanced-country sample. The implication is that local interactions are important to understand the international comovement of growth. However, the estimated spatial effects are substantially larger when modeling spatial dependence in terms of bilateral trade. Importantly, growth around the world also reflects a latent common factor, which we interpret as capturing the global business cycle. The factor shows a robust positive correlation with a measure of worldwide total factor productivity – as found by Crucini et al. (2011) for G-7 countries – and with global commodity prices.====The model does a good job at accounting for the pattern of growth across the world and in particular its cross-country dependence. We find that the global cycle – as summarized by the common factor – and spatial interactions account for a substantial portion of the variance of GDP growth – around a quarter in the full sample, and over half in the advanced-country subsample.====Our results also speak to the determinants of countries’ exposure to global shocks, an issue at the core of the policy debate. We find that the impact of the common factor on GDP growth is significantly bigger in countries with more open trade accounts, and those whose production structure is more tilted towards commodities. It is also significantly weaker in countries featuring larger financial depth.====Finally, the paper underscores the importance of properly addressing cross-sectional dependence, both strong and weak, in cross-country growth empirics. Ignoring it, by omitting both common factors and spatial effects, leads to a gross overstatement of the persistence of growth. It also weakens the estimated model’s in-sample fit, as well as its out-of-sample forecasting ability. Including either the common factor or the spatial effects helps mitigate these problems, but does not fully solve them. Including both the factor and the spatial effects yields the best model performance, in terms of both in-sample fit and out-of-sample forecasting.====The rest of the paper is organized as follows. Section 2 introduces the factor-augmented dynamic spatial model of output growth employed in the paper. Section 3 presents the data. Section 4 reports the results, and Section 5 provides some conclusions.",Adding space to the international business cycle,https://www.sciencedirect.com/science/article/pii/S0164070420301373,16 May 2020,2020,Research Article,195.0
"Yu Po-yang,Lai Ching-chong","Department of Economics, National Cheng Chi University, Taiwan,Institute of Economics, Academia Sinica, Taiwan,Institute of Economics, National Sun Yat-Sen University, Taiwan","Received 25 September 2019, Revised 10 March 2020, Accepted 31 March 2020, Available online 7 May 2020, Version of Record 20 May 2020.",https://doi.org/10.1016/j.jmacro.2020.103205,Cited by (3),"This paper analyzes the growth and welfare effects of the privatization of public firms in a Schumpeterian growth model. Two alternative definitions of privatization are proposed in our model. The first is the ratio of mixed R&D firms’ equity shares owned by the household, which is dubbed vertical privatization. The second is the number of unmixed R&D firms, which is called horizontal privatization. We find that, under both definitions, privatization is beneficial to economic growth while the effect of privatization on social welfare is ambiguous. Accordingly, our analysis reveals that a partial privatization could be an optimal policy. Moreover, we also discuss how the extent of ==== protection is related to optimal privatization.","The collapse of authoritarianism in Central and Eastern Europe (CEE) and the fall of the former Soviet Union led to a new wave of privatization in the 1990s, with over 50% of state-owned enterprises (SOEs) being privatized between 1989 and 1995 in CEE and the Commonwealth of Independent States. However, after 2000, the main purposes of the governments of those countries in conducting privatization changed. Of these purposes, we encapsulate the following three categories. First, the governments sought to reduce their budget deficits by selling shares of SOEs. Second, the participation of private stakeholders in corporate governance was able to improve the production efficiency of the SOEs. Third, the infusion of capital from outside investors was able to enhance the financial flexibility of these SOEs.====A more recent OECD report (OECD, 2009) investigated the distribution of privatization by industry and by country in the OECD area from 2000 to 2007, and found that privatization commonly occurred in the telecom, transport and logistics and utilities sectors. It also showed that privatization was concentrated in European countries. In particular, that in France, Italy and Germany accounted for almost 50% of the total privatization revenue in the OECD area. Bai et al. (2009) and Privatization Barometer (2007) have explored the privatization process in China, other than within the OECD area. They observed that 20% of SOEs implemented privatization programs during 1999-2004 in China. The privatization revenue within China reached 60 billion in 2007, more than that of any country in Europe. Even more recently, at the third plenum of the Communist Party Congress of China held in 2013, the government announced that it would continue to reduce the proportion of SOE output in China's GDP. Equipped with the above observations, it is a common fact that privatization continues to be an important and complex issue for the governments of developed and developing countries.====It is generally accepted in the existing empirical literature on privatization, such as in Plane (1997), Barnett (2000), and Boubakri et al. (2009), that privatization leads to a favorable effect on economic growth in both developed and developing countries. This paper is motivated to provide a plausible theoretical explanation for the positive linkage between privatization and economic growth. To achieve this objective, this paper constructs an R&D-based growth model to analyze the extent to which privatization is related to economic growth.==== In addition to providing a positive analysis, this paper also conducts a normative analysis of the optimal privatization policy from the perspective of social welfare maximization.====Until now, to the best of our knowledge, no theoretical analysis has been devoted to dealing with how privatization affects economic growth. Existing studies unanimously focus their attention on the relationship between privatization and social welfare.==== Among them, De Fraja and Delbono (1989) find that the privatization of public firms is more likely to improve social welfare when the market is more competitive. Matsumura (1998) introduces the possibility of partial privatization, and shows that partial privatization could be an optimal policy. By bringing the free entry and exit of firms into the picture, Matsumura and Kanda (2005) analyze the effect of privatization on social welfare. They find that the optimal policy for public firms is not to be privatized.====Compared with previous studies on privatization, this paper has the following distinctive traits. First, existing studies unanimously employ a partial equilibrium framework to analyze the impact of privatization on social welfare. Our study instead uses a general equilibrium framework, the Schumpeterian growth model, to investigate how privatization affects economic growth and social welfare by way of mutual interactions among the labor market, the capital market, and commodity markets. Our analysis finds that partial privatization could be an optimal policy. Although this result is similar to that of Matsumura (1998), as illustrated in our later analysis, the reasoning behind the result is not the same. Moreover, our paper shows that, with free entry among private firms, both partial privatization and full privatization could be the optimal policy in the long run, which is more in line with the empirical evidence. For instance, Jones et al. (1999) indicated that 43.9% of public firms had partially engaged in share issue privatizations (SIPs) and 11.5% of them had been fully privatized based on data for 59 countries over the 1977-1997 period.====The second trait of this paper is that it provides an analytical framework to clarify the relationship between privatization and R&D investment. Since the 1950s, R&D investment has exhibited dramatic growth in developed countries.==== Empirical studies on privatization and R&D investment lead to inconclusive results regarding the linkage between privatization and R&D investment. For example, Gupta (2005) comprehensively studies the Indian partial privatization program and finds that partial privatization leads to a positive impact on firms’ sales, labor productivity and R&D investment. Domadenik et al. (2008) analyze firm-level data for Slovenia and show that firms owned by insiders invest more in R&D than firms owned by other types of owners after privatization. Conversely, Munari (2002) and Munari and Oriani (2002) show that, in Western Europe, the privatization of companies tends to reduce their R&D investment. Although these studies provide fruitful empirical evidence to examine the linkage between privatization and R&D investment, it is strange that the literature is largely silent in terms of building theoretical models to clearly explain through which channels privatization affects R&D investment. By constructing an R&D-based growth model, this paper makes an attempt to provide a theoretical analysis to explain the empirical observations.====The third trait is that our paper develops an R&D-based growth model that is able to analyze how privatization affects economic growth. As mentioned previously, no theoretical analysis has been put forward to deal with the linkage between privatization and economic growth. This paper thus makes an effort to shed light on the channels through which a country's privatization policy influences economic growth.====The fourth trait is that, with a view to matching the empirical findings, our paper develops an analytical framework to explain the positive relationship between privatization and firm productivity. Based on empirical data for 1983, Boardman and Vining (1989) analyze the 500 largest non-US firms and find that public firms are significantly less productive than private firms. Laurin and Bozec (2001) compare two large Canadian rail carriers, one public and one private, and find that the public firm has lower productivity. Following privatization, the productivity of the public firm converges to the level of the private firm. Brown et al. (2006) focus on public firms in Eastern European countries and show that productivity increases due to privatization, but its extent varies across industries. In their recent study, Hanousek et al. (2009) find a positive relationship between the privatization and performance (evaluated in terms of labor cost, profit, etc.) of firms in central Europe. Estrin et al. (2009) investigate the effect of privatization in transition countries. They find that privatization in the case of foreign owners could improve the firm's performance (evaluated in terms of total factor productivity, profitability, etc.), but that the effect of privatization is less obvious in the case of domestic owners. By contrast, Koman et al. (2015) emphasize that privatization leads to asset-stripping which is detrimental to the firm's performance. Equipped with these studies, this paper sets up a theoretical model that is able to describe the empirical findings.====The remainder of this paper is organized as follows. Section 2 develops a Schumpeterian growth model in which the government is allowed to hold equity shares as its assets. Section 3 derives the macroeconomic equilibrium, and then examines the relationship between privatization and economic growth. Section 4 explores how privatization affects the long-run social welfare and analyzes the optimal privatization policy. Finally, Section 5 concludes.",Optimal Privatization and Economic Growth in a Schumpeterian Economy,https://www.sciencedirect.com/science/article/pii/S0164070419303970,7 May 2020,2020,Research Article,196.0
"Sengul Gonul,Tasci Murat","Department of Economics, Ozyegin University, Turkey,Research Department, Federal Reserve Bank of Cleveland, United States","Received 21 February 2019, Revised 12 March 2020, Accepted 18 March 2020, Available online 1 May 2020, Version of Record 19 May 2020.",https://doi.org/10.1016/j.jmacro.2020.103202,Cited by (2),"We use a parsimonious unobserved components model with flow rates to estimate a time-varying unemployment rate trend for Turkey. Our approach is grounded in the modern theory of labor market search. This trend estimate yields a level that the unemployment rate would converge to in the absence of cyclical shocks that move different flow rates away from their underlying trends. By the end of 2018, the unemployment rate trend for Turkey stood at 12 percent, gradually increasing since the end of the global recession. The key drivers of the unemployment rate trend have been rising separation and unemployment exit rate trends in the first decade of our sample period. Since 2012, these trends have stagnated somewhat, but the labor force participation rate has continued trending up.","The long-run rate of unemployment (natural rate) has attracted a lot of attention since the Great Recession. In an environment where many developed countries and emerging market economies face exceptionally high levels of unemployment, policymakers and economists have focused on identifying the unemployment rate that is feasible in the long run, i.e, the “natural” rate, to gauge the extent of labor market slack (see, for instance, Daly et al., 2012 and Weidner and Williams, 2011). There are two major approaches to estimating the natural rate of unemployment.==== One approach exploits the co-movement between inflation (nominal variables) and unemployment in the context of a Phillips curve. The second approach uses labor market related indicators such as flow rates and estimates a secular trend. The former approach produces a useful concept for understanding the extent of inflationary pressures in an economy, whereas the latter one yields an estimate of a secular trend for the unemployment rate that would be consistent with the underlying labor market trends. This latter concept could be particularly useful if, for instance, we are interested in measuring the extent of labor market slack in the aggregate economy, i.e., the distance between the actual unemployment rate and a benchmark level that can be attained with “normal” churn in the labor market.====This paper follows this latter approach and provides a secular trend estimate for the unemployment rate in Turkey. Our main contribution is providing a framework that can be especially useful for developing countries and emerging markets. Since these countries go through major structural changes and transitions more often, an approach based on the Phillips curve may not prove to be very informative about the dynamics of the labor market, ====.==== We think that Turkey provides a unique laboratory in this respect. First, the country has gone through significant changes in its monetary policy environment, followed by sharply declining inflation in the early 2000s.==== In addition to major structural changes in central bank objectives, labor force participation increased dramatically in a short period of time. The participation rate was around 46 percent in 2001, at the beginning of our sample, and then gradually ticked up to 54 percent by the end of 2018, mostly led by rising female labor force participation. Hence, any attempt to understand the long-run potential of the labor market for Turkey cannot ignore the participation margin. Even though this margin has been relatively stable (and hence ignored) in developed economies, there is now some evidence that the Great Recession might have changed this as well (Elsby, Hobijn, Şahin, 2015, Erceg, Levin, 2013). This paper confronts both challenges and develops a method to estimate the rate of unemployment in the long run without relying on variations in the general price level and taking into account changes in the labor force participation rate. Finally, our approach requires only aggregate data, as opposed to detailed labor market data which are a challenge to obtain for a developing country.====To implement this, we follow Tasci (2012), who builds an unobserved-components model and uses data on flows between employment and unemployment for the US. Tasci (2012) uses the trend flow rate estimates from the unobserved components model to compute the unemployment rate trend. This method uses the law of motion for unemployment to relate labor market flow rates to the unemployment rate. Thus, one can define the long-run unemployment rate consistent with the underlying trends in the labor market in a way that is consistent with the modern theory of labor market search. In this paper, we adopt the methodology in Tasci (2012) to incorporate labor force changes to estimate the unemployment rate trend for Turkey.====In addition to Tasci (2012), we draw on the work by Sengul (2014), which estimates Turkey’s monthly flow rates from 2005 to 2011, including flows from nonparticipation to unemployment. We first obtain quarterly flow rates from 2001 to 2018 relying on the measurement proposed in Sengul (2014). Then, using a parsimonious unobserved components method, we decompose the flow rates into their trend and cyclical components. Once we estimate the trend components, we infer the unemployment rate trend, i.e., the natural rate, implied by the steady-state description of the unemployment rate in a standard labor market search model.====Our results suggest that the unemployment rate trend for Turkey fluctuated between 8% and 13% over the sample period, ending 2018 at the 12% mark. The first decade of our sample period was dominated by a rising separation rate trend as well as a trend increase in the unemployment exit rate. This decade also featured a low participation rate. Our estimates suggest that since about 2012, Turkish labor markets have stagnated somewhat with relatively flat trend estimates for the flow rates. Moreover, the gradually rising labor force participation in the first decade of the sample accelerated further and settled at a relatively high trend growth rate. Our estimates confirm that the participation margin is important for Turkey and its effect on the estimated unemployment rate trend is nontrivial.====The rest of the paper proceeds as follows: In the next section, we lay out the unobserved components model. After describing the methodology for measuring the flow rates and estimating the trends, we present the data and the estimation results in Section 3. A more detailed discussion of the natural rate concept we develop here is presented in Section 4, followed by Section 5, which contains robustness checks. The last section concludes.","Unemployment flows, participation, and the natural rate of unemployment: Evidence from turkey",https://www.sciencedirect.com/science/article/pii/S0164070419300783,1 May 2020,2020,Research Article,197.0
Kwon Dohyoung,"Investment Policy Division, National Pension Research Institute, 180 Giji-ro, Deokjin-gu, Jeonju-si, Jeollabuk-do 54870, South Korea","Received 16 January 2019, Revised 30 March 2020, Accepted 3 April 2020, Available online 25 April 2020, Version of Record 3 May 2020.",https://doi.org/10.1016/j.jmacro.2020.103208,Cited by (3),"What moves corporate bond credit spreads? This paper employs a novel statistical method to extract the shock that accounts for the maximal amount of the forecast error variance of credit spreads over a given forecast horizon. I find that the extracted shock can explain a substantial portion of unpredictable fluctuations in credit spreads. In particular, impulse response functions indicate that it has a significant adverse effect on economic activity and financial markets, and closely resemble those of the risk shock as reported in Christiano et al. (2014). To investigate this interpretation more formally, I identify the risk shock using the VIX index as a measure of uncertainty proposed by Bloom (2009) and show that surprisingly, the two shocks are intimately related despite using different identification procedures. This finding implies that the risk shock is the main driver of movements in credit spreads, providing empirical evidence on their strong linkages with ==== dynamics, as well as on their roles in presenting valuable information about future economic activity.","Credit spreads—the difference in yields between private debt securities and government bonds with the same maturity—have been investigated in many separate studies. In macroeconomics, credit spreads typically serve as a key gauge of the tightness of financial conditions in the economy and thus have been suggested as an important business cycle indicator (e.g. Gilchrist and Zakrajšek (2012); Gourio (2013); Gilchrist et al. (2014)). In forecasting, it is well established that credit spreads contain valuable predictive content for future economic activity (e.g. Gilchrist et al. (2009); Faust et al. (2013)). Moreover, as argued by Cúrdia and Woodford (2010), credit spreads have been considered as a crucial factor for policymakers when conducting monetary policy.====All of those papers demonstrate strong linkages between credit spreads and macroeconomic dynamics, but important questions still remain unsolved. What really moves credit spreads? Do the fundamental sources resemble macroeconomic shocks or are they simply related to innovations occurring independently in credit markets? Especially, can the driving forces explain the well-known relation between credit spreads and macroeconomic variables?====To answer those questions, I construct a vector autoregression (VAR) model that combines the GZ credit spread developed by Gilchrist and Zakrajšek (2012) with key macro aggregates and financial variables over the period 1973–2015. Most of the previous studies identify credit shocks by imposing the short-run zero restrictions (e.g. Lown and Morgan (2006); Gilchrist and Zakrajšek (2012)) or sign restrictions (e.g. Meeks (2012); Gambetti and Musso (2017)), and then analyze their effects on economic activity and financial markets. This paper, however, takes a reverse identification strategy in order to uncover the underlying main driver of movements in credit spreads. Specifically, similar to Barsky and Sims (2011) and Kurmann and Otrok (2013), I adopt the statistical approach proposed by Uhlig (2003) to extract the shock that accounts for the maximal amount of the forecast error variance (FEV) of credit spreads over a given forecast horizon. Then, plausible economic interpretations of the extracted shock are suggested by examining the impulse response functions of different variables in the VAR.====I find that one single shock can explain more than 80% of all unpredictable fluctuations in credit spreads over a 5-year forecast horizon. This result is obtained without any requirement that a small number of shocks account for a large fraction of variations in credit spreads. More importantly, impulse response functions suggest that the shock leads to a significant reduction in output and investment, as well as disinflation. Furthermore, it causes a sharp rise in credit spreads and has a strong negative effect on stock markets, increasing the VIX index dramatically.====The considerable decline in economic activity and financial markets indicates that the underlying sources of movements in credit spreads are intimately associated with fundamental macroeconomic shocks, rather than with disturbances arising independently in credit markets. In particular, the impulse responses to the extracted shock look much like those of the risk shock reported in Christiano et al. (2014). To assess the plausibility of the interpretation formally, I use the VIX index as a measure of uncertainty proposed by Bloom (2009) and identify the risk shock as the innovation that explains the majority of the FEV of the VIX index, with additional sign restrictions consistent with the well-documented empirical implications of the risk shock. Surprisingly, I find that the risk shock results in almost identical impulse responses and is also highly correlated with the extracted shock. This finding is noteworthy because identification procedures behind the two shocks are completely different from each other. Therefore, I conclude that variations in credit spreads are mostly driven by the risk shock.====This paper contributes to a growing body of literature on Dynamic Stochastic General Equilibrium (DSGE) models with risk or uncertainty shocks by presenting empirical evidence on their close relationship with credit spreads. Several existing studies, such as Gourio (2013), Christiano et al. (2014), Gilchrist et al. (2014), and Arellano et al. (2019), build a DSGE model with financial frictions and show that an unexpected increase in uncertainty leads to a sharp widening of credit spreads, which induces firms to reduce production inputs, resulting in a sizable contraction in economic activity.==== This paper complements those studies by directly identifying the underlying sources of movements in credit spreads within an empirical framework and linking them to the business cycle literature. In addition, the finding that the risk shock is the main driver of variations in credit spreads supports their roles in providing substantial information regarding future economic activity, as stressed by Gilchrist et al. (2009), Gilchrist and Zakrajšek (2012), and Faust et al. (2013).====The remainder of the paper proceeds as follows. Section 2 describes the empirical methodology including a VAR specification, identification strategy, and estimation. Section 3 reports the main results and presents possible economic interpretations. Section 4 shows several robustness checks and Section 5 concludes.",Risk Shocks and Credit Spreads,https://www.sciencedirect.com/science/article/pii/S0164070420301348,25 April 2020,2020,Research Article,198.0
"Elroukh Ahmed W.,Nikolsko-Rzhevskyy Alex,Panovska Irina","Lehigh University and DePauw University, 621 Taylor Street, Bethlehem, PA United States,Lehigh University, 621 Taylor Street, Bethlehem, PA United States,Department of Economics, The University of Texas at Dallas, 800 W. Campbell Rd GR 31, Richardson, TX 75080, United States","Received 24 September 2019, Revised 24 March 2020, Accepted 3 April 2020, Available online 23 April 2020, Version of Record 30 April 2020.",https://doi.org/10.1016/j.jmacro.2020.103206,Cited by (3),"We study changes in the plucking behavior of employment growth, as well as changes in its relationship with the output cycle in the ====. Using both revised and real-time data, we consider several popular measures of the output cycle. For most countries, we see significant evidence in favor of structural changes in the response coefficients of employment growth to its own gap, suggesting much slower recoveries in labor markets, consistent with the jobless recoveries hypothesis. However, we also find evidence in favor of heterogeneity across countries, both in the responses of employment to employment gaps and in the responses of employment to the output cycle.","The Global Financial Crisis (GFC) and its aftermath brought to light many concerns about the changing nature of the business cycle. An issue that has received a lot of attention is whether the link between employment and the business cycle has broken down in recoveries. Labor market conditions continued deteriorating for an unusually long period following the end of the recession in the US, and employment took more than three years to return to its pre-recession levels, about two and a half times longer than the average for the previous recessions. Many economists therefore refer to the 2009–2013 period as a “jobless recovery”.====While recessions and expansions are well-defined business cycle concepts, there is no formal definition of an economic recovery. The conventional empirical and theoretical approaches do not provide a guide for predicting the length, the speed, or the robustness of a recovery. Recessions are thought to simply be followed by expansions with no difference between the early and the late stage (Fatas and Mihov, 2013). Similarly, there is no single formal definition of a jobless recovery in the business cycle literature. However, the following two definitions have been commonly used and accepted in the empirical literature (see, inter alia, Bradley and Jansen, 2018, DeNicco, 2015, Panovska, 2017): “a period when the speed and the amplitude of the upturn in employment following a recession do not match the speed and the amplitude of the downward movement during the preceding recession” or “a period following a recession when the employment level does not recover to its pre-recession peak for an extended period following an upturn in output”. The first definition links the movement of employment in the period following a recession to its own dynamic during the recession, the second is linked to a decoupling between the employment cycle (employment gap) and the business cycle (output gap), but both are closely related to each other and indicate lack of a bounceback in employment following a recession.====Three important questions arise in this context. First, what is an appropriate way to empirically model recoveries? Second, was the slow recovery following the financial crisis is an outlier, or a part of an ongoing change where recoveries in employment get more sluggish? Finally, if the latter is correct, is this phenomenon unique to the US, or more widespread?====From the policy point of view, it is crucial to understand if and how the dynamics of employment has changed over time. For example, if the relationship between employment and the output cycle became weaker, expansionary policies during downturns and the initial recovery stages that are meant to stimulate output growth may not lead to employment growth. If this phenomenon is spread beyond the US, it means that there were global changes to the nature of the business cycle.====This paper investigates the response of employment growth to business cycle conditions in the G7 countries (United States, Canada, France, Germany, Italy, Japan, and the United Kingdom). We use an extension of the (Friedman, 1993) plucking model that allows for distinct recovery dynamics. In our context, the plucking model is an empirical model that can be used to capture asymmetry in business cycles.====The plucking model was first proposed by Friedman as an alternative view of symmetric business cycles. In contrast to conventional models that assume that output and other macroeconomic variables fluctuate symmetrically around their long-run potential rates, the plucking model suggests that macroeconomic variables are typically at their potential levels, except for recessions that “pluck” the economy downward. That is, using nomenclature borrowed from Morley (2019), instead of assuming that business cycles are predictable symmetric deviations from an underlying trend (“rollercoaster” dynamics), the plucking model assumes that the economy grows at its full capacity and recessions are occasional unpredictable health shocks that pull the economy’s strings down that are followed by a recovery stage (“healthy versus sick state”’ dynamics).====A number of studies in the empirical business cycle literature, for example, Morley and Piger (2012) and Sinclair (2010), have shown that nonlinear models that incorporate plucking or bounceback capture the dynamic in output better than symmetric models. Dupraz et al. (2019) show that the unemployment rate also exhibits significant plucking behavior. Bradley and Jansen (2018) utilize a plucking model for studying asymmetry in employment and jobless recoveries in the US. Therefore, a plucking model would be an appropriate starting point for studying recoveries and jobless recoveries. If there is indeed plucking in employment, then recessions (declines in employment) are followed by a “bounceback” or catch-up phase, and a plucking coefficient proxies the speed of the bounceback phase. In our context, if recoveries are jobless or are becoming more lackluster over time, the plucking coefficient would become smaller or insignificant over time. To account for the fact that jobless recoveries can also be defined as decoupling between employment and the output gap, we also examine whether there is evidence of a change in the response of employment to the output cycle, while also accounting for uncertainty about the magnitude and shape of the output cycle.====Practically all previous empirical studies on jobless recoveries have used revised data. However, this data does not necessarily reflect the imperfect information available to the (labor) demand side at the time when hiring decisions were made. Instead, employers have to rely on the potentially wrong or incomplete information set that is available to them at the time when they make their hiring decisions, known as real-time data (the preliminary numbers, usually issued by the government). Therefore, to fully evaluate the response of employment growth to output fluctuations (that is, the output gap), we compare the response of employment to its gap and to output fluctuations based on both real-time data and on revised estimates (which reflect the complete information set that became available later).====On the one hand, using output gaps based on real-time data helps us evaluate which output gaps are perceived to be important when it comes to employment growth, and it helps us evaluate how the relationship between employment growth and real-time economic conditions has changed over time. On the other hand, revised data helps to assess if the true relationship between employment growth and the output gap really has changed over time. Not only we contrast the response of employment to revised output versus real-time output gaps for each economy, but we also carry out a cross-country comparison to look for differences and similarities among the G7 economies.====Our study expands the existing literature on jobless recoveries in four dimensions. First, rather than simply conducting a split sample analysis that only considers the pre and post Great Moderation periods before and after 1984 (as, in, for example, DeNicco, 2015 or Panovska, 2017), or imposing the break dates as known, we allow for multiple breaks and we estimate the break dates for the changes in the dynamics in employment from the data. Second, we account for the uncertainty in the output gap directly. We consider both models where the output gap is estimated from the data by using both bounceback plucking models and symmetric models. Third, we consider both revised and real-time data to account for the fact that employers had an imperfect data set when they were making decisions. Finally, we conduct a multi country comparison for all G7 countries.====We find that there is strong evidence in favor of jobless or slower recoveries in the US, Canada, Japan, and Italy, where recoveries in labor markets became significantly slower over time. For the US, the plucking behavior of employment declines significantly and uniformly over time. This finding is robust to both assuming that economic agents condition on a model with smooth or more abrupt movements in the output cycle, as well as to conditioning on real-time data. We observe a similar pattern for Canada and Japan, and for Italy. There is also evidence in favor of hysteresis in labor markets for Japan, where employment does not return to its pre-recession levels and the employment gap stays large for an extended period of time.====There is no evidence that the US cycle is necessarily more “European”, because there is significant heterogeneity across European countries.==== Unlike Italy, which exhibits jobless recoveries, the labor market recoveries in the UK and in France (conditioning on the state of the output cycle) became ==== over time. On the other hand, the labor market recoveries in Germany do not exhibit evidence of significant time-variation or nonlinearity. We also find a lot of disparities in the response of employment to the output cycle across the G7 countries.====In Section 2 we provide a brief overview of the related literature that most directly motivates our study. The dataset we use and the empirical model are introduced in Section 3. In Section 4 we discuss our main findings. Section 5 concludes.",A look at jobless recoveries in G7 countries,https://www.sciencedirect.com/science/article/pii/S0164070420301324,23 April 2020,2020,Research Article,199.0
"Liu Jinan,Dery Cosmas,Serletis Apostolos","Department of Economics, University of Calgary, Calgary, AB T2N 1N4, Canada","Received 28 October 2019, Revised 23 March 2020, Accepted 25 March 2020, Available online 13 April 2020, Version of Record 7 May 2020.",https://doi.org/10.1016/j.jmacro.2020.103203,Cited by (6),"The main objective of this paper is to examine the information content of the credit card-augmented Divisia monetary aggregates and credit card-augmented Divisia inside monetary aggregates, recently produced by the Center for Financial Stability. We compare the inference ability of the credit card-augmented Divisia monetary aggregates and credit card-augmented Divisia inside monetary aggregates to the conventional Divisia monetary aggregates, at all levels of monetary aggregation. Using cyclical correlations analysis and ====, we find that both the conventional Divisia monetary aggregates and the credit card-augmented Divisia monetary aggregates are informative in predicting output. Moreover, during, and in the aftermath of the 2007–2009 financial crisis, the credit card-augmented Divisia measures of money are more informative when predicting real economic activity than the conventional Divisia monetary aggregates. We also find that broad Divisia monetary aggregates provide better measures of the flow of monetary services generated in the economy.","There is a long tradition in macroeconomics of using models with money to study economic fluctuations, with an influential early example being the discussion of procyclical money supply movements by Friedman and Schwartz (1963). Friedman et al. (1963) provide further evidence that monetary instability can be, and in fact has been, an important source of instability in the American economy. However, the real business cycle approach to economic fluctuations posits that postwar business cycles could be explained within a framework that makes no reference to money or indeed to nominal variables at all. Although the New Keynesian model reintroduced a role for monetary policy in stabilizing or destabilizing the economy, strikingly, measures of the money supply remain well behind the scenes in these newest business cycle models and hinder empirical work — see Belongia, Ireland, 2016, Belongia, Ireland, 2019.====Barnett (1980) demonstrated that simple-sum monetary aggregates produced by the Federal Reserve do not correctly measure the true flow of monetary services generated in an economy where the monetary assets are not perfect substitutes. Barnett, 1978, Barnett, 1980 proposed an alternative measure of money, the Divisia monetary aggregates, that can track changes in the flow of monetary services much more accurately under a lot of circumstances. To illustrate the important role of measurement on inference, both Belongia (1996) and Hendrickson (2014) found that simply replacing the official simple-sum measures of money with their Divisia counterparts suffices to overturn earlier empirical results that suggested that fluctuations in the money supply can be ignored in business cycle analysis. As Belongia (1996) put it, inferences about the effects of money on economic activities may depend importantly on the choice of a monetary index.====Traditionally, the effects of monetary policy on output draw on the hypothesized links between money, interest rates, and output through monetary or credit channels. An extensive literature exists on the policy relevance of the Divisia monetary aggregates, which do not include the liquidity services that credit cards produce. For example, Schunk (2001) has shown that forecasts of U.S. real GDP are most accurate when a Divisia aggregate is included. Belongia and Ireland (2016) extend a New Keynesian model to include roles for monetary aggregates and show that movements in both quantity and price indexes for monetary services correlate strongly with movements in output when the Divisia monetary aggregates are used. Dery and Serletis (2020) show that the inference ability of the Divisia monetary aggregate is even stronger when we use the broad Divisia monetary aggregates.====Taken together, these studies all show that the monetary aggregate that could track liquidity properly has a strong performance in forecasting economic activities. Motivated by this, both theoretical and empirical work on tracking the exact liquidity services are in need to catch up with innovations on alternative payments, such as credit card transaction services. Credit card transactions play a significant role in facilitating the flows of goods and services. Credit cards provide a deferred payment service that is not available from traditional monetary assets — see Barnett and Liu (2019). According to the Center for Financial Stability (CFS), in July 2006, the monthly credit card transactions volume was $146 billion in the U.S. economy. By March 2019, the monthly credit card transactions volume had reached $323 billion. With the rapid growth of credit card transactions, the question is whether the increase in credit card transaction services creates risks to future macroeconomic performance?====In this article, we identify the increase in credit card transaction services and their macroeconomic consequences by using the credit card-augmented Divisia monetary aggregates, recently produced by the CFS, and which account credit card transaction services into liquidity measurement — see Barnett and Su (2018). We reexamine the link between money and economic fundamentals ying attention to money measures that include the transactions services provided by credit cards. In our setting, asking whether the rapid growth in credit card transaction services leads to risks in macroeconomic outcomes boils down to the inference ability of the credit card-augmented Divisia monetary aggregates on macroeconomic fluctuations.====Despite the well established literature on Divisia monetary aggregates, the credit card-augmented Divisia monetary aggregates are rather young. Data are available only since 2006, and therefore they are mostly unexplored. In fact, there is very little empirical evidence available on the relative inference performance of the credit card-augmented Divisia monetary aggregates, although Barnett et al. (2016) made the assertion that much of the policy relevance of the Divisia monetary aggregates literature could be strengthened by the use of credit card-augmented Divisia money measures.====The strong inference performance of the conventional Divisia monetary aggregates on macroeconomic activities, together with the rapid growth in the use of credit card transaction services, make it particularly interesting to investigate the inference ability of credit card-augmented Divisia monetary aggregates on key macroeconomic variables. One of the key issues yet to be analyzed is their cyclical movements and inference ability compared to conventional Divisia monetary aggregates. Do the credit card-augmented Divisia monetary aggregates have a stronger correlation with industrial production and economic activity? Do the credit card-augmented Divisia monetary aggregates outperform the conventional Divisia monetary aggregates in forecasting the business cycle?====In this paper, we investigate the cyclical properties of the credit card-augmented Divisia monetary aggregates using Hamilton’s (2018) filter and Granger (1969) causality tests. We present a comprehensive comparison of the inference ability of the credit card-augmented Divisia monetary aggregates versus the conventional Divisia monetary aggregates at all levels of monetary aggregation. This paper contributes to the literature in several ways. First, it fills the gap in investigating the cyclical behavior and information content of credit card-augmented Divisia monetary aggregates on economic activities. Second, it examines and compares the performance of the credit card-augmented Divisia monetary aggregates and the traditional Divisia monetary aggregates. We show that both the conventional Divisia monetary aggregates and the credit card-augmented Divisia monetary aggregates are informative in predicting real output. Moreover, the credit card-augmented Divisia monetary aggregates have stronger statistical predictive power. Third, we think this paper constitutes an important step in the direction of quantifying the effects of credit card transaction services on the business cycle. Finally, it also sheds light on the Barnett critique and reinforces the irreplaceable role of money in the conduct of monetary policy.====The remainder of the paper is organized as follows. We begin in Section 2 by providing a brief discussion of the Divisia approach to monetary aggregation, from both the demand side and the supply side, following Barnett et al. (2016). Section 3 discusses the data. In Section 4, we undertake an empirical investigation of the cyclical behavior of the conventional Divisia monetary aggregates, the credit card-augmented Divisia monetary aggregates, and the credit card-augmented Divisia inside monetary aggregates, and output. In doing so, we use the Hamilton (2018) filter, the Kydland and Prescott (1990) methodology, and Granger (1969) causality tests. In Section 5, we investigate the predictive content of credit-card services in isolation, with the hope of understanding the grounds in which the standard New-Keynesian model is lacking. The final section concludes the paper and discusses the implications for monetary theory and policy and directions for future work.",Recent monetary policy and the credit card-augmented Divisia monetary aggregates,https://www.sciencedirect.com/science/article/pii/S0164070419304513,13 April 2020,2020,Research Article,200.0
Richard Higgins C.,"Colgate University, 13 Oak Drive, Hamilton, NY 13346, United States","Received 17 June 2019, Revised 16 March 2020, Accepted 25 March 2020, Available online 29 March 2020, Version of Record 6 April 2020.",https://doi.org/10.1016/j.jmacro.2020.103204,Cited by (2)," or reduced shock volatility, two common explanations, in causing the Great Moderation.","A key feature of the United States’ economy in the second half of the 20th century is the dramatic reduction in the volatility of many real and nominal macroeconomic variables starting in the 1980s, which is commonly called the Great Moderation.==== Much research has focused on the Great Moderation since it provides an ideal setting to study time-varying volatility, but the cause of the Great Moderation is still unclear and widely debated. Research has focused mainly on whether the moderation can be attributed to improved monetary policy (“good policy”) or smaller macroeconomic shocks (“good luck”).==== Clarida et al. (2000) argue that improved monetary policy, where the Federal Reserve responded more aggressively to inflation, led to the Great Moderation. Sims and Zha (2006) point to less volatile exogenous shocks to the economy as the reason for reduced volatility.====This paper will explore an additional explanation: changing financial frictions. Financial frictions may have changed during the 1980s and 1990s, a period of widespread financial innovation and deregulation. The role financial frictions played in the Great Moderation is studied in a setting that can also study the roles “good policy” and “good luck” played. This is done using an estimated dynamic stochastic general equilibrium (DSGE) model with stochastic volatility, which lets the standard deviation of the exogenous shocks in the model change over time, time-varying monetary policy, and time-varying financial frictions to study the role that the changes in financial frictions played in the Great Moderation.====Understanding the role that changing financial frictions, “good luck,” and “good policy” played in causing the Great Moderation is of historical importance and has implications for the economy today. If the main cause was simply “good luck,” then the Great Recession can be viewed as the likely end of moderate business cycles and we can expect higher volatility in the future. While “good policy” is more optimistic about the future, it is difficult to understand how the Great Recession could have happened when monetary policy was “good.” If a reduction in financial frictions caused the Great Moderation, then moderate business cycles may persist into the future as long as these frictions remain low, which can be aided by prudent financial regulation and appropriate supervision. Additionally, much research has focused on the importance of financial frictions in DSGE models during the Great Recession and following years, such as Del Negro et al., 2015 and Cai et al., 2019. The results from this study will shed light on whether financial frictions need to be considered when studying times outside of financial crisis periods.====Little research has been conducted into what role financial and credit markets may have played in the Great Moderation. This is surprising since Bernanke et al., 1999 showed how financial frictions can affect the size of business cycle fluctuations through the financial accelerator mechanism.==== If financial frictions fell during the 1980s due to deregulation or financial innovation, this could weaken the financial accelerator mechanism and make business cycles less volatile. Jermann and Quadrini (2009) develop a model that shows how financial innovation can reduce macroeconomic volatility. In an empirical study, Dynan et al. (2006) find that financial innovation may be an important factor in explaining the decrease in volatility during the Great Moderation.==== This paper differs from Dynan et al. (2006) since it studies corporate borrowing using a structural model, while their study used household borrowing data and a vector autoregression (VAR).====This paper is the first to study the causes of the Great Moderation with a DSGE model that allows for changing monetary policy, stochastic volatility, and changing financial frictions. The DSGE model is used for two reasons: first, it allows several competing hypotheses (good luck, good policy, reduced financial frictions) to be studied at the same time; second, Benati and Surico (2009) showed that using a VAR can lead to incorrect conclusions about the causes of the Great Moderation. A nonlinear approximation is necessary because of the inclusion of stochastic volatility. The methodology of Fernández-Villaverde et al. (2015) is extended in order to estimate the model using additional observable variables. The model is estimated using Bayesian techniques.====The inclusion of financial frictions into the model is a departure from the standard New Keynesian models previously used in most studies of the Great Moderation. This is important since excluding them from the model may lead to incorrect conclusions about the causes of the Great Moderation. Financial frictions are modeled with a financial accelerator mechanism where entrepreneurs borrow funds from financial intermediaries in order to purchase capital that is then rented to firms. Each entrepreneur’s productivity is determined by an idiosyncratic productivity draw that is freely observed by the entrepreneur. The lender must pay a fee to observe the productivity level, which leads to the financial frictions. Changing financial frictions are modeled as a change in the standard deviation of the productivity draw.==== When the standard deviation is low, there is little uncertainty about entrepreneurial productivity and financial intermediaries are more willing to lend. When the standard deviation is large, asymmetric information is more of a problem and financial intermediaries need to be compensated with a larger risk premium. It is important to note that the difference in lending happens despite average entrepreneurial productivity and all freely observable characteristics being unchanged. While this is modeled as a characteristic of borrower risk, a change in banks’ abilities to screen borrower credit risk or a change in how well banks can diversify risk would have similar effects. Therefore, long-term trends in the standard deviation of the productivity draw should be viewed as trends in financial frictions. This provides a tractable way to study how financial frictions changed over time in a large, nonlinear model.====The results show that changes in financial frictions are important for explaining the Great Moderation. During the early 1980s, financial frictions fell for an extended period before increasing slightly leading up to the Great Recession. The reduction in financial frictions observed in the 1980s and 1990s was an important contributor to the fall in macroeconomic volatility witnessed during the Great Moderation. This is supportive of the results of Fuentes-Albero (2019), a similar study that models financial frictions differently and does not include stochastic volatility. There is little evidence that changing monetary policy or stochastic volatility played a role in the Great Moderation, a finding that differs from the conclusions of Fernández-Villaverde et al. (2010). These results suggest that part of the reason for good luck being considered a strong driver of the Great Moderation may be due to some prior studies ignoring the role that changing financial frictions played in the Great Moderation.====The rest of the paper is organized as follows. Section 2 presents the model to be estimated. Sections 3 describes the data sources and presents the estimation procedure. Section 4 presents the priors and estimation results. Sections 5 and 6 show the underlying states and results from counterfactual experiments. Finally, Section 7 provides some concluding remarks and describes some potential future related research.",Financial frictions and changing macroeconomic volatility,https://www.sciencedirect.com/science/article/pii/S0164070419302629,29 March 2020,2020,Research Article,201.0
"Pietrunti Mario,Signoretti Federico M.","Banca d’Italia, DG Economics, Statistics and Research, Via Nazionale, 91 00184, Rome, Italy","Received 12 June 2019, Revised 21 December 2019, Accepted 17 March 2020, Available online 20 March 2020, Version of Record 3 April 2020.",https://doi.org/10.1016/j.jmacro.2020.103201,Cited by (4),We study the transmission of conventional and ,"The interest in the relationship between household debt and the transmission of monetary policy has surged in recent years, largely on account of the central role that the housing markets had in the Global Financial crisis and in the ensuing deleveraging process. One aspect that is stressed by recent research is the importance of income (or ====) effects related to household debt in the transmission of monetary policy. This aspect has been the subject of a fair amount of empirical studies, which have focused on the impact of changes in the short-term policy rate (i.e. ==== monetary policy) and have shown that the income channel represents a powerful channel, as changes in installments determine a redistribution of resources between borrowers and savers, who have different marginal propensities to consume (Cloyne, Ferreira, Surico, 2020, Di Maggio, Kermani, Keys, Piskorski, Ramcharan, Seru, Yao, 2017, Ehrmann, Ziegelmeyer, 2017, Flodén, Kilström, Sigurdsson, Vestman, 2016).====At the same time, two questions are still largely underdeveloped in the literature: what is the importance and the pattern of the income effects in a New Keynesian framework? How does this channel work for ==== monetary policy operations? Existing studies have shown that quantitative easing operations rely on portfolio rebalancing effects, which in turn depend on imperfect substitutability across assets with different features in terms of maturity or liquidity (Alpanda, Kabaca, 2019, Chen, Cúrdia, Ferrero, 2012, Vayanos, Vila, 2009), but the relevance of the cash flow channel due to unconventional policies has not been explored yet.====In this paper we study the transmission of monetary policy via the household debt market, focusing on the role of the income channel. We use a standard DSGE model with financial frictions and heterogeneous households ==== Iacoviello (2005) adding long-term debt modelled alongside Kydland et al. (2016). In our model household debt can be loosely interpreted as mortgage debt and can be either an ==== or a ====-rate mortgage (ARM and FRM, respectively): the rate on ARMs is equal to the short-term interest rate, set by the central bank; the rate on new FRMs reflects the expectations over the future path of short-term rates plus a term premium component (Cochrane 2001).==== The term premium arises because patient households are assumed to have preferences for a portfolio composed of both a short-term and a long-term asset (as in Alpanda and Kabaca 2019) and it is determined by the covariance between consumption and the installment in each period. Conventional monetary policy is modelled in a standard way, with the short-term nominal interest rate set by the central bank according to a Taylor rule.====Unconventional monetary policy (UMP) is modeled in a stylized way which aims at capturing portfolio balance effects arising from quantitative easing policies. More precisely, UMP directly affects the long-term interest rate by tilting relative demand for short- and long-term bonds, in line with the “duration extraction” activity of the central bank via asset purchases (d’Amico et al. 2012). The equation describing UMP includes both an endogenous component, which captures the fact that central banks implemented these measures in response to weak inflation and GDP dynamics, and an exogenous shock, which leads to a deviation of the term premium unrelated to the path of future short-term policy rates.====We use our model to study the transmission of both a conventional and an unconventional monetary policy shock. In the analysis we consider separately an economy with only ARMs and one in which all debt is FRMs. While this choice may somehow constitute a limitation of the analysis, it allows us to isolate the functioning of the income channel in the two models. It is easy to show that, if we allowed for an endogenous choice between fixed- and variable-rate contracts, results would be in-between the two cases. As unconventional policy affects only long-term rates, we study the transmission of non-standard measures in the FRM model. In addition, we also show the impact of refinancing on the transmission of monetary policy shocks and its impact on the cash-flow channel.====We find the following results. First, while the response of output and aggregate consumption to a conventional monetary policy shock is quantitatively similar between the ARM and FRM models, the importance and the pattern of the cash-flow channel is rather different. The contribution of this channel to consumption dynamics is significantly larger in the ARM model (in absolute values, as the two agents are affected in opposite ways): in our baseline calibration the cumulative change after 5 years corresponds to about 30 and 170 per cent of steady-state consumption, for Patient and Impatient agents respectively. This compares with only 20 and 100 percent, respectively, in the FRM model. This finding is in line with the recent empirical literature on the cash-flow effects of monetary policy.====Second, the pattern of the income channel is also very different between the ARM and the FRM models: the contribution of the cash-flow effect to consumption dynamics is very strong in the first few quarters after the shock and then quickly dies off; on the contrary, the impact slowly builds up for FRMs, reflecting the fact that changes in the long-term interest rate have persistent effects on the charge of fixed-rate mortgages.====Third, we find that the effect of a 25bp reduction in the term premium (unconventional monetary policy shock) has roughly half the impact on aggregate consumption of a comparable reduction in the short-term policy rate (conventional monetary policy shock). The transmission mechanism of the income channel is very similar to that of a conventional shock (in the FRM model). When this policy is implemented at the effective lower bound (ELB), its impact is stronger, due to the lack of endogenous reaction to the expansionary stimulus in the short term rate.====Lastly, allowing for refinancing in a model where agents have FRM contracts does not significantly affect the impact of conventional monetary policy, whereas the reaction of endogenous variables to an unconventional policy shock is significantly hampered.====This paper is related to three different strands of literature. First, it is connected to the recent empirical literature on the income or ==== effects, which underscores how monetary policy determines a redistribution of resources between borrowers and savers (Cloyne, Ferreira, Surico, 2020, Ehrmann, Ziegelmeyer, 2017, Flodén, Kilström, Sigurdsson, Vestman, 2016, Di Maggio, Kermani, Keys, Piskorski, Ramcharan, Seru, Yao, 2017).==== All of these papers focus on the effects of ==== monetary policy (i.e., changes in the short-term nominal interest rate), while we also analyze how the cash-flow channel works in response to ==== monetary policy operations. Secondly, this paper is related to the literature that introduces long-term mortgages in DSGE models (Rubio, 2011, Bluwstein, Brzoza-Brzezina, Gelain, Kolasa, 2018, Kydland, Rupert, Šustek, 2016, Garriga, Kydland, Šustek, 2017 and Garriga et al. 2019). Also in this case, a crucial difference with our work is that these studies focus on conventional monetary policy under different institutional characteristics of the mortgage market, while we model an economy in which unconventional monetary policy is able to affect real activity.==== Finally, we are connected to the studies on the theoretical underpinnings of the transmission channels of ==== monetary policy, which have stressed the importance of investors’ preferences for assets of specific maturities, portfolio rebalancing effects, segmented markets (Vayanos, Vila, 2009, Chen, Cúrdia, Ferrero, 2012, Alpanda, Kabaca, 2019). Our main contribution is to combine these strands of literature in an integrated framework and show the distinguishing features in the transmission of unconventional monetary policy, mainly via the cash-flow channel, compared to the conventional policy case.====The rest of the paper is structured as follows. Section 2 describes the model, focusing on the description of the mortgage market, and the pricing of loans. Section 3 discusses the calibration of key parameters. Section 4 presents the main exercises of the paper and the results. Section 5 shows the results of the model with refinancing. Section 6 concludes.",Unconventional monetary policy and household debt: The role of cash-flow effects,https://www.sciencedirect.com/science/article/pii/S016407041930254X,20 March 2020,2020,Research Article,202.0
"Stone Joe A.,Jacobs David","University of Oregon, 435 PLC, kincaid street, eugene, OR 97405, United States,Ohio State University, Eugene Oregon 97408, USA","Received 29 November 2019, Revised 2 March 2020, Accepted 2 March 2020, Available online 7 March 2020, Version of Record 16 March 2020.",https://doi.org/10.1016/j.jmacro.2020.103200,Cited by (0),Unique evidence presented in this study challenges previous findings about presidential politics and business cycles. Prior studies find strong evidence for a Democratic economic growth advantage of about 1.8% per year over the course of a term but only weak evidence for a pre- election surge in growth for incumbent Presidents of either party. This study finds a much smaller Democratic advantage and strong evidence for a pre-election growth surge for Republican Presidents relative to Democratic Presidents. The novelty of these results is attributable to the use of repeated party-change reversals in adjacent terms for identification in place of binary changes in isolated terms separated by as much as a half-century in prior studies. We find a strongly partisan Federal Reserve effect on growth as well. Results are insensitive to an extensive battery of robustness checks.,None,Presidential party affiliation and electoral cycles in the U.S. economy: Evidence from party changes in adjacent terms,https://www.sciencedirect.com/science/article/pii/S016407041930504X,7 March 2020,2020,Research Article,203.0
Morris Stephen D.,"Federal Reserve Board Washington, DC 20551, United States","Received 7 November 2019, Revised 19 February 2020, Accepted 20 February 2020, Available online 26 February 2020, Version of Record 7 March 2020.",https://doi.org/10.1016/j.jmacro.2020.103192,Cited by (2),The Taylor principle states that the policy rate must respond more than one-for-one with ,"Nonlinearity due to the effective lower bound (ELB) for interest rates is important.==== Indeed, although we are conditioned to think of this as a modern technical issue, the concept is as old as the discipline itself, and appears in its seminal contributions. In the very first work on macroeconomic modeling, Hicks (1937) formalized Keynes (1936)’s theory that money supply pins down a relationship between income and interest rates. He theorized that in part because “there is some minimum below which the rate of interest is unlikey to go,” that the liquid-money curve (LM) had a specific shape in the (income, interest rate) plane:====In other words, because of an ELB, the LM curve is nonlinear.====But regardless of this heritage, most modern intuition for the effects of monetary policy arises from a linear setting. There are several reasons why, but the upshot is that many basic principles are simply inconsistent with the existence of an ELB (Appendix A). One particularly salient property is the ====, that the policy rate must respond more than one-for-one with inflation, or else inflation is indeterminate. Obviously, with an ELB, a one-for-one response is not always possible. It is unclear how this fundamental property changes in the modern nonlinear environment of low rates (Laubach and Williams, 2016) and high ELB risk (Hills, Nakata, Schmidt, 2019, Morris, 2020).",Is the Taylor principle still valid when rates are low?,https://www.sciencedirect.com/science/article/pii/S0164070419304690,26 February 2020,2020,Research Article,204.0
"Burlon Lorenzo,D’Imperio Paolo","European Central Bank, DG Monetary Policy, Italy,Sapienza University of Rome, Italy","Received 26 August 2019, Revised 23 December 2019, Accepted 14 February 2020, Available online 17 February 2020, Version of Record 12 March 2020.",https://doi.org/10.1016/j.jmacro.2020.103191,Cited by (4),"The paper provides real-time estimates of the euro-area output gap, based on a standard medium scale ","Economic analysts often rely on the output gap (i.e., the deviation of output from its potential) as an indicator of the overall state of the economy. This is true also for fiscal and monetary authorities, which explicitly include the output gap among the indicators used to ground their policies. However, since the output gap is a latent variable, its estimation and especially its validation are challenging. In this work, we show that the estimate of the euro-area output gap obtained with the (nowadays standard) DSGE model developed by Smets and Wouters (2003) can overcome some of the limitations encountered using the most common methodologies based on statistical filtering and the aggregate production function. In particular, we show that model-based measures are remarkably stable vis-à-vis ex-post revisions.====Several methodologies have been developed to obtain a reliable and economically meaningful measure of the output gap. At the same time, new approaches based on the estimation or direct measurement of the supply capacity of an economy are continuously being proposed. Despite the numerous available methodologies, several studies have questioned the reliability of the output gap estimates in real time. In particular, Orphanides and Van Norden (2002) apply alternative detrending methods to the US data and conclude that the ex-post revisions can be as big as the estimated output gaps. In the same vein, Marcellino and Musso (2011) find that the sign and the magnitude of the euro-area real-time estimates are characterized by a high degree of uncertainty. Recent studies on this topic confirm that there is still no consensus on the best methodology to use for the estimation of the output gap.====This paper documents the advantages in terms of reliability vis-à-vis ex-post revisions of considering a measure of the output gap based on an estimated structural model. The output gap is defined as the deviation of actual output from its potential, where the latter is defined as the outcome of an economy with flexible prices, flexible wages, and constant markups. The more stringent structure imposed to the data by the DSGE model helps to provide stable and reliable measures of the output gap compared to reduced-form methods, especially in periods of greater uncertainty. At the same time, the relatively parsimonious parametrization of the model we adopt, that is, the Smets & Wouters model (henceforth, SW), does not overcharge the data with cross-equation restrictions, achieving a balance between structure and flexibility.====We use standard Bayesian methods to recursively estimate the model on euro-area data at a quarterly frequency over the period 1985q1-2019q2, and compare the DSGE-based measure obtained in real time with that obtained via standard statistical approaches or published by international institutions.==== While a number of studies use structural models to produce output gap estimates, our main goal is to document the reliability of these measures against the ones provided by international institutions or based on more standard methodologies, and to do so we produce yearly vintages of estimates to fairly compare our model-based measure to others. To the best of our knowledge, we are the first to perform a systematic comparison between the reliability of a model-based measure with that of other approaches using real-time estimates over more than twenty years.====Our main findings can be summarized as follows. First, unlike measures of output gap obtained by means of statistical filtering techniques, real-time DSGE-based estimates are remarkably stable and hence are less prone to ex-post revisions, which implies a higher reliability of model-based measures of the output gap over time. Second, our measure of output gap identifies episodes of expansion and recession generally in line with the official business cycle dating of the CEPR. Third, according to our results, the euro-area output gap was still negative and even declining in 2019 at around -2.1%, lower than assessed by most economic analysts and institutions but arguably more consistent with the still weak dynamics of inflation.====The rest of the paper is organized as follows. Section 2 reviews relevant issues in the literature. Section 3 summarizes the main features of the model and defines a model-consistent concept of potential output. Section 4 reports the estimation strategy. Section 5 studies the properties of our measure of output gap. Section 6 compares our estimates with other standard measures in terms of their stability and reliability. Section 7 discusses the relation between model misspecification and the measurement and reliability of the output gap. Section 8 concludes.",Reliable real-time estimates of the euro-area output gap,https://www.sciencedirect.com/science/article/pii/S0164070419303362,17 February 2020,2020,Research Article,205.0
"Caro Paolo Di,Sacchi Agnese","Department of Finance, Italian Ministry of Economy and Finance, Rome (Italy). Tax Administration Research Centre, University of Exeter, United Kingdom,Department of Economics and Law, Sapienza University of Rome. Governance and Economics research Network, University of Vigo, Spain","Received 23 April 2019, Revised 20 January 2020, Accepted 4 February 2020, Available online 6 February 2020, Version of Record 13 February 2020.",https://doi.org/10.1016/j.jmacro.2020.103190,Cited by (5),"The value-added tax is spread in many countries primarily because it allows rising public revenues, but its revenue performance can be undermined in the presence of informality. We document the existence of heterogeneous effects of labor informality on ","The value-added tax (VAT) is widely spread in many advanced and developing countries primarily because it allows rising public revenues by preserving production efficiency and promoting compliance through self-enforcing mechanisms (Keen and Lockwood, 2010). International organizations often suggest switching tax revenues from distortionary income and trade taxes to commodity taxation for sustaining long-run economic growth without discouraging saving, investment and labor supply (Ormaechea and Morozumi, 2019). In recent years, 13 out 27 of the European Union (EU) countries increased the VAT rates for keeping public finances under control following the crisis; in the same period, rising VAT rates and lowering payroll taxes was a suggested strategy for promoting exports without affecting public budgets in the Euro area (De Mooij and Keen, 2013). The revenue performance of the VAT depends on three main factors (Ebrill et al., 2001): the structure of the tax (e.g., the rules for defining the tax base and tax rates); the taxable activities such as final consumption expenditures; and, the presence and the extent of informal economy. In this work, we aim at investigating the third factor, namely the consequences of labor informality on VAT revenues in a developed country.====Different factors, both in the demand and supply side of the economy, can be at work when assessing the effects of labor informality on VAT collection and, therefore, determining the direction of this relation is primarily an empirical issue (Schneider and Enste, 2000). The few empirical works that studied the effects of the informal economy on tax revenues provide mixed results. Using survey data for the EU countries, Williams (2013) does not find any significant relationships between (total) tax revenues and informal employment. For a sample of 125 developed and developing countries, Vlachaki (2015) finds that domestic indirect tax revenues increase up to a threshold value of informality of about two-thirds of official gross domestic product (GDP), after which tax revenues decrease. By contrast, Mazhar and Méon (2017) study the tax burden and the size of the informal economy, by finding a negative relation for 153 developed and developing countries over the period 1999–2007. Drawing conclusions from cross-country studies only, however, can be difficult due to the very different national legislations for the VAT (Davies and Paz, 2011), and the heterogeneous nature, definition and size of the informal economy in developed and developing countries (Schneider et al., 2015). Cross-country comparisons, moreover, do not generally provide information on regions and sectors that are relevant for understanding the welfare effects of informality (Berdiev et al., 2015), VAT performance (Cevik et al., 2019), and reducing specification issues (Herwartz et al., 2011).====In this paper, we provide novel evidence on the effects of labor informality on VAT collection for the case of a developed country, by applying different panel models to new VAT and labor informality regional data that are available for Italy. The main contribution of our work is that we are able to distinguish the heterogeneous effects of labor informality on VAT collection, based on the different demand and supply side channels, since we have information on both total VAT revenues, which include business-to-business (B2B) and business-to-consumer (B2C) information, and B2C VAT revenues only. Moreover, by using data on the twenty Italian regions and for specific sectors of production, we show that labor informality has different effects on VAT revenues across areas and sectors given the heterogeneous characteristics and functions of labor informality in regions and productive sectors (López, 2017; Ulyssea, 2018).====Italy is a good candidate for this analysis for different reasons. It is a developed country where the size of the shadow economy, 211 billion euro (about 12% of the Italian GDP), and the VAT gap, 34 billion euro (about one-third of the VAT revenues), both referring to the year 2017, are the highest among the advanced economies. In Italy, the regional distribution of VAT revenues and informality, with the description of the variables provided in Section 3.1, is highly heterogeneous following the very different economic conditions and industrial structures across regions (Boeri and Garibaldi, 2002). In this country, the regional shadow economy labor markets are different in size, characteristics, and economic implications (Chiarini et al., 2013). In the North, informal occupations are mostly made up of secondary jobs and moonlighting activities, while in the South informal workers, which are concentrated in agriculture, building and private services, are the byproduct of weak labor markets, low productivity, and high unemployment levels (Marzano, 2005). The graphs in Fig. 1 illustrate the distribution of total VAT (panel a) and labor informality (panel b) across Italy. High (low) VAT revenues (labor informality) are registered in the Central and Northern regions; the opposite is true in the Southern regions. Fig. 2 gives a preliminary view of the connections among labor informality, labor productivity, and unemployment: high levels of informality are registered in the regions having low productivity and high unemployment.====The main results of our work are the following. First, we find that labor informality produces negative effects on total VAT collection, which include the VAT self-enforcing mechanism operating in the production side of the economy and consumption effects. Yet, we find that labor informality has positive consequences on VAT collection if we limit the attention to B2C VAT revenues only. Second, our results suggest that the relation between labor informality and VAT collection, both total and B2C, shows heterogeneity depending on the size of informality, by approximating an inverted U-relationship. In particular, we document that, for relative low (high) levels of informality, which in Italy means being located in the center-North (South) of the country, the effects of labor informality on the growth of total VAT revenues are positive (negative). The situation partially changes when looking at the effects of labor informality on B2C VAT collection, where we find low heterogeneity depending on the size of informality. As discussed in the Section 2, our findings are consistent with the theoretical predictions deriving from general equilibrium macro models analyzing informality and tax revenues (Bennett, 1990; Kesselman, 1993; Leal-Ordónez 2014), and with the literature highlighting the heterogeneous welfare effects of labor informality (Busato et al., 2011; Loayza and Rigolini, 2011). We also present, in the Appendix for saving space, a simple two-sector economy macro model that builds on the literature studying the macroeconomic effects of the informal economy (Adam and Ginsburgh 1985; Loayza, 1996) in order to help interpreting our results.====Third, we find that the heterogeneous consequences of informal jobs on VAT revenues vary across regions and sectors and that they can be explained by differences in labor market characteristics. Specifically, the negative effects of labor informality on VAT collection are mostly concentrated in regions characterized by weak labor market conditions, such as low productivity, high unemployment and low activity rates. This is line with recent cross-country evidence (Amin et al., 2019; Dabla-Norris et al., 2019). Lastly, our results suggest that the recent Great Recession, which mostly produced a drop in the external demand, credit constraints, and public debt issues (Bartoletto et al., 2019), had asymmetric cyclical effects depending on the size of informality. In particular, we find counter-cyclical (pro-cyclical) effects for high (low) levels of labor informality, when analyzing the consequences of informal jobs on total VAT collection. A possible explanation for this result is that the Great Recession produced an asymmetric effect on shadow economy labor markets across areas, by reducing (augmenting) informality where informal jobs are complement to (substitute for) formal ones (Colombo et al., 2016).====Our findings are robust to different sensitivity checks, including possible endogeneity concerns when analyzing the consequences of informality on economic variables as well as the use of alternative measures for labor informality, and additional controls to the baseline specification. The work is organized as follows. The next section discusses the theoretical motivations guiding our empirical analysis. Section 3 describes data (3.1) and the econometric methodology (3.2). Section 4 contains the main results of the paper. Section 5 discusses the robustness of our findings. Section 6 concludes the work. Additional information and results are provided in the Appendix.",The heterogeneous effects of labor informality on VAT revenues: Evidence on a developed country,https://www.sciencedirect.com/science/article/pii/S0164070419301818,6 February 2020,2020,Research Article,206.0
"Lagravinese Raffaele,Liberati Paolo,Sacchi Agnese","Department of Economics and Finance, University of Bari, Largo Abbazia Santa Scolastica, Bari 70124, Italy,Department of Economics, University Roma Tre, Via S. d'Amico, 77, Rome 00145, Italy,Department of Economics and Law, Sapienza University of Rome, Via del Castro Laurenziano 9, Rome 00161, Italy","Received 4 July 2019, Revised 28 October 2019, Accepted 6 January 2020, Available online 9 January 2020, Version of Record 25 January 2020.",https://doi.org/10.1016/j.jmacro.2020.103189,Cited by (13),"A renewed interest in the link between business cycle and ==== revenues has recently emerged, especially during economic crises. In this paper, we provide an empirical analysis on 35 ==== framework, changes in governments’ tax policies, budgetary and political variables possibly affecting how taxes react to GDP fluctuations. By adopting the dynamic common correlated effects estimator, we find that both short- and long-run tax responses are lower than those reported in previous cross-country studies. We suggest that this slightly lower than expected reaction of tax revenue can be interpreted as a reduced power of both automatic stabilization in the short-run and fiscal sustainability in the long-run. Results are robust to possible endogeneity issues between tax revenues and business cycles.","In recent years, international organizations, national governments, and academics have shown a renewed interest in the way in which tax revenues react to the business cycle, especially during economic crises (Arnold et al., 2011; Acosta-Ormaechea and Yoo, 2012; Gemmell et al., 2014; Baiardi et al., 2018; Aizenman et al., 2019). Measuring this reaction is crucial both for monitoring and forecasting governments’ public finances, as it helps to predict how to implement stabilization policies in response to expansionary or contractionary stages of the economic cycle (Blanchard and Perotti, 2002; Arachi et al., 2015; Lagravinese et al., 2018), and for the general sustainability of public finances. This issue has even greater importance in the presence of financial stress – as in the last decade – where the tax revenue management has been affected by a certain degree of uncertainty and instability. In this context, how taxpayers respond either to tax incentives or to discretionary tax changes might be unpredictable (Mourre and Princen, 2019), especially when the monetary policy has limited room for country-specific macroeconomic policies as for most European countries in the Euro area.====Since the first contribution on the elasticity of tax revenues (Groves and Kahn, 1952), the literature has gradually enriched over the years. For example, many studies have estimated the elasticity of single taxes (e.g. Huton and Lambert, 1980; Fries et al., 1982; Clausing, 2007), while other contributions have provided either case-study analyses (Sobel and Holcombe, 1996; Dye 2004; Bruce et al., 2006; Wolswijk 2009; Poghosyan, 2011; Koester and Prismeier, 2012; Havranek et al., 2016; Lagravinese et al., 2018) or comparative analyses of tax buoyancy and tax elasticity using different samples of countries (Sancak et al., 2010; Brückner, 2012; Fricke and Sussmuth, 2014; Belinga et al., 2014; Dudine and Jalles, 2018; Boschi and d'Addona, 2019; Mourre and Princen, 2019).====Over time, the econometric methodologies have also been refined, moving from the Ordinary Least Squares (OLS) and Dynamic OLS (DOLS) estimators to more sophisticated econometric techniques that take into account the presence of cross-sectional dependence among panel units (Pesaran, 2006; Chudik and Pesaran, 2015). This is a crucial point for fiscal policies, which are often influenced by supranational constraints (e.g., the European rules on public budget). For this reason, our analysis is carried out using a new panel data method that takes into account unobserved heterogeneity, temporal persistence, and cross-sectional dependence. In particular, in the presence of cross-sectional dependence in the data, the output obtained with the fixed effect estimator cannot be relied upon.====Most importantly, when addressing the tax revenues reaction to the economic cycle, an important distinction between the concepts of tax buoyancy and tax elasticity should be made. Tax buoyancy is a measure of how taxes react to economic growth, without disentangling the impact of discretionary and automatic tax changes. Tax elasticity, instead, is a measure of the reaction of taxes due to the built-in flexibility of the tax system, which disregards the impact of discretionary tax changes.====Compared to the previous studies, our paper concentrates exclusively on the tax buoyancy of 35 OECD countries during the period 1995–2016. By focusing on the tax buoyancy, instead of tax elasticity, we are able to capture the reaction of both structural characteristics and discretionary tax policies to GDP changes, not least because of the debatable methods by which discretionary changes are isolated in the common practice. Indeed, estimating tax elasticities through a regression of tax changes on tax bases may fail to adequately disentangle automatic and discretionary changes, thus being not able to assess their distinctive impacts (Sen, 2006).====The time interval used in this paper is also particularly important as it crosses relevant institutional changes and economic cycles for the countries in our sample. In particular, we extend previous works that are normally based on a time span that ends at years 2013 or 2014 (Sancak et al., 2010; Belinga et al., 2014; Boschi and d'Addona, 2019; Dudine and Jalles, 2018), by adding further empirical evidence capturing the impact of the recent Great Recession on both short- and long-run responses of taxes to GDP changes.====On the institutional side, all OECD countries belonging to the European Union (EU), since 1992, have experienced tighter budget constraints and fiscal discipline, which might have conditioned the response of taxes compared to those OECD countries that do not belong to the EU. As for the economic cycle, our period includes both a severe recession and a recovery. This element is particularly important for measuring both short- and long-run tax reactions. In doing so, we also exploit both the heterogeneity of tax responses across countries and the different intensity of recessions in different countries.====From the econometric side, by limiting the analysis to OECD countries that share similar institutional systems and economic development, one can expect either weak or strong forms of cross-sectional correlation that, if ignored, may lead to biased estimates (Pesaran 2006; Pesaran and Tosetti, 2011; Everaert and De Groote, 2016). To address this issue, we adopt the Dynamic Common Correlated Effects estimator (DCCE) as developed by Ditzen (2018) to correct for small sample time-series bias (Chudik and Pesaran, 2015). The DCCE controls for dependence by adding cross-sectional means and lags (Everaert and De Groote, 2016; Pesaran and Tosetti, 2011) and by testing, at the same time, for cross-sectional dependence in the error terms.====The main results of the paper show a significant mitigation of the levels of tax buoyancy over the period investigated both in the short- and the long-run, with respect to estimates provided by previous studies. Such divergence can be mostly attributed to the different time span included in the analysis and to the novel use of the dynamic estimator in calculating short- and long-run tax responses. In detail, the short-run coefficients for total revenue display a slightly lower reaction to the economic cycle than expected for a good automatic stabilizer. Nevertheless, this average response is the outcome of a significant variability of the short-run buoyancies of specific tax items. On the other hand, when considering long-run tax buoyancies, all coefficients are significantly lower than 1. Since long-run responses would give information on the sustainability of fiscal policies, they reveal that both the aggregate of total taxes and single tax items could not guarantee fiscal sustainability in the long-run, as they do not seem to converge to a long-run equilibrium in the period analysed.====When considering the macroeconomic conditions, we note that inflation and unemployment contribute to foster tax buoyancy at least in the short-run, while they do not significantly affect estimates of long-run tax buoyancy. This would suggest that while the short-run tax reaction is impaired by automatic sources, the long-run response could be mostly affected by the combination of both automatic and discretionary changes in governments’ policy. Most interestingly, when taking into account the role of country- specific economic downturns occurred during the observed period, short- and long-run tax buoyancies are lower than those obtained with the baseline results.====Robustness checks are performed by considering changes in governments’ tax policy, and the presence of European institutions that could differently impact on the tax system of the selected countries and, eventually, on the tax reactions to the economic cycle. Finally, other robustness checks concern the use of instrumental variables to take into account for possible endogeneity issues and the use of fixed effects only to test the stability and the accuracy of our dynamic estimator, also compared to previous studies mostly based on the former.====The rest of the paper is organized as follows. Section 2 describes the importance of tax buoyancy approach used in this paper. Section 3 presents the methodological framework. Section 4 shows the main results for the long and short-run tax buoyancies, also reporting country-specific coefficients, and some robustness checks. Section 5 provides additional evidence taking into account the role of budgetary parameters, shadow economy and political variables possibly affecting how taxes react to GDP changes. Section 6 briefly concludes.",Tax buoyancy in OECD countries: New empirical evidence,https://www.sciencedirect.com/science/article/pii/S0164070419302836,9 January 2020,2020,Research Article,207.0
"McKnight Stephen,Mihailov Alexander,Pompa Rangel Antonio","Centro de Estudios Económicos, El Colegio de México, Camino al Ajusco 20, Col. Pedregal de Santa Teresa, Mexico City, 10740, Mexico,Department of Economics, University of Reading, Whiteknights, PO Box 218, Reading, RG6 6AA, United Kingdom,Dirección General de Investigación Económica, Banco de México, Calle 5 de Mayo 18, Col. Centro, Mexico City, 06069, Mexico","Received 9 November 2018, Revised 18 December 2019, Accepted 20 December 2019, Available online 27 December 2019, Version of Record 7 January 2020.",https://doi.org/10.1016/j.jmacro.2019.103188,Cited by (1),"This paper employs ==== estimation to uncover the central bank preferences of the five Latin American inflation targeting countries with floating exchange rates: Brazil, Chile, Colombia, Mexico, and Peru. The target weights of each country’s central bank loss function are estimated using a medium-scale ==== ","For many central banks in both developed and developing countries, inflation targeting (IT) has become the operational monetary framework of choice to achieve price stability.==== According to the International Monetary Fund (IMF) (see, e.g., Jahan, 2012), since the adoption of IT by New Zealand in December 1989, there are now 28 IT central banks worldwide, of which 6 currently originate from Latin America: Brazil, Chile, and Colombia all adopted IT in 1999, shortly followed by Mexico (2001), Peru (2002), and Guatemala (2005) (see Table 1).==== While there is some empirical evidence to suggest that IT has been successful in reducing inflation in developing countries (see, e.g., Batini, Laxton, 2007, Goncalves, Salles, 2008, Lin, Ye, 2009, Lee, 2011),==== little is known about the policy preferences of central banks operating in these countries.==== As discussed by Castelnuovo and Surico (2004) and Ilbas, 2010, Ilbas, 2012, such information can help in evaluating the performance of central banks, as well as improving our understanding of monetary policy actions and its effects on the formation of expectations by private agents.====The aim of this paper is to use Bayesian estimation techniques to uncover and compare the central bank preferences of the five Latin American inflation targeting (LAIT) countries operating under a floating exchange-rate regime.==== Since the IT framework can be considered as “constrained discretion” (Bernanke and Mishkin, 1997), we assume that in each country monetary policy is conducted under optimal discretion (Dennis, 2007). Each central bank is assumed to optimally set the nominal interest rate by minimizing a quadratic loss function that includes four specific policy objectives: price stability via control of inflation, stabilizing the output gap, reducing real exchange rate variability, and nominal interest rate smoothing. The weight attributed to each policy objective will depend on the institutional preferences of each central bank, which we can make inferences about using estimates of the respective Bayesian posterior distributions.====The structural model used to represent the LAIT economies is a dynamic medium-scale small open economy New Keynesian model. Following the modeling frameworks of Monacelli (2005), Kam et al. (2009), and Justiniano and Preston (2010), we allow for imperfect exchange-rate pass-through (ERPT) such that the law of one price fails to hold. However, in similar set-ups the literature has assumed that international asset markets are either complete, e.g., as in Galí and Monacelli (2005), Monacelli (2005), and Kam et al. (2009), or incomplete, e.g., as in Adolfson et al. (2007) and Justiniano and Preston (2010). Instead, we consider both international asset market structures and use the Bayes factor to compare between them.==== Using the popular Random-Walk Metropolis-Hastings Markov Chain Monte Carlo algorithm, we present posterior estimates and convergence diagnostics for both the structural parameters and the persistence and standard deviations of the shocks we consider as most important for the LAIT economies, including cost-push shocks to the prices of imported and domestic goods, and shocks to the risk premium, the terms of trade, and technology.====Our main findings are as follows. First, we find that for the LAITs in our sample the assumption of complete asset markets is unambiguously preferred to the alternative of incomplete asset markets, as commonly but simplistically modeled by a riskless bond-only financial system. The superiority of the marginal likelihoods under complete asset markets is always sufficiently large to suggest strong statistical evidence in favor of complete markets over the incomplete asset markets version.====Second, all five central banks are strongly concerned about stabilizing inflation and smoothing the nominal interest rate. In particular, relative to the weight of inflation stabilization, we find that Chile and Peru place very high weights on interest rate smoothing. Third, there is significant heterogeneity amongst the five central banks concerning the target weights of output gap stabilization and real exchange rate stabilization. Our analysis suggests that Mexico and Peru show little concern for the stabilization of the output gap, whereas Brazil, Chile, and Colombia assign sizable weights. While all the LAIT central banks except that of Peru are concerned about real exchange rate stabilization, only Brazil and Chile are found to assign a sizable weight to minimizing real exchange rate fluctuations.====Overall, Peru and, less so, Mexico show evidence of implementing a strict inflation targeting regime, whereas Brazil, Chile, and Colombia appear much more flexible in terms of their macroeconomic policy objectives. Our estimated preference weights for the four policy objectives are shown to be broadly consistent with regard to the respective legal mandates of the five LAIT central banks.====In terms of the estimated key structural parameters influencing the endogenous propagation mechanism of the model, we find that these are statistically reliable, economically plausible, and broadly comparable to analogous estimates for other countries available in the literature using non-Bayesian econometric methods. For example, the estimated elasticity of substitution between home and foreign goods is within the typical range reported in Corsetti et al. (2008). Further, the estimated inverse Frisch elasticity of labor supply is also clustered tightly for all five LAITs in a typical range for other economies. With the exception of Chile, our estimates also reveal lower habit persistence than that for advanced economies, including those reported by Kam et al. (2009).====In terms of the sources of exogenous fluctuations affecting the five LAIT economies, our results indicate a high relative volatility (measured by the posterior mean standard deviation) for cost-push shocks to both imported and domestic goods, terms of trade shocks and, less so, productivity shocks, dominating in most of the LAIT economies. For four of our five LAIT economies, risk premium shocks are estimated to be the most persistent, whereas terms of trade shocks are estimated to be the least persistent.====There are few papers that have used Bayesian techniques to estimate central bank preferences in an open-economy setting.====Lubik and Schorfheide (2007) find, examining four advanced IT economies, that the central banks of Canada and the UK react to the nominal exchange rate in the monetary policy rule, whereas the central banks of Australia and New Zealand do not. Kam et al. (2009) estimate the central bank preferences for three developed IT countries, Australia, Canada, and New Zealand under optimal discretionary monetary policy. They find that the central banks of these countries all have very similar preferences: the highest priority is inflation stabilization, followed by interest rate smoothing, with no concern for stabilizing the output gap (with the exception of Australia) and the real exchange rate. Palma and Portugal (2014) estimate the model of Kam et al. (2009) using Brazilian data. They find that the major concern of the central bank of Brazil is inflation stabilization, followed by interest rate smoothing, real exchange rate stabilization, and output gap stabilization. While the estimation approach adopted in this paper is similar to Kam et al. (2009), who assume complete asset markets, we additionally examine the model fit of incomplete asset markets. Moreover, Rabanal and Tuesta (2010) show that the extent of international financial market integration affects both the Bayesian estimates of the parameters and the transmission mechanism of shocks. Our analysis emphasizes the implications for Bayesian estimation of using different international asset markets assumptions in fitting the data, particularly when uncovering central bank preferences.====The paper is organized as follows. Section 2 outlines the theoretical model. Section 3 describes the data and explains the estimation strategy. Section 4 reports our main results. Finally, Section 5 concludes.",What do Latin American inflation targeters care about? A comparative Bayesian estimation of central bank preferences,https://www.sciencedirect.com/science/article/pii/S0164070418304737,27 December 2019,2019,Research Article,208.0
"Caraiani Petre,Gupta Rangan","Institute for Economic Forecasting, Romanian Academy, Bucharest University of Economic Studies, Calea 13 Septembrie no. 13, Casa Academiei, Bucharest, Romania,Department of Economics Pretoria 0002, University of Pretoria, South Africa","Received 23 May 2019, Revised 20 August 2019, Accepted 12 December 2019, Available online 19 December 2019, Version of Record 24 December 2019.",https://doi.org/10.1016/j.jmacro.2019.103187,Cited by (0),"In this paper, we estimate a ","The United Kingdom (UK), just like the United States (US), is a major player in the world economy, with monetary policy decisions of the Bank of England (BoE) being of interest to both academicians and financial markets. Given this, what variables determine the interest rate-setting behaviour of the BoE is, understandably, an important question. While the role of the output-gap and inflation rate in determining the policy rate of the BoE, and central banks across the world, is well-accepted along the lines of the Taylor-rule (Taylor (1993)), whether information in exchange rate movements should also be accounted for remains a debatable issue.====UK is a natural resource exporter, and hence, domestic business cycle fluctuations are likely to have substantial international relative price components. In addition, monetary policy is partly transmitted to the real economy through its effect on the exchange rate. The BoE therefore may have a specific interest in explicitly reacting to and smoothing exchange rate movements as a predictor of domestic volatility. However, based on various alternative econometric approaches (for example, single-equation interest rate rules, structural vector autoregressions (SVARs), Small Open Economy Dynamic Stochastic General Equilibrium (SOEDSGE) models), evidence regarding that the BoE responds to (nominal) exchange rate movements is mixed (see for example, Lubik, Schorfheide, 2007, Dong, 2013, Bjornland, Halvorsen, 2014).====Low frequency movements in exchange rates are likely to be tied with fundamentals more than high-frequency movements of the same, which in turn could be associated with speculation, and hence (harder to predict) random behaviour ((Rapach, Wohar, 2002, Balke, Ma, Wohar, 2013, Caraiani, 2017). Given this, it is possible that central bankers find it more comfortable to respond to long-term (i.e., low-frequency) movements of the exchange rate rather than its corresponding short-term fluctuations. With this hypothesis in mind, the objective of this paper is to revisit the question of whether the BoE respond to exchange rate movements, with us now analyzing not only the aggregate nominal effective exchange rate depreciations, but also its various frequency components. Given the well-known econometric issues associated with single-equation rule-type and atheoretical VAR approaches in light of the Lucas Critique (Lucas, 1976), we estimate the SOEDSGE model of Lubik and Schorfheide (2007) for the UK to provide an answer to our question, over the period of 1986:Q1 to 2018:Q1. While, closed-economy frequency-based models for the US economy has been estimated before (see, Caraiani, 2015 for a detailed discussion in this regard), to the best of our knowledge, this is the first attempt to estimate a SOEDSGE model in both time and frequency-domains to determine whether the BOE’s response to exchange rate movements is contingent on its frequency components.====The remainder of the paper is organized as follows: Section 2 lays out the basics of the SOEDSGE and the frequency decomposition of the data using wavelet, Section 3 presents the data and results, with Section 4 concluding the paper.",Is the response of the bank of England to exchange rate movements frequency-dependent?,https://www.sciencedirect.com/science/article/pii/S0164070419302344,19 December 2019,2019,Research Article,209.0
"Miller Stephen Matteo,Ndhlela Thandinkosi","Mercatus Center, George Mason University, 3434 Washington Blvd., 4th Floor, Arlington, VA 22201, United States,Monash University Wellington Road Victoria 3145 Australia","Received 9 February 2019, Revised 11 December 2019, Accepted 11 December 2019, Available online 12 December 2019, Version of Record 19 December 2019.",https://doi.org/10.1016/j.jmacro.2019.103186,Cited by (5),"The delayed end to Zimbabwe's hyperinflation in 2009 gave rise to an official dollarization. Before then, the Reserve Bank of Zimbabwe (RBZ) operated on the correct side of the ","Zimbabweans once again face the daunting prospects of living with hyperinflation.==== Reflecting the fiscal dominance of monetary policy, Zimbabwe now like during the run-up to the hyperinflation that ended in 2009 has faced high budget deficits such that monetary policy serves fiscal objectives. Yet, few studies of the earlier hyperinflation exist. Munoz (2007), Kairiza (2009) and Mandishara and Mupamadzi (2016) detail how the Reserve Bank of Zimbabwe (RBZ) fueled the hyperinflation by expanding quasi-fiscal activities (QFAs)—essentially activities such as expenditures, subsidies or taxes that a fiscal authority such as the Ministry of Finance could undertake. McIndoe-Calder (2018) shows that the regime turned increasingly to seignorage as other sources of government revenues dried up, and finds evidence consistent with the so-called Cagan's ""Paradox"" (see Cagan (1956)), suggesting that over the long-term the RBZ operated on the wrong side of the inflation tax Laffer curve. Still, the Zimbabwean hyperinflation ended with an unusual official dollarization, such that the regime outlived the currency, when the opposite typically happens (see Paldam (1994) and Bernholz (2003)).====The official dollarization followed the widespread unofficial dollarization, through currency substitution, and the diminished note-printing capacity of the RBZ after July 1, 2008, when note paper shipments from Germany officially ceased.==== Using parameter estimates for Zimbabwe, Miller (2016) shows how a delayed end might arise by deriving a Hotelling (1931) type optimal stopping time rule to extract all remaining seignorage before abandoning the currency. In that sense, the delayed end to Zimbabwe's hyperinflation may have had more in common with the Revolutionary French hyperinflation (see Sargent and Velde (1995) and White (1995)), as that regime also outlived the currency by abandoning it in favor of specie. In what follows, we show that in the run up to the hyperinflation's end, amidst deteriorating institutional and political conditions, the regime turned increasingly to seignorage and the RBZ appeared to operate on the correct side of the inflation tax Laffer curve.====To measure inflation, we apply relative purchasing power parity (PPP) to compute changes in the imputed parallel market rate implied by share prices of the Old Mutual insurance company, whose shares trades in London, England and Harare, Zimbabwe. To measure real balances, we deflate both the monetary base and currency in circulation by the Old Mutual parallel market rate (OM rate). Our sample extends from October 1999 through March 2008, as the daily Old Mutual series was sparsely reported after the first week of April 2008, although the RBZ's access to note paper ended just three months later.====Using the OM rate of inflation, we show that seignorage generally rises throughout the sample as the institutional environment deteriorated, which is consistent with findings in Aisen and Veiga (2008). We measure institutional deterioration using changes in contract intensive money, proposed by Clague et al. (1999), and changes in the PRS Group's political risk index. Given the upward trend in seignorage, we also examine the efficacy of the RBZ's seignorage generation based on the seignorage maximizing rate suggested by Cagan (1956). That approach calls for estimating the semi-elasticity of some measure of the (natural log of) real balances with respect to some measure of inflation. The negative inverse of the semi-elasticity equals the seignorage maximizing rate. Since Cagan (1956), studies tend paradoxically to find that the magnitude of the semi-elasticity is too small (typically less than -1), such that measured inflation exceeds the seignorage maximizing rate. However, Mladenovic and Petrovic (2010) find that inflation rates derived from black market exchange rates lay below the seignorage maximizing rate for much of the last 7 months of Serbia's experience in the early 1990s. Similarly, our semi-elasticities suggest that the RBZ operated on the correct side of the inflation tax Laffer curve through March 2008, as inflation rates lie below all estimated seignorage maximizing rates.====To obtain our estimates, we start with Mladenovic and Petrovic's (2010) approach to estimating the demand for money. The multivariate cointegation technique used in their study provides estimates of a stable long-run equilibrium relationship between real balances and inflation during the Serbian hyperinflation in the early 1990s. Their specifications relate real balances, defined as currency in circulation, to inflation by combining the specification proposed by Cagan (1956) with the monetary model of the exchange rate (MMER) proposed by Frenkel (1976). The MMER in turn relies on PPP, which relates the exchange rate between two countries’ currencies with the ratio of their price levels. Intuitively, if exchange rate determination happens in the money market as the MMER suggests, holding the foreign country's price level fixed, a country that experiences currency depreciation should subsequently experience rising prices. Accordingly, during a hyperinflation, assuming the foreign country experiences ordinary rates of inflation, the rate of exchange rate depreciation provides an estimate of the domestic inflation rate.====Michael et al. (1994) observe that a hyperinflation may be comprised of more than one phase, soMladenovic and Petrovic (2010) use the last 7 months of daily data to examine the last stage of the Serbian hyperinflation. We take a similar approach and use the available monthly data surrounding former RBZ Governor Gideon Gono's tenure from late 2003 onward through the end of our sample in March 2008, given that the hyperinflation was associated with his tenure. We do so for three reasons.====First, in applying the method proposed by Bai and Perron (1998, 2003) for detecting structural breaks to the full sample, we find three structural breaks for each measure of real balances. These include one for real currency in circulation in September 2003, and another for the real monetary base in November 2003. Second, we also apply the non-stationarity tests proposed by Lee and Strazicich (2003, 2013), which can incorporate up to two structural breaks, and their method identifies similar break dates. Third, we use the Del Negro and Primiceri (2015) correction of the Bayesian, time-varying parameter, structural vector autoregression (TVP-SVAR) approach proposed by Primiceri (2005) to estimate the dynamic semi-elasticities generated from impulse response functions. The approach requires selecting a pre-estimation period training sample. Primiceri (2005) uses a training sample of 40 quarters, while we use a training sample of 42 months (from April 2000-September 2003), although we find that using smaller or larger training sample sizes may not affect the results much. We use this method instead of the standard multivariate cointegration methods used since Taylor, (1991) to examine money demand under hyperinflation because the (Lee and Strazicich, 2003, 2013) non-stationarity tests indicate that the orders of integration differ for our inflation and real balances series.====Aside from accounting for the potentially changing structural relationship between the series used during the sample, Primiceri's Bayesian TVP-SVAR model allows for stochastic volatility. However, for the parameter values that generate estimates that converge, we observe little variation in the residual standard deviations for the OM rate of inflation, as well as our real balances series, which suggests that stochastic volatility was not a key characteristic of the hyperinflation.====Impulse response functions generated from the Bayesian TVP-SVARs estimated for 18 monetary base specifications and 9 currency in circulation specifications suggest that inflation lay well below the seignorage maximizing rates throughout the sample. We compute the seignorage maximizing rates by inverting the sum of the first two median semi-elasticities, as they are economically significant for all 27 specifications and the 95th percentiles lie below zero for the first two steps and lie above zero for the last two steps of the impulse response functions. We get similar results when using the average semi-elasticities. For the monetary base, the sum of the first two median semi-elasticities range from −0.407 to −0.337, implying seignorage maximizing rates that range from 246% to 297%, which exceed the sample maximum monthly rate of inflation of 135%. For currency in circulation, the sum of the first two median semi-elasticities range from −0.413 to −0.318, implying seignorage maximizing rates that range from 242% to 315%. We describe and examine the statistical properties of our data, report our estimates of the seignorage maximizing rate, and discuss our results before concluding.",Money demand and seignorage maximization before the end of the Zimbabwean dollar,https://www.sciencedirect.com/science/article/pii/S0164070419300539,12 December 2019,2019,Research Article,210.0
"Qureshi Irfan,Liaqat Zara","Economic Research and Regional Cooperation Department (ERCD), Asian Development Bank, 6 ADB Avenue, Metro Manila, Philippines,Department of Economics, University of Waterloo, Hagey Hall 162, Canada","Received 22 May 2019, Revised 26 November 2019, Accepted 30 November 2019, Available online 3 December 2019, Version of Record 18 December 2019.",https://doi.org/10.1016/j.jmacro.2019.103184,Cited by (18)," threshold level in the relationship between public debt and economic growth across countries. Savings and investment are the primary channels through which external debt impacts economic growth. These results are robust to various model specifications, additional controls, and identifying restrictions.","The rise in external debt in many countries has invigorated a debate about the costs of escalating public and private debt. High and unsustainable levels of external debt can be especially risky for developing countries, exposing them to exchange rate fluctuations, sudden-stops in capital flows and sharp capital outflows, which may precipitate into a banking or currency crisis (Hemming et al., 2003).==== Therefore, governments and policymakers around the world have become increasingly apprehensive about the short and long run effects external debt may have on growth, raising a set of testable policy questions: what are the macroeconomic effects on longer-term growth of high external debt? Are these effects conditional on the ==== (or types) of external debt and growth, as well as on the income level of countries? Finally, what are the channels through which external debt works to affect growth?====Our study examines the relationship between the types of external debt (total, public, and private external debt) and income growth by estimating a panel vector autoregression (PVAR) model using data for 123 countries from 1990 to 2015. In addition to the difference in estimation strategy used in the previous literature, we examine the macroeconomic impact of types of external debt, which may have divergent implications for growth. Furthermore, it is natural to expect that the impact of external debt varies across countries due to the difference in their income levels, institutional development, fiscal framework and degree of openness. These extensions enable us to present novel empirical findings that are more granular than those offered by the previous literature.====Our empirical results reveal several key insights. Total external debt appears to have a negative effect on GDP growth in the aggregate data. This result especially holds for the sample of low-income countries. On the other hand, external debt is positively associated with income growth in the middle-income groups. While public external debt lowers output growth for most countries, there is no obvious effect of private external debt on income. These results are derived after controlling for a set of relevant endogenous variables in our estimation and are robust to various model specifications. We also find that the effect of a higher GDP growth on total and public external debt is visibly negative for most countries. Our analysis across multiple debt windows confirms the existence of a non-linear effect on growth. Interestingly, we detect no evidence for a ==== threshold level in the relationship between public external debt and economic growth once we account for the impact of global factors and their spillover effects. Finally, we pinpoint savings and investment as the primary channels through which external debt is likely to have an impact on economic growth.====The impact of public debt on growth has been the topic of various studies (see, for instance, Reinhart and Rogoff (2010), and Eberhardt and Presbitero (2015)). Westphal and Rother (2012) detect a non-linear relationship between income growth and public debt in the Euro area. Égert (2015) employ a bivariate regression model to determine the threshold level for central government debt. Although the results are dependent on the country groups and time period under consideration, the analysis concludes that the detrimental effect of debt arises at debt levels as low as 20% for some country groups, while at 60% debt-to-GDP ratio for others. Panizza and Presbitero (2014), Cecchetti et al. (2011) and Casares (2015) all highlight the relationship between public debt and growth.====Among the very few studies underlining the negative impact of external debt on income growth, Patillo et al. (2002, 2004) are perhaps the most prominent, although their findings are based exclusively on developing countries. Schclarek (2004) suggests that the negative relationship between total external debt levels and growth rates is primarily driven by public external debt. By implementing a system of generalized method of moments (GMM) dynamic panel econometric technique, they examine the channels through which this link may manifest itself. Along with extending and updating the data used in these studies, we pay close attention to improving the estimation techniques employed in the existing analyses of external debt and income growth nexus through estimation of a panel vector autoregression model. Using frontier econometric techniques, we methodically investigate the transmission of idiosyncratic shocks to external debt across countries and over time by generating impulse response functions (IRFs).====The inclusion of several controls in our baseline estimation as well as under robustness checks helps shed more light on the specific channels through which external debt affects growth. Our primary result, which highlights savings and investment as the primary channels through which external debt is likely to have an impact on economic growth, connects with a broad empirical literature. Schclarek (2004), Kumar and Woo (2010), and Westphal and Rother (2012) analyze empirically the channels through which external debt can potentially affect economic growth. The former study finds that the main channel is private capital accumulation. However, this relationship holds only for emerging economies, while there is some supporting evidence for the channel of private savings for advanced economies. Kumar and Woo (2010) find evidence in favor of the investment channel for advanced economies. Our results are especially remarkable from a methodological perspective as well, since the previous literature does not fully account for endogenous interactions amongst factors influencing the growth of external debt.====Panel VAR models have been used to inspect multivariate time-series for panel data and in the context of a variety of macroeconomic inquiries.==== PVARs are often used to construct coincident or leading indicators of economic activity (Canova and Ciccarelli 2009), or to evaluate the macroeconomic effects of unconventional monetary policies (Gambacorta et al., 2014). Based on a large annual dataset on 22 OECD countries over the period 1987–2009, Boubtane et al. (2012) empirically examine the interaction between immigration and host country economic conditions to test how immigration shocks are transmitted in a variety of countries. Another example is the study by Love and Zicchino (2006) which attempts to measure the effect of shocks to financial factors on a cross-section of U.S. firms. In a recent study, Liaqat (2019) employs a dynamic PVAR approach to assess the adverse effect of public debt on the growth of capital formation in a large group of countries. Lof and Malinen (2014) estimate a PVAR to analyze the relationship between debt and growth and observe that while a rise in income growth has had a negative effect on debt, there is an insignificant long-run reverse impact of debt on growth.====Our interest in the use of PVARs is particularly motivated by our emphasis on uncovering the extent of the dynamic heterogeneity in the effect of external debt, and thus, to endogenously group economies in order to characterize their differences. Since PVARs allow for interdependencies in testing whether feedbacks are generalized or involve only certain groups of countries, it is an exceptionally useful empirical technique for the analysis at hand. PVARs have been commonly used to construct average effects across heterogeneous groups of units (Canova and Ciccarelli 2013). As discussed later in the paper, we use this approach to distinguish between the average effects of private and public external debt across countries belonging to different income categories.====The rest of the paper is organized as follows. The empirical methodology as well as a description of the data used is provided in Section 2. Section 3 discusses the baseline estimation results. A discussion of various extensions to the baseline model, and robustness checks are presented in Sections 4 and 5. Finally, Section 6 concludes.",The long-term consequences of external debt: Revisiting the evidence and inspecting the mechanism using panel VARs,https://www.sciencedirect.com/science/article/pii/S0164070419302332,3 December 2019,2019,Research Article,211.0
Ioannidis John P.A.,"Centre for Environmental Science and Data Science Institute, Hasselt University, Martelarenlaan 42, 3500 Hasselt, Belgium,Department of Economics, University of Goettingen, Humboldtallee 3, 37073 Goettingen, Germany,Departments of Medicine, Health Research and Policy, and Statistics, and Meta-Research Innovation Center at Stanford, Stanford University, 1265 Welch Rd, Medical School Office Building Room X306, Stanford, CA 94305, USA","Received 6 February 2019, Revised 1 December 2019, Accepted 2 December 2019, Available online 3 December 2019, Version of Record 16 December 2019.",https://doi.org/10.1016/j.jmacro.2019.103185,Cited by (15),Almost all studies that use ,"A large empirical literature estimates the determinants of cross-country growth differences. Recent studies focus on Bayesian model averaging (BMA) to address the substantial model uncertainty inherent to growth regressions (e.g. Fernandez, Ley, Steel, 2001, Sala-i-Martin, Doppelhofer, Miller, 2004, Durlauf, Kourtellos, Tan, 2008, Masanjala, Papageorgiou, 2008, Ciccone, Jarocinski, 2010, Eicher, Papageorgiou, Raftery, 2011, Feldkircher, Zeugner, 2012, Rockey, Temple, 2016). These studies focus their analysis on identifying growth determinants either for the growth period 1960–1992 or the growth period 1960–1996. Using a new data set, we provide the first systematic assessment of the stability of inferences on growth determinants across growth periods. Our findings indicate that inferences are unstable across growth periods, but determinants related to demography and education tend to become important in recent growth periods.====Many variables have been suggested to determine economic growth. Durlauf et al. (2005) tabulate 145 growth determinants that have been discussed in the literature with the vast majority of them being statistically significant at least in one study. However, many of these variables may appear statistically significant due to specification searching. Hendry (1980) and Leamer (1983) have pointed to the flexibility of econometric research and the wide range of estimates that can be obtained for a given coefficient of interest. Estimates that appear to be insignificant in the research process may remain unreported, while estimates that are statistically significant may be selected for presentation in an article. This specification searching and selective reporting is the response of researchers to the incentive system of academic publishing as discussed by Ioannidis (2005) and Glaeser (2011) and it may be particularly prevalent if model uncertainty is large with little consensus about what control variables should be included in the regression model.====Recent empirical evidence indicates the prevalence of specification searching in economics. For the top economics journals, Brodeur et al. (2016) find a lack of ====-values just above the significance thresholds of 0.05 - 0.10 and conclude that researchers may have preferred (marginally) significant ====-values resulting in an abundance of ====-values just below the significance threshold of 0.05. This indicates that authors may have searched across a variety of specifications to turn their marginally insignificant estimates into statistically significant findings. Similarly, Bruns et al. (2019) and Vivalt (2019) find evidence for discontinuities in the distribution of published ====-values, with marginally significant ====-values being overrepresented compared to marginally insignificant ====-values. Ioannidis et al. (2017) show for 159 research fields in economics with more than 60,000 estimates that power is low in many economics studies while the point estimates of these studies are frequently statistically significant. This finding indicates that authors with sample sizes that are actually too small to reliably detect the effect of interest may engage in specification searching to exaggerate the point estimates in order to achieve statistical significance.====These findings demonstrate the need for methods that reduce some of the flexibility of econometric research by basing inference on the coefficient of interest on a large set of models rather than on a single model or a small selective set of models. The growth literature put very early emphasis on model uncertainty to improve credibility and reliability of growth regressions. Prominently, Levine and Renelt (1992) and Sala-i-Martin (1997) assess the robustness of growth determinants with respect to different sets of control variables based on Extreme Bounds Analysis (Leamer, 1983, Leamer, Leonard, 1983). Fernandez et al. (2001b) and Sala-i-Martin et al. (2004) pioneer the use of BMA to identify robust growth determinants. Many studies followed in this vein including studies that analyze the jointness of growth determinants (Doppelhofer, Weeks, 2009, Ley, Steel, 2007), the relative importance of alternative growth theories (Durlauf et al., 2008), the specific growth determinants for African countries (Masanjala and Papageorgiou, 2008), and studies with a more methodological focus (e.g. Ley, Steel, 2009, Eicher, Papageorgiou, Raftery, 2011).====However, the priors on model coefficients used in these studies have been shown to result in fragile inferences due to minor changes in international income data leading to minor changes in the dependent variable. Ciccone and Jarocinski (2010) show that posterior inclusion probabilities (PIPs) are sensitive to different vintages of the Penn World Table (PWT). The PIP is a measure of whether models including the variable under consideration account for a high proportion of the posterior mass (over the model space). This finding has stimulated a discussion how BMA can be made more robust. Feldkircher and Zeugner (2009) propose the use of priors on model coefficients that allow for data-adaptive shrinkage and Feldkircher and Zeugner (2012) show that the use of these priors makes the inference more robust to changes in international income data. Rockey and Temple (2016) demonstrate that the use of particular fixed regressors also improves the stability of inferences across different vintages of the PWT. However, both the use of priors on model coefficients that allow for data-adaptive shrinkage and the use of fixed regressors come to the conclusion that PIPs are evenly distributed across the potential growth determinants for the growth period 1960–1996. This means that it is difficult to identify some growth determinants to be more important than others. Feldkircher and Zeugner (2012) and Rockey and Temple (2016) conclude that BMA leads to robust ambiguity.====The contribution of this study is threefold. First, we provide a systematic assessment of the stability of inferences on growth determinants across growth periods. Prior studies are either based on the data by Sala-i-Martin et al. (2004) which analyses the growth period 1960–1996 or the data by Fernandez et al. (2001b) which is a subset of the data by Sala-i-Martin (1997) which analyses the growth period 1960–1992. We apply BMA to growth regressions using a rolling time window of 20 and 35 years to a dataset with 37 growth determinants between 1960 and 2010. We use the priors on model coefficients suggested by Feldkircher, Zeugner, 2009, Feldkircher, Zeugner, 2012 in combination with the fixed regressors suggested by Rockey and Temple (2016) to ensure that the inferences obtained by BMA are robust regarding noise in the dependent variable. We show that inferences on growth determinants with respect to PIPs are substantially unstable across growth periods. Second, we find support for robust ambiguity as suggested by Feldkircher and Zeugner (2012) and Rockey and Temple (2016) in the early growth periods but PIPs become less evenly distributed over time. Our analysis suggests that growth determinants can be identified in more recent growth periods. Particularly, determinants related to demography and education seem to matter most in recent growth periods. Education was also found to be an important growth determinant in Rockey and Temple (2016). We provide suggestive evidence that these changes in PIPs may represent substantive changes in what explains growth across the rolling time window rather than being merely caused by noise in the data or improvements in data quality in some specific variables over time. Third, determinants of 20 year growth periods differ from determinants of 35 year growth periods. While determinants related to demography, trade, investment and education matter for at least some of the growth periods of both 20 and 35 years, religion seems to matter only for growth periods of 20 years.====Section 2 provides an introduction to BMA, Section 3 introduces the estimation strategy, and Section 4 presents the data. Section 5 provides an overview of the results, Section 6 discusses the results and Section 7 concludes.",Determinants of economic growth: Different time different answer?,https://www.sciencedirect.com/science/article/pii/S0164070419300503,3 December 2019,2019,Research Article,212.0
"Dash Pradyumna,Rohit Abhishek Kumar,Devaguptapu Adviti","Department of Economics & Business Environment, Indian Institute of Management Raipur, Raipur, Chhattisgarh 493661, India,Department of Finance & Strategy, T A Pai Management Institute, Manipal, Karnataka 576104, India","Received 30 March 2019, Revised 18 November 2019, Accepted 27 November 2019, Available online 29 November 2019, Version of Record 16 December 2019.",https://doi.org/10.1016/j.jmacro.2019.103183,Cited by (6),Well-anchored ,"Well-anchored long-term inflation expectations are very important for the successful conduct of monetary policy in an inflation-targeting economy. Central banks often adopt an explicit numeral inflation target, partly to anchor long-term inflation expectations. If the central bank is credible, the target inflation should anchor the long-term inflation expectations. Accordingly, the long-term inflation expectations are expected to be determined only by target inflation, not by actual inflation, news, and/or short-term inflation expectations.====The (de-)anchoring of long-term inflation expectations in the US, the Euro area, and the emerging economies has been under debate since the sub-prime crisis. The empirical results are mixed, ranging from perfect anchoring to severe de-anchoring. For instance, in the context of the US, several studies have found the long-term inflation expectations to be perfectly anchored (Blanchard, 2016; Buono and Formai, 2018; Nautz et al., 2019; Strohsal et al., 2016), whereas a few studies have shown them to be de-anchored (Kumar et al., 2015; Nautz and Strohsal, 2015). Similarly, in the context of the Euro area, Scharnagl and Stapf (2015) have found that the medium- to long-term inflation expectations are well anchored, whereas Cruijsen and Demertzis (2011) have determined that inflation expectations are less well-anchored. Further, de Mendonça (2018) has shown that the inflation expectations in seven inflation-targeting emerging economies (Brazil, Chile, Colombia, Mexico, Poland, South Africa, and Turkey) are not perfectly anchored. This paper reexamines the debate by analyzing the evolution of (de-)anchorage of inflation expectations in the US from 1990 to 2019.====Our study contributes to the literature on inflation expectations anchorage in two ways. First, we analyze the (de-)anchorage of survey-based measures of households’ long-term inflation expectations. This is in contrast to most of the previous studies, which have analyzed either professional forecasters’ inflation expectations (Buono and Formai, 2018; Cruijsen and Demertzis, 2011; Lyziak and Paloviita, 2018) or market-based break-even inflation rates (Jochmann et al., 2010; Nautz et al., 2017; Strohsal et al., 2016; Strohsal and Winkelmann, 2015)====. Our approach is justifiable on twofold grounds. First, as per the New Keynesian Phillips Curve, inflation expectations of firms are highly relevant to capture their pricing decisions. However, the quantitative measure of this information is unavailable for many countries, including the US. Binder (2015) and Coibion and Gorodnichenko (2015) have provided evidence that in the absence of firms’ inflation expectations, households’ inflation expectations are a better proxy than professional forecasters’ inflation expectations in explaining actual inflation. Second, the market-based inflation rates are not very informative in forecasting inflation because they include not only the public's expectations of inflation but also the impact of other factors that affect prices. For instance, they are affected by liquidity risk, inflation risk, and animal spirits that are not related to expectations (Bauer and Rudebusch, 2015). Bauer and McCarthy (2015) have shown that the market-based measures do not provide useful forward-looking information about inflation. On the basis of the above two rationales, we deviate from the existing literature and ask whether households’ inflation expectations are well anchored in the US.====Our second contribution is identification of two different channels that may explain the evolution of (de-)anchorage of households’ long-term inflation expectations. First, change in perception of inflation may alter the de-anchorage of households’ long-term inflation expectations. The relevance of this argument can be seen in some recent surveys indicating that households are not well-informed about the objectives of the central bank, target inflation, and the official statistics of actual inflation (Kumar et al., 2015; Rowe, 2016).==== Further, several studies have shown the importance of frequently purchased goods and services, such as food and gasoline, in forming economy-wide perceptions of inflation (Georganas et al., 2014; Giovane and Sabbatini, 2008; Jonung, 1981; Kumar et al., 2015; Nam and Go, 2018). Under such conditions, the (de)-anchorage of long-term inflation expectations may depend on what households ==== about current inflation. In this paper, we use current year-on-year food away from home (FAFH) inflation as a measure of perception of inflation. The FAFH inflation has fluctuated around its long-term average (i.e., 2.7%) in our study period, but it has been below its long-term average since the global financial crisis. We hypothesize that the effects of an increase in inflation perception on degree of de-anchorage would be different in the periods when the inflation perception is below its average as compared to the periods when it is above. This is because in the low inflation perceptions environment, an increase in inflation perception means moving toward the long-term average value (i.e., contributing to re-anchorage). By contrast, in the high inflation perceptions environment, an increase in its value indicates moving away from its long-term average value (i.e., contributing to de-anchorage). Therefore, we expect a U-shaped relationship between perception of inflation and change in degree of de-anchorage. We examine this relationship by comparing the effects of the rise in perception of inflation on degree of de-anchorage across periods when inflation perception is (persistently) below, near, and (persistently) above its long-term average.==== Second, an increase in economic policy uncertainty may also increase the de-anchorage of households’ long-term inflation expectations, and vice versa. As per this channel, an increase in economic policy uncertainty (i.e., a situation in which economic agents are unsure about the actions that policymakers are going to take and the consequences thereof) is expected to increase the degree of de-anchorage (Istrefi and Piloiu, 2014) because as policy uncertainty rises, households begin to doubt the policymakers’ ability or willingness to keep promises. In the context of monetary authorities, this may mean questioning the credibility of the Fed regarding its ability to maintain price stability. As a result, the degree of de-anchorage of inflation expectations may increase.====In a nutshell, the main aim of this paper is to assess and explain the evolution of the degree of (de-)anchorage of long-term inflation expectations in the US from 1990 to 2019. We define long-term inflation expectations (measured by five-years-ahead households’ inflation expectations) as perfectly anchored only if their deviations from the target inflation are not responsive to the respective deviations in short-term inflation expectations (measured by one-year-ahead households’ inflation expectations), following Jochmann et al. (2010), Kumar et al. (2015), Strohsal et al. (2016).==== If long-term inflation expectations are perfectly anchored, the pass-through from deviations of short-term inflation expectations to deviations of long-term inflation expectations is expected to be zero or near zero. We follow a two-step estimation procedure. In the first step, we apply a time-varying parameter approach to estimate the (de-)anchorage of inflation expectations, given by the coefficient of the pass-through, similar to that in Strohsal et al. (2016). We define this as the degree of (de-)anchorage of long-term inflation expectations. In the second step, we analyze the changes in the degree of (de-)anchorage of inflation expectations using two channels: (i) inflation perception and (ii) economic policy uncertainty.====The papers most closely related to our work are those of Binder (2018), Buono and Formai (2018), Kumar et al. (2015), Strohsal et al. (2016). Our approach differs from Buono and Formai (2018) and Strohsal et al. (2016) in the usage of the measures of inflation expectations. While the analyses of Buono and Formai (2018) and Strohsal et al. (2016) were based on professional forecasters’ and a market-based measure of inflation expectations, respectively, we focus on households’ inflation expectations. Our work also differs from Binder (2018) and Kumar et al. (2015) on two counts. First, while Binder (2018) and Kumar et al. (2015) have assessed the (de-) anchorage of households’ inflation expectations in a constant parameter model, we do so in a time-varying framework. Second, we employ perceptions of inflation and economic policy uncertainty to explain the changes in the (de-)anchorage of long-term inflation expectations. To the best of our knowledge, this is the first paper that explains the (de-)anchorage of long-term inflation expectations using inflation perception and policy uncertainty channels.====We find four important results: (i) The long-term inflation expectations of households in the US responded to the short-term inflation expectations in a dynamic manner from 1990 to 2019. In other words, the de-anchorage of long-term inflation expectations has varied over time in the entire sample period. (ii) The de-anchorage was greater in the first half of the 1990s. Subsequently, it has declined but has not yet anchored. (iii) The relationship between inflation perception and the change in degree of de-anchorage is U-shaped if inflation perceptions are low and high persistently, and (iv) A rise in policy uncertainty increases the degree of de-anchorage of long-term inflation expectations.====The rest of the article is structured as follows: Section 2 discusses relationships between households’ long-term and short-term inflation expectations in the US. Section 3 presents the empirical model to estimate the degree of (de-)anchorage of households’ long-term inflation expectations. Section 4 presents the estimates of the degree of (de-)anchorage in the US and the channels which explain its evolution. The conclusions and policy implications are presented in Section 5.",Assessing the (de-)anchoring of households’ long-term inflation expectations in the US,https://www.sciencedirect.com/science/article/pii/S016407041930134X,29 November 2019,2019,Research Article,213.0
Hamaguchi Yoshihiro,"Faculty of Management, Department of Management, Osaka Seikei University, 3-10-62, Aikawa, Higashiyodogawa-ku, Osaka, 533-0007, Japan,Graduate School of Economics, Osaka University, 1–7, Machikaneyama, Toyonaka, Osaka, 560-0034, Japan","Received 10 May 2019, Revised 8 November 2019, Accepted 13 November 2019, Available online 14 November 2019, Version of Record 21 November 2019.",https://doi.org/10.1016/j.jmacro.2019.103169,Cited by (12),"Using an R&D-based growth model with dual regulation, we analyse how ","With worsening climate, more EU member states are participating in the European Union Emissions Trading System (EU-ETS). Thus, a dual regulation problem is created, whereby firms emitting greenhouse gases are subjected to overlapping regulations: environmental taxes and emissions permit trading. However, industries in the EU have resisted these regulations because such overlapping regulations for the same pollution impose additional costs on firms (Sorrel, 2002). The additional costs from dual regulation may incentivise firms to evade environmental taxes through corruption. The government suffers from corruption, which damages the efficiency of environmental policies. This is because the weakening of environmental regulations as a result of corruption may inhibit economic development and increase pollution (Pellegrini and Gerlagh, 2006). In developing countries, corrupt officials demand bribes from firms in return for overlooking the under-reporting of pollution emissions. Previous studies show that polluting firms evade environmental tax via corruption (i.e. Damania, 2002, Iskandar, Bhaduri, Wünscher, 2014). Thus, tax evasion with corruption may slow economic development and worsen environmental quality if there is tax evasion in developed countries.====Environmental tax evasion is not expected in developed countries. However, European Union (EU) member states such as the United Kingdom (Customs, 2011) report that polluting firms do engage in environmental tax evasion. Thus, in contrast to developing countries, dual regulation in developed countries detrimentally affects the efficiency of emissions permit-trading via tax evasion with corruption. This damage via dual regulation on developed countries has not been addressed in previous literature (e.g. Damania, 2002, Lapatinas, Litina, Sartzetakis, 2011, Liu, 2013, Cerqueti, Coppier, 2016, Iskandar, Bhaduri, Wünscher, 2014). The examination of this point is important because it provides clues to the improvement of the emissions permit-trading system under dual regulation. Moreover, this study provides clues to preventing tax evasion with corruption from inhibiting economic development via environmental policies. Our research goal is to analyse how additional costs under dual regulation influence emissions permit trading and economic development via environmental tax evasion with corruption.====Here, we aim at ====, which implies that an allocation rent increases the polluting firms’ profit (Smale et al., 2006) and their value (Bovenberg, Goulder, 2006, Oestreich, Tsiakas, 2015). This distribution effect promotes the entry of new polluting firms, which aim to obtain more rents and, thus, may increase the number of firms participating in the EU-ETS. The total permit level allocated by the government fixes the total pollution level and, thus, an increase in the number of firms leads to a change in the average pollution in the long run. Moreover, this increase also leads to a change in the average corruption because each firm bribes officials to evade tax. Under dual regulation, environmental tax evasion influences the market permit price. Thus, environmental tax evasion may have some effects on the long-term relationship between pollution and corruption via the distribution effect.====To study this point of argument, we apply Nash bargaining between the firms and officials, as considered by Harstad and Svensson (2011), to the variety expansion model developed by Romer (1990) and Grossman and Helpman (1991). Then, we regard the government’s regulation motivation as pollution reduction and consider ====) emissions permit trading and ====) endogenous changes in the number of polluting firms via innovation. This model setting expands Harstad and Svensson (2011)’s model. In this setting, we investigate the effects that changes in the grandfathered permit level, environmental tax rate, and penalty rate have on permit price, growth rate, pollution and corruption per firm, and on the welfare of a household and a bureaucrat. Our analysis shows the following: A decrease in the grandfathered permit level stimulates the growth rate via an increase in permit price, reduces the pollution and corruption per firm, and results in improving the household’s welfare and worsening the bureaucrat’s welfare. By contrast, stricter environmental tax and penalty rates reduce the growth rate via a decrease in permit price, expand pollution and corruption per firm, and result in worsening households’ welfare and improving the bureaucrat’s welfare. These findings imply that tax evasion improves social welfare under dual regulation.====Our findings add several insights to the literature. The first finding is that the permit price is determined by environmental tax. In the present model, the government imposes overlapping regulations for the same pollution on polluting firms. Thus, under the framework of the general equilibrium model, environmental tax influences the permit price via the emissions permit market clearing condition. This result will provide beneficial suggestions to the discussion on the equivalence between environmental tax and emissions permit trading. Additionally, our study states that the effect of environmental tax on permit price changes according to tax evasion. In the present model, the government imposes a penalty rate on corrupt officials to prevent polluting firms from evading tax via corruption. This enables us to analyse how the permit price changes according to the degree of quality of the political system, which depends on political factors such as the penalty rate and the officials’ bargaining power. Previous studies on dual regulation focus on environmental and cost efficiencies and ignore the effect of environmental tax evasion with corruption on permit price (e.g. Böhringer, Koschel, Moslener, 2008, Mandell, 2008, Eichner, Pethig, 2007). Thus, our finding suggests that the permit price is influenced under dual regulation via environmental tax, corruption, and so on.====The second finding is a growth-enhancing effect via the distribution effect. In this model, increasing the permit price via a decreased grandfathered permit level leads to an increase in the firms’ profits because it expands the permit allocation rents. This leads to an increase in the economic growth rate because it stimulates the incentive of innovation via the no-arbitrage condition between assets and shares. This result is contrary to Ono, 2002, who shows that a decrease in equilibrium employment enhances the growth-reducing effect of permit level. Previous studies find the growth-enhancing effect of environmental policy to be via substitution of education for leisure (Hettich, 1998), substitution of innovation for leisure (Hamaguchi, 2019b), a general equilibrium effect via markup (Nakada, 2004), a substitution effect via social status preference (Hamaguchi, 2019c), an indeterminacy of equilibrium (Itaya, 2008), a generation turnover effect (Pautrel, 2008), and an externality effect of pollution on lifespan (Pautrel, 2009). Additionally, Hamaguchi (2019a) shows that the growth-enhancing effect via the distribution effect leads to tourism-led growth in an agglomeration model. We add the distribution effect to the literature. Additionally, although the literature of public economics has shown that tax evasion has a positive supply-side effect on output (e.g. Chang, Lai, Chang, 1999, Peacock and Shaw, 1982) and economic growth (e.g. Chen, 2003), the literature of environmental economics ignores the effect of tax evasion and corruption on the economic growth rate. We find that tax evasion with corruption stimulates the economic growth rate via an increased permit price whereas a stricter penalty leads to a decrease in the growth rate via a decreased permit price. These results imply that, under dual regulation, economic growth accelerates via environmental tax evasion. Thus, we combine the growth-enhancing effect via environmental policy with the political factor not considered in previous studies.====The third finding is corruption per firm in proportion to the pollution per firm. This positive relationship between pollution and corruption is confirmed by recent empirical studies (e.g. Welsch, 2004, Farzin, Bond, 2006, Pellegrini, Gerlagh, 2006, Fredriksson, Wollscheid, 2008). Thus, our study provides a rationale for this empirical relationship. Moreover, we find positive effects of environmental tax and penalty rates on the pollution and corruption per firm. The key to finding this relationship and its effects is endogenous changes in the number of polluting firms in the long run. In this model, a decrease in the economic growth rate via the distribution effect implies a decrease in the number of polluting firms. Under the total pollution level fixed by the total permit level, the decrease in the number of firms leads to an increase in the pollution and corruption per firm. By contrast, analysis of previous studies is static and assumes a constant number of polluting firms. For example, Damania (2002) shows that an environmental tax prompts polluting firms to under-report their pollution emissions, whereas a higher penalty prevents firms from evading the tax. Iskandar et al. (2014) also find that corruption prompts polluting firms to evade environmental tax depending on the tax rate and confirms that the results obtained by their model analysis apply to the economic experience in Indonesia. Therefore, it is difficult for previous studies to investigate how the long-term effect influences the relationship between pollution and corruption in the static framework of their analyses. Thus, our dynamic analysis adds to the literature on the long-term effect of endogenous changes in the number of firms on the relationship between pollution and corruption.====The fourth finding is that tax evasion itself improves social welfare under dual regulation. We observe that tax evasion improves the household’s welfare and worsens the bureaucrat’s, while a higher penalty rate worsens the household’s welfare and improves the bureaucrat’s. The welfare-improving effect of tax evasion is counter to ==== found by Liu (2013). This effect implies that welfare improves via a cost reduction of tax evasion because firms substitute their existing low-cost tax evasion for high-cost environmental tax evasion. In the present model, the key to improving social welfare via tax evasion is dual regulation because the welfare effect depends on the effect of environmental tax on permit price. Thus, our finding suggests to previous studies that the welfare effect of tax evasion may change according to the regulation system.====The remainder of the paper is structured as follows: We describe a dynamic general equilibrium model in Section 2 and then define the equilibrium in Section 3. In Section 4, we investigate how environmental policies affect pollution, corruption, and the economic growth rate. We conduct a welfare analysis of the environmental policies in Section 5. Finally, Section 6 concludes the paper.",Dynamic analysis of bribery firms’ environmental tax evasion in an emissions trading market,https://www.sciencedirect.com/science/article/pii/S0164070419302162,14 November 2019,2019,Research Article,214.0
Kelly Mark,"Baylor University, Hankamer School of BusinessWaco, TX 76798-8003, United States","Received 14 March 2019, Revised 8 November 2019, Accepted 13 November 2019, Available online 14 November 2019, Version of Record 19 November 2019.",https://doi.org/10.1016/j.jmacro.2019.103170,Cited by (3),", and access to employer-provided ","Healthcare in the United States is widely criticized for its lack of efficiency and equity relative to the healthcare systems of other OECD nations. Critics have argued that inefficiency and inequity are inherent to U.S.’ two-tiered healthcare system due to its reliance on non-compulsory private health insurance markets which act as the entire system’s foundation. These criticisms follow from the work of numerous economists who have identified various market failures associated with private health insurance markets. These studies have typically concluded that the most effective solution is to restructure the entire system around some form of universal, compulsory health insurance (e.g. Arrow, 1963, Pauly, 1974, Rothschild, Stiglitz, 1976, Dahlby, 1981).====One such proposal that has received some attention recently is the so-called “Medicare for all” (MFA) plan. As the name indicates, the MFA plan would expand Medicare to cover all individuals in the economy, thereby eradicating inequity within the U.S.’ healthcare system by providing all individuals with equal access to care. Proponents of MFA argue that the scheme would also improve the efficiency of the healthcare sector by eliminating the problem of adverse selection that is characteristic of non-compulsory private insurance markets and gives the government greater ability to enforce the utilization of cost effective healthcare technologies.==== However, given the relative magnitude of healthcare expenditures in the US,==== the importance of employer-provided health insurance (EPHI) in the labor market,==== and the considerable burden entitlement programs already place on government budgets,==== the expansion of Medicare to all individuals in the economy will have significant downstream consequences for the rest of the economy.====To date, the literature has largely relied on partial equilibrium models of healthcare and health insurance markets that omit the rest of the economy (e.g. Feldman and Dowd, 1991). As a result, it is difficult to draw any conclusions about the broader impact that major healthcare reforms, such as MFA, will have on the economy as a whole. Therefore, in this study, I utilize a new class of overlapping generations model developed by Dalgaard and Strulik (2014) and Dalgaard and Strulik (2017) to analyze the general equilibrium welfare response to the creation of a MFA healthcare system. I adapt their framework to include public and private health insurance and I introduce heterogeneous households that differ according to age, skill level, and access to EPHI benefits. More specifically, I assume that the economy is composed of a continuum of finitely-lived cohorts, each consisting of three distinct household types: skilled households, who I assume all receive EPHI, unskilled households with access to EPHI, and unskilled households without access to EPHI.====After calibrating the benchmark model to match U.S. data from the 10-year period between 2005 and 2014, I evaluate the welfare response to the creation of a MFA program for each of the three household groups. Initially, I assume that MFA is financed through income taxes. In order to be consistent with recent proposals, the income tax rate assessed on low income (i.e. unskilled) households is assumed to be held constant at its benchmark level, placing the burden of funding the Medicare expansion on high income (i.e. skilled) households. Unskilled households, regardless of age or access to EPHI, are made better off, having received lifelong Medicare coverage without having to pay any additional taxes. Skilled households, on the other hand are made significantly worse off after having to bear a 39.59% increase to their equilibrium income tax rate in order to finance the Medicare expansion.====Alternatively, the same Medicare income tax could be assessed on all households, regardless of skill level. Under this tax scheme, all young households are made better off, while the welfare of older households is diminished. On average, unskilled households will benefit from this policy change, while the average welfare response for skilled households is negative. In the final version of MFA that I consider, I assume that Medicare is financed via a consumption tax. This scheme results in a sub-optimal substitution away from non-medical consumption toward healthcare, benefiting young households to the detriment of old households. On average, the welfare of all household groups in the economy will decline following the implementation of this policy.====Conceivably, universal health insurance coverage could be achieved in a single-tier privatized health insurance system. Therefore, in the second part of the analysis, I consider a reform (referred to herein as “Medicare for none”) that eliminates all forms of public health insurance and creates a single market for private health insurance. There are several strategies that the government could pursue to ensure that all households will obtain insurance in the private market. One option is to expand the Affordable Care Act’s (ACA) individual mandate and subsidies to cover all households and increase the penalty until universal coverage is achieved. Income-based subsidies alone may not be sufficient to induce all individuals to acquire health insurance, making the penalty necessary. However, assessing a fine on people who make an optimal decision to not insure is unlikely to generate a socially optimal outcome for all individuals in a society.====Therefore, in my analysis, I assume that the premium subsidies are based on the household’s utilization of their health insurance. This policy guarantees that the out-of-pocket cost of obtaining health insurance accurately reflects the true value of health insurance to the household. Utilization-based subsidies also have the advantage of creating a greater incentive for households to internalize the cost of healthcare (similar to HSAs), making household healthcare investment decisions more efficient. In addition to the aforementioned advantages of utilization-based subsidies, Medicare for none (MFN) significantly reduces the tax burden on households by eliminating the EPHI tax exemption and all Medicare outlays, resulting in positive welfare gains (on average) for all households in the economy that typically exceed the average welfare gains that occur under MFA.====The rest of the paper is laid out as follows: Section 2 presents the model. Section 3 details the data utilized in the calibration exercise and compares the calibration results to the data. In Section 4, I analyze the general equilibrium response to each of the five policy experiments I consider. In Section 5, I provide my conclusions and discuss potential challenges to the healthcare reforms proposed in this study.",Medicare for all or medicare for none? A macroeconomic analysis of healthcare reform,https://www.sciencedirect.com/science/article/pii/S0164070419301120,14 November 2019,2019,Research Article,215.0
"Kim Youngju,Lim Hyunjoon","The Bank of Korea 39, Namdaemun-Ro Jung-Gu, Seoul 04531, Republic of Korea","Received 8 May 2018, Revised 5 November 2019, Accepted 6 November 2019, Available online 7 November 2019, Version of Record 20 November 2019.",https://doi.org/10.1016/j.jmacro.2019.103168,Cited by (6),This paper explores whether the degree of household indebtedness can affect the effectiveness of ,"We investigate whether household indebtedness affects the effectiveness of monetary policy, using a panel of 23 countries and covering the period from 1984:Q1 to 2015:Q4. To this end, we estimate the asymmetric effect of monetary policy dependent on the direction of policy stance, and the level of household indebtedness, thereby departing from the existing related literature, most of which focuses on the average effect of monetary shocks with opposite signs. In addition, we further examine the cash-flow channel in the transmission of monetary policy shocks, analyzing the cross-country differences in the predominant types of interest rate contracts that apply to household debts. If a country is dominated by fixed-rate mortgages (FRMs), one would expect the cash-flow channel to be relatively muted. However, in economies with mostly adjustable-rate mortgages (ARMs), the cash-flow channel may be more important for the transmission of monetary policy.====There are some channels through which higher levels of household debt may amplify the effects of monetary policy shocks on overall economic activity. According to the supposed cash-flow channels, monetary policy can have a direct impact on aggregate household spending via the transfer of income between household borrowers and lenders. For example, a decline in interest rates will reduce lenders interest income, while also reducing interest payments on indebted households, resulting in income transfers between the two groups. It has been widely acknowledged that changes in cash flows may affect consumption, particularly for households that are more financially constrained. This implies that the aggregate effects of this transfer may not be muted if borrowing households are financially constrained. Since households in a high-debt state are more likely to be financially constrained, they should have higher propensities to consume than the lenders. In a high household debt state, the same change in the policy rate, therefore, could have an arguably much larger effect on household spending through a cash-flow channel than otherwise.==== The dominant type of interest rate contracts fixed versus floating interest rates that apply to household debt could also affect the responsiveness to monetary shocks of aggregate output in the economy.====Recent studies provide evidence that monetary policy may be less effective when households have high levels of debt, as seen in the recent US recession. Mian, Sufi, 2014, Mian, Sufi, 2015 argue that the precautionary savings motive, induced by the heightened risk to future employment income and low-equity mortgages, renders high-leveraged households less responsive to monetary stimulus during the balance sheet recession associated with the crisis. From a theoretical perspective, Alpanda and Zubairy (2016), presenting a partial equilibrium model in which households are financially constrained, argue that the cash-flow channel of monetary policy is stronger in a high-debt state, but new borrowing through home equity loans (i.e., a home-equity loan channel) works only when debt levels are relatively low and borrowers hold an adequate level of home equity.==== They argue that expansionary monetary policy may have a weaker effect under high levels of debt, when the effect of the home equity channel dominates that of the cash-flow channel. Also, Bhutta and Keys (2016) show that low home equity levels make it more difficult for households to tap into their home equity lines of credit. In a similar vein, Chen et al. (2013) and Beraja et al. (2015) present evidence that households with low home equity levels have difficulties in refinancing at a lower mortgage rate.====In our empirical analyses, we use an interacted Panel Vector Auto Regression (IPVAR), first proposed by Towbin and Weber (2013) and Sá et al. (2014), as a framework to test whether household indebtedness affects the effectiveness of monetary policy. IPVAR allows VAR coefficients to vary as a deterministic function of observable economic characteristics, thereby enabling us to examine the impact of economic characteristics on the transmission mechanism of an economic shock of interest. We estimate a PVAR and augment it with an interaction term that allows the estimated coefficients to vary with the level of household indebtedness. With this approach, we can investigate how the macroeconomic variables impulse response to a monetary shock varies with the level of household indebtedness. In addition to the strength of monetary policy that is dependent upon household indebtedness, monetary policy may have an asymmetric effect based on its stance: expansionary or contractionary. To test for the possible asymmetric effect of monetary policy, we extend the interaction term to allow the effect of monetary policy to depend on both the direction of monetary policy and household indebtedness. In this framework, we test the asymmetric effect of monetary policy across the state of household indebtedness. Lastly, we split the sample countries into two groups by the predominant type of interest rate that applies to household debt floating versus fixed-interest rate contracts to study the robustness of the monetary policys cash-flow channel. Then, we estimate the Panel VAR with the extended interaction term for each group, and compare the impulse responses to monetary policy stance.====Our first findings suggest that monetary policy has a stronger average effect on real economic activities, particularly on consumption and investment, when households are highly indebted. This result does not discriminate between an expansionary and a contractionary monetary policy stance. We disentangle the effect of monetary policy on output and prices by the policy stance in a high-debt state, finding that monetary policy has a stronger effect on consumption and investment in a contractionary monetary policy stance than in an expansionary one. Finally, we investigate whether the effect of monetary policy can vary by the type of interest rate contracts and find that when a country is in a high debt state and in a contractionary monetary policy stance, monetary policy is more powerful in countries with a higher share of adjustable-rate loans. This finding is in line with what the cash-flow channel implies: monetary policy is more powerful when households are highly indebted and have adjustable-rate contracts. However, our results imply that the cash-flow channel may be more important in a contractionary monetary policy stance.====The remainder of this paper is organized as follows. Section 2 discusses the related literature. Section 3 describes the data and presents the empirical methodology. In Section 4, baseline estimates of impulse responses with several extensions are reported, along with the results from various robustness checks. Section 5 concludes.",Transmission of monetary policy in times of high household debt,https://www.sciencedirect.com/science/article/pii/S0164070418302015,7 November 2019,2019,Research Article,216.0
"De Schryder Selien,Peersman Gert,Wauters Joris","Department of Economics, Ghent University, Sint Pietersplein 5, Gent B-9000, Belgium,National Bank of Belgium, de Berlaimontlaan 14, Brussels 1000, Belgium","Received 15 November 2018, Revised 30 October 2019, Accepted 30 October 2019, Available online 6 November 2019, Version of Record 15 November 2019.",https://doi.org/10.1016/j.jmacro.2019.103166,Cited by (2), is truly structural.,"New Keynesian dynamic stochastic general equilibrium (DSGE) models typically assume sticky wages and partial wage indexation to past inflation. Notably, the degree to which wages are indexed to past inflation is hard-wired as a fixed and policy invariant parameter (e.g., Christiano, Eichenbaum, Evans, 2005, Smets, Wouters, 2007).====The assumption of a constant degree of wage indexation has, however, been rejected by institutional and empirical evidence for the United States (US). In particular, the coverage of private-sector workers by cost-of-living adjustment (COLA) clauses rose substantially between the late 1960s and mid-1980s, after which it declined again (Holland, 1986). Hofmann et al. (2012) estimate the extent of wage indexation in the US over time and find a considerably higher degree of indexation during the “Great Inflation” of the 1970s compared to the earlier and later periods. They infer that the reduction in indexation from the mid-1970s to 2000 led to a decline in the long-run impact of a supply and demand shock on prices by respectively 44% and 39%. Since changes in wage indexation practices have clearly had significant macroeconomic consequences, it is essential to understand why the degree of indexation changed over time.====Hofmann et al. (2012) argue that the rise and fall of wage indexation can be explained by a weaker reaction of the Federal Reserve to inflation during the “Great Inflation” and more aggressive inflation stabilization before and after this period. Intuitively, a regime of high and volatile inflation fosters the use of wage indexation clauses as protection against inflation uncertainty.==== This conjecture is also supported by economic theory that shows that the optimal proportion of wage contracts indexed to inflation increases with the variance of monetary disturbances (Gray, 1978) and that the likelihood of indexation rises when inflation uncertainty is higher (Ehrenberg et al., 1983).====However, other explanations for changes in wage indexation have also been put forward. Carrillo et al. (2017) demonstrate that utility-maximizing workers base their indexation choice on the relative importance of shocks in the economy. More specifically, workers want to index wages to past inflation when technology and permanent shocks to the inflation target dominate output fluctuations, but not when aggregate demand and temporary inflation target shocks dominate. Based on a counterfactual calibration exercise for the US, they attribute the high degree of wage indexation in the 1970s primarily to very volatile supply-side shocks relative to demand-side shocks. Changes in the monetary policy rule and the stability of the inflation target, in contrast, had probably only a minor influence on wage indexation in the US. Furthermore, Duca and VanHoose (1998) find empirical evidence supporting their theory of an inverse relationship between the degree of product market competition and wage indexation. Intuitively, increased goods market competition raises the price elasticity of demand, which makes equilibrium employment less sensitive to demand shocks and thereby reduces the incentive to index. Finally, the degree of wage indexation might also be linked to labor market characteristics. For example, Messina and Sanz-de Galdeano (2014) find that a decline in union coverage and a more decentralized wage bargaining reduces wage indexation. In sum, it remains unclear to what extent the degree of wage indexation in the economy, and its variation over time, can be linked to the monetary policy regime in place. Our paper aims to fill this gap.====This study is the first to use cross-country data to examine whether wage indexation varies systematically across monetary policy regimes. We estimate variants of the reduced-form empirical New Keynesian wage Phillips curve of Galí (2011) on a panel dataset covering 24 OECD countries between 1975Q1 and 2016Q4. We measure structural differences in wage indexation across countries, as well as changes in wage indexation over time. We innovate relative to the literature in two ways. First, we take into account several possible drivers behind changes in wage indexation. To measure the effects of the monetary policy regime, we distinguish between the presence of an inflation, money growth, and exchange rate target. This distinction is motivated by the differences between the underlying dynamics of these strategies and their impact on the formation of inflation expectations. For example, inflation-targeting central banks typically try to stabilize inflation in the short to medium term, whereas money growth targeting is more a commitment to low inflation in the long run.==== In addition, we take into account the degree of product market competition and wage bargaining characteristics as alternative drivers for wage indexation changes.====Second, we use a panel dataset to exploit the information across the cross section and to increase the number of observations significantly. This is crucial because monetary policy targets and wage bargaining characteristics can show limited variation over specific periods, yet differ importantly between countries. By contrast, existing studies typically focus on the US alone and thereby capture less variation.====The estimation results provide important considerations for macroeconomic analysis and policymakers. First, our benchmark regressions without interaction effects show that wage indexation is significant and economically relevant for the sample under analysis. Second, when allowing wage indexation to vary depending on the monetary target, we find an economically and statistically significant reduction of wage indexation for countries with an inflation target. Exchange rate or monetary targets, by contrast, do not show systematic effects. This result resonates with Benati (2008), who estimates the price Phillips curve on historical data for a set of countries and finds the price indexation parameter to be very low or zero under stable monetary policy regimes with clearly defined nominal anchors. Third, when we include structural characteristics on the degree of product market competition and wage bargaining, the effect of inflation targeting essentially vanishes. More precisely, we find evidence that a higher degree of product market competition lowers the degree of wage indexation in an economically and statistically significant way. This effect is found to withstand a broad set of robustness checks. Therefore, we generalize the results from Duca and VanHoose (1998) for a broad set of countries and a longer time span.====Since product market competition is an outcome of economic policy, we conclude that the constant indexation assumption embedded in standard DSGE models is susceptible to the Lucas (1976) critique, i.e., it is not intrinsic to the deep structure of the economy and not a policy invariant parameter. This is relevant for policymakers, given the macroeconomic importance of wage indexation on output and inflation. Furthermore, our results suggest that monetary policy is not a crucial driver of wage indexation, which is consistent with Carrillo et al. (2017). But we caution against interpreting the statistically insignificant effects from the monetary policy targets as definitive evidence that monetary policy is irrelevant for wage indexation. Our results are consistent with insignificant effects of monetary policy, but they are also consistent with the view that it is challenging to disentangle monetary policy effects from product market competition effects. Indeed, we show that the shift towards more product market competition is strongly correlated with the adoption of inflation targets.====Our work relates to literature that links changing wage or price dynamics to shifts in macroeconomic policy. For instance, Messina and Sanz-de Galdeano (2014) use micro-level data to document how Brazil’s and Uruguay’s disinflation policies changed the nature of wage rigidities. Alogoskoufis and Smith (1991) study wage and price inflation series from 1892 to 1987 for the US and the UK; they report coinciding shifts in the wage Phillips curve and price inflation persistence, which they link to departures from international fixed exchange rate regimes. Levin et al. (2004) find that inflation expectations appear to be more forward-looking, and inflation less persistent, in inflation-targeting countries.====A related study is also Fatás et al. (2007), who find that having an explicit quantitative target for monetary policy, in particular, an inflation target, is systematically related to a lower average level of price inflation. Moretti (2014) furthermore finds that inflation targeting and product market deregulation have both lowered the level of price inflation in OECD economies. More recently, López-Villavicencio and Saglio (2017) provide cross-country evidence that wage indexation to past inflation has decreased when countries shifted to a low inflation environment.==== Finally, our study is related to the literature that analyzes the role of monetary policy institutions for inflation outcomes and economic growth, such as central bank independence (Alesina and Summers, 1993) and transparency (Sterne, Stasavage, Chortareas, 2002, Eijffinger, P., 2006, Dincer, Eichengreen, 2014).====The remainder of the paper is organized as follows: In the next section, we present the estimation results for a benchmark wage Phillips curve model with a constant degree of wage indexation. In Section 3, we extend the benchmark model to analyze the influence of the monetary policy regime on the extent of backward-looking wage indexation, while controlling for the degree of goods market competition and labor market characteristics. Finally, Section 4 concludes.",Wage indexation and the monetary policy regime,https://www.sciencedirect.com/science/article/pii/S0164070418304786,6 November 2019,2019,Research Article,217.0
"Hotchkiss Julie L.,Moore Robert E.,Rios-Avila Fernando","Federal Reserve Bank of Atlanta, United States,Georgia State University, Georgia, United States,Levy Economics Institute of Bard College, United States","Received 6 February 2019, Revised 29 October 2019, Accepted 30 October 2019, Available online 31 October 2019, Version of Record 24 November 2019.",https://doi.org/10.1016/j.jmacro.2019.103167,Cited by (2),"This paper calculates the welfare cost to families of an unemployment shock. Using U.S. data, we find an average annualized expected dollar equivalent welfare loss of $1,156 when the unemployment rate rises by one-percentage point. Relative to single families, the welfare loss is greater for married families and increases with education. We also estimate that a loss in purchasing power of 1.8% generates the same amount of welfare loss as a one percentage point rise in the unemployment rate. Additionally, the magnitude of the shock to purchasing power that a family is willing to endure to avoid a one percentage point increases in the aggregate unemployment rate rises with income. The results in this paper informs policy makers about the distributional implications of decisions likely to affect labor markets.",None,Cost of policy choices: A microsimulation analysis of the impact on family welfare of unemployment and price changes,https://www.sciencedirect.com/science/article/pii/S0164070419300515,31 October 2019,2019,Research Article,218.0
"Tang Yang,Zeng Ting,Zhu Shenghao","Nanyang Technological University, Singapore,Southwestern University of Finance and Economics, China,University of International Business and Economics, China","Received 24 July 2018, Revised 17 October 2019, Accepted 17 October 2019, Available online 28 October 2019, Version of Record 10 November 2019.",https://doi.org/10.1016/j.jmacro.2019.103163,Cited by (4),"We investigate the rapid growth in the dispersion of housing prices across metropolitan statistical areas (MSAs) in the United States during 1975–2017. We first examine several explanations for this pattern, and find that it is difficult to fully explain it. Our ==== analyses show that the log of price-to-rent ratios follows a random walk process. We then set up a parsimonious asset-pricing island model. We find that the dispersion of fundamental housing prices grow too slow relative to that in data. Incorporating rational bubble solutions, our calibrated model can match the rapid growth in the dispersion of housing prices.","The housing market in the United States during 1975–2017 is featured by a rapid growth in the dispersion of housing prices across metropolitan statistical areas (MSAs). Since housing typically takes up a major proportion of household net worth, fluctuations in housing prices can thus exert significant impacts on the macro-economy.==== Hence, it is important to understand what drives the rapid rise in the dispersion of housing prices. In this paper we attempt to investigate this issue.====In Fig. 1, we plot the cross-sectional coefficient of variation (CV) for housing prices in the United States during 1975–2017.==== Our sample consists of a panel of 330 major MSAs in US. Fig. 1 shows that there is a rapid increase in the dispersion of housing prices. The CV of housing prices in 1975 is 0.17, while this number reaches 0.55 in 2007. Despite a decline due to the 2007 financial crisis, the CV recovers quickly and restores its pre-crisis level by 2017.====We first examine several intuitively plausible explanations for patterns observed in Fig. 1. These explanations emphasize the population concentration, the divergence in income growth rates across MSAs, or the divergence in income growth rates of top income groups across MSAs. Through investigations of these explanations we find that it is difficult to fully explain the rapid rise in the dispersion of housing prices during 1975–2017.====We then conduct econometric analyses of panel data. We empirically show that housing price processes are non-stationary and housing prices grow at different rates across MSAs. These findings can potentially lead to the pattern depicted in Fig. 1. We find that rental growth rates are stationary. While different MSAs have different growth rates of housing prices, they have the same average growth rate of rental prices. This comparison hints that rentals are not the main reason of the rapid rise in the dispersion of housing prices. This comparison also encourages us to further investigate house price-to-rent ratios. Through a panel-data unit-root test we find that house price-to-rent ratios are also non-stationary.====To investigate further patterns in Fig. 1 we set up a parsimonious asset-pricing island model. Each island corresponds to an MSA. We first study the fundamental solution of the asset pricing model, in which the fundamental housing prices are completely supported by rents while both the inter-temporal Euler equation and the transversality condition hold. Our calculations show that the cross-sectional CV of housing prices is larger than that of housing prices implied by the fundamental solution for each year during 1975–2017. Housing prices in the United States display excessive dispersion. Also we find that the growth in the dispersion of fundamental housing prices is too slow relative to the pattern in data.====Inspired by our empirical finding that the log of price-to-rent ratios follows a random walk process, we then investigate rational bubbles in our asset pricing model. Following Froot and Obstfeld (1991) and Lansing (2010), we generate bubble components of asset prices in the Lucas asset-pricing model by removing the transversality condition. Specifically, the stochastic growth component of the price-to-rent ratio causes housing price bubbles in our model. Bubble components can only be supported by speculations. Furthermore, our model can deliver explicit expressions for both fundamental and bubble components of the price-to-rent ratio. Our calibrated model with rational bubbles can simultaneously match four stylized facts in the United States during 1975–2017, the rapid growth in the dispersion of housing prices, a moderate increase in the dispersion of rental prices, the rising mean of housing prices, and the rising mean of rental prices. The stochastic growth component of the price-to-rent ratio in the bubble solution is the key mechanism, through which our model could match the rapid growth in the dispersion of housing prices. We also perform several robustness checks. Our model still successfully match housing price moments.====One may think that the stochastic growth component of the price-to-rent ratio eventually leads to explosive dispersion of housing prices. In an extension, we introduce an extrinsic uncertainty, which represents the confidence state, into the benchmark model. We construct a sunspot equilibrium in which bubbles eventually burst in the long-run. But before bubbles burst, there is a rapid growth in the dispersion of housing prices. Thus our paper also contributes to the literature of sunspot equilibria and asset pricing.",Bubbles and house price dispersion in the United States during 1975–2017,https://www.sciencedirect.com/science/article/pii/S0164070418303215,28 October 2019,2019,Research Article,219.0
"Horvath Roman,Kaszab Lorant,Marsal Ales,Rabitsch Katrin","Charles University, Prague and University of Ss. Cyril and Methodius in Trnava, Slovakia,Central Bank of Hungary, Vienna University of Economics and Business and Masaryk University, Brno,National Bank of Slovakia, Charles University, Prague and Vienna University of Economics and Business, Austria,Vienna University of Economics and Business, Austria","Received 27 April 2018, Revised 3 September 2019, Accepted 8 October 2019, Available online 28 October 2019, Version of Record 6 November 2019.",https://doi.org/10.1016/j.jmacro.2019.103162,Cited by (4)," are lower, so that fiscal multipliers at the ZLB tend to be lower. Finally, we also discuss the role of solution methods in determining the size of fiscal multipliers.","After the introduction of the $750 billion US fiscal stimulus package in 2009 there has been a renewed interest in the effectiveness of fiscal policy in the environment of ultra-low interest rates. Several authors show that the size of fiscal multipliers is significantly higher when the economy is at a zero lower bound (ZLB) of the nominal interest rate (see Eggertsson, 2011, Erceg, Linde, 2014, Christiano, Eichenbaum, Rebelo, 2011 or Woodford (2011)), making a case for the ability of fiscal policy to curb the adverse effects of financial crisis. The economic consensus on fiscal multipliers in normal times is, that they tend to be small. This is for two reasons: one, increases in government expenditure need to be financed, and thus come with a negative wealth effect, which crowds out consumption and decreases demand; two, a fiscal expansion, increasing inflation and output, triggers an endogenous response of the monetary authority, which raises interest rates, offsetting some of the expansionary effect of fiscal policy. In times when the economy is at the zero lower bound, such endogenous dampening response of monetary policy is absent, as the nominal interest rate stuck at the lower bound and thus constant; in such case, an increase in (expected) inflation, resulting from a fiscal expansion, leads to a drop in the real interest rate, which further stimulates demand and thus increases fiscal multipliers.====This paper extends the New Keynesian model of Eggertsson (2011) and studies the size of various types of fiscal multipliers, in normal times, when the nominal interest rate is positive, and when the economy is at the zero lower bound. We calibrate our model to the US economy and study four different types of fiscal multipliers: a government spending, a payroll tax, a sales tax, and a financial asset tax multiplier. We document that the size of fiscal multipliers at the ZLB crucially depends on the slope of the Phillips curve, with a flatter Phillips curve being associated with smaller multipliers. This is because in the context of the New Keynesian model an, e.g., increase in government spending can raise output owing to a rise in expected inflation which, at the zero lower bound, decreases the real interest rate, stimulating consumption and output. A flatter Phillips curve attenuates the inflation channel and, thus, decreases the value of the multiplier. A sufficiently flat Phillips curve, consistent with recent empirical estimates, delivers a spending multiplier at or below one and a consumption tax cut multiplier that is strictly below one.====The reasons behind the flattening of the Phillips curve that we consider in our model are consistent with both the macroeconomic and microeconomic empirical evidence. In particular, we do not obtain a flatter Phillips curve from employing a higher degree of nominal rigidity; instead, it results from an increase in the degree of strategic complementarity in price-setting, invoked in the model through assumptions of (i) a specific labour market==== and (ii) decreasing returns-to-scale in production. There is a growing macroeconomic literature suggesting a flattening of the Phillips curve (see, e.g., Blanchard et al., 2015, among others), i.e. a weaker link between economic activity and inflation. The reasons and implications of the flattening of the Phillips curve have been primarily examined for the (lack of) inflation after the crisis or more generally, for monetary policy strategy (Blanchard et al., 2015). We document that this consideration is equally consequential for fiscal multipliers. This macroeconomic literature on the flattening of the Phillips curve is supported by a growing microeconomic literature suggesting that strategic complementarity is an important factor in how firms set prices, and that a high degree of strategic complementarity results in a flat Phillips curve (Coricelli, Horvath, 2010, Woodford, 2003). Using micro-level Belgian consumer prices data, (Amiti et al., 2019) develop a general theoretical framework and empirical identification strategy to directly estimate firm price responses to changes in prices of their competitors. Their results suggest an elasticity of more than one-third in response to the price changes of its competitors (i.e. strategic complementarity) and an elasticity of nearly two-thirds in response to its own cost shocks. Interestingly, this ’strategic complementarity’ elasticity increases to one-half for large firms.====Our results suggest that the empirically relevant reasons for a flattening of the Phillips curve, that we incorporate in our model, lead to smaller fiscal multipliers at the ZLB. More generally, we present detailed results for multipliers for our four types of fiscal instruments, in both normal and ZLB times, and show how they are influenced by the different settings of specific versus economy-wise labour market and constant versus decreasing returns to scale.==== We also present evidence that shows that the level of steady-state government spending-to-GDP ratio affects the size of the resulting multiplier.==== Finally, we present results from robustness checks in terms of the solution method used to compute fiscal multipliers, considering multipliers that are computed not only from a linear solution method but also from more accurate global solution methods.====Our work is closely related to Boneva et al. (2016) and Ngo (2019), who also study the consequences of a flattening of the Phillips curve for fiscal multipliers, which, however, in their setting is due to an increase in price rigidity parameters. Two further, recently published papers also emphasize the importance of the slope of the Phillips curve for the conduct of monetary policy at the zero lower bound, or for the value of the fiscal multiplier. Belgibayeva and Horvath (2019) explore how the degree of strategic complementarity in price-setting affects optimal monetary policy in a New Keynesian model with wage and price setting frictions. Linde and Trabandt (2018) find that strategic complementarity, introduced via a Kimball consumption basket instead of the constant-elasticity-of-substitution (CES) aggregator, accounts for the difference between the value of the multiplier calculated from the linear and non-linear solution of the model.====Other related contributions include (Miao and Ngo, 2019), who find that the multipliers behave differently in the non-linear Calvo and Rotemberg models. Surprisingly, they find that the mutliplier is increasing (decreasing) with the duration of the ZLB in the Calvo (Rotemberg) model. They also find that the spending multiplier is a non-linear function of the persistence of the government spending shock. Eggertsson and Singh (2016) argue that the multipliers do not differ a lot across the linear and non-linear New Keynesian models (with either Calvo or Rotemberg pricing) as long as we consider empirically realistic calibration of the models. Boneva et al. (2016) also show the sign and size of the multipliers with respect to the slope of the NKPC and the duration of the zero lower bound using the linear and non-linear New Keynesian model with Rotemberg pricing. Importantly, they show that the labour tax cut multiplier is negative for empirically realistic durations of the zero lower bound in the linear as well as the non-linear New Keynesian model. Ngo (2019) uses US data to calculate the unconditional probability of hitting the zero lower bound and calibrates a model with occasionally binding zero lower bound constraint. He finds a government spending multiplier of around 1.25, which is larger than the one in the model without occasionally binding constraint or transient government spending shocks. He also confirms the finding of Miao and Ngo (2019) regarding the nonlinearity of the multiplier with respect to the persistence of the government spending shock. The focus of our paper differentiates us from the previous papers. In particular, we explore how the recent flattening of the Phillips curve as resulting from a higher degree of strategic complementarity, and show that this affects the size of fiscal multipliers significantly.====Hills and Nakata (2018) show that the government spending multiplier is very sensitive to the inclusion of interest rate smoothing in the Taylor rule. Once one allows for inertia in the interest rate rule, the multiplier decreases from 1.9 to 0.5. Leeper et al. (2017) estimate fiscal multipliers using Bayesian methods on US data. With several combinations of model specifications and different priors they find impact multipliers of about 1.4. Further, they find that multipliers are much higher in a regime with passive monetary and active fiscal policy relative to a regime with active monetary and passive fiscal policy.====The paper proceeds as follows. Section 2 lays out our modelling framework, while Section 3 describes the equilibrium of the model. Section 4 discusses intuition and economic channels at play to help interpret fiscal multipliers. Section 5 focuses on the calibration of the model. Section 6 contains the numerical results as well as an explanation of the sign and magnitude of fiscal multipliers. Section 7 presents results from a non-linear solution method to verify robustness of our results. Section 8 provides concluding remarks. An Appendix with the model derivations can be found at the end of the paper.",Determinants of fiscal multipliers revisited,https://www.sciencedirect.com/science/article/pii/S0164070418301794,28 October 2019,2019,Research Article,220.0
Klomp Jeroen,"Development Economics Group, Wageningen University, P.O. Box 8130, Wageningen 6700 EW, the Netherlands,Netherlands Defence Academy, Breda, the Netherlands","Received 25 September 2018, Revised 17 October 2019, Accepted 17 October 2019, Available online 23 October 2019, Version of Record 28 March 2020.",https://doi.org/10.1016/j.jmacro.2019.103164,Cited by (17),This study explores whether the economic consequences of earthquakes affect the ,"One of the main global challenges for sustainable economic development in the next decade is to limit the economic consequences of natural disasters. Since the 1970s, the damage-related costs of these events have risen dramatically. Record losses of some US$380 billion were recorded in 2011, the year of the Tohoku earthquake in Japan, equivalent to 0.9% of global GDP (Guha-Sapir et al., 2015). A common conclusion that is shared among most studies is that natural catastrophes put the short-run macroeconomic performance of many countries under considerable downward pressure (i.e., Noy, 2009; Loayza et al., 2012; Felbermayr and Groschl, 2014). It is therefore expected that these events will influence the monetary policy decisions taken by the central bank in their attempt to stabilize the economy again. However, one problematic concern is that natural disasters create a classic monetary policy challenge: how to use one instrument to accommodate the real shock in the short-run with the objective of anchoring inflation when these two competing objectives demand opposite policy actions. The optimal interest rate policy in the period following a disaster should balance this trade-off. On the one hand, natural disasters cause a shortfall in the output produced leading to a temporary deviation from the balanced growth path. Meanwhile, private consumption and investment will drop due to a lack of borrowing opportunities in many countries (Loayza et al., 2012; Klomp, 2016; Fomby et al., 2013; Felbermayr and Groschl, 2014). To stimulate economic recovery, central banks should follow an expansionary policy and reduce the policy interest rate. However, loosening the monetary stance at a time when domestic output is temporarily low, may risk fuelling inflation. On the other hand, natural disasters raise the marginal costs of domestic producers creating an upward pressure on the inflation rate (Heinen et al., 2016; Parker, 2018). In order to stop the inflation rate from rising any further, central banks need to adopt a more contractionary policy by raising the nominal interest rate. However, following this policy will depress the output even more. This dilemma demonstrates that central banks are unable to expand output and reduce the inflation rate at the same time after a disaster. Thus, whether monetary policy should be loosened or tightened in the wake of a natural disaster is contested and theoretically a priori not clear. One can therefore argue that the question of whether, and if so, in which direction natural disasters affect monetary policy is ultimately an empirical one.====Surprisingly, there has been hardly any empirical research on the monetary policy aspects of disasters. The existing studies I am aware of are mainly based on the calibration of general equilibrium models using a simple policy rule sharing the same objectives of output and price stability.==== For instance, Keen and Pakko (2011) evaluate the optimal interest rate response by the Federal Reserve after hurricane Katrina that struck the US economy in 2005. Their findings of a calibrated DSGE model indicate that the optimal monetary policy response should involve raising the short-run nominal interest rate. This contractionary policy is mainly motivated by the anti-inflationary bias of the Federal Reserve suggesting that the nominal interest rate should respond primarily to higher inflation rather than to lower output. Besides, the nominal policy rate should be tightened to accommodate the increase in the real interest rate caused by the widespread destruction of physical capital needed in the production and the related upsurge in the marginal product of capital. In contrast, a number of subsequent studies find opposite results. For instance, White (1997) argues that because the first-round effects of inflation following a disaster are only temporary, local and concentrated in specific sectors, monetary policy should prioritize growth over low inflation. Besides, keeping monetary policy expansionary will also provide additional liquidity for the financial system needed to finance recovery efforts. Moreover, Isoré and Szczerbowicz (2017) calibrate a New-Keynesian model. Their findings demonstrate that disasters do not only cause a supply shock, but also induce a negative demand shock. Higher disaster risk is associated with an increase in the discount rate reducing agents’ propensity to consume and puts a downward pressure on real factor prices, real marginal costs and inflation. The policy interest rate of the central bank must therefore diminish to withstand deflationary pressures and to stimulate aggregate demand again. Likewise, the simulation results reported by Brede (2013) predict that natural disasters cause a recession in the economy as households have only limited saving and borrowing opportunities and are therefore not able to completely smooth investment and consumption. Consequently, the drop in demand encourages firms to cut prices. According to the Taylor principle, the nominal interest rate should fall as both output and inflation are below their long-run equilibrium levels. Finally, Fratzscher et al. (2018) find that the policy outcome relies on whether or not a particular policy rule is being followed by the monetary authorities. In more detail, they find that central banks that have a strict inflation target raise the nominal interest rate in the first quarter after a natural disaster, while central banks with a more lenient inflation target reduce the policy rate.====So far studies that use ex-post interest rate data to explore the response of the central bank after a natural disaster are lacking. The contribution of this paper is to fill this gap. For this purpose, I use a quasi-experiment in which the frequency of earthquakes is taken as my measure of natural shocks hitting the economy. A simultaneous equation model is estimated covering more than 85 countries over the period 1960–2015. The choice for earthquakes as my indicator for natural disasters is based on a number of reasons. First, these disasters occur rather exogenously as they cannot directly be influenced by human behavior and are not related to the ongoing process of climate change. Second, although earthquakes happen mainly along the fault lines between two tectonic plates on land or the ocean floor, it is still hard to predict or forecast the timing of an event or develop an effective early warning system.==== Third, it is difficult to take precautionary measures that reduce the vulnerability of the population for geophysical disasters other than enforcing strict building codes or zoning rules. Fourth, earthquakes are more equally divided across LDCs and industrialized countries compared to other disasters such as floods or droughts. This reduces the possibility that my empirical results later on suffer from a sample selection bias. Finally, compared to other natural disasters, earthquakes have the largest real economic impact. Based on the estimated damage figures reported by EM-DAT, the amount of damage per affected caused by a single earthquake is on average about 2.5 times larger than, for instance, for a major flood. Even more, about half of the total reported physical damage related to natural disasters is attributed to earthquakes (EM-DAT, 2015).====To estimate the empirical impact of earthquakes, I construct an exogenous measure that is related to the frequency and intensity of these events based on the information provided by the “Global Significant Earthquake Database” collected by the National Oceanic and Atmospheric Administration (NOAA). In total, I consider around 400 major earthquakes in about 85 countries. As my dependent variable, I use the end of year policy interest rate set by the central bank.====The most important empirical results reported in this study indicate that on average the policy interest rate falls within the first year after the earthquake. This finding implies that the monetary authorities prioritize short-run economic recovery above price stability after a natural shock. Though, after about two years the central bank raises the policy rate again to curb the inflation pressure. Within three years the earthquake effect has disappeared and the policy rate is almost on the same level as before the incident. However, it turns out that the direction of the short-run interest rate response depends to a certain extent on a number of monetary policy characteristics. For instance, central banks that tie their monetary policy to a certain policy target (i.e., inflation targeting or pegging the exchange rate) tend to raise the interest rate in the period following a disaster to fight the inflationary pressure. In turn, monetary authorities that have much freedom in their policy decisions are more likely to lower the interest rate to stimulate economic recovery.====The remainder of the paper is organized as follows. In the next section, I discuss my theoretical foundation underlying my hypothesis. In section three, I describe my data and methodology used. In section four, I report my estimation results on the relationship between earthquakes and the interest rate set by the central bank. Finally, I end in section five with my conclusion and discussion.",Do natural disasters affect monetary policy? A quasi-experiment of earthquakes,https://www.sciencedirect.com/science/article/pii/S0164070418304026,23 October 2019,2019,Research Article,221.0
Abdulla Kanat,"Graduate School of Public Policy, Nazarbayev University, Kabanbay batyr 53, Nur-Sultan 010000, Republic of Kazakhstan","Received 5 April 2019, Revised 12 September 2019, Accepted 14 September 2019, Available online 17 September 2019, Version of Record 1 November 2019.",https://doi.org/10.1016/j.jmacro.2019.103160,Cited by (4),"This paper investigates labor market outcomes and their effects on aggregate productivity in Brazil and India. The empirical evidence points to the inefficient allocation of talent across occupations in both countries. Two main factors are identified as causes of the inefficiency: frictions in human capital accumulation and frictions in the labor markets. The resulting distribution of talent negatively affects aggregate productivity, which is examined by using an augmented Roy model. The model predicts that the elimination of barriers to human capital accumulation and in the labor markets in Brazil and India increases the output on average by 22–52% and 38–53%, respectively.","The question of allocation of talent in Brazil and India is well studied in the literature, where the allocation of talent refers to the occupational distribution of groups categorized by gender and social groups – caste in India, and race in Brazil. However, the question of how this distribution affects the aggregate productivity in both countries is largely unaddressed. This paper intends to fill this gap.====In 1888, Brazil was one of the last countries to abolish slavery in the Americas (Bergad, 2007). During the three centuries of slavery in the region, it was the largest importer of slaves from Africa, and today, Brazil is home to the world’s largest population of African origin. The 2010 census reveals that about 50% of nearly 190 million Brazilians are considered black or brown.==== The colonial past of Brazil has affected the gap in socio-economic opportunities among population groups and created barriers to social mobility. As shown by a number of studies the racial prejudice and segregation in the country is more or less standard practice (e.g., Telles, 1992, Telles, 1995, Lovell, Wood, 1998). Black or mixed-race people tend to earn less than white Brazilians and work in low-skill occupations (Lovell, 1993, Telles, 2004, Loureiro, Carneiro, Sachsida, 2004). Moreover, there is a gap between the country’s white population and African descent in such an essential component of human capital as education. Until 2001, for example, the country’s top universities were primarily restricted to white Brazilians. Affirmative action in 2001, which implemented quotas in university admissions, gave non-whites the preferred access to higher education. Despite this, the labor market still favors the whites. In particular, the prestigious positions in management and administration remain the preserve of white men.====Historically, the unjust or prejudicial treatment of certain categories of people has affected the Indian population as well. It was mainly based on caste, a social stratification that divides society into hierarchically ranked groups. The ethnic groups known as Dalits were excluded from the caste system and were considered as untouchables. Today, Dalits are classified as Scheduled Castes (SCs), and other socially and economically disadvantaged indigenous ethnic groups are regarded as Scheduled Tribes (STs). By instituting affirmative action in 1950, the Indian government has tried to decrease the socio-economic gap among social groups in India. These actions were maintained by introducing quotas in the government sector jobs and admission to higher education for historically excluded groups (SC/STs). Despite these efforts, the living standards, health status, educational attainment, and labor market outcomes of SC/STs fall behind those of the so-called upper castes (Madeshwaran, Attewell, 2007, Thorat, Attewell, 2007; Deshpande, 2011; Kumar, 2011).====Differences in key socio-economic dimensions by gender are also significant in Brazil and India. Although there is evidence that male-female gaps in education have declined in recent years for both countries, it has not been translated into equal opportunities in the labor market. Many studies show that female labor force participation and wage rates lag behind those of male (Arabsheibani, Carneiro, Henley, 2003, Klasen, Lamanna, 2009, Madalozzo, 2010, Deshpande, Goel, Khanna, 2018). To some extent, these disparities in the labor market can be attributable to social norms or stereotypes that restrain women to unpaid housework, child care, and taking care of elderly or sick people (Morrison et al., 2007). For instance, in India, women devote ten times more hours than men to unpaid care responsibilities, out of which 85% goes into housework (Ferrant et al., 2014).====The gender and social disparities in the labor market outcomes show that a substantial part of the population in Brazil and India is not pursuing its comparative advantage, resulting in the talent misallocation. Recent studies have documented that the distortions in the allocation of talent have adverse aggregate effects. An example is Cuberes and Teignier (2016), who showed the negative impact of gender gaps in entrepreneurship on the allocation of resources and then on aggregate productivity. Hsieh et al. (2013) conduct time series analysis to address the question of whether an improved allocation of workers according to their talents is an important source of productivity growth in the US. Their study is motivated by substantial differences in the occupational choices between men/women and blacks/whites. They argue that the decline in the differences in the occupational distribution results in high productivity growth in the US.====This study is most directly related to Hsieh et al. (2013), and it differs from their study in that it conducts the cross-sectional analysis to assess the effect of allocation of talent on aggregate productivity in Brazil and India. The main forces driving the differences in the labor market outcomes between gender and social groups are the frictions in the labor market, frictions in the acquisition of human capital. An example of frictions in Brazil is that browns face unequal access to jobs and experience discrimination before labor market entry, and before affirmative action in 2001, they were restricted from elite colleges. If the frictions in Brazil are based on skin color, in India, they are based on other individual characteristics. For example, job applicants with upper-caste names are significantly more likely to be called for an interview than equally qualified applicants with lower caste names (Thorat and Attewell, 2007) or networks play an essential role in securing a job, they even influence the educational choice of people (Munshi, Rosenzweig, 2005, Munshi, Rosenzweig, 2016, Munshi, Rosenzweig, 2006). Additional forces that drive the wedge between female and male labor market outcomes are social norms or traditions, which retrict women to stay at home and discourage to invest in education.====The purpose of this study is to investigate the effects of the economic and social disparities among groups in Brazil and India resulting from frictions on the aggregate productivity of the countries. These two countries are given special attention both because they are large and because they are well known for the high prevalence of prejudice and discrimination towards certain groups. The study employs micro-level survey data from Brazil and India with detailed information on individual socio-economic and occupational characteristics. The analysis is performed on four groups (white men, white women, brown men, and brown women) in Brazil and four groups (other men, other women, SC/ST men, SC/ST women) in India.====Following the approach of Hsieh et al. (2013), the paper measures the differences in the occupational distribution across groups resulting from frictions to human capital accumulation and frictions in the labor market. Further, by using the Roy model of occupational choice, the paper evaluates the potential gains to output from decreasing the frictions in Brazil and India. The findings suggest that there are significant adverse impacts of the frictions in these countries. When the frictions are reduced by half the aggregate productivity increases by 9–13% in Brazil and by 14–15% in India. Eliminating the frictions increases the aggregate productivity by 22–52% in Brazil and by 38–53% in India.====The paper is organized as follows. Section 2 describes the data obtained from the Integrated Public Use Microdata Series. Section 3 discusses the model. Section 4 provides empirical evidence on earnings of various population groups and their occupational distribution. Section 5 presents the results of the model. Section 6 concludes.",Productivity gains from reallocation of talent in Brazil and India,https://www.sciencedirect.com/science/article/pii/S0164070419301442,17 September 2019,2019,Research Article,226.0
"Arin K. Peren,Braunfels Elias,Doppelhofer Gernot","Zayed University, UAE,Oslo Economics, Norway,NHH, Norway,CAMA, Australia,CESifo, Germany","Received 8 May 2018, Revised 3 September 2019, Accepted 4 September 2019, Available online 12 September 2019, Version of Record 27 September 2019.",https://doi.org/10.1016/j.jmacro.2019.103158,Cited by (16)," on medium-to-long-run growth. With regard to timing of short-to-medium run effects, our results show that most effects occur with a lag of two years.","The effects of fiscal policy on economic growth received renewed attention during the aftermath of the global financial crisis. What role taxes and public spending play as determinants of economic growth continues meanwhile to be widely debated in the theoretical and empirical literature. Neoclassical growth models suggest that taxes and spending have no effect on long-run economic growth but may have transitional effects on output levels (Solow, 1956). In contrast, endogenous growth models predict that fiscal policy can have both transitory and permanent effects on economic growth (cf. Barro, 1990, Jones, Manuelli, 2005). However, the effects predicted by economic theory are heterogeneous across different categories of fiscal policy variables and the timing of the effect (Jones and Manuelli, 2005). Public spending may contribute positively to output growth, for example in the form of investment into productive public infrastructure, whereas public consumption could either have no effect or a negative effect. Jones et al. (1993) argue that deviations from optimal (Ramsey) taxes have sizable negative effects on growth rates in models of endogenous growth. Piketty et al. (2014), on the other hand, show that a change in income tax rates may not influence the growth rate.====Empirical studies come to differing conclusions as to which kind of taxes and spending matter for economic growth. For example, Lee and Gordon (2005) focus on top statutory (=marginal) tax rates and provide evidence that top corporate tax rates negatively affect growth in a cross-section of countries. Conversely, Kneller et al. (1999) and Bleaney et al. (2001) analyze the effects of average tax rates, defined as tax revenue from the corresponding taxes as a percentage of GDP. They find, for a panel of OECD countries, that taxes classified as distortionary based on economic theory (i.e. income and corporate taxes) have a negative growth effect while other tax measures have no effect. For the United States, Aschauer (1989) finds positive effects of public investment on output growth, but the robustness depends on the specific measure of spending used and on the time frame (cf. Kocherlakota and Yi, 1996). The relation between taxes, public spending, and economic growth is therefore characterized by model uncertainty, which casts doubt on the robustness of existing results.====This paper explicitly addresses uncertainty about the relation between fiscal policy and economic growth by using ==== (BMA) (cf. Fernandez et al., 2001, and Sala-i Martin et al., 2004). Model averaging assesses the robustness of the relationship between a set of possible growth determinants and economic growth ==== over the model space. The unconditional parameters do not depend on a particular model as they are averages of conditional parameters of all models within the model space. Thus, model averaging avoids bias stemming from the choice of one particular model.====With this approach we overcome the problem of model uncertainty and address open questions in the empirical literature regarding the relationship between fiscal variables and economic growth. First, we investigate a broader set of fiscal variables to test which have a robust and economically important effect on economic growth. Second, we analyze whether marginal (top) tax rates and average taxes (tax revenue as a percentage of GPD) have different effects on economic growth. Third, we test the timing of the impact of fiscal policy variables on economic growth. Finally, we allow for a larger set of alternative growth determinants proposed in the empirical growth literature, and the effects of fiscal variables can therefore be interpreted as holding other growth determinants fixed (ceteris paribus).====We construct and use a panel data set for 28 OECD countries for the period 1990–2013. Our data set has several advantages over previous data sets used in the literature. First, to improve consistency of the data we source fiscal variables from OECD databases.==== Second, we aim to increase the generality of our results by using a data set that covers a broader set of OECD countries. Third, the selected period ensures that our results are timely and directly interpretable within currently used measurement and accounting standards (these have changed over time) of fiscal data. We believe that these features increase the relevance of our results to researchers and policy makers.====We identify the effects of fiscal policy variables by assuming that policy variables do not react to future shocks to economic growth. This means that fiscal variables are assumed to be predetermined with respect to future income, which can be motivated by the budgetary process and by the argument that it takes time for policy makers to learn about and react to an economic shock (Blanchard, Perotti, 2002, Ramey, 2011). Moreover, we explore the timing of effects by comparing results for different lag length of fiscal variables. The findings clearly support the hypothesis of delayed effects from fiscal variables toward economic growth.====Our major findings can be summarized as follows.==== First, we find a robust effect of some but not all fiscal variables on economic growth controlling for other growth determinants.==== Second, we show that the structure of public spending and taxes matters for short-to-medium-run growth. On the expenditure side, productive government spending has a positive effect on economic growth, whereas other spending categories and overall government consumption do not have robust growth effects. On the revenue side, we find that average taxes have no effect, while ==== top corporate tax rates have a robust negative effect on economic growth. A positive cyclically adjusted budget balance also promotes growth. Third, partly different fiscal variables may affect medium-to-long-run growth. We find some, but limited, evidence for a positive effect of productive expenditure also in the medium-to-long-run and a negative effect of top income tax rates. Finally, regarding the timing of effects, most fiscal measures that we identify as robust growth determinants affect economic growth within two years.====These results provide several new insights to the existing literature on the relationship between fiscal variables and economic growth. We show that it is not average taxes, but top (marginal) tax rates that matter. Moreover, we document that while the top corporate tax rate is a robust determinant of short-to-medium-run growth, it is the top income tax rate that may be more important in the medium to long run. By doing so, we provide some empirical support to Lee and Gordon (2005). The documented positive effect of a cyclically adjusted budget surplus is in line with Adam and Bevan (2005), who argue that large budget deficits are detrimental to economic growth. Finally, we show that productive expenditures are growth enhancing, a result that is consistent with Kneller et al. (1999).====Our results also contribute to an ongoing debate about the timing of growth effects of fiscal policy. One part of the literature focuses on the long-run effects (cf. Kocherlakota, Yi, 1997, Bleaney, Gemmell, Kneller, 2001, Romer, Romer, 2010) by looking at the sum of the coefficients of lagged fiscal variables. Gemmell et al. (2011) find that the peak effects occur after one to two years, while aggregate effects can take up to eight years to fully materialize. We look further into the timing at which fiscal measures impact growth. We find some evidence that fiscal variables may also affect average yearly growth in the period five years after a change in fiscal policy, but the evidence is limited. However, our results for short-to-medium-run growth underline the conclusion that fiscal policy does matter contrary to some previous studies, including Mendoza et al. (1997). More importantly, we find that the most robust effects occur within two years, in line with the peak identified by the literature.====The remainder of the paper proceeds as follows. Section 2 discusses the empirical model and estimation method. Section 3 gives an overview of the data. Section 4 presents results and Section 5 concludes.",Revisiting the growth effects of fiscal policy: A Bayesian model averaging approach,https://www.sciencedirect.com/science/article/pii/S0164070418302039,12 September 2019,2019,Research Article,227.0
"Nikolsko-Rzhevskyy Alex,Papell David H.,Prodan Ruxandra","Department of Economics, Lehigh University, Bethlehem, PA 18015, US,Department of Economics, University of Houston, Houston, TX 77204-5882, US","Received 23 January 2019, Revised 5 September 2019, Accepted 11 September 2019, Available online 12 September 2019, Version of Record 28 September 2019.",https://doi.org/10.1016/j.jmacro.2019.103159,Cited by (7),"We use tests for structural change to identify periods of low, positive, and negative ","The Taylor principle that the nominal interest rate should be raised more than point-for-point when inflation rises, so that the real interest rate increases, has become a central tenet of monetary policy. Satisfying the Taylor principle is both necessary and sufficient for stabilizing inflation in a model with an IS Curve, Phillips Curve, and Taylor rule such as Taylor (1999b) and is sufficient but not necessary for determinacy of inflation in a model with a forward-looking IS Curve, a New Keynesian Phillips Curve, and a Taylor rule such as Woodford (2003).====The Taylor principle is embedded in the Taylor (1993) rule. According to the Taylor rule, the policy interest rate (the federal funds rate in the U.S.) equals the inflation rate plus 0.5 times the inflation gap, inflation minus the target inflation rate, plus 0.5 times the output gap, the percentage difference between GDP and potential GDP, plus the equilibrium real interest rate. With the target inflation rate and the equilibrium real interest rate both set equal to 2.0, the rule simplifies to the policy rate = 1.0 + 1.5 * inflation + 0.5 * output gap. With the coefficient on inflation being greater than one, the Taylor rule necessarily satisfies the Taylor principle.====The converse, however, is not correct, as satisfying the Taylor principle is necessary, but not sufficient, for adhering to the Taylor rule. There are four elements in the Taylor rule, which we call the “Taylor principles”. The first element, discussed above, is that the coefficient on inflation equals 1.5. Following standard practice, we say that the first Taylor principle is satisfied if the coefficient on inflation is greater than and significantly different from one. The second element is that the coefficient on the output gap equals 0.5, so that the nominal (and real) interest rate increases when the output gap rises. We say that the second Taylor principle is satisfied if the coefficient on the output gap is greater than zero, less than one, and significantly different from both zero and one. Both the first and second principles are symmetric, so that the real interest rate decreases when inflation and/or the output gap falls. Satisfying the first and second principles serves to stabilize business cycle fluctuations.====The third element is that the target inflation rate equals 2.0 percent. This target has been adopted either implicitly or explicitly by many central banks, and has been an explicit target of the Fed since January 2012. The fourth element is that the equilibrium real interest rate be constant and equal to 2.0 percent. Because neither the inflation target nor the equilibrium real interest rate are estimated, the criteria for satisfying the third and fourth Taylor principles will not have an exact statistical interpretation.====Most research on Taylor rules focuses on the first principle. Taylor, 1999a, Clarida et al., 2000, Boivin, 2006, and Coibion and Gorodnichenko (2011) are among the many researchers who have found that the Taylor principle was satisfied in the 1980s and 1990s, but not in the 1960s and 1970s.==== These papers estimate Taylor rules for a “pre-Volcker” period ending in 1979 and a second period starting between 1979 and 1987. While the papers report estimates for the coefficients on inflation, the output gap, and (usually) the intercept, the focus of the discussion is whether or not the coefficient on inflation is greater than one.====The second principle became prominent in the policy, although not the academic, literature following the Great Recession. Yellen (2012) argued that a modified Taylor rule, with a coefficient of 1.0 instead of 0.5 on the output gap, was preferable to the original Taylor rule.==== In contrast to the original Taylor rule, the modified rule implies negative policy rates starting in 2009 which, combined with the zero lower bound on the federal funds rate, provides a justification for quantitative easing and forward guidance. The third principle, that the target inflation rate equals 2.0 percent, has been questioned in the aftermath of the Great Recession by, among others, Wiliams (2009), who argues that a 2 percent inflation target may provide an inadequate buffer against the zero lower bound. The fourth principle, that the equilibrium real interest rate equals 2.0 percent, has traditionally received relatively little attention in the policy analysis literature. This has changed, however, as Summers, 2013, Summers, 2014) advocated conducting policy based on an equilibrium real interest rate that is zero or even negative and Yellen (2015) argued that the federal funds rate should be lower than that prescribed by the original Taylor rule for a considerable period of time going forward because the equilibrium real interest rate was close to zero.====This paper proposes a new approach to policy evaluation with Taylor rules. Instead of choosing periods ==== before and after 1979, we select periods endogenously based on Taylor rule deviations, defined as the difference between the federal funds rate and the policy rate prescribed by the original Taylor rule described above. Using structural change tests, we divide the sample into various periods and estimate Taylor rules over the periods. We then investigate the implications of altering the original Taylor rule to incorporate a higher coefficient on the output gap and/or a time-varying equilibrium real interest rate. We use real-time data on real GDP and the GDP deflator from 1965:4 – 2015:3, the last quarter before the federal funds rate was raised above the zero lower bound, and construct output gaps using real-time quadratic detrending. We replace the federal funds rate with the shadow federal funds rate calculated by Wu and Xia (2016) between 2009:Q1 – 2015:Q3 when the federal funds rate was constrained by the zero lower bound.====The structural change tests for deviations from the original Taylor rule provide evidence of four distinct eras. There is a low deviations era, where the federal funds rate is close to the prescribed Taylor rule rate, during the Great Moderation period from 1987 to 2000, two negative deviations eras, where the federal funds rate was below the prescribed Taylor rule rate, during the Great Inflation period from 1965 to 1979 and the period from 2001 to 2015, and a positive deviations era, where the federal funds rate is above the prescribed Taylor rule rate, during the Volcker disinflation period from 1980 to 1987. Our results are broadly, although not exactly, in accord with Taylor (2012), who uses narrative methods to identify the late 1960s and 1970s as a period of discretionary policy, 1980 to 1984 as a transition, 1985 to 2003 as the rules-based era, and 2003 to 2012 as the ad hoc era.====Why do we care about high and low deviations eras? There is an extensive literature that considers the normative implications of Taylor rules. Taylor (1993) describes how the rule was derived from optimal policy in estimated macroeconomic models, Woodford (2003) emphasizes the importance of a time-varying equilibrium real interest rate for optimality, and Yellen (2012) discusses how a modified Taylor rule is closer to optimal policy from the Fed's macroeconomic model than the original Taylor rule. Taylor (2012) presents qualitative evidence that economic performance is better in “rules-based” than in “discretionary” eras. Nikolsko-Rzhevskyy et al. (2014) provides quantitative evidence that economic performance is better in low deviations eras than in high deviations eras using either the original or the modified Taylor rule as the prescribed rule and Nikolsko-Rzhevskyy et al. (2019) extend this result to almost all Taylor rules with a coefficient on the inflation gap of 0.3 or higher. While Bernanke (2003) argues that Fed policy should be conducted using “constrained discretion” that respects the dual mandate, policy rule prescriptions have been compared with the effective federal funds rate in the Fed's Monetary Policy Report since 2017.====We estimate Taylor rules for the various eras in order to understand (1) what factors contribute to the high deviations eras and (2) whether the factors were the same for the positive and the negative deviations eras. The coefficient on inflation is greater than and significantly different from one, so that the first Taylor principle holds, for the 1980 – 1987 and 1987 – 2000 periods. Between 1965 and 1979, the coefficient on inflation is close to and not significantly different from one, so that the first Taylor principle is not satisfied. This is in accord with much previous research, and reinforces the evidence that the violation of the Taylor principle was an important contributing factor to the high inflation in the 1970s. Between 2001 and 2015, the coefficient on inflation is greater than but not significantly different from one.====The coefficient on the output gap is relatively close to Taylor's postulated value of 0.50 and significantly different from both zero and one for all eras except for the Volcker disinflation period, where it is small and not significantly different from zero. Since the output gap was negative during most of 1980 – 1987, a larger response to the output gap would have lowered the policy rate and decreased the size of the Taylor rule deviations.====The largest differences in the estimates across the eras are in the intercept. According to the Taylor rule, the intercept is positively correlated with the equilibrium real interest rate and negatively correlated with the coefficient on inflation and the inflation target. Because the inflation target and equilibrium real interest rate both affect the intercept, they cannot be separately identified. The value of the intercept is only consistent with Taylor's postulated values for the inflation target and equilibrium real interest rate during the low deviations era from 1987 to 2000.====The results of the structural change tests illustrate the importance of all four Taylor principles. Monetary policy in the 1987–2000 Great Moderation period is well-explained by the original Taylor rule. Violations of the first Taylor principle that the coefficient on inflation should be greater than one account for deviations from the Taylor rule during the Great Inflation from 1965 – 1979. Violations of the second Taylor principle, that the coefficient on the output gap equals 0.5, contribute to, but do not account for, the large deviations during the Volcker disinflation from 1980 – 1987. Finally, the large deviations during 2001 – 2015 are difficult to understand in the context of violations of the Taylor principles.====By estimating Taylor rules for the low, positive, and negative deviations eras, we have identified how violations of one or more of the four Taylor principles contributed towards the Taylor rule deviations in the various eras. We proceed to analyze deviations from two alterations of the Taylor rule where one of the Taylor principles is violated. If the alteration results in a switch from either a positive or negative deviations era to a low deviations era, the violation of the principle can account for the Taylor rule deviations.====We first conduct structural change tests on deviations from the modified Taylor rule with a higher output gap coefficient of one. The major difference is that there is an additional break at the middle of 2007, producing a negative deviations era from 2000 - 2007 and a low deviations era from 2007 – 2015. Between 2007 and 2015 the policy rate responded strongly to the output gap and didn't respond at all to inflation. Not only is the coefficient on inflation not significantly greater than one, it is not significantly greater than zero. While these results are arguably in accord with Fed policies during an era of high unemployment and very low inflation, they are not consistent with any variant of the Taylor rule because the first Taylor principle is clearly not satisfied. The estimates are consistent with a low equilibrium real interest rate.====We proceed to conduct structural change tests on deviations from the original Taylor rule with a time-varying equilibrium real interest rate. We proxy the unobservable equilibrium real interest rate with the estimates in Laubach and Williams (2003). The deviations are low from 1987 – 1999 and 2007 – 2015. As with the modified Taylor rule, Fed policy from 2007 to 2015 is consistent with a rule where the federal funds rate responds strongly to the output gap but does not respond at all to inflation.====Policy evaluation in the context of the Taylor rule has been almost entirely conducted on the basis of the first Taylor principle that the nominal interest rate should be raised by more than point-for-point when inflation increases. We show that departures from the other Taylor principles, in particular the second Taylor principle during the Volcker disinflation period, are also important for understanding Taylor rule deviations.",The Taylor principles,https://www.sciencedirect.com/science/article/pii/S016407041930028X,12 September 2019,2019,Research Article,228.0
Zeida Teegawende H.,"Department of Economics, University of Ottawa, 120 University Private, ON K1N 6N5, CA","Received 22 February 2019, Revised 31 July 2019, Accepted 20 August 2019, Available online 7 September 2019, Version of Record 11 November 2019.",https://doi.org/10.1016/j.jmacro.2019.103156,Cited by (5),"The enactment of the Tax Cuts and Jobs Act in 2017 in the US, specifically the large ==== cut provision for corporate businesses is a major overhaul since the Tax Reform Act of 1986. However, the reform is not revenue neutral. This paper assesses the revenue-neutral trade-offs when the government ==== a ","The U.S. corporate tax is comprised of a relatively high statutory tax rate among OECD countries and a tax base narrowed by loopholes and tax planning strategies. In addition, the double taxation faced by traditional C-corporations as opposed to pass-through entities (subjected to one layer of taxation via individual tax code), distorts the allocation of capital among business legal forms of organization. Moreover, the recent wave of corporate inversions is gradually eroding the tax base, giving rise to the need for business tax reform for the Congress.==== Even in the mainstream debate, proponents suggest that vigorous reforms must be taken to make the U.S. business environment more competitive and attractive.==== The signing into law of the Tax Cuts and Jobs Act (TCJA 2017) is a first step toward fulfilling that goal. For instance, the statutory corporate tax rate has been cut permanently from 35% to 21%.====Despite the common ground on the idea of the business tax reform, an unanswered question still remains: ==== Inasmuch as a corporate tax cut - or (simply) a repeal - will generate a loss of revenues for the government that would have been used for social expenditures such as Medicare, Medicaid and other types of social transfers for needy families, a straightforward implementation might still induce unintended consequences. For instance, the Bush-era tax cuts, with its extensions to 2010 and 2012, have put a ==== threat on the U.S. economy by the end of December 2012 provided that the U.S. was technically surpassing its debt ceiling.==== With respect to the Trump’s tax cuts, even a dynamic scoring embodying the positive feedback of growth still adds $ 1.45 trillions to the current ballooned deficit of $ 20 trillions over the next decade.==== Therefore, a responsible fiscal reform must add a revenue-neutral condition as a supplementary criterion. The revenue-neutral reform can be achieved by closing loopholes or tax exemptions specific to particular investments and raising other tax rates such as labor income tax, dividend tax, and capital gains tax.====This paper adopts the latter alternative and looks for the optimal mix of reform for designing a corporate tax cut. In doing so, not only can one derive a fiscal balanced multiplier determining the extent to which any given tax raise offsets a corporate tax cut, but also ranks different reforms by welfare. A corporate tax cut is a step towards an elimination of the double taxation faced by the C-corporations. Accordingly, if the entity level of taxation is removed, then this levels the playing field between C-corporations and pass-through entities (S-corporations, partnerships, and sole proprietorships), making the legal form of organization irrelevant for where a firm must allocate its capital in tax planning strategy lens. In this case, the tax burden is transferred to the shareholders, instead. As already pointed out by Harberger, 1962, Harberger, 1966, the tax differential between the corporate sector and non-corporate sector leads to a misallocation of capital into the low return non-corporate sector. Furthermore, a high corporate statutory tax rate reduces a firm’s investment at the margin, given that the user cost of capital is high enough and the return on investment is low (Hall, Jorgenson, 1967, Auerbach, Aaron, Hall, 1983, Cummins, Hassett, Hubbard, Hall, Caballero, 1994, Gravelle, 1994).====I address the previous question in an environment where a representative household and a representative corporate firm take dynamic decisions in the presence of four types of tax rates - namely, labor income tax, dividend tax, capital gains tax, and corporate profit tax, all of which are used to fund a fixed government outlay. I first set out the fiscal trade-offs that a revenue-neutral policymaker faces when he is willing to reduce corporate profit tax while keeping expenditures unchanged. Accordingly, I introduce a notion of a ==== to capture these potential trade-offs. This procedure then generates various possible reforms to implement, but only a parsimonious set of reforms is analyzed herein. A follow-up step is the determination of the optimality using a welfare-maximizing criterion to discriminate the best alternative. A first result reminiscent of the ==== literature states that the long-run capital stock does not depend on dividend payments policy. That is, a mature firm prefers using retained earnings to finance its investment as opposed to redistributing dividends to shareholders such that accumulation decision and dividend payments policy are disconnected from one another.====Secondly, when labor income tax or dividend tax are individually used to make up the corporate profit tax cut, the balanced fiscal multiplier associated with the labor income tax is smaller than that of the dividend tax rate, and both are negative. Intuitively, since the household values leisure, its labor supply is elastic and it is capable of reducing labor supply while increasing leisure when labor income tax is raised. By contrast, given that the dividend tax does not directly affect capital accumulation in the long run, the high tax burden is transferred to shareholders. However, I find that there is no trade-off when only capital gains tax is used to offset the cut. In fact, the multiplier is positive, suggesting that a corporate tax cut can be coupled with a cut in capital gains tax. This result is not innocuous to the mature corporate firm set-up used in this paper, which relies on retained earnings to finance its investment. Subsequently, the latter reform ends up being the welfare-maximizing scenario. The worst-case scenario is when dividend and capital gains are both used to finance the cut. In addition, when the transition is accounted for, the ranking of reforms changes for some reforms with respect to the certainty equivalent variation. Although the best and the worst scenario reforms still remain unchanged, the final objective of the planner shifts their ranking. Indeed, if the government is only preoccupied with a revenue-neutral policy (the fiscal perspective), simultaneous hikes in dividend and capital gains taxes is preferred to the double cut in corporate profit and capital gains taxes because surpluses are generated along the transition. On contrary, when the policymaker is guided by the welfare-maximizing criterion, then the double cut scenario is the best one.====This paper is related to the large corporate finance literature. The earlier development found in Harberger (1962) discusses the incidences of corporate tax in the capital allocation made by the firms between corporate and non-corporate sectors. Since it is not possible to address corporate profit tax without underscoring dividend and capital gains tax, a link between the current paper and dividend and capital gains tax discussion is relevant. Depending upon which strand of view is considered, dividend tax might or might not impact the firm’s investment and dividend payment decision. Following the ====, high dividend taxes increase the user cost of capital and reduce the firm’s investment, generating lower dividends paid out to shareholders (Poterba, Summers, 1983, Gravelle, 1994). The ====, however, disconnects firms investment decision from its dividend policy provided that the firm uses retained earnings to finance rather than equity (King, 1977, Auerbach, 1979, Bradford, 1981). Unlike the previous papers, the current analysis uses a general equilibrium setup and derives condition for fiscal neutrality when the corporate tax is cut.====Another strand of literature is that related to the assessment of the 2003 dividend with heterogeneous firms in incomplete markets. Results on investment are mixed to the extent that temporary low dividend and capital gains tax rates have surged higher dividend redistribution by firms (Chetty, Saez, 2005, Gourio, Miao, 2010), or has generated no significant effect on capital accumulation (Korinek, Stiglitz, 2009, Yagan, 2015). An investment reduction is even found in Anagnostopoulos et al. (2012) when the dividend tax is cut in the presence of household heterogeneity. The current paper is built upon a representative firm and household to understand the basic mechanisms. Finally, the closest paper is Anagnostopoulos et al. (2014), where double heterogeneity is used to assess the best mix of different taxes in financing a corporate tax reform. To some extent, heterogeneity has a small effect on fiscal neutrality. That is, the tax ranges of labor income tax, in particular, are close in the two models. However, once one acknowledges the welfare-maximizing criterion, heterogeneity generates diametrically opposed results.====The rest of the paper is organized as follows. Section 2 presents the model and derives the balanced budget multipliers. Section 3 discusses the quantitative results of the benchmark economy. Section 4 evaluates the transitional dynamics implications when counterfactual proposals (21% and 0%) are implemented, and Section 5 concludes.",On the corporate tax reform: Coordination and trade-offs,https://www.sciencedirect.com/science/article/pii/S0164070419300795,7 September 2019,2019,Research Article,229.0
"Dupor Bill,Li Jingchao,Li Rong","Federal Reserve Bank of St. Louis, United States,School of Business, East China University of Science and Technology, China,Institute of Public Finance and Taxation, School of Finance, Renmin University of China, China","Received 17 August 2018, Revised 21 August 2019, Accepted 23 August 2019, Available online 24 August 2019, Version of Record 11 September 2019.",https://doi.org/10.1016/j.jmacro.2019.103157,Cited by (8),"This paper demonstrates how adding nominal wage rigidity to a standard, closed economy ==== model can ","While there is a substantial body of empirical research on the size of fiscal policy multipliers, there has been relatively less theoretical work on the issue.==== This paper presents the theoretical underpinnings of a mechanism that relies only on sticky wages and prices, which can lead to fiscal policy output multipliers that are greater than one.====As background, we observe that two workhorse modern macroeconomic models, the neoclassical growth model and the basic New Keynesian model, as typically formulated, both imply a negative consumption multiplier. Since government spending is treated as a deadweight resource loss, the taxes required to finance the spending reduce households’ after-tax wealth. The negative wealth effect leads households to reduce their consumption. A rise in government spending combined with a decline in consumption implies an output multiplier that is less than one.====From a modeler’s perspective, one way to overcome the negative wealth effect is for a government spending increase to induce a change in a relative price that encourages production. In our model, that price is the real wage. Specifically, our starting point is the assumption that the nominal wage is stuck at a high enough level such that equilibrium labor input is labor-demand determined.==== Then, an increase in the nominal price level will drive down the real wage. An increase in government spending will put the required upward pressure on the price level. The simplest explanation of the mechanism is given in Fig. 1.====At a lower real wage, producers hire more workers, which increases the equilibrium labor input. If the resulting increase in labor is sufficiently large, then there will be a summed increase in producers’ profits and workers’ wages so that households will be able to afford greater consumption, despite the tax increase arising from government spending.====Sections 2 and 3 present a stylized model in which we derived closed-form solutions for the equilibrium and prove theorems related to the real wage channel for fiscal multipliers. In that model, the nominal wage is a combination of two parts: one part adjusts according to the price level, while the other part stays fixed forever. In Section 4, we analyze a more traditional model with Calvo nominal wage adjustment. We deliver the same mechanism, however, we lose tractability and must rely on simulation methods.====The often-used logic motivating government purchases during recessions is that public spending puts more income into workers’ hands, allowing them to spend more. Because output is determined by the total demand for goods, rather than supply forces, the increased income will expand both equilibrium output and consumption.====In absence of other frictions, this thinking is fallacious when agents make rational consumption decisions. Since government purchases must be paid for, the unavoidable taxes (in either the present or the future) lead households to reduce consumption. Therefore, government spending will crowd-out private spending, leading to an output multiplier that is less than one. In contrast, to make the output multiplier greater than one, the economy should experience a movement in a relative price that encourages production. In the simple closed economy New Keynesian model, there are two relative prices: the real wage and the real interest rate. Much existing research that generates large multipliers works through the real interest rate channel.====Under the interest rate approach, the real rate should fall in order that consumption will rise. This requires that monetary policy be passive in the sense that the government increases the nominal interest rate in a less than one-for-one manner in response to increases in inflation. Under a passive rule, an increase in government spending drives up output which increases marginal cost. An increased marginal cost, in turn, drives up expected inflation. Under a passive monetary policy, higher inflation ==== the real interest rate.====While this mechanism is sufficient, it only works under a passive policy. Relying on this mechanism to explain large multipliers requires one to focus on periods when policy makers have chosen to pursue passive policies, even though these are known to have poor stabilizing properties in response to many shocks, or when the economy is stuck at the zero lower interest rate bound. In contrast, the mechanism we put forth relies on the real wage as the price channel that can generate a large multiplier. It works even when monetary policy is not passive.====Monetary policy is important in our model, because under an active policy, the real interest rate channel operates and puts downward pressure on consumption. In the theorems from our baseline models, a positive consumption multiplier will obtain when the real interest rate channel is weaker than the real wage channel.====There exists a wealth of evidence of nominal wage rigidity. Empirical microeconomic studies based on the Panel Study of Income Dynamics (PSID) or the Current Population Survey (CPS) provide strong support for nominal wage stickiness in the United States. For example, Altonji and Devereux (2000), Card and Hyslop (1997), Daly et al. (2012), Kahn (1997), and Lebow et al. (1995) examine the distribution of nominal wage changes and report a substantial spike at zero, indicating that the nominal wage stays constant over a year for many workers. The percentage of the sample with a constant wage in these studies ranges from 7% to 16%. Moreover, Gottschalk (2005) shows that the PSID data overstates the degree of nominal wage flexibility because of measurement error, and that adjusting for measurement error leads to remarkably fewer cuts in nominal wages.====Studies that use data from specific labor markets or individual firms in the United States find that the incidence of a zero nominal wage change is much more frequent than that reported in the PSID or the CPS. For instance, a telephone survey of individuals in the Washington, D.C., area in Akerlof et al. (1996) shows that 30.8% of respondents had no change in their base pay from the previous year and only 2.7% experienced wage cuts. Using data from a large financial corporation, Altonji and Devereux (2000) find that over 40% of the sample had zero nominal change; and that nominal wage cuts were received by only about 0.5% of salaried workers and 2.5% of hourly workers.====Barattieri et al. (2014) use data from the Survey of Income and Program Participation (SIPP) and show that the nominal wage changes with an average quarterly probability ranging from 21.1% to 26.6%. Other research offers evidence of nominal sticky wages using European data. Bihan et al. (2012) obtain data from French firms and report that a wage change occurs with a quarterly frequency of around 35%. Druant et al. (2012) collect data from a firm-level survey conducted in 17 European countries. The authors find that on average, firms adjust wages every 15 months. Fehr and Goette (2005) examine Swiss data and present a spike at zero nominal wage change. The authors also show that nominal wage rigidity persist even in periods of sustained low inflation. The mean monthly frequency of nominal wage changes is reported to be 9.9% by Avouyi-Dovi et al. (2013) using French data and 12.9% by Sigurdsson and Sigurdardottir (2016) using Icelandic data.====Researchers have also conducted interview surveys to explore the underlying causes of nominal wage stickiness. Bewley (1998) shows that the major reason for employers’ reluctance to cut wage is that they believe employee morale would be hurt. Blinder and Choi (1990) emphasize the perception of fair relative wages as a source of wage stickiness. Campbell and Kamlani (1997) find that downward nominal wage rigidity comes mainly from managers’ concern that cutting wages would induce the most productive workers to quit.","Sticky wages, private consumption, and Fiscal multipliers",https://www.sciencedirect.com/science/article/pii/S0164070419300394,24 August 2019,2019,Research Article,230.0
"Shy Oz,Stenbacka Rune","Research Department, Federal Reserve Bank of Atlanta, 1000 Peachtree St. NE, Atlanta, GA 30309, U.S.A,Hanken School of Economics, P.O. Box 479, Helsinki 00101, Finland","Received 3 May 2019, Revised 14 July 2019, Accepted 20 August 2019, Available online 23 August 2019, Version of Record 1 October 2019.",https://doi.org/10.1016/j.jmacro.2019.103155,Cited by (5),"We analyze how an increase in the degree of common ownership of firms in the same market affects consumption and investment. Such an increase is shown to reduce real investment and therefore intertemporal consumption. Overall, institutional investors’ common ownership of firms competing in the same market serves as a device for weakening market competition. The resulting increase in the price of acquiring shares with institutional investors then crowds out savings directed to real investments.","Institutional investors (such as mutual and pension funds) hold an increasingly high share of US publicly traded firms. Azar et al. (2018) estimate it to be in the 70–80 percent range, and Backus et al. (2019a) point out that in 2018 one of the four largest asset managers (Blackrock, Vanguard, State Street and Fidelity) was the largest shareholder for 88 percent of firms on the S&P 500 Index. As pointed out in several recent studies, frequently, such institutional investors have considerable ownership stakes in several firms competing in the same industry. He and Huang (2017) present evidence showing an increase, from below 10 percent in 1980 to about 60 percent in 2014, in the fraction of US public firms with common institutional ownership, such that the institutional owner simultaneously holds at least 5 percent of the common equity of other firms in the same industry. Similarly, according to Azar (2016), the share of S&P 500 firms having overlapping owners with at least 3 percent ownership stakes in firms operating in the same industry has increased from 25 percent to 90 percent in the decade between years 2000 and 2010.==== Seldeslachts et al. (2017) document an increasing trend of common ownership in Germany. However, in Germany, the pattern of common ownership by institutional owners is not a general economy-wide phenomenon and is restricted to specific industries, such as the chemical industry.====Some recent studies examine the hypothesis that a higher degree of common ownership relaxes competition. Azar et al. (2016) and Azar et al. (2018) present empirical evidence from the banking and US airline industries, suggesting that prices in these industries have risen in response to increased common ownership.==== Theoretical models exploring the effects of common ownership or overlapping ownership on various dimensions of market performance include O’Brien and Salop (2000), López and Vives (forthcoming), Newham et al. (2018), Vives (2019). Shy and Stenbacka (2019b) analyze the tradeoff between relaxed competition and enhanced diversification introduced by a higher degree of common ownership within an intraindustry framework with risk aversion. Motivated by these research approaches, economists as well as legal scholars have formulated policy proposals. Elhauge (2016) and Posner et al. (2017) propose rules restricting the possibilities for institutional owners to hold ownership stakes in several firms operating in the same industry.====This study analyzes the implications of common ownership from a perspective very different from the earlier ones by addressing the following research questions: (i) What are the effects of increased common ownership on savings, intertemporal consumption, and welfare under circumstances where individuals with a finite lifetime allocate their savings between institutional investors maintaining ownership in several firms competing in the same market? (ii) How does such common ownership affect the allocation of resources between financial investments to acquire ownership of institutional investors and real investments that enhance resources available to future generations? Analyzing these questions is important because individuals channel a significant proportion of their long-term savings into pension funds. These funds constitute a significant portion of the institutional investors described above.====We design an overlapping generations (OLG) model, where in each period the young consumers determine the allocation of their savings between consumption, acquisition of shares with institutional investors, and real investment financed via interest-bearing bonds. The real investment is assumed to promote the endowment of resources available to the subsequent generation. At old age, consumers collect dividends paid by institutional investors, sell their ownership shares to the young of a new generation, and collect their principal and interest on bonds, all to support old-age consumption.====We demonstrate that real investments decrease in response to a higher degree of common ownership that the institutional investors hold in firms producing in the same industry. Also, in a steady state equilibrium, the consumption of the young as well as old generation of consumers decreases with the degree of common ownership as long as the return on real investments exceeds that of holding shares in the institutional investors.====Common insights gained from static oligopoly models typically emphasize the uncontroversial view that weak competition tends to hurt consumers. Such a perspective focuses on competition between firms operating in the same relevant market and on consumption opportunities restricted to the relevant market in question. The present study adds another dimension by analyzing consumers in the role of investors seeking to optimize the net present value of lifetime consumption. More precisely, it focuses on consumers in their role as shareholders of institutional investors, say pension funds, under circumstances where these individuals have consumption opportunities outside the industry under consideration. We find institutional investors’ common ownership of firms competing in the same market to be a device for weakening price competition, which consequently diverts savings from real investments to the acquisition of financial ownership. The increased value of financial assets crowds out real investment, thereby reducing lifetime consumption and hence welfare. In other words, common ownership of firms competing in the same market generates a distortion in the intertemporal resource allocation of consumers, and this distortion adds to the traditional welfare loss familiar from static models of cross-ownership between firms operating in oligopolistic markets.====It is important to emphasize the distinction between a model of institutional investors’ common ownership of product market firms, and a model of cross ownership where each product market firm owns a (minority) equity share in a rival firm. The existing theoretical literature in industrial organization suggests that cross ownership tends to reduce competition no matter whether it focuses on the unilateral effects (see, Reynolds and Snapp, 1986 or Farrell and Shapiro, 1990) or the coordinated effects (see, Gilo et al., 2006). These theoretical predictions are also supported by empirical evidence, see Nain and Wang (2016). However, by design, the models evaluating the effects of cross ownership cannot evaluate how ownership links among firms affect the allocation of resources by consumers with finite lifetime. Our model is designed precisely to evaluate effects of such ownership links on consumption, real investments, and acquisition of ownership of institutional investors with shares in competing same-industry firms.====This study is organized as follows. Section 2 designs a static duopoly model in order to measure how the share value of institutional investors varies with the degree of their common ownership in firms competing in the same product market. Section 3 constructs an overlapping generations model of consumers who allocate their savings among young-age consumption, buying shares of institutional investors, and interest-bearing bond-financed real investments. Section 4 derives the equilibrium share value of institutional investors, allocation of savings to bond-financed real investment, and lifetime consumption. Section 5 presents the main results by characterizing the effects of varying the degree of common ownership on real investment and consumption. Section 6 extends the model to uncertain returns. Section 7 presents concluding comments. Appendices provide algebraic derivations for some results.",An OLG model of common ownership: Effects on consumption and investments,https://www.sciencedirect.com/science/article/pii/S0164070419301983,23 August 2019,2019,Research Article,231.0
Liu Xiaochun,"Department of Economics, Finance and Legal Studies, Culverhouse College of Business, University of Alabama, Tuscaloosa AL 35487 USA","Received 1 August 2018, Revised 4 August 2019, Accepted 12 August 2019, Available online 14 August 2019, Version of Record 20 August 2019.",https://doi.org/10.1016/j.jmacro.2019.103154,Cited by (9),". Finally, the forecasting performance decomposition suggests that modeling tail fatness in macroeconomic disturbances provides significantly better predictive content than the benchmark models.","Mishkin (2011) is concerned that the shocks hitting an economy, especially in economic recessions and crises, may exhibit excess kurtosis, commonly referred to as “tail risk.” Excess kurtosis implies that the probability of relatively large disturbances, either positive or negative, is higher than that would be implied by a Gaussian distribution. Fagiolo et al. (2008) show that in the majority of OECD countries, output growth rate distributions are well approximated by symmetric exponential power distributions with tails much fatter than those of a Gaussian (but with finite moments of any order). Their empirical results indicate that extreme output growth patterns tend to occur more frequently than what a Gaussian assumption would predict. Ascari et al. (2015) further support Fagiolo et al. (2008)’s finding of macroeconomic tail fatness by simulating real business cycle models with exponential-power distributions.====Alternatively, the recent literature has also applied Student-t distributions to investigate tail fatness of macroeconomic dynamics. Cúrdia et al. (2014) and Chib and Ramamurthy (2014) find evidence that models with a Student t-distributed shock structure are strongly favored in real data analyses over standard Gaussian models. Moreover, Clark and Ravazzolo (2015), Cross and Poon (2016), and Chiu et al. (2017) provide evidence that simultaneously modeling tail fatness (via Student-t distributions) and time-varying volatility is important to improve out-of-sample forecasting performance for the U.S. major macroeconomic variables.====The shape of a distribution is critical in selecting appropriate theoretical and econometric models for macroeconomic dynamics. From an empirical perspective, Fagiolo et al. (2008) interpret the emergence of fat-tailed distributions as a candidate new stylized fact for output dynamics. Hence, econometric estimation and testing procedures that are heavily sensitive to normality of residuals should be replaced with estimators and testing procedures that are either robust to non-Gaussian errors or based on thick-tailed errors. From a theoretical perspective, this emerging fact may be assumed for theoretical models so as to possibly improve their performance. In any case, the implications stemming from these models should first be checked against the assumption of fat-tailed data distributions.====The current literature has shown that both exponential power and Student-t distributions are capable of modeling macroeconomic tail fatness as discussed earlier. However, as shown in Section 4.1, these assumed thick-tailed distributions have very different statistical properties. For instance, the exponential power distribution includes all normal and Laplace distributions, and as limiting cases it also includes all continuous uniform distributions on bounded intervals of the real line. By contrast, as a special case of the generalized hyperbolic distribution, a Student’s t distribution converges to a Gaussian distribution as its degrees of freedom go to infinity.====Therefore, one cannot expect a priori that these assumed thick-tailed distributions would perform equally well in characterizing tail fatness of macroeconomic dynamics.==== In this regard, a critical question naturally arisen and yet being tackled is whether and in what ground one type of thick-tailed distributions is better than the other in characterizing tail fatness of macroeconomic variables. The goal of this paper is to answer this question empirically when data normality is rejected.====This paper hence offers two major contributions. First, I evaluate economic importance of modeling tail fatness by comparing the exponential power and Student-t distributions in multiple-period-ahead forecasts of macroeconomic dynamics. The models are specified with time-varying volatility in order to account for the caveat discussed in Cúrdia et al. (2014) regarding potential bias towards fat tails if volatility is restricted to be constant.====Several important results are highlighted as follows. First, ignoring changes in macroeconomic volatility can lead to a biased estimation of macroeconomic tail fatness, consistent with the findings in Clark and Ravazzolo (2015), Cross and Poon (2016), and Chiu et al. (2017). Second, the symmetric exponential power distribution performs better than the Student’s t and normal distributions for forecasting quarterly macroeconomic dynamics, while the symmetric Student’s t distribution with time-varying volatility is the best-performing model for forecasting monthly macroeconomic variables. Third, modeling tail fatness of macroeconomic disturbances has improved out-of-sample forecasts during the periods of economic recessions. Finally, the forecasting performance decomposition of Rossi and Sekhposyan (2011) suggests that modeling tail fatness in macroeconomic disturbances provides significantly better predictive content than the benchmark models.====The second main contribution is that this paper proposes a quantile-based approach to investigate the effect of potentially misspecified distributions on characterizing macroeconomic tail fatness. Particularly, the new approach decouples the measure of kurtosis to two separate components, namely quantile-based tailedness and peakedness, at chosen quantile levels. I further decompose the quantile-based tailedness to a right and a left tail component for characterizing asymmetric macroeconomic variables. This new quantile-based measure of tailedness is free of distribution assumptions (Koenker, 2005), so that it is useful for dealing with the concern by Cúrdia et al. (2014) that the fat tails delivered under the exponential power and Student’s t distributions could be the result of non-Gaussian distributions assumed for exogenous errors.====Importantly, this decoupling approach avoids ambiguous interpretations of tail fatness measured by moment-based kurtosis. As discussed in footnote 3, excess kurtosis quantifies both the fatness of the tails and the peakedness of a distribution relative to those of a normal distribution in one single statistic, so that it is unclear what it exactly measures (Brys, Hubert, Struyf, 2006, Ranciere, Tornell, Westermann, 2008). Owing to this issue, the empirical estimates of the quantile-based tailedness in this paper have shown that the assumed thick-tailed distributions tend to overestimate the degree of macroeconomic tail fatness, although the conclusion of macroeconomic tail fatness remains unaltered and is robust to distribution misspecifications.====The remaining of this paper is structured as follows. Section 2 proposes a quantile-based robust measure of tailedness, which is further decomposed to its right and left components for characterizing tail fatness of asymmetrically distributed macroeconomic variables. Section 3 reports the quantile-based estimates of tailedness for an extended set of the U.S. and U.K. macroeconomic variables. Section 4 fits the data to compare the performance of exponential power and Student-t distributions in modeling tail fatness of macroeconomic variables. Section 5 concludes the paper.",On tail fatness of macroeconomic dynamics,https://www.sciencedirect.com/science/article/pii/S0164070418303367,14 August 2019,2019,Research Article,232.0
Hwang Youngjin,"Department of Economics, Hanyang University ERICA, Ansan 15588, South Korea","Received 19 February 2019, Revised 24 July 2019, Accepted 8 August 2019, Available online 10 August 2019, Version of Record 18 August 2019.",https://doi.org/10.1016/j.jmacro.2019.103153,Cited by (12),"This study presents a flexible recession forecast model where predictive variables and model coefficients can vary over time. In an application to US recession forecasting using pseudo real-time data, we find that time-varying logit models lead to large improvements in forecast performance, beating the individual best predictors as well as other popular alternative methods. Through these results, we also demonstrate the following features of the forecast models: (i) substituting roles between the two key features of predictor switching and coefficient change, (ii) considerable variations in the model size (i.e., the number of predictors used) over time, and (iii) substantial changes in the role/importance of major individual predictors over business cycles.","A substantial amount of research has investigated the predictive ability of various variables regarding the probability of future recessions. The usual approach in the literature is to employ a binary forecast model (such as a probit model), where the recession indicator is a non-linear function of a set of potential predictors. Since Estrella and Hardouvelis (1991) and Estrella and Mishkin (1998) found that yield spread, the difference between long- and short-term interest rates, has strong predictive power for future changes in real economic activity and recession periods, several studies have reaffirmed the usefulness of yield spread (Chauvet, Potter, 2005, Kauppi, Saikkonen, 2008, Rudebusch, Williams, 2009). Furthermore, a number of recent papers have suggested other financial variables, such as equity returns, housing prices, and credit variables (Nyberg, 2010, Ng, 2012, Liu, Moench, 2016, Ponka, 2017) and sentiment variables (Christiansen et al., 2014), all of which have been found to be useful leading indicators of recession periods.====Most of these studies adopt a class of fixed models, in that the predictors and associated model coefficients do not change during the forecast periods. However, an increasing body of evidence suggests that the predictive content of many macro time series is unstable over time (e.g., see Ng and Wright, 2013). In particular, the issue of temporal instability has been pointed out in recession prediction regressions that use yield spread (Chauvet, Potter, 2002, Chauvet, Potter, 2005, Estrella, Rodrigues, Schich, 2003, Rudebusch, Williams, 2009). These suggest that the conventional approach based on fixed models may, due to their restrictive nature, fail to incorporate time-varying features in data, leaving room for improvement by adopting flexible models.====Motivated by these observations, this study extends the literature on recession forecasting in the following ways. First, we consider a range of variables for potential predictors of recession, covering various real as well as financial sectors, and allow their roles to vary over time. There is no evidence to suggest that a single predictor (or set of fixed predictors) will perform best at all times and for all forecast horizons; rather, it is more reasonable to postulate that useful predictors change over time with the evolution of business cycles. Second, given the additional possibility of changes in the data-generating processes over time, we incorporate time-varying coefficients into the predictive regressions. Even when a set of good predictors is identified, their role may change over time or vary in different phases of business cycles. In such cases, forecast models with constant coefficients would be unsuitable (even when estimated recursively).====These two extensions add great flexibility to model specifications and will be especially fruitful in the forecasting context, as forecast failures are frequently attributed to structural changes whose types and timings are often uncertain. However, at the same time, incorporating these features into a recession forecasting model is non-trivial and challenging since, with time-varying predictors, the number of possible models to consider would be very large, and, due to the non-linear nature of the forecast model, no exact derivation of a Kalman filter for time-variation in coefficients is available.====To address these problems, we use the algorithm proposed by Raftery et al. (2010) and McCormick et al. (2012) and extended by Koop, Korobilis, 2012, Koop, Korobilis, 2013, which is referred to as dynamic model averaging/selection (DMA/DMS). In this algorithm, to account for model uncertainty in a dynamic way, the forecast model is updated at each time point in a data-based manner through a prediction-updating algorithm similar to Kalman filtering and more posterior model probabilities or weights are allocated to models that have forecasted well in the past. Within each individual model, the coefficients can evolve over time in a state-space model framework, where we use Laplace approximation in updating steps, which has been found to be accurate (McCormick et al., 2012). Both the state-space model for the evolving coefficients and DMA/DMS for changing predictors are specified using versions of “forgetting,” which reduces a highly complicated model such as ours to a parsimonious representation and makes the prediction based on this method computationally feasible and attractive.====We apply these methods to a forecasting exercise based on Bayesian logistic models of US recession from 1967:01 to 2014:08. Our pseudo out-of-sample forecasting exercise is performed in an explicit real-time environment, replicating the information available as closely as possible when a forecast is made. In particular, contrary to prior research that usually assumes some fixed lag regarding the National Bureau of Economic Research (NBER) turning points announcements, we nowcast for the periods whose regimes are unknown, using available pseudo real-time vintage data for predictors and relevant information==== This attempt, accounting for time-varying uncertainty regarding turning point announcements in a realistic way, may add further practical benefit to the forecast models. We then compare the forecasting performance of the time-varying models with a wide variety of alternative forecasting procedures.====Our main findings are as follows. First, the models with switching predictors and time-varying coefficients work well for forecasting recessions, outperforming the ex-post identified individual best predictors for each horizon. In contrast, a fixed model that uses all the predictors at all times is useful only for a few medium horizons and the performance of the simple averaging of an individual predictor’s forecasts is also limited. Furthermore, the models that use a composite leading indicator as a single predictor performs poorly, worse than the individual best predictors in most cases. Intriguingly, while the two key ingredients of models (i.e., predictor switching and coefficient changes) play an important role in improving performance, we find that their roles are not complementary; gains from faster model/predictor switching seems to be offset or absorbed by time-varying coefficients, and vice versa.====Through these results, we also highlight the following features of time-varying forecast models. First, while time-varying models tend to be generally eclectic in selecting predictors (i.e., rather than relying on a few predictors exclusively, they use multiple predictors most of the time), there are substantial changes in model size (i.e., the number of predictors models use) over time – for example, more than five predictors switching in less than a year. Second, in a similar manner, we also demonstrate that the (relative) importance of several key indicators significantly changes over time. For example, while yield spread is found to be a key predictor for medium- and long-horizon forecasts most of the time, its importance has declined and been replaced in recent periods, particularly around the Great Recession, when short-term interest rates and housing prices turn out to be more useful. In addition, although several labor market indicators (such as the employment ratio and initial jobless claims) lose their importance after the mid-1980s, they seem to play an important role in recent periods. These results overall serve as further evidence for predictor/model switching and confirm the benefits of time-varying forecasting models that flexibly adapt themselves to the evolution of business cycles over time.====While it must be acknowledged that a range of other methods for predicting binary response variables are available,==== we restrict ourselves to the probit/logit framework to focus on the role of time-variation in predictors and associated coefficients in this popular class of models, thanks to its simplicity and ease of use.====In addition to extending the recession forecast literature in a flexible framework, this study is also methodologically linked to and builds upon the recent development of forecast combinations or averages of multiple models (see Timmermann, 2006 for a survey). While the weights are constant over time or constructed somewhat arbitrarily in the traditional model averaging scheme, model averaging in DMA is flexible, allowing the weights to be updated coherently in a data-based manner. Examples of DMA/DMS (with time-varying coefficients) in the forecasting literature include univariate linear regressions, in which target variables are inflation (Koop and Korobilis, 2012), equity returns (Dangl and Halling, 2012), housing prices (Bork and Møller, 2015), and exchange rates (Korobilis and Ribeiro, 2018), as well as multivariate linear models such as Bayesian vector autoregressions (VARs) (Koop, Korobilis, 2013, Koop, 2014). These all demonstrate that the DMA/DMS approach can be a useful addition to forecasting methods.====The rest of this article is organized as follows. Section 2 describes the model specification and algorithm, while we discuss the data and forecast strategies in Section 3. Section 4 provides the empirical results and discusses their implications before the paper concludes in Section 5.",Forecasting recessions with time-varying models,https://www.sciencedirect.com/science/article/pii/S0164070419300758,10 August 2019,2019,Research Article,233.0
"Corrado Luisa,Rossi Isolina","Department of Economics and Finance, University of Rome Tor Vergata, Via Columbia 2, Rome, 00133, Italy,World Bank, 1818 H Street NW, Washington DC, 20433, USA","Received 12 December 2018, Revised 7 August 2019, Accepted 8 August 2019, Available online 9 August 2019, Version of Record 11 November 2019.",https://doi.org/10.1016/j.jmacro.2019.103152,Cited by (1),"The recovery from the global crisis that erupted in 2007 shows that the decoupling between real and financial variables during the business cycle can lead to negative and long-lasting consequences for the economy. A key feature of the past global crisis in many countries is that the recovery in aggregate output has not been accompanied by a contemporary pick-up in lending flows to the private sector, rendering the recovery ====. This paper uses data on output and credit to study the relative roles of demand and supply drivers of credit growth during economic recoveries on a sample of advanced and emerging countries between 1980 and 2014. Using a simple endowment economy model, the paper shows that credit-less recoveries are correlated with liquidity shocks in real and financial markets and with the pace of private sector deleveraging. The empirical analysis shows that during these episodes demand-side frictions played a relatively larger role in predicting the occurrence of the episodes, reflecting weak demand for liquidity by the private sector in the aftermath of the crisis.","There is a unanimous consensus among economists on the central role played by bank lending in supporting economic activity, especially in the aftermath of severe economic downturns. Recent economic history shows that credit flows are characterized by boom-bust cycles which can lead to high market volatility and damaging consequences for the economy (Claessens et al., 2011). Financial frictions play a crucial role in determining the shape of these cycles and during recessionary episodes they can magnify the effects of macroeconomic fluctuations. The recent global crisis that erupted in 2007 sparked renewed interest on the drivers of such frictions and different methodologies have been developed to model new narratives of business and financial cycles. It remains unclear, however, whether these frictions translate into shocks to the demand or to the supply of credit (Adrian et al., 2013).====A key feature of the recent global crisis is that in many countries worldwide the recovery in aggregate output has not been accompanied by a contemporary pick-up in bank lending flows, rendering the recovery ====. During these episodes GDP growth is found to be on average between 2 and 3 percent lower than during normal recoveries, and investment remains weak and below pre-crisis levels with potentially negative long-term effects for the economy.====Credit-less episodes provide an ideal experiment to test whether financial frictions are predominantly supply or demand-driven, and to study their impact on the liquidity-output nexus during recoveries. At a theoretical level, credit-less recoveries could be the outcome of demand or supply-side constraints. Demand-side constraints to credit growth can arise from the reluctance of firms and households to resume borrowing in the aftermath of a recession due to weak growth and employment prospects. At the same time, weak credit demand could be driven by a deterioration of borrowers’ credit-worthiness position or excessive debt levels. On the contrary, supply-side constraints manifest through higher lending costs and could be the outcome of a temporary shortage of liquidity following a crisis accompanied by financial sector stress and excessive pressure on banks’ balance sheets. In this context, failure to restore adequate liquidity levels may result in a tightening of credit supply to the economy.====The paper uses a novel panel dataset on output and domestic bank credit for a sample of advanced and emerging countries between 1980 and 2014 to analyze the role of demand and supply-side drivers of bank lending growth during credit-less recoveries. Following the literature on the cyclical relationship between prices and output (den Haan, Wouter, 2000, Cooley, Ohanian, 1991, Pakko, 2000, Smith, 1992) among others), we exploit simple information on the correlation between prices and output during the recession phase of the cycle to identify demand and supply-driven recessions originating in the real and financial markets. We argue that during these episodes the evidence points to the prevalence of demand-side frictions to credit growth, reflected in lower demand for liquidity by the private sector in the aftermath of recessions. During these episodes output recovery takes place while the economy is deleveraging, i.e. the stock of credit is decreasing. However, as in Biggs et al. (2009) we show that as long as the pace of deleveraging is decreasing a rebound in output can occur in the presence of negative credit growth.====Our work is carried out in two steps. In the first one, we model credit-less recoveries in a simple endowment economy framework featuring representative infinitely lived households. In this model, credit dynamics are a function of the policy rate and of liquidity shocks. These shocks reflect frictions in financial intermediation and affect the value of liquidity needed to finance consumption and investment. Our paper is the first to provide a comprehensive general equilibrium framework to analyze creditless recoveries by analyzing demand and supply drivers of liquidity levels and growth in the economy.====In the second step, we test empirically these hypotheses and examine the role of demand and supply-driven recessions in influencing the likelihood of a credit-less recovery in our sample of advanced and emerging countries.====: Our work builds on the theoretical literature on credit constraints and is closely related to empirical studies on financial crises and credit-less recoveries. From an economic theory perspective, the relationship between output and credit has been analyzed in the context of macroeconomic equilibrium models featuring real and financial markets. A large body of literature concerned with the notion of ==== (Bernanke, Gertler, 1989, Bernanke, Gertler, Gilchrist, 1999 and Kiyotaki and Moore, 1997) has documented the channels through which financial markets affect the real economy. In these models, the interaction between real and financial variables stems from the need of firms to access funds in credit markets. Due to financial market imperfections, firms’ ability to access liquidity is dependent on the value of their financial wealth, which is assumed to be pro-cyclical. Shocks affecting the economy impact on borrowers’ credit-worthiness and on their ability to access funds to finance investment, contributing to amplify the magnitude of business cycle fluctuations.====The financial amplification mechanism can work through distinct channels, which can potentially affect both the demand and supply of liquidity in the economy. When credit constraints arise from high collateral requirements (Kiyotaki and Moore, 1997), a drop in the value of collateral during recessions limits borrowers’ ability to access funds resulting in a contraction in the amount of credit supplied. At the same time, a tightening of agents’ borrowing capacity can lead consumers to increase precautionary savings, while reducing consumption and ultimately credit demanded (Guerrieri and Lorenzoni, 2017). Recent empirical literature has also investigated the link between lending standards, informational asymmetries and access to credit during the business cycle. For example, Dell’Ariccia and Marquez (2006) present a framework showing that when informational asymmetries in loan markets become less severe, as may occur during the expansionary phase of the cycle, lending standards become less stringent. This leads to higher credit growth and to increased financial systemic vulnerability.====Credit demand and supply can also be affected by excessive leverage in the economy. A high debt overhang can make firms and households reluctant to resume borrowing in the aftermath of recessions, keeping credit demand low. Similarly, excessive indebtedness among financial intermediaries, including banks can prevent their ability to secure liquidity in the inter-bank market, thus affecting the liquidity supplied to the economy (IMF, 2013).====Chadha et al. (2010) study how liquidity effects affect monetary conditions in a simple endowment economy model, providing a comprehensive strategy to analyze the demand and supply drivers of liquidity levels in the economy.====Our work also contributes to the recent empirical literature on credit-less recoveries, examining the lending-growth nexus during the business cycle. The first authors to document these episodes were Calvo et al. (2006). In their seminal work on post-collapse crisis recoveries in emerging countries they name these episodes Phoenix Miracles, as output is found to “rise from its ashes” without any recovery in the stock of credit. The recovery in output is explained by a process of financial engineering whereby firms discontinue long-term investment projects in order to restore liquidity to finance activity amidst financial constraints.====More recent studies document significant cross-country heterogeneity in the distribution of post-crisis performance. Using the same sample of crisis episodes of Calvo et al. (2006), Huntley (2008) re-examines the evidence on post-crisis performance using GDP per-capita data. The path of post-crisis recoveries is characterized by a bi-modal distribution leading to two different types of recoveries: slow and fast recoveries. Sixty percent of all recovery episodes are found to take place in five years, accompanied by strong investment levels, credit and consumption; in the remaining cases, output recovery takes place in 15 years or more without any rebound in domestic credit.====Credit-less recoveries are also a central feature of business cycles in advanced countries (Abiad, Dell’Ariccia, Li, 2011, Claessens, Kose, Terrones) and tend to follow recessions accompanied by financial sector stress, including boom-busts in credit markets. Analyzing the financial cycles of a sample of advanced economies, Claessens et al. (2009b) show that recessions associated with strong credit contractions and house prices busts tend to be more severe and last longer. More often than not, following these episodes a recovery of the real economy occurs ahead of improvements in financial conditions. Similarly, Abiad et al. (2011) show that when credit-less recoveries are preceded by systemic banking crises, their frequency is more than three times higher. If a credit boom ==== a banking crisis precede the recovery, the relative frequency of such episodes in the sample of countries under analysis increases to 77 percent.====Other macroeconomic indicators positively correlated with the probability of credit-less recovery are the size of output contractions and the extent of external adjustment during the recession phase of the cycle (Sugawara and Zalduendo, 2013 and Bijsterbosch and Dahlhaus, 2015). During credit-less episodes sectors more dependent on external finance tend to grow disproportionately less then during normal recoveries (Abiad et al., 2011) and resources tend to depart from sectors which are more (bank) credit dependent (Coricelli and Frigerio, 2015).====Our work differs significantly from current studies. While existing literature predominantly focuses on the macroeconomic conditions preceding credit-less recoveries, our analysis focuses on the study of demand and supply dynamics underlying credit patterns during crisis episodes, using price-output correlations to disentangle the relative contributions of distinct (demand and supply) frictions during the recession phase. In particular, the work contributes to the above mentioned literature in several ways. First, it is the first work to provide a comprehensive theoretical framework to analyze credit-less recoveries, which places major emphasis on the role of demand and supply side frictions in credit markets to explain liquidity dynamics during the recovery phase of the cycle. Second, it employs a novel panel database of crisis episodes between 1980 and 2014 - including data covering the global financial crisis - to examine the relative contributions of demand and supply drivers of credit growth. By analyzing distinct drivers of output recovery, our work shows that subdued lending activity in the aftermath of recessionary episodes, reflects (on average) weak demand for liquidity by the private sector. These results suggest that when constraints to credit growth are mainly demand-driven, policy interventions aimed at stimulating aggregate demand making full usage of the fiscal and monetary levers should be prioritized.====The remainder of the paper is organized as follows. Section 2 presents the model and the testable assumptions. Section 3 describes the data and the empirical strategy and then discusses the results and presents the robustness analysis. Finally, Section 4 concludes.",Anatomy of credit-less recoveries,https://www.sciencedirect.com/science/article/pii/S0164070418305238,9 August 2019,2019,Research Article,234.0
"Kim Insu,Kim Young Se","Department of Economics, Chonbuk National University, 567 Baekje-daero, Deokjin-gu, Jeonju-si, Jeollabuk-do, 54896 Korea,Department of Economics, Sungkyunkwan University, 25-2 Sungkyunkwan-ro, Jongno-gu, Seoul, 03063, Korea","Received 12 July 2018, Revised 30 July 2019, Accepted 2 August 2019, Available online 5 August 2019, Version of Record 6 September 2019.",https://doi.org/10.1016/j.jmacro.2019.103139,Cited by (2),This paper studies a ,"This paper studies a simple dynamic stochastic general equilibrium (DSGE) model with Calvo-style sticky prices as well as information rigidity. Economic agents facing some degree of information friction due to costs of acquiring and processing information occasionally update their optimal plans and expectations of future changes in aggregate variables including inflation. By utilizing this dual-stickiness model, we scrutinize whether the model-implied inflation expectations are consistent with what the survey forecasts tell us. Moreover, its empirical performance is evaluated in accounting for the dynamic behavior of inflation forecast errors. Some of well-known implications from rational expectations models with assumption of complete information about economic environment are also revisited, as a basis for comparison with those from this model of inattentive agents. Employing a variety of shocks into the model, we place a special emphasis on the question what dominant driving forces are to understand the dynamic patterns of forecast errors in the data.====Most macroeconomic models contend that inflation expectations play a crucial role in explaining aggregate inflation dynamics. For example, the New Keynesian Phillips curve, which is a key ingredient of the standard dynamic stochastic general equilibrium models, associates current inflation with the forecast of future inflation. To utilize a macroeconomic model, expected inflation has been predominantly formed using rational expectations, which is the standard methodology for modeling expectations. Notwithstanding their theoretical appeal, the macroeconomic models under full-information rational expectations has not been quite successful to account for even some salient features of the dynamic behavior of inflation and forecast errors commonly computed from survey expectations.==== For instance, by examining the role of survey expectations in the inflation process, Adam and Padula (2011), Faust and Wright. (2013), and Fuhrer (2012) argue that survey expectations appear to be substantially different from those from a rational expectations model, and survey expectations better explain how the expectations of agents are formed. Indeed, short-horizon inflation forecasts from the Survey of Professional Forecasters (SPF) are found to have a crucial role in explaining inflation dynamics.====Despite the fact that there is ample evidence that the survey measures of inflation expectations are not rational in the sense that inflation forecast errors display substantial persistence and involve significant components that are predicted by macroeconomic fundamentals (Coibion, Gorodnichenko, 2015, Mankiw, Reis, Wolfers, 2004), the dynamic patterns of survey forecast errors and their driving forces are relatively less known.==== As economic agents and policymakers periodically review their forecasting performance allowing them to check their understanding of economic structure and potentially to promote their forecasting efforts, it is imperative to discuss with precision about the nature of forecast errors. For that purpose, we introduce a simple DSGE model with the nominal rigidities to address several important empirical issues on inflation forecasts and their forecast errors. This is motivated by the observation that economic agents do not respond every instant to incoming stream of news on economic environment due to costs of acquiring and processing information, and thus only infrequently update their forecasts and re-optimize their plans, while they are rationally inattentive between updating dates (Reis, 2006a, Reis, 2006b, Sims, 2003).==== In addition, a number of theoretical and empirical studies have suggested that, rather than increasing the model complexity or employing other frictions, the introduction of information rigidity can be an auspicious start to improve the empirical performance. That is, this empirical regularity is often found to be consistent with the implication of the sticky-information models, in which information disseminates gradually across economic agents, such as Andrade and Le Bihan (2013), Carroll (2003), and Mankiw and Reis (2002), to name a few.====To study the dynamic patterns of inflation forecasts and forecast errors, we first consider a DSGE model with a dual stickiness, sticky price and sticky information. This framework allows the effects of monetary policies on inflation and its forecast error to be considered. The complete model system yields the dual-stickiness Phillips curve and the dynamic IS curve used to describe the short-run determinants of aggregate output and inflation, given monetary policy and exogenous disturbances. Using quarterly data from the sample spanning from 1968:Q4 through 2008:Q4, the model is estimated with Bayesian estimation techniques. For each parameter, we employ prior distribution that is standard in the literature, and estimate the mode of the posterior distribution by maximizing the log posterior function. The estimation results suggest that, in general, the parameter estimates are quite similar to those found in previous studies (Carrillo, 2012, Coibion, 2010, Dupor, Kitamura, Tsuruga, 2010, Knotek, 2010), and the model with measurement errors in aggregate output and inflation as well as in average inflation forecast maximizes the log marginal likelihood among the empirical specifications considered.====With the estimated model, we examine the role of information rigidity in explaining some basic features of inflation forecasts measured with the SPF data. We begin with evaluating the model regarding its ability to explain survey expectations data in comparison with the full-information rational expectations model. In addition, how forecasting performance differs between high and low inflation regimes is investigated.==== After having shown that the model fits actual inflation and how agents think about future changes in inflation quite well, we address some important issues. Since this model allows us to study the effects of monetary policies along with various types of exogenous disturbances widely used in the literature, a special emphasis is placed on the factors that drive the observed dynamic patterns of forecast errors. First, with impulse response analysis, we examine how aggregate inflation and the forecasts of inflation respond to each of the shocks, “supply shock,” “demand shock,” “inflation target shock,” and “interest rate shock.” Next, as their dynamic response patterns differ in terms of persistence, how each shock has different effects on forecast errors is also investigated. Under the assumption of sticky information, an inflationary shock causes a rise in inflation while inflation forecast responses to the shock in a relatively sluggish manner, and thus it tends to yield positive and persistent forecast errors. Third, we also discuss how important the role of information rigidity is to explain inflation forecast errors that display substantial persistence. Finally, historical decomposition analysis allows us to identify the main driving forces of the observed dynamic patterns of inflation forecasts and their forecast errors, which is not much revealed in the literature.====Our empirical findings suggest some important conclusions. Inflation forecasts implied by our dual-stickiness model are more consistent with surveys of inflation expectations than those from a full-information rational expectations model. This model also dominates the model under rational expectations in its ability to account for why the actual inflation is systematically related with the forecasts of inflation and forecast errors, and for generating excessively persistent inflation forecast errors. For an inflationary shock, sluggish response of inflation forecasts produces positive and persistent forecast error dynamics, while rational expectations forecast errors do not exhibit any systematic pattern with variations in inflation. In addition, to explain moderately persistent forecast errors in the SPF inflation expectations, information rigidity appears to be inevitable to some extent. Historical decomposition analysis reveals that supply shock and measurement errors in inflation are found to be dominant forces driving movement of inflation forecast errors, while the secular shifts in inflation are generated mainly by supply and inflation target shocks. Moreover, the patterns of survey forecast errors that agents primarily under-predict inflation during the 1970s and have shown negative forecast errors since the early 1980s are well explained by this model. Finally, negative demand shocks that are attributable to the episodes of economic downturns are routinely associated with negative forecast errors.====The remainder of the paper is organized as follows. The next section introduces the DSGE model with inattentive agents. The model-implied average inflation forecast and forecast error are also discussed. In Section 3, structural parameters and shock processes for alternative model specifications are estimated with Bayesian estimation techniques. Section 4 discusses the performance of the model fitting the survey measure of inflation expectations and several important issues with regard to inflation and forecast error dynamics. Concluding remarks are contained in Section 5.",Inattentive agents and inflation forecast error dynamics: A Bayesian DSGE approach,https://www.sciencedirect.com/science/article/pii/S0164070418303033,5 August 2019,2019,Research Article,235.0
"Döpke Jörg,Fritsche Ulrich,Müller Karsten","University of Applied Sciences Merseburg, Eberhard-Leibnitz-Straße 2, Merseburg D-06217, Germany,University of Hamburg, Welckerstraße 8, Hamburg D-20354, Germany","Received 16 August 2018, Revised 12 July 2019, Accepted 18 July 2019, Available online 26 July 2019, Version of Record 14 August 2019.",https://doi.org/10.1016/j.jmacro.2019.103135,Cited by (9),We analyze the forecast accuracy for the periods before and after the ,"The ==== in the aftermath of the 2007–08 financial crisis marked a turning point for macroeconomics in more than one respect. For the first time in post-WWII history, numerous industrialized economies hit the “zero lower bound” of short-term nominal interest rates, thus limiting the power of monetary policy to kick-start the economy (Borio and Hofmann, 2017). According to some authors, this also altered the size of key macroeconomic indicators such as the fiscal multiplier (e.g., Ji and Xiao, 2016) that are relevant for macroeconomic forecasts. Moreover, in reacting to the zero lower bound, several monetary authorities implemented unprecedented policies in post-WW II times. Therefore, the estimated monetary reaction functions based on, say, the Taylor rule, show signs of modifications in central bank behavior after the crisis (e.g., Belke, Klose, 2013, Selgin, Sumner, 2017).====Another noteworthy aspect is, for most of the advanced countries, the recovery after the financial crisis has been weak as compared to previous upswings. Several reasons have been suggested as a possible cause for this: financial frictions cause slow recoveries (Christiano et al., 2015) or the “slow recovery” feature seems to be especially pronounced in the economy where a credit boom had gone bust (Schularick and Jorda, 2012). Ng and Wright (2013, p. 1149) report “Examining business cycles over many countries and over a long time period, most researchers find that recoveries from recessions with financial market origins are systematically different, and slower.” Jordà et al. (2013) also argue that recoveries after large financial shocks are slower than after usual downturns.====Similarly, Koo (2014) argues that “balance sheet recession” adjustments are responsible for the relatively weak recovery in most countries. For the German data we study, it should be noted that the recovery after the financial crisis is not necessarily weaker than after previous pronounced recessions in Germany but it is stronger than in other advanced economies (Storm and Naastepad, 2015). Owing to the strong interconnectedness of export-oriented Germany with other countries, macroeconomic forecasting is still challenging and forecasters are faced with uncertainties over a structural break in the underlying relationships.====Beyond that, some more long-term trends possibly change the situation for macroeconomic forecasters and policymakers with regard to the steady-state path the economy might approach in the medium run. For example, Summers (2014) argues that productivity slowdown and factors dampening demand had been masked during the long boom period preceding the financial crisis. The “secular stagnation” debate (Summers, 2014, Teulings, Baldwin, 2014, DeLong, 2017) has been revived partly based on empirical evidence of long-lasting negative ==== interest rates (Holston et al., 2017). In addition, Bianchi and Civelli (2015) and Ihrig et al. (2010) identify globalization as a factor changing at least the inflation dynamics. Perhaps, consequently, the dynamic aspects of the economy seemed to have changed, and several of them might still change. For example, the stability of the Phillips curve relation has become doubtful of late (IMF, 2013, Ball, Mazumder, 2011, Blanchard, 2016).====Furthermore, for the Euro area, the financial crisis has been an asymmetric shock, affecting member states in markedly different ways. This was the first pronounced shock of its kind for the Euro area. Additionally, economic policy reacted in different ways to the aftermath of the financial crisis. This in turn might have pushed the level of uncertainty over economic policy to unknown heights. The economic policy uncertainty (EPU) index of Baker et al. (2016) has not yet returned to its pre-crisis level, which might indicate a regime shift.==== In a text-mining exercise Fritsche and Puckelwald (2018) applied a modal word dictionary on texts explaining German business cycle forecasts and found a similar pattern after the ====.====As a response to the financial crisis and the large forecast errors related to the ====, the theoretical foundations of macroeconomic forecasting have also been called into question. While there is no consensus on the overall state of macroeconomics judgments range from something went fundamentally wrong with macroeconomics even before the crisis (e.g., Stiglitz, 2018) to a differentiated defense of modern macro (Reis, 2018).From a forecaster’s perspective, the underlying theoretical assumptions of macroeconomic forecasting have surely become shakier. The uncertainty over which models provide useful guidance for forecasting and related policy advice has surely increased.====Additionally, some theoretical considerations point to a change in the loss function underlying the forecasts. According to Clements (2019), the existing evidence is not consistent with rational expectations in a full-information environment. For example, he argues that to explain biased forecasts, inefficient use of publicly available information, or disagreement between forecasters, one has to refer to behavioral models of expectations formation. One possible model to explain the change in forecaster behavior is simple adaptive learning. As Clements (2019) points out, this model of learning performs poorly during times of rapid changes in the variable to be forecasted. Hence, the decline in real GDP (unprecedented for a long time) could have led the forecasters to change their forecasting attitude, especially the assessment on medium- or long-run growth perspectives. Such adaptive adjustment was reported to occur slowly before the financial crisis (Batchelor, 2007). From surveys on methods, theories, and ideological positions of German professional forecasters (Döpke et al., 2019) documented evidence (especially in the replies to open answer categories documented in Döpke et al., 2019) that the experience of the financial crisis led to more cautious behavior: more careful risk assessment, more regular forecast error evaluations, and newer methods. A more conservative stance on forecasting as a recipe for success was also discussed in the general business literature (Armstrong et al., 2015) after the crisis. Furthermore, the experience of the crisis might have led to an imprinting effect in the same manner as Malmendier and Nagel (2011) found, based on U.S. household data, that negative financial shocks have rather long-lasting effects on the attitude towards risk. Similarly, the crisis could have given rise to a substantial change in the way professional forecasters deal with risk assessments.====Some empirical studies support these considerations and find that forecasters’ underlying loss functions have indeed changed. Capistrán (2008) analyzed inflation forecasts of the Federal Reserve and separated the forecasts in accordance to the respective Federal Reserve Chairperson. He reports that the loss functions have been mostly asymmetric and have changed considerably with different Chairpersons. In the pre-Volcker era, the cost of underpredicting inflation was lesser than the cost of overpredicting. After this period until the data ends in 1998, the forecasters had an incentive to overestimate inflation.====Wang and Lee (2014) used a rolling window loss function estimation and found that the degree and direction of asymmetry in growth and inflation forecasts are time varying. The analysis examines the rationality of forecasts from the Greenbook and the Survey of Professional Forecasters and points out incentives to overpredict inflation rate during the 1980s and underpredict growth for the 1990s.====All in all, several factors imply that macroeconomic forecasting has become more challenging in the last couple of years; most of it is directly or indirectly related to the financial crisis and its aftermath. Against this background, we ask whether the German forecasters have learned from the failure to predict the ==== and whether this learning effect is visible in the standard measures of accuracy, unbiasedness, and efficiency. Furthermore, we ask whether forecaster behavior has changed. For example, forecasters might be more cautious in avoiding another overestimation of growth after a similar mistake before the recession, which results in a changed loss function.====These questions are important from a macroeconomic policy perspective, since, business cycle forecasts are an important input for fiscal and monetary policy and professional forecasters play a crucial role in forming the macroeconomic expectations of the public. Carroll (2003) emphasizes the latter. In his model, households derive their expectations from the media, which in turn reflects the rational expectations of professional forecasters. Since households update their expectations only partially and occasionally, the expectations of households as an aggregate adjust slowly to new information, while professional forecasters have rational forward-looking expectations.====Some studies have elaborated on related questions. Frenkel et al. (2011) addressed the question of whether the expectation formation process of professional forecasters has changed due to the crisis. Based on expectations from the ==== provided by the European Central Bank (see Garcia, 2003), the authors conclude that the core equations of applied macroeconomics, namely the Okun relation, Phillips curve, and Taylor rule have not changed for professional forecasters. Pain et al. (2014) argued that international organizations such as the OECD learned from the crisis and now pay more attention to global economic or financial developments. In a survey of related research, Castle et al. (2016) found that model misspecification does not by itself cause large forecast errors but structural breaks in the estimated relationships do.====In this study, we use a panel consisting of annual data from 1971 to 2017 covering 17 growth and inflation forecasts from 14 institutions and compare the standard measures of forecast accuracy for the periods immediately before and after the crisis. We use regression-based tests of unbiasedness and efficiency and test for parameter stability of these models. Furthermore, we ask whether the behavior of forecasters has changed since the crisis by analyzing their loss functions.====Our results indicate that there are only small differences in forecast accuracy on average between the periods before and after the crisis. The quantitative measures of forecast accuracy are nearly unchanged (growth) or indicate only slightly increased errors (inflation) after the crisis. Errors in predicting directional change, however, differ significantly between the two periods. Tests for the efficiency of the forecasts over the entire sample indicate that growth and inflation forecasts are inefficient. Performing rationality tests separately for the periods before and after the crisis confirms these findings.====As for growth forecasts, the number of overestimations exceeds the number of underestimations before the crisis, while the opposite is true for the period after the crisis. A significant change in the correlation between inflation and growth forecasts errors provides an additional hint of a change in forecasters’ behavior. In more formal analyses, the estimated loss functions are different in the two periods. For the period after the ====, the estimated asymmetry parameter points to incentives for underestimation of growth forecasts and overestimation of inflation forecasts, whereas the same parameter estimated with pre-recession data points to an incentive to deliver overly optimistic forecasts (growth) or a symmetric loss function (inflation). The 10-year rolling window loss function estimates show shifts in the level and direction of loss asymmetry and strengthens the impression of changed forecaster behavior after the ====. Summing up, the quantitative and qualitative measures of forecast errors do not imply a (strong) change in forecast quality, but the overall results support the hypothesis of changed forecaster behavior.====The remainder of the paper is organized as follows. Section 2 briefly describes the data. Section 3 discusses whether the accuracy of German business cycle forecasts has changed after the crisis. Section 4 examines whether forecasters’ behavior has changed based on estimations of implied loss functions. Section 5 concludes.",Has macroeconomic forecasting changed after the ,https://www.sciencedirect.com/science/article/pii/S0164070418303550,26 July 2019,2019,Research Article,236.0
Waddle Andrea,"University of Richmond, United States","Received 26 December 2018, Revised 22 July 2019, Accepted 24 July 2019, Available online 25 July 2019, Version of Record 8 August 2019.",https://doi.org/10.1016/j.jmacro.2019.103138,Cited by (2),"Slow rebounds in employment have become a salient feature of recoveries from recessions over the past few decades. During this time, U.S. production has become increasingly globalized. In this paper, I provide evidence that offshoring contributes to slow recoveries in labor markets. Using data from the Current Population Survey, I show that employment in offshorable occupations mimics employment in routine occupations, recovering more slowly than other types of occupations. Additionally, I use data from the Bureau of Economic Analysis on activities of multinationals to show that offshoring contributes directly to this phenomenon. I then provide a theoretical framework that rationalizes these observations in the context of a modified growth model.","For much of the post-war era, labor markets began to recover one to two quarters after GDP reached its trough. Moreover, this growth tended to be robust. However, after the three most recent recessions (1990, 2001, and 2007), labor recoveries have been both slow and weak relative to their predecessors, earning them the moniker “jobless recoveries.” Even when one accounts for the fact that GDP growth was slower in these recoveries than in earlier ones, the sluggishness of labor market recoveries is remarkable. Much attention has been paid to this change and the uncharacteristically high and persistent unemployment rate that followed the Great Recession renewed interest in the jobless recovery phenomenon. In this paper, I argue that jobless recoveries are related to trend growth in emerging markets and the related increase in global production opportunities. Emerging markets offer companies alternative means of expansion, but the costliness of reallocation encourages companies to wait until the potential benefit of reallocation outweighs the costs. Recessions provide an opportunity to reallocate because lower productivity in the advanced country lowers the relative cost of reallocating resources during recessionary periods. I offer evidence that this reallocation occurred over the same time period in which jobless recoveries emerged, and that occupations that are more easily offshored are the ones that recover most slowly from recessions. I then provide a counterfactual exercise which shows that if multinationals had expanded U.S. employment at the same rate that they expanded employment in developing countries, jobless recoveries could have been mitigated.====In order to explore my hypothesis further, I build a modified growth model in which multinational corporations choose to produce in either an advanced, high-productivity country whose productivity is not growing or in an emerging, lower-productivity country with growing productivity. The multinational produces a final consumption good using labor and managerial services==== which are produced in the advanced economy but can be reallocated and used for production in the emerging economy. There are two forces operating in the model. The first is the relative growth of the emerging country, which leads to a secular shift in production to the emerging market. The second mechanism slows this secular shift, ensuring that it occurs primarily during recessions: a cost of adjusting resources from one country to the next. The particular resource that is shifted in the model is managerial services. Adjustment costs cause the shift in production to occur primarily during recessions, leading to the emergence of jobless recoveries. Essentially, recessions are “cheap” times to reallocate resources.====I show that the theoretical model is consistent, under certain conditions, with the emergence of jobless recoveries. I then use a numerical exercise to show that falling production of the consumption good in the advanced economy does not coincide with falling GDP. Thus, without adjustment costs, the model produces increasing GDP and falling labor; moreover, labor productivity rises as factors are reallocated. With adjustment costs, recessions become a time when firms are willing to pay to make adjustments, shifting resources to the more efficient production location. Therefore, the model produces large and sustained drops in labor in the advanced economy following a recession, while GDP recovers as the emerging market grows and production shifts from the advanced to the emerging economy. Thus, the model is able to produce a jobless recovery. Additionally, the model is consistent with increasing income inequality across individuals in the advanced economy. This is due, in part, to a decrease in labor demand for the laborer households in the economy. It is also because labor by managerial households becomes relatively more valuable as productivity in the emerging market grows.",Globalization and jobless recoveries,https://www.sciencedirect.com/science/article/pii/S0164070418305366,25 July 2019,2019,Research Article,237.0
Thorbecke Willem,"Research Institute of Economy, Trade and Industry and Center for International Development 1-3-1 Kasumigaseki, Chiyoda-ku Tokyo, 100-8901 Japan","Received 11 January 2019, Revised 4 May 2019, Accepted 22 July 2019, Available online 22 July 2019, Version of Record 25 July 2019.",https://doi.org/10.1016/j.jmacro.2019.103137,Cited by (28),"Using three identification strategies, this paper finds that supply-driven oil price increases lowered U.S. ==== in many sectors before the shale oil revolution but not after. It also reports that oil prices are a priced factor in a multi-factor asset pricing model both before and after the shale revolution. While oil prices mattered in both periods, the beneficial effects of oil price increases on the U.S. stock market have risen and the harmful effects have fallen since U.S. oil production soared after 2010.","U.S. oil production fell steadily from 1990 until the 2008–2009 Global Financial Crisis (GFC). Beginning in 2010, as shale oil output came online, U.S. production soared (see Fig. 1). This paper investigates whether the impact of oil prices on the U.S. stock market differed before and after the shale revolution (SR).====Many predict that an increase in oil prices will lower stock prices and reduce growth for oil-importing nations such as the U.S. For instance, the IMF (2014), using its G20 economic model, reported that oil price increases after the GFC disrupt the macroeconomies of oil-importing countries. It forecasted that a 20% increase in oil prices would raise inflation in advanced economies by between 0.5 and 0.8 percentage points, lower GDP by between 0.4 and 1.9%, and decrease aggregate equity prices by between 3 and 8%.====Contrary to these predictions, Bernanke (2016) found that oil prices during the SR were positively correlated with U.S. aggregate stock returns. Using daily data over the June 2011 to December 2015 period, he reported that the log difference of the price of West Texas Intermediate (WTI) crude oil was positively correlated with the log difference of the Standard & Poor's 500 (S&P 500) stock price index. To control for the fact that increases in aggregate demand may raise both oil prices and stock prices, he used Hamilton's (2014) method to decompose oil price changes into changes driven by aggregate demand and residual changes driven by oil supply and other factors. Hamilton used the first differences of the log of copper prices, the ten-year Treasury constant maturity interest rate, and the log of the trade-weighted dollar exchange rate to capture the effect of aggregate demand on oil prices.====Bernanke (2016) noted that if investors retreat from both commodities and stocks during periods of high uncertainty, then shocks to volatility may cause oil prices and stocks to covary positively. To control for this, he regressed the daily change in the log of oil prices on the daily change in the log of the Chicago Board Options Exchange volatility index (VIX) together with the variables Hamilton (2014) employed to capture the impact of aggregate demand. Bernanke reported a correlation of 0.68 between S&P price changes and changes in the component of WTI prices explained by the VIX and demand factors. He found a correlation of 0.05 between changes in the S&P and residual changes in WTI prices. He questioned why there was a positive correlation between stock prices and oil price increases driven by supply factors, given the presumption that negative oil supply shocks that increase oil prices would decrease output and raise inflation in the U.S.====Bernanke (2016) stated that one explanation for why supply-driven oil price decreases did not benefit the S&P is that they damage the creditworthiness of oil-producing companies and worsen financial conditions. Obstfeld et al. (2016) similarly noted that low oil prices could lead to corporate defaults that roil the financial sector. They also observed that low oil prices make oil exploration and extraction activities less profitable and lead to large declines in energy-related investment.====In previous work Chen et al. (1986) investigated whether crude oil prices are a priced factor in a multi-factor asset pricing framework. Using monthly data over five-year periods they regressed a sample of assets on news of potential economic state variables. They then used the assets’ betas to the state variables as independent variables in cross-sectional regressions for each of the next 12 months, with the dependent variable being monthly asset returns. The coefficients from the cross-sectional regressions provide estimates of the risk premia associated with the state variables. They repeated this procedure for every year over the 1958–1984 sample period. Over the entire sample period and over several sub-sample periods, they found no evidence that there was a risk premium associated with oil prices.====Kilian and Park (2009) distinguished between demand and supply shocks in the oil market. They employed a monthly vector autoregression over the January 1975 to September 2005 period including global crude oil production, an index of real economic activity to capture global commodity demand, and crude oil prices. They reported that higher oil prices arising from oil-market specific demand shocks lowered stock returns. They also found that unexpected increases in the global aggregate demand for industrial commodities raised oil prices and stock prices. Increases in global oil production, on the other hand, had a much smaller impact on stock returns.====Ready (2018), citing articles by Chen et al., 1986, Kilian and Park, 2009, Huang et al., 1996, and others noted that authors sometimes found relations between oil prices and stock returns at various leads and lags but found only weak evidence of contemporaneous impacts. He asked where the oil price beta is. Ready identified demand shocks as returns to an index of oil producing firms that are orthogonal to innovations in the VIX index and supply shocks as oil price changes that are orthogonal to demand shocks and to changes in the VIX. Using monthly regressions over the 1986 to 2011 period, he found that oil price increases driven by supply shocks decrease aggregate returns and oil price increases driven by demand shocks increase aggregate returns. He also reported almost all stocks but especially consumer stocks are negatively impacted by oil supply shocks. He interpreted these results as implying that oil supply shocks work primarily by influencing consumer spending.====This study focuses on the contemporaneous response of stock returns to oil price changes and examines whether the response has changed before and after U.S. oil production surged in 2010. To do this it first uses Bernanke (2016) and Hamilton's (2014) approaches and daily data to distinguish between oil price changes due to demand and supply factors. The results indicate that after 2010 oil price increases driven by both demand and supply factors increase aggregate stock prices in the U.S. Before the SR, price increases driven by demand reduced aggregate stock prices while increases driven by supply factors had no effect. To shed light on why these responses differed before and after the SR the paper investigates how oil shocks affected industry stock returns. The coefficient on supply-driven oil price increases went from negative to positive for industrial machinery, industrial engineering, chemicals, commercial vehicles, and other sectors.====This paper then uses Kilian and Park's approach (2009) and monthly data to distinguish between oil price changes due to demand and supply factors. As Kilian and Park also found, the results indicate that orthogonalized oil price increases lowered stock returns before the SR. After 2010, however, they did not. Many of the sectors whose coefficients on supply-driven oil price increases changed from being negative to positive also had coefficients on orthogonalized shocks to oil prices that went from being negative to positive. These include industrial machinery, industrial engineering, chemicals, and commercial vehicles. Over the 1990–2007 sample period 30 industries were harmed by higher oil prices and six benefited while over the 2010–2018 period only seven were harmed and 11 benefited.====The paper next uses Ready's (2018) approach and daily data to calculate demand-driven and supply-driven oil price changes. When oil production was falling the coefficients on oil shocks driven by both supply and demand side factors were negative and insignificant for the aggregate stock market. After shale oil production soared these coefficients were both positive and were statistically significant for demand shocks. Examining supply-driven oil price shocks, over the 1990–2007 period 26 sectors were harmed by oil price increases and eight sectors benefited. Over the 2010–2018 period six were harmed and six benefited. This follows the pattern using Kilian and Park's (2009) approach that indicated that supply-driven increases in oil prices were negative for many stocks before the SR but not after. Among the industries that were harmed before the increase in oil production but not after are industrial machinery, industrial engineering, commercial vehicles, and many consumer-oriented stocks.====Finally this investigation revisits the question that Chen et al. (1986) studied of whether crude oil prices are a priced factor in a multi-factor asset pricing framework. To do this it employs iterated nonlinear seemingly unrelated regression techniques to estimate whether there is an economy-wide risk premium associated with oil prices. The results indicate that the risk price associated with the price of crude oil is negative and statistically significant both before and after the shale revolution. This implies that the price of oil is a state variable that influences the macroeconomy.====This study builds on the ones discussed above by examining whether the response of U.S. stock prices to oil prices differed before and after the shale revolution. Using several identification strategies, the findings indicate that oil price increases reduced financial wealth by lowering aggregate stock prices over the 1990–2007 period but did not have this effect after U.S. oil production accelerated in 2010.====The next section builds on Bernanke's (2016) and Hamilton's (2014) approaches and uses daily data to investigate how oil prices affect stock returns. Section 3 modifies Kilian and Park's (2009) approach and uses a monthly vector autoregression to examine oil market shocks and returns. Section 4 employs Ready's (2018) approach to examine these issues. Section 5 tests whether oil prices are a systematic factor in a multi-factor pricing model. Section 6 concludes.",Oil prices and the U.S. economy: Evidence from the stock market,https://www.sciencedirect.com/science/article/pii/S0164070419300114,22 July 2019,2019,Research Article,238.0
"Geiger Martin,Scharler Johann","Liechtenstein Institute, St. Luziweg 2, LI-9487 Bendern, Liechtenstein,Department of Economics, University of Innsbruck, Universitätsstrasse 15, Innsbruck A-6020, Austria","Received 27 October 2018, Revised 8 July 2019, Accepted 18 July 2019, Available online 19 July 2019, Version of Record 31 July 2019.",https://doi.org/10.1016/j.jmacro.2019.103134,Cited by (10),"We use survey data to study how consumers assess the macroeconomic effects of oil market shocks on the U.S. economy using a vector ====. To structurally decompose oil price changes into oil supply shocks, oil-specific demand shocks, and global business cycle shocks, we impose zero and sign restrictions, as well as elasticity bounds. We find that survey-based measures of ==== and unemployment expectations increase in response to shocks that result in higher oil prices, where revisions in unemployment expectations are less pronounced in response to oil-specific demand shocks and global business cycle shocks. We also find that our measure of ","How do people perceive macroeconomic developments? Several recent contributions have addressed this question. Carvalho and Nechio (2014) and Dräger et al. (2016) analyze whether consumers form expectations in line with key macroeconomic concepts such as the Taylor rule and the Phillips curve. In this paper, we study how consumers assess the effects of structural oil market shocks on the U.S. economy, and compare the responses of expectation measures obtained from survey data to the actual macroeconomic effects of these shocks.====Our focus on oil market shocks is motivated by the fact that these shocks are generally viewed as important drivers of the business cycle to which central banks pay particular attention (Mishkin, 2007, Bernanke). Moreover, as gasoline is purchased frequently, gasoline price developments, which are visible at the pump, should be salient to consumers (Georganas, Healy, Li, 2014, Binder, 2018). And to the extent that there is a direct pass-through, gasoline prices should reflect oil market developments. In fact, the existing literature shows that oil price fluctuations shape consumer confidence (Edelstein, Kilian, 2009, Güntner, Linsbauer, 2018).====We obtain measures of consumer expectations about key macroeconomic variables, such as the unemployment rate, the inflation rate, and interest rates, from the University of Michigan’s Surveys of Consumers, to which we refer as the Michigan Survey. To evaluate the effects of the oil market shocks on expectations, we estimate VAR models with suitably aggregated survey data, macroeconomic data, and global oil market data. While we restrict the signs of the impulse responses of the global oil market variables for identification purposes, we leave the responses of the survey measures and the macroeconomic variables unrestricted.====As survey respondents’ assessments may differ depending on the source of fluctuations in the oil market,==== we structurally decompose oil price fluctuations into exogenous oil supply shocks, oil-specific demand shocks, and movements in oil prices that are driven by fluctuations in global economic activity (see Kilian, 2009, Kilian, Murphy, 2012, Baumeister, Peersman, 2013). Oil supply shocks are associated with unanticipated changes in the physical supply of crude oil triggered by e.g. the outbreak of a war. Oil-specific demand shocks arise due to changes in the precautionary demand for crude oil. And finally, rather than originating in the market for oil itself, oil price fluctuations may also arise in the wake of global business cycle fluctuations. To identify the structural oil market shocks, we impose sign restrictions on the impulse responses of the global oil market variables (Faust, 1998, Uhlig, 2005) and elasticity bounds on the impact multiplier matrix (Kilian and Murphy, 2012).====We find that an increase in the price of oil leads to higher inflation expectations, regardless of the type of shock, which is in line with the existing empirical evidence (Coibion, Gorodnichenko, 2015, Wong, 2015). Our results also show that more survey participants expect unemployment to go up in response to shocks that drive up the price of oil, although revisions in unemployment expectations are generally less systematic with the exception of the oil supply shock. The dynamics of interest rate expectations differ depending on the type of shock. Following a global business cycle shock, more survey participants expect interest rates to go up. This is also the case following an oil-specific demand shock, albeit only temporarily. In response to an oil supply shocks, more survey participants expect lower interest rates. These results suggest that survey respondents expect interest rates, and hence monetary policy, to not only respond to inflation developments, but also to real economic activity. In addition, the forecast error variance decomposition shows that the contributions of oil market shocks to fluctuations in expectation measures and macroeconomic variables are of similar orders of magnitude. Overall, we conclude that the expectation measures respond to shocks in a way that is consistent with the actual developments.====The paper is structured as follows: We present the survey data that we use in this paper in Section 2 and discuss the estimation and identification strategy in Section 3. In Section 4, we present our results. Section 5 explores the robustness of the results and Section 6 concludes the paper.",How do consumers assess the macroeconomic effects of oil price fluctuations? Evidence from U.S. survey data,https://www.sciencedirect.com/science/article/pii/S0164070418304464,19 July 2019,2019,Research Article,239.0
"Foellmi Reto,Jaeggi Adrian,Rosenblatt-Wisch Rina","University of St.Gallen, Department of Economics, SIAW-HSG, Bodanstrasse 8, St.Gallen CH-9000, Switzerland,Swiss National Bank, Börsenstrasse 15, Zürich CH-8022, Switzerland","Received 8 March 2019, Revised 12 July 2019, Accepted 18 July 2019, Available online 19 July 2019, Version of Record 30 July 2019.",https://doi.org/10.1016/j.jmacro.2019.103136,Cited by (10),"Preferences are important when thinking about ==== problems and questions. Differences in preferences might, for example, explain cross-country variations in economic fundamentals.====In recent years, differences in preferences across countries and cultures have been studied more frequently, usually concentrating on micro evidence. However, it is an open question as to how differences in average preferences affect the aggregate economy. Coming from a ==== perspective, we test whether preferences stated in Kahneman and Tversky’s prospect theory, namely, reference point dependence and loss aversion, prevail on the aggregate and whether the average degree of loss aversion differs across countries.====We find evidence of loss aversion for a broad set of ==== countries, while the average loss aversion clearly differs across these countries. We find little evidence that these differences could be linked to micro evidence. Furthermore, we analyse whether the different degrees of loss aversion correlate with economic fundamentals such as the level of GDP and consumption per capita. We find that indeed loss aversion is negatively correlated with GDP and consumption per capita and positively correlated with consumption smoothing.","Preferences are important features in macroeconomic modelling. Differences in preferences might correlate with aggregate economic fundamentals. In recent years, differences in preferences across countries and cultures have been studied more frequently. Several papers found differences in preferences across cultures and/ or countries using evidence generated at the micro level, in the form of surveys or experiments (see e.g. Rieger, Wang, Hens, 2015, Herrmann, Thöni, Gächter, 2008, Vieider, Lefebvre, Bouchouicha, Chmura, Hakimov, Krawczyk, Martinsson, 2015).====To gain progress in determining whether differences in preferences matter for aggregate outcomes, our paper approaches this from the opposite direction: We start from a purely macroeconomic perspective and test whether preferences, namely, reference point dependence and loss aversion, two key elements of Kahneman and Tversky’s prospect theory, vary across countries by only using a macroeconomic time series. To do so, we follow Rosenblatt-Wisch (2008), in which she introduced prospect theory in a stochastic version of the Ramsey–Cass–Koopmans optimal growth model. The preferences of the representative agent in that model are given by the experimentally validated prospect utility function of Kahneman and Tversky (1979) and Tversky and Kahneman (1992). She then tested the model with US data and found evidence of loss aversion in a US macroeconomic time series, in line with the values found by Kahneman and Tversky (1979) and Tversky and Kahneman (1992).====Our contribution is two-fold. First, we test empirically for loss aversion across countries for the aggregate economy. We find that loss aversion prevails at the aggregate level in all countries and that the average degree of loss aversion clearly differs across countries. To check whether these degrees of loss aversion could be linked to micro data, we apply the cultural dimensions constructed by Hofstede et al. (2010) and data from the World Values Survey. Because of the large heterogeneity of the data, we find little statistical evidence that either the Hofstede dimensions or the World Values Survey data match with the cross-country variations in the estimated loss aversion.====Second, we analyse whether the different degrees of loss aversion correlate with economic fundamentals such as GDP and consumption per capita. We find that indeed, according to our analysis, loss aversion is negatively correlated with GDP and consumption per capita and is positively correlated with consumption smoothing. These empirical results are in line with the theoretical ones found by Foellmi et al. (2011).====We concentrate on two key elements of Kahneman and Tversky’s experimentally validated prospect theory, namely, reference point dependence and loss aversion. In a recent survey on thirty years of prospect theory, Barberis (2013) notes that the concept of loss aversion relative to a reference point could be promising when thinking about macroeconomics. Focusing on these two aspects of prospect theory, namely, reference point dependence and loss aversion, is common for analysing the aggregate level. Barberis et al. (2001) apply these aspects in order to assess the aggregate stock market behaviour, and Benartzi and Thaler (1995) study the equity premium under loss aversion. The paper by Berkelaar et al. (2004) uses GMM to estimate loss aversion in the aggregate U.S. stock market. They find an implied loss aversion coefficient of the same size as the one found by Tversky and Kahneman (1992). Kahneman and Tversky (1979) formulated their theory on individual choice under uncertainty. The above-cited papers find loss aversion even in aggregate market data. Brooks and Zank (2005), Kőszegi and Rabin (2006) and Abdellaoui et al. (2007) found experimental evidence of loss aversion at the aggregate level. In addition, loss aversion and thinking in differences have also been found in purely deterministic models (see e.g. Thaler, 1980, Kahneman, Knetsch, Thaler, 1990, Tversky, Kahneman, 1991). Chen et al. (2006) find, in an experiment with Capuchin monkeys, that these two behavioural biases even extend beyond species and may be innate, rather than learned.====The rest of the paper is structured as follows. Section 2 reviews the model. Section 3 discusses the data. Section 4 estimates loss aversion across countries, presents the results and tries to link it to micro evidence by applying cultural dimensions constructed by Hofstede et al. (2010) and/ or data from the World Values Survey. Section 5 analyses whether and in what manner differences in loss aversion correlate with economic fundamentals. Section 6 then concludes the paper.",Loss aversion at the aggregate level across countries and its relation to economic fundamentals,https://www.sciencedirect.com/science/article/pii/S0164070419301028,19 July 2019,2019,Research Article,240.0
"Cardani Roberta,Paccagnini Alessia,Villa Stefania","European Commission – ISPRA, Italy,University College Dublin – Michael Smurfit Business School, Ireland,Banca d’Italia, Italy","Received 13 March 2018, Revised 30 April 2019, Accepted 15 June 2019, Available online 28 June 2019, Version of Record 4 July 2019.",https://doi.org/10.1016/j.jmacro.2019.103133,Cited by (14),"We assess the importance of parameter instabilities from a forecasting viewpoint in a set of medium-scale ==== with and without financial frictions using US real-time data. We find that, first, failing to update DSGE model parameter estimates with new data arrival deteriorates point forecasts due to the estimated parameters variation. And second, the presence of financial frictions helps to better address, city, state, ZIP code, province, country with country codes for all authors.","Recent macroeconometric literature suggests that estimated dynamic stochastic general equilibrium (DSGE) models are suitable for forecasting comparisons (see Del Negro, Schorfheide, 2013, Kolasa, Rubaszek, 2015b, Caraiani, 2016, among others). However, the forecasting evaluation of these macroeconomic models is subject to the estimation of the parameters of the model and, as discussed in Giacomini and Rossi (2016), there is ample evidence of instabilities in parameters that might affect their forecasting performance. Gürkaynak et al. (2013) find that the forecasting performance of the medium-scale DSGE model of Smets and Wouters (2007) (hereafter, SW) has been unstable over time. In particular, this model started to fail to produce accurate forecasting performance when researchers included the recent crisis years. One of the main reasons of this failure is that the modeling choices of the SW model reflect the properties of the sample data before the Great Recession (see Giacomini and Rossi, 2015).====The DSGE empirical literature offers alternative approaches to deal with the issue of parameters instabilities (see Fernández-Villaverde, Guerrón-Quintana, Rubio-Ramírez, 2010, Inoue, Rossi, 2011, Bianchi, 2013, Caldara, Fernández-Villaverde, Rubio-Ramírez, Yao, 2012, Castelnuovo, 2012, Bekiros, Paccagnini, 2013, Hurtado, 2014, Bekiros, Cardani, Paccagnini, Villa, 2016, Galvao, Giraitis, Kapetanios, Petrova, 2016, among others).====A practical way to deal with the parameter instabilities in a DSGE model forecasting analysis is discussed in Kolasa and Rubaszek (2015b). They observe that central banks are used to re-estimate DSGE models only occasionally but this practice might affect the forecasting performance. Hence, they investigate how frequently models should be re-estimated so that the accuracy of forecasts they generate may be unaffected. Even if they show the advantage of updating the model parameters for calculating density forecasts, this procedure does not lead to a significant deterioration in the accuracy of point forecasts.====We investigate the role of parameters instabilities in forecasting analysis by addressing two main questions: (1) ==== and (2) ====We borrow the first research question from Kolasa and Rubaszek (2015b), where they provide evidence about the role of parameters re-estimation only in case of the SW model. In our exercise, we compare the workhorse SW model with two models incorporating the financial sector: a SW economy augmented by a banking sector as in Gertler and Karadi (2011) (hereafter, SWBF); and a SW economy augmented with financial frictions as Bernanke et al. (1999) (hereafter, BGG). DSGE models with financial frictions have become popular, as financial factors have played a central role in the recent financial crisis by affecting the amount of credit available in the economy. The seminal DSGE model proposed by Bernanke et al. (1999) considers financial frictions at the level of firms. Their model implies that borrowers can obtain funds directly from lenders without any active role for the banking sector. In the wake of the financial turmoil understanding the disruption in financial intermediation has become a priority. This explains why we consider the model by Gertler and Karadi (2011), in which an endogenous leverage constraint on banks effectively ties the provision of credit to the real economy.====We estimate, using Bayesian techniques, the three models – SW, SWBF and BGG – on the US real-time data using rolling windows of 120 observations. The out-of-sample forecasting period is from 2003Q2 to 2018Q1, split into two sub-samples: (pre-crisis) 2003Q2-2008Q4 and (post-crisis) 2009Q1-2018Q1. Hence, the three models are re-estimated to allow the parameters to be updated each quarter.====Second, we compare the forecasting performance of the three models conducting both point forecast, using Root Mean Square Forecast Error (RMSFE) and Fluctuations test as in Giacomini, Rossi, 2010, Giacomini, Rossi, 2016, as well as density forecast using the average of the log predictive density scores (LPDS).====Our main findings are as follows. First, re-estimating every quarter the model parameters leads to a better forecasting performance due to the estimated parameters instabilities, differently from the result in Kolasa and Rubaszek (2015b). It should be noted that Kolasa and Rubaszek (2015b) investigate only the SW model and consider another sample period: their estimation period is 1966Q1-1989Q4, while their forecasting sample is 1990Q1-2011Q4. The different results are driven by these features. In order to rationalize these results, we show the rolling estimation of the main parameters of the three DSGE models and we find a considerable degree of parameters variation. Second, static forecasting analysis, based on RMSFE and LPDS, suggests that models with financial frictions outperform the SW model in terms of forecasting accuracy. A dynamic forecasting analysis, conducted using the Fluctuations test, shows that the prediction performance of the models with financial frictions is statistically different from that of the SW model for GDP growth rate and inflation, except a few quarters around 2008 where the SW is preferred to SWBF for inflation.====The remainder of the paper is organized as follows. Section 2briefly sketches the three models and describes the Bayesian estimation procedure. Section 3 presents the two forecasting comparisons. Finally, Section 4 concludes.",Forecasting with instabilities: An application to DSGE models with financial frictions,https://www.sciencedirect.com/science/article/pii/S0164070418301046,28 June 2019,2019,Research Article,241.0
"Alloza Mario,Burriel Pablo,Pérez Javier J.","Banco de España, Calle Alcalá 48, Madrid 28014, Spain","Received 16 January 2019, Revised 27 May 2019, Accepted 14 June 2019, Available online 18 June 2019, Version of Record 2 July 2019.",https://doi.org/10.1016/j.jmacro.2019.103132,Cited by (14),"The issue of the size of fiscal ==== in the euro area has gained prominence recently, given proposals to coordinate fiscal policies that aim at achieving an appropriate “aggregate fiscal stance”, consistent with economic and ==== conditions. Given the heterogeneous fiscal positions of member states, such stance would be achieved by fine-tuning policies of countries with enough fiscal space. However, such proposals have so far been based on limited empirical evidence. On the one hand, the literature based on calibrated/estimated ","Contrary to the US or Japan, in the euro area the use of fiscal policy as a area-wide stabilization tool can only be achieved through a coordination of national (country-specific) fiscal policies, given that no area-wide fiscal authority or centralised fiscal capacity do exist. In the euro area, the fiscal policy stance is the result of aggregating the member countries’ fiscal policies which are designed, nevertheless, under the constraint of having to fulfill the EU’s common budgetary rules framework, defined in the Stability and Growth Pact (SGP). The implementation of such coordinated policies might become particularly relevant when the common monetary policy is constrained or limited in its ability to react to area-wide symmetric shocks. Accordingly, joint fiscal policy responses have to be based on ad hoc initiatives coordinated by the European Commission or via ad hoc inter-governmental agreements. This was the case, for instance, of the November 2008 “European Economic Recovery Plan” (EERP), the European Investment Plan (the so-called “Juncker Plan”), which was approved in late 2014, or, more recently, the European Commission Communication on the need for a more expansionary fiscal policy stance in the euro area, formulated in November 2016.====To make a proper assessment on the usefulness of such policy actions it is essential to understand what are the economic effects of fiscal policy shocks in a member country on neighbouring countries. Nevertheless, euro area wide discussions about this matter have typically been based on limited empirical evidence. On the one hand, the literature based on calibrated/estimated general equilibrium models tends to find that fiscal spillovers within the euro area are small once all channels are considered (trade channel vs. monetary policy reaction, exchange rate, and risk premium).==== On the other hand, the available empirical studies hinge on pools of countries, given data limitations, and do not usually provide country-specific estimates or, when provided, tend to hinge on relatively short time periods.====In our paper we revisit the issue at hand. In order to estimate fiscal spillovers effects among euro area countries, we first assemble a novel country-level fiscal dataset for the four largest EA countries (Germany, France, Italy and Spain) and the region-wise aggregate for the period 1980q1-2016q4, given the scarcity of publicly available detailed historical fiscal data. Such database is built to some extent on the basis of interpolation methods, but the raw indicators we use to do so are closely linked to the ones employed by national statistical agencies to provide their best estimates (i.e. monthly fiscal data, mostly on a public accounts basis), and the unobserved components, mixed-frequencies time series method we use preserves full coherence with official, annual and quarterly fiscal data when available.==== Next, we employ this dataset to estimate a vector autoregresion (VAR) for each of the four major euro area countries. We then impose restrictions on the contemporaneous response of the variables following Blanchard and Perotti (2002) to identify government spending shocks. Finally, we use these shocks to explore the dynamic effects of fiscal changes in the major euro area countries both on the country undertaking this policy (i.e. the conventional fiscal multiplier) and on neighbour countries (fiscal spillovers).====To analyze ==== (defined as the cross-border effects of domestic fiscal policies), our empirical strategy employs local projections (Jordá, 2005) to relate the dynamic response of economic activity in a country to a government spending shock in a different country. We propose two statistics that allow us to quantify the degree of spillovers present in the euro area, while preserving the potentially heterogeneous effects that they may exhibit. First, we construct a measure of how much an economy benefits from changes in fiscal policy abroad and, second, we estimate how large are the effects that each individual country is able to generate on the economic activity of other countries. We refer to these two measures as spillovers by destination and spillovers by origin, respectively. Overall, our results suggest that the fiscal spillovers are positive and large. This is particularly so, for fiscal actions that are based on public investment policies. Our results also point to a notable degree of heterogeneity, with some countries benefiting more than others from fiscal policies abroad (e.g. Germany). We investigate a potential channel explaining our results and find evidence that support the hypothesis that government expansions increase both domestic and external demand, fostering exports in neighbour countries and enhancing their economic activity.====In order to compare our work with previous literature, we follow Auerbach and Gorodnichenko (2013), and construct a trade-weighted measure of spillovers. Our results corroborate our previous findings. Particularly, we find that an average increase of one euro in the rest of countries, can increase economic activity in the average country by as much as 0.6 euros by the third year. Again, we find that a substantial degree of heterogeneity is hidden behind these average figures. Our results point to important policy implications for the design of fiscal policies. First, countries that may not have enough fiscal space to execute fiscal expansions to support a contracting domestic demand may benefit from similar actions taken by their neighbours. However, despite the fact that these spillover effects are relatively large for the average economy, not all the countries benefit from these action to the same extent. And lastly, the composition of fiscal plans is also an important determinant of the size of potential cross border effects. Euro area-wide policies based on public investment are likely to generate higher spillovers.====This paper is structured as follows. In Section 2 we discuss the relation of our paper to the relevant literature of reference. Then in Section 3 we describe the new dataset and the methodology used for its production, while in Section 4 we explain our empirical strategy to identify exogenous fiscal shocks and to estimate their effects. Next, in Section 5 we present our main results on the impact of fiscal shocks on neighbour countries in the EU, and investigate potential mechanisms to understand them. In Section 6 we test the robustness of these results to different specifications. Lastly, Section 7 concludes an offers future avenues of research.",Fiscal policies in the euro area: Revisiting the size of spillovers,https://www.sciencedirect.com/science/article/pii/S0164070419300187,18 June 2019,2019,Research Article,242.0
"Bartoletto Silvana,Chiarini Bruno,Marzano Elisabetta,Piselli Paolo","University of Naples “Parthenope” and CNR-ISSM, Naples, Italy,University of Naples Parthenope, Naples, Italy,CESifo, Munich, Germany,Bank of Italy, DG Economics, Statistics and Research, Rome, Italy","Received 12 September 2017, Revised 31 May 2019, Accepted 4 June 2019, Available online 5 June 2019, Version of Record 26 June 2019.",https://doi.org/10.1016/j.jmacro.2019.103130,Cited by (4),"We propose a joint dating of Italian business and credit cycles on a historical basis by applying a local turning-point dating algorithm to the level of the variables. In addition to short cycles corresponding to traditional business cycle fluctuations, we also investigate medium cycles because there is evidence that financial booms and busts are longer and more persistent than business cycles. The results show that medium-term cycles account for the largest part of fluctuations of the Italian credit cycle. Second, we find evidence that credit and business cycles are weakly synchronized in the medium term, whereas they steadily comove in the short term, when the GDP cycle is leading the bank credit cycle. However, the study found that, over the medium cycles, economic downturns associated with ==== and financial disruption are more severe than other types of economic downturns. Finally, by modelling interaction between credit and business cycles in a simple threshold VAR model, we confirm that GDP response to credit shock is much more intense in the recessionary phase of credit cycle.","Following the international crises of 2007–2008, there has been renewed interest in the connection between real and financial variables. Historically, the study of the business cycle has focused on the behavior of macroeconomic data, with cycles lasting no more than 8 years on average (A'Hearn and Woitek, 2001, Zarnowitz, 1992). Although the idea of long swings in economic activity stems back to the ideas of economists such as Kuznets, Abramovitz and Schumpeter, and the idea of long financial swings was discussed in Minsky (1964), recent studies have shown that real and financial cycles interact at lower frequencies than those of the traditional business cycle (Aikman et al., 2015, Drehmann et al., 2012).====In this paper, using a local turning-point dating algorithm based on the level of variables (or the NBER approach), we propose a joint dating of Italian business and credit cycles during the last 150 years (1861–2013). Our study is innovative in two ways. First, we focus on two types of cyclical patterns, namely those that have the same periodicity as the business cycle (short cycles) and those that have considerably longer periodicity (medium cycles). With regard to the latter, we find support for our hypotheses using spectral analysis. Second, while previous works on Italy have considered either only the credit (De Bonis and Silvestrini, 2014) or business cycle (Clementi et al., 2014), this study examines both cycles together. To our knowledge, so far only one study has proposed a joint dating of business and credit cycles with a specific focus on Italy (Bulligan et al., 2017), but the authors analyze only recent years, using quarterly data from 1970 to present day.====The dating of the turning points is a preliminary step to investigate, using historical narratives and econometric estimates, how comovement changes with the definition of the cycle and the real impact of the credit cycle. In this regard, we first examine the degree of synchronization between the credit cycle and GDP cycle (short and medium) using parametric and non-parametric methods (Harding and Pagan, 2002 and 2006) as well as Granger causality. Then, we shift our attention to the rate of growth of GDP to investigate whether the short and medium credit cycles can explain the short-term economic performance observed during the period 1861–2013. Specifically, we first examine how the probability of a recession varies conditional on the credit cycle and to what extent the rate of growth of GDP in recession decreases conditional on a credit contraction. Second, we analyze the real impact of credit cycles in a simple VAR model to detect possible nonlinear responses of GDP dynamics to the phases of credit cycles.====The results of our study show that medium-term fluctuations account for the largest part of the Italian credit cycles. Second, we find evidence that credit and business cycles are weakly synchronized in the medium term, whereas they steadily comove in the short term, when the GDP cycle is leading the bank credit cycle. However, we found that, over the medium cycles, economic downturns associated with credit crunches and financial disruption are more severe than other types of economic downturns. Finally, we found strong evidence that the examined cycles express a non-negligible amount of non-linearity. By exploiting a simple asymmetric VAR model, we find that while GDP is totally unaffected by credit expansions, it is significantly downsized by credit contractions.====The rest of the paper proceeds as follows: Sections 2 and 3 describe the data and the dating methodology and provide evidence of long-lasting fluctuations in the series under scrutiny. Section 4 presents the results of the turning-point analysis for the business and credit cycles. Section 5 details descriptive evidence regarding the business and credit cycles and presents results regarding synchronization/comovement==== and Granger causality. Section 6 discusses the real impact of credit cycles as estimated by univariate and multivariate analysis, which considers possible asymmetric responses of GDP to credit shocks (threshold VAR model). Section 7 draws conclusions from the study's results.","Business cycles, credit cycles, and asymmetric effects of credit fluctuations: Evidence from Italy for the period of 1861–2013",https://www.sciencedirect.com/science/article/pii/S0164070417303932,5 June 2019,2019,Research Article,243.0
Kim Seongeun,"Department of Economics and Trade, Sejong University, Neungdong-ro 209, Gwangjin-gu, Seoul 05006, Republic of Korea","Received 19 November 2018, Revised 2 April 2019, Accepted 31 May 2019, Available online 31 May 2019, Version of Record 7 June 2019.",https://doi.org/10.1016/j.jmacro.2019.103129,Cited by (2),"The low-price, low-quality brands change prices more frequently than the high-price, high-quality brands within the same product category. The greater price rigidity for high-quality brands leads to asymmetric effects of ","People generally buy more expensive, higher-quality products as they get richer. This implies that product prices have a positive correlation with consumers’ income or wealth. Although this has been well known to economists for a long time, macroeconomists’ research regarding quality upgrade has been focused on a narrow range of topics such as the bias in the measurement of inflation and growth. Seminal work in this area includes Bils and Klenow (2001).====This paper suggests that the relationship between quality and income not only biases inflation and growth but also affects the real effects of monetary policy. To illustrate this, I connect the degree of price stickiness with the quality of the products. Most studies about the transmission channels of monetary policy assume sticky prices, at least in the short run, because price rigidity is one of the necessary conditions for monetary policy to have real effects on the economy. Using the Kilts-Nielsen retail scanner and consumer panel data, I provide empirical evidence that more expensive, higher-quality products change prices less frequently than other products in the same product category. The premise is that more expensive products have better quality than cheaper products in the same product category.====The heterogeneous price stickiness provides a new channel through which unexpected monetary policy can have asymmetric effects on the rich and the poor. A typical prediction of the new-Keynesian analysis with menu costs is that demand for sticky-price products is more responsive to unexpected monetary shocks.==== The greater price rigidity for high-quality products implies that monetary shocks have larger real effects on high-income consumers who have more high-quality products in their consumption basket. This means that the rich benefit more from expansionary monetary shocks but suffer more from contractionary shocks than the poor. The heterogeneous price rigidity across different sectors, or product categories, is a well-known fact that has been analyzed by many researchers including Carvalho (2006), Nakamura and Steinsson (2010), Bouakez et al. (2014), Pasten et al. (2016) and Cravino et al. (2018). This study differs from these studies because the main focus of this paper is the heterogeneous frequencies for high- and low-quality brands within the same narrow product category.====In the empirical analysis, I provide several novel facts. First, high-quality brands change prices less frequently than low-quality brands. I assign each product with a unique Universal Product Code (UPC) to quartiles—high, middle-high, middle-low, and low—based on their average prices per unit size within the same product category. The fraction of prices at the annual mode is 60% for low-quality brands and 70% for high-quality brands. After filtering out temporary price changes, the weekly frequency of regular price adjustment is 16% for low-quality brands and 10% for high-quality brands.==== Also, low-quality brands have larger price changes than the high-quality brands. These facts indicate that the degree of price rigidity increases with the relative price or quality of a product.====Second, the consumption share of high-quality brands increases with household income, while the consumption share of low-quality brands decreases with income. The consumption share of the high-quality brands is around 28% for the richest 10% households and 18% for the poorest 10% households. Also, I conduct a panel regression to investigate the relationship between the consumption share of each product group and household income, using the panel data from 2006 to 2010, including the Great Recession. After controlling for unobserved heterogeneity that can affect preferences, a 10% increase in total expenditure leads to a 0.19 percentage points decrease in the consumption share of the low-quality brands and a 0.16 percentage points increase in the consumption share of the high-quality brands. This implies that the cross-sectional variation in consumption share over the income distribution is much larger than the variation over business cycles.====To see how these empirical findings are related to the real effects of monetary policy, I develop a menu-cost model in which retailers sell vertically differentiated products. I assume a Constant Elasticity of Substitution (CES) utility. The aggregate consumption is divided into different product categories and each category has two types of brands, which are ==== and ====. These two types of brands have some substitutability, though the premium brands have a lower elasticity of substitution than the regular brands as the demand for the premium brands are less sensitive to relative prices. In addition, I assume the multi-product retailers sell both brands in their stores, and the retailers need to pay menu costs to adjust their prices.====I calibrate the model to match important data moments including the frequency and size of price changes for low- and high-quality brands. Then I measure the real effects of monetary shocks using the business cycle statistics, such as the standard deviation and serial correlation of real consumption. As expected from the higher price rigidity for high-quality brands, monetary shocks have larger real effects on the consumption of high-quality brands. For instance, the standard deviation of real consumption due to monetary shocks is 0.69% for high-quality brands and 0.19% for low-quality brands.====This implies that monetary policy shocks have larger real effects on high-income consumers who have more high-quality products in their consumption basket. This also means that the rich benefit more from expansionary monetary shocks but suffer more from contractionary shocks than the poor.====Both the Calvo pricing model and the menu cost model can generate real effects of monetary policy shocks. However, they have different implications and frequently give very different results regarding the real impact of monetary policy shocks. In a multiproduct setting, the Calvo model cannot capture the economies of scope in price adjustment, which helps to match the distribution of the size of price changes, because it is a time-dependent model. Therefore, it is hard to match the dispersion of the size of price changes with the Calvo model. This paper investigates real effects of monetary policy shocks when multiproduct firms sell both high- and low-quality products within the same product category. Therefore, I use the menu-cost model with multiproduct firms and assume that there are economies of scope in price adjustment. This model is state-dependent and can match the wide dispersion of the size of price changes observed in the microdata.====This paper is similar in spirit to Guimaraes and Sheedy (2011) and Chevalier and Kashyap (2014), as they connect heterogeneous consumers’ shopping behaviors, which involve loyal customers and bargain hunters, with firms’ pricing strategies, for example, temporary sales. Also, this paper is related to the recent studies concerning time-varying shopping and sales intensity, such as Coibion et al. (2015), Nevo and Wong (2015), Jaimovich et al. (2015), Kryvtsov and Vincent (2016), and Kaplan and Menzio (2013).====The relationship between quality and price rigidity implies that the degree of price rigidity decreases when there are more low-income consumers, because they purchase more low-quality goods and their prices change more frequently. This is consistent with the higher volatility of inflation rates for low-income consumers as indicated by Kaplan and Schulhofer-Wohl (2016). There are also many studies about the different movement of price indices for different income groups, such as Broda and Romalis (2009), Handbury (2013), Argente and Lee (2016), Pisano and Stella (2015), and Jaravel (2016). These studies are similar to this paper, because they take into account the different consumption baskets of heterogeneous consumers. However, these studies focus on the movement of price levels rather than the frequency of price adjustment.====Recently, rising income and wealth inequality has sparked heated debates about the relationship between monetary policy and inequality. Theoretically, the relationship is ambiguous because there are several different channels through which monetary policy affects inequality as summarized in Coibion et al. (2012), Auclert (2016), and Amaral (2017).==== This paper suggests that the real effects of monetary policy shocks can decrease if the poor consist of a larger share of the population in an economy.====This paper also has an implication for the cyclical behavior of markups.==== In a typical New Keynesian model, sticky prices lead to counter-cyclical markups due to increases in real marginal costs (MC/P) during booms. The empirical evidence, however, is mixed. If the share of investment in or consumption of durable goods increases during booms, markups can decrease, as noted in Rotemberg and Woodford (1999), Gali (1994), and Bils (1989). The reason for this is that firms, which make decisions about investments, and consumers, who demand durable goods, are more sensitive to prices. Meanwhile, there is evidence that consumers are less sensitive to prices when they become richer (Stroebel and Vavra, 2014). The less-elastic demand during booms allows firms to set higher markups. This argument, however, ignores the consumption reallocation among vertically differentiated products. Markups can, in fact, look pro-cyclical, because consumers purchase more premium brands when their incomes increase. As premium product prices are stickier, the markups for these goods could actually decrease during booms even if consumers become less sensitive to prices. The overall result depends on the relative size of the two effects: the less elastic demand and the stickier prices for premium brands.====A limitation of this research is that I use only the retail sector data. Can the results be applied to other sectors such as durable products or services? Theoretically, there is no reason why the channel proposed in this paper does not apply to other sectors. Also, Kaplan and Schulhofer-Wohl (2016) indicate that this data set represents many different types of products, including food, beverages, housekeeping supplies, pet products, and personal care items, and that the aggregate price index from this data behaves similarly to the CPI.====This paper is organized as follows. In Section 2, I discuss the data used for the empirical analysis in Section 3. Then I introduce a menu-cost model in Section 4 and discuss the calibration process in Section 5. In Sections 6 and 7, I simulate the model economy and discuss the results. In Section 8 concludes the paper.","Quality, price stickiness, and monetary policy",https://www.sciencedirect.com/science/article/pii/S0164070418304944,31 May 2019,2019,Research Article,244.0
"Belongia Michael T.,Ireland Peter N.","Otho Smith Professor of Economics, University of Mississippi, Box 1848, University, MS 38677, United States,Department of Economics, Boston College, 140 Commonwealth Avenue, Chestnut Hill, MA 02467, United States","Received 30 January 2019, Revised 17 May 2019, Accepted 21 May 2019, Available online 23 May 2019, Version of Record 30 May 2019.",https://doi.org/10.1016/j.jmacro.2019.103128,Cited by (30),. That role can be of greater prominence when traditional ==== are constrained by the ====.,"Since Milton Friedman's (1956) “restatement,” the existence of a stable money demand function has been regarded as a necessary pre-condition for the success of any quantity-theoretic approach to monetary policy that would use information in broad monetary aggregates to achieve goals for aggregate spending or the price level. The general message of Friedman's essay, supported by the empirical papers that accompanied its publication, motivated a broad and active line of research in monetary economics that lasted more than three decades.==== The primary focus of this agenda was the search for statistical relationships that linked the demand for real money balances to a small number of determinants, especially income or spending and interest rates. By the early 1970s, considerable evidence had been assembled to support the notion of a stable demand for money function and, when inflation had risen to increasingly higher rates, this evidence provided the foundation for a monetary policy strategy based on intermediate targets for growth rates of the money supply.====This foundation, however, was not without cracks. Perhaps most famously, Goldfeld (1976) identified the “missing money” problem when a money demand equation, estimated only several years before (Goldfeld (1973)), no longer predicted holdings of real money balances out of sample. His results renewed interest in the question of stability in the demand for money function, with results taking both sides of the issue. As surveyed in Judd and Scadding (1982), the consensus conclusion seemed to be that any observed instability could be attributed to financial innovations. They also concluded, however, that the post-1973 research largely failed to distinguish between alternative theories of the demand for money.====If questions about money demand's instability were unresolved in the early 1980s, evidence accumulated during the 1980s shifted the balance of evidence toward the conclusion of instability. As more, and more disruptive, financial innovations evolved – most notably the relaxation of Reg. Q interest rate ceilings and the introduction of interest-bearing checkable deposits – the velocity of both M1 and M2 exhibited pronounced shifts that led to the Federal Reserve's abandonment in 1982 of a monetary policy strategy based on intermediate targets for money growth it had adopted only three years earlier. As the 1980s drew to a close, Rasche (1987), Benjamin Friedman, 1988a, Friedman, 1988), and others reported that previously stable demand for money functions could no longer be characterized as such and that, while conventional wisdom held that sweeping financial innovations during the 1980s permanently altered whatever associations might have been found in the data of the 1960s and 1970s, this remained at the level of conjecture not supported by a clear theory or empirical evidence.====Rather than making an argument for a monetary policy strategy based on monetary aggregates as they had only a decade earlier, the results from the 1980s led to the formation of a professional consensus that measures of money could be safely excluded from the information set used to evaluate the effects of monetary policy on economic activity or to assess the relative ease or tightness of policy at a moment in time.==== Rather than money, the canonical New Keynesian model associated with Clarida et al. (1999) and Woodford (2003) linked the expected path of the federal funds rate to variations in output and prices. The Taylor (1993) formalized the means by which the central bank can manipulate a short-term interest rate to pursue goals for these variables.====Recent events, however, provide good reason to reconsider the role of the monetary aggregates in monetary policy analysis. Most significantly, the zero lower bound on nominal interest rates, which became a binding constraint on the Federal Reserve's interest rate policy over an extended period from 2008 through 2015, highlights the limitations of any approach that uses the federal funds rate alone to gauge the stance of monetary policy. If, for example, Federal Reserve actions affect spending and prices through changes in variables other than the funds rate, these effects will not be captured by models that exclude, by construction, any channel of monetary transmission apart from an interest rate. This omission, however, runs counter to empirical evidence that shows how Divisia monetary aggregates can be used within structural vector autoregressive time series models to help identify monetary policy shocks before and during the period of zero nominal interest rates; it does so by capturing the consequences of “unconventional” policy actions such as large-scale asset purchases or “quantitative easing” through their effects on the growth rate of money when measured as a Divisia, rather than simple sum, aggregate.==== Because the results from these studies are consistent with the ideas that helped motivate the money demand research agenda sixty years ago, these same Divisia monetary aggregates might be used to estimate stable money demand relationships of the kind envisioned by Friedman (1956). If identified, these relationships then could serve as the foundations for a quantity-theoretic approach to monetary policymaking and analysis that downplays the significance of the zero lower bound, yet also works reliably to stabilize inflation during normal periods with positive nominal interest rates.====To explore this possibility, this paper begins by modifying Lucas’ (2000) version of the money-in-the-utility function model, developed originally by Sidrauski (1967) and Brock (1974), by introducing separate roles for noninterest-earning currency and interest-earning deposits in providing a representative household with liquidity services that allow it to purchase goods and services at the expense of less time and effort. This extension to the theory makes clear that the money demand relationship implied by the model applies to a Divisia monetary aggregate but not a simple-sum measure of the type provided officially by the Federal Reserve. The same theoretical extension also reinforces Belongia's (2006) argument that the price, or user-cost, dual to the Divisia monetary aggregate ought to appear in place of a short-term nominal interest rate as a preferred opportunity cost measure in the money demand equation. Finally, the extended money-in-the-utility function model can be used to motivate renewed interest in classic empirical specifications for money demand, originally proposed by Cagan, 1956, Selden, 1956, Latané, 1960, and Meltzer (1963), adapted to apply to the Divisia monetary aggregates instead of the standard simple sum measures.====The paper goes on to estimate money demand equations using the Pesaran et al. (2001) approach together with quarterly data on the Divisia aggregates described by Barnett et al. (2013) and made available through the Center for Financial Stability's (CFS) website. In this sample of data, running from 1967:1 through 2019:1, cointegrating money demand relationships of the form suggested by the theory link the Divisia M2 and MZM monetary aggregates to either spending or income as a scale variable and the associated Divisia user cost aggregate as an opportunity cost measure. The results imply that a monetary policy strategy focused on the growth of either Divisia aggregate also has the potential to stabilize the aggregate price level or its rate of change.====By using the most recent data to estimate statistical money demand equations, this paper joins with a few others, including Judson et al., 2014, Lucas and Nicolini, 2015, and Anderson et al. (2017), that also are motivated by the financial crisis and period of zero nominal interest rates to reconsider a more traditional, quantity-theoretic approach to monetary policy analysis. Meanwhile, Benati et al. (2016) revive the analysis of long-run money demand by estimating equations using international data extending back, in some cases, to the 19th century. All of this work, however, is based on traditional simple sum monetary aggregates, which cannot internalize pure substitution effects and therefore introduce the possibility of erratic behavior in the monetary measure. Serletis and Gogas (2014) estimate long-run money demand relationships for the CFS Divisia quantity aggregates, but use the three-month United States Treasury bill rate as their opportunity cost measure rather than the user cost dual to the economic quantity aggregate suggested by theory. The empirical specifications derived in the next section use this measure instead. The results presented here, therefore, provide evidence of stable money demand relationships based on price and quantity data derived from a uniform set of principles. In doing so, the results also offer additional support for the idea that reported instability of money demand functions is more likely related to measurement issues than to instability in the underlying economic relationships themselves.",The demand for Divisia Money: Theory and evidence,https://www.sciencedirect.com/science/article/pii/S0164070419300412,23 May 2019,2019,Research Article,245.0
"Hirakata Naohisa,Sunakawa Takeki","Bank of Japan, Japan,Center for Social Systems Innovation, Kobe University, 2-1 Rokkodaicho, Nada Kobe, Hyogo 6570013, Japan","Received 25 April 2018, Revised 9 May 2019, Accepted 14 May 2019, Available online 21 May 2019, Version of Record 11 July 2019.",https://doi.org/10.1016/j.jmacro.2019.103127,Cited by (8),"We develop a two-sector growth model with financial frictions to examine the effects of a decline in the working population ratio and change in the structure of household demand on sectoral TFP and structural change. Our findings are twofold. First, with financial frictions, a decline in labor input reduces the real interest rate and increases excess demand for borrowing, tightening collateral constraints at a given credit-to-value ratio and generating capital misallocation and lower sectoral TFP. Second, compared to the case with no financial frictions, such changes in sectoral TFP impede structural change driven by the change in the structure of household demand. We also estimate the model’s parameters using the Japanese data and undertake a counter-factual simulation to demonstrate the role of financial frictions and capital misallocation in structural change.","In recent years, many developed countries have experienced significant demographic change, especially in the form of rapidly aging populations. As the number of retirees increases, the value-added share of industries producing those goods and services necessary for the elderly, such as hygiene and medical services, has also increased. For example, as shown in Fig. 1, since about the 1980s in Japan, the population ratio of persons aged 65 years and over to the total population has continued to increase. This ratio was around 10% at the beginning of the 1980s, but increased to 26.7% in 2015 and is projected to continue to increase to around 38% in 2060. During the same period, the nominal value–added share of service sectors related to aging has increased by more than 5%. In addition, firms in these industries in Japan are typically new and small, as shown in Table 1. They tend to have low capital adequacy ratios and face higher loan interest rates, which implies that they are financially constrained.==== Also, regarding the household side, Table 2 shows that the consumption share of health of the elderly (age of 65 over) is much higher than that of young persons (ages 0–14 and 15–64).====At the same time, the growth rate of total factor productivity (TFP) has declined over this period. The average growth rate of TFP from 1973 to 1990 was 1.5%, whereas that from 1991 to 2012 was only 0.3%. The relative TFP of the elderly-related sector (i.e., hygiene and medical services) to the other sectors has also been low, whereas its relative prices have increased during this period, especially after the 1980s.====Motivated by these facts, this paper provides a theory of how a reduction in the share of the working population can explain the slowdown of the economy through a decline in aggregate TFP caused by financial frictions and capital misallocation among firms. Our theory suggests that the reduced working population ratio may have led to a disproportionate decline in sectoral TFP, which affects the pace of structual change induced by demand shifts by the elderly.====This paper links capital misallocation, TFP, and structural change in the presence of financial frictions in a unified dynamic general equilibrium framework. We develop a two-sector neoclassical growth model with financial frictions and two household types: workers and retirees. We introduce two types of firms to analyze financial frictions: borrowers and savers. We assume that borrowers are newer and smaller firms dominant in industries such as hygiene and medical services, while savers are older and larger firms typically found in manufacturing and in services other than hygiene and medical services. For the most part, borrowers have large financial needs, but their borrowing is limited by collateral constraints (c.f., Kiyotaki and Moore, 1997). In contrast, savers have less need for financing, and their borrowing is unconstrained. Our model has two sectors: one new and the other old. The new sector is more credit constrained (i.e., the new sector has more borrowers than the old sector). As the number of retirees increases—that is, as the working population ratio declines—the demand for goods produced in the new sector increases. This is the driving force for the structural change explored in the paper.====Our findings are twofold. First, in the presence of financial frictions, a decline in the working population ratio distorts the allocation of capital, which lowers TFP in both the new and old sectors. The decline in the working population ratio also lowers real interest rates, thereby increasing borrowing demand. This increased demand for borrowing tightens the collateral constraint at a constant credit-to-value ratio because the value of collateral does not rise proportionally. This is because borrowers have too little capital, whereas savers have too much, and borrowers produce fewer goods than they would do in the absence of financial frictions. This capital misallocation then lowers sectoral TFP in both sectors.====Second, TFP in the new sector is more and disproportionately affected by financial frictions than that in the old sector, impeding the structural change driven by changes in the structure of household demand. The number of borrowers in the new sector exceeds that in the old sector, which means the new sector has a greater need for financial resources and is more vulnerable to financial frictions than the old sector. Thus, capital misallocation in the new sector is more severe than in the old sector. This difference in the effect of financial frictions between sectors ensures TFP in the new sector is lower than in the old sector, thereby impeding structural change. That is, it leads to too short a supply of goods produced in the new sector compared to that without financial frictions.====To examine whether these theoretical findings can be supported empirically, we estimate the model parameters by fitting the model to actual data. Our estimates are consistent with our assumptions. That is, there are more borrowers in the new sector than the old sector and retirees demand the new sector’s goods more than workers. We simulate the model for the Japanese economy from 1973 to 2012 and undertake a counter-factual simulation in the absence of financial frictions. We find that financial frictions and capital misallocation indeed impede structural change as the price for the new sector’s goods becomes higher.","Financial frictions, capital misallocation and structural change",https://www.sciencedirect.com/science/article/pii/S0164070418301745,21 May 2019,2019,Research Article,246.0
Binder Carola,"Haverford College, Department of Economics, 370 Lancaster Avenue, Haverford, PA 19041","Received 4 February 2019, Revised 3 May 2019, Accepted 3 May 2019, Available online 11 May 2019, Version of Record 16 May 2019.",https://doi.org/10.1016/j.jmacro.2019.103122,Cited by (12),Numerous studies document a positive correlation between ,"The distributional implications of inflation have been studied at least since Hume (1742),==== and are an important consideration in determining the welfare costs of inflation (Camera and Chien, 2014). In recent years, as the distributional implications of monetary policy have become more salient, understanding the relationship between inflation and inequality has become a larger priority in policymaking (Acemoglu, Johnson, 2012, Stiglitz, 2015, Furceri, Loungani, Zdzienicka, 2016, Coibion, Gorodnichenko, Kueng, Silvia, 2017). The conventional wisdom is that inflation is regressive, or “the cruelest tax.”==== But the relationship between inflation and inequality is both bidirectional and theoretically ambiguous (Kane and Morisett, 1993).====Inflation may increase inequality if low-income households hold more cash as a share of their total purchases (the portfolio channel) or if wages, pensions, and transfers to poorer households are less indexed to inflation (the real wage channel) (Erosa, Ventura, 2002, Cardoso, 1992, Pessino, 1993).==== Inflation can also cause “bracket creep” in income tax rate schedules, increasing inequality (International Monetary Fund, 1996). However, inflation can also decrease inequality by eroding the real value of debt service (the savings redistribution channel) (Doepke and Schneider, 2006). These channels imply that rich and poor households may have different preferences over inflation.==== They may also have different abilities to influence macroeconomic policymaking. Thus the distribution of income can also affect inflation outcomes, depending on the nature of the political process (Beetsma, Ploeg, 1996, Bhattacharya, Bunzel, Haslag, 2001, Albanesi, 2007). Moreover, high inequality may increase distributional conflict, which can lead to fiscal stalemates and reliance on monetary finance or delay macroeconomic stabilization programs (Sachs, 1989, Alesina, Drazen, 1991).====Empirically, numerous studies have reported a ==== correlation between inflation and income inequality across countries (Beetsma, Ploeg, 1996, Al-Marhubi, 1997, Romer, Romer, 1999, Bulír̆, 2001, Crowe, 2006, Albanesi, 2007). These studies are summarized in Table A.1. In Section 2, I document the reversal of this correlation for a large sample of countries, and show that its sign and magnitude depend on the time period and sample composition.====In Section 3, I discuss and test candidate explanations for these results, drawing on the literature on the political economy of inflation. In political economy models of inflation and inequality, policymakers typically use a combination of inflation and income taxation to finance expenditures. I find that as the correlation between inequality and inflation has become more negative, the correlation between inequality and income taxation has, correspondingly, become more positive. In Albanesi (2007), the government’s relative reliance on monetary growth versus income tax revenue to finance spending is the result of a bargaining game between higher-income households, who are more protected from inflation, and lower-income households, who are more exposed. In this game, if the high- and low-income households do not reach an agreement, the government imposes a low tax rate on labor income and mostly uses monetary growth to finance spending. This threat point increases the bargaining position of the higher-income households who are less vulnerable to inflation, and results in a positive correlation between inflation and pre-tax inequality in the equilibrium of Albanesi’s calibrated model.====Albanesi (p. 1099) notes that this choice of high-inflation threat point “reflects the idea than an increase in the money supply is easy to implement, since it does not require parliamentary approval and it is always feasible, if the government can costlessly run the printing press.” Beginning in the 1980s, however, widespread central bank independence (CBI) reforms were intended to make it less feasible for elected officials to increase the money supply at will. If this has changed the threat point of the bargaining game, it could help explain the declining correlation between inflation and inequality. Other political economy models of inflation, such as the median voter model in Dolmas et al. (2000), also predict that greater CBI can reduce the political pressure for inflationary policies that rises with inequality in democracies. The European economies are the most democratic and experienced the largest increase in CBI; they also experienced the largest decline in the correlation between inflation and inequality.====To more formally test whether CBI and democratization modify the association between inequality and inflation, I regress the inflation tax on the market Gini, CBI, and polity score, and interactions of these three variables. I use both cross sectional and panel specifications. The results indicate that greater CBI reduces the correlation between inflation and inequality in more democratic economies.",Inequality and the inflation tax,https://www.sciencedirect.com/science/article/pii/S0164070419300436,11 May 2019,2019,Research Article,247.0
Wesselbaum Dennis,"University of Otago, Department of Economics, P.O. Box 56, Dunedin 9054, New Zealand","Received 28 February 2019, Revised 5 May 2019, Accepted 6 May 2019, Available online 8 May 2019, Version of Record 16 May 2019.",https://doi.org/10.1016/j.jmacro.2019.103126,Cited by (2),This paper establishes a link between labor market frictions and financial market frictions. We argue that this link had large effects on labor market outcomes and helps to explain the jobless recovery. We build a stylized ,"Jobless recoveries appeared to be a European phenomenon. Blanchard and Summers (1986) argue that high and persistent unemployment rates in Europe are the results of hysteresis due to labor market rigidities. However, from the1990′s on, jobless recoveries can also be found in the United States. Here, the rigid labor market hypothesis fails to explains this observation. In contrast, the flexible labor market was blamed to cause jobless recoveries due to structural change, uncertainty, and increasing health costs (see Bernanke, 2003, and Groshen and Potter, 2003). Most recently, the Great Recession created a jobless recovery.====In this paper, we propose a channel that can explain this jobless recovery. We suggest a direct link between labor market frictions (via search and matching) and financial market frictions (via a costly state verification problem). In our model the matching efficiency is a function of the financial frictions. Therefore, if financial frictions fluctuate in response to a shock, there will be an additional direct effect (via matching efficiency) on the labor market on top of the standard indirect effect (via the external finance premium on economic activity).====Fig. 1 presents estimated time series for match efficiency - measuring frictions in the labor marke - and a measure of financial frictions developed in Hall (2013).==== This graph shows that prior to the Great Recession, the labor market was highly efficient and financial frictions were low. Further, there was a clear negative correlation between the two variables. This implies high efficiency in one market and a high efficiency in the other market. This holds for the recession in 2001, but does not hold for the Great Recession. It appears that there is a structural break in the relation between those two variables. This point occurs around 2009 when both time series started to positively comove. From 2009 on, increasing labor market efficiency and decreasing financial market efficiency occurred simultaneously.====The central idea of this paper is that this negative link contributed to the jobless recovery. While output returned to its pre-crisis level fairly quickly unemployment stayed above its pre-crisis value for a prolonged period of time. Unemployment peaked around the first quarter of 2010 and only gradually declined. This is the exact time our preliminary analysis identifies the negative relationship between financial and labor market efficiency. Put differently, the observed pattern might offer some insights into the slow recovery of the U.S. economy. First, higher financial frictions interfere with a proper functioning of financial markets and, since investment should be inversely related to this friction, this will - ceteris paribus - lead to a lower level of output. Second, a lower match efficiency leads to less jobs created and the value of the out-of-labor force option increases; two phenomena that we observed in the United States: a strong and fairly persistent increase in the unemployment rate and a drop in the labor market participation rate.====The paper presents a stylized model with this link, in order to assess whether this channel generates important qualitative differences that warrant further analysis. For this purpose, we build and estimate a DSGE model that features this channel. It should be stressed that the story in this paper is not a propagation story in the sense that combining search and financial frictions will amplify the effect of shocks. On the contrary, we establish a direct link between the two frictions, a novelty in the literature. The model presented in this paper is what Blanchard (2018) calls a “theoretical” model, in that the model is not designed to fit the data perfectly, but to capture the relevant dynamics. We start with an established framework that captures the essential macroeconomic characteristics, but is still tractable and investigate an extension. Along this line, the link between financial and labor market frictions is modelled as an ad-hoc relationship. This approach allows to model all factors affecting match efficiency (discussed below) at the same time and allows us to isolate the effects of the link between financial frictions and match efficiency.====The key reason why financial frictions increased during the Great Recession was credit rationing. Gavazza et al. (2016) show that credit tightening during the GFC explains roughly half of the drop in aggregate matching efficiency. Chodorow-Reich (2014) shows that employment decisions are influenced by bank lending frictions. Firms who were in a relation with a more healthy lender were more likely to obtain a loan after the fall of Lehman Brothers. A related feature of the Great recession is the large employment losses in small firms (see Greenstone et al., 2014). Small firms face tighter bank lending standards and are more heavily affected by the increase in financial frictions (see Campello, Graham, Harvey, 2010, Greenstone, Mas, Nguyen, 2014). Therefore, the increase in financial frictions reduced match efficiency on the microeconomic (firm-level) and, in aggregate, on the macroeconomic level. Firms exposed to tighter lending standards found it harder to obtain funds for investment and (associated) employment increases. Some macro-labor models assume that firms have to borrow at the beginning of a period to pay wages. This clearly presents an (extreme) example how financial market frictions would reduce the efficiency of the labor market. If a firm faces less access to the financial market it is not able to borrow (or at least not at a profit maximizing price) and, therefore, will have less incentives to hire and vacancy postings will fall. Because financial frictions persistently increased, we find a persistent spill-over towards the labor market. Further, the findings by Moscarini and Postel-Vinay (2016) support this line of reasoning. They find that the financial shock had adverse effect on the growth of very productive, large, but relatively young, firms. Hence, we observe the largest cuts in hiring and the largest increase in the vacancy yield in these firms.====As we identify the increase in bank lending standards (credit rationing) as the key driver of the negative link, it is the prime candidate to microfound this relation.==== This paper shows that this micro-level disturbance has important macroeconomic effects.====The paper closest to ours is the work by Gavazza et al. (2016) and their model could be used to microfound our channel. They show that financial shocks lead to procyclical fluctuations in matching efficiency. They build a search and matching model with endogenous firm entry and exit and heterogeneous recruiting intensity across firms. A negative financial shock reduces recruiting spending as more unemployed workers search for an open vacancy which allows firms to keep recruitment targets at lower recruitment spending. Similarly, Leduc and Liu (2016) find a procyclical recruiting intensity which implies, using their model, a slow recovery of the hiring rate. Shimer (2012) shows that real wage rigidity combined with a frictional labor market generates jobless recoveries. Similarly, Schmitt-Grohé and Uribe (2017) show that liquidity traps cause jobless recoveries. In their model, the interaction of nominal wage rigidity, a confidence shock, and a zero lower bound on the nominal interest rate gives rise to a jobless recoveries. Adverse confidence shock decrease inflation and the central bank reacts by cutting nominal interest rates until the zero lower bound constraint binds. Then, due to nominal wage rigidity, wages do not fall as much as prices do and real wages are too high to establish full employment. Technological progress can increase output growth but fails to increase employment and, hence, a jobless recovery emerges.====Calvo et al. (2012) show that financial crises have been associated with jobless recoveries.==== They show that financial crises are particularly bad for the labor market as they amplify the usual adverse effects of recessions. Due to the different nature of financial crises it might be expected that they have larger effects on output. However, it is less obvious that they have larger effects on the labor market. They develop a stylized model with binding collateral constraints and the assumption that labor costs are harder to collateralize than physical capital. Therefore, capital-intensive projects benefit and the recovery becomes jobless.====Further, the paper adds to the literature that combines search and matching and financial frictions.==== Chugh (2013) shows that the financial accelerator channel increase the fluctuations in response to a technology shock. Petrosky-Nadeau (2014) builds a model where vacancy costs require external financing on a frictional credit market. In an expansion, financing constraints are eased and less resources are allocated towars financing job creation. In this model, this additional cost channel increase persistence of shocks. Caggese and Perez (2015) study the interaction between simultaneous financial shocks to households and firms. They also document an increased propagation in their model. Similarly, Zanetti, 2015, Zanetti, 2019 studies financial shocks and shows the sizable effects on labor market variables. Mumtaz and Zanetti (2016) extend this literature by documenting differences in the sign of the interaction effect between the two types of frictions conditional on the type of shock. For example, for a technology shock the interaction effect reinforces the original effect by affecting the hiring decision leading to persistent movements in employment and the return on capital. In contrast, for a cost-push the opposite holds. Here, the real cost of repaying existing debt is reduced which, in turn, lowers the external finance premium. Finally, Christiano et al. (2011) build an open economy version of a search and matching model with financial frictions.====Several results stand out. We build a DSGE model with search and matching frictions and financial frictions that includes our new channel: a direct link between labor market and financial frictions. We estimate both models (with and without the link) using Bayesian methods on U.S. data and show that the augmented model fits the data better than the model without the link. Further, the augmented model, with the strong, negative relation between the two frictions observed in the data, generates a jobless recovery and creates a strong internal propagation mechanism. The model dynamics are in line with the stylized empirical facts of the Great Recession.====Our results point out that the reaction of labor market efficiency differs across types of recessions: demand-side recessions generate the smallest effects on match efficiency, while supply-side and financial recessions generate the largest reactions in match efficiency.",Jobless recoveries: The interaction between financial and search frictions,https://www.sciencedirect.com/science/article/pii/S0164070419300916,8 May 2019,2019,Research Article,248.0
"Dées Stephane,Zimic Srečko","Banque de France and Larefi (University of Bordeaux), Contact information: Banque de France, 31 rue Croix des Petits Champs, 75049 Paris cedex, France,European Central Bank, Contact information: European Central Bank, Sonnemannstrasse 22, 60314 Frankfurt am Main, Germany","Received 20 February 2018, Revised 2 May 2019, Accepted 4 May 2019, Available online 7 May 2019, Version of Record 16 May 2019.",https://doi.org/10.1016/j.jmacro.2019.103123,Cited by (7),"We explore empirically the role of noisy information in cyclical developments to separate fluctuations due to genuine changes in fundamentals from those driven by temporary animal spirits (or noise shocks). Exploiting the fact that the econometrician has a richer dataset in some dimensions than the consumers, we use a novel identification scheme in a structural VAR framework and show that noise shocks are important drivers of business cycle fluctuations. In particular, noise shocks play a large role in consumption expenditures showing how false perceptions about future fundamentals influence consumer behaviors.","The 2008/09 financial crisis and the Great Recession that followed has led many observers and academics to interpret the recession as a sharp decline in aggregate demand resulting from a collapse in confidence. The sharp collapse in confidence might have led to a self-fulfilling shock to expectations.==== This suggests that the way households and firms form expectations of the future may therefore be an independent driving force of the business cycle.====The idea that agents’ beliefs may be a source of economic fluctuations has a long history. Pigou (1929) was among the first to stress that expectations were key in explaining business cycles, as psychological factors (i.e. undue optimism and pessimism) lead entrepreneurs to make errors when forming their expectations about future profits. These errors generate cycles through rises and falls in investment. These psychological factors are also very often called “animal spirits”, following Keynes (1936). Although the Real Business Cycle theory does not incorporate such psychological factors in its explanation of economic fluctuations, Pigou’s ideas have recently been reintroduced into the theory of cycles in the context of equilibrium business cycle models, notably by Beaudry and Portier (2006) or Jaimovich and Rebelo (2009). In these models, although technology remains the only determinant of output in the long run, news about future fundamentals can imply a change in expectations, which affects agents’ behaviors in the short run in anticipation of fundamental changes. However, economic agents receive only noisy signals about future technology, leading to expectational errors (Lorenzoni (2009)). If the information subsequently appears to have been wrong (i.e. it was just noise), the agents readjust their expectations and decisions accordingly. Conversely, if the information proves correct, the economy adjusts gradually to the level of activity consistent with technology. These changes in expectations generate economic fluctuations, both in the short and the long term.====Blanchard et al. (2013) explore the role of noisy information in cyclical developments empirically, separating fluctuations that are due to genuine changes in supply (news shocks) from those due to temporary expectational errors (noise shocks). They show that the identification of those shocks is only possible via the estimation of a full structural model, such as the one in Barsky and Sims (2012) or Hürtgen (2014). Since economic agents face a signal extraction problem when separating news from noise shocks, the econometrician, using the same data, cannot use structural VARs to recover such shocks. However, although this point is valid in real time, the econometrician can potentially have access to a richer dataset in hindsight.====Based on this idea, this paper shows that a structural VAR model can be used to identify news and noise shocks. First, while economic agents can observe only current and past data, the econometrician can also observe “future” data. In other words, by using the data from the whole sample, the econometrician can have a better estimate of the technological trends than the economic agents. Second, the economic agents only observe real-time data, while the econometrician also has access to revised data.====Recent papers have also proposed alternative ways to solve the issue put forward by Blanchard et al. (2013). Forni et al. (2017) use a modification of the structural VAR method to disentangle real from noise shocks, using future data and future residuals. However, their methodology is applicable only when the true state of the economy can be exactly retrieved. In this paper we show that our methodology can be used to approximately identify supply and noise shocks in more general models, such as that of Blanchard et al. (2013), in which the true state of economy can never be retrieved. Masolo and Paccagnini (2018) identify noise shocks by exploiting the informational advantage of the econometricians, who have access, with the benefit of hindsight, to data revisions as well. While we also use the noisy early data releases to identify the noise shocks, we also base our identification of agents’ misperceptions about the future using forecast errors regarding trend output. Other recent research include Enders et al. (2013), who identify noise shocks in a standard VAR model by including ‘nowcast errors’, defined as the difference between actual GDP growth (available during the next period) and growth estimated contemporaneously by professional forecasters. Similarly, Benhima and Poilly (2017) also use a sign restriction approach that relies on survey expectation errors. By expanding the identification strategy with expectation errors on inflation, they are also able to identify separately supply- and demand-noise shocks. Compared to those authors, the main innovation of our approach is to make use of potential output to measure the misperceptions of economic agents. We therefore use data at much later dates to form estimations of the output gap, which is then used for the identification of noise shocks. Using potential output has two advantages. First, it uses more data that becomes available at futures dates, instead of using only information available during the next period. Second, it is likely to be more directly linked to agents’ decisions regarding consumption.====We start our analysis by designing a slightly modified version of the model by Blanchard et al. (2013) and show how a structural VAR can be used to disentangle news and noise shocks. The methodology relies on the forecast errors consumers make when predicting the trend of GDP. These forecast errors are estimated by exploiting the fact that the econometricians have access to ‘future’ and revised data. This approach is first used to show how the superior information set of the econometrician can be separated into more precise information and information that is available at a later date. We apply thereafter this method to U.S. data and identify supply and noise shocks in two separate VAR models: a simple two-variable VAR model relying on restrictions based on our theoretical model and a large-scale VAR model where the empirical restrictions on predictions are derived from the full structural model of Hürtgen (2014). Our empirical exercises show that the identified supply and noise shocks have effects as predicted by our theoretical model. A permanent supply shock has an expansionary effect, which builds through time until all variables settle at a new, higher value. A noise shock also has an expansionary effect on the economy, but the impact fades away over time until all variables settle at their initial value. Nevertheless, noise shocks are as important as permanent supply shocks for business cycle fluctuations, explaining 25 percent of output variations at business cycle frequencies. On the other hand, permanent shocks drive the economy in the long run, but also account for around 20 percent of output variations at business cycle frequencies. Interestingly, consumption is even more affected by noise shocks (25–30 percent of variations in consumers’ expenditures at business cycle frequencies), showing the extent to which false perceptions about future fundamentals play a role in consumers’ behaviors.====After a presentation of the theoretical model in Section 2, we explain in Section 3 the problems related to identifying noise shocks in the data with structural VAR models. We then show that, by using future observations and revisions of data, we can circumvent those problems and still use VAR models to extract supply and noise shocks. In Section 4, we present empirical evidence on the effects of supply and noise shocks, by applying our methodology to U.S. data. Several robustness checks are performed in Section 5, all confirming our main results. We conclude in Section 6.","Animal spirits, fundamental factors and business cycle fluctuations",https://www.sciencedirect.com/science/article/pii/S016407041830079X,7 May 2019,2019,Research Article,249.0
Kim Wongi,"Department of Economics, Chonnam National University, 77 Yongbong-ro, Buk-gu, Gwangju 61186, Republic of Korea","Received 1 April 2018, Revised 30 April 2019, Accepted 4 May 2019, Available online 6 May 2019, Version of Record 15 May 2019.",https://doi.org/10.1016/j.jmacro.2019.103124,Cited by (17),"In this paper, I empirically examine how uncertainty about government spending policy affects economic activity by using US time series data. To this end, I build government spending policy uncertainty indexes and estimate a proxy structural vector ==== (VAR) model. The model shows that an increase in government spending policy uncertainty has negative, sizable, and prolonged effects on economic activity. Firms’ external financing premiums seem to be an important transmission channel of government spending policy uncertainty shocks. The results also imply that the standard recursive VAR model systematically underestimates the adverse effect of government spending policy uncertainty. I also discuss the advantages and disadvantages of the proxy VAR versus the sign restriction VAR.","What are the macroeconomic consequences of fiscal policy uncertainty? Many economists and policy makers have recently expressed concern about the adverse effects of fiscal policy uncertainty on economic activity. In the United States, for example, the debt-ceiling crisis, the federal government shutdown, and the Trump administration’s plan for huge infrastructure investment have elevated government spending policy uncertainty, and many are worried about the consequent adverse effects. In Europe, there are concerns about future spending paths and tax policy owing to the escalated government debt level since the European debt crisis. Uncertainty about a consumption tax hike is one of recent biggest issues in Japan.====This study investigates the size of such adverse effects of fiscal policy uncertainty, particularly government spending policy uncertainty, and its transmission channels. To this end, I build two government spending policy uncertainty indexes for the United States and estimate the proxy structural vector autoregression (SVAR) model using those indexes and US time series data.====I build two government spending policy uncertainty indexes, a “disagreement index” and a “combined index”, based on well-known uncertainty measures: the Philadelphia Fed forecasting disagreement measure and the uncertainty index provided by Baker et al. (2016). The forecasting disagreement measure has long been used in the literature to capture increases and decreases in uncertainty since the pioneering work of Bomberger (1996). The Philadelphia Fed provides a measure of forecasters’ disagreements about US federal government consumption and investment based on its Survey of Professional Forecasters (SPF). Also, Baker et al. (2016) provide a category-specific uncertainty index, focusing on a specific policy, based on their newspaper method. I use their government spending policy uncertainty index.====Next, I estimate the proxy SVAR model developed by Stock and Watson (2012) and Mertens and Ravn (2013) using the indexes. The proxy SVAR model uses external instruments to identify exogenous shocks. To identify an exogenous shock of government spending policy uncertainty, I use the defense news constructed by Ramey and Zubairy (2014) as an instrumental variable (IV). Defense news, the expected discount value of government spending induced by overseas military events, is unlikely to be related to US economic conditions. Moreover, it contains information on the future path of government spending. It may thus affect uncertainty about future government spending.====The main findings can be summarized as follows. First, government spending policy uncertainty has prolonged negative effects on economic activity and those negative effects are not negligible. An exogenous increase in government spending policy uncertainty reduces gross domestic product (GDP), private consumption, investment, labor hours, real wages, and markups but increases firms’ external financing cost and price levels. The increase in price levels would be caused by an increase in firmsâ;; external financing costs and a consequent increase in marginal costs. This external financing cost channel has received relatively little attention from researchers. Pioneering research with dynamic stochastic general equilibrium (DSGE) done by Born and Pfeifer (2014) and Fernández-Villaverde et al. (2015) emphasize the countercyclical markups to explain inflation in response to fiscal uncertainty shocks rather than the cost push channel via the external financing premium.====Second, the standard recursive VAR, which is widely used in research on policy uncertainty, tends to underestimate the adverse effects of government spending policy uncertainty. The results using the recursive VAR reveal that the effects of spending policy uncertainty on economic activity are small. This result could be caused by the shortcomings of the identification strategy in the recursive VAR as pointed out by Ludvigson et al. (2015). I also discuss the advantages and disadvantages of the proxy VAR versus the sign restriction VAR.====Uncertainty has attracted renewed attention since the 2007/08 financial crisis. Bloom (2009) shows that uncertainty harms economic activity through several channels, such as the wait-and-see option channel and the precautionary saving motive, using empirical and theoretical models.==== Since the pioneering work of Bloom (2009), several studies have tried to show how uncertainty impacts economic activity.==== Among these, some research focus on uncertainty about specific policies. For instance, Mumtaz and Zanetti (2013) focus on monetary policy uncertainty; they use a SVAR model and US data to show that such uncertainty, measured as the time-varying volatility of monetary policy shocks, harms economic activity. Handley (2014) investigates the effects of trade policy uncertainty on firmsâ;; exporting behavior using panel data analysis.====A few studies focus on the effects of fiscal policy uncertainty. Davig and Foerster (2014) focus on the effects of uncertainty about tax policy using a calibrated DSGE model. Born and Pfeifer (2014) and Fernández-Villaverde et al. (2015) focus on uncertainty about fiscal policy. Born and Pfeifer (2014) show that monetary and fiscal policy uncertainty, measured by time-varying volatility, are not the major sources of business fluctuations using an estimated DSGE model. Fernández-Villaverde et al. (2015) show that capital tax uncertainty reduces output, private consumption, and investment, at least in the short run, through a VAR analysis and an estimated DSGE model. Ricco et al. (2016) investigate how the effectiveness of government spending varies depending on levels of government spending policy uncertainty using a nonlinear VAR model. However, they do not examine the effects of uncertainty about government spending policy itself.====Although these studies provide helpful insights on the macroeconomic consequences of fiscal policy uncertainty, most focus on tax policy uncertainty, rather than government spending policy uncertainty. Furthermore, I use a more data-driven method while they use structural models. Most significantly, I provide a new empirical strategy for identifying government spending policy uncertainty shocks.====The rest of this paper is structured as follows. Section 2 provides a detailed explanation of how I build the government spending policy uncertainty indexes. In Section 3, I briefly describe the proxy SVAR, data set, and other econometric issues. Section 4 provides empirical results, and Section 5 discusses them. Finally, Section 6 concludes the paper.",Government spending policy uncertainty and economic activity: US time series evidence,https://www.sciencedirect.com/science/article/pii/S016407041830140X,6 May 2019,2019,Research Article,250.0
"Fisher Lance A.,Huh Hyeon-seung","Department of Economics, Macquarie University, Sydney 2109, Australia,School of Economics, Yonsei University, 50 Yonsei-ro, Seodaemun-gu, Seoul 03722, Republic of Korea","Received 23 April 2018, Revised 27 April 2019, Accepted 4 May 2019, Available online 5 May 2019, Version of Record 20 May 2019.",https://doi.org/10.1016/j.jmacro.2019.103125,Cited by (1),"This paper derives the sufficient conditions on the coefficients in a system of structural equations which imply the long-run exclusion restrictions on the impulse responses which are used to identify the model. A recent contribution shows a method to impose the sufficient conditions on the structural equations and to estimate them by ==== (IV). This method has the advantage that it can be combined with a new method for sign restrictions which can be cast in an IV framework. This paper shows that the sufficient conditions which imply the two long-run exclusion restrictions in a SVAR taken from the literature imply other long-run exclusion restrictions as well which are not part of the identifying assumptions. In this case, this method is not suitable. This paper shows how to impose the two long-run exclusion restrictions directly on the structural equations of the model on each draw in sign restrictions which utilizes the IV method.","The method in Fisher, Huh and Pagan (FHP, 2016) for imposing zero long-run identifying restrictions in SVARs, which draws on the earlier instrumental variables (IV) approach of Shapiro and Watson (1988), is particularly suitable for combining long-run zero restrictions with the sign restrictions methodology of Ouliaris and Pagan (OP, 2016) in an IV setting. In fact, OP (2016, p. 614) suggest the method in FHP as one way to combine the two long-run zero restrictions on the responses with their sign restriction methodology in the context of their small macro model. The method in FHP shows how to impose the sufficient conditions on the coefficients in a system of structural equations which imply the long-run zero restrictions on the responses which are used to identify the model. However, the sufficient conditions in some SVARs can imply other long-run zero restrictions on the responses as well which may not be justifiable on economic grounds. In such circumstances, the method in FHP is not suitable and another method is required which can be easily combined with the sign restrictions approach of OP, which utilizes IV estimation. This paper develops such a method in the context of Peersman's SVAR (2005). The paper also considers a variation of Peersman's model for which the method in FHP is suitable.====Traditionally, SVARs are identified by imposing zero restrictions on the contemporaneous and/or long-run impulse responses of the variables to certain shocks. An early example is Galí (1992) who identified the structural shocks by a combination of contemporaneous and long-run zero restrictions on the responses. Peersman (2005) also used both types of exclusion restrictions for the case of full parametric identification of his model. Fisher et al. (2000) extended the identification that utilizes both contemporaneous and long-run zero restrictions to cointegrated SVARs.==== The methods of these papers produce a system of non-linear equations in the unrestricted coefficients which are solved for using a non-linear equation solver. In an important paper, Rubio-Ramírez et al. (2010) formulate the problem of imposing contemporaneous and long-run zero restrictions to exactly identify a SVAR as one of finding a rotation matrix ====, which satisfies the exclusion restrictions. This produces a system of linear equations in the unrestricted coefficients which can easily be solved for using matrix methods. In the same paper, they establish the rank conditions for identification of SVARs.====Sign restrictions on the impulse responses have recently become a popular way of identifying the shocks in SVARs. The sign restrictions approach was first introduced by Faust, 1998, Canova and De Nicoló, 2002 and Uhlig (2005). This approach involves rotating an initial set of orthogonal shocks in a SVAR by applying an orthogonal rotation matrix ==== to form a new set of orthogonal shocks from which impulse responses are obtained. If the responses satisfy the sign restrictions they are retained and, if not, they are discarded. In order to consider the responses from all possible SVARs that are consistent with the reduced-form VAR, the rotation matrix ==== on each draw needs to come from the space of all possible orthogonal matrices ====. Canova and De Nicoló (2002), and Peersman (2005) construct ==== by way of Givens rotation matrices while Rubio-Ramírez et al. (2010) obtain ==== from the QR decomposition of a matrix, which may be computed using the Householder transformation. Baumeister and Hamilton (2015) point out that the method of generating the rotation matrix ==== will affect the distribution of the impulse responses before the sign restrictions are applied.====In the OP (2016) approach to sign restrictions, the above diagonal elements in the matrix of contemporaneous coefficients in the SVAR (the ==== matrix in Eq. (1) below) are drawn randomly and the elements along the principal diagonal are normalized to unity. For each draw of the above diagonal elements, the structural equations of the model are estimated by instrumental variables (IV) estimation. Impulse responses are then obtained and judged by the sign restrictions. Fisher and Huh (2016) apply this method to SVARs of small open economies. This method, rather than sampling from the space of all possible orthogonal ==== matrices, samples from the set of all possible values for the elements of ====, which is typically the set of all real numbers. The Baumeister and Hamilton (2015) observation applies here as well as the method by which the above diagonal elements of ==== are generated will affect the distribution of the impulse responses before the application of signs.====There is an extensive literature on combining parametric restrictions with sign restrictions when the latter are implemented by rotating the initial set of orthogonal shocks with the matrix ====. Baumeister and Benati (2013) impose a single contemporaneous zero restriction on a response in combination with sign restrictions utilizing the Givens rotation matrix. Haberis and Sokol (2014) extend their methodology to multiple contemporaneous zero restrictions with signs. Benati and Lubik (2014) and Benati (2015) impose long-run exclusion restrictions on the responses in combination with sign restrictions using the Householder transformation i.e. by obtaining ==== from a QR decomposition. A more general algorithm for combining contemporaneous exclusion restrictions, long-run exclusion restrictions and sign restrictions which utilizes the Householder transformation has been developed by Arias et al. (2018). It is an extension of the algorithm in Rubio-Ramírez et al. (2010) to the case of where there are fewer zero parametric restrictions than needed for exact identification and sign restrictions are added to these to complete the identification. A detailed exposition of both algorithms is provided in Kilian and Lütkepohl (2017). Binning (2013) developed an algorithm which is similar to that of Arias et al. (2018).====OP (2016) provide two examples of combining exclusion restrictions on the responses with their sign restrictions approach. The first combines a contemporaneous zero restriction on an impulse response with their signs approach in a model of the interest rate, inflation and the output gap. They present an algorithm, which estimates the structural equations by IV to implement their signs approach given the contemporaneous restriction. The second combines long-run zero restrictions on the responses with their signs approach in a model of the interest rate, inflation and the log level of output. In this model, output is an I(1) variable. They impose two long-run zero restrictions, namely, that the shocks associated with the interest rate and inflation equations have a zero long-run effect on output and combine these two parametric restrictions with their sign restriction approach to fully identify the shocks. They suggest that the two long-run zero restrictions can be imposed on the SVAR using the method in FHP (2016). In this paper, we show that the method in FHP is suitable to use in this case because the sufficient conditions on the coefficients in the structural equations which imply the two long-run zero restrictions on the responses have no other implications. However, this is not necessarily always the case. In this paper, this is demonstrated for the SVAR of Peersman (2005).====Peersman's SVAR consists of four variables which are the oil price, output, consumer prices and the interest rate. The variables are I(1), except for the interest rate which is I(0). He separates the shocks first by using parametric restrictions alone and then by using sign restrictions alone where the signs method is implemented by way of a Givens rotation matrix. In the parametric identification, he utilizes two long-run and four contemporaneous exclusion restrictions. The two long-run restrictions are that the shocks associated with the interest rate and consumer price equations have a zero long-run effect on output. In this paper, we take Peersman's model and data and combine his two long-run zero restrictions with sign restrictions utilizing the approach of OP. We first establish the sufficient conditions on the coefficients in the structural equations which will imply that the response of output to the shocks associated with the interest rate and consumer prices is zero in the long-run. We show that these sufficient conditions imply that some of the other structural shocks have a zero long-run impact on some of the other variables as well and that such restrictions on the model are difficult to justify on economic grounds. While the method in FHP (2016) shows how to impose the sufficient conditions on the structural model, one would not want to proceed with this method as the sufficient conditions also imply zero long-run impacts of some of the shocks that can't be justified by standard macroeconomic reasoning. In cases such as this, another approach which can easily be combined with the sign restrictions approach of OP is required.====In this paper, we develop a method to directly impose the two long-run zero restrictions on the responses in Peersman's model, and which is combined with sign restrictions following the approach of OP. This method involves finding the values for certain contemporaneous coefficients in the structural equations which enforce the long-run exclusion restrictions on each draw in the sign restrictions approach of OP. This method is not as general as the approach of Arias et al. (2018), which can implement zero parametric and sign restrictions in SVARs of a general nature. The method here is more restrictive because in some SVARs it may not be possible to solve for the values of the contemporaneous coefficients to enforce the long-run restrictions on each draw. However, this is not the case in Peersman's SVAR which is considered here.====The structure of the paper is as follows. Section 2 describes the econometric methods in a three variable setting and discusses their application to the small macro model of OP (2016). Section 3 considers Peersman's model. It shows that the method in FHP for imposing the two long-run zero restrictions on the responses is not suitable and develops a direct method to impose them in combination with the sign restrictions methodology of OP. Section 4 presents the results from this method. Section 5 considers a variation of Peersman's model for which the method in FHP is suitable and presents the results under the long-run zero restrictions and signs. Section 6 provides concluding remarks.",An IV framework for combining sign and long-run parametric restrictions in SVARs,https://www.sciencedirect.com/science/article/pii/S0164070418301708,5 May 2019,2019,Research Article,251.0
Burkovskaya Anastasia,"Rm 638 Social Sciences building A02, University of Sydney, NSW 2006, Australia","Received 7 April 2018, Revised 26 April 2019, Accepted 29 April 2019, Available online 30 April 2019, Version of Record 16 May 2019.",https://doi.org/10.1016/j.jmacro.2019.103121,Cited by (2),"This paper proposes a model that analyzes the reasons behind the establishment and persistence of ==== (CBI) in a competitive democracy where both incumbent and opposition parties have the right to veto the delegation of ====. We show that in a country with a high level of ====, the opposition party uses the absence of CBI to keep the economy unstable to increase its own chances of getting elected. The model also predicts persistence of CBI once it is established, due to the incumbent’s fear of losing the office if the autonomy is removed.","The ability of a central bank to commit to a specific monetary policy often engenders strong economic performance in the long run (Kydland, Prescott, 1977, Barro, Gordon, 1983). However, governments might have an incentive to limit central bank independence (CBI), which can hinder a central bank from sticking with a specific policy. Such perverse incentives arise from the fact that the results of future elections often depend on short-term economic performance. Without institutional control, politicians might manipulate economic variables before elections through fiscal and monetary instruments. Yet, as suggested by Rogoff (1985), creating an independent monetary authority should resolve the time-consistency problem associated with monetary policy. In other words, if monetary policy is delegated to an independent institution, the government loses access to monetary tools, which reduces the extent of a political cycle and provides more economic stability. Unfortunately, such independent delegation does not always exist.====This paper studies two questions: (1) why do some countries establish an independent monetary authority, whereas others do not? and (2) why is CBI persistent as suggested by the data? The explanation we offer applies only to competitive democracies wherein two parties have veto power in relation to the de facto independence of monetary authority. We show the presence/absence of CBI is associated with a number of parameters, including the level of corruption, the quality of politicians, the discount factor of the future, the value of economic growth and inflation in the voter preferences. Even though such parameters are responsible for creating a particular economic environment, the most striking result we obtain is that in countries with high levels of corruption, CBI might be vetoed by the opposition, not by the potentially corrupt incumbent, because the opposition is interested in keeping the economy unstable to undermine the incumbent and increase its own chances of winning the election. In addition, the model predicts persistence of CBI once it is established, because of the incumbent’s fear of losing the office if CBI is removed. We recognize that in reality, many additional reasons explain the (non)existence of CBI; however, our goal is to demonstrate an additional and non-trivial channel.====Classical models of political business cycles (Rogoff, Sibert, 1988, Persson, Tabellini, 1990) predict the permanent presence of manipulations with policy instruments in electoral years. Much empirical work confirms the existence of fiscal political cycles (e.g., Persson, Tabellini, Brender, Drazen, 2005, Akhmedov, Zhuravskaya, 2004, Brender, Drazen, 2013). However, prior investigations of political monetary business cycles have yielded mixed results, mostly due to reforms in CBI (Acemoglu et al., 2008). Studies that find evidence of pre-electoral monetary expansion (e.g., Tufte, 1978, Alesina, Roubini, 1992, Hallerberg, de Souza, Clark, 2002, Block, 2002, Burkovskaya, 2013), are focused on countries without an independent monetary institution at the time. By contrast, Drazen (2000) discovers that monetary cycles disappeared in the US after 1979, when Paul Volcker became chairman of the Federal Reserve. Furthermore, numerous empirical findings support the negative relationship between CBI and inflation (e.g., Alesina, Summers, 1993, Loungani, Sheets, 1997, Franzese, 1999, Berger, de Haan, Eijffinger, 2001, Klomp, de Haan, 2010). Despite evidence of the clear advantages of CBI for the economy, not all countries choose to establish this delegation. An important question is why? Fig. 1 demonstrates the 2012 CBI de jure index from Garriga (2016).====The economic literature offers various reasons for the absence of CBI. Cukierman (1994) points out CBI is unnecessary given political stability. McCallum (1995) argues the time-consistency problem cannot be resolved by CBI, because the government can always reverse the delegation. However, Jensen (1997) and Moser (1999) assert that the time-consistency problem can be reduced when such a reversal is costly. In addition to these reasons, researchers find a lack of desire to solve the problem of inflationary opposition (Milesi-Ferretti, 1995), inflation aversion, benefits of unanticipated inflation, and the natural rate of unemployment (Eijffinger, Schaling, 1998, Farvaque, 2002), the rise of populism (Masciandaro and Passarelli, 2018), and, in the light of the financial crisis, the richer set of tools useful during recessions (e.g., Blinder, 2013).====In contrast to the above, our contribution to the literature centers on general change and the persistence of institutions. Acemoglu, Robinson, 2000, Acemoglu, Robinson, 2001, Acemoglu, Robinson, 2008 and Acemoglu et al. (2008) discuss the relationship between distribution of de jure and de facto political power and the persistence and change of institutions. The authors argue those with political power who are negatively affected by the policy reform are likely to use that power to block its effective implementation. Indeed, the main reason for the absence of CBI in our model is one of the politicians. The incumbent might be corrupt or the opposition might veto CBI to improve her own chances of winning the election.====An example of the first scenario is modern-day India, where demand for CBI comes from the central bank and economists; however, the incumbent government is trying to undermine the attempts through fixing the bill in its own interests.==== By contrast, in Brazil, active preparation of a CBI bill was taking place after the impeachment of President Dilma Rousseff in 2016.==== However, a year later, the reform was postponed due to lack of support in Congress.==== Another example includes South Korea in 1997 before it finally achieved CBI. The incumbent formed a commission that strongly recommended improving the independence of the monetary authority (Cargill (2001)), but the recommendation did not proceed due to lack of support. Not until a financial crisis occurred and the International Monetary Fund applied pressure did South Korea establish CBI in 1998.====Another key point is that multiple studies of various CBI indices over different time periods (e.g., Crowe, Meade, 2007, Crowe, Meade, 2008, Dincer and Eichengreen (2014)) indicate the steady development and persistence of independence of monetary authority over time. Even in spite of the pressure to reduce CBI during the financial crisis, de Haan et al. (2018) show CBI has not diminished since then.====As long as the country experiences strong political competition, changing the status of de facto CBI would presumably require an agreement between the majority of the main domestic political actors. The previous literature suggests several veto players approving CBI lends it credibility (e.g., Moser (1999), Keefer and Stasavage (2003)), which might explain the persistence. However, the presence of two veto players delivers credibility only if at least one of the players is not interested in undermining CBI. In our model, the presence of two veto players is the reason for a lack of CBI in some cases, whereas persistence is guaranteed through the threat of economic instability and loss of office for the incumbent. For example, a Central Bank reform in Argentina in 2012 away from independence resulted in hyperinflation and loss of the next presidential elections for the Frente Para la Victoria==== candidate. In other words, the incumbent is the guarantor of the credibility of CBI when put in place, whereas the opposition might be using its veto power to prevent the establishment of CBI in the first place.====This paper also relates to the models of political business cycles inspired by the pioneering work of Nordhaus (1975). Alesina (1987) models partisan cycles with the economy influenced by electoral expectations. After the rational expectations revolution, Rogoff and Sibert (1988) and Rogoff (1990) propose a signaling model with rational voters, who do not have information on the politician’s type. In this model, politicians have different levels of competency in delivering government services. Persson, Tabellini, 1990, Persson, Tabellini, 2000 apply the approach developed by Rogoff and Sibert to monetary policy and the economy described by the Phillips curve. Drazen (2000) introduces a separate monetary authority that is neither directly associated with the government nor completely independent. Ferre and Manzano (2014) add CBI to the partisan model of Alesina (1987). In the current study, we modify the model of Persson and Tabellini (2000) by adding a negotiation stage wherein both candidates might agree to establish or abolish CBI.====In this paper, we offer a toy two-period game that helps demonstrate the additional causes behind the existence or absence of CBI. We restrict the political business cycle model of Persson, Tabellini, 1990, Persson, Tabellini, 2000 to two periods, while adding CBI as a potential monetary policy provider. The game goes as follows: An incumbent and a contender are up for elections at the end of period 1. Before any economic activity in period 1 starts, both politicians decide whether to change the current status of the central bank. The status can be changed only if both parties agree on the issue. The voters observe the current status of the central bank and know whether it has changed. The voters do not observe the actual voting, and they form their expectations about inflation based on the status of CBI. All economic activity is described by a rational expectations Phillips curve that takes into account the ability type of the incumbent to provide economic policy. Such ability can be high or low. Note the low-type politician might be interpreted as a populist. After period 1 economic activity occurs, the voters observe the current level of output and form beliefs about the type of the incumbent. At the end of period 1, elections take place, and in period 2, post-electoral economic activity occurs. The game has two types of perfect pure strategy equilibria: (1) separating equilibrium – both politicians agree on CBI in the beginning of the game; and (2) pooling equilibria – the contender vetoes CBI, keeping monetary tools in the incumbent’s hands. In pooling equilibria, both types stimulate the economy to produce the same output in period 1, preventing voters from distinguishing the quality of the economic policy or the performance of the incumbent. Multiple pooling equilibria of this type occur, and each of them can be characterized by two parameters: the level of inflation expectations and the probability of incumbent re-election, which is bounded from above by the probability of the high-type politician.====The existence of one or another equilibrium is defined by a number of parameters in the model. The rent from office holding, which we associate more with corruption than with the presidential salary, and the future discount factor both reduce the probability of CBI. The reason is that the higher rent from office-holding and the discount factor increase re-election incentives for both types of politicians. Consequently, the low-type incumbent would prefer to mimic the output of the high type through monetary expansion, for which the absence of CBI is crucial. The probability of a high-type politician, the weight of output in voter preferences, and the difference between the politiciansabilities have a positive influence on the establishment of CBI. All of these parameters make the low-type incumbent prefer a more able politician to run the country instead of herself.====On the other hand, the rent from office-holding, the future discount factor, and the probability of incumbent re-election positively affect the existence of any given pooling equilibrium. All of these factors increase the incumbent’s value from being re-elected and encourage the low-type incumbent to hide private information about her type. However, this story is not complete. The above justification explains why the type of equilibrium is pooling rather than separating in the absence of CBI, yet the reason for the actual absence is different: As long as the probability of incumbent re-election is lower than that of the high-type probability (which is the existence requirement), ==== prefers the lack of CBI to increase her own chances of winning election. Specifically, imagine a situation with high inflation expectations and the high level of corruption that guarantees high rent from office-holding. As a result, the low-type incumbent is encouraged to provide monetary stimulation in the absence of CBI. Even though an incumbent of any type might prefer living as a private citizen in a country with CBI and no inflation instead of running the country with high inflation, the foundation of CBI still fails, because voters generally do not favor any incumbent government due to high uncertainty about how well the country is actually run. The opposition (contender) is interested in keeping that uncertainty in place by refusing CBI.====However, if the game starts with CBI, independence will never get reversed. If an incumbent agrees to do so, the voters will immediately take it as a signal of the low type being in power and form inflation expectations. Therefore, the incumbent will never be able to get re-elected. To make matters worse, she will have to face inflation. This explanation is in line with the high-cost delegation argument in Jensen (1997) and Moser (1999), but in our model, the cost of delegation is ==== high because the price is losing the office. Such a high cost delivers persistence of the delegation and solves the time-consistency problem.====Our model relies on several fundamental assumptions: (1) The country is democratic, (2) at least two veto players are in the process of determining the de facto status of the central bank, (3) the voters observe only CBI status and not the voting behind it, and (4) the independence variable can take only two values: either the central bank is independent or it is not. Below, we discuss limitations imposed by these assumptions in more detail.====First, democracy is crucial for the existence of political cycles. A dictator has a wide range of means==== to win political competition; therefore, the ruler does not usually need to stimulate the economy. Consequently, the economy might show stability during the regime (e.g., Mexico during Porfiriato, Putin’s Russia). Moreover, the dictator is interested in improving the quality of work done by the central bank, because it contributes to the stability of the regime itself. For example, Mexico increased the autonomy of its central bank in 1993 under the Partido Revolucionario Institucional.==== However, real independence of monetary authority is not possible in the presence of a dictator.====Second, as we already discussed, two veto players provide credibility to CBI when it is established and permanent absence otherwise. Removing this assumption would allow the incumbent to change the delegation at any moment. The model would then predict the existence of only one equilibrium with CBI. In fact, the high-type incumbent will always choose CBI, whereas the low-type incumbent might prefer its absence. However, reversing independence would send a negative message about the politician’s type. As a result, the equilibrium will be separating regardless of the actual choice of the incumbent. Given that CBI delivers price stability, even the low-type incumbent would choose to keep it in this case.====Third, we assume the voters do not observe the actual voting, because in many cases, when de facto CBI is permanently absent, no voting actually takes place. The incumbent or the other party might want to introduce CBI; however, such a proposal often dies before reaching the formal legislative process or serious practical consideration, due to clear lack of support (e.g., Brazil). Even though nowadays a lot of information about political process and discussions are easily available on the internet, voters often do not pay attention to such details and stay quite uninformed. For example, the 2014 Annenberg Public Policy Center survey found only 27% of Americans knew a two-thirds vote of the House and Senate is necessary to override a presidential veto, and a similar picture was drawn in relation to other survey questions on political knowledge. This fact of political ignorance is the reason behind what we model as unobservable voting. If the voters were more cautious about the behind-the-scenes political process, we would have to give up this assumption, in which case, voters could punish the party that vetoes CBI, hence leading to the only separating equilibrium with CBI.====Fourth, CBI takes only two values: 0 and 1. In reality, CBI is a spectrum of outcomes, and any independence index would be a combination of a number of different attributes. Extending this assumption to different degrees of independence necessarily brings the question of how to model the effect of various levels of CBI on the economic outcomes. Potentially, this issue could be addressed by allowing the incumbent to generate limited inflation depending on the degree of CBI. Accounting for this extension would likely change the conditions on the existence of each type of equilibria and potentially add new ones.====This paper is organized as follows: Section 2 introduces the model. Section 3 analyzes the possible equilibria. Section 4 provides comparative statics. Section 5 concludes. All proofs are provided in the Appendix.",Political economy behind central bank independence,https://www.sciencedirect.com/science/article/pii/S0164070418301484,30 April 2019,2019,Research Article,252.0
"Corrado Carol,Haskel Jonathan,Jona-Lasinio Cecilia","The Conference Board and Georgetown University Center for Business and Public Policy, USA,Imperial College Business School, CEP, CEPR and IZA, England,ISTAT and LUISS Lab of European Economics, Rome","Received 19 March 2018, Revised 19 April 2019, Accepted 22 April 2019, Available online 24 April 2019, Version of Record 17 May 2019.",https://doi.org/10.1016/j.jmacro.2019.04.006,Cited by (4),"How has capital reallocation affected productivity growth since the financial crisis? For example, have low ==== disrupted the reallocation process? This paper calculates the effect on productivity growth of capital reallocation between ====.","It is widely alleged that, since the financial crisis, the financial system has been impaired so that it functions less well in allocating capital thus restraining productivity growth; see for a review e.g. Borio et al. (2018). Whilst this suspicion is widespread, it has proved difficult to gather evidence to examine it.====This is perhaps unsurprising because determining financial impairment and its impact is hard for at least four reasons. First, we require a plausible counter-factual against a well-functioning system. That is we need to calculate (1) productivity (or productivity growth) if capital were being allocated to “right” sectors and compare that to (2) productivity (or productivity growth) under the current, allegedly, misallocated situation. Second, we need information before and after the financial crisis (and perhaps over countries as well). Third, we need a sense of scale. Suppose for example that a very particular, new high technology investment is misallocated. Since much of the extant (business) capital stock is buildings, any such reallocation might be too small scale to make much material difference. Finally, if we do manage to find evidence of capital misallocation, we want to know what is causing it. Maybe low interest rates make for more misjudged investment. Or, perhaps the problem is inadequate bank competition or regulation.====We see two broad streams of research that speak to the question of whether capital movement has been impaired post-financial crisis. We propose to complement these with a third. The first broad stream of work follows from the more general work on misallocation and productivity exemplified by Hsieh and Klenow (2009). This has been implemented on cross-country data before and after the financial crisis by, for example Gopinath et al. (2017), Dias et al. (2016), and Gamberoni et al. (2016); see Restuccia and Rogerson (2013) for an extensive survey. This work calculates total factor productivity (TFP) ==== relative to a benchmark undistorted equilibrium predicted by theory.====The second broad stream of work centers on financial “frictions”. Broadly, this literature regresses measures of TFP growth on candidate variables measuring financial frictions that might impair the movement of capital from less to more productive activities. Cecchetti and Kharroubi (2015) look at TFP growth and R&D, Cette et al. (2016) focus on TFP growth and interest rates, Caggese and Perez-Orive (2017) develop a model that predicts low interest rates impair investments in an intangible-intensive economy, and McGowan et al. (2017) investigate whether slow TFP growth is linked to debt-laden (“zombie”) firms.====In this paper we implement a third method using a measure developed by Jorgenson and co-authors (e.g., Jorgenson, Gollop, Fraumeni, 1987, Jorgenson, Ho, Samuels, Stiroh, 2007) that we see as complementary to the above literature, the contribution of “capital reallocation” to productivity growth. The Jorgenson capital reallocation effect falls out of industry-level growth accounting. For a given type of capital used in a given industry, it is the contribution to aggregate economic growth of that capital minus the contribution calculated as if the capital earned the average, economy-wide return to all capital used in all industries. This is a reallocation effect because if capital is flowing to above-average return industries, the measure will be positive; if capital is flowing to below-average return industries it will be negative. Thus this is a ==== counterfactual that is similar in spirit to the level counterfactual in Hsieh and Klenow (2009). Note too that the Jorgenson measure refers to reallocation ==== industries, whereas the Hsieh and Klenow inspired and closely related (and so-called) “reallocation” literature (e.g., Bartelsman et al., 2013, in which there is no conterfactual) are ==== industry findings. All told, following Jorgenson and co-authors, we shall use the term “capital reallocation” or “reallocation” in this paper, not “misallocation”.====We calculate capital reallocation for countries over time, 1998–2013, and then explore factors potentially affecting it, such as real interest rates, economic optimism and banking regulation. Thus our paper is new in the following regards. First, we calculate capital reallocation over countries and time, before and after the financial crisis. Second, we perform this calculation with and without intangible assets and look at whether reallocation differs between intangible and tangible assets to determine whether the growing intangible intensity changed the relation between allocation and interest rates as argued in Caggese and Perez-Orive (2017). Third, we study the relationship between both the level and change in reallocation and the set of financial and non-financial indicators that we use to uncover the main drivers of capital reallocation.====To preview our results, our findings are the following. First, we document how reallocation varies over time. It has generally fallen in most economies since the 2000s, has fallen notably in Mediterranean countries and fell in most countries in the financial crisis years. Second, we examine some correlates of this fall. These associations reveal that reallocation has fallen in countries with more economic uncertainty and weaker financial systems. We also find that reallocation is negatively correlated with real interest rates, contrary to the hypothesis that low real interest rates have hurt reallocation, and that the effects are somewhat stronger for intangibles.====The rest of this paper proceeds as follows. The next section reviews the related literature in more detail, and following that, section three sets out some theory. Section four illustrates some descriptive results, and section five reports econometric results. Section six concludes.","Productivity growth, capital reallocation and the financial crisis: Evidence from Europe and the US",https://www.sciencedirect.com/science/article/pii/S0164070418301149,24 April 2019,2019,Research Article,253.0
Chen Shu-Hua,"Department of Economics, National Taipei University, 151 University Road, San Shia District, New Taipei City, 23741 Taiwan","Received 29 September 2018, Revised 8 April 2019, Accepted 18 April 2019, Available online 24 April 2019, Version of Record 29 April 2019.",https://doi.org/10.1016/j.jmacro.2019.04.005,Cited by (1),"I analytically show that the adoption of a linearly progressive income tax scheme destabilizes the endogenously growing economy of Barro (1990) by giving rise to dual balanced growth path equilibria, wherein the low-growth equilibrium exhibits local indeterminacy and belief-driven growth fluctuations, and the high-growth equilibrium displays saddle-path stability. I propose that both a sufficiently high lump-sum taxes-to-capital ratio and a sufficiently high ","In a seminal paper in the endogenous growth literature that explores the macroeconomic effects of tax policy, Barro (1990) shows that the ==== income tax rate should be set equal to the elasticity of output with respect to government spending so as to attain maximal levels of the output growth rate and social welfare. It is noted that the model economy studied in Barro (1990) displays saddle-path stability and hence is not subject to endogenous growth fluctuations driven by agents’ animal spirits or sunspots. While the assumption of a constant tax rate on households’ taxable income is commonly adopted in the majority of related works, it is not consistent with the progressive income tax policies observed in many developed and developing countries. Several subsequent works on the macroeconomic implications of progressive income taxation are thus motivated.==== Among the theoretical efforts, Chen and Guo (2013) study the (in)stability effects of Guo and Lansing's (1998) non-linear income tax structure in Barro's (1990) model. It is found that, in sharp contrast to traditional Keynesian-type stabilization policies, progressive taxation operates like an automatic destabilizer that magnifies growth fluctuations and thus destabilizes the economy.==== However, the growth and welfare implications of income tax policy are not the focus of the paper.====The theoretical framework of this paper differs from that of Chen and Guo (2013) in only one aspect; i.e., I adopt an alternative formulation of the income tax scheme ==== Dromel and Pintus (2007). In particular, Dromel and Pintus point out that the feature of continuously increasing average and marginal tax rates ==== (Guo and Lansing, 1998) is not consistent with the practice in countries that implement schemes of progressive income taxation. Dromel and Pintus thus propose an alternative fiscal formulation featured by a constant marginal tax rate imposed on the household’s taxable income when it exceeds a fixed exemption threshold – namely, a linearly progressive taxation.====By incorporating the fiscal formulation of Dromel and Pintus (2007) into Barro's (1990) determinate one-sector representative-agent model of endogenous growth with inelastic labor supply and productive public expenditures, this paper carries out a comprehensive analytical investigation of the interrelations between a linearly progressive income tax, equilibrium (in)determinacy, and economic growth along the economy’s balanced growth path(s). I show that, in contrast to Chen and Guo (2013) where there is a chance that the economy can maintain saddle-path stability – i.e., when the long-run average income tax rate is lower than the output elasticity of government spending and the degree of tax progressivity is below a critical level – equilibrium determinacy is impossible in this paper, which employs Dromel and Pintus' (2007) linear fiscal formulation. In particular, I find that linearly progressive income taxation leads the model of Barro (1990) to display two interior balanced growth path (BGP) equilibria, wherein the low-growth one is an indeterminate sink that exhibits belief-driven growth fluctuations, and the high-growth one displays saddle-path stability and equilibrium uniqueness.====I start from a particular BGP and suppose that agents expect an expansion in future economic activities. Acting upon this belief, the representative household will reduce consumption and increase investment today, which in turn lead to a higher stock of capital in the next period. The consequential effect on the economy’s future rate of return on capital is two-fold. First, because of diminishing returns, the return on capital investment falls (the diminishing-returns effect). Second, as a result of higher real activities, the government collects more tax revenues and hence provides more productive government spending. This increases the return on capital (the tax-revenue effect). Under high-growth BGP, the tax-revenue effect outweighs the diminishing-returns effect, leading to an increase in the return on capital. Agents’ initial optimistic expectations are thus validated as a self-fulfilling equilibrium. By contrast, for the low-growth BGP, the tax-revenue effect exerts a relatively weaker impact on the return on capital investment, hence preventing agents’ rosy anticipation from becoming self-fulfilling.====In terms of the growth-rate effect of the marginal income tax rate, I find that it can be negative, positive, or zero, and is closely linked with the local stability properties of a BGP. In particular, when the economy is on the indeterminate low-growth BGP, it displays a negative long-run growth-rate effect of the marginal income tax rate. On the contrary, when the economy is on the determinate high-growth BGP, the nexus between long-run output growth and the marginal income tax rate is positive (negative) under lower (higher) levels of the marginal income tax rate. Hence, in the high-growth BGP equilibrium, there exists an interior positive marginal income tax rate that attains the maximum rate of output growth. I find that, under linearly progressive income taxation, the growth rate-maximizing marginal income tax rate is strictly higher than the elasticity of output with respect to public expenditures. Moreover, the more progressive the income tax schedule is, the higher the growth rate-maximizing marginal income tax rate will be. This result runs in sharp contrast to that of Barro (1990), who shows that maximizing output growth is equivalent to maximizing social welfare.====Intuitively, there are two channels through which the marginal income tax rate influences the output growth rate. First, under a given level of the government spending-to-capital ratio which determines the marginal product of capital, a higher marginal income tax rate shrinks the after-tax return on capital (the direct effect). Second, a higher marginal income tax rate raises government spending. This in turn, by raising the equilibrium rental price of capital, reduces the firm’s demand for capital and hence causes reductions in production, tax revenues, and the provision of productive infrastructure. Hence, a higher marginal income tax rate may lead to a rise or a decrease in public expenditures and hence the return on capital investment (the indirect effect).====I find that the indirect effect that the marginal income tax rate imposes on the return on capital is negative on the low-growth BGP. This, together with the negative direct effect, gives rise to an inverse relationship between the rates of output growth and marginal income tax. By contrast, a positive indirect effect is present on the high-growth BGP. It turns out that a positive nexus between the rates of output growth and the marginal income tax is present under lower levels of the marginal income tax rate, because of a dominating positive indirect effect. When the marginal income tax is at higher values, the negative direct effect dominates, and hence output growth and the marginal income tax rate become inversely related.====In view that the adoption of a linearly progressive income tax schedule leads to the emergence of multiple BGPs with a low-growth trap, I propose policy instruments that help to eliminate the indeterminate low-growth BGP, thereby ensuring the existence of a unique determinate BGP that displays a high balanced growth rate. I show that this can be achieved by setting either the lump-sum taxes-to-capital ratio or the consumption tax rate above a critical level. I further find that, when the low-growth BGP is eliminated by either lump-sum taxes or a consumption tax and when the marginal income tax rate is set optimally, both the growth rate- and the welfare-maximizing marginal income tax rate are strictly lower than the output elasticity of public expenditures. Moreover, the higher the lump-sum taxes-to-output ratio and/or the consumption tax rate are, the lower the growth rate- and the welfare-maximizing marginal income tax rate will be. The welfare-maximizing marginal income tax rate is shown to be higher than the rate that attains the maximum rate of output growth.====I further demonstrate that, under a given/fixed level of the marginal income tax rate, a sufficiently high investment subsidy rate helps to eliminate belief-driven growth fluctuations and the low-growth trap. Nonetheless, when the marginal income tax rate is set to achieve the highest level of either output growth or social welfare, the model always displays two BGPs and equilibrium indeterminacy, and hence investment subsidies do not function as an automatic stabilizer. Finally, I find that government consumption is incapable of serving as an automatic stabilizer whether or not the income tax rate is optimally set. This runs in sharp contrast to Guo and Harrison’s (2004) finding that government consumption and lump-sum transfers/taxes display the same stabilizing effect that guarantees saddle-path stability of the model under constant income tax rates.====The remainder of this paper is organized as follows. Section 2 describes the model under the income tax formulation ==== Dromel and Pintus (2007) and analyzes the equilibrium conditions. Section 3 derives the economy’s balanced growth equilibria and examines the associated local stability properties and growth-rate effects. Section 4 explores fiscal policy instruments that potentially serve as automatic stabilizers. Section 5 quantitatively investigates the determination of the optimal marginal income tax rate and the effectiveness of fiscal policy instruments in regaining equilibrium uniqueness. Section 6 analyzes a combined linear and non-linear income tax scheme, and Section 5 concludes.",On economic growth and automatic stabilizers under linearly progressive income taxation,https://www.sciencedirect.com/science/article/pii/S0164070418304063,24 April 2019,2019,Research Article,254.0
"López-Villavicencio Antonia,Pourroy Marc","GATE-CNRS, University Lumière Lyon 2 93, chemin des Mouilles - B.P.167, ECULLY cedex 69131, France,CRIEF, University of Poitiers, France","Received 24 October 2018, Revised 20 March 2019, Accepted 16 April 2019, Available online 23 April 2019, Version of Record 28 April 2019.",https://doi.org/10.1016/j.jmacro.2019.04.004,Cited by (9),This paper estimates the effects of different forms of ,"It is well documented that exchange rate variations are less than completely associated with changes in prices in recent times. The most common interpretation for this finding is that improvements in monetary policy performance - reflected in stronger nominal anchors and low, stable inflation - result in an endogenous reduction in the exchange rate pass-through to consumer prices.==== Moreover, the adoption of inflation targeting (IT) is often associated with this stability====Indeed, it is argued that in the context of a stable and predictable monetary policy environment, nominal shocks –such as exchange rate shocks– play a vastly reduced role in driving fluctuations in prices (Taylor, 2000). Thus, improvements in monetary policy performance–reflected in stronger nominal anchors and low, stable inflation–result in an endogenous reduction in the exchange rate pass-through to consumer prices: when the inflation environment is more stable, firms resist passing exchange rate changes on to prices.==== Similar arguments are developed in Gagnon and Ihrig (2004), Bailliu and Fujii (2004), Devereux et al. (2004), Ihrig et al. (2006), Marazzi and Sheets (2007), Bouakez and Rebei (2008), Devereux and Yetman (2010) and Dong (2012) where the size of pass-through is a function of the stance of monetary policy.====Following this strand of the literature, many studies provide evidence that the adoption of an inflation targeting framework is associated with an improvement in overall economic performance (Bernanke and Mishkin (1997); Svensson (1997)). For instance, Mishkin and Schmidt-Hebbel (2007) suggest that exchange rate pass-through (ERPT) seems to be attenuated by the adoption of IT. The basic underlying idea is that adopting IT leads to credibility gains that are responsible for keeping low inflation expectations following an exchange rate appreciation. Consequently, opting for an inflation targeting framework is a means to reduce ERPT since under this regime, (i) inflation is expected to be diminished and stabilized, and (ii) central banks are expected to gain credibility as inflation-fighters. In addition, as shown by Reyes (2007), under inflation targeting regime, central banks respond to an exchange rate appreciation by increasing the interest rate to impede that exchange rate changes feed into inflation.====Most of the previous literature on ERPT and its link with inflation targeting, however, misses some key elements: self selection bias, endogeneity and heterogeneity of the inflation target regime. In the first case, selection bias occurs when IT is not randomly allocated across countries, but is instead correlated with other variables. A difference in ERPT between countries faced with IT (the so-called treated group) and the other countries (the so-called control group) could then be attributable to systematic differences in some variables between the treated and control groups rather than the effect of the treatment itself (IT adoption). In the second case, the adoption of inflation targeting is clearly an endogenous choice (see Mishkin and Schmidt-Hebbel (2001)). For instance, countries with histories of high inflation or expecting future high inflation are more likely to have felt compelled to adopt an inflation targeting framework. The finding that lower ERPT is associated with inflation targeting thus may not imply that inflation targeting causes ERPT. Finally, note that this literature provides no evidence as to which of the different forms and institutional arrangements of IT is more effective at reducing the ERPT.====The objective of this paper is to establish whether and how inflation targeting alter the way exchange rate changes impact prices. We contribute to the literature in different aspects. First, we use the Kalman filter to estimate the ERPT. By doing so, we allow this parameter to vary without imposing assumptions about whether or how it varies. Second, we pay special attention to self selection bias and endogeneity with regard to the monetary policy regime by relying on a methodology that allows us to determine whether a treatment leads to different outcomes than the absence of treatment. To this end, we match treated observations with control observations that share similar characteristics other than the presence of the treatment. That is, we construct a counterfactual for the treatment, based on a set of observable characteristics. This is particularly important since while a large part of the literature proposes that explicit IT regimes are generally associated with higher macroeconomic performance (Levin, Natalucci, Piger, 2004, Mishkin, Schmidt-Hebbel, 2007), other studies suggest that there is no evidence that performance is attributable to IT (see Ball and Sheridan (2003); Lin and Ye (2007) or Angeriz and Arestis (2008)).==== Third, as the benefits of explicitly adopting an IT regime are still an open debate in the literature, our main contribution is to analyze, in detail, the effectiveness of the IT regime under different circumstances. In particular, we alter our original sample by dropping one at a time from the whole sample IT countries that present different characteristics in terms of the monetary regime. Therefore we first distinguish countries regarding their initial level of inflation. Second, from the original sample, we distinguish observations according to the inflation targeted level. Third, from all IT and Non IT observations, we use different treatments regarding the deviations of actual inflation to the announced target. Fourth, we differentiate by the durability of the regime. Fifth, we formed the treatment with respect to the independence of the central bank. Sixth, observations were differentiated between point or band target. By performing this exercise, we try to shed light into the mechanisms through which IT lowers the ERPT.====Since the ERPT is not an observable variable, our empirical assessment relies on a two-stage procedure. In the first stage, we estimate time-varying coefficients of exchange rate pass through for each economy by means of state space models. In the second step, we explore whether these estimates are related to our proxies of monetary policy objective using a propensity score matching (PSM) methodology. We estimate different models and use several alternative definitions in order to ensure the robustness of our findings.====Our results can be summarized as follows. First, IT significantly reduces the ERPT. Second, this benefit is robust to different structural characteristics. Third, we reveal some important heterogeneities among IT countries. In particular, older regimes outperform newer regimes. Also, allowing for a band or range target is found to be more efficient than a single point targeting regime. Finally, keeping inflation relatively close to the objective, even if this objective is higher than 2 percent, makes a difference for achieving lower pass through.====The rest of the paper is organized as follows. Section 2 describes in detail our methodology. Section 3 presents the data. Section 4 displays our estimation results, and Section 5 concludes the paper.",Does inflation targeting always matter for the ERPT? A robust approach,https://www.sciencedirect.com/science/article/pii/S0164070418304397,23 April 2019,2019,Research Article,255.0
"Dufrénot Gilles,Paret Anne-Charlotte","Aix-Marseille University (Aix-Marseille School of Economics), CNRS & EHESS and CEPII, AMSE, 5-9 Boulevard Maurice Bourdet, CS 50498, 13205 Marseille Cedex 1, France,Aix-Marseille University (Aix-Marseille School of Economics), CNRS & EHESS and CEPII, France","Received 5 June 2018, Revised 1 April 2019, Accepted 1 April 2019, Available online 9 April 2019, Version of Record 15 April 2019.",https://doi.org/10.1016/j.jmacro.2019.04.002,Cited by (0),", we model the external debt-to-fiscal revenue ratio as a diffusion process for which the stochastic steady state distribution is derived using the properties of Itô diffusion processes.","External debt crises have been recurrent in emerging countries since the beginning 1980s. This happens when a country has difficulties servicing its debt, negotiates a debt rescheduling, or suspends debt repayments (default). Prevention of debt default has been a crucial debated issue in emerging countries. They have indeed witnessed several rounds of debt crises, during the eighties (in Latin America) and during the nineties (Asian and Russian crises). Over the last decade, external debt levels have been on the rise again.====So far, the centerpiece of preventive policies usually consists of quantifying the macroeconomic variables that drive the debt ratio to calculate the probability that it surpasses a certain threshold above which debt is considered to be unsustainable. This practice is known as a “signal extraction approach”. Debt crises are related to inflation, trade imbalances, fiscal and monetary policies, financial stress, capital flight, currency crises (see the seminal papers by Berg, Patillo, 1999, Herrera, Garcia, 1999, Kamin, Babson, 1999, Kaminsky, Reinhart, 1999, Ciarlone, Trebeschi, 2005 and for recent contributions by Arellano, 2008, Boonman, Jacobs, Kuper, 2014, Manasse, Roubini, 2009, Savona, Vezzoli, 2015). Debt sustainability analysis (DSA) has been developed to assess, prospectively, the necessary policy actions to undertake in order to prevent future occurrence of debt crises if adverse shocks were to happen. For recent contributions, we refer the reader to the papers by Adler and Sosa (2013), Aguiar and Amador (2014), Tanner and Samake (2008).====This paper adopts an alternative view. We consider debt crises “extreme events” and not as “anomalies”. Indeed, because they happen regularly, debt distress episodes cannot be considered “exceptional”. When we adopt the signal extraction and DSA approaches, one explanation of their recurrence may be the “this time is different” myopia as stated by Reinhart and Rogoff (mismanagement of macroeconomic policies). The “extreme value” approach focuses on the modelling of low risk events of a significant magnitude (high debt ratios in our case). In this framework, to predict the next probability of a crisis, what is needed is not the whole history of past debts, but only past episodes of high debt ratios. The adoption of this new framework changes the perspective of debt policies. First, after a debt crisis, it is impossible to say whether there exists a low or high probability of a future debt crisis, because the debt dynamics becomes unpredictable when governed by an extreme value process. Second, as a consequence, the concern of policymakers should be preventing the debt ratio from entering an unpredictable zone - which is different from the usual debt sustainability approach.====Though the methodology we employ is new, our paper belongs to a long tradition of models in which debt crises emerge from endogenous mechanisms in the financial markets (animal spirits, self-fulfilling prophecies, or the so-called Minsky moment). Even if they are “black swans” (extreme rare events), high debt levels are naturally occurring processes. The economic is considered to switch between low and high debt ratios that are both an integral part of the functioning of the markets.====The methodology we propose is based on a two-step approach. We first look at an empirical model that allows to fit the skewed thick tails observed in the debt ratio data. We show that a power-law distribution fits well the observed skewed tailed distribution in the external debt-to-fiscal ratio data. Then, we propose a theoretical model showing some mechanisms that can explain the underlying mechanisms generating the power-law distribution. More specifically, we develop a continuous time stochastic growth model where the dynamics of debt are related to tax evasion and corruption.====Our focus on tax evasion and corruption is motivated by two strands of literature. First, we formalize a recent intuition in the policy arena pointing to the weaknesses of the debt management capacity in some developing and emerging countries as a factor conducive to debt distress (this is an argument stressed by the International Monetary Fund and the World Bank). The idea is that debt carrying capacity is related to governance risks. Among the indicators accounting for the weaknesses of governance, tax evasion and corruption are two important factors. Tax evasion is multifaceted. It can be the result of a large shadow economy. It can reflect a deliberate avoidance of taxes in the labour markets (to keep at minimum wages), in the good and service markets (income concealed from the public administration to avoid the payment of taxes on income, indirect taxes, and taxes on capital). Corruption can be interpreted as a rent-seeking activity that obliges governments to borrow in order to finance public spending when tax revenues fall.====Secondly, we also consider the literature on the prevalence of tax evasion and corruption in developing and emerging economies. Higher levels of corruption are shown to be associated with a lower fiscal capacity, lower tax revenue, lower economic development, and lower global integration (see Besley, Persson, 2014, Shabbir, Anwar, 2007). Besides, Svensson (2005) also suggests that advanced economies present a lower overall level of corruption than emerging economies. Moreover, higher tax to GDP ratios are associated with higher GDP per capita. Kreiner et al. (2016) show that tax compliance and tax collection are lower in the developing and emerging economies, as opposed to advanced economies (see, also IMF, 2011). Brodzca (2013) relates this to a lack of capacity of tax administration authorities to operate and supervise the tax system. As an illustration, Figs. 1 and 2 show levels of tax evasion and corruption, distinguishing between advanced and emerging economies (based on the IMF classification). The difference between both groups is quite striking: the emerging and developing economies show significantly higher degrees of perceived corruption and tax evasion than the advanced economies.====Our theoretical findings and simulation exercises have several policy implications. First, we show that the intrinsic frequency of extreme events on external debt depends on countries’ wealth and its underlying volatility. Second, we show that such extreme events can happen when the agents take advantage of the weakness of administrative surveillance or an opaque fiscal environment, both of which make tax evasion easier. Indeed, in our model, tax evasion reduces the amount of resources available to finance productive spending which leads to an increase in external financing. However, we show that, above a certain critical threshold, a system that is too punitive may create incentives for tax evasion, making further punishment less effective. Third, we provide evidence that a higher degree of corruption also leads to more frequent extreme events on external debt, because more public resources are extorted. Here again, our results underline a nonlinear effect, with too much corruption actually leading to an exhaustion of corruption opportunities, which stabilizes the external debt ratio.====There is a flourishing literature about the act that some economists that some economic variables (both real and financial) seem to be governed by power laws. This discovery is of the same importance as the one made during the 1990s on the persistent dynamics of macroeconomic and financial times series, which has motivated a subsequent abundant literature on unit root and cointegration analyses. This finding forces us to rethink the functioning of markets and to explain the regularities produced by power laws, e.g. extreme events that are not caused by big shocks or external factors (which is the common assumption made by economists). What is new is that some economic mechanisms can give rise to stable dynamics and unstable dynamics with disruptions (at the same time) in an endogenous manner. Until now, the explanation that has been provided in the literature is that this happens because economic systems are nonlinear and give rise to complex dynamics (there is a vast literature on complex dynamics and catastrophe theory applied in the macroeconomic and finance field). The framework of this literature is very often deterministic. That i why a recent new strand of the literature is also trying to understand how disruptions and extreme events can happen in a stochastic environment, even without any big shock. The literature on power-laws belongs to this strand. So far, the fields of applications have been income and wealth or finance. To the best of our knowledge, evidence of such dynamics in international economics remains unexplored, despite the occurrence of several extreme events including external debt, financial and currency crises. Our paper is the first to show that power-laws can also explain the occurrence of external debt crises in emerging countries, a hotly debated topics in the policy arena.====After some empirical evidence, we propose a theoretical model which matches the presence of a power-law in the external debt ratio and the weaknesses of governance in these countries captured by corruption and tax evasion. We are not claiming that these are the sole factors causing debt crises. Rather our paper provides a simple illustration of a tractable micro-founded growth model that generates power-laws of external debt ratios. In this framework our continuous time stochastic growth model has several differences compared to the usual models of optimal growth and debt used in the literature. First, the borrowing constraints are internalized by the domestic agents. Consumers bear the cost of borrowing and the latter is taken into account in their optimization program when they decide how much income to hide from tax administration. Therefore, the upper bound on government fiscal capacity is endogenous and changes over time. Second, the borrowing constraint is also endogenous and depends upon the risk-adjusted return of tax evasion and corruption. These characteristics have the following implications in our model: when fighting against corruption and tax evasion imply some Laffer nonlinear effects on income tax returns (a high punishment increases the incentive to evade or being corrupted and therefore makes tax collection less effective), a government may have an incentive to finance public expenditure by debt, because this relaxes the high tax burden on taxpayers. We also add several technical innovations. First, the steady-state is not a value but a whole distribution (an invariant distribution). This contrasts with many existing stochastic growth models in the literature, because the authors usually assume that the underlying distributions of the shocks are iid (independently and identically distributed) and focus on their first two moments (mean and variance). Secondly, we have two notions of uncertainty here. The first is usual and refers to some economic shocks on inflation, terms of trade, foreign and domestic interest rates. The second is new and comes from the existence of lotteries related to tax evasion and corruption activities.====The remainder of the paper is organized as follows. Section 2 establishes empirical evidence that the external debt-to-fiscal revenue ratio has a power-law distribution in the emerging countries. We go beyond the visual inspection of a log-log graph of the distribution of the series, by estimating the scale coefficient and by testing formerly the hypothesis of a power-law distribution. Section 3 contains our theoretical model. Section 4 derives the steady state power-law distribution for the external debt ratio and presents simulations of the model aiming to examine the impact of corruption and tax evasion on the shape of this power-law distribution. Finally, Section 5 concludes. The reader will find the Appendix to this paper at ====.",Power-law distribution in the external debt-to-fiscal revenue ratios: Empirical evidence and a theoretical model,https://www.sciencedirect.com/science/article/pii/S016407041830257X,9 April 2019,2019,Research Article,256.0
"Bagliano Fabio C.,Fugazza Carolina,Nicodano Giovanna","Università di Torino and CeRP (Collegio Carlo Alberto), Italy,Università di Torino and CeRP (Collegio Carlo Alberto), Italy,Università di Torino, CeRP (Collegio Carlo Alberto) and Netspar, Italy","Received 26 July 2018, Revised 19 March 2019, Accepted 26 March 2019, Available online 4 April 2019, Version of Record 12 April 2019.",https://doi.org/10.1016/j.jmacro.2019.03.006,Cited by (11),"The recent ==== highlighted that long-term unemployment spells may entail persistent losses in workers’ human capital. This paper extends the life-cycle model of savings and portfolio choice with unemployment risk, by allowing the possibility of permanent reductions in expected earnings following long-term unemployment. The optimal risky portfolio share becomes flat in age due to the resolution of uncertainty about future returns to human capital that occurs as the worker ages. This may help explaining the observed relatively flat, or only moderately increasing, risky share of investors during working life, and have important consequences for the design of optimal life-cycle portfolios by investment funds.","Several findings in the macro-labor literature indicate that long-term unemployment may lead to a loss of human capital. In this paper, we embed the possibility of entering long-term unemployment with permanent consequences on human capital in a life-cycle model of consumption and portfolio choice. We model working life careers as a three-state Markov chain driving the transitions between employment, short-term and long-term unemployment states, as in Bremus and Kuzin (2014), calibrated to broadly match recently observed U.S. labor market features. Importantly, we allow for human capital loss during unemployment. When unemployed, individuals receive benefits but simultaneously experience a cut in the permanent component of labor income which captures diminished future income prospects. This represents the observed permanent earning losses (Arulampalam, Booth, Taylor, 2000, Arulampalam, 2001, Schmieder, von Wachter, Bender, 2016) due to skill loss during long-term unemployment (Neal, 1995, Edin, Gustavsson, 2008).====Potential losses of human capital considerably lower the optimal portfolio share invested in stocks with respect to the case of no unemployment risk. Importantly, optimal stock investment is no longer decreasing with age but remains remarkably flat over the whole working life, in line with the evidence on U.S. portfolios (Ameriks and Zeldes, 2004). On the contrary, traditional life-cycle models imply that households should reduce exposure to risky stocks as they approach retirement (Bodie, Merton, Samuelson, 1992, Viceira, 2001, Cocco, Gomes, Maenhout, 2005). The reason is that human capital provides a hedge against shocks to stock returns, making financial risk bearing more attractive. Investment in stocks should therefore be relatively high at the beginning of working careers, when human capital is large relative to accumulated financial wealth, and then gradually falling until retirement as human capital decreases relative to financial wealth. This model implication is embodied in the popular financial advice of a stock exposure steadily decreasing with age, the so-called “age rule”. In our model with human capital loss, such effect is instead moderated by the resolution of uncertainty concerning labor and pension income, as the worker safely comes close to retirement age. Since the risk of long term unemployment falls together with human capital as retirement approaches, the resolution of uncertainty compensates the hedge effect and the optimal investment in stocks is relatively flat over the life-cycle.====Optimal risky portfolios are highly heterogeneous in models without human capital loss. On the contrary, the permanent consequences of long-term unemployment shrink the heterogeneity of optimal portfolio choices across agents characterized by different employment histories. In the face of possible human capital depreciation, individuals accumulate substantially more financial wealth during working life to buffer possible adverse labor market outcomes. Optimal early consumption consequently falls, becoming higher during both late working life and retirement years. The working-year responses to unemployment risk, including the flat age profile in stock investment, are remarkably robust to changes in preferences on the intertemporal correlation of shocks. In fact, allowing for Epstein-Zin preferences only causes slower wealth decumulation and less risk taking during retirement years. Similarly, an increase in the correlation between stock returns and labor income shocks leaves the flat shape of optimal equity investment during working age unaltered, only increasing the portfolio share allocated to the riskfree asset. Thus, it is the human capital loss the first order determinant of the optimal financial risk-taking at different ages.====The above results obtain in calibrations to U.S. data: in particular, the implied unconditional probabilities of being short-run unemployed (3.78%) and long-run unemployed (1.72%) are set at the levels observed in the U.S. after the Great Recession. The human capital loss amounts to some 25% of all future expected earnings only in the occurrence of a long-term unemployment spell, in a calibration that captures the relatively slow re-employment process experienced by U.S. workers. We select the magnitude of the human capital loss during long-term unemployment considering both the total loss of human capital for the fraction of workers abandoning the labor force, and the partial loss for those who are able to find a job. Our results go through even when the human capital loss parameter is reduced as far as 15% of future expected earnings, and when the probability of moving into long-term unemployment from an initial unemployment state is reduced by half (from 0.15 to 0.075).====Previous life-cycle models with unemployment and self-insurance leave the observed age pattern of stock holding during working life largely unexplained. Some versions of the life-cycle model account for the risk of being unemployed by introducing a (small) positive probability of zero labor income: in these models unemployment risk affects income only during the unemployment spell with no consequences on subsequent earnings ability (Cocco et al., 2005) even when unemployment is persistent (Bremus and Kuzin, 2014). With no permanent consequence on subsequent earnings ability, the stock holding is still counterfactually decreasing in age till retirement although, on average, lower than what obtained without unemployment risk. Thus, it is the possibility of human capital loss entailed by long–term unemployment -rather than unemployment per se - that restrains risk-taking by the young and middle-aged workers.====Several papers already investigate alternative hypotheses that may deliver the relatively flat stock profile observed in the data, departing from the pattern implied by traditional life-cycle models. Some of this prior research already relate the flattening of the age profile of stock investment to the resolution of uncertainty over working life. Hubener et al. (2016) point to the time-varying risk of changing family status during working age due to marriage, fertility and divorce, which affects consumption both directly and through labor supply. In Bagliano et al. (2014), such flattening crucially depends on the presence of both another risky asset, besides equities, and a positive correlation between stock returns and permanent labor income shocks. Moreover, it only appears when risk aversion or the variance of labor income shocks are higher than in the baseline calibration of Cocco et al. (2005). Most importantly, Chang et al. (2018) introduce labor market uncertainty into an otherwise standard life-cycle model. They show that the interaction between unemployment risk, occupational uncertainty and gradual learning about earnings ability generates a moderately increasing age profile of stock investment, with an average portfolio risky share (conditional on participation) substantially lower than in a typical life-cycle setting. Our model complements and strengthens their main conclusions by exploring the effects of an additional dimension of age-dependent labor market uncertainty, namely the risk of permanent human capital losses due to long-term unemployment, yielding an average optimal stock share below 60% and remarkably flat during working life. Notably, as in Chang et al. (2018), our results are achieved under the assumptions of a moderate degree of risk aversion and the absence of positive correlation between labor income and stock market returns.====The rest of the paper is organized as follows. Section 2 presents the benchmark life-cycle model and briefly outlines the numerical solution procedure adopted. We detail the model calibration in Section 3 and discuss our main results in Section 4. Various robustness checks are presented in Section 5. Section 6 concludes the paper.","Life-cycle portfolios, unemployment and human capital loss",https://www.sciencedirect.com/science/article/pii/S0164070418303239,4 April 2019,2019,Research Article,257.0
"Ketteni Elena,Kottaridi Constantina","Department of Business Administration, Frederick University, 7, Y. Frederickou Str., Nicosia 1036, Cyprus,Department of Economics, University of Piraeus, Greece","Received 22 March 2018, Revised 28 March 2019, Accepted 1 April 2019, Available online 2 April 2019, Version of Record 27 November 2019.",https://doi.org/10.1016/j.jmacro.2019.04.001,Cited by (5),"This study provides further insights to the debate of more or less financial regulation, especially after the financial crisis of 2007–08 and skepticism that has been raised. It uses an extensive dataset covering 66 countries for the period 2000–13 to test for the isolated effects of credit market liberalization on economic growth. During that period, most of the developed countries had already reached a high degree of credit liberalization. We apply marginal integration to a Partially Additive Linear Model which allows for a separate treatment of individual components. Consequently, we can uncover the individual shapes of the relationship pointing to linear or nonlinear links between regulation and GDP growth. We establish that credit deregulation exhibits threshold effects as well as heterogeneous effects among groups of countries. Initial credit reforms have a positive impact on growth. Developed countries reach a threshold value, which they manage to overcome and move again towards positive growth effects. In developing countries, growth is negatively affected after a threshold, potentially triggered by institutional voids and weaknesses. Even more pronounced is the negative effect in emerging economies; obviously due to particular ==== which differentiates them from the rest of the countries. As such, credit market deregulation is not the way to go to stimulate growth in this region under the prevailing conditions. Therefore, credit market deregulation is not a panacea: countries should liberalize their financial markets at the same time they proceed to other reforms and strengthen their institutions to reap beneficial growth effects.","Addressing the issue why some countries grow faster than others and what policies would best accommodate faster growth is an ongoing debate. Hall and Jones (1999) and Acemoglu et al. (2001), demonstrate that institutions are a major determinant of wealth and long-term growth. Countries with better political and economic institutions and better regulations are richer today (Djankov et al., 2006). Yet the results from recent literature with respect to whether financial liberalization leads to economic growth appear conflicting, inconclusive, heterogeneous and conditional on country characteristics (Bumann et al., 2013).====We provide further insights in this discussion by examining a specific aspect of financial liberalization that impacts on growth, that of credit regulation. We study how relaxing credit market constraints affects country economic growth using an extensive dataset, in a period after 2001 by applying a newly advanced semiparametric estimation technique. We specifically deal with credit market regulations, while most of the literature deals with other financial liberalization schemes.====Regulations were initially introduced as a policy measure to promote economic and social welfare. They were considered the most persistent form of government intervention in economic activity and as being essential for the proper functioning of markets, to correct market failures or to promote considerations of equity. This is one part of the story. Regulations are often excessive and/or of poor quality and are thus imposing unnecessary burdens on business and, overall, on the economy. How and to what extent to regulate has become a top priority for governments around the world and the challenge is not only to maintain the benefits that regulations create but also reduce the negative effects of these measures. Thus, governments in several countries set regulatory interventions at the heart of reforms.====The effect of regulations is more confusing when it comes to developing or less developed countries. In developing countries, the objectives of regulation are likely to be concerned with wider goals towards promoting sustainable development and reducing poverty, not just achieve economic efficiency. Also, usually, they are subject to political capture; that is regulatory goals are distorted to pursue political ends (Acemoglu et al., 2008). The capacity of the state to provide a sound regulatory framework is an important determinant of how well markets perform. The evidence suggests that in developing countries, regulations have been disappointing and need improvement.====The relationship between financial liberalization and economic growth holds a prominent part in related literature, pointing to contradictory views. Also, the financial failure that resulted in the outburst of the severe financial crisis in 2007–08 brought into light some scepticism on the extent of financial deregulation. Theoretically, financial deregulation affects long-term economic growth through alternative channels that lead to more efficient resource allocation. However, it is recently argued that beyond a certain threshold, most likely to be reached in most developed countries, the size of a domestic financial system may not be an adequate indicator of efficiency in terms of accessibility to credit and financial services, intermediation costs, or productivity of capital employed (Guisoet al., 2004). Simultaneously, we find voices pointing to dangers associated with complexity and financial fragility due to the high degree of financial innovation resulting from financial liberalisation (Gennaioli et al., 2012, Rajan, 2005. And the above results are likely to be different across different regions, sectors and income levels (Barajas et al., 2013, Nili and Rastad, 2007, Khan et al., 2001).====This work aspires to extend current research in several ways. Firstly, we apply semiparametric techniques that can capture nonlinear effects along the spectrum of financial deregulation within countries. Recent papers (Henderson et al., 2011, Henderson et al., 2013; Maasoumi et al., 2007; Panizza, 2017) show that inclusion of nonlinearities is necessary for determining the empirical relevant variables and uncovering key mechanisms of the growth process. Specifically, Henderson et al. (2011) argue that non-parametric model selection procedures are invaluable as a tool for uncovering the salient features of growth processes. Also, it has been recognized that misspecification of functional form can have both a detrimental impact on policy prescriptions and the general understanding of the underlying economic structure (Henderson et al., 2013).====Secondly, while most of the empirical studies focus on foreign banks entry (Claessens et al., 2001; Hermes and Lensink, 2003), equity market liberalizations (Bekaert et al., 2005, Bekaert et al., 2005, stock market liberalization (Henry, 2000, Kose et al. 2006), capital account liberalization (Stiglitz, 2000; Eichengreen and Leblang, 2003),==== credit provided to the private sector (Beck et al., 2007), or a multidimensional aspect of financial liberalization (Abiad et al., 2010), we concentrate on an under investigated measure of financial regulation, i.e. credit market regulation which measures three important aspects of credit markets. Specifically, it accounts for the extent to which the banking industry is privately owned, the extent to which credit is supplied to the private sector and whether controls on interest rates interfere with the market in credit. This the first time, to the best of the authors’ knowledge that this regulation measure is examined in related literature. This is important because it allows to isolate the effect of bank competition and credit provide to the private sector in the groups of countries under investigation and induce important policy implications for the future. Moreover, most studies dealing with financial liberalization do not systematically investigate the financial liberalization-growth nexus (Bumann et al., 2013). We instead focus on this relationship in an attempt to uncover the entire spectrum of credit deregulation effects on economic growth intra and intertemporally within countries.====Thirdly, we carry out our analysis from 2001 onwards during which the financial crisis of 2007–08 occurred, which will provide insights on these effects in a current and problematic period. Additionally, given current research stressing threshold effects in higher levels of financial development and depth, this study extends the analysis to a higher credit market freedom by examining a more recent period when most of the economies, especially the most advanced ones, have reached extensive credit market liberalization. No study to date, according to the authors’ knowledge, have examined the relevant period.====We should stress at this point that our objective is not to develop varying models of economic growth; our aim is to test for the effect of credit market regulation both intra and inter temporally within a general econometric framework, allowing the effect of credit market regulation to differ both across each country and within the time period of investigation.====The rest of the paper is organized as followed. Section 2 presents a literature review on the topics of credit regulations. Section 3 presents the methodology and the data sources and explanations and Section 4 presents the results of both parametric and non-parametric methods. Finally, the last section concludes and draws some policy implications.",Credit market deregulation and economic growth: Further insights using a marginal integration approach,https://www.sciencedirect.com/science/article/pii/S0164070418301216,2 April 2019,2019,Research Article,258.0
"Hecq Alain,Jacobs Jan P.A.M.,Stamatogiannis Michalis P.","Department of Quantitative Economics, School of Business and Economics, Maastricht University, Maastricht, The Netherlands,Faculty of Economics and Business, University of Groningen, PO Box 800, 9700 AV, Groningen, The Netherlands,University of Tasmania, Australia,CAMA, Australia,CIRANO, Canada,Management School, University of Liverpool, Liverpool, UK","Received 28 June 2018, Revised 7 February 2019, Accepted 17 March 2019, Available online 25 March 2019, Version of Record 7 May 2019.",https://doi.org/10.1016/j.jmacro.2019.03.003,Cited by (1),"This paper focuses on testing non-stationary real-time data for forecastability, i.e., whether data revisions reduce ==== or are ====, by putting data releases in vector-error correction forms. To deal with historical revisions which affect the whole vintage of time series due to redefinitions, methodological innovations etc., we employ the recently developed ==== approach, which involves potentially adding an indicator dummy for each observation to the model. We illustrate our procedures with the U.S. real GNP/GDP series of the Federal Reserve Bank of Philadelphia and find that revisions to this series neither reduce noise nor can be considered as news.","Before being considered definitive, data currently produced by statistical agencies typically undergo a recurrent revision process resulting in different releases of the same phenomenon. The collection of all these vintages is referred to as a ====. In the recent past, economists and econometricians have come to realize the importance of this type of information for economic modeling, forecasting and policy formulation. Consequently there exists a growing interest for investigating this type of data (see ==== Croushore and Stark (2001), Orphanides and van Norden (2002), and Croushore, 2011a, Croushore, 2011b).====Several aspects of real-time data can be investigated: (i) structural or trend breaks (see Jacobs and van Norden (2016)) for a summary of the reliability of productivity growth rate trends); (ii) forecastability, i.e., whether revisions reduce ==== or are ==== (the literature is briefly reviewed in Section 3.1); (iii) ====, which affect the whole vintage of time series due to redefinitions, methodological innovations, etc., make testing difficult. The standard approach to dealing with historical revisions is either to employ growth rates to mitigate the effects of historical revisions, or to ‘clean’ the series in an attempt to get rid of the effects of historical revisions. The former approach has been criticized by Knetsch and Reimers (2009). Götz et al. (2016) illustrate that growth rates can also be affected by large revisions.====Whereas the tests and the procedures to deal with historical revisions are well-documented for stationary time series (e.g., using Mincer-Zarnowitz type tests), the situation is less clear for non-stationary time series. The paper aims at filling this void, building upon Hecq and Jacobs (2009). We focus on testing forecastability for non-stationary real-time data, putting data releases in vector-error correcting forms (VECMs hereafter). To deal with forecastability under historical revisions at unknown dates, we estimate VECMs using an automatic modelling method for selecting conditional mean parameters (the Autometrics algorithm, see Hendry and Doornik (2014)) together with the Impulse Indicator Saturation approach (IIS hereafter, see e.g., Hendry and Santos (2005)). Briefly, IIS involves adding an indicator dummy==== for potentially each observation to the model and hence is able to determine a parsimonious model that fits model requirements in terms of misspecification. We illustrate our procedures with the U.S. real GNP/GDP series of the Federal Reserve Bank of Philadelphia and find that, in general, revisions neither reduce noise nor can be considered as news. Conclusions would have been different without the IIS approach.====An alternative strategy to the IIS algorithm consists of introducing dummy variables for each historical revision. This operation is less obvious than one might think at first glance and can be very tedious and time consuming for an external researcher who does not have complete information on thousands of economic variables for different countries. While one can easily find the description of the modifications for the main aggregates for the U.S. or the European Union for instance, this task is much more demanding when the information about data revisions is for instance not in English or not available online on national statistical agencies websites. Using IIS helps in investigating those time series within a few seconds. Secondly, one can also notice that the date at which vintages are released might differ from the date at which the series has been theoretically modified. As an example, books may describe that there is a new definition of an economic indicator in January but the series published on, say the 10th of January, still applies the old definition. It might be for this latter example that a second vintage is available at the end of the month such that we observe multiple vintages for one particular month, a situation that adds difficulties for the researcher. Third, IIS can also capture smaller revisions (e.g., annual or seasonal revisions due to e.g., the change of seasonal factors) that would have been ignored based on historical revisions only. Finally, many real-time databases have been build manually, either by merging files or using manpower for scanning or copying figures from statistical reports. Those operations can also introduce errors. So identifying historical revisions is not always straightforward and as a result “dummying-out” specific dates without using a statistical method can be considered as a subjective practice.====The remainder of the paper is structured as follows. After a brief introduction of data revisions and notations in 2 Data revisions and notation, 3 Method describes news-noise tests for stationary and non-stationary real-time data as well as the intuition underlying the IIS approach. Section 4 illustrates our procedure with the U.S. real GNP/GDP series. Section 5 concludes.",Testing for news and noise in non-stationary time series subject to multiple historical revisions,https://www.sciencedirect.com/science/article/pii/S016407041830291X,25 March 2019,2019,Research Article,259.0
"Crespo Cuaresma Jesus,von Schweinitz Gregor,Wendt Katharina","Vienna University of Economics and Business, Austria,Wittgenstein Center for Demography and Global Human Capital (IIASA,VID/OEAW,WU), Austria,International Institute of Applied Systems Analysis (IIASA), Austria,Austrian Institute of Economic Research (WIFO), Austria,Halle Institute for Economic Research (IWH), Germany,University of Leipzig, Germany","Received 27 April 2018, Revised 18 March 2019, Accepted 19 March 2019, Available online 20 March 2019, Version of Record 26 March 2019.",https://doi.org/10.1016/j.jmacro.2019.03.004,Cited by (2)," model averaging methods. Regulation has on average a negative effect on GDP in tranquil times, which is only partly offset by a positive (but not robust effect) in crisis times. Credit over GDP is positively affected by higher requirements in the longer run.","Since the outbreak of the global financial crisis, central banks have employed a wide variety of macroprudential instruments to strongly differing degrees. These instruments were often introduced without much prior empirical evidence on their potential effects. The main aim of this study is to provide robust inference on the medium to long-run effects of reserve requirements, one of the most widespread macroprudential measures, on credit growth and economic growth. To ensure robustness to model specification, we use Bayesian model averaging techniques to quantify the role played by reserve requirements. Our main interest lies in quantifying the real growth effects of reserve requirements as a regulatory (macroprudential policy) tool during normal and stress times. Reserve requirements are not the only existing macroprudential policy instrument==== Our paper contributes to a growing empirical literature that aims to identify the effects of these instruments (see Aizenman, Chinn, Ito, 2017, Richter, Schularick, Shim, 2018, for two very recent examples). Although the interest of researchers in assessing the macroeconomic effects of reserve requirements has increased enormously after the crisis, their use has a long history in economic policy, having been first established in the US at the national level in 1863 (Feinman, 1993) and used extensively in emerging markets and developing economies over the last half a century (Federico et al., 2014).====The main objective of macroprudential policy is to promote the resilience of the financial system (Schoenmaker, 2014). This is important because financial risk taking may not always be beneficial for economic growth. On the one hand, financial risk taking has been found to have – on average – a positive effect on growth by mitigating financial bottlenecks (Rancière et al., 2008). On the other hand, this may not be true to the same extent for countries with strong and sound institutions or for extremely high levels of risk. Financial crises as an undesired outcome of systemic risk taking can have severe adverse consequences for the financial system and the real economy (Laeven and Valencia, 2013). To maximize welfare, a balance therefore needs to be struck between financial stability and some (systemic) risk taking (Borio and Shim, 2007).====A variety of theoretical papers have evaluated costs and benefits of regulation. Statements on the most efficient approach usually depend on the setup of the economy and modeled frictions (Jeanne, Korinek, 2013, Benigno, Chen, Otrok, Rebucci, Young, 2013). The assessment of the effects of macroprudential instruments in empirical studies has – in comparison – not kept pace. The first generation of empirical studies on macroprudential instruments and their effects uses aggregate measures in international panel studies, for example by creating simple indices identifying the legal existence of these instruments (Borio, Shim, 2007, Cerutti, Claessens, Laeven, 2017). The focus of these studies – influenced by the recent financial crisis – has mostly been on measuring the effect on credit growth and house prices. Due to the use of variables based on legal existence, the intensity of an instrument is never a factor in these studies. This problem is addressed in a second group of studies, which employ either microeconomic or time-series analysis to identify the effects of the strengthening of one instrument in a single country (Tovar, Garcia-Escribano, Martin, 2012, Arregui, Beneš, Krznar, Mitra, 2013, Camors, Peydro, 2014). In general, both types of studies agree that macroprudential instruments have countercyclical effects on credit and house price growth found in the first group of studies mentioned above.====The theoretical literature on the link between reserve requirements and economic growth highlights several mechanisms that imply beneficial effects in terms of smoothing cyclical volatility in the short and medium term. In the short term, the reaction of real activity to changes in reserve requirements has been highlighted in different studies both theoretically and empirically. The theoretical framework in Bernanke and Blinder (1988), for instance, expands the standard IS-LM model to include bank reserves as a policy instrument. The analysis shows that reserve requirements can indeed act as a stabilizing device for short-term fluctuations, a result that is confirmed in the empirical results of Loungani and Rush (1995).====Theoretical and empirical results on the long-run effects of reserve requirements are rare in the literature, and our contribution aims precisely to provide robust empirical results on the link between reserve requirements, credit and real activity beyond the short-run horizons usually employed in the existing econometric studies. Exceptions to the lack of theoretical modeling exercises based on longer-run macroeconomic effects of reserve requirements on real activity are the works of Gomis-Porqueras (2002) or Ma (2018). Gomis-Porqueras (2002) shows that imposing reserve requirements may increase welfare in equilibrium under certain parametric assumptions due to changes in bank asset allocations, which affect capital accumulation and therefore long-run real output. Ma (2018) presents an endogenous growth model where the productivity process is affected by potentially binding collateral constraints and where the social planner can choose an optimal macropolicy path based on setting a tax on capital flows. It is precisely the interpretation of reserve requirements as a tax on capital flows (see Reinhart, Reinhart, 1999, Magud, Reinhart, Rogoff, 2018) that allows the interpretation of the predictions of the theoretical model in Ma (2018) as providing testable hypotheses on the effects of reserve requirements on income growth. The results of the model predict an ambiguous impact of macroprudential policy on economic growth, with positive effects during crises through its effect on financial vulnerabilities and negative effects during tranquil periods. We are, to the best of our knowledge, the first to test if these theoretical predictions hold empirically.====In this contribution, we aim to address empirically the role played by one particular macroprudential instrument, reserve requirements, as a determinant of credit and economic growth beyond the usual business cycle frequencies used in previous studies. We aim to identify the medium to long-run effect of reserve requirements on credit and GDP per capita growth in the context of economic growth regressions under the presence of model uncertainty, using Bayesian methods. Both the focus on a longer horizon and an extension to GDP growth are aimed at bringing our study more in-line with the welfare-maximizing objective of forward-looking policymakers. The Bayesian analysis has the additional advantage in that it does not presuppose a particular econometric specification to address the effect of reserve requirements on income growth for the sample. In particular, the method creates weighted averages of the effects found across different model specifications. Using posterior model probabilities as weights, it integrates away the uncertainty embodied in the choice of a particular model. Thus it provides inference that accounts for the fact that the true specification is unknown to the researcher. The advantages of such a method depend on the set of models entertained, so we place an emphasis on collecting a dataset that includes the most relevant variables employed in the literature to assess differences in long-run economic growth. Moreover, we address potential nonlinearities by allowing for quadratic and interaction effects in the models used.====Our estimations show that reserve requirements affect medium- to long-run GDP growth negatively at all times, but that higher requirements during crises improve growth prospectives after the crisis beyond the usual catching up. These results are robust across different horizons. We show that both the negative single effect, and the positive interaction effects of requirements and crises are stronger if requirements are either differentiated, or take on extremely high or low values.====The remainder of this paper is organized as follows. Section 2 provides a literature review of the relation of crises, regulation and growth. Section 3 and 4 describe the data and Bayesian model averaging techniques employed in the empirical exercise. Section 5 presents the results of the analysis, and Section 6 concludes.",On the empirics of reserve requirements and economic growth,https://www.sciencedirect.com/science/article/pii/S0164070418301782,20 March 2019,2019,Research Article,260.0
"Aloui Rym,Eyquem Aurélien","Univ Lyon, Université Lumière Lyon 2, GATE UMR 5824, F-69130 Ecully, France,Institut Universitaire de France, Université Lumière Lyon 2, GATE UMR 5824, F-69130 Ecully, France","Received 8 November 2018, Revised 7 March 2019, Accepted 15 March 2019, Available online 19 March 2019, Version of Record 27 March 2019.",https://doi.org/10.1016/j.jmacro.2019.03.002,Cited by (5),"We investigate the link between the size of government indebtedness and the effectiveness of government spending shocks in normal times and at the ==== (ZLB). We develop a ==== with capital, distortionary ","The massive rise in government debt levels and sovereign spreads that followed the 2008 Great Recession and the 2011 recession in countries of peripheral Europe raises the question of whether the level of public debt affects the effectiveness of government spending shocks. In other words, do high levels of sovereign debt undermine the ability of governments to make use of government spending to stabilize the economy? Conventional wisdom suggests that countries with high levels of public debt have less room for fiscal stimulation than countries with low levels of public debt in the event of an economic crisis, and would therefore advocate for low debt levels on average.====We investigate this question in a standard New-Keynesian model with capital accumulation where fiscal solvency is achieved through distortionary taxes on either labor or capital income. In the data, countries with higher levels of debt also feature higher levels of tax rates (see Appendix A). Because debt is high, they potentially face larger costs of debt rollover, and because taxes are high, they face larger efficiency costs of distortionary taxation. In this paper, we show that the initial level of debt lowers public spending multipliers in normal times but raises public spending multipliers at the Zero Lower Bound (ZLB). In normal times, a spending shock financed by a combination of public debt and a distortionary tax rule leads to a larger increase in public debt and in future tax rates. This imposes more distortions on production factors and reduces the positive effects of a spending shock on output. At the ZLB, in line with Erceg and Lindé (2014), we find that a spending shock at the ZLB is self-financing. This leads public debt and future tax rates to ====, lowers the amount of distortions imposed on production factors, and further raises output. These movements are larger for economies featuring a high initial debt-to-GDP ratio due to their higher tax levels, and because the costs of taxation are convex. The shock leading the ZLB to be binding is here a negative capital quality shock, as in Gertler and Karadi (2011), but our results are robust to considering a discount factor shock. One important difference between capital quality shocks and discount factor shocks is that the latter produce a ==== in private investment while the former produce a ==== in private investment. We thus consider the former to be more consistent with the Great Recession narrative, and with the factors that most likely led nominal interest rates to hit the ZLB.====Our results are qualitatively robust to keeping the steady-state level of tax rates constant and using a steady-state lump-sum tax to finance the changing steady-state level of debt. They are only slightly attenuated quantitatively speaking. Indeed, what crucially matters for our results to hold is the size of the ==== in tax rates that is needed to ensure fiscal solvency after spending shocks. The latter is larger when the steady-state level of debt is higher. The change in tax rates is positive in the case of a spending shock hitting the economy around the steady state because debt rises, which in turn reduces the size of the public spending multiplier, and negative in the case of a spending shock hitting at the ZLB because public debt falls, which in turn raises the value of the spending multiplier. We also show that our results hold and are quantitatively magnified when introducing hand-to-mouth households.====Quantitatively speaking, our model produces public spending multipliers that line-up quite well with the literature (see Ramey (2019) for a recent overview). During normal times, short-run multipliers (at a 2 years horizon) roughly range from 0.2 to 0.3 for an empirically plausible calibration of the model. When the spending shock hits conditional on a negative capital quality shock that pushes the economy at the ZLB for a few quarters, short-run multipliers range from 1.4 to 1.45 for the same calibration. In any case, spending multipliers at the ZLB are larger than spending multipliers at the steady-state, as in Christiano et al. (2011) and the subsequent literature. The impact of the initial level of debt is relatively small for short-run multipliers at the steady state, but can be very large for short-run multipliers at the ZLB. These results are quantitatively but not qualitatively sensitive to a small subset of parameters such as the Frisch elasticity on labor supply, the degree of complementarity between public and private goods in the utility function of households, or the responsiveness of the tax rules. Finally, we show that the initial level of debt crucially affects the optimized response of government spending to a large crisis that pushes the economy to the ZLB. The size and persistence of the rise in public expenditure varies depending on the initial level of debt, which is only a consequence of the fact that the effects on output of changes in spending are stronger when the initial level of debt is higher.====Our paper relates to the literature on spending multipliers that questions how the economic environment may affect the latter. Empirically, one of the first papers to raise the question was Perotti (1999). More recently, the subject has been revived by Auerbach and Gorodnichenko (2012), investigating whether the business cycle position matters for the value of multipliers. Two papers, respectively by Corsetti et al. (2012) and by Ilzetzki et al. (2013) question more precisely the impact of debt or fiscal stress on spending output multipliers. Their results converge and conclude that fiscally stressed or highly indebted economies tend to be characterized by lower spending multipliers. Our results about the impact of the initial level of debt on the size of fiscal multipliers are in accordance with those results, as we find that a high level of debt lowers the spending multiplier. Further, according to Corsetti et al. (2012), when an economy experiences a financial crisis, spending multipliers are much larger than in normal times. If one admits that financial crises are more likely to lead to ZLB episodes, this result is also consistent with more theoretical contributions like Christiano et al. (2011) and the subsequent literature. Our results are also consistent with this empirical result, as we find that spending multipliers are larger during a financial crisis, triggered by a negative capital quality shock that pushes the economy at the ZLB. A recent paper by Boitani and Perdichizzi (2018) tests the joint conditional impact of recessions and the level of debt on the size of spending multipliers. Boitani and Perdichizzi (2018) find evidence of self-financing public spending shocks during recessions in Euro Area countries, and find that multipliers are larger in high deficit countries than in low deficit countries, providing empirical evidence in favor of our results.====Our paper also belongs to a model-based literature that investigates the effects of the ZLB on the size of fiscal multipliers, summarized and referenced in Eggertsson (2011). In particular, Erceg and Lindé (2014) find that fiscal policy becomes self-financing at the ZLB, a result that is also present in our paper and key to our main result. To our knowledge however, there are only very few papers questioning the effect of the initial debt level on the size of public spending multipliers. Corsetti et al. (2013) do investigate this question but their analysis does not consider capital accumulation, and essentially focuses on the case of lump-sum taxes, while our main focus is on distortionary taxes. Along this dimension, our framework is closer to Nakata (2017), although we consider a richer model with capital accumulation, and our main focus is not on Ramsey equilibria but on the size of government spending multipliers and optimized policies. Our paper also echoes the recent contribution of Bilbiie et al. (2018), who derive optimal spending policies at the ZLB with an additional focus on spending multipliers. However, they consider lump-sum taxes and do not investigate the impact of the initial level of debt on spending multipliers and optimized policies.====The paper is organized as follows. Section 2 presents the model and details our calibration. Section 3 analyzes the Impulse Response Functions to spending shocks hitting at the steady state or conditional on a negative capital quality shock that pushes the economy at the ZLB, depending on whether the initial debt-to-GDP ratio and taxes are low or high. Section 4 summarizes our results by presenting the value of spending multipliers at various horizons and under the different cases considered, and produces an extensive sensitivity analysis. Section 5 investigates the design of optimized spending rules, and Section 6 concludes.",Spending multipliers with distortionary taxes: Does the level of public debt matter?,https://www.sciencedirect.com/science/article/pii/S0164070418304701,19 March 2019,2019,Research Article,261.0
"Bandyopadhyay Debasis,King Ian,Tang Xueli","Department of Economics, University of Auckland, Level 6, Sir Owen G Glenn Building, 12 Grafton Road, Auckland 1010, New Zealand,School of Economics, University of Queensland, Colin Clark Building (39), St. Lucia QLD 4067, Australia,Department of Economics, Deakin University, 221 Burwood Highway, Burwood, VIC3125, Australia","Received 5 February 2019, Revised 23 February 2019, Accepted 27 February 2019, Available online 12 March 2019, Version of Record 9 April 2019.",https://doi.org/10.1016/j.jmacro.2019.02.005,Cited by (15),"We analyse the impact of human capital misallocation and redistribution on GDP, welfare, and TFP, in economies with financial market imperfections. We present an overlapping generations model in which the innate abilities of children are drawn from a probability distribution and where parents are unable to insure against these shocks or borrow against their child’s future incomes. The government has access to a redistributive ==== and transfer system that can partially mitigate the effects of these missing markets. The steady state equilibrium of the model features closed form analytical solutions for the distributions of human capital, physical capital, and income, along with aggregate measures of TFP and welfare. We calibrate the model to the US economy and find that the misallocation, and its adverse effects on TFP and GDP, can be significant. However redistributive policies, aimed at mitigating the misallocation from these missing markets, have only small positive effects on TFP but large negative effects on GDP.","The importance of total factor productivity (TFP) for variations in per capita income has been recognized in macroeconomics and development economics ever since the seminal paper of (Solow, 1957). In the context of the standard neoclassical growth model, while factor inputs can play a role in these variations in the short run, TFP is the only source in the long run. Understanding the determinants of TFP in a consistent way requires a theory of TFP – something (Prescott, 1998) famously called for.====A considerable body of work, following Restuccia and Rogerson (2008) and Hsieh and Klenow (2009), has examined the role that the misallocation of resources, due to market distortions of some kind, play in determining TFP differences. Hsieh and Klenow (2009), for example, estimated that, if market distortions were somehow reduced down to US levels, TFP would increase in China by 30–50% and in India by 40–60%. Moreover, in their baseline, if market distortions were eliminated entirely in the US economy, this would increase TFP in that country by 42.9%. These large numbers have attracted considerable attention, and subsequent work has focussed attention on particular markets that may be responsible for these figures.====A particular emphasis has been placed on the role that imperfections in financial markets play in misallocation and TFP differences. Buera et al. (2011), for example, estimate that financial frictions can reduce TFP by up to 36%. Midrigan and Xu (2014) found that, while financial frictions do affect misallocation, relative to their effects upon firm entry and technological adoption, these effects are, quantitatively, quite small (reducing TFP by approximately 5% in developing countries). Subsequently, using a (Lucas, 1978) span-of-control model, based on Buera et al. (2011) and Bhattacharya and Ventura (2013), Yoon (2016) found that, by encouraging substitution from physical capital into managerial capital, credit market imperfections can actually ==== TFP. David et al. (2016) provide a theoretical link and quantitative estimate of the effects of informational frictions, arising from imperfect information regarding firm-specific demand conditions, indicated by the stock market data and the aggregate inefficiency in the economy, causing a positive variance of marginal products.====Some recent work has focused on the losses to TFP due to the misallocation of human capital. For example Hsieh et al. (2018) offer evidence to demonstrate how barriers to participation based on non-pecuniary factors, such as race and gender, which deter equalization of marginal products, cause loss of TFP and output by discouraging people with talent to pursue occupations in which they have a comparative advantage. Wang et al. (2018) report that a significant reduction in TFP could result from the mismatch between skill and technology reflecting the lack of assimilation of factor inputs in the economy in a way to equalize their marginal products.====In this paper we examine the implications of financial market imperfections upon the distribution of human capital – and their consequent effects on misallocation and TFP. Intuitively, human capital cannot be used as collateral. Consequently, in the presence of idiosyncratic innate talent dispersion among children, there exists no credit market through which rich parents can invest in the education of more talented children born into poorer families than their own. Similarly, current decision makers in families have no way of insuring against the risk that future family members will be born with disappointing innate abilities. Facing these financial frictions, parental investment in the education of children with different abilities can generate a misallocation of human capital (and a reduction in TFP) since, once again, marginal products will be not equated. Given these missing markets, and that the misallocation issue is driven largely by income inequalities across households, it is also natural to ask to what extent they can be mitigated by redistributional policies.====To study this issue, we consider a variant of Benabou’s (2002) model of the joint determination of the distribution of human capital and output, in equilibrium, in an environment with financial market imperfections. Following Benabou, we analyse a dynastical overlapping generations economy in which agents are restricted from passing debts to their descendents and face uninsurable idiosyncratic risks concerning the innate abilities of their offspring. A non-degenerate distribution of income persists in the long-run equilibrium of this model. This income distribution is influenced by, among other things, a redistributive tax-and-transfer scheme operated by the government, with the key policy parameter being the income-weighted average marginal tax and transfer rate, ====. As the government increases the value of ==== it redistributes from richer households to poorer ones.====In this setting, facing the financial constraints, poorer households are restricted when financing the education of their young. With diminishing marginal returns to education, this, alone, generates inefficiencies due to misallocation. The misallocation problem is worsened, though, when children are born with different abilities; particularly when poor parents of high-ability children are not able to afford the appropriate level of education for them. Thus an increase in ==== can, in principle, mitigate this problem by diminishing the effect of the credit constraint. However, as Benabou points out, this type of redistribution also distorts saving and labor supply decisions. Higher values of ==== therefore not only reduce misallocation (a positive effect) but also reduce savings and labor supply (a negative effect). Under reasonable parameter values, steady-state TFP, GDP, and welfare (using a utilitarian social welfare function), are hump-shaped in ==== and, in principle, one can find values of ==== that maximize each of these variables in equilibrium.====For tractability, Benabou’s (2002) original model has only one accumulable factor: human capital. His model is effectively one of a small open economy, where physical capital is rented in a large world market with fixed real interest rates. While this is a sensible approach for the issues that Benabou focuses on in his paper, here, we are focusing on determining TFP – and, consequently, physical capital should be treated more explicitly as an accumulable factor. In this paper, following Tang (2008), we introduce physical capital in a way that preserves many of the convenient mathematical properties of Benabou’s model. In particular, analytical solutions are available for all of the key endogenous variables – making both calibration and interpretations relatively easy.====We calibrate the model to the US, and use it to answer the following quantitative questions. What effects does ==== have on TFP, GDP, and welfare? What are the trade-offs between efficiency and equity in the US economy? How far is the current degree of redistribution from the ones that maximize TFP, GDP, and welfare? What are the relative effects of human capital versus physical capital misallocation? To what extent can redistribution eliminate the misallocation induced by the financial constraints?====In the calibrated model we find that steady state TFP, GDP, and welfare are all inverted-U-shaped functions of the policy variable that we focus on: ====. We also find that the values of ==== that maximize TFP, GDP, and welfare are quite different from each other: GDP is maximized at ==== (ie., 27.3%), welfare is maximized at ==== and TFP is maximized at ====. According to the latest estimate, the current value of ==== in the US is approximately ==== – slightly above the value that maximizes GDP, but significantly below the values that maximize welfare and TFP. In the baseline model an increase in the degree of redistribution, from the current 31.4% up to the welfare-maximizing 57.3%, would lead to a reduction in GDP, but also a reduction in inequality, an increase in welfare, and an increase in TFP. Any increase in ==== beyond that rate would reduce GDP and inequality further, but decrease welfare overall, while continuing to increase TFP. If ==== is increased beyond 83.8% then GDP, inequality, welfare, and TFP all decrease.====The model also implies that TFP is reduced significantly more by the misallocation of human capital, rather than physical capital. The inclusion of physical capital in this model is quite important, though. In Benabou’s original model without physical capital (corrected), the value of ==== that maximizes GDP is significantly higher (34.8%), rather than 27.3% in our model. According to the model without physical capital, the existing value of 31.4% is slightly below the value that maximizes GDP. Allowing the inclusion of physical capital, though, this model implies that the existing value of ==== is too high to maximize GDP – but still too low to maximize welfare or TFP.====In the spirit of the (Hsieh and Klenow, 2009) exercise, we also use the model to quantify the overall implications of the misallocation, in equilibrium, upon TFP. We find that the complete elimination of the misallocation problem in this model (by allowing a full set of financial markets) would increase TFP by approximately 15.1% (compared with 42.9% in Hsieh and Klenow (2009)). The effects on TFP through changing the value of ==== are considerably smaller. For example, in our baseline model, if the TFP-maximizing tax-and-transfer rate ==== replaces the existing ==== it increases TFP by only 0.76%. It also ==== GDP by 10.7%.====We conclude, therefore, that human capital misallocation may in fact be an important determinant of TFP. However, using redistributive policy to alleviate this misallocation involves significant trade-offs.====The remainder of the paper is structured as follows. The model is presented in Section 2, its equilibrium dynamics are characterized in Section 3, and its steady state is analysed in Section 4. In Section 5 we present the quantitative analysis, and Section 6 contains some concluding remarks. The appendix in the paper provides proofs for some of the key results, but lengthier proofs are provided in the online appendix.","Human capital misallocation, redistributive policies, and TFP",https://www.sciencedirect.com/science/article/pii/S016407041930045X,12 March 2019,2019,Research Article,262.0
"Gerlach Stefan,Stuart Rebecca","EFG Bank, Bleicherweg 19, CH-8001 Zurich, Switzerland,Central Bank of Ireland, New Wapping Street, North Wall Quay, Dublin 1, Ireland","Received 8 July 2018, Revised 23 February 2019, Accepted 4 March 2019, Available online 6 March 2019, Version of Record 13 March 2019.",https://doi.org/10.1016/j.jmacro.2019.03.001,Cited by (2),"The FOMC's “dot plots” contain members’ views regarding what federal funds rate will be necessary in the end of this and the coming years for the FOMC to achieve its statutory objectives. The dots can be interpreted as instantaneous forward rates. We fit a curve, which is characterised by four parameters, through them and study how it moves with the economy. We find that the level of the federal funds rate the month before the FOMC meeting, the unemployment rate and (updated) estimates by Laubach and Williams (2003) of the natural real interest rate shape the curves.","In January 2012, the FOMC published its first “dot plot” which provided members’ assessments of “appropriate monetary policy.” The Fed defines this as:====Since then, the dot plots have become a keenly watched indicator by financial market participants, policymakers and commentators alike. Changes from one dot plot to the next are taken as important signals of the FOMC's policy intentions. By enabling readers to infer policy makers’ views of the appropriate future path of interest rates, they provide information that is not available in other FOMC communications.====This is illustrated by Fig. 1, which shows the ‘dot plot’ for September 2015, the last meeting at which a dot plot was presented before the FOMC raised rates in December 2015.==== Each dot represents one FOMC member's view about what federal funds rate the FOMC would need to set in the future to achieve its statutory objectives. In September 2015, interest rates were still at the zero lower bound after the global financial crisis. However, the median FOMC member projected interest rates for the end of 2015 to be 0.25–0.5%, signalling the rate increase that subsequently took place in December 2015. Observers understood this signal, and when the interest rate increase was eventually announced, there was little market reaction.====Thus, to avoid surprising market participants and the public, it is necessary for them to understand the information contained in the dot plots. It would also be desirable for them to understand how the FOMC's views about the appropriate path of interest rates evolve with the state of the economy. This would allow market participants and the public to infer the FOMC's reaction function and form expectations about how monetary policy will respond to different events. Since the FOMC has a dual mandate for price stability and maximum employment, one would expect that measures of inflation pressures and labour market slack impact on FOMC members’ views about the appropriate path of interest rates.====However, deducing the FOMC's reaction function from the dot plots is not as easy as it may seem at first glance. Returning to Fig. 1, the dots indicate FOMC members’ view of the appropriate level of interest rates at the ends of 2015, 2016, 2017, 2018 and in “the longer run” (which the Federal Reserve implicitly defines as the level that interest rates would converge to over time in the absence of further shocks).==== Herein lies the difficulty of inferring the reaction function: since the dot plots provide information on only four or five discrete points in time, changes in them between FOMC meetings reflect both shifting economic conditions and changes in the forecast horizon.====Furthermore, the dots represent the views of individual members and not the overall FOMC. The dispersion of the dots in Fig. 1 indicates considerable differences in members’ views about future monetary policy. While in the short run the range of views of the appropriate interest rate is more than 1 percentage point wide, in the medium term the range is even greater and is sometimes in excess of 2 percentage points. Members’ views about the longer run are more similar: the range falls to 1 percentage point. Overall, the degree of dispersion seems very large; FOMC members were evidently highly uncertain about what future path of interest rates will be appropriate.====The dot plots can be thought of as representing members’ expectations of the appropriate instantaneous forward rate at selected points in time. This suggests that one could fit a smooth curve to the dots and use the parameters that describe that curve to characterize monetary policy. In this paper we propose a way to do so.====We find that the logistic model fits the data well. By estimating the parameters characterizing the logistic function on data from the 25 dot plots published between January 2012 and December 2017, we can study how the path of interest rate expectations changes with economic conditions. This allows us to answer questions such as whether a strengthening of the economy leads FOMC members to expect an earlier or a faster increase in interest rates (or both).====The logistic function is characterized by four parameters. We estimate these and ask how they evolve over time and whether the estimates appear correlated with economic conditions. The first parameter determines the lower level that, in the present case, interest rates rise from. The estimates suggest that it is given by the effective federal funds rate in the month before the release of the dot plots. The second parameter is the level the federal funds rate settles at in the steady state. We find that it appears to decline with the estimates of the natural interest rate presented by Laubach and Williams (2003). The third parameter is the speed by which rates are expected to be changed. Our estimates suggest that it is constant and independent of economic conditions. The fourth parameter is the time it is expected to take before half of the adjustment of rates is completed. Empirically, it seems strongly correlated with the unemployment rate but not inflation.====We then go on to estimate the 25 equations jointly, but introduce restrictions suggested by the observations noted above: that the curve starts from last months’ level of the federal funds rate; that the level the curve asymptotes to depends on estimates of the natural interest rate; that the speed of the rate increases is constant; and that length in months to the midpoint of the adjustment depends on the unemployment rate. These restrictions reduce the total number of parameters from 25 × 4 = 100 to 6. We use simulations to show how the curves would change if the state of the economy changed.====The paper is organized as follows. In the next section we discuss the related literature. Section 3 describes the data, including the sample size, frequency and dispersion of FOMC members’ projections. In Section 4 we discuss and estimate the logistic function. In Section 5 we consider in more detail the estimated coefficients from the logistic function, and how these are dependent on macro variables. Section 6 concludes.",Plotting interest rates: The FOMC's projections and the economy,https://www.sciencedirect.com/science/article/pii/S0164070418302970,6 March 2019,2019,Research Article,263.0
"Aizenman Joshua,Jinjarak Yothin,Nguyen Hien Thi Kim,Park Donghyun","University of Southern California & the NBER, United States,Victoria University of Wellington, New Zealand,Asian Development Bank, Philippines","Received 18 September 2018, Revised 27 February 2019, Accepted 28 February 2019, Available online 1 March 2019, Version of Record 22 March 2019.",https://doi.org/10.1016/j.jmacro.2019.02.006,Cited by (33),"This paper compares fiscal cyclicality across advanced and developing countries, geographic regions as well as income levels over 1960–2016 period, then identifies factors that explain countries’ government spending and tax-policy cyclicality. Public debt/tax base ratio provides a more robust explanation for government-spending cyclicality than public debt/output ratio but the reverse is true when capital investment is accounted for in government spending. On average, a more indebted (relative to tax base) government spends more in good times and cuts back spending indifferently compared with a low-debt country in bad times. We also find that country's ==== has a countercyclical effect in our estimation. Finally, the analysis depicts a significant economic impact of an enduring interest-rate rise on fiscal space, that is, a 10% increase of public debt/tax base ratio is associated with an upper bound of 5.9% increase in government-spending procyclicality.","The Global Financial Crisis (GFC) focused attention on unsustainable leverage growth as a key contributing factor in growing financial fragility associated with “bubbly” dynamics. Essentially a prolonged appreciation of financial and real estate markets increases the vulnerability to sharp asset valuation corrections. A deep enough correction may trigger banking crises and fire sales dynamics, potentially pushing the economy into a prolonged depression and a growing exposure to social and political instability.==== Concerns about reliving the 1930s Great Depression explain the complex set of policies implemented by the U.S. and other affected countries in the aftermath of the GFC: a massive infusion of liquidity in support of financial and banking systems and bailing out systemic banks and prime creditors. The forced deleverage of private borrowers, and the growing fear of a prolonged recession, induced higher household savings and lower investment, further deepening recessionary forces.====To counter these forces, many countries therefore experimented with fiscal stimuli aimed at mitigating the deepening recessions. Stabilizing the banking and financial systems, in addition to the stimuli, ended up sharply raising countries’ public debt/GDP ratio, pushing advanced countries towards a public debt/GDP ratio of above 100%. Similar trends applied to emerging market economies (EMEs), driving their ratio of public debt to GDP upward, with some reaching well above 50%.==== Notwithstanding the fact that the average public debt/GDP ratio of EMEs is below that of OECD countries, EMEs’ lower tax base/GDP ratios, as well as the higher interest rates paid on their debt (due to sovereign risk premia), imply a rising fragility of EMEs compared with OECD countries. As such, while the ratio of public debt to GDP is used frequently in policy discussions, the ratio of public debt to tax base accounting for tax revenue may provide a more informative measure of the fiscal burden associated with the stock of public debt (Aizenman and Jinjarak, 2011). Henceforth, we refer to this fiscal measure as ====.====Importantly, the post-GFC trajectory failed to deal with leverage concerns: “At $164 trillion—equivalent to 225% of global GDP—global debt continues to hit new record highs almost a decade after the collapse of Lehman Brothers. Compared with the previous peak in 2009, the world is now 12% of GDP deeper in debt, reflecting a pickup in both public and nonfinancial private sector debt after a short hiatus. All income groups have experienced increases in total debt, but, by far, EMEs are in the lead.” (International Monetary Fund, 2018). In other words, stabilizing a crisis triggered by an unsustainable leverage growth in turn contributed to a potentially untenable increase in leverage/GDP ratios.====For the past decade, the monetary easing associated with the U.S. Federal Reserve (FED) and the European Central Bank policies in the aftermath of the GFC led to an unprecedented decline of policy interest rates and risk premia. These developments markedly reduced the flow costs of serving the rising public and private debt, thereby masking the increasing fragility associated with the rising aggregate leverage/GDP. This period has now passed: the (so far) robust recovery of the U.S., the gradual unwinding of the FED's balance sheet, the projected upward trajectory of the FED's funds rate, and the recovery of the Eurozone will impose growing fiscal challenges that will test countries’ fiscal space and their ability to cope with projected higher interest rates by raising their resilience.====A key resilience margin is securing fiscal space—the fiscal capacity of countercyclical policy aimed at mitigating business cycles and preventing a prolonged depression in the aftermath of financial crises (Auerbach, 2011, Ostry et al., 2010); see also Gavin et al. (1996) on the identification of fiscal procyclicality as a major amplifier of developing countries’ vulnerability to shocks. Remarkably, over the last two decades leading to the GFC, a growing share of fiscal policies in developing countries and EMEs had graduated from procyclicality and become countercyclical (see Frankel (2011) and Frankel et al. (2013)). Cross-country studies offer several explanations. Woo (2009) presented some evidence showing that social polarization, as measured by income and educational inequality, is consistently and positively associated with fiscal procyclicality, controlling for other determinants; there is also a robust negative impact of fiscal procyclicality on economic growth. Aizenman and Jinjarak (2012) found that higher income inequality is strongly associated with a lower tax base, lower de facto fiscal space, and higher sovereign spreads. Végh and Vuletin (2015) find that tax policy is less procyclical (more countercyclical) in countries with better institutional quality and more financially integrated; tax and spending policies are conducted in a symmetric way over the business cycle. For brevity, Table 1 provides a summary of the related literature.====It has been common wisdom that when there is a sharp decline in economic activity of an uncertain duration, government intervention should take the form of an expansionary fiscal policy. Yet, the viability of this option hinges upon the degree to which the government has elastic enough access to borrowing. In the past, this has been the case for most OECD countries, allowing them to contemplate both tax cuts and fiscal expansions in recessionary times. In contrast, developing countries and emerging markets with a low tax base and a large public debt burden have a limited fiscal space, facing thereby more complex trade-offs. Specifically, cutting taxes or increasing government expenditure in recessionary times will increase the interest rate on their public debt, dampening thereby the stimulus and increasing the cost of serving the debt overhang. Gaining more fiscal space for heavily indebted countries requires practicing more countercyclical policies in good times, allowing them to repay more of their debt in expansions, thereby increasing their fiscal space in recessions.====The rapid increase in public debt/GDP of most OECD countries in recent years suggests that the old fiscal dichotomy between the fiscal space of the OECD and other countries is blurred by now. This was vividly illustrated by the growing challenges facing Belgium, Greece, Italy, Iceland and Ireland in recent years. Thereby, greater countercyclicality in good times may be the key for securing greater fiscal space in future rescissions. Considering these developments, our paper focuses mostly on the cyclicality patterns exhibited in recent decades, and the projected challenges associated with possible future interest rate hikes.====Against this background, we assess definitions and empirical measures of fiscal cyclicality, compare fiscal cyclicality across Asia, Latin America, the OECD, and other regions, then identify factors accounting for spending- and tax-policy cyclicality patterns. We link the capacity of countercyclical policy to the fiscal space and the stage of economic and institutional development, as both are associated with the servicing capabilities of domestic and foreign debt. Our analysis focuses on differences across the country groups and examine the role of economic structure (commodity versus manufacturing outputs), financial openness, as well as institutional and socio-economic factors (political risks, polarization, and ethnic polarization). The paper concludes with an analysis of possible scenarios and suggested policies aiming at increasing the resilience of EMEs.====Our study reveals a mixed fiscal scenery, where more than half of the countries are characterized by limited fiscal space, and fiscal policy is either pro- or acyclical. More limited fiscal capacity, as measured by the ratio of public debt to 3-years moving-average tax revenue and its volatility are positively associated with fiscal cyclicality, while those of public debt/GDP are statistically significant in several cases, suggesting that public debt/tax base ratio provides a robust fiscal-space explanation for studying government-spending and tax-rate cyclicality.==== We calculate the impact of an enduring interest-rate rise on fiscal space, rank countries and regions by the fragility of their fiscal space to such an environment, and discuss policies to increase fiscal resilience. The rest of the paper is organized as follows. Section 2 reports the empirical analysis with the baseline estimation and robustness checks. Section 3 provides the conclusion.","Fiscal space and government-spending and tax-rate cyclicality patterns: A cross-country comparison, 1960–2016",https://www.sciencedirect.com/science/article/pii/S0164070418303938,1 March 2019,2019,Research Article,264.0
"Boubakri Salem,Guillaumin Cyriac,Silanine Alexandre","Univ. Grenoble Alpes, Grenoble INP, 38000 Grenoble, France","Received 5 August 2018, Revised 4 January 2019, Accepted 24 February 2019, Available online 25 February 2019, Version of Record 13 March 2019.",https://doi.org/10.1016/j.jmacro.2019.02.004,Cited by (18),"The aim of this paper is to contribute to the existing literature by exploring the relationship between the real commodity price volatilities and the real effective exchange rate (REER) of commodity-exporting countries, taking into account the transition variable of financial market integration. To this end, we consider a sample of 42 commodity-exporting countries subdivided into 4 panels: food and beverages, energy, metals, and raw materials. Our results highlight that the relationship between real commodity price volatility and REER is non-linear and depends on the degree of ","For countries in which the majority of revenue emanates from export activities, the real effective exchange rate (REER) appears to be a crucial variable in determining trade capabilities. The REER measures the real value of a specific currency of a given country against its main trade partners. In addition, working on the fluctuation in REER allows a comprehensive study, as the REER does not fluctuate randomly and is expected to react to certain macroeconomic variables, which have been found to be a key determinant of its dynamics in the medium term. However, in the literature there are many different views and there is no consensus regarding the main factors impacting exchange rates. The empirical research on this topic has evolved over time and evidenced many results based on different approaches to monetary factors, or to other factors such as volatilities in world prices for domestic exports. In their seminal paper, Meese and Rogoff (1983) showed that the random walk model outperforms a range of fundamentals-based models of exchange rate determination at different horizons. Following this result, the literature, and most researchers, have concluded that macroeconomic fundamentals do not play a role in exchange rate fluctuations.==== However, as noted by Rogoff (2008) or, more recently, Ferraro et al. (2015), the findings of Meese and Rogoff (1983) are limited to major floating currencies and not necessarily relevant to emerging market currencies (where interest and inflation differentials are often much larger). Moreover, a large body of literature (for instance, Amano and van Norden, 1998a, Amano and van Norden, 1998b, Chen and Rogoff, 2003, Cayen et al., 2010, Alquist et al., 2011) has been developed and found that real commodity prices could be a key explanatory variable for the real exchange rate. Finally, as demonstrated by Rossi (2006) or Engle et al. (2007), according to the forecasting horizon (short or long), monetary models help to forecast changes in exchange rates, and therefore exchange rates do not follow a random walk.====For the special case of net commodity-exporting countries, almost all are developing economies, and an extensive body of literature has focused on the determinants of their real exchange rate. They provided evidence that real commodity-export prices are the preponderant factor in the determination of the real exchange rates of these commodity-exporting developing countries (Cashin et al., 2004). This finding led to the labelling of such real exchange rates as “commodity currencies”. This denomination is widely used nowadays: for example, the Forex market uses it to gather currencies from commodity exporters whose economies are developed, such as Australia and Canada, as well as a number of currencies issued by emerging commodity exporters.====Furthermore, the intensification of practices such as portfolio rebalancing and hedging has led to a growing impact in the financialization of the commodities on the determination of their prices (Cheng and Xiong, 2013). Indeed, the volume of crude oil futures traded on the New York Mercantile Exchange (NYMEX) has more than quintupled since the early 2000s (Fratzscher et al., 2014). Therefore, the REER of some countries may be impacted more depending on whether they are more or less developed financially. In addition, investment funds, which are most likely to impact commodity prices through their actions as a result of their high purchasing power tend to buy their futures commodity contracts based on commodity price indices. Consequently, we should be able to capture most of the effect generated by the financialization of the commodity markets by selecting the commodity price indices in respect to the main type of commodity exported by the different countries.====In this paper our goal is to contribute to the existing literature by exploring the potential non-linear impact of the volatility of real commodity price indices on REER variations when analysing commodity-exporting countries, by considering the degree of financial integration as one of the most important transmission channels. Indeed, considering the growing financialization of the commodity markets, it seems that the level of financial integration of a country may play a role in the transmission channel, from real commodity price volatilities to the REER. Moreover, the existing literature focuses mainly on the impact of oil price volatility on the REER of energy-exporting countries. Our aim is to fill the existing gap in the current literature in different ways. First, we extend our study to many types of commodity, not just to the oil market, in order to take into account the heterogeneity that may exist across countries and their reaction to the dynamics of the transition variable. To this end, we subdivide our commodity-exporting countries into four panels: food and beverages, energy, metals, and raw materials. Second, as our main variable of interest is the volatility of the real commodity price indices, each panel will be respectively linked to a different index in adequacy, with the main type of commodity exported by the countries included in the panel. Third, we test a new transition variable linked to the financial market, namely, the degree of financial integration, to consider the growing financialization of the commodity markets. To check the robustness of our results, we test alternative measures of the degree of financial integration, such as M2 in percentage of GDP, private credit in percentage of GDP and foreign direct investment in percentage of GDP. To the best of our knowledge, no previous studies have included the degree of financial integration as the transition variable in the relationship between real commodity price volatilities and REER variations.====We consider a sample of 42 commodity-exporting countries, subdivided into 4 panels, as mentioned above. We rely on a panel smooth transition regression (PSTR) model proposed by González et al., 2005, González et al., 2017) in order to consider the non-linear impact of the volatility of the real commodity price indices on the real effective exchange rate variations of commodity-exporting countries. Our results show that the short-term dynamics of the real exchange rate of commodity-exporting countries are affected by their respective real commodity price index volatility, depending on whether the country is better integrated financially. Our findings also highlight the growth of financialization of commodities post-2000, particularly in the case of the energy sector.====This paper is structured as follows. Section 2 reviews the existing literature. Section 3 describes the econometric methodology used. Section 4 describes the data and its statistical properties. Section 5 is devoted to commenting on the results found and the specification tests. Section 6 concludes.",Non-linear relationship between real commodity price volatility and real effective exchange rate: The case of commodity-exporting countries,https://www.sciencedirect.com/science/article/pii/S0164070418303392,25 February 2019,2019,Research Article,265.0
"Glocker Christian,Sestieri Giulia,Towbin Pascal","Austrian Institute of Economic Research, Arsenal, Objekt 20, Vienna 1030, Austria,Bank of France, 31 rue Croix des Petits Champs, Paris 75001, France,Swiss National Bank, Bundesplatz 1, Berne 3003, Switzerland","Received 20 April 2018, Revised 29 January 2019, Accepted 15 February 2019, Available online 18 February 2019, Version of Record 13 March 2019.",https://doi.org/10.1016/j.jmacro.2019.02.003,Cited by (7),We study government spending multipliers of the UK economy using a time-varying parameter factor augmented vector ,"In this paper we investigate if and why government spending multipliers vary across time, taking the example of the United Kingdom over the past fifty years.====In the recent economic history of the UK there have been several moments characterized by intense debates about how changes in government spending may affect the economy. Together with the U.S., the UK was one of the major advanced countries that responded to the financial crisis with a strong counter-cyclical fiscal expansion in 2008. The reversal of the fiscal stimulus in 2010 gave rise to a harsh debate on austerity. Advocates of austerity were of the view that the measures were necessary to ensure the confidence of financial markets about the sustainability of public debt (Rogoff, 2013) and could possibly crowd-in private consumption and investment (Trichet, 2010). Opponents claimed that the program would put at risk the timid recovery and feared large negative effects, arguing that government spending multipliers were particularly large at that moment because of the presence of idle economic resources and of monetary policy being constrained by the zero lower bound (Krugman, 2015).====This paper contributes to the literature on the non-linear effects of fiscal policy by using an econometric framework that is flexible enough to distinguish cyclical variations from structural changes in government spending multipliers and rich enough to discriminate between different theoretical transmission mechanisms.====Economic theory offers a number of reasons why government spending multipliers may vary over time. They can be broadly divided into two groups: cyclical theories and structural theories.====Prominent examples of cyclical theories put forward three main explanations for time variation: economic slack, financial frictions, and the zero lower bound (ZLB) on nominal interest rates. According to the economic slack hypothesis, government spending multipliers should be larger in recessions because government spending can mobilize idle resources without generating inflationary pressures (Michaillat, 2014). The financial friction theory emphasizes access to credit rather than idle resources. Financial frictions impede the access of the private sector to credit. Government spending alleviates these frictions through its stabilizing effect on output (Galí, López-Salido, Vallés, 2007, Canzoneri, Collard, Dellas, Diba, 2016). The zero lower bound hypothesis posits that at the zero lower bound central banks will not tighten monetary policy to contain inflationary pressures generated by positive fiscal shocks, as they would otherwise. This could create larger multipliers (Christiano, Eichenbaum, Rebelo, 2011, Woodford, 2011, Coenen, Erceg, Freedman, Furceri, Kumhof, Lalonde, Laxton, Lindé, Mourougane, Muir, Mursula, de Resende, Roberts, Roeger, Snudden, Trabandt, in’t Veld, 2012).====On the structural side, explanations for time-variation in government spending multipliers include trade openness and the amount of fiscal space, among others. The trade integration theory predicts a downward trend in the size of government spending multipliers, as higher trade integration should increase the share of the government spending impulse that leaks abroad through higher imports. According to the fiscal space theory, government spending multipliers should be larger when governments have more time to stabilize debt after an expansion, i.e., when public debt or the interest rates payments on debt are low (Perotti, 1999, Corsetti, Meier, Müller, 2012, Nickel, Tudyka, 2014).====To shed light on the time-variation of UK government spending multipliers and their drivers, we estimate a time-varying parameter factor augmented vector autoregressive model (TVP-FAVAR) and identify government spending shocks. A TVP-FAVAR has two key advantages, as detailed in Section 2. First, the TVP component of the model is agnostic on whether changes in multipliers are structural or cyclical. Second, the use of factor models allows us to track the responses of a large number of variables and to address the problems of limited information and fiscal foresight, from which many small scale VAR models suffer.====Our results show that UK government spending multipliers vary over time and that most of the variation is cyclical. The multiplier on GDP is typically above one in recessions and below one in expansions. By contrast, multipliers do not exhibit a structural trend. Regarding the different demand components of GDP, the increase of the multiplier in recession is mainly driven by the behavior of investment, which contributes positively in recession, while it responds negatively or negligibly in normal times. The positive effect on consumption is also substantially larger in recession. The amplifying effects of consumption and investment are dampened to some extent by a stronger response of imports, whereas the response of exports does not vary much over time.====As regards the drivers of the cyclical variations, the results of the model are consistent with theories emphasizing the role of financial frictions and economic slack. Typically, a larger impact on output is accompanied by stronger effects on private sector credit generation, suggesting that fiscal expansions help relieving credit constraints. This is in line with the literature that studies the interaction between fiscal multipliers and the financial cycle, which generally finds higher multipliers in periods of financial distress (Corsetti, Meier, Müller, 2012, Ferraresi, Roventini, Fagiolo, 2015, Borsi, 2018, Pragidis, Tsintzos, Plakandaras, 2018).==== As for the economic slack hypothesis, we do find that multipliers are generally larger in recessions. However, when investigating inflationary pressures – a variable that has received little attention in the literature on fiscal multipliers - we find only limited evidence for prices to be systematically less sensitive to spending stimuli in recessions. Finally, according to our findings, UK multipliers are not systematically higher during the recent zero lower bound period.====As an additional exercise, we then regress the estimated output multiplier on a number of potential cyclical and structural determinants. The results from this regression confirm our previous findings. Credit and financial stress variables have a significant impact on the multiplier with the predicted sign. A measure of economic slack and the real policy rate, which captures the degree of monetary policy accommodation, are also significant cyclical drivers of the government spending multiplier. Structural factors are only of second order importance in explaining the variation of the UK multiplier over time.====The rest of the paper is organized as follows: Section 2 details how our paper contributes to the literature. Section 3 presents data and methods. Section 4 presents and discusses the main empirical results of the model while Section 5 focuses on the possible transmission channels and their respective importance for the UK economy. Section 6 presents some robustness exercises, including an analysis of the sensitivity of the results with respect to the identification approach used in the paper. Section 7 concludes. Technical details on the estimation technique and more information on the database are left to the online appendix.",Time-varying government spending multipliers in the UK,https://www.sciencedirect.com/science/article/pii/S0164070418301642,18 February 2019,2019,Research Article,266.0
"Henckel Timo,Menzies Gordon D.,Moffatt Peter,Zizzo Daniel J.","Australian National University and Centre for Applied Macroeconomic Analysis, Australia,University of Technology, Sydney and Centre for Applied Macroeconomic Analysis, Australia,University of East Anglia, United Kingdom,University of Queensland and Centre for Applied Macroeconomic Analysis, Australia","Received 13 August 2018, Revised 19 December 2018, Accepted 17 January 2019, Available online 13 February 2019, Version of Record 4 April 2019.",https://doi.org/10.1016/j.jmacro.2019.01.012,Cited by (3),"We use the behavior of ==== framework that the latter has deteriorated in recent history; that is, price setters are less likely to rely on the ECB target when forming ====.","The 2007/8 global financial crisis exposed large inconsistencies among member countries of the European Monetary Union (EMU). Significant inflation differentials compounded over time to generate large differences in real exchange rates and current account imbalances, raising doubts about the unionʼs viability. This suggests that central bank credibility, at least in a monetary union, is multi-dimensional and the usual measure of credibility, normally interpreted as the commitment to follow well-articulated and transparent rules and policy goals (Bordo and Siklos, 2014), may not suffice.====We use the behavior of inflation among Eurozone countries to provide information about the degree of credibility of the European Central Bank (ECB). Our model offers two distinct advantages: first, the setup is able to econometrically extract information about inflation expectations from observed inflation data alone, that is, without any reference to survey data or market price-based measures. This allows for a simple single-equation specification, which is typically more robust to specification error than a system of equations.====Second, our econometric model has the innovative feature that discrete changes in the marginal effects of information are endogenous. In our setup some explanators in regressions can seem to be insignificant most of the time but may suddenly become highly significant, namely when agents revise their beliefs and reweight the importance attached to alternative sources of information.====We generalize (Lucas, 1972), showing how private sector inference affects the time series properties of inflation. Within this inferential expectations (IE) framework (Menzies and Zizzo, 2009) we define three dimensions of credibility - ==== and ==== and show theoretically as well as empirically that a central bank need not be equally credible along all dimensions. Estimation for the Euro zone reveals that since the crisis, official target and cohesion credibility have been maintained but at a cost of detioriating anchoring credibility.====The paper proceeds as follows. In the following section we define and discuss three dimensions of central bank credibility. Section 3 reviews the theory of inferential expectations (IE) and derives a new Phillips curve, the IE Phillips curve, based on the classic Lucas islands model. In Section 4 we show how the IE Phillips curve relates to credibility, which serves as the basis for the econometric work in Section 5. This is followed by a discussion and conclusion in Section 6.",Three dimensions of central bank credibility and inferential expectations: The Euro zone,https://www.sciencedirect.com/science/article/pii/S0164070418303501,13 February 2019,2019,Research Article,267.0
"Guo Jang-Ting,Izumi Yutaro,Tsai Yi-Chan","Department of Economics, University of California, 3133 Sproul Hall, Riverside, CA 92521, USA,Department of Economics, Northwestern University, 2001 Sheridan Road, Evanston, IL 60208, USA,Department of Economics and Center for Research in Econometric Theory and Applications, National Taiwan University, No. 1, Sec. 4, Roosevelt Rd., Taipei 10617, Taiwan","Received 18 April 2018, Revised 25 January 2019, Accepted 5 February 2019, Available online 8 February 2019, Version of Record 18 February 2019.",https://doi.org/10.1016/j.jmacro.2019.02.002,Cited by (3),"This paper quantitatively examines the long-run macroeconomic effects of resource misallocation in an otherwise standard one-sector ==== with heterogeneous establishments, characterized by different ages and productivity levels, that are subject to ==== as well as endogenous entry decisions. Under a progressive fiscal policy rule, capital and labor inputs move from more productive to less productive establishments because the latter face a lower ","There has been a growing literature that explores the effects of resource misallocation on a macroeconomy’s output and measured total factor productivity (TFP).==== Moreover, several recent studies have found that varying entry costs may help explain the substantial cross-country income differences.==== In the context of a dynamic general equilibrium model with heterogeneous establishments, aggregate TFP depends not only on the productivity level of each individual firm, but also on how productive factors are allocated across these production units. As a result, government policies that distort the relative prices faced by heterogeneous enterprises will influence the resource allocations and thus generate considerable effects on aggregate economic activity. These policies also affect the expected value of incumbent establishments, which in turn impacts the entry decision of potential entrants, as well as the total number of firms in operation and the resulting overall output. Motivated by this strand of previous research, our paper quantitatively examines the long-run effects of resource misallocation on the economy’s aggregate variables within an otherwise standard one-sector neoclassical growth model in which output is carried out by heterogeneous firms, characterized by different ages and productivity levels, that are subject to a progressive tax policy ==== (Guo and Lansing, 1998) together with endogenous entry decisions.====Given the progressive fiscal policy rule under consideration, below-average productive establishments face lower tax rates whereas above-average firms within the same age-cohort are subject to heavier taxation. In terms of across-cohort distortions, since older establishments tend to be larger and more productive than their younger counterpart, progressive taxation will disproportionally hurt more mature firms. Our nonlinear tax formulation therefore affects macroeconomic aggregates through the channels of both intensive and extensive margins. On the one hand, capital and labor inputs will move from more productive to less productive establishments within as well as across cohorts. When the tax progressivity rises, resource misallocation exacerbates the overall production as low-productivity establishments use an inefficiently high level of productive resources – this is the adjustment along the ==== margin. On the other hand, since establishments may freely enter the market upon the payment of an entry cost, progressive taxation decreases the expected value of an incumbent establishment. This discourages potential entrants to enter the market, which in turn further reduces aggregate production – this is the adjustment along the ==== margin.====For the quantitative analyses, our tax-progressivity parameter is calibrated to match with Hsieh and Klenow (2014) empirical estimate on the elasticity of distortion with respect to productivity for the U.S. economy. Together with this baseline degree of tax progressivity, we also calibrate the returns-to-scale parameters and firm-level productivities in the model economy to match with the age and size distributions across U.S. establishments, as reported in Boedo and Mukoyama (2012); and then postulate the benchmark specification to feature variable labor supply. We focus on the model’s stationary competitive equilibrium and analyze the macroeconomic impact of changes in the level of tax progressivity through two numerical experiments. First, we find that compared to the flat (==== no progressive) taxation case, our “baseline tax distortion” formulation exhibits the same decrease in the number of entrants as well as incumbents (by 0.39 percent). Moreover, there are reductions in both labor (by 1.82) and capital (by 3.92 percent) inputs, which in turn generates a decline of total output by 2.28 percent. Finally, the economy’s aggregate total factor productivity will slightly fall (by 0.09 percent). Second, we find that when the tax progressivity doubles, the percentage decreases in all key macroeconomic variables are approximately doubled as well, indicating that the aggregate effects of more progressive taxation are quantitatively significant. In sum, these results demonstrate the quantitative interrelations between progressive taxation, resource misallocation and endogenous entry decisions within our model economy.====To obtain further insights about the proceeding tax-change outcomes, we decompose the total effects into adjustments along the intensive margin (the average employment level across operating establishments) and the extensive margin (the total number of producing firms). Specifically, the extensive-margin changes are suppressed by adjusting the parameter value of entrance cost such that the stationary productivity distributions over establishments under different degrees of tax progressivity are identical to that in our “no progressive taxation” (or constant-tax) case. As a result, the resource-misallocation or the intensive-margin effects across incumbent firms can be singled out. Based on this decomposition exercise for our benchmark model, the intensive margin, i.e. effects due to resource misallocation, accounts for a quantitatively larger fraction of falls in aggregate output (by 2.21 to 4.24 percent), aggregate consumption (by 2.06 to 4.00 percent), aggregate capital (by 3.85 to 7.27 percent), aggregate labor (by 1.82 to 3.41 percent), equilibrium wage (by 2.06 to 3.99 percent) and aggregate TFP (by 0.03 to 0.12 percent).====For the sensitivity analyses, we first consider an alternative formulation by postulating the number of working employees to be a fixed constant over time. Under the presence of endogenous entry decisions, more progressive taxation yields smaller decreases in capital (by 2.14 to 4.10 percent) and output (by 0.46 to 0.96 percent) than those in our benchmark model with variations in hours worked. However, the total numbers of new entrants and operating incumbents increase by 1.46 to 2.94 percent when the fiscal policy becomes more progressive. These turn out to be qualitatively opposite to those in our baseline specification with endogenous labor supply. Moreover, this positive effect along the extensive margin dominates the negative effect of resource misallocation from the intensive margin. Therefore, the measured aggregate TFP will rise (by 0.18 to 0.29 percent).====Next, since the returns-to-scale parameters play an important role in affecting the number of producing establishments and thus the economy’s aggregate variables, we examine two alternative calibrations that are higher or lower than those in our baseline parameterization. In the former case, the optimal scale for each operating establishment is relatively larger, hence progressive taxation generates a stronger negative effect from misallocation of productive resources. On the other hand, the expected value of an incumbent producer rises when the fiscal policy rule becomes more progressive, which in turn will encourage more potential entrants to enter the market and more establishments to produce. We also find that the measured aggregate TFP will slightly increase under a higher tax progressivity, reflecting the counteracting forces between the positive effects from the extensive margin versus the negative effects from resource misallocation along the intensive margin. Furthermore, when incumbent establishments of all age categories exhibit a relatively lower degree of returns-to-scale in production, we find that the tax-change outcomes are all qualitatively identical to those within our benchmark model.====Finally, the quantitative importance of progressive taxation is examined with a counterfactual cross-country comparison exercise. In particular, we first construct the respective employment-size distributions for China and India; and then derive the resulting key macroeconomic aggregates when these developing countries are subject to the same baseline degree of tax progressivity as observed in the U.S. economy. With the exception of labor hours, all the remaining variables are found to be relatively lower in China and India. To understand these results, we note that U.S. has more larger/productive establishments than China and India. It follows U.S. will display relatively smaller decreases in factor demands from more productive incumbents under progressive taxation. This in turn implies that as in our benchmark model, the negative effects of resource misallocation along the intensive margin, as well as the reductions in capital and labor services from the extensive margin, are quantitatively stronger in China and India because of their positively-skewed size distributions. As a result, these developing countries exhibit lower steady-state levels of aggregate output, aggregate consumption, aggregate capital and the numbers of entrants and incumbents. On the other hand, the additional labor-supply channel resulting from a negative wealth effect turns out to quantitatively dominate the combined strength of intensive and extensive margins in China and India, hence their aggregate labor hours will be relatively higher. This counterfactual cross-country comparison thus helps shed some light on the quantitative significance of progressive taxation within a developing economy.====This paper is closely related to Restuccia and Rogerson (2008) who also quantitatively investigate the effects of establishment-level distortions on aggregate production and productivity. Our analyses differ from theirs in four aspects. First, we allow the household’s labor supply decision to be endogenous, which turns out to yield substantial impact along the intensive as well as the extensive margins; and thus can exert significant qualitative and quantitative effects on the economy’s aggregate TFP when the tax policy becomes more progressive. Second, we incorporate firm life-cycle dynamics by gradually expanding the capacity constraint with an establishment’s years in operation, which in turn captures the empirical regularities that (i) age is related to firms’ size and (ii) young firms are smaller and grow faster than old firms. Third, we consider the establishment-level distortions with a progressive feature that is commonly observed in industrialized countries. Fourth, we take into account the dynamic changes of factor prices and their corresponding general equilibrium effects as the tax progressivity changes, whereas Restuccia and Rogerson’s study focuses on the case in which the relative prices remain unchanged.====The remainder of the paper is organized as follows. Section 2 describes our benchmark model in which production is carried out by heterogeneous establishments, and the household’s labor supply as well as establishments’ entry decisions are endogenously determined. We then derive the optimal conditions that characterize the economy’s stationary equilibrium prices and allocations. Section 3 quantitatively compares and contrasts the model’s steady states under alternative specifications. Section 4 concludes.",Resource misallocation and aggregate productivity under progressive taxation,https://www.sciencedirect.com/science/article/pii/S0164070418301630,8 February 2019,2019,Research Article,268.0
Tura-Gawron Karolina,"Gdansk University of Technology, Faculty of Management and Economics, Narutowicza 11/12, 80-233 Gdansk, Poland","Received 30 December 2017, Revised 4 February 2019, Accepted 5 February 2019, Available online 6 February 2019, Version of Record 27 November 2019.",https://doi.org/10.1016/j.jmacro.2019.02.001,Cited by (2),Modern ==== focuses on credibility and shaping ====. In keeping with the concept of ,"Almost every central bank that implements an inflation targeting regime publishes inflation forecasts. According to Svensson (1997), inflation forecasts play a crucial role in modern forward-looking monetary policy. Firstly, inflation forecasts shape consumers’ inflation expectations (CIE) and, in the long term, anchor them to the inflation target. These forecasts are the foundation of the transparency strategy of central banks and should increase their credibility (Szyszko, 2017). Inflation targeting may take the form of inflation forecast targeting (IFT). Svensson (1997) created the concept of IFT under which the inflation forecast plays the role of an intermediate target of monetary policy and becomes the basis for the instrument rate decisions made by central banks.====Studies of the inflation forecasts published by central banks and CIE can be divided into three types. Studies of the first type involve an analysis of the accuracy of the forecasts. The Bank of England (BoE) publishes density forecasts for each quarter (mean, standard deviation, and skewness). For external researchers, such transparency facilitates the conduct of studies. Inflation forecast errors were analysed by Wallis (2003) and Dowd (2007), although these studies were limited to inflation forecasts assuming a constant instrument rate (CIR) during the entire forecast horizon. Inflation forecasts assuming market expectations of future interest rates (ME) were examined by Knüppel and Schultefrankenfeld (2008). The accuracy of inflation forecasts based on the assumptions of CIR and ME and published by the BoE are yet to be compared. Inflation forecasts based on the assumption of CIR and published by Sveriges Riksbank (SR) and the BoE were also analysed by Dowd (2004), who demonstrated that the inflation forecasts by SR were more accurate than those published by the BoE.====In studies of the second type, inflation forecasts are analysed from the point of view of IFT. In this process, the direct impact of the inflation forecasts published by central banks on CIE is analysed using correlation coefficients and models that capture causality (e.g. structural VAR, VECM, etc.). This type of study has been performed by Szyszko (2017) for Czech Republic, Hungary, Poland, and Sweden, by Hubert, 2014, Hubert, 2015a, Hubert, 2015b) for Canada, Japan, Sweden, Switzerland, the United Kingdom, and the United States, and by Łyziak and Paloviita (2017a) for the Euro area. The studies confirm the influence of inflation forecasts on CIE. Studies analysing the effect of the communication of central banks (understood as a set of communication tools, of which the inflation forecast is one) on CIE were examined by Binder (2017) for the Federal Reserve, who found that anchoring the expectations of more informed consumers increased more than anchoring those of less informed consumers.====The third type of study assumes the analysis of CIE as an outcome of individual inflation perceptions. This approach has highlighted the cognitive sources of reactions to news in forming inflation expectations. It is assumed that expectations are highly heterogeneous and that consumers use different models, data sets, and sources to shape their expectations and have varying ability to process the information (Pfajfar, 2013). Expectations are also heterogeneous because of consumers’ socioeconomic and demographic factors (Pfajfar and Santoro, 2009), financial situation, and purchasing attitudes (Ehrmann et al., 2015). Studies of the formation of CIE have included revising the expectations and learning-to-forecast models in monetary policy (Anufriev and Hommes, 2012, Bernasconi and Kirchkamp, 2000, Hommes, 2011, Hommes et al., 2005, Marimon and Sunder, 1994). The most recent research was performed by Pfajfar and Žakelj (2016), who analysed the monetary policy rules (contemporaneous and forward-looking rules) in comparison to the formation of CIE.====In this study, we analyse the main features of the inflation forecasts published by central banks and their ability to shape CIE, measured as the absolute difference between the forecast and expectation. As consumers are non-specialists, we seek to identify the unique cognitive features of forecasts that have influenced the human brain and directed the expectations of consumers. These assumed features include the accuracy of previous forecasts as well as the similarity between consecutive forecasts and the absolute deviations of the forecast from the inflation target. The linear function of these attributes determines the credibility function of the inflation forecast. We propose an index that may be used to measure consumers’ attitudes towards the credibility of the inflation forecasts by central banks. According to linear belief function theory, the credibility function has a normal distribution, with the variance derived from the residuals of the estimated linear attribute function. In this study, three types of models are estimated: models with forecasts assuming CIR, models with forecasts assuming ME, and models with forecasts assuming an endogenous rate. In line with these models, we derive different credibility functions.====The aim of the study is thus to analyse (1) whether the inflation forecasts published by the central banks of the United Kingdom and Sweden are credible for consumers, and (2) whether their credibility changed after reaching the zero lower bound (ZLB) on the policy rate. We hypothesise that the inflation forecasts published by the central banks of the United Kingdom and Sweden were credible before and after reaching the ZLB on the policy rate.====The study examines the forecasts published by two central banks: SR and the BoE. The reference periods for the individual banks are 1999–2016 for SR and 1993–2016 for the BoE, and the starting point in each case corresponds to the publication of the first forecast. The choice of central banks was based on two main factors. First, SR is the only central bank that implements an inflation targeting regime and has published all three types of inflation forecasts associated with the three assumptions: CIR, ME, and an endogenous rate. Second, the BoE is the only central bank that implements an inflation targeting regime and publishes CIR-based forecasts in tandem with ME-based inflation forecasts. Moreover, the two central banks selected here officially declare the use of IFT procedures and have already published at least 50 forecasts.====The paper concludes in several directions; however, the main conclusion is that deviations of the forecast in the last year of the forecast horizon from the inflation target and the similarity between consecutive forecasts are important forecast attributes in shaping CIE. The deviations of the forecast from the inflation target affected the forecast's credibility in the same direction before and after reaching the ZLB on the policy rate, whereas the similarity to consecutive forecasts affected it in opposite ways. This study is, to the best of our knowledge, the first complex analysis of consumers’ attitudes towards the credibility of inflation forecasts.====The article consists of five sections: a description of the role of inflation forecasts under the IFT framework, methodology, the estimation and the empirical results of the credibility of the forecasts of the chosen central banks, and conclusions.",Consumers’ approach to the credibility of the inflation forecasts published by central banks: A new methodological solution,https://www.sciencedirect.com/science/article/pii/S0164070417305827,6 February 2019,2019,Research Article,269.0
"Di Bella Gabriel,Grigoli Francesco","International Monetary Fund, European Department, USA,International Monetary Fund, Research Department, USA","Received 30 August 2018, Revised 7 December 2018, Accepted 18 January 2019, Available online 29 January 2019, Version of Record 31 January 2019.",https://doi.org/10.1016/j.jmacro.2019.01.010,Cited by (11)," offers several explanations as to why shifting expectations about future economic activity affect current demand. Abstracting from whether changes in expectations originate from swings in beliefs or fundamentals, we test empirically whether optimism and pessimism about the economy trigger short-term fluctuations in private consumption and investment. Under the assumption that cyclical movements in private consumption and investment growth are exogenous to potential output growth forecasts far into the future, our results are consistent with the idea of private economic agents learning about future potential output growth and adjusting their current demand accordingly. We also propose a simple Keynesian model to illustrate that revisions in expected future income can affect short-term equilibria, in line with the results of the empirical analysis.","The notion that changes in expectations about future economic activity affect current aggregate demand has a long tradition in macroeconomics. The vast literature about the impact of confidence on economic activity includes: (i) the view arguing that “animal spirits” (and “sunspots”) eventually lead to busts as they are not supported by fundamentals (Cass, Shell, 1983, Akerlof, Shiller, 2010, De Grauwe, Ji, 2016);==== (ii) the view for which the same waves of optimism and pessimism associated with “animal spirits” do not necessarily lead to busts, rather they lead to self-fulfilling changes in fundamentals (Acharya, Benhabib, Huo, 2017, Benhabib, Liu, Wang, 2016), as changes in expectations of some agents trigger the actions of rational agents exhibiting strategic complementarities (Weil, 1989, Cooper, John, 1988); and (iii) a view generally referred to as “news-driven business cycles”, which posits that agents become optimistic or pessimistic based on the imperfect (or noisy) information they gather about future developments. If the information is correct the boom lasts, but if it is incorrect or agents are overly optimistic, a bust would occur (Cochrane, 1994, Beaudry, Portier, 2004, Beaudry, Portier, 2006, Lorenzoni, 2009, Jaimovich, Rebelo, 2009, Beaudry, Dupaigne, Portier, 2011, Schmitt-Grohé, Uribe, 2012, Blanchard, L’Huillier, Lorenzoni, 2013, Forni, Gambetti, Lippi, Sala, 2017).====Regardless of whether changes in expectations about future economic conditions are caused by swings in beliefs or fundamentals, there is a broad theoretical consensus that they do affect current business activity. This is because consumers react to changes in their permanent income by smoothing consumption inter-temporally in line with the permanent income hypothesis (Friedman, 1957, Hall, 1978). Current economic booms or busts could then occur without actual technology advances or regresses (Sims, 2009). In other words, they could happen either because revisions in long-term growth are predicated on fundamentals, or just because agents become more or less optimistic about the future. In this sense, Blanchard et al. (2017) argues that persistently low real GDP growth in the United States in the aftermath of the global financial crisis (GFC) can, at least in part, be attributed to systematic downgrades to future potential output growth, giving rise to a strong positive correlation between revisions in potential output growth forecasts and current consumption and investment.====In this paper, we empirically test whether private economic agents expecting higher (lower) future potential output growth decide to consume and invest more (less) today relying on data revisions. Differently from other papers in the literature focusing only on one country, we rely on a panel data set of actual data and forecasts for 89 countries over the 1990–2022 period. Also, we study dynamics and selected nonlinearities of expectation shocks.====This study belongs to a vast literature—largely focused on the United States—that employs a variety of approaches to identify expectation shocks and test their impact on current economic activity. Beaudry and Portier (2006), Beaudry and Lucke (2010), and Beaudry et al. (2011), for example, estimate reduced-form VARs for the United States using stock price innovations orthogonal to total factor productivity (TFP) as a measure of news shocks, and short- and long-run restrictions to allow for a long-run effect on TFP. These papers generally report a large and positive effect of news on economic activity. Matsusaka and Sbordone (1995) also estimate a reduced-form VAR, relying on consumer confidence data as a proxy of expectations and an indicator of economic activity for the United States. They find that expectations Granger-cause future output even after controlling for a set of macroeconomic variables.==== Chauvet and Guo (2003) estimate a similar VAR (with both consumer confidence data and a business sentiment indicator) allowing for different responses of economic agents depending on the stage of the business cycle. They find that shocks to expectations played a significant role during recession episodes in the United States. Barsky and Sims (2012) use a similar reduced-form VAR and focus on disentangling “news shocks” and “animal spirits shocks”. As the effects of these shocks are mostly permanent, they conclude that news shocks account for most of the variation in measured confidence.====The major drawback of the reduced-form VAR is that the estimated coefficients capture only the effect of past shocks on current output. In contrast, Choy et al. (2006) estimate a structural VAR and find that expectation shocks unrelated to fundamentals are not a significant source of output fluctuations. The identification scheme assumes no contemporaneous effects from expectations to macroeconomic variables, based on the fact that forecasts are published at the end of each quarter, which is after consumption and investment decisions are taken. This approach, however, is questionable as economic agents also access information about the economy that is not statistically observed or published. If this information is unbiased, it reflects current macroeconomic variables, regardless of whether the realizations have been announced. Also, given that surveys are conducted during the quarter, expectations can affect macroeconomic variables in the same period. Grisse (2009) estimates a structural VAR using data on business conditions for Germany. Differently from most of the papers in the literature, the identification scheme relies on the heteroskedasticity in the data.==== His results suggest that expectation shocks about business conditions have a causal effect on industrial production.====Our approach is closer in spirit to the literature that departs from VAR specifications. Oh, Waldman, 1990, Oh, Waldman, 2005 rely on revisions of leading economic indicators for the United States and find that announcements of a mistakenly booming economy have a positive effect on economic activity, supporting the self-fulfilling hypothesis. Our empirical strategy follows Blanchard et al. (2017), which use revisions to analyze the role of pessimism about future potential output growth in determining current consumption and investment in the United States. Given that potential output is a structural measure and therefore unlikely to be determined by current fluctuations, they argue that its revisions far into the future have a causal effect on current fluctuations in consumption and investment.==== They find that there is a strong positive correlation between revisions in potential output growth forecasts and current consumption and investment.====Our results for a large panel of countries are supportive of the notion for which changes in expectations about future income cause short-term fluctuations. Under the assumption that cyclical movements in private consumption growth and private investment growth are exogenous to potential output growth forecast far into the future, our results are consistent with the idea that private economic agents learn about future potential output growth and adjust consumption and investment accordingly over the two years following the shock in expectations. For the average country with a share of private consumption (investment) to GDP of 65 (16) percent, a 0.1 percentage point (pp) upward revision to potential output growth forecast would bring about an acceleration in GDP growth in the range of 0.09–0.14 (0.06–0.11) percent. We also find that despite changes in expectations became more frequent, negatively skewed, and volatile in the aftermath of the GFC, the estimated effects are not different with respect to the period preceding the GFC. Similarly, pessimistic and optimistic expectations do not present a differential impact on private consumption nor private investment, and large changes in expectations generally induce proportional changes in demand.====We complement the analysis with a theoretical model that illustrates the empirical results. In a Keynesian framework, we show that changes in expected long-term income have short-term effects in the direction of the change in expectations. Concretely, if agents expect long-term output growth to be high (low), short-term consumption, investment, and output in the short-term increase (fall), and unemployment falls (increases). In turn, increases (decreases) in investment today reinforce the expected increase (decrease) in long-term output, giving rise to a self-fulfilling mechanism. Broadly in line with the empirical analysis, we assume that agents rely on forecasters to form their views about the future. In turn, forecasters are prone to herding (==== Banerjee (1992)), so that changes in forecasts may either reflect a change in fundamentals or herding in the formation of expectations, in a process that could be loosely assimilated to “animal spirits”.====The rest of the paper is organized as follows. Section 2 outlines the empirical strategy. Section 3 examines the statistical properties of the expectations measure and its relationship with private consumption and investment. Section 4 discusses the econometric results, some extensions, and the robustness. Section 5 presents a theoretical model supporting the empirical results. Section 6 concludes.","Optimism, pessimism, and short-term fluctuations",https://www.sciencedirect.com/science/article/pii/S0164070418303823,29 January 2019,2019,Research Article,270.0
"Biljanovska Nina,Vardoulakis Alexandros P.","Research Department, International Monetary Fund, United States,Board of Governors of the Federal Reserve System, United States","Received 11 June 2018, Revised 17 January 2019, Accepted 23 January 2019, Available online 24 January 2019, Version of Record 5 February 2019.",https://doi.org/10.1016/j.jmacro.2019.01.009,Cited by (1),"We study optimal long-run capital taxation in a closed economy with heterogeneity in agents’ time-discount factors and collateralized borrowing. The collateral constraint distorts intertemporal optimization margins and therefore the ==== system serves to alleviate the distortion on top of financing ====. The discrepancy between the private and the social discount factors pushes for a capital subsidy, while the collateral constraint pushes for a capital ====. Consumption smoothing motives result in a strictly positive capital tax in the long run, while when they are muted, the two effects counter-balance and the tax is zero.","What should the tax on capital income be in the long run? Chamley (1986) and Judd (1985), working in somewhat different settings, found that in a dynamic Ramsey model with infinitely lived agents and no distortions in the economy, capital should not be taxed given that the economy converges to a steady state (Straub and Werning, 2014). The result is based upon the intuition that capital income taxation induces differentiated consumption taxes on present and future consumption. In other words, taxing capital income distorts individuals’ intertemporal consumption behavior as they substitute the more heavily taxed future consumption with current consumption (see also Chari, Christiano, Kehoe, 1994, Atkeson, Chari, Kehoe, 1999).====This paper recasts the optimal taxation problem in an economy with a collateral constraint. In particular, we examine whether the Chamley–Judd result of a zero tax on capital income in the long run survives in an economy where agents face collateral constraints akin to those present in Kiyotaki and Moore (1997) and Iacoviello (2005). We retain the environment in Judd (1985), which consists of two classes of agents, workers and capitalists, but we allow them to discount the future differently. Capitalists are relatively more impatient and want to borrow from patient workers. Moreover, we modify the bond market structure by having capitalists’ borrowing be limited by a collateral requirement. The consideration of a collateral constraint is important since one of the key assumptions in Chamley (1986) and Judd (1985) is the ability of private agents to freely shift consumption intertemporally, whereas the presence of a collateral constraint precludes it. Given the prevalence of secured lending worldwide==== and the plethora of macro-models featuring collateral constraints, it is of interest to understand how the presence of collateralized borrowing influences the key results on long-run capital taxation.====Our analysis identifies two forces that drive the decision of a Ramsey planner to levy distortionary taxes on capital income in the long run, which operate in opposite directions. On the one hand, the discrepancy in the discount rates between workers and capitalists, generating a binding collateral constraint,==== induces a discrepancy between social discounting and that of capitalists. Without assuming an exogenous discount rate for the planner, we derive that the planner discounts the future at the rate of the workers in the long run. This difference between the planner’s and capitalists’ discounting of future consumption pushes for a subsidy on capital income in the long run, which is a general result also applying to other economies (see De Bonis, Spataro, 2005, Reis, 2012).====On the other hand, the presence of binding collateral constraints pushes for a tax on capital income in the long run. In economies with collateral constraints of the class considered here, agents evaluate the investment in capital not only as an input in the production function, but also on the grounds that it relaxes the collateral constraint. Therefore, as argued by Fostel and Geanakoplos (2008) and Geanakoplos and Zame (2014), capital embeds a collateral premium, which pushes the marginal product of capital down. A Ramsey planner who is endowed only with linear taxation tools cannot undo the financial friction resulting in binding collateral constraints, but could use these tools to affect the distorted intertemporal margins.====The sign of the tax on capital income in the long run is determined by capitalists’ consumption smoothing motives. In the case of a linear utility, the discrepancies in discounting and the collateral distortion counter-balance each other and the tax on capital income is zero in the long run. Hence, the Chamley–Judd result survives in collateral constrained economies in this special case. Nevertheless, for finite elasticity of intertemporal substitution, the discrepancy in the shadow values of collateral dominates the discrepancy in discounting and the Ramsey planner levies a strictly positive tax on capital income in the long run. Moreover, the tax on capital income increases as the elasticity of intertemporal substitution decreases, i.e. consumption smoothing motives are lower. Hence, our positive capital tax result in the presence of collateral constraints is not obtained as a special case for certain parameters governing the motive of capitalists to smooth consumption intertemporally. For example, Lansing (1999) shows that the zero tax result in Judd (1985) can be invalidated, but only when capitalists have logarithmic utilities.====Finally, the Ramsey planner plays a dual role. The first goal is well-known to the Ramsey literature, which is to minimize distortions resulting from linear taxation used to finance exogenous government expenditure. The second goal is to address distortions in optimization margins. In particular, we show that a tax on labor income is fully used to finance the expenditures of the government and it is zero if lump-sum transfers are available. On the contrary, a tax on capital income serves to correct for the inefficiencies induced by the binding collateral constraint and is generally non-zero even if lump-sum transfers are allowed.====To check the robustness of our argument we consider three extensions / modifications to the framework described above. First, we show that the results do not depend on the assumption that capitalists borrow from workers. A positive tax on capital income obtains even if workers are hand-to-mouth as long as capitalists can borrow at low enough interest rate such that the collateral constraint binds. Second, we exogenously require the planner to discount the future at the rate of capitalists, instead of deriving the social discount factor endogenously and show that the tax of capital remains positive. Intuitively, from the discussion above, making the planner more impatient enforces the desire to tax capital to address the distortion from a binding collateral constraint. Third, we show that the planner’s allocations can also be decentralized with a tax on savings levied on lenders. This result also highlights the key role of binding collateral constraints as both a tax on capital and a tax on savings discourages investment; the former directly by reducing its payoff, while the latter indirectly by making the cost of funding investment higher. In all these extensions, the result of positive capital taxation carries through because the distortion from the binding collateral constraint is still present.====The paper is organized as follows. Section 2 describes the model environment. Section 3 derives the Ramsey problem, computes the optimal tax policy and discusses the role of taxes in the economy. Section 4 discusses the three extensions to the baseline model. Finally, Section 5 concludes. All proofs are relegated to the Online Appendix.====. Park (2014) studies an optimal Ramsey taxation problem in an environment where agents face a limited commitment problem as in Alvarez and Jermann (2000). She shows that the Ramsey government faces two conflicting objectives: first, to finance government expenditure; second, to internalize the externality of labor and capital to improve risk sharing. Thus, she argues that the Ramsey planner has a dual role. The steady state tax on capital income is levied to correct for the pecuniary externalities induced by the binding borrowing limits; whereas the tax on labor income is used to finance the budgetary needs of the government. Our paper differs because of the nature of the collateral constraint as well as the absence of idiosyncratic uncertainty. We also argue that pecuniary externalities are not the sole reason to levy distortionary capital taxation.====In general, our conclusions differ sharply from models with idiosyncratic risk, which cannot be insured because asset markets are incomplete (see Aiyagari, 1995) and subsequent literature). Under uninsurable idiosyncratic uncertainty, agents have a precautionary motive to over-accumulate capital and, thus, the return on capital is suppressed. The planner levies a positive capital tax to bring the return on capital up to its efficient level. These models feature natural borrowing limits to guarantee that agents will be able to honor their debt obligations even after bad realizations. Such constraints are quite different from the collateral constraint we consider, which arises from an agency problem and binds deterministically. Indeed, uninsurable idiosyncratic risk rather than borrowing limits, is the main driver of the positive capital tax in those models as shown in Aiyagari (1995); if idiosyncratic uncertainty could be perfectly hedged with a complete set of contracts, the tax on capital would be zero. In our paper, there is neither idiosyncratic nor any other source of uncertainty, yet the collateral constraint arising from agency frictions results in binding collateral constraints and a positive tax on capital income in the long run.====Another paper that is related to ours is Reis (2012). She finds that the tax on capital income is positive at the deterministic steady state as long as the benevolent government is more impatient than the private agents, accumulates debt and is not able to commit to future policies. She finds that these conditions need to hold for a positive tax on capital income to emerge in the long run. In our paper, contrary to hers, there are three agents: workers, capitalists and a benevolent government, which does not face a commitment problem. The positive capital taxation in the long run is due to the collateral premium and the difference in discount factors, rather than the government’s impatience and its inability to commit. Aguiar and Amador (2016) model a small open economy with impatient agents compared to the rest of the world and a government with limited commitment. They show that labor income taxes can go to zero in the long run, while capital income taxes may not be zero. Our paper is different because we model a closed economy with heterogeneous agents where the role of capital as collateral distorts intertemporal margins and calls for positive capital taxation.====Biljanovska (2017) studies a variety of corrective (Pigouvian) policy tools in the presence of collateral constraints and their ability to replicate first- and second-best allocations. Itskhoki and Moll (2015) also study optimal dynamic Ramsey policies when borrowing is constrained, but tax proceeds are rebated back to the same agents and, thus, policy is Pigouvian. Bianchi (2016) studies optimal interventions—implemented by linear taxation and debt bailouts—to improve welfare when firms face constraints for both debt and equity financing. Our analysis differs, importantly, because we do not allow for corrective Pigouvian taxation or other transfers, but rather study distortionary taxation used to fund government expenditure as is the norm in the Ramsey literature. Positive capital taxation would arguably be more difficult to obtain in our framework, since it would remove resources for resource-constrained capitalists.====Our analysis also differs from the literature on pecuniary externalities with occasionally binding collateral constraints (see Bianchi, Mendoza, 2018, Jeanne, Korinek). In that literature, the stock of the real asset that can be used as collateral is fixed, whereas its price fluctuates with shocks. Borrowers do not internalize the impact on their actions on the price of collateral and pecuniary externalities arise when the collateral constraint becomes binding. Instead, in our model the stock of capital is not fixed, but capital is accumulated and depreciated over time. Resources can be consumed or invested in capital at the same rate, hence there are no relative price considerations as is typical in the neoclassical growth models. Our focus is also different. We study whether capital should be taxed to fund government expenditure when collateral constraints bind in the long run, rather than the efficiency implications of short-run price fluctuations in response to shocks.====Our paper is more broadly related to the literature studying optimal capital income taxation. Chamley (1986) and Judd (1985) established the result of zero capital taxation in the long run, which rests critically on the possibility of shifting consumption across periods through perfect capital markets (see for e.g. Chamley, 2001). Albanesi and Armenter (2012) show that, for a general class of economies, the presence of permanent intertemporal distortions is linked to the ability to front-load them. Although they do not discuss the collateral constrained economies we consider herein, we explain later how this property fails. More recently, Chari et al. (2016) revisit the Chamley–Judd result in an environment that allows for a richer tax system (i.e. without caps on linear taxation, unlike Straub and Werning (2014), who impose limits on taxation), and find that the tax on capital income is still zero in the long run. However, the environment they consider does not involve any (financial) market imperfections. In our setup, a collateral constraint yields intertemporal wedges that, as we will see, call for a positive tax on capital income in the long run. Finally, a positive tax on capital income has also been found in life-cycle model frameworks (Erosa, Gervais, 2002, Conesa, Kitao, Krueger, 2009).",Capital taxation with heterogeneous discounting and collateralized borrowing,https://www.sciencedirect.com/science/article/pii/S0164070418302647,24 January 2019,2019,Research Article,271.0
"Reed Robert R.,Ume Ejindu S.","Department of Economics, Finance, and Legal Studies, University of Alabama, Tuscaloosa, AL 35487, USA,Department of Economics, Miami University, Oxford, OH 45056, USA","Received 26 June 2018, Revised 21 December 2018, Accepted 9 January 2019, Available online 18 January 2019, Version of Record 18 February 2019.",https://doi.org/10.1016/j.jmacro.2019.01.003,Cited by (5)," shocks which impede housing accumulation. Intermediaries arise to help insure individuals against such idiosyncratic risk. However, ==== also limits the extent of risk-sharing. In this manner, the model follows recent contributions which emphasize that shocks to discount rates contribute to wealth inequality. Moreover, in contrast to one-sector monetary growth models, we demonstrate that it is important to disaggregate fixed investment between the residential and non-residential sectors to determine the effects of money growth. In particular, monetary policy will have asymmetric effects across the components of the overall capital stock.","There is increasing recognition that the effects of monetary policy are not symmetric across different sectors in the economy. For example, using disaggregate data on the components of consumer expenditures in the United States, Boivin et al. (2009) and Baumeister et al. (2013) document that there is significant variation in the impact of monetary policy on different types of consumption goods. By comparison, Barsky et al. (2003) present evidence that the production of durable goods responds more to changes in monetary policy than non-durable goods. In addition, Ghent (2012) points out that monetary policy has a larger impact on residential investment than other components of GDP. Further, in a recent contribution, Givens and Reed (2018) focus exclusively on the capital goods sector. Notably, they find that disaggregated capital goods respond differently to changes in monetary policy – their results indicate that the impact of monetary policy on residential investment is more than twice as high as the behavior of fixed investment at the aggregate level.====Given these significant observations, it is critical that economists develop a thorough understanding of the diversity in capital sector activity in order to have adequate knowledge of the monetary transmission mechanism. On the one hand, the impact of changes in monetary policy might be expected to have the strongest impact on the components of the capital goods sector where demand would be the most responsive to changes in interest rates. However, it may also be due to the impact of policy on the returns to different types of capital in response to inflation over time. In particular, both Summers (1981) and Piazzesi and Schneider (2012) suggest that housing investment becomes more attractive relative to corporate capital during inflationary periods. Similarly, Fama and Schwert (1977) stress that housing investment is an effective way of avoiding the effects of inflation on real returns.====The objective of this paper is to develop a general equilibrium framework to study how the effects of monetary policy vary across the components of the capital goods sector. As expected, the model disaggregates between residential fixed investment and nonresidential investment. In particular, the different degree of durability across the residential and non-residential categories is a key component of our modeling framework – that is, the depreciation rate of non-residential investment is obviously higher than the housing stock. In addition, individuals value the utility of home ownership. Moreover, there are separate production functions for each sector with different inputs.====It is also critically important that a model takes into account that monetary policy affects activity by impacting the incentives of financial intermediaries in the banking system. In fact, commercial banks play an important role in promoting housing markets – based upon evidence from the Financial Accounts of the United States (Flow of Funds) from 2007, mortgages accounted for over 40% of total financial assets among U.S. Chartered Depository Institutions. Yet, by design, the model does not include a mortgage market – all housing transactions are self-financed – this is because our explanation of the monetary transmission mechanism articulates a role for policy to affect housing market activity other than standard explanations based on the availability of mortgage funding which affect borrowing behavior.====Instead, as emphasized by Li and Yao (2007), our framework posits that housing is both a consumption good and an investment good which is a significant component of personal wealth. Notably, according to the Survey of Consumer Finances, U.S. primary residences made up approximately 30% of total household wealth in 2010. In particular, saving through homeownership provides individuals with a mechanism for modifying consumption behavior later in life. On top of this, a wide array of evidence demonstrates that housing wealth promotes consumption regardless of the age of the homeowner. For example, Carroll (2004) finds the long-run marginal propensity to consume out of housing wealth to be around 9 cents per dollar. In this manner, the model aims to show how monetary policy affects activity across the different components of the capital goods sector through the desire to accumulate housing wealth and the effects of inflation resulting from money growth rather than the extension of mortgage financing. We also include the utility value of housing as motivation for the desire to purchase a home.====Despite the absence of a mortgage market in our modeling framework, we view that financial intermediaries play an important role in housing market activity. To do so, as argued by Smith (2003), “ intermediation should be taken seriously.” In particular, we view that intermediaries play an important part in housing through a rigorously structured “ liquidity risk” channel. Notably, in our framework, though individuals save in advance so that they can gain access to housing, they are also exposed to liquidity risk. That is, individuals face the possibility that an adverse shock interrupts their savings plan and forces them to liquidate their holdings. However, as initially articulated by Diamond and Dybvig (1983), intermediaries perform important risk-pooling functions. Further, as shown by Bencivenga and Smith (1991), these risk pooling services promote investment as individuals do not need to hold excessive amounts of money balances to self-insure against stochastic liquidity preference shocks. Interestingly, our framework extends their work by including separate categories of capital goods: residential fixed investment and non-residential investment. Hence, intermediaries promote housing wealth by pooling deposits (and idiosyncratic risks) to acquire a portfolio of assets that avoids large holdings of unproductive money balances and offers higher expected returns than individuals would be able to obtain on their own. With higher returns from their savings, individuals have more income to acquire housing. As a result, individuals are also wealthier over time because of the risk-pooling services provided by financial intermediaries.====Moreover, as previously mentioned, the housing stock is more durable than the non-residential stock of capital — this has important implications for the asymmetric effects of monetary policy across the capital sector in our model. To begin, money growth expands the deposit base. In turn, the expansion of the deposit base promotes activity across the entire capital sector. However, it favors the asset with the highest degree of durability: housing. Consequently, the model provides important insights into the asymmetric effects of monetary policy across the capital goods sector as discussed by Givens and Reed. The disparity in the effects of money growth also depends on the utility value of housing. Further, it is increasing in the rate of time preference – housing is an important form of wealth accumulation and promotes intertemporal consumption later in life.====Finally, we offer a few brief additional details of our modeling setup. As in Bencivenga and Smith (1991), the model uses an overlapping generations structure. However, in contrast to their approach, there are multiple production sectors: a standard (non-residential) capital goods sector as in the neoclassical growth model and a residential sector which produces new housing.==== Money is the only asset available to meet short-term liquidity needs.====The overlapping generations structure is important for our framework as we want to incorporate that housing is a durable, yet reproducible asset that is traded over time – across different generations of individuals. In the first period, young individuals work and save. In the second period, middle-aged individuals who are not exposed to liquidity shocks purchase homes and consume. In the final period, old homeowners finance their consumption based upon proceeds from the sale of their homes.====The remainder of the paper is as follows: Section 2 describes the environment of the benchmark economy in which monetary policy is super-neutral and does not affect residential investment. In doing so, it helps explain how housing market activity depends upon demand-side factors such as the utility value of home ownership and the rate of time preference. However, our general equilibrium framework also emphasizes how supply-side factors in the residential sector affect housing markets. In Section 3, we modify the benchmark model so that money growth expands the deposit base in the banking system and stimulates investment. However, it favors the asset with the highest degree of durability – housing. Section 4 offers concluding remarks.====As discussed by Ghent, most monetary models omit a separate role for housing. Further, much of the existing theoretical macroeconomic literature on housing looks at how housing demand varies across the lifecycle without a detailed analysis of the effects of monetary policy. For example, models such as Li and Yao (2007) and Iacoveillo and Pavan (2013) tend to be partial equilibrium frameworks. Although they show how housing prices and mortgage rates affect housing consumption, they do not speak to how demand and supply conditions in the housing sector affect housing market activity as they do not determine housing prices endogenously. Consequently, they do not say anything about the effects of monetary policy on residential investment or how the effects of policy depend on housing fundamentals.==== This is an important omission that we seek to address.====Yet, as emphasized by Li and Yao (2007), we develop a life cycle model in which housing has a dual role as both a consumption good and an investment asset. In comparison to their work, we capture the role of the life cycle through an overlapping generations structure. Further, in contrast to the existing literature, we develop a structure in which individuals are exposed to idiosyncratic liquidity risk which can interrupt wealth accumulation. In this manner, the model follows recent contributions such as Krusell and Smith (1998) which emphasize that shocks to discount rates contribute to wealth inequality. Because of this uncertainty over the life cycle, financial intermediaries arise endogenously to promote risk-sharing between depositors. In this manner, by developing rich microeconomic foundations for intermediaries, we present new insights into the role of financial intermediation for housing market activity and wealth accumulation above how they channel funds to the mortgage market.====The closest papers to our work are Barsky et al. (2007) and Ghent (2012) in that all three papers look at the impact of monetary policy on different sectors of activity and show that policy has a stronger impact on residential investment than production of non-durables. In contrast to our approach, Ghent argues that monetary policy has the largest impact on housing because the sector depends on the availability of mortgage funding. Moreover, Ghent looks at an infinite horizon model and assumes that the production function for nondurables and housing is the same.====As in our setup, Barsky et al. do not impose that individuals must borrow to acquire consumer durables. Instead, we both stress that policy favors the good with the lowest depreciation rate. However, they look at an infinite horizon model which requires that the prices of durables are sticky. By comparison, we adopt a life cycle approach which is often used in macroeconomic models with housing. Furthermore, there are no price rigidities in our framework. Simply put, the returns from housing in our framework are dynamic in that they include the value of housing as a consumption good and as an investment good where individuals use the proceeds from the sale of their home to finance consumption late in life. Notably, the transfer of the housing stock over time occurs across generations of individuals. Thus, the reason why policy favors housing is not because monetary policy can promote access to mortgage financing but instead it is because it is the most durable component of the component of the capital sector.","Housing, liquidity risk, and monetary policy",https://www.sciencedirect.com/science/article/pii/S0164070418302866,18 January 2019,2019,Research Article,272.0
"Dia Enzo,Bartolomeo Giovanni Di","Dipartimento di Economia, Metodi Quantitivi e Strategie di Impresa, Università degli Studi di Milano Bicocca, Piazza AteneoNuovo 1, 20126 Milano, Italy,Dipartimento di Economia e Diritto, Università di Roma La Sapienza, Via del Castro Laurenziano, 9, 00161, Roma, Italy","Received 7 January 2019, Accepted 12 January 2019, Available online 18 January 2019, Version of Record 24 January 2019.",https://doi.org/10.1016/j.jmacro.2019.01.006,Cited by (0),", heavily criticized after the financial crisis, to provide the background for the contributions presented in this issue. We also briefly review the articles presented in this special issue.",None,"Macroeconomics, rationality, and institutions",https://www.sciencedirect.com/science/article/pii/S0164070419300072,18 January 2019,2019,Research Article,273.0
Morikawa Masayuki,"Research Institute of Economy, Trade and Industry (RIETI), 1-3-1 Kasumigaseki, Chiyoda-ku, Tokyo 100-8901, Japan","Received 27 February 2018, Revised 8 January 2019, Accepted 15 January 2019, Available online 18 January 2019, Version of Record 21 February 2019.",https://doi.org/10.1016/j.jmacro.2019.01.007,Cited by (11),"This study, using monthly micro data on firms’ forecasted and realized production quantities, presents new evidence of the uncertainty of production forecasts. Forecast errors are heterogeneous among individual manufacturers. Firms operating in the information and communications technology-related ","Uncertainty and its impacts on economic activities attract attention from policy practitioners and economic researchers. Uncertainty, which arises from financial crises, unexpected policy developments in major countries following changes of political power, and natural disasters, among other factors, negatively affects firm behavior over the course of the economy, particularly impacting on long-term investments including innovation and recruitment (see Carruth et al. (2000) and Bloom (2014) for surveys).====The negative impact of economic uncertainty on investment has often been theoretically explained by the option value of waiting. Due to the irreversibility or the adjustment costs of investment, uncertainty has a negative effect on investment, since firms may avoid taking action and prefer to “wait and see” under uncertain circumstances (Bernanke, 1983, McDonald and Siegel, 1986, Bloom et al., 2007). Studies concerning precautionary saving motives have shown similar implications for the impact of uncertainty, although their main focus were on consumption and saving decisions, particularly pertaining to the purchase of durable goods.====Empirical studies generally support the theoretical prediction that uncertainty has a negative effect on equipment investment (Leahy and Whited, 1996, Guiso and Parigi, 1999, Ghosal and Loungani, 2000, Bloom et al., 2007, Bontempi et al., 2010), research and development investment (Bloom, 2007, Caggese, 2012, Bontempi, 2016), and employment (Ono and Sullivan, 2013, Ghosal and Ye, 2015).====Several recent studies emphasize the role of financial friction as an important transmission channel of uncertainty to the real economy (Gilchrist et al., 2014, Caldara et al., 2016, Alfaro et al., 2018). Additionally, certain studies focused on the role of the zero-lower bound (ZLB) problem as an amplification mechanism of uncertainty shocks, as it constrains the conduct of effective monetary policy (Basu and Bundick, 2017, Plante et al., 2018). A recent empirical study investigated whether financial uncertainty is an exogenous source of business cycles or an endogenous response to economic fundamentals. It argues that uncertainty concerning real economic activity in recessions is an endogenous response to uncertainty in a financial market (Ludvigson et al., 2018). Although the roles of financial friction and financial market uncertainty are beyond the scope of this study, these recent studies have shed light on the mechanisms of uncertainty and the real economy and the relationship between them.====Since uncertainty is subjective in nature and not directly observable from statistical data, various proxy variables have been proposed to capture the uncertainty faced by economic agents.==== Representative uncertainty measures include the (1) volatility of stock prices (Bloom et al., 2007, Bloom, 2009), (2) cross-sectional disagreement of forecasts by professional economists (Driver and Moreton, 1991, Bomberger, 1996, Giordani and Söderlind, 2003, Dovern et al., 2012), (3) unexplained portion of macroeconomic variables derived from econometric models (Jurado et al., 2015), (4) ex post forecast errors in firms’ business outlook (Bachmann et al., 2013, Arslan et al., 2015, Morikawa, 2016a), (5) survey-based firms’ subjective uncertainty (Guiso and Parigi, 1999, Bontempi, 2016, Morikawa, 2016b, Coibion et al., 2018), and (6) frequency of newspaper articles on policy uncertainty (Baker et al., 2015, Arbatli et al., 2017).====The measure of uncertainty adopted in this study is the ex post errors in the production forecasts of manufacturing firms. Although firms’ forecast errors have been used as proxies for uncertainty in the literature, empirical studies have generally depended on the qualitative outlook of business conditions (e.g., improving, unchanging, or deteriorating) available from business surveys (Bachmann et al., 2013, Arslan et al., 2015, Morikawa, 2016a). By contrast, this study uses quantitative data on ex ante production forecasts and ex post realized production at the firm-product-level taken from a monthly survey of Japanese manufacturers conducted by the Ministry of Economy, Trade and Industry (METI), namely the Survey of Production Forecast (SPF).====A small number of studies analyze quantitative forecast errors at the firm-level. For example, Bachmann and Elstner (2015), using quarterly survey data on manufacturing firms in Germany (i.e., the IFO Business Climate Survey), quantify and analyze production errors. However, since production quantities are not directly available from the survey data, they construct quantitative expectation errors for firms’ production growth from the expectation of capacity utilization rates based on several assumptions, such as production capacity remaining constant. Bachmann et al. (2017) present a quantitative analysis of a firm-level investment expectation error termed investment surprise for a 40-year panel of German manufacturing firms (i.e., the IFO Investment Survey). Although the availability of a long panel is an advantage, the investment data used in their study have only an annual frequency.====By contrast, the SPF captures the cyclical movements of Japanese manufacturers’ production on a monthly basis. The survey specifically asks firms for their production forecasts for the next month, estimated production for the current month, and realized production for the previous month. Because no analyses of forecast errors using monthly frequency quantitative firm-product-level production have thus far been carried out, this study contributes to the literature on uncertainty in two main ways.==== First, when only data on qualitative forecasts and realizations are available, unexpected improvements (or deteriorations) in business conditions are treated equally. However, in practice, the economic impacts of forecast errors of 5% and 50%, for example, are very different. Second, adopting firm-product-level micro data enables us to analyze not only the time-series properties of uncertainty but also its cross-sectional heterogeneity by industry or product type. While production uncertainty is naturally heterogeneous and rests heavily on the characteristics of the industries or products in question, such an analysis has been hampered by data limitations. By using these novel data, we present important findings about production uncertainty at the firm-product-level.====The remainder of this paper is structured as follows. Section 2 explains the data used in this study, the procedure for calculating the forecast errors and uncertainty measures, and the method of analysis. Section 3 reports the results, including descriptive observations on the time-series movements of forecast uncertainty; differences in uncertainty by industry, product type, and firm size; the relationship between forecast uncertainty and production volatility; the cyclical characteristics of forecast uncertainty. Section 4 presents the relationship between uncertainty measures and real economic activity by vector autoregression (VAR) estimations. Section 5 concludes, presenting the policy implications, limitations of the study, and issues to be addressed in future work.",Uncertainty over production forecasts: An empirical analysis using monthly quantitative survey data,https://www.sciencedirect.com/science/article/pii/S0164070418300855,18 January 2019,2019,Research Article,274.0
"Ciccarone Giuseppe,Giuli Francesco,Marchetti Enrico","Department of Economics and Law, Sapienza University of Rome, Via del Castro Laurenziano 9, Rome 00161, Italy,Department of Economics, Roma Tre University, Via S. D’Amico 77, Rome 00145, Italy,Department of Economic and Legal Studies, University of Naples Partehnope, Via G. Parisi 13, Naples 00134, Italy","Received 1 December 2016, Revised 14 January 2019, Accepted 15 January 2019, Available online 16 January 2019, Version of Record 24 January 2019.",https://doi.org/10.1016/j.jmacro.2019.01.008,Cited by (1)," and ==== rigidities due to wage bargaining, or ====. We show that an increase in workers’ loss aversion: (i) reduces the equilibrium wage and in this way increases potential output; (ii) induces workers to work/ consume less and in this way decreases potential output. Sharper loss aversion may hence increase or decrease potential output according to the relative strength of these two effects. We also show that if loss aversion reduces equilibrium output, it also enhances the effect of nominal price rigidities.","In the last two decades, Tversky and Kahneman’s (1992) Prospect Theory (PT) has been increasingly accepted as an effective and realistic description of economic behavior in risky situations (Barberis, 2013) and it has been extensively employed to tackle several microeconomic problems, usually cast in a partial equilibrium context, or even in single choice settings. In spite of the decades-long debate on the microfoundations of macroeconomics and of Akerlof’s (2002) early intuitions, the gap between PT and macroeconomic general equilibrium models remains instead rather wide, even though some attempts have recently started to populate the literature.====The common characteristic of these macroeconomic attempts is to focus on only one choice dimension in which PT may play a role, be it consumption, wealth allocation, or search behavior in the labor market. While this was reasonable and useful in an initial modelling stage, it is at odds with Barberis and Huang’s (2009) notion of imperfect rationality based on the concept of framing. According to this view, the same PT elements should in fact separately characterize all the choices which are made by an individual under risky circumstances. This observation motivates our aim to start overcome the limitation of existing models by constructing a simple economy in which PT elements are included into different choice dimensions and by investigating the general equilibrium consequences of this more pervasive influence of behavioral elements on agents’ decision processes.====More in particular, we model a standard general equilibrium overlapping generations economy with monopolistic competition in the goods market (Blanchard and Kiyotaki, 1987) and real wage rigidities in the labor market due to wage bargaining, or to efficiency wages. We separately include PT behavioral elements into the consumption/wealth choice and into the wage/labor input determination mechanism. On the one side, the representative household is averse to possible losses in his/her wealth; on the other side, when acting as a representative worker, she/he is averse to (indirect) utility losses with respect to a reference level. Our modelling strategy conforms to the PEEM (portable extensions of existing models) approach proposed by Rabin (2013), which is based on the modification of an existing model by means of different psychological assumptions to be represented in terms of parameter values. The model is made portable by using the same independent variables employed in the field of research the modification tries to extend.====Our main result is that an increase in the agent’s loss aversion parameter produces two distinct and opposite effects: (i) a lower wage effect increasing equilibrium output; (ii) a saving restraint effect decreasing equilibrium output. If the former effect is greater (smaller) than the latter one, then higher loss aversion produces an increase (decrease) in equilibrium employment and in potential output. The opposite occurs if the loss aversion parameter falls. When general equilibrium relationships are taken into account, macroeconomic interdependence may hence increase the economic complexity produced by PT elements, correctly interpreted as forms of imperfect rationality pervasively characterizing the agents’ choice procedures. We also we show that, in the right-to-manage version of the model, there exist values of the loss aversion parameters that generate a value of the bargaining power able to eliminate the inefficiencies produced by imperfect rationality.====Our analysis also provides new insights on another important issue raised by behavioral macroeconomics: the relationship between imperfect rationality and the presence of nominal rigidities and money non-neutrality (Akerlof, 2002). It is well known that one possible source of PT-related nominal rigidities is money illusion in wage bargaining.==== When workers focus on gains or losses with respect to a reference nominal wage level, the greater salience of losses can easily justify the strong aversion to wage cuts, i.e., downward nominal rigidity. We can then ask whether the “distortions” described by PT can create room for money non-neutrality even when agents do not suffer from money illusion. If this issue is to be properly tackled within the framework proposed by Akerlof and Yellen (1985) and Mankiw (1985) - which is based on near rationality and small costs of nominal adjustment - the evaluation of all the macroeconomic interaction effects of behavioral elements is necessary, as the same interdependence described above is also crucial to determine the incentives for price setters to change, or not to change, nominal good prices in response to a nominal demand shock. By taking this interdependence into account, we are able to show that the final effect of loss aversion on equilibrium output also rules the amount of the “small” menu costs borne by a price setter when she/he decides not to change the nominal price. In particular, when a greater value of the loss aversion parameter induces a reduction in equilibrium output, then, ====, the private cost of not changing prices in the face of a nominal shock falls. In this case, an increase in loss aversion enhances the effect of nominal price rigidities.====Finally, in the spirit of Lucas (2000), we carry out an illustrative analysis of the welfare losses produced by agents’ imperfect rationality in the right-to-manage version of the model (but the same analysis could be carried out also in the efficiency wage case), focusing on the possible impact of nominal uncertainty on the stationary equilibrium values of the economy,==== and discussing the offsprings of this important feature of our model economy.====The paper is structured as follows. In the next section we describe the model economy and in Section 3 we characterize different types of wage rigidity. The two main effects produced by imperfect rationality on equilibrium output and employment are then discussed in Section 4. Section 5 addresses the issue of nominal price rigidity and shows the conditions under which imperfect rationality can favor this type of firms’ behavior. Section 6 provides a synthetic analysis of the welfare costs related to nominal uncertainty/instability in the presence of the PT’s framing effect. Section 7 concludes.",Macroeconomic equilibrium and nominal price rigidities under imperfect rationality,https://www.sciencedirect.com/science/article/pii/S0164070416302063,16 January 2019,2019,Research Article,275.0
"Khan Hashmat,Metaxoglou Konstantinos,Knittel Christopher R.,Papineau Maya","Department of Economics, Carleton University, B 843 Loeb Building, 1125 Colonel By Drive, Ottawa, K1S 5B6, Canada,Sloan School of Management, MIT, and NBER, United States,Department of Economics, Carleton University, Canada","Received 17 May 2018, Revised 8 January 2019, Accepted 10 January 2019, Available online 15 January 2019, Version of Record 18 January 2019.",https://doi.org/10.1016/j.jmacro.2019.01.005,Cited by (37)," shocks account for less than 1 percent. Importantly, close to two thirds of the variation in emissions appears to be due to a structural shock not yet identified in the literature.","It is well-documented that U.S. carbon dioxide (CO====) emissions are strongly procyclical (Heutel (2012) and Doda (2014)).==== The contemporaneous correlation between the cyclical components of real GDP and emissions using quarterly data for the last 40 years is 0.64 and statistically significant (Fig. 1). This relationship suggests that macroeconomic shocks inducing cyclical fluctuations in output should also account for the cyclical behavior of emissions. In this paper, we draw on the empirical macroeconomics literature that has identified a variety of such shocks as potential sources of business cycles to provide a quantitative assessment of the emissions-GDP comovement.====Our paper is motivated by—and informs—the recent literature on optimal environmental policy under uncertainty due to fluctuations in economic activity, which has received increased attention in the aftermath of the financial crisis of 2008. As emphasized in Bowen and Stern (2010), since business cycles are difficult to predict, a better understanding of their sources can help improve environmental policy readiness and implementation during expansions and recessions. In this context, a growing literature uses calibrated dynamic stochastic general equilibrium models to prescribe optimal environmental policy over the business cycle (hereafter, E-DSGE models) (Fischer and Springborn (2011); Heutel (2012)).====Using quarterly U.S. data for 1973–2016, we study the response of emissions to four prominent technology shocks, also viewed as “supply shocks” to GDP. We start with the unanticipated and anticipated neutral technology (NT) shocks as in Galí (1999) and Barsky and Sims (2011). We also consider the unanticipated and anticipated investment-specific technology (IST) shocks as in Fisher (2006) and Ben Zeev and Khan (2015).====To the best of our knowledge, we are the first to document the empirical response of emissions to the primary macroeconomic shocks whose role in driving U.S. aggregate output fluctuations has been extensively documented in the literature. Importantly, we provide a comparison between the estimated impulse responses with their theoretical counterparts using an E-DSGE model that features all four technology shocks. Additionally, we assess the importance of each macroeconomic shock in accounting for the forecast error variation (FEV) in emissions, which can guide researchers developing E-DSGE models towards shocks that are important contributors to the variation in emissions.====The contribution of our paper is not limited to the E-DSGE literature. If a particular structural macroeconomic shock identified in the U.S. data is viewed as an important source of the business cycle then it should also (i) generate a positive comovement between output and emissions, consistent with the large positive correlation observed in the data, and (ii) account for a considerable share of the cyclical variation in emissions. Hence, the estimated response of emissions to a shock and the FEV of emissions offer a novel check on that shock’s importance for the fluctuations in the U.S. economy. Our empirical analysis provides such a check.====We begin by presenting an E-DSGE model that augments (Heutel, 2012) along two dimensions. First, in addition to the unanticipated NT shock, we include the unanticipated IST and the two anticipated technology shocks that have not been studied in the E-DSGE literature to date. Second, we include capital utilization and investment adjustment costs necessary for understanding the propagation of anticipated shocks as stressed in Jaimovich and Rebelo (2009). The model explains how the different technology shocks affect output and emissions. It also delivers theoretical impulse response functions that we can compare with their estimated empirical counterparts. As a preview of our findings, the technology shocks produce hump-shaped responses of output and emissions indicating a strong positive correlation between these two variables.====We then proceed to the empirical identification of the technology shocks and determine their effects on emissions. We start with a simple and informative analysis based on a bivariate Vector Autoregression (VAR) of GDP and emissions. This exercise establishes how emissions respond to a shock to GDP without imposing any structural restrictions, which we term the “reduced-form” GDP shock. Emissions increase in a hump-shaped manner after a positive shock to GDP; the response is highly statistically significant. This strong comovement of output and emissions after the reduced-form GDP shock reflects the unconditional large positive correlation between emissions and output over the business cycle.====Next, we identify the technology shocks using structural VAR (SVAR) specifications and identification restrictions from the empirical macroeconomics literature, and examine the effects of each identified shock on emissions. Following well-established methodologies, we identify the unanticipated NT and IST shocks using long-run restrictions, and the anticipated NT and IST shocks using medium-run restrictions.====Our estimated impulse responses show that emissions increase on impact after a positive one-standard deviation unanticipated NT shock in a persistent manner, as does output. This positive comovement between output and emissions is also confirmed using conditional correlations. However, the response of emissions is not statistically significant. The FEV share of the unanticipated NT shock in the case of emissions is quite small and ranges between 4 and 7 percent over the 5-year horizon we consider.====Emissions increase on impact after a positive unanticipated IST shock and reach a peak response after 4 quarters. Subsequently, the response declines gradually but remains positive. The response is also not statistically significant. The shock generates a positive comovement between output and emissions, and accounts for about 10 percent of the emissions’ FEV. Hence, the unanticipated IST shock is somewhat more important than the unanticipated NT shock in explaining the variation in emissions.====Emissions fall on impact after an anticipated NT shock, and then gradually increase in a hump-shaped manner, turning positive by the third quarter. The response remains statistically insignificant throughout the 5-year horizon, and the shock accounts for less than 7 percent of the of the emissions’ FEV. Therefore, similar to the unanticipated NT shock, its role in explaining the cyclical variation in emissions is limited.====Emissions rise in a hump-shaped manner after an anticipated IST shock. Although the impact response is muted and statistically insignificant, the peak response with a 4-quarter lag is similar to the peak response after an unanticipated IST shock. The shock generates a positive comovement between emissions and output, as observed in the data. Importantly, the anticipated IST shock accounts for about 25 percent of emissions’ variation. As a benchmark, the reduced-form GDP shock from the bivariate VAR accounts for up to 36 percent of the variation in emissions. Thus, within the set of the four technology shocks, the anticipated IST shock turns out to be the most important source of the business-cycle variation in emissions. This finding lends strong support to its inclusion in E-DSGE models among the set of technology shocks.====Although the positive emissions responses to technology shocks in the E-DSGE model are consistent with their estimated empirical counterparts, especially beyond the impact period, the shapes of the impulse responses are quite different. In particular, the empirical responses to unanticipated NT and IST shocks are persistent but not hump-shaped as in the E-DSGE model. On the other hand, the empirical responses to anticipated technology shocks display a somewhat muted hump shape, qualitatively consistent with their counterparts in the E-DSGE model.====We conclude our analysis with the response of emissions to two widely studied policy shocks that are viewed as “demand shocks” to GDP. We consider the monetary policy shocks (Christiano et al. (1999)) and government spending shocks (Blanchard and Perotti (2002)), identified using short-run restrictions. We find that emissions decrease over the first three quarters after each of the two policy shocks, and the responses are not statistically significant. Both shocks account for a negligible amount of the emissions’ FEV. Thus, neither monetary policy nor government spending shocks are important sources of the variation in emissions.====Overall, the FEV analysis establishes an important point: nearly two-thirds of the variation in emissions is not accounted for by the structural shocks commonly considered in macroeconomic models. Put differently, close to two thirds of the variation in emissions is due to a structural shock not yet identified in the literature. This finding raises caution for determining optimal environmental policy using E-DSGE models that rely on shocks accounting for a small share of the cyclical variation in emissions.====The remainder of the paper is organized as follows. In Section 2 we present the E-DSGE model with four types of technology shocks, which provides impulse response functions that serve as theoretical benchmarks to their empirical counterparts. In Section 3, we provide an overview of the types of structural shocks we consider along with the identification methodology. In Section 4, we discuss the empirical results. Section 5 concludes. The Appendix contains additional details for our E-DSGE model, the identification of the structural shocks, and the data.",Carbon emissions and business cycles,https://www.sciencedirect.com/science/article/pii/S0164070418302246,15 January 2019,2019,Research Article,276.0
"Beqiraj Elton,Di Bartolomeo Giovanni,Di Pietro Marco","Sapienza University of Rome, Italy","Received 28 May 2018, Revised 29 December 2018, Accepted 7 January 2019, Available online 14 January 2019, Version of Record 19 January 2019.",https://doi.org/10.1016/j.jmacro.2019.01.001,Cited by (9)," below the natural rate for a given period of time. Focusing on the former puzzle, by a parsimonious macro-model that allows for the role of bounded rationality and heterogeneous agents, we obtain tempered responses for both real and nominal variables.","In the aftermath of the Great Crisis, the effectiveness of monetary policy has been challenged by the zero lower bound (ZLB) constraint. As a result, many central banks have largely adopted the so–called “forward guidance.” Essentially, forward guidance is the practice of communicating the future path of the policy rate (Svensson, 2015). In forward-looking economies, by announcing intentions about the future monetary stance, the central bank may be able to manipulate private sector expectations and affect current outcomes in spite of the ZLB. Therefore, a growing number of papers have analyzed forward guidance from several perspectives.====We aim to study the extent to which the belief–formation process affects the dynamics of macroeconomic variables when the central bank uses forward guidance. Agents’ optimal decisions are modeled to be consistent with their forecasts, but their expectation operators may differ across them. We assume two types of individuals: rational and boundedly rational agents. The latter form their beliefs based on a simple perceived linear law of motion on past observed values (as Brock and Hommes (1997); Branch and McGough (2009)). Alternatively, our framework can be equivalently interpreted as a model composed by homogeneous agents who all form their expectations by some degrees of bounded rationality (as, e.g., Bomfim and Diebold (1997); Ball (2000); Weder (2004)).==== Nevertheless, we prefer the heterogeneity interpretation since it is more in line with the empirical evidence as later discussed.====Our paper is in line with a wave of research that aims to rethink how to model the process by which people form their expectations. We explicitly consider non-homogeneous expectations in New Keynesian models as many recent authors.==== Others have instead explored different forms of inattentiveness, i.e., infrequent information updating,==== or simple least squares learning algorithms to form expectations (Evans and Honkapohja (2001), for a survey). A virtue of our approach, shared with many works in this wave, is that we remain rooted in classical economics and its powerful tools as our agents are modelled as maximizing utility subject to constraints. In all these frameworks, agents populating the economy are ex–ante homogeneous, but they differ in the way they form expectations. To avoid confusion, it is worth noting that recent strand of literature introduces heterogeneity in DSGE models in a more radical manner (see, e.g., Kaplan et al. (2018)).====By our behavioral macro-model, we focus on forward guidance. The literature has associated forward guidance to two different, but related, puzzles.====We refer to the former as the forward-guidance-power puzzle (FGPP) and to the latter as the forward-guidance puzzle (FGP).====Our paper focuses on the FGPP following McKay, Nakamura, Steinsson, 2016, McKay, Nakamura, Steinsson, 2017, but some of the results also apply to Del Negro et al. (2012) FGP. In our behavioral model, we obtain tempered responses for real and nominal variables to future (and current) forward guidance. The idea is that bounded rationality prevents a fraction of agents to smooth their (ex-post) consumption. On ag gregate, this mitigates output responses to changes in future interest rates. Consequently, forward guidance has substantially less power on the current economic outcomes and, in general, to stimulate the economy. We show that our assumption implies that output behaves as though there is a discount factor on future consumption in the Euler equation, discounting mitigates the effects of forward guidance policies designed to produce real interest rate changes more and more the further in the future. It is worth noting that our boundedly rational agents fully understand and believe the central bank announcements about future interest rates, but they are not able to correctly forecasts the effects of the forward guidance.====Our assumption is consistent with the empirical macro-evidence according to which agents disagree in their forecasts about the future. Using US data over the past 30 years, Andrade et al. (2016) provide strong evidence on heterogeneity in expectations. Forecasters agree on the Fed’s reaction function but disagree about two fundamentals, namely future inflation and future output. By using expectation surveys, Mankiw et al. (2004) also find a substantial heterogeneity in beliefs and reject the rationality of US consumers’ inflation forecasts.==== Similar results are obtained by Carroll (2003); Branch (2004), Andolfatto et al. (2008), Pfajfar and Santoro (2010); Andrade and Bihan (2013), and Coibion and Gorodnichenko (2015).==== We use Coibion and Gorodnichenko (2015) methodology to calibrate our model.====The most related paper to our work is clearly McKay et al. (2016). They propose an alternative solution to FGPP. They assume that agents face uninsurable income risk and borrowing constraints,==== a precautionary savings effect then tempers their responses to changes in future interest rates. As a consequence, an announcement of a policy plan implying a reduction of the real interest rate in the future is not fully anticipated in the consumption plans and, after the announcement, output rises gradually until the interest rate falls and then return on the steady state, after a short recession.====In McKay et al. (2016), output responses to changes in future interest rates are mitigated. However, the responses of nominal variables are not. Announcement of future reduction in the real interest rate implies an immediate increase in the inflation rate, which grows with the horizon of the forward guidance. Consequently, inflation responses and announced current interest rates may be very high as long as the horizon of the forward guidance is far in the future.==== By contrast, our model provides tempered responses for both real and nominal variables.====We share the general idea with two recent papers: Gertler (2017) and Gabaix (2016).==== Both provide a solution of forward-guidance puzzles based on the introduction of some form of bounded rationality in a New Keynesian DSGE framework. Specifically, the former introduces a hybrid adaptive/rational expectations belief mechanism. The latter uses the idea of cognitive discounting that measures their attention to the future. Agents are assumed to have different discounting. Both concepts are very similar to the Euler learning used in our paper and lead to the same aggregate dynamics. So, putting together ours and their results, the tempered response to forward guidance of DSGE models including bounded rationality seems to be a very (and peculiar) robust result.====Finally, the findings of our paper are also related to those of Gasteiger (2017) and Lustenhouwer (2018), who focus on commitment when agents are boundedly rational. Even focusing on different problems, their results are in line with ours.====In a stochastic version of the model used in this paper, Gasteiger (2017), studies the ability to manipulate private sector expectations under commitment, which is close to forward guidance, as this is a credible commitment to a future path of the policy instrument. He shows that the ability to manipulate private sector expectations under commitment declines with an increasing share of adaptive expectation.====Assuming that agents have finite planning horizons and heterogeneous expectations, Lustenhouwer (2018) studies liquidity traps. He assumes that some agents base their expectations on past observations, while others are fully rational and are forward looking. Although his main focus is on fiscal policy, he also looks at forward guidance in the form of a commitment to keep interest rates at zero for a number of periods. He illustrates that this commitment is more likely to be effective when the liquidity trap is driven by fundamentals and there is a large fraction of forward–looking agents. By contrast, it is not when the liquidity trap is driven by the expectations of backward–looking agents. His findings are thus related to ours since he finds that bounded rationality somehow smooths the effects of forward guidance, but he focuses only on FGP.====The rest of the paper is organized as follows. Section 2 presents our parsimonious sticky–price model consistent with heterogeneous agents. Section 3 derives the analytical results. Section 4 offers some quantitative evaluations focusing on the FGPP. Section 5 provides some robustness checks. Section 6 concludes.",Beliefs formation and the puzzle of forward guidance power,https://www.sciencedirect.com/science/article/pii/S0164070418302349,14 January 2019,2019,Research Article,277.0
"Xuan Chunji,Kim Chang-Jin,Kim Dong Heon","Northeast Asian Studies College, Jilin University, China,Department of Economics, University of Washington, USA,Department of Economics, Korea University, 145 Anamro, Seongbuk-Gu, Seoul 02841, South Korea","Received 31 March 2018, Revised 15 November 2018, Accepted 31 December 2018, Available online 14 January 2019, Version of Record 24 January 2019.",https://doi.org/10.1016/j.jmacro.2018.12.008,Cited by (1),"Since the mid-1980s, the volatility of major macro variables has decreased remarkably, known as the Great Moderation. Even though a great deal of literature addresses the sources of the Great Moderation, its association with potential changes in the dynamics of consumption or output has not been investigated rigorously. This paper reports two empirical findings: i) a faster adjustment of output to changes in its long-run random walk component and ii) a faster adjustment of consumption to changes in permanent income (as approximated by the random walk component of real output) since the mid-1980s. Our interpretation is that a faster diffusion of technology shocks induced by improved information technology may be responsible for the former. In the presence of ====, a change in the consumer behavior induced by lower uncertainty about future income may be responsible for the latter. The change in output dynamics itself may also be responsible for the change in consumer behavior, as implied by Deaton (1991).","Since the mid-1980s, the U.S. economy has been stabilized remarkably (i.e., the Great Moderation), as first documented by Kim and Nelson (1999) and McConnell and Perez-Quiros (2000).==== There exists large literature that investigates the sources of the Great Moderation, which include improved monetary policy, milder economic shocks (i.e., good luck), improved inventory management resulting from improved information technology, and financial innovation.==== However, little attention has been paid to changes in consumption or output dynamics since the onset of the Great Moderation.====On the output side, Lippi and Reichlin (1994) show that the growth of technology-driven trend output may not follow a random walk and its first difference may be serially correlated, unless firms adopt technical innovations simultaneously, i.e., unless the diffusion of technology shocks is instantaneous. Under this situation, one can conjecture that improved information technology may result in a change in the dynamics of output. On the consumption side, the buffer stock model of savings, which has evolved from Zeldes (1989); Deaton (1991), and Carroll (1997), suggests that consumers have a precautionary savings motive in the presence of future income uncertainty. Thus it would be natural to expect that consumers may adjust their behavior in response to a reduction in uncertainty caused by the Great Moderation. Furthermore, Deaton (1991) theoretically show that consumption dynamics is closely related to income dynamics.====We first investigate whether and how the dynamics of U.S. consumption and output have changed with the onset of the Great Moderation. For this purpose, we build on the work of Morley (2007), who employs a bivariate unobserved component (UC) model of consumption and output with a common stochastic trend to investigate the slow adjustment of consumption to permanent income. Our empirical results show that, with the onset of the Great Moderation in the mid-1980s, i) there has been a reduction in the persistence of transitory consumption, resulting in a faster adjustment of consumption to a change in permanent income; and ii) there has been a decrease in the persistence of transitory output, resulting in a faster adjustment of output to a change in its long-run random-walk component.====We then provide implications and interpretations of our model and empirical results. We analytically show that, if the diffusion of technology shocks is not instantaneous, shocks to transitory output and the long-run component of output will be negatively correlated, leading to a slow convergence of output to changes in its long-run trend. Following Morley (2007), we also provide a theoretical background behind a negative correlation between shocks to transitory consumption and the permanent income which results in a slow adjustment of consumption to changes in permanent income. Based on these, we provide the following interpretations of our empirical results: First, the structural break in the dynamics of output resulted from improvement in the information technology and a faster diffusion of technology shocks.==== This aspect in turn contributed to a reduction in output volatility. Second, on the contrary, we interpret that the structural break in the dynamics of consumption occurred as a result of the Great Moderation.====The structure of this paper is as follows. In Section 2, we present our empirical model, in which structural breaks are incorporated in Morley (2007) bivariate unobserved components of consumption and output with a common stochastic trend. Estimation results and the nature of structural break in consumption and output dynamics are presented in Section 3 and the interpretations and the implications of the structural break are presented in Section 4. Section 5 concludes.",New dynamics of consumption and output,https://www.sciencedirect.com/science/article/pii/S0164070418301393,14 January 2019,2019,Research Article,278.0
"ALBULESCU Claudiu Tiberiu,PÉPIN Dominique,MILLER Stephen M.","Management Department, Politehnica University of Timisoara, Romania,CRIEF, University of Poitiers, France,Department of Economics, University of Nevada, Las Vegas, United States","Received 30 October 2018, Revised 3 January 2019, Accepted 7 January 2019, Available online 12 January 2019, Version of Record 19 January 2019.",https://doi.org/10.1016/j.jmacro.2019.01.002,Cited by (5),"This paper investigates the effect of currency substitution between the currencies of Central and Eastern European (CEE) countries and the euro on CEE money demand functions. In addition, we develop a model with ==== foundations, which identifies the difference between currency substitution and money demand sensitivity to exchange rate variations. More precisely, we posit that currency substitution relates to the money demand sensitivity to interest rate spreads between CEE countries and the euro area. Moreover, we show how the exchange rate affects money demand absent a currency substitution effect. This model applies to any country in which an international currency offers liquidity services to domestic agents. The model generates empirical tests of long-run money demand using two complementary cointegrating equations. The opportunity cost of holding the money and the scale variable, either household consumption or output, explain the long-run money demand in CEE countries.","Empirical estimations that rely on open-economy money demand models with currency substitution typically use a standard two-country portfolio balance model (e.g., McKinnon, 1982, Cuddington, 1983, Marquez, 1987, and Leventakis (1993)). This macro-model does not include microeconomic foundations and, thus, is subject to Lucas's (1976) critique.==== Because of its static nature, the estimated money demand may appear unstable for modified monetary policy strategies.====This paper investigates the effect of currency substitution between the currencies of CEE countries and the euro on CEE money demand functions. As CEE countries move toward more financial integration with the European Union (EU), standard theory suggests that CEE households should use an increasing share of euro money relative to their own domestic money.====Two policy implications emerge from our study. The monetary authorities in CEE countries should consider not only the opportunity cost of holding money, but also the effect of the exchange rate, which occurs even absent strong currency substitution. Given that CEE currencies and the euro are complements rather than substitutes, CEE countries need to continue pursuing higher monetary integration prior to the euro adoption.====The empirical studies of the money demand that incorporate the currency substitution effect typically do not provide a micro-founded theoretical model to justify the specification of their empirical money demand functions.==== These models also test for currency substitution through the money demand's sensitivity to the exchange rate. This framework imposes important limitations, since one cannot examine one phenomenon (currency substitution) independently of the other (exchange rate sensitivity). Moreover, these models examine open-economy money demand sensitivity without differentiating between currency substitution and currency complementarity.====We contribute to the existing literature in two ways. First, we develop a micro-founded model that describes a mechanism through which the exchange rate affects money demand even absent currency substitution. That is, money demand responds to exchange rate fluctuations even after removing currency substitution, because the exchange rate affects the liquidity service associated with foreign money holding. Indeed, our model measures currency substitution intensity without explicitly considering the exchange rate. The model also captures both the currency substitution and currency complementarity hypotheses, which enables the assessment of the currency substitution intensity.==== Further, to capture recent economic circumstances in which interest rates went negative, we consider an additional opportunity cost of holding money (domestic or foreign), which keeps the overall opportunity cost positive, even where the interest rate itself becomes negative.====Our micro-founded model, which integrates the liquidity production function and a constant elasticity of substitution (CES) consumption function, produces a money demand function close to Miles (1978), who, however, does not consider the consumption choice. Thus, we demonstrate that the Bordo-Choudri (1982) criticism of Miles's (1978) money demand function is not relevant, since the function does not reflect the omission of consumption or income.====Second, we parameterize our model to test the long-run sensitivity of an open-economy money demand. Our model fits the CEE case in which the euro offers liquidity services to domestic agents.==== After their transition from centralized to market-based economic systems, CEE countries joined the European Union. Several CEE countries already belong to the euro area, while others continue the integration process. Investigating money demand in euro-area candidate countries offers information about their degree of monetary integration and about the liquidity services provided by the domestic currency compared to the euro.====Dreger et al., 2007, Hsieh and Hsing, 2009, and Fidrmuc (2009) investigate money demand in CEE countries. These empirical analyses, however, do not provide a theoretical framework. Therefore, we test the long-run relationship between money demand and its explanatory variables in a micro-founded model that generates two cointegrating equations. The first equation captures the sensitivity of real money demand for foreign currency to the opportunity cost spread of holding that currency. The second equation captures the long-run relationship between real money demand for domestic currency and the opportunity cost of holding the domestic currency, the opportunity cost spread, and a scale variable.====We use Hansen's (1992) instability test to check for long-run relationships and the Dynamic OLS (DOLS) and the Fully-Modified OLS (FMOLS) methods to estimate the cointegrating relationships. We employ monthly data from 1999:M1 to 2015:M11 on four CEE countries – the Czech Republic, Hungary, Poland, and Romania – that use a floating exchange rate mechanism.==== Our model proves, nevertheless, compatible with any exchange rate regime. Also, investigating the effect of currency substitution requires flexibility, but not necessarily a free-floating mechanism (e.g., Fidrmuc (2009)).====The rest of the paper is structured as follows. Section 2 briefly describes the models of money demand in an open economy. Section 3 presents our micro-founded money demand model. Section 4 parameterizes the model and identifies the cointegrating equations. Section 5 reports our empirical investigation for CEE countries. Section 6 concludes.",The micro-foundations of an open economy money demand: An application to central and eastern European countries,https://www.sciencedirect.com/science/article/pii/S0164070418304506,12 January 2019,2019,Research Article,279.0
"Fiorelli Cristiana,Meliciani Valentina","Faculty of Political Science, University of Teramo, Via R. Balzarini, 1, 64100 Teramo, Italy,Department of Economics and Business, LUISS Guido Carli, Viale Romania, 32, 00197 Rome, Italy","Received 31 December 2017, Revised 6 December 2018, Accepted 9 January 2019, Available online 11 January 2019, Version of Record 27 November 2019.",https://doi.org/10.1016/j.jmacro.2019.01.004,Cited by (12), and ,"After the global financial crisis, the ECB, as other major central banks, has introduced new monetary instruments to effectively respond to economic and financial shocks and avoid the deflation risk in zero lower bound (ZLB) era (Peersman, 2011, Woodford, 2012, Boeckx et al., 2017). We refer to non-standard or unconventional expansionary monetary policies implemented by the ECB since 2008, which have involved modifications to procedures and measures usually adopted by policymakers. Such modifications have implied major changes in the role played by the ECB both in interbank money market and in sovereign securities transaction. In fact, having to work in extreme uncertainty conditions, the ECB has had to partially modify policy strategies in favour of greater flexibility, to ensure the correct transmission of monetary policy and achieve monetary targets. The exceptional nature of events required an equally extraordinary intervention by the ECB, which has experimented with new monetary policy strategies, in order to counter-balance the crisis.====The introduction of unconventional measures has had positive implications for the transmission of monetary policy both to financial markets and real economy (Lenza et al., 2010). There are several empirical evidences for financial benefits (Altavilla et al., 2014, Acharya et al., 2016). The effects of unconventional approach on the real economy are less visible and immediate, as the impact of these measures acts indirectly on macroeconomic variables. In any cases, there is no evidence for different impact of conventional and unconventional strategy (Boeckx et al., 2017, Gambacorta et al., 2014).====This paper aims to analyse the role of overall monetary policy strategy in the Eurozone's economic activity and studies responses of industrial production and inflation to standard and non-standard monetary tools. We estimate the impact of post-crisis policies and evaluate the power of stabilization of conventional and unconventional tools. In particular, we determine which policy strategy has been most effective on economic activity and test the hypothesis that combined use of conventional and unconventional tools has led to better results.====Starting from theoretical hypotheses, we perform two econometric models, structural vector autoregressive model (SVAR) and factor-augmented vector autoregressive model (FAVAR), in order to estimate the relationship between monetary instruments and macroeconomic variables. The results will be useful to understand the effectiveness of the strategy followed by ECB and, as suggested by the literature (Krugman, 1998, Eggertsson and Woodford, 2003; Bernanke and Reinhart, 2004; Borio and Disyatat, 2009, Woodford, 2012), to prove the joint effect of standard and non-standard monetary policies, according to their complementary impact on the real economy.====First, we perform a SVAR model, following Boeckx et al. (2017) and Kremer (2016), to estimate the effects of exogenous innovations in conventional and unconventional tools on industrial production and inflation. Furthermore, we use a two-step FAVAR model (Bernanke et al., 2005) which allows to disentangle the conventional from the unconventional monetary policy through factor analysis. This approach departs from the unsatisfactory hypothesis that unconventional policy can be identified with changes in the ECB balance sheet. The results show that the two factors summarizing the conventional and unconventional tools closely mimic the behaviour of the ECB. Moreover, the use of the FAVAR, differently from the VAR, allows detecting the positive impact of unconventional monetary policy on both inflation and real variables. We add to existing literature a comparison of the effectiveness of conventional and unconventional regimes, tracing two variables able to summarize the main monetary measures adopted. Our approach differs from other studies since we introduce a measure of ECB policy through latent factors.==== We explicitly offer conventional and unconventional policy linkages as the source of economic stabilization.====The factors are included in a new VAR (FAVAR) in order to estimate the behaviour of macroeconomic variables in response to monetary innovations. The impulse response functions (IRF) show good results according to the theory and highlight a positive impact of both conventional and unconventional monetary policies on economic activity, prices, and financial stabilization. Moreover, our results prove the necessity to guide the private expectations with non-standard monetary tools, suggesting how the contemporaneous implementation of the two different policies is “big guns” on hand to ECB.====The remainder of the paper is organized as follows. Section 2 introduces the unconventional measures adopted in Eurozone by ECB. Section 3 provides the literature review. We examine the methodological approaches and data in Section 4 and estimate the FAVAR model in Section 5. Section 6 shows main results. Conclusions and policy implications are in Section 7.",Economic growth in the era of unconventional monetary instruments: A FAVAR approach,https://www.sciencedirect.com/science/article/pii/S0164070417305839,11 January 2019,2019,Research Article,280.0
Battiati Claudio,"School of European Political Economy, LUISS University, Via di Villa Emiliani 14, Roma, 00197, Italy","Received 29 June 2018, Revised 18 November 2018, Accepted 29 December 2018, Available online 30 December 2018, Version of Record 12 January 2019.",https://doi.org/10.1016/j.jmacro.2018.12.007,Cited by (0),"Recent evidence suggests that credit booms and asset price bubbles may undermine economic growth even as they occur, regardless of whether a crisis follows, by crowding out investment in more productive, R&D-intensive ==== with credit-constrained entrepreneurs to show how shocks affecting firms’ access to credit can generate boom-bust cycles featuring large fluctuations in land prices, consumption, and investment. During the expansion, rising land prices tend to crowd out capital and (especially) R&D investment: over the medium term, this results in a slowdown in the rate of creation of new (innovative) firms. Moreover, higher initial loan-to-value ratios tend to be associated with larger ==== fluctuations. A counter-cyclical LTV ratio targeting credit growth has relevant stabilization effects but brings about small gains in terms of long-run consumption levels, and thus of welfare.","The recent financial crisis has prompted a renewed interest in the causes and mechanisms of large boom-bust cycles featuring excessive investment and asset accumulation in some sectors of the economy. Specifically, since the crisis was triggered by a burst in the US housing bubble, a great deal of effort has been devoted to understanding the links between the housing market and the macroeconomy.====Recent research, for instance by Brunnermeier and Schnabel (2015) and Jordà et al. (2015), points to credit-financed housing price bubbles as posing a particularly serious threat to both the financial system and the real economy. They find that although housing bubbles can be in general more disruptive than equity market ones, the financing of the bubble is crucial; upon collapse, the ensuing crises are most severe when the bubbles were accompanied by a lending boom and high leverage.====The housing boom of the mid-2000s in the US has been associated with heavily and rapidly increasing leveraged borrowing as well as with a persistent rise above trend of investment, housing and consumption demand in the absence of significant productivity gains followed by a sharp fall below trend levels once the bubble burst (Fig. 1).====These features of the data can be described using DSGE models with collateral constraints à la Kiyotaki and Moore (1997) that explain credit booms and busts as the outcome of financial accelerators and balance sheet effects.====Along these lines, Iacoviello (2005), Iacoviello and Neri (2010), and Justiniano et al. (2015) incorporate collateral constraints into a real business cycle model with heterogeneous agents, assuming that a subset of households use land or houses as collateral to finance consumption expenditures. The assumption of credit-constrained households allows us to explain some features of the data at business cycle frequencies, such as the procyclicality and the volatility of housing prices and investment, the positive co-movement between house prices and consumption expenditure, or the run-up of housing prices during the years leading up to the Great Recession. Some authors, among whom are Liu et al. (2013) and Pintus and Wen (2013), assume that firms, instead of households, are credit constrained, in order to account as well for the positive co-movement between land prices and business investment, which was especially observed during the recent crisis but is hardly accounted for by models with constrained households, as also noted by Iacoviello and Neri (2010). Indeed, when firms are financially constrained by land values, a positive shock to land demand originating in the household sector raises the entrepreneur’s net worth. The ensuing credit expansion triggers a competing demand for land, setting off a financial spiral whose effects propagate to the whole economy. Using US data, Liu et al. (2013) propose that such shocks are the main driver of investment and output fluctuations. This finding is in line with Justiniano et al. (2015), who suggest that fluctuations in house prices, driven by factors other than credit availability, provide a closer account (as compared to exogenous increases in the loan-to-value ratio) of the credit cycle and its aggregate macroeconomic consequences.====This class of models, which embeds housing and financial frictions in a dynamic stochastic general equilibrium setting, has been adopted by a growing strand of literature that investigates the effects of macroprudential policies, often in the form of counter-cyclical loan-to-value (LTV) ratios or capital requirements. Among these====, Rubio and Carrasco-Gallego (2014) study the impact of a macroprudential rule for the LTV ratio on the business cycle, financial stability, and welfare, as well as its interaction with monetary policy. A similar analysis of the optimal design of monetary and macroprudential policies is conducted within a two-country setting by Quint and Rabanal (2014) –in an estimated DSGE model of the Euro Area– and by Lambertini et al. (2013) –in a calibrated closed-economy– and Mendicino and Punzi (2014) in models where credit cycles are driven by news shocks. The literature indicates that in the presence of financial shocks, a counter-cyclical macroprudential rule which targets financial variables, such as house price dynamics or credit growth, may yield significant gains in terms of macroeconomic stabilization and welfare, although a trade-off between the welfare of Savers and Borrowers can arise.====This paper aims to assess the effectiveness of dynamic LTV requirements in mitigating macroeconomic fluctuations driven by boom-bust cycles in land prices and their desirability from a welfare point of view. Diverging from existing literature, the analysis is conducted in the context of an endogenous growth model. The rationale for the adoption of a growth framework, as pointed out by Benigno (2013), is that it allows for a full appreciation of the effects of prudential policies by taking into account the permanent level effects caused by crisis events. If, as Comin and Gertler (2006) suggest, high-frequency non-technological disturbances can influence the pace of R&D activities, the effects of such shocks can then extend far beyond the business cycle frequencies and impose larger welfare costs because of their effect on growth. In order to capture these effects, I rely on the real business cycle model with heterogeneous agents and credit limits of Liu et al. (2013) and extend it to incorporate Schumpeterian endogenous growth and knowledge spillovers à la Aghion and Howitt (1998), as modeled in a DSGE setting by Nuño (2011).====Schumpeterian models often predict a counter-cyclical behavior of the fraction of savings allocated to R&D activities, due to the opportunity cost of R&D being lower during recessions. Countering this prediction, recent studies show that R&D expenditures may in fact be pro-cyclical, and that they drop in a recession. Among these, Barlevy (2007), who explains this as the result of dynamic externalities that encourage entrepreneurs to concentrate innovations in booms, even if this concentration is not socially optimal. Stiglitz (1993) and Aghion et al. (2010) argue that the procyclical behavior of productivity-enhancing investment may be due to firms being credit constrained. This feature of R&D expenditures can exacerbate the costs associated with recessions by making these more persistent and the return to balanced growth more costly. The Great Recession has in fact been followed by a prolonged period of subdued growth, with the gap between actual GDP per capita and its original trend path widening until 2014 (Fig. 1a).====Recent literature on the effects of credit booms and asset price bubbles highlights furthermore that such phenomena, in addition to the risk of precipitating the economy into a protracted recession, may undermine economic growth even as they occur, regardless of whether or not a crisis follows. In this regard, Cecchetti and Kharroubi (2015) find that an exogenous increase in financial sector growth can harm productivity growth by inducing resource misallocations; the credit expansion may disproportionately benefit investment projects with high collateral but low productivity while crowding out more productive sectors, especially highly R&D-intensive industries.====Indeed, the slowdown in productivity growth started well before the economy plunged into recession (Fig. 1b). Consistent with the aforementioned literature, using data for the 50 US states and District of Columbia over the period 1997–2015, evidence can also be found of a negative relation between business investment in R&D (BERD) and house prices, after controlling for output. In fact, once time and state fixed effects and GDP (which is positively correlated to both BERD and house prices) are taken into account, house prices and R&D investment show a significant negative correlation (Fig. 2).====By incorporating endogenous R&D-led growth==== into a DSGE framework with financial frictions, this paper attempts to explain these features of the investment dynamics in response to financial shocks, also in order to obtain a better evaluation of the welfare gains associated with stabilization policies. The modelling framework assumes an economy populated with two agents, namely, households (Borrowers) and entrepreneurs (Savers). Borrowing is subject to an LTV constraint, so that loans need to be collateralized against a fraction of the value of the assets (land) that the Borrower owns. Borrowed funds are allocated between consumption and investment, which in turn can be used to buy land, capital or to undertake R&D. I calibrate a version of the model that replicates some key dynamic and long-run features of the US economy over the last four decades, using it to assess the impact of several financial and real shocks, and to evaluate the effects of a macroprudential rule that automatically reduces the LTV ratio when the economy overheats. The financial shocks considered here are of two types: an exogenous increase in the LTV ratio, and a positive shock to householdsâ taste for housing services, which triggers a run-up in land prices. Following (Justiniano et al., 2015), the former can be intended as a loosening of lending standards due to either deregulations or innovations in the financial market, while the latter is meant to capture other factors (frictions or some deeper shocks) unrelated to credit availability and not modeled here, leading to an increase in house valuations.====Both shocks drive up land prices, which results in increased borrowing capacity for the firms and facilitates a protracted expansion in consumption, investment, and production. However, the initial increase in the capital accumulation rate is short-lived, while R&D spending starts declining right after the initial boom. Nonetheless, the competing demand for land keeps pushing up land prices, whose increase stimulates aggregate consumption, even after production has begun to slow down. With diminishing marginal returns to capital and land, however, rising debt levels will eventually erode entrepreneurs’ demand for consumption and land, laying the ground for a turnaround in the land price dynamics. Falling land prices then reduce both agents’ net worth, which depresses aggregate demand. As a result, production capacity and output decline at an increasing speed, precipitating the economy into a recession followed by a prolonged period of subdued economic performance. Over the medium term, insufficient R&D investment results in a decline in the rate of creation of new firms, until a new wave of innovations will start pushing economic activity back to its trend level, approximately 11 years after the shock.====The length of the expansionary phase of the cycle is about six years, which is consistent with the results reported in Mendoza and Terrones (2012) on the effects of credit booms. Only the shock to the LTV ratio, however, is associated in this model with a boom in firms’ loans. This shock is also associated with much larger fluctuations in all the macroeconomic aggregates, suggesting that shocks which directly affect credit availability explain better than shocks to asset valuations the boom-bust cycles of the kind observed during the recent financial crisis. This result counters (Liu, Wang, Zha, 2013, Justiniano, Primiceri, Tambalotti, 2015), who find shocks to housing demand to be a more important source of macroeconomic fluctuations, but it is in line with Jermann and Quadrini (2012), who stress the importance of financial shocks that affect firmsâ ability to borrow as a driver of business cycle movements.====The simulations show that an exogenous productivity shock is also capable of generating similar movements, with a protracted hump-shaped expansionary phase followed by an excessive contraction, due to declining land prices that lead to insufficient aggregate savings during the downturn. Indeed, a positive technology shock stimulates both entrepreneurs’ and households’ expenditures, thus triggering competing demand for land and the financial multiplier effect. These results are consistent with the findings of Pintus and Wen (2013) and Jensen et al. (2015) on the role of credit constraints as a source of propagation and amplification of shocks to technology or credit conditions.====Overall, the two financial shocks combined emerge in this model as the major driver of fluctuations in the main economic aggregates. At the same time, TFP shocks also explain a large share of volatility in output growth as well as in land price and R&D investment, due to the amplification effect of collateral constraints. The model also incorporates investment-specific technology shocks which, consistently with recent DSGE literature, play a minor, but not irrelevant, role in driving output and investment fluctuations.====The model also shows that higher initial values of the LTV ratio tend to be associated with larger fluctuations and a shorter expansionary phase followed by a deeper recession when the economy is hit by financial or productivity shocks.====The second part of the paper is thus devoted to assessing the effects of an LTV requirement that responds counter-cyclically to changes in the growth rate of output or to financial variables such as land prices or credit. The impulse response functions show that a rule targeting output is effective in dampening the economy’s response to both financial and productivity shocks, although a rule that targets credit growth should be preferred in response to a shock to lending standards: by directly tackling credit, such a rule hampers the amplification effect of the collateral constraint. The use of a counter-cyclical response of the LTV ratio to land prices can also mitigate the consequences of the “bubble” burst by relaxing the borrowing constraint during periods of recession, but overall it is less effective in stabilizing the economy. The welfare-maximizing rule prescribes strong reactions to credit growth, while alternative rules targeting output or land prices, even if effective in stabilizing the economy, are not welfare-improving. At any rate, welfare gains under the optimizing rules are small. In fact, as the analysis is performed in the context of a growth model, the level effects are particularly relevant for welfare evaluation: once the boom and the bust phases of the cycle are both taken into consideration, over the long term the net effect on the levels of consumption and output is small, and so are the welfare gains.====Sensitivity analysis shows that the utility specification assumed strongly affects the ability of the model to describe boom-bust cycles as well as its welfare results. When constant relative risk aversion (CRRA) preferences are introduced, financial shocks result in less intense expansions followed by a smoother return to balanced growth, whereas TFP shocks fail to inject boom-bust cycles. At the same time, substantial welfare gains stem now from the introduction of a macroprudential policy rule.====The rest of the paper is organized as follows. The next section describes the model. Section 3 discusses the calibration of the model and the impulse responses to several shocks. Section 4 investigates the effects of an LTV rule: impulse responses associated with simple rules are shown, and then the quantitative implications of optimized rules are discussed. Section 5 concludes the paper.","R&D, growth, and macroprudential policy in an economy undergoing boom-bust cycles",https://www.sciencedirect.com/science/article/pii/S0164070418302921,March 2019,2019,Research Article,281.0
"Hallett Andrew Hughes,Acocella Nicola","Kings College, University of London, United Kingdom,George Mason University, 3351 Fairfax Dr., MS 3B1 Arlington, VA 22201, USA,“Sapienza” University of Rome, via castro laurenziano, 9 - 00161, Rome, Italy","Received 23 April 2018, Revised 10 December 2018, Accepted 17 December 2018, Available online 28 December 2018, Version of Record 14 January 2019.",https://doi.org/10.1016/j.jmacro.2018.12.006,Cited by (2), derived from the New Keynesian (NK) approach to modeling the economy. It turns out that there are some differences between the bounds on the permitted policy parameters which depend on whether we are tied to the ZLB or not. But stabilizing policy rules under forward guidance are always feasible and always available.,"Forward Guidance (FG), together with Quantitative Easing, has been at the forefront of practical and largely successful innovations in recent monetary policy. But the formal literature on FG has, so far, been limited to certain special cases or particular circumstances (Kool and Thornton, 2012, Del Negro et al., 2013, Den Haan, 2013, Filardo and Hofmann, 2014).====However, the impact of FG on economic outcomes is mixed. One strand of the literature argues that the impact is large in a model with feedback monetary rules (McKay et al., 2016): the effect of an expected rise in interest rates 5 years hence is 18 times larger than the impact of an equal sized rise in current rates. Such implausible results can be removed by introducing imperfect competition, incomplete markets or asymmetric information. But a better and more general explanation of this claim is that, for plausible parameterizations, the model used has no steady state/equilibrium path. It is easy to verify that the roots of the model are not stable.====These results are problematic because FG has been seen only in terms of ==== (the ability to reach or regain certain target values) rather than ==== (the ability to return an economy to some steady state path following an arbitrary shock). This paper removes this constraint by making the policy rules themselves forward looking. In the jargon of the literature, this is a matter of using ==== rather than ==== FG.====Previous studies have examined stabilization under FG in a few particular cases. This does not supply a general result, so we cannot tell if an economy can always be so stabilized or not. To answer that question, this paper shows that it can be stabilized in all circumstances, but that a suitably designed rule will be needed; that is to say, a rule with parameters large enough and possibly time varying (Appendix 2). This is consistent with previous papers that show standard Taylor rules are not generally sufficient either (Cochrane, 2011, Levin et al., 2010).====Some attention also needs to be given, at the theoretical level, to the limits to stabilizability when the FG horizon is not long enough to let private sector expectations adjust, or where there are bounds to legitimate policy action (Evans, 2012, Gavin et al., 2013, Woodford, 2013). We deal with both issues by noting that, under stabilization, the policy horizon is large (if not infinite) ==== repeatable. This implies that stabilizability is unconditional. Hence, time inconsistency is not an issue for stabilizability.====The contribution by Levin et al. (2010) is relevant here. These authors warn that an economy's stability may be at risk if there are physical or formal bounds to the policies that can be imple-mented. If that is true, FG can be used to stabilize the system only when shocks are moderately negative. When negative shocks are large (in a big recession or serious deflation), or at the ZLB, they conjecture FG will be unable to take them into account and ensure future stability. The issue is whether this unproven hypothesis actually stands up. It conflicts with the theory of dynamic controllability or stabilizability which, as illustrated below, shows that stabilization is ==== possible, although possibly not without cost, when the private sector as well as policymakers have forward looking expectations. The explanation for the difference is that Levin ==== did not consider the possibility that their results might depend on their choice of fixed and insufficiently responsive policy rules (theory implies that may require time varying rules), or on inappropriate guidance. This is the specific hypothesis we test.====This distinction would have been important to the Euro-zone when it contemplated extending quantitative easing in the face of deflationary pressures; and to the US, UK, and Euro-zone, as they try to avoid inflation while exiting the quantitative easing and low interest rates regime. In this paper we show that, in the presence of rational expectations, stabilizability can always be achieved through FG, independent of the size of shocks, if an appropriate policy rule is chosen – even if the economy is currently at the zero-lower bound (ZLB).====The paper is organized as follows. Section 2 presents our policy model with forward looking expectations. Section 3 introduces the ideas of dynamic controllability and stabilizability, and why time inconsistency does not arise in the absence of preference or information changes. Section 4 then uses the usual dynamic reduced form (DSGE) model, standard in this literature, to investigate the issue of stabilizability under all sizes of shock. Section 5 goes on to deal specifically with the impact of ZLB constraints in that framework. Section 6 clarifies some possible limits of FG. Section 7 then concludes by outlining our contribution to the theory of economic policy literature.",Forward guidance reassessed: Stabilizability under endogenous policy rules,https://www.sciencedirect.com/science/article/pii/S016407041830171X,March 2019,2019,Research Article,282.0
"Kinfemichael Bisrat,Morshed A.K.M. Mahbub","School of Management, New York Institute of Technology, New York, NY 10023, USA,Department of Economics, Southern Illinois University, Carbondale, IL 62901, USA","Received 4 August 2017, Revised 12 December 2018, Accepted 14 December 2018, Available online 15 December 2018, Version of Record 19 December 2018.",https://doi.org/10.1016/j.jmacro.2018.12.005,Cited by (16),"Using disaggregated service sector data for 95 countries, we found unconditional convergence in real labor productivity for the service sector. The aggregate service sector yields a large unconditional convergence coefficient of −0.035; we also found the presence of unconditional convergence for individual sub-sectors. Since the service sector now faces both domestic and international competition, the presence of unconditional convergence in labor productivity in this sector is not surprising. Unlike Rodrik (2013), we found evidence of unconditional convergence for aggregate labor productivity in the most recent data available, and the convergence of the service sector productivity appears to be a crucial component of this reversal.","The service sector currently contributes more than 60% of the world's production and employment and has a share in international trade of approximately 20% (WTO, 2014). Although the service sector's share in international trade may appear modest compared with its contribution to production and employment, this sector has become increasingly internationally mobile (WTO, 2013), as many services that had been considered domestic activities have now become internationally tradable (WTO, 2014). For example, the outsourcing of labor-intensive services has received a great deal of attention from both academics and policymakers (Liu and Trefler, 2008). Banking, medical services, entertainment, and sports activities have become increasingly globalized. This trend toward the internationalization of services is expected to grow with the rise of electronic transmission technologies. As a result of the increasing importance of the service sector, the Uruguay Round of trade negotiations, which took place between 1986 and 1993, resulted in the adoption of the General Agreement on Trade in Services in 1995 (see WTO 2013, WTO 2014).====With the increase in the tradability of services, firms engaged in providing services face intense competition not only from their domestic rivals but also from foreign firms. As the service industry has become increasingly globalized, firms engaged in providing services, even those located in low-income countries, strive to emulate their foreign counterparts. In addition, multinational companies are setting quality standards for the industry as they compete with local companies. Faced with increasing competition, domestic service providers in low-income countries must improve their efficiency. As a result, we find an increase in productivity in the service sector and signs of the diffusion of modern technology in developing countries. For example, in the financial sector, we can observe remarkable similarities between firms in both low- and high-income countries with regard to both the organizational structure and work environment (Konan and Maskus, 2006). Taking these factors into account, it is imperative to examine the ways in which labor productivity in the developing countries is catching up to the labor productivity of the rich countries in service sectors. This paper represents an attempt to assess this trend.====In contrast to early studies in the convergence literature that focused on aggregate variables such as convergence in per capita real GDP, researchers have recently been examining the presence or absence of convergence of labor productivity, disaggregated at a sectoral level, with a greater emphasis on manufacturing. Freeman and Yerger (2001) focused on labor productivity in the manufacturing sectors in the Organization for Economic Co-operation and Development (OECD) countries. They found unconditional convergence in pre-1970 data and an absence of convergence in post-1970 data. Rodrik (2013), using data with different levels of aggregation drawn from 118 countries, found unconditional convergence in labor productivity in the manufacturing sector. Similarly, using disaggregated data, Inklaar (2014) found stronger convergence in manufacturing sectors. He also found rapid convergence in labor productivity in the manufacturing sectors of emerging market economies.====Although the service sectors of almost every country in the world have expanded and both the international trade in services and the diffusion of technology in these sectors have increased at a rapid rate, we are not aware of any studies that examine the convergence of labor productivity in the service sector using disaggregated data. The lack of availability of detailed data for the service sectors of many countries may be the main reason behind this gap in the literature. For this paper, we compiled annual data on labor productivity, both for the aggregate service sector and for 12 sub-sectors thereof, from 95 countries using different but compatible data sources. The value-added data were collected from the National Accounts Official Country Data (United Nations Statistics Division [UNSD]), and the employment data were collected from the International Labor Organization (ILO) Statistics database (ILOSTAT). The matching of these data from two different sources has provided us with a consistent data set for the service sector and its sub-sectors.==== This dataset enabled us to examine whether labor productivity in the service sector exhibits unconditional convergence.====Our main results are based on data from 95 countries, representing 12 sub-sectors of the service sector during the period 1975–2012. Following the methodology used by Rodrik (2013), we find unconditional convergence for the aggregate service sector. This suggests that countries with a low initial labor productivity in their service sectors experience more rapid labor productivity growth than countries with a high initial labor productivity. When we examined the disaggregated data, we find the presence of unconditional convergence in 11 of the 12 sub-sectors of the service sector.====Unlike researchers who found a lack of convergence in the aggregate per capita GDP (for example, Rodrik, 2013), we find unconditional convergence for aggregate labor productivity in the most recent data. It appears that unconditional convergence in labor productivity of the service sector, an increasingly large component of GDP, is one of the crucial factors responsible for this recent reversal of the convergence of aggregate labor productivity.====The remainder of this paper proceeds as follows: Section 2 presents a brief theoretical background of the convergence literature. In Section 3, we describe the methods used to compile a compatible data set for value-added and employment that we used to calculate labor productivity in the service sector. In Section 4, we present our results and robustness checks. In Section 5, we discuss the results, focusing on 12 individual sub-sectors within the service sector. In Section 6, we consider issues related to convergence in the aggregate GDP per capita, along with the presence of sectoral unconditional convergence. In Section 7, we present our concluding remarks.",Unconditional convergence of labor productivity in the service sector,https://www.sciencedirect.com/science/article/pii/S0164070417303312,March 2019,2019,Research Article,283.0
"Chin Kuo-Hsuan,Li Xue","Department of Economics, Feng Chia University, No. 100, Wenhwa Rd., Seatwen Dist., Taichung City, Taiwan, 40724, R.O.C.,Institute of Chinese Financial Studies & Collaborative Innovation Center of Financial Security, Southwestern University of Finance and Economics, 612 Gezhi Building, 555 Liutai Road, Wenjiang District, Chengdu, Sichuan, 611130, China","Received 2 February 2018, Revised 23 November 2018, Accepted 14 December 2018, Available online 15 December 2018, Version of Record 28 December 2018.",https://doi.org/10.1016/j.jmacro.2018.12.004,Cited by (6),We evaluate the performance of individual and combination forecasts produced by ,"Forecasting macroeconomic indicators of interest is clearly important to forecast-based monetary policy. With respect to model-based forecasting, policymakers nowadays rely on a variety of macro-econometric models, a reduced-form vector autoregression (VAR) or a structural-form dynamic stochastic general equilibrium (DSGE) model in particular, to predict the target variables as precisely as possible. It is widely accepted that information such as prior knowledge about model parameters, the selection of variables, and the data that the researcher feeds into the model play a critical role in the improvement of forecasting performance. The Bayesian approach provides a logical and consistent way to combine the prior information, provided by the researcher, with the empirical data. This approach is widely used by policymakers for macroeconomic forecasting.====Over the last two decades, some studies even show that information combination is helpful for forecasting. Stock, Watson, 2003, Stock, Watson, 2004 extract information on a range of macroeconomic variables from a large data set, and then combine the data information in a simple way to produce combination forecasts of U.S. inflation or output. Specifically, they generate individual forecasts of a variable of interest from a univariate regression model, in which the explanatory variables include the lags of the forecasted variable and a specific predictive variable, chosen from the large data set. Using alternative predictive variables, a group of individual forecasts are generated and then averaged according to a variety of weighting schemes to construct combination forecasts.==== Stock and Watson Stock, Watson, 2003, Stock, Watson, 2004 argue that the simplest forecast combination with nearly equal weights has the best predictive performance, relative to the individual forecasts and the forecasts generated from a random walk or a dynamic factor model. By contrast, Wright (2009) argues that combination forecasts become more accurate if the equal weights on individual forecasts of U.S. inflation are replaced by the posterior model probability, obtained via a Bayesian approach.====In this paper, we adopt a Bayesian approach to construct combination forecasts based on individual forecasts produced by Bayesian VARs. More specifically, individual forecasts are generated from VARs with different priors, obtained from an economic or a non-economic perspective. Regarding the non-economic prior information (henceforth, statistical priors), we consider a range of statistical priors, which have been extensively used in small-scale VARs. These priors include the Minnesota prior, natural conjugate prior, independent normal-Wishart prior and the stochastic search variable selection (SSVS) prior. As shown in the literature, these statistical priors have great advantages of capturing the time series properties of macroeconomic variables, of modeling flexibility or of computational convenience. For example, Doan et al. (1984) find that estimating VARs with the Minnesota prior produces more accurate out-of-sample forecasts than estimating univariate equations.====With respect to the economic prior information, we construct priors for VARs based on a structural DSGE model with nominal, real, and financial frictions. Ingram and Whiteman (1994) state that it is helpful in macroeconomic forecasting to add to VARs prior information from a simple stochastic growth model. Del Negro and Schorfheide (2004) extend the approach of Ingram and Whiteman (1994) by attaching to VARs prior information from a small-scale New Keynesian DSGE model. They find that the forecasting performance is competitive with or even better than Bayesian or conventional VARs with respect to output growth, inflation, and the nominal interest rate.====In this paper, those individual forecasts, generated from the ARMA(1,1) model and from the VARs with statistical and/or DSGE prior(s), are combined via simple averaging and Bayesian model averaging (BMA) approaches.==== Stock and Watson Stock, Watson, 2003, Stock, Watson, 2004 state that the simple average of individual forecasts has the best predictive performance. By contrast, both Jacobson and Karlsson (2004) and Wright (2009) argue that the BMA approach is an excellent alternative to other prediction procedures, measured by out-of-sample forecasting performance, for forecasting Swedish and U.S. inflation rates respectively in a univariate regression framework. Here, we assess the out-of-sample forecasting performance of individual and combination forecasts in a multivariate framework over two non-adjacent subperiods, the “Great Inflation” and “Great Moderation” periods.====The recent literature finds that the forecasting performance of macro-econometric models (with respect to inflation) deteriorates during the “Great Moderation” period. Several studies attribute such time variation in forecast accuracy to the nexus between persistence and forecastability (e.g., Stock, Watson, 2007, Cogley, Primiceri, Sargent, 2010). Particularly, Stock and Watson (2007) argue that the reduced persistence of inflation during the “Great Moderation” period has restricted the forecast improvement of elaborate macro-econometric models upon simple univariate predictors, because weakly persistent inflation dynamics are mostly dominated by transitory components.====We are interested in several questions pertinent to macroeconomic forecasting. First, can the forecasting performance of VARs be greatly improved by incorporating prior economic or statistical information? Second, can the combination forecasts obtained by utilizing the ARMA (1,1) forecast and the forecasts from Bayesian VARs with statistical and/or DSGE prior(s) outperform the individual forecasts? Third, which weighting scheme, simple averaging (adopting equal weights) or Bayesian model averaging (weighted by the sum of log predictive likelihoods), is preferable for forecast combination? Lastly, what is the relationship between persistence and forecastability with respect to the macroeconomic variables examined? These questions will be answered in this paper.====We contribute to the current studies by discovering the superior performance of combination forecasts constructed from ARMA(1,1) and VARs with statistical and/or DSGE prior(s) via a BMA approach. Different from recent studies evaluating the quality of individual forecasts produced by reduced-form VARs, structural DSGEs, and hybrid DSGE-VARs (Kolasa, Rubaszek, Skrzypczyński, 2012, Bekiros, Paccagnini, 2014), we further assess the quality of combination forecasts. In addition, instead of generating and then combining the individual forecasts from univariate models (Stock, Watson, 2003, Jacobson, Karlsson, 2004, Stock, Watson, 2004, Wright, 2009), we produce and average individual forecasts in a multivariate framework.==== Although (Andersson and Karlsson, 2008) average forecasts from VARs via a BMA approach, they do not take into consideration economic information.====Here, we employ a New Keynesian DSGE model with financial frictions. The inclusion of the DSGE prior in forecast combination allows us to assess the extent of improvement brought about by the researcher’s prior economic information in an out-of-sample forecasting exercise. On the other hand, Del Negro et al. (2016) and Kolasa and Rubaszek (2015) construct combination forecasts by simply averaging the individual forecasts, directly generated from New Keynesian models with financial frictions. However, we adopt a different approach because our forecasts are produced by Bayesian VARs which indirectly incorporate prior information from a New Keynesian model. Boivin and Giannoni (2006) incorporate the DSGE prior into the transition equation of the latent factors in a dynamic factor framework and in a data-rich environment. While their empirical framework is valuable, in the present paper we do not take into account unobserved components, when introducing the DSGE prior into Bayesian VARs.====Our main findings are summarized as follows. First, regarding individual forecasts, the inclusion of prior informationimproves the accuracy of point forecasts produced by Bayesian VARs, relative to the ARMA(1,1) benchmark. With respect to the quality of density forecasts, however, Bayesian VARs cannot compete with the ARMA(1,1) model in most cases. Within Bayesian VARs, neither statistical nor DSGE prior(s) can consistently dominate the other in terms of forecasting performance. Second, compared with the individual forecasts, forecast combination helps to produce unbiased forecasts in most cases. Moreover, it significantly improves the forecast accuracy of the macro-econometric models, especially during the “Great Moderation” period. Third, the combination forecasts obtained via simple averaging or the BMA approach are qualitatively equivalent. Put differently, we do not benefit from adopting the BMA approach in forecast combination. Finally, partially in line with the recent literature, we find a positive relationship between the persistence of inflation and its relative forecast accuracy at the four-quarter horizon; however, we observe a negative relationship between persistence and the relative forecast accuracy of output and the interest rate.====The remainder of this paper proceeds as follows. Section 2 introduces the BMA approach in a VAR framework. Section 3 briefly introduces Bayesian VARs with statistical and DSGE priors. The data and forecasting results are presented in Section 4. Section 5 explores the relationship between the persistence and the volatility of a macroeconomic variable, on the one hand, and its forecastability, on the other hand. Section 6 concludes.",Bayesian forecast combination in VAR-DSGE models,https://www.sciencedirect.com/science/article/pii/S0164070418300521,March 2019,2019,Research Article,284.0
"Gelain Paolo,Iskrev Nikolay,J. Lansing Kevin,Mendicino Caterina","Federal Reserve Bank of Cleveland, 1455 East 6th Street Cleveland, OH 44114, USA,Bank of Portugal, Economics and Research Department, Av. Almirante Reis 71, Lisbon 1150-012, Portugal,Federal Reserve Bank of San Francisco, P.O. Box 7702, San Francisco, CA 94120-7702, USA,European Central Bank - Directorate General Research - Monetary Policy Research, Kaiserstrasse 29, Frankfurt am Main D-60311, Germany","Received 31 May 2018, Revised 5 December 2018, Accepted 6 December 2018, Available online 11 December 2018, Version of Record 24 December 2018.",https://doi.org/10.1016/j.jmacro.2018.12.002,Cited by (8),We estimate a “hybrid expectations” version of the Smets and Wouters (2007) model in which a subset of agents employ simple moving-average forecast rules that place a significant weight on the most recent data observation. We show that the overall fit is improved relative to an otherwise similar version in which all agents have fully rational expectations. In-sample and out-of-sample analyses show the superiority of the hybrid expectations model in generating an ,"A wide variety of survey evidence suggests that agents do not form their expectations rationally. Survey data on inflation and from both stock and real estate markets provide strong empirical support for considering extrapolative or moving-average type forecast rules.====On the basis of that evidence, this paper estimates a “hybrid expectations” version of the Smets and Wouters (2007) model in which a subset of agents employ simple moving-average forecast rules that place a significant weight on the most recent data observation. In this respect those agents have adaptive expectations. Our aim is to quantitatively evaluate the performance of the model relative to an otherwise similar version in which all agents have fully rational expectations. Our analysis builds on the work of Gelain et al. (2013) who introduce hybrid expectations into the model of Iacoviello (2005).====We estimate two versions of the Smets and Wouters (2007) model, one with hybrid expectations and one with rational expectations. We find that the data strongly and statistically support the hybrid expectations version from a goodness-of-fit point of view. Nevertheless, there is not strong evidence in favor of one model or the other when it comes to a moments matching comparison. But in contrast, in-sample and out-of-sample forecast analyses clearly indicate the superiority of the hybrid expectations model in generating an expected inflation series that more closely tracks expected inflation from the Survey of Professional Forecasters.====Our estimation indicates that about 70% of agents uses moving-average forecast rules to form their expectations. For most of the forecasted variables, we estimate a large weight on most recent observations. This allows the hybrid expectations model to generate more endogenous persistence and rely less on the structural parameters that the rational expectations model typically relies on to account for the persistence in the data. The overall fit, as measured by marginal data density, points out a better fit of the hybrid expectations model.====Local identification analysis suggests that the observable variables that are more informative regarding the estimates of the fraction of households who employ the moving-average forecast rule are consumption and the short-term interest rate. The identification tests do not provide results that strongly favour one of the two models. Indeed, despite the larger number of free parameters, the model with hybrid expectations does not perform necessarily worse than the rational expectations model in terms of parameter identification analysis.====A clear rank between the models can be determined on the basis of two forecast exercises. First, we compare the estimated implied inflation expectations from both models with the expected inflation from the Survey of Professional Forecasters. We find that at all horizons the presence of backward looking agents helps in tracking survey data. This is especially true for the continuous decline of inflation expectations in the 1980s and 1990s. Second we compute out-of-sample inflation point and density forecasts and show that there is a statistical dominance of the hybrid expectations model.====In what follows we present a review of the related literature. We then describe the model and how we introduce hybrid expectations. Finally, before providing some concluding remarks, we show and discuss the estimation results.",Inflation dynamics and adaptive expectations in an estimated DSGE model,https://www.sciencedirect.com/science/article/pii/S0164070418302428,March 2019,2019,Research Article,285.0
"Ciccarone Giuseppe,Giuli Francesco,Marchetti Enrico","Department of Economics and Law, Sapienza University of Rome, Via del Castro Laurenziano 9, Rome, 00161, Italy,Department of Economics, Roma Tre University, Via S. D’Amico 77, Rome, 00145, Italy,Department of Economic and Legal Studies, University of Naples Parthenope, Via G. Parisi 13, Naples, 00134, Italy","Received 25 May 2018, Revised 1 December 2018, Accepted 6 December 2018, Available online 11 December 2018, Version of Record 19 December 2018.",https://doi.org/10.1016/j.jmacro.2018.12.003,Cited by (2)," and output deviations from their targets are small. If this was not the case, the central bank could run the risk of increasing bubble volatility and rapidly turn the expansionary shock into a recession.","The relevance of asset bubbles in affecting the business cycle has been increasingly acknowledged in the economic literature, especially after the worldwide financial crisis that started at the end of the first decade of the new century. Yet, what is commonly known as the “conventional” view, i.e., that monetary policy can prevent the formation of bubbles by properly controlling interest rates (or the money supply), has not been unanimously shared. Such a monetary policy, known as “leaning against the wind”, asks monetary authorities to possess a theory of bubble formation and to accurately predict the occurrence of asset bubbles driven by non-fundamental shocks, a requirement that does not seems to be easy to meet (Posen, 2011, see, e.g.). This conventional policy may also call for a change in the inflation target regimes that flourished before the financial crisis, or a change in the targets. An alternative view, dating back to Bernanke, Gertler, 1999, Bernanke, Gertler, 2001 and to the original discussion on the arguments of the Taylor rule, acknowledges the negative effect that asset prices can produce on the business cycle, but envisages little advantage in including asset prices in monetary policy rules.====The debate on the subject matter was recently heated by Galí (2014) “unconventional” result, and the associated policy suggestions. In his overlapping generations (OLG) model, where there exist a continuum of stable bubbly steady states and a continuum of unstable bubbly steady states, a strong response of the interest rate to bubble fluctuations may increase both asset price volatility and their bubble component. Monetary policy should accordingly take into account the stabilization of both aggregate demand and the bubble, and it may well be optimal in some instances to lower the interest rate in the face of a growing bubble. This clear-cut conclusion is not however widely shared by the flourishing literature on rational bubbles, where several recent contributions have reached contrasting results. As stressed by Hirano et al. (2018), there hence remains wide room for analyses of the interactions of monetary policies and asset bubbles.====In the face of this unsettled discussion, in this paper we aim to contribute to the debate on the role of monetary policy in economies displaying equilibrium rational asset bubbles by building a simple model which allows us to simulate the effects on real macroeconomic variables of different policy rules controlling nominal interest rates. To this aim, we develop an OLG framework similar to those employed in several recent contributions (Galí, 2014, Martin, Ventura, 2016, Ikeda, Phan, 2016, e.g.) we endow the model economy with heterogeneity among households, so as to easily allow for the existence of borrowers and lenders in the credit market, and hence put together two main elements which are also present in the some recent infinite-horizon models: (i) frictional financial markets coupled with the presence of physical capital accumulation, which we model in a way similar to that introduced by Martin et al. (2012) and Martin, Ventura, 2015, Martin, Ventura, 2016; (ii) nominal frictions in the formation of final goods’ prices, as proposed, e.g., by Galí (2014) and Dong et al. (2017). On the one hand, due to the existence of financial frictions, the agents looking for funds to be used for productive investments face a significant constraint: the amount of credit that can be obtain by borrowers is provided by lenders on the basis of the amount of collateral investors can pledge. On the other hand, due to price stickiness, monetary policy can affect the real macroeconomic variables by using the nominal interest rate as the policy instrument.====The main results we obtain can be summarized as follows. First of all, we show that this environment can generate stationary equilibria with rational asset bubbles of different types and we provide an explanation for the evidence that the presence of purely bubbly assets can increase the value of stationary output in some of these equilibria and reduce it in others. In the first case, the expansionary effect of the bubble is induced by the loosening of the collateral constraint, which favors an increase in the demand for funds (what we label the ====). In the second case, which was typical of the early models by Samuelson (1958) and Tirole (1985), the contractional effect we obtain is the outcome of two further channels: an indirect one, which increases the cost of borrowing funds for productive investment (the ====), and a “substitution” one, due to the fact that the bubble competes with productive capital in the investment choice of borrowers (====). Numerical simulations suggest that when the bubble size is small the credit demand channel prevails but, as the size of the bubble increases, the other two channels may push the economy into a recession.====Numerical simulations of the linearized model show that, under sticky prices, if the policy rule does not react to the bubble, a positive shock to the bubble’s return increases capital and production. If the bubble is small, the expansionary credit demand channel prevails over the price/allocation channels along the whole path to steady state; if the bubble is large, the credit demand channel prevails only at impact and the recessive push generated by the price/allocation channels subsequently dominates. The increase in marginal costs and inflation brings about a raise in the real interest rate, which subsequently falls back to the steady state value at a pace which depends on the value of the weight attached to inflation in the policy rule. We also show that, if the weight attached to the bubble in the policy rule is too big, the “leaning against the wind” policy may increase the economy’s volatility and produce an excessive reaction of the real rate (especially if the bubble size is small). Our conclusion is that, in an OLG economy under credit frictions and sticky prices, a “leaning against the wind” policy is desirable only if the reactions of the central bank to inflation and output deviations from their targets are small. If this was not the case, the central bank could run the risk of increasing bubble volatility and rapidly turn the expansionary shock into a recession.====The paper is structured as follows. In the next section we briefly survey the related literature. Section 2 describes the model economy. Section 3 analysis the macroeconomic equilibrium and carries out a stationary state analysis. Section 4 studies the dynamics of the linearized model. Section 5 addresses the issues on monetary policy under bubble-induced macroeconomic fluctuations. Section 6 summarizes the main results and concludes.",Should central banks lean against the bubble? The monetary policy conundrum under credit frictions and capital accumulation,https://www.sciencedirect.com/science/article/pii/S0164070418302313,March 2019,2019,Research Article,286.0
Zhang Yahong,"Department of Economics, University of Windsor, Windsor, ON N9B 3P4, Canada","Received 28 March 2017, Revised 2 October 2018, Accepted 4 December 2018, Available online 7 December 2018, Version of Record 19 December 2018.",https://doi.org/10.1016/j.jmacro.2018.12.001,Cited by (4), is more effective in dampening the downturn when it targets the assets backed by housing wealth.,"The collapse of housing prices in the US during the Great Recession eroded not only housing wealth but also the value of the assets in the banking sector. The deterioration of banks’ balance sheets resulted in a rise in the spread of mortgage loans. Fig. 1 illustrates the movements in both housing prices and mortgage spreads for the US from 1991Q1 to 2014Q1. The most striking is the steep decline in housing prices and the sharp rise in mortgage spreads seen during the Great Recession (the second shaded area). On the real side of the economy, the rise in mortgage spreads led to a decline in borrowing for both households and firms, causing both business and housing investments to fall. On the policy front, the deterioration of banks’ balance sheets and the rise in mortgage spreads were the key factors that triggered the Federal Reserve to intervene in the mortgage markets by directly purchasing mortgage-backed securities.====The goal of this paper is to build a model that allows us to study the interaction between housing prices, households’ debt, and banks’ balance sheet positions, and to assess quantitatively the impact of the shocks affecting housing demand and banking sector balance sheets on household debt and the aggregate economy. To achieve this, I introduce a banking sector as in Gertler and Karadi (2011) and Gertler and Kiyotaki (2015) to a DSGE model with household debt (Iacoviello, 2005, Neri, Iacoviello, 2010). In the model, there are two principal-agent problems: one between borrowers and banks, and the other between depositors and banks. On the one hand, the default risks and the enforcement problem between households and banks lead to a standard collateral requirement for borrowers. On the other hand, frictions in the principal-agent relationship between the banks and the depositors generate a spread between the lending rate and the rate that banks pay to their depositors. In particular, there is a moral hazard problem between the banks and the depositors: The banks can divert a fraction of the funds supplied by the depositors at the end of each period at the costs that the depositors may terminate the relationship with the banks. In the model, the banks’ capacity to raise deposits is limited due to the concerns of the depositors that the banks can divert part of the funds. A lending spread is a natural outcome due to this limitation since otherwise, the no-arbitrage condition would drive the gap between the lending rate and the borrowing rate to zero.====In addition to providing funds to households, the banks in the model also offer funds to firms. As in Gertler and Karadi (2011) and Gertler and Kiyotaki (2015), firms issue claims to the banks to finance their capital purchases and there are no financial frictions between the firms and banks. Different from Gertler and Karadi (2011) and Gertler and Kiyotaki (2015), firms in this paper also use housing as an input in production and issue claims to the banks for housing purchases. This type of claim can be considered as a mortgage-backed security (MBS), and its return is closely related to housing prices. A bank’s assets include claims backed by capital (capital claims), claims backed by housing (housing claims), and loans to households. Since both mortgage loans and housing claims are backed by housing, I assume that housing claims and mortgage loans have the same diversion rate. In contrast, I assume different diversion rates between the assets backed by housing and those backed by capital; this causes the lending spread for housing claims (mortgage spread) to be different from that for capital claims (business spread).====To make the model more suitable for delivering quantitative results, the baseline framework in this paper is the conventional monetary business cycle framework as in Christiano et al. (2005) and Smets and Wouters (2007), including features such as habit persistence, investment adjustment costs, and nominal rigidity. There are nine shocks in the model, including four shocks that are related to housing and household debt: a shock to housing demand, a shock to the loan-to-value ratio, a banking sector shock, and a default shock. The banking sector shock, as in Ferrante (2015), is a shock to the diversion rate for the assets related to housing wealth, including housing claims and mortgages. We can consider this shock as a sudden shift of the confidence of depositors in the banking sector. For example, when the market for MBSs becomes particularly risky, depositors believe that the option to divert funds that are backed by housing becomes more attractive to bankers and the depositors begin to have less confidence in the banking sector. The default shock is introduced in a similar ad hoc fashion as in Iacoviello (2015). It transfers assets from the banking sector to the household sector; thus, it captures the loss of assets suffered by banks when households default on their debt obligations. In this sense, the default shock is essentially a transfer or redistribution shock: a positive default shock leads to a decline in the net worth of the banking sector and a rise in households’ wealth (transfer of wealth from banks to borrowers).====I apply the Bayesian method to estimate the model using the US data from 1991Q1 to 2014Q1, conducting policy exercises based on the estimates. The main findings are as follows. First, the shocks that are related to housing and household debt are the most important driving forces for the business cycle fluctuations for this period. There is a significant spillover effect from the housing market to the rest of the aggregate economy. Together, the housing demand shock, the banking shock and the default shock contribute about 69% of the variations in output and about 46% of variations in business investment. Neri and Iacoviello (2010) estimate a monetary DSGE model using US data for the period before the recent financial crisis and find housing market spillovers arise mainly through consumption. When housing prices rise, households’ collateral value rises, causing consumption to rise due to the collateral effect. In contrast, the spillover effect in this paper emerges mainly on investment through the banking sector. The initial source of housing market disturbances could be a negative shock to housing demand, a shock to the diversion rate of the assets backed by housing, or a shock to the default rate. Using a housing demand shock as an example, the mechanism is as follows. A shock that reduces households’ demand for housing leads to declines in house prices. This decreases the return of the housing claims held by the banks. The banks’ net worth declines and the constraint on raising deposits tightens. The tightening of the banks’ ability to raise deposits leads not only to a rise in the mortgage spread but also an increase in the business spread (a spillover effect). The increase in borrowing costs further depresses the demand for household and business loans, causing both housing prices and capital prices to decline. As a result, aggregate investment and output decline.====Second, the model captures the fact that mortgage spread is countercyclical and negatively correlated with housing prices. In the theoretical DSGE models with financial frictions, although corporate spreads are commonly modeled (for example, Bernanke, Gertler, Gilchrist, 1999, Christiano, Motto, Rostagno, 2014), mortgage spreads are largely ignored. Walentin (2014) shows that the variations in mortgage spread have important implications for business cycles. He uses a structural VAR model to study the business cycle effects of a mortgage spread shock and shows that a shock that increases mortgage spreads reduces house prices and GDP. The results in this paper are in line with the empirical findings in Walentin (2014). The advantage of the approach used in this paper is that the variations in mortgage spread are endogenously determined due to the financial frictions in the banking sector.====Third, similar to Iacoviello (2015), who also introduces a simple banking sector to a DSGE model with household debt and estimates the model using US data from 1985Q1 to 2010Q4, I find that the demand shocks from the household sector (the shocks to households’ consumption and housing demands) contributed significantly to the decline in output in late 2008 and early 2009. Different from Iacoviello (2015), who finds that the default shock is one of the leading shocks contributing to the GDP decline through 2009, I find that the negative impact of the default shock is not significant until 2009Q4. The banking sector shock, which is absent in Iacoviello (2015), turns out to be an important shock in explaining the sharp decline in GDP during the Great Recession.====Lastly, I find that unconventional monetary policies, such as purchasing troubled assets, can mitigate the downturn. Moreover, the results show that the central bank’s credit policy is more effective when it targets assets backed by housing wealth.====This paper is related to two particular strands of literature. The first explores the role of financial intermediation in business cycle fluctuations (Gertler, Karadi, 2011, Gertler, Kiyotaki, 2015, Meh, Moran, 2010, Gerali, Neri, Sessa, Signoretti, 2010) and the second studies the effect of household debt on the aggregate economy during the Great Recession (Philippon, Midrigan, 2011, Eggertsson, Krugman, 2012, Mian, Sufi, 2011, Justiniano, Primiceri, Tambalotti, 2015). This paper includes both the banking sector net worth and household sector net worth channels, which are related to Iacoviello (2015) and Ferrante (2015). Iacoviello (2015) introduces a simple banking sector to an otherwise standard DSGE model with household debt. In his model, the limited ability to raise deposits from households for banks generates a wedge between the deposit rate and the lending rate. The drawback of Iacoviello (2015) is that there is no micro-foundation for why banks face this limitation. In addition, Iacoviello (2015) is a purely real model and thus, monetary policy is absent in the model.==== Similar to this paper, Ferrante (2015) incorporates a banking sector based on Gertler and Karadi (2011) and studies the interaction of balance sheets of the household sector and banking sector. However, Ferrante (2015) only studies two types of shocks: housing risk and MBS collateral shocks. In addition, the model in Ferrante (2015) is calibrated instead of estimated.====The remaining of the paper is organized as follows. In Section 2, I describe the model. Section 3 contains the data and estimation. I analyze the model’s performance and the key mechanism in the model in Section 4 and conduct policy experiments in Section 5. Section 6 concludes.","Household debt, financial intermediation, and monetary policy",https://www.sciencedirect.com/science/article/pii/S0164070417301179,March 2019,2019,Research Article,287.0
"Isakin Maksim,Serletis Apostolos","Department of Economics, Cleveland State University, Cleveland, Ohio, 44115, USA,Department of Economics, University of Calgary, Calgary, Alberta, T2N 1N4, Canada","Received 23 May 2018, Revised 18 November 2018, Accepted 22 November 2018, Available online 5 December 2018, Version of Record 11 December 2018.",https://doi.org/10.1016/j.jmacro.2018.11.007,Cited by (1),We take the user cost approach to modeling the financial firm that maximizes capitalized variable profit to investigate whether the ==== mechanism differs in low and high ==== environments. We use the panel of U.S. commercial banks from 1992 to 2014 to construct the user costs of financial goods and propose a two-step procedure to estimate a regime-dependent variable profit function in the normalized quadratic semiflexible functional form. We derive demands for and supplies of financial and non-financial goods and provide evidence consistent with neoclassical ,"Conventional dynamic new Keynesian models that are widely used to study business cycle and monetary policy assume no active role for the financial intermediary sector. This assumption implies that banks perfectly channel monetary policy conducted by the central bank to the private sector. However, this simplification is unrealistic in most macroeconomic conditions. For example, changes in banks’ willingness to lend significantly affect the monetary transmission mechanism through the bank lending channel as discussed, for example, in Bernanke et al. (1988) and Kashyap and Stein (1995). In periods of economic expansion, when the interest rate is relatively high and the opportunity cost of holding excess reserves is positive, banks increase lending and expand deposits until reserves reach the required (or desired) levels. The increase in the money supply stimulates the level of economic activity and puts upward pressure on prices. However, during economic downturns when the interest rate is low and close to the zero lower bound, a low opportunity cost of holding excess reserves removes the incentives for banks to lend out their excess reserves. This may lead to the creation of a large quantity of excess reserves as the global financial crisis of 2008 has revealed. In this case, the monetary transmission mechanism becomes ineffective, because of underinvestment in firms with limited access to external capital other than banks.====There are several economic channels through which low interest rates affect the banking technology. First, periods of low interest rates are usually associated with recessions, high risk premia, and low asset prices. A large fraction of nominal bank liabilities accompanied by declining prices of assets suggests that the leverage of the banking sector rises during a crisis. However, a stricter regulatory environment and stronger precautionary incentives force many financial intermediaries to deleverage during crises. Therefore, the overall effect on bank leverage is ambiguous. He et al. (2010) find that while leverage dynamics vary across intermediary sectors, commercial banks increased their leverage during the Great Recession. Second, low interest rates, increased volatility of return on assets, and stricter regulation during recessions change the composition of bank assets. Finally, the presence of government-backed debt financing and government liquidity injections in the form of hybrid debt (preferred stocks) alter bank decisions with respect to the sources of funds in the low interest rate environment. Changes in banks’ lending and borrowing policies in periods with relatively high and low interest rates, including the recent period of persistently low interest rates, suggest that the banking technology is different across different interest rate regimes. If these differences are not recognized, they can severely affect estimates and conclusions drawn from the data.====In this paper, we develop and estimate a model of the banking firm where the interest rate switches between low and high regimes. We take the user cost approach to modeling the financial firm that maximizes capitalized variable profit choosing the quantities of financial and non-financial goods — see Barnett (1978) and Hancock (1985). We impose the theoretical regularity conditions to obtain inference consistent with neoclassical economic theory — for discussion, see Barnett and Serletis (2008). To estimate the model, we use a Markov regime switching filter, studied in Hamilton (1989), Krolzig (1997), and Sims et al. (2008), and Bayesian estimation methods developed in Albert and Chib (1993) and Kim and Nelson (1999). We propose a two-stage procedure to estimate our model. In the first stage, we estimate a regime switching interest rate process and the filtered probabilities of the regimes at each moment of time. In the second stage, we apply the structural model of a profit maximizing bank and, using the filtered regime probabilities, formulate the posterior distribution of the parameters of the normalized quadratic demand system. Since the number of parameters in demand systems is usually large (70 in our two-regime case) the numerical efficiency of the estimation procedure is important. The two-stage estimation routine allows us to significantly reduce the estimation complexity of the problem.====We conduct the empirical analysis in two stages. At the first stage, we model the interest rate regime using an index built on interest rates with different tenors. In doing so, we make an implicit assumption that the federal funds interest rate is an exogenous variable with respect to an individual bank. Though at the macro level the interest rate can hardly be deemed exogenous, an individual bank has little effect on it and we find the assumption reasonable. We construct factors underlying movements in the term structure of interest rates using the principal component analysis of yields on treasury bonds with different maturities. We use the first principal component as a “level”factor of the yield curve — see Litterman and Scheinkman (1991) — to estimate the Markov switching model and recover filtered probabilities of the interest rate regimes.====In the second stage, we use the panel of the U.S. commercial banks from 1992 to 2014 to estimate a regime-dependent variable profit function with a normalized quadratic approximation. We estimate the price elasticities of demands and supplies of financial and non-financial goods and their substitutability implied by the financial technology under different interest rate regimes. We find several significant differences in the technology under alternative regimes. For example, the own-price elasticity of the supply of loans and leases declines from 6.02 to 4.68 when the economy moves from a high to a low interest rate regime. Also, the supply of loans and leases is very inelastic with respect to the cost of debt capital in both regimes. These changes in elasticities of bank’s demands and supplies determine the ability of monetary and regulatory policies to affect markets of financial and non-financial goods through changes in their user costs.====Our two-step approach differs from a “standard” hidden regime (one-step) setup where the coefficients of the profit function are regime dependent and a latent regime follows a Markov chain. In our model the economic regime depends on the macroeconomic interest rate, i.e. based on additional macro-level information, while the bank’s profit function relates bank-level prices and quantities of financial goods. As a result we mitigate the risk of overparameterization inherent in the standard hidden regime model. In addition, since we explicitly define economic regimes as the interest rate regimes, our model suggests a clear interpretation of differences in the banking technology in alternative regimes compared to the standard hidden regime model. Finally, our approach de-links the estimation of the interest rate regime from the sample of micro-level data and makes it possible to estimate the regime more efficiently using a much longer sample of macroeconomic time series.====It is to be noted that the user cost approach allows us to classify financial goods into inputs and outputs based on their contribution to the bank profit. In particular, an asset is considered to be an output if the return on investment into this asset exceeds the opportunity cost of funds. Otherwise, the asset is an input. Similarly, a liability is classified as an input if the cost of holding this liability is greater than the opportunity cost of funds. Otherwise, the liability is an output. According to this classification scheme, deposits are likely to be labeled as outputs, especially, if only interest expenses are taken into consideration. We find that an asset or liability can be classified as input for one bank and as output for another bank, because of differences in the opportunity cost of funds. Also, for a particular bank, the type of a financial good can change in different economic regimes.====Our paper contributes to the growing body of literature studying financial disintermediation in crises and financial technology in a low interest rate environment. See, for example, Diamond and Rajan (2005), Farhi and Tirole (2012), He and Krishnamurthy (2013) and Tobias and Liang (2018). We quantify the effect of the low interest rate regime on the elasticity of banking technology and the efficiency of the monetary policy transmission mechanism. We find that the monetary policy transmission mechanism becomes less efficient in the low interest rate environment. This finding is consistent with Karras (1996), Gambacorta and Rossi (2010) and Borio and Gambacorta (2017). In a broader context, we find that the banking technology is relatively inelastic in both regimes. This confirms the conclusion in Hancock (1985).====An alternative to the estimation of the bank’s profit function in the user cost approach is the estimation of the bank’s cost and revenue functions. This approach is often used in the analysis of the cost-efficiency, profit-productivity and scale economies in the banking sector. See, for example, (Berger and Mester, 2003) and Wheelock and Wilson (2018). An advantage of this approach is the absence of the assumption that the banks are price takers. One drawback of the cost function estimation is that it requires an ex ante classification of bank financial goods into inputs and outputs. Since there is an inconclusive debate about whether some financial goods (e.g. deposits) are inputs or outputs, the classification often amounts to a researcher’s arbitrary choice. Another drawback of the cost function estimation is that it ignores bank’s cost of capital in calculating bank’s input prices. The user cost approach takes into account banks’ cost of capital and does not require an ex ante classification of financial goods into inputs and outputs. We summarize the ex post classification in Table 3. We also discuss the assumption that the banks are price takers below.====The rest of the paper is organized as follows. Section 2 provides a brief review of the user-cost approach to modeling the financial firm. In Section 3, we discuss data and measurement matters. Section 4 deals with econometric setup. Section 5 presents and discusses the empirical results. The final section concludes the paper.",Banking technology in a Markov switching economy,https://www.sciencedirect.com/science/article/pii/S0164070418302271,March 2019,2019,Research Article,288.0
"Bishnu Monisankar,Guo Nick L.,Kumru Cagri S.","Economics and Planning Unit, Indian Statistical Institute, Delhi Centre,Department of Business and Economics, Wheaton College, United States,Research School of Economics, The Australian National University, Australia","Received 30 December 2017, Revised 13 November 2018, Accepted 19 November 2018, Available online 5 December 2018, Version of Record 27 November 2019.",https://doi.org/10.1016/j.jmacro.2018.11.005,Cited by (9),"they contribute to and receive payments from this program as well as how ====. Implications of the low income groups’ shorter longevity are examined both analytically and quantitatively. In the analytical part, in a simple two period partial equilibrium model, we observe that a social security program can have regressive outcome even though the benefits of the program are designed to be progressive. We also derive the conditions under which this can happen. Afterwards, we create a large scale quantitative OLG model calibrated to the US economy to compare aggregate and welfare implications of the US type PAYG, a non progressive PAYG, and a means tested pension program. Our results clearly indicate that incorporating differential mortality into the model changes the welfare implications of social security programs.","This paper evaluates welfare aspects of Pay As You Go (PAYG) social security when different income groups have different mortality rates. It clearly demonstrates that incorporating differential mortality has significant welfare implications. Empirical evidence suggests that agents who have low income tend to start working earlier and have shorter longevity than those with middle or high income. Since a PAYG social security program collects payroll taxes whenever agents are working, and it pays retirement benefits as long as retirees are alive, each individual’s well being depends on how ==== they contribute to and receive payments from this program as well as how ====. We first analytically show that when the differential mortality rates across income groups are taken into account, PAYG social security systems can have regressive outcomes even though the benefits of these programs are designed to be progressive. We then quantitatively examine the welfare implications of PAYG under this differences in the mortality rates. Social security programs are large expenditure items and play important insurance and redistribution roles. Hence, aggregate and welfare implications of various programs are well analyzed both analytically and computationally starting with Diamond (1965) and Auerbach and Kotlikoff (1987), respectively. Imrohoroglu et al. (1995), Huggett and Ventura (1999), Nishiyama and Smetters (2007), Kitao (2014), and Fehr and Uhde (2013) quantify the aggregate and welfare effects of PAYG, fully funded, and means-tested social security programs. The overall conclusion is redistributive and insurance benefits of PAYG and means-tested programs are exceeded by behavioral distortions generated by those programs. Fehr et al. (2013) quantitatively characterize the consequences of rising pension progressivity in an incomplete market OLG model and show that more redistributive pension system will improve welfare. Most developed countries have nominally progressive PAYG social security programs as their benefits are. The US Social Security program has a highly progressive benefit formula to determine monthly payments. Hence, the people with low lifetime earnings get a much higher replacement rate than those with high lifetime earnings. For instance, Social Security might replace 70 percent of earnings for someone with a full-length career in the bottom quantile of the earnings distribution (see Goda et al. (2011) for a detailed discussion). Since benefits are paid as annuities, the total amount of benefits an individual receives depends on the that individual’s longevity. If individuals from high income groups can live relatively long enough, the progressive structure of the PAYG system would disappear. Starting with Kitagawa and Hauser (1973), the extent, causes, and trends of differential mortality in the US have been well analyzed empirically. Meara et al. (2008) and Hadden and Rockswold (2008) find increases in indices of mortality inequality by education groups. Waldron (2007) finds evidence of a significant increase in differential mortality by lifetime earnings among males aged 60 and above in the 1972–2001 period. Cristia (2009) finds significant differences in age-adjusted mortality rates across individuals in different quintiles of the individual lifetime earnings distribution. According to this paper, for instance, men aged between 35–49 in the bottom quintile have age-adjusted mortality rates 6.4 times larger than those in the top quintile. The existence of strong empirical evidence regarding mortality differentials across different earning quintiles requires evaluating social security programs once again. The aforementioned earlier studies on social security assume away differences in mortality rates. In this paper, our aim is to fill up this gap. Precisely, we analyze the implications of social security programs taking differential mortality rates across different earnings quintiles into account. There is a limited number of studies that analyze the implications of differential moralities across different income groups in the context of PAYG social security. Bommier et al. (2011) study the normative problem of redistribution between individuals who differ in their lifespans. They show the social optimum is obtained when long-lived individuals retire later and consume less per period than short-lived individuals. Le Garrec and Lhuissier (2017) study macroeconomic and distributional consequences of global gain in life expectancy. By considering a framework where individuals decide to acquire skills depending on economic incentives and differential mortality, they show that introducing a ‘long career’ exception cannot be to the advantage of future unskilled workers unless education yields no spillover effects. Goda et al. (2011) calculate internal rates of return and net present values for the US PAYG program under the assumption of differential mortality without providing any formal model. They show that under the assumption of constant mortality across lifetime income subgroups, the Social Security system is progressive but a good deal of the progressivity is undone or even reversed when differential mortality is taken into account. Tan (2015) and Bagchi (2017) also show that differential mortality matters in welfare rankings of various pension programs.====Longevity is a private information but occupation is observable. Considering that, Pestieau and Racionero (2016) study the optimality of allowing pension policies that differ by occupations when individuals differ in terms of longevity and occupation. They show that there is a case for differentiating the pension policy by occupation when longevity is (imperfectly) correlated with occupation. While they point out the importance of differential mortality in pension designing and can be considered as a complement to our paper, our focus is neither on the pensions that may vary with the occupation nor on the search of an optimal pension program. Instead, first we show that a PAYG program with progressive benefit structure can have a regressive welfare outcome. Later we provide a numerical model demonstrating that a simple proportional benefit structure might improve overall welfare when mortality differentials taken to account. We should mention here a related point that the pension system, in order to pursue long run sustainability, should adopt defined contribution formula, rather that defined benefit, while redistribution should be carried out by resorting to general taxes. In this paper, we focus on pension programs with defined benefit structures since this is the case for many public pension programs and hence, we do not analyse the implications of pension programs with defined contribution structures i.e. we do not take the defined contribution pension programs into our account.====So the main message that the paper conveys is that incorporating differential mortality changes the welfare implications of a social security program. To show this formally, we first generate a simple two period partial-equilibrium OLG model with differential mortality and lay out the conditions under which a PAYG program can even be regressive despite its progressive benefits design. This happens because low income individuals receive pension benefits for relatively shorter period of time. As a result, the progressive benefits would be outweighed by differential mortality risks, and hence the social security becomes regressive in terms of welfare. In the analytical model the focus is on the welfare changes along the distribution of income. Then, we generate a large scale general equilibrium incomplete market OLG model that is calibrated to the US economy. The model mimics the features of the US income tax system and PAYG Social Security program. We then generate models in which a means-tested pension program and a non-progressive PAYG program replaces the current US PAYG program. The focus of the computable OLG model is aggregate welfare changes. We show that once we take into account differential mortality risks, welfare rankings of the PAYG and means-tested programs do not change. In both non-differential and differential mortality cases, the fixed tax means-tested pension programs dominate the PAYG. Among the means-tested pension programs, the least redistributive one in which benefit reduction rate is equal to zero generates the highest welfare. When we fixed the maximum benefits instead, the most progressive means-tested program in which benefit reduction rate is 100% generates the highest welfare since it comes with the least tax burden. Yet, the welfare ranking of the PAYG and non-redistributional programs depend on whether mortality differentials are taken into account or not. More precisely, when we ignore mortality differentials, progressive PAYG dominates non-progressive-non-redistributional PAYG program. This result changes when take differential mortality into our account and non-redistributional PAYG dominates the progressive PAYG which is in line with our analytical results. In sum, both analytical and computational models imply that the existence of mortality differences have important aggregate and behavioral implications and should have been taken into account seriously. The paper is organized as follows. In Section 2, we use an analytical model to show that the regressive outcome is possible as a result of a social security program, even though its benefits are designed to be progressive. The only driving force behind this qualitative result is the differential mortality risks. In Section 3, we introduce the quantitative model. Section 4 introduces parameter values. In section 5, we calibrate the overlapping generations model to data and provide the results implied by the model. In Section 6, we conclude.",Social security with differential mortality,https://www.sciencedirect.com/science/article/pii/S0164070417305815,5 December 2018,2018,Research Article,289.0
Yao Yao,"School of Economics and Finance, Victoria University of Wellington, Rutherford House 307, Pipitea Campus, Wellington 6011, New Zealand","Received 29 November 2018, Accepted 29 November 2018, Available online 4 December 2018, Version of Record 12 December 2018.",https://doi.org/10.1016/j.jmacro.2018.11.009,Cited by (28),"This paper studies the impact of the higher education expansion in China on average labor productivity. I argue that in an economy such as China’s, where allocation distortions widely exist, ====. Quantitative results show that, given policy distortions, China’s higher education expansion had a small but negative effect on its average labor productivity (–2.5%). The crowding out of productive capital caused by magnified resource misallocation plays a key role in driving down productivity. However, the productivity effect of the ==== would turn positive if distortions were further reduced or removed.","Human capital has long been considered the engine of economic growth (Schultz, 1961); (Lucas, 1988); (Barro, 1991); (Galor et al., 2004); (Manuelli et al., 2014),==== and government policies that promote the society-wide level of education have thus been well regarded. In fact, many countries have experienced a government-led education expansion program at some stage of their development.==== However, a salient feature of developing countries is the widely existing factor misallocation, which has caused substantial productivity losses (Restuccia, et al., 2008); (Hsieh et al., 2009); (Brandt et al., 2013).==== Since the working of an educational policy is expected to channel through a production factor - human capital, its effects may be limited if factor misallocation becomes more severe following the policy. In this paper, I examine how an educational policy may affect the average labor productivity through its effect on the allocation as well as the stock of human capital using China’s higher education expansion, and how an economic reform may influence the effectiveness of the educational policy by triggering more efficient allocation.====While empirical studies find positive returns to education in China at the individual level,==== a strand of literature argues that the contribution of education to aggregate productivity depends critically on how talent and skills are allocated across different sectors or activities (Baumol, 1990); (Murphy et al., 1991). Pritchett (2006) claims that, in many developing countries, the social returns to education can be far below the private returns because an overwhelmingly large share of college graduates were employed by the less-efficient public sector. While China’s higher education expansion in the late 1990s increased the supply of skilled workers significantly, no previous study has examined its impact on aggregate productivity, taking into account the widespread allocation distortions in China. Moreover, this higher education expansion was accompanied by a large-scale economic reform of the state sector and other market-oriented policies; thus, its effect may be masked by the concurrent institutional changes that were enforced to improve productivity. It is therefore of critical importance to understand the isolated effect of the educational policy on productivity as well as its interactive effects with other policies.====To address these questions, I construct a two-sector general equilibrium model with policy distortions, where overlapping generations of households make educational and occupational choices depending on ability as well as government educational and economic policies. The key features of the model are as follows. (i) There two sectors, and despite their lower productivity (TFP), state sector firms receive subsidies from the government for the use of production factors. (ii) Households who are heterogeneous in ability make an educational choice of whether to acquire college education, and then an occupational choice between the private sector and the state sector upon graduation. Both choices depend on ability. Higher ability not only lowers the disutility of college education, but also the layoff probability of a skilled worker in the private sector (while a skilled worker in the state sector may secure her job perfectly). (iii) The higher education policy enters the model via an exogenous component of the disutility cost of college education; that is, higher education expansion lowers this disutility.====The model thereby characterizes two main tradeoffs regarding the lifetime choices. For the educational choice, college education enhances one’s future labor income, but incurs a disutility cost. For the occupational choice, the private sector may pay a higher wage to the skilled workers due to its higher productivity, but its jobs are less secure than that of the state sector. In equilibrium with reasonable parameterization, the model presents a sorting mechanism under which households are self-selected into three categories based on ability: the ablest acquire college education and then become skilled workers of the private sector, the least able do not go to college and become unskilled workers, and the mediocre enter college and then become skilled workers of the state sector.====The educational policy then affects average labor productivity through two main channels. One is the ==== effect. By lowering the disutility cost of college education, the higher education expansion encourages more people to enter college and hence increases the society’s human capital stock. Since skilled labor complements the more productive technology, average labor productivity can be improved. The other is the ==== effect. As more individuals with lower ability enter college and then become skilled workers under this policy, relatively more of them prefer the state sector to the private sector, since they would have a higher chance of being unemployed if choosing the latter. Thus, the policy plays a role in reallocating relatively more skilled workers to the state sector, intensifying human capital misallocation. The sectoral reallocation of human capital then directs physical capital toward the state sector due to factor complementarity, magnifying physical capital misallocation as well. Furthermore, deepened misallocation raises subsidies demanded by the (expanding) state sector firms, which tightens the loanable funds market and crowds out capital from production, further dampening labor productivity.====I calibrate the model to fit the data of the pre- and post-regimes in China, and then apply the numerical model to quantitative analysis including various policy experiments.==== I also construct a productivity-based measure of human capital for the purpose of policy evaluation. I find that, while the higher education expansion increased China’s human capital by 11% , it reallocated relatively more human capital toward the state sector: the private sector share of human capital would increase by 29% if there was no college expansion. Overall, the policy had a small but ==== effect on the average labor productivity in China. If there was no college expansion, China’s average labor productivity would increase by 2.5% . While magnified factor misallocation per se plays a relatively minor role in driving down labor productivity, it raises subsidies to the state sector and tightens the loanable funds market, crowding out productive capital. This crowding out effect turns out to have a substantively negative impact on labor productivity.====While the negative effect of the educational policy seems striking, it is not to say that higher education expansion in China was wrong. Instead, my analysis suggests that in order to reach the maximal social welfare or labor productivity, higher education should be further expanded. However, this expansion must be accompanied with deepened economic reform that further reduces or completely eliminates allocation distortion. In fact, the productivity effect of the higher education expansion would turn positive when distortions are sufficiently small.====To compare my estimates with those in the literature, I first examine total productivity gains from eliminating allocation distortions. These turn out to be 73% for the pre-regime and 17% for the post-regime, respectively. While these estimates are comparable to those in Brandt et al. (2013), they are smaller than those in Hsieh et al. (2009), which are 115 and 87% for the two regimes, respectively. I argue that there are two main reasons for this result. First, my model does not account for the within-sector distortion as in Hsieh et al. (2009), whereas this distortion could be large due to firm-level heterogeneity across regions, industries, and sizes within the state or the private sector; thus, my paper may have underestimated policy distortions. Second, Hsieh et al. (2009) may have overestimated the productivity gains from eliminating distortions because they do not take into account the human capital effect of distortion by simply assuming a fixed supply of human capital. Nonetheless, using a model with endogenous educational choices, my result shows that policy distortions indeed have a positive effect on human capital stock (10 and 7% for the two regimes, respectively) through their effect on factor prices.====Regarding the literature on the educational policy, to the best of my knowledge, there is no existing work that assesses the impact of China’s higher education expansion on aggregate productivity to compare with====; but in a relevant study, Vollrath (2014) investigates the efficiency of human capital allocation in 14 developing countries (not including China) and finds that eliminating wage distortions between sectors only has a small effect (<5 percent) on aggregate productivity. This estimate is smaller than that generated from my model, which are 47 and 16% in the pre- and post-regimes, respectively. I argue that this difference can be largely attributed to the assumption of Vollrath’s model that physical capital stock and allocation are both constant, while my analysis abandons this assumption and shows that although human capital misallocation per se has a minor effect on labor productivity, it generates physical capital misallocation and more importantly, the crowding out of productive capital, which turns out to have a substantively negative effect on labor productivity. In fact, Vollrath also points out in his paper that incorporating the dynamic response of physical capital accumulation and allocation to human capital allocation could generate a much larger productivity effect of eliminating wage distortion.====Furthermore, in their recent work, Liao et al. (2017) documents that while education-based rural-to-urban migration (via college entry) plays an important role in China’s economic development, college admission has been relatively more selective for rural residents since the college expansion in late 1990s, which has a sizable negative effect on China’s output per capita. To take this important view into account, I extend my benchmark model to a two-region model where urban and rural areas face different technologies of production and opportunities of college education; migration takes place though, either via work (i.e., work-based migration) or via college admission (i.e., education-based migration). The model is calibrated and results show that higher education expansion has a even larger negative effect (−6.6%) on average labor productivity than that in the benchmark model, mainly due to the additional human capital distortion created by relatively increased college selectivity of rural individuals following college expansion. Moreover, the effect of education-based migration on average labor productivity is more considerable than that estimated by Liao et al. (2017) (i.e., 7.1 versus 2.0 in the pre-regime, and 13.6 versus 8.0 in the post-regime) because eliminating rural college admission would amplify sectoral misallocation, the effect of which is not captured by the one-sector model in Liao et al. (2017).====The contribution of this paper is summarized as follows. This paper offers the first quantitative macroeconomic model for studying the impact of the higher education expansion in China on its aggregate productivity. By developing a general equilibrium model with policy distortions, I document a reallocation effect of the educational policy that generates a sizeable negative effect on labor productivity, which, however, has been largely ignored by the human capital literature. Importantly, my analysis incorporates dynamic feedback between the labor market and the capital market in a two-sector economy; thus, it produces more reasonable estimates than studies focusing on one market assuming the other fixed. Furthermore, different from that in the recent misallocation literature, my theory makes both human capital stock and allocation endogenous and affected by policy distortions through factor prices; thus, it identifies an additional channel through which distortion affects productivity. Finally, my results underscore that, to enhance the effectiveness of an education expansion policy, it is crucial to enforce complementary economic policies to improve allocation efficiency. While this study focuses on China, this policy implication can be generally applied to many other developing countries with severe allocation distortions.====The rest of the paper proceeds as follows. Following a brief introduction of the background, Section 2 provides the details of the model economy, followed by a characterization of the equilibrium in Section 3. Section 4 presents calibration and Section 5 quantitative analysis. Section 6 concludes.",Does higher education expansion enhance productivity?,https://www.sciencedirect.com/science/article/pii/S0164070418305093,March 2019,2019,Research Article,290.0
Yilmazkuday Hakan,"Department of Economics, Steven J. Green School of International and Public Affairs, Florida International University, Miami, FL 33199, USA","Received 26 August 2018, Revised 28 November 2018, Accepted 30 November 2018, Available online 4 December 2018, Version of Record 7 December 2018.",https://doi.org/10.1016/j.jmacro.2018.11.010,Cited by (4)," studies have higher macro elasticity measures compared to international finance studies, which has evoked mixed policy implications regarding the effects of a change in trade costs versus exchange rates on welfare measures. This so-called ==== is investigated in this paper by drawing attention to the alternative strategies that the two literatures use for the aggregation of foreign products in consumer utility functions. Using the implications of having a finite number of foreign countries in nested ==== frameworks that are consistent with the two literatures, the discrepancy between the elasticity measures is explained by showing theoretically and confirming empirically that the macro elasticity in international trade is a weighted average of the macro elasticity in international finance and the corresponding elasticity of substitution across products of foreign source countries.","International trade studies have higher macro elasticity measures compared to international finance studies. Since price movements due to policy changes are converted into welfare adjustments through these elasticities, this observation evokes mixed policy implications regarding the effects of trade costs in international trade versus the effects of exchange rates in international finance (e.g., see Ruhl, 2008). Due to these mixed implications on welfare, this observation is called ====.====In order to have a better idea about the magnitude of this puzzle, consider a short summary of studies given in Table 1. Although elasticity measures differ across these studies, international finance studies mostly follow Backus et al. (1994) with a macro elasticity value of about 1.5, while international trade studies mostly follow Anderson and Van Wincoop (2004) or recently Simonovska and Waugh (2014a) and Simonovska and Waugh (2014b) with a macro elasticity value of about 5.==== It is implied that if we directly employ these numbers in a policy analysis, say, in order to investigate the effects of a foreign price change due to tariffs or exchange rates, international trade studies imply quantity changes that are at least three times the international finance studies.====This paper attempts to understand ==== by drawing attention to the alternative strategies the two literatures have for the aggregation of foreign products in consumer utility functions. In particular, while the majority of international finance models include a ==== foreign country (in their two-country frameworks) in order to have an understanding of the macroeconomic developments in the home country, the majority of international trade models include ==== foreign countries in order to investigate the bilateral trade patterns of the home country. Since having alternative numbers of foreign countries is reflected as alternative macro elasticity measures between the two literatures in a nested constant elasticity of substitution (CES) framework, as shown in this paper, ==== can be understood by paying attention to the alternative ways that foreign products are aggregated in the two literatures.====Regarding the details, when a finite number of goods and foreign countries is considered in nested CES frameworks that are consistent with both literatures, this paper finds alternative expressions for the price elasticity of demand as a function of the macro elasticity measures in the two literatures. In order to investigate the conditions under which the two literatures have the very same policy implications (e.g., regarding changes in trade costs versus exchange rates), this paper equalizes the price elasticity measures between the two literatures. This strategy results in an expression that connects the alternative macro elasticity measures in the two literatures, where good-level details are cancelled out during the equalization of the price elasticity measures. In particular, it is theoretically shown that the macro elasticity in international trade is a weighted average of the macro elasticity in international finance and the elasticity of substitution across products of different foreign source countries, where the weight is shown to depend on the number of foreign countries and home expenditure shares. Therefore, the alternative strategies in the two literatures for the aggregation of foreign products are reflected as alternative macro elasticity measures between the two literatures.====The implications of equalizing the price elasticity of demand measures between the two literatures are also tested empirically. Since this investigation requires data on both domestic and foreign trade, it cannot be achieved by using any international trade data set, where domestic trade are not recorded. As an alternative, this paper uses the available trade data within the U.S. by considering interstate trade as foreign trade and intrastate trade as domestic trade. The results based on the estimation of macro elasticity measures in both literatures confirm the theoretical solution provided in this paper that the macro elasticity in international trade is a weighted average of the macro elasticity in international finance and the elasticity of substitution across products of different foreign sources. Therefore, the discrepancy between the macro elasticity measures in the two literatures can in fact be understood by paying attention to the alternative ways that foreign products are aggregated in the two literatures.====Compared to the literature, the nested CES framework in this paper works in a similar way to the one introduced by Atkeson and Burstein (2008) who have shown that considering finite number of goods (and thus good-specific market shares implying variable markups) is essential to explain why export and import prices show substantial and systematic deviations from relative purchasing power parity (PPP) in comparison with source country producer prices. In comparison, having a finite number of source countries in this paper results in having source-country-specific expenditure shares entering into price elasticity measures. This is in contrast to studies such as by Gali and Monacelli (2005) considering infinite number (a continuum) of source countries, where source-country-specific expenditure shares are ineffective in the calculation of price elasticity measures. Since good-level details (and thus the corresponding expenditure shares) are shown to cancelled out during the equalization of the price elasticity measures between the two literatures, having a finite number of source countries (rather than having a finite number of goods) is the key to understand the international elasticity puzzle. Such a theoretical implication (of having sizable source-country-specific expenditure shares) is highly supported by the data as well; e.g., within the U.S., the expenditure share of Kentucky on the products imported from Ohio is about 48% for the second quarter of 2012, while the expenditure share of Delaware on the products imported from New York is about 43% for the first quarter of 2012 (according to the data used in the empirical investigation, below).====This paper deviates from the existing literature due to two main reasons. First, while existing studies in the literature have attempted to understand the puzzle by using the very same functional forms to aggregate across foreign products, this paper draws attention to the difference between the strategies in the aggregation of foreign products between the two literatures. In particular, while providing several supply-side explanations to the puzzle, studies such as by Ruhl (2008), Fitzgerald and Haller (2014), Ramanarayanan (2017), Crucini and Davis (2016) or Feenstra et al. (2018) all consider the very same functional forms between the two literatures in order to aggregate across foreign products. However, a common aggregation strategy is not consistent with either international trade or international finance studies, where the former aggregates across source countries in the upper-tier (e.g., see Anderson and Van Wincoop (2003), Anderson and Van Wincoop (2004), Head and Ries (2001), Hillberry and Hummels (2013), or Hummels (2001), among many others), and the latter aggregates across home and foreign countries in the upper-tier (e.g., see Backus et al. (1994), Blonigen and Wilson (1999), Corsetti et al. (2008), Enders et al. (2011), or Heathcote and Perri (2002), among many others). Therefore, recognizing the difference between the functional forms to aggregate foreign products (as in this paper) is the key to understand the puzzle in the first place.====Second, compared to the existing literature, the overall investigation in this paper abstracts from the complications due to having a time dimension so that we can focus on the differences between the two literatures due to the way that they aggregate across foreign products. In contrast, studies such as by Ruhl (2008), Fitzgerald and Haller (2014) Ramanarayanan (2017) or Crucini and Davis (2016) all focus on solutions based on the difference between the two literatures due to the frictions created by the time dimension, such as uncertainties on productivities, the speed of adjustment of capital, or firm entry/exit decisions over time. However, since these studies do not recognize that the two literatures have distinct functional forms to aggregate across foreign products, their time-related frictions will only complicate the investigation, leading to improper comparisons between the two literatures.====The rest of the paper is organized as follows. The next section introduces demand-side models that are consistent with the two literatures. Section 3 attempts to understand the ==== by considering the importance of expenditure shares in CES frameworks with finite number of goods and countries. Section 4 provides empirical support for the theory introduced in this paper. Section 5 concludes. The Appendix shows the derivations of certain equations used in the main text.",Understanding the international elasticity puzzle,https://www.sciencedirect.com/science/article/pii/S0164070418303756,March 2019,2019,Research Article,291.0
"Engwerda Jacob,van Aarle Bas,Anevlavis Tzanis","Department of Econometrics and O.R., Tilburg University, P.O. Box: 90153, LE Tilburg 5000, The Netherlands,Leuven Centre for Irish Studies, KU Leuven, Belgium,UCLA Department of Electrical and Computer Engineering, Los Angeles, CA 90095, USA","Received 12 July 2018, Revised 26 November 2018, Accepted 27 November 2018, Available online 29 November 2018, Version of Record 6 December 2018.",https://doi.org/10.1016/j.jmacro.2018.11.008,Cited by (5),"This paper analyzes how the introduction of ==== affects debt dynamics in a two-country monetary union model. Monetary and fiscal authorities are engaged in dynamic government debt stabilization games in which ==== on government debt adjust endogenously. Three different equilibria are considered: the non-cooperative Nash open-loop equilibrium, the fiscal coordination equilibrium and the fully cooperative equilibrium. It is shown how the effects of ==== depend on the game-theoretic equilibrium/institutional framework in place, the initial debt levels, policy makers’ concerns with debt stabilization and the strength of financial market discipline.","Europe’s debt crisis has demonstrated a number of remaining weaknesses in the EU’s institutional framework of budgetary policies and management. In particular, the impossibility to stem rapidly evolving sovereign debt crises in a number of peripheral euro area countries raises questions about the functioning of the institutional framework under conditions of stress. Sovereign debt crises were triggered by financial markets that were in doubt -rightfully or not- whether the debt obligations of member states of the monetary union would be honored. The uncertainty was not just fueled by the worrying direction of government debt dynamics in some countries but also by absence/inadequacy of crisis resolution mechanisms in the EMU.====Sovereign debt issuance in the euro area is currently conducted by Member States on a decentralized basis without any form of joint debt guarantees to investors, reflecting the ‘no-bailout’ clause (art. 125 Treaty of the European Union). An alternative would be to introduce Eurobonds. The introduction of commonly issued Eurobonds would mean a pooling of sovereign issuance among the Member States, the sharing of associated revenue flows and debt-servicing costs and most importantly, the introduction of joint and several debt guarantees to investors in case of defaults on government debt of Euro Area member states: each Member State would be responsible not only for its own debt liabilities but also for a share in obligations of any other member state failing to honor its obligations.====Such a pooling of European debt would have a major advantage: it would prevent that members of the Euro Area that are facing a sovereign debt crisis are confronted with the adverse debt feedback effect on the risk premium in the current system of decentralized sovereign debt issuance, allowing the sovereign to get its debt back on a sustainable trajectory more easily. Eurobonds will shelter sovereign debt of Member States from sudden shifts in risk aversion, unwarranted market volatility or animal spirits. Hence, by enabling member states to continually tap capital markets at a stable borrowing rate, a more resilient and less volatile debt trajectory should ensue.====At the same time, it is often outlined that Eurobonds could suffer from one major drawback: a weakening of market discipline and the associated introduction of adverse incentive effects, i.e. the fostering of moral hazard where national fiscal authorities may run excessive deficits and retard fiscal adjustments anticipating a bailout by the European Union/other Member States in case of imminent sovereign default. If the problem of moral hazard is not dealt with, Eurobonds will not bring stability for the EMU as a whole: deteriorating economic and fiscal fundamentals might cause spiraling government debt in some member states that are moreover less disciplined by financial markets. This will also aggravate adjustment for countries that are following sound fiscal policies as interest rates rise in the entire monetary union. The same could happen if doubts arise about the viability of the guarantees underpinning the Eurobonds. Eurobonds, therefore, to be successful would have to be accompanied by a substantially reinforced fiscal surveillance and policy coordination framework as an essential counterpart, so as to avoid moral hazard and to ensure sustainable public finances.====In this paper, we analyze the effects of introducing Eurobonds in a two-country monetary union model where fiscal policymakers and the common Central Bank are engaged in a dynamic debt stabilization game. The effects of introducing Eurobonds are determined by comparing outcomes in case where risk premia on government debt depend on the national debt levels with the case where risk premia on government debt depend on the average debt level in the monetary union. Moreover, these effects are considered for three game-theoretic equilibria: (i) the non-cooperative Nash open-loop equilibrium, (ii) the fiscal coordination equilibrium where both fiscal players cooperate their policies and than act non-cooperatively with the common Central Bank, (iii) the full-cooperation equilibria where both fiscal authorities and the common Central Bank act in a cooperative manner. We analyze several cases using numerical simulations of the model. In most cases, Eurobonds contribute to more government debt stabilization in the high debt country and a reduction of its losses. The low debt country and the common Central Bank incur either small losses or gains. The analysis also considers a scenario where the high debt country is subject to a slippage of fiscal discipline and a scenario in which financial markets concerns with debt stabilization are increased. Both cases imply that the timing of debt stabilization is altered: in the first case debt stabilization is slowed down, in the second case with more market discipline, players speed up debt stabilization as postponing adjustment now and the near future leads to a much larger burden later on. Section 2 considers some relevant literature on government debt stabilization, endogenous risk premia and Eurobonds. Section 3 provides our analytical framework: dynamic debt stabilization games in a monetary union with endogenous risk premia. Section 4 provides a number of analytical insights on non-cooperative and cooperative equilibria in the dynamic debt stabilization game. Section 5 provides a detailed numerical example. The conclusion summarizes some main results and points out the policy implications.",Debt stabilization games in a monetary union: What are the effects of introducing eurobonds?,https://www.sciencedirect.com/science/article/pii/S0164070418303069,March 2019,2019,Research Article,292.0
"Barnett William A.,Wang Chan,Wang Xue,Wu Liyuan","Department of Economics, University of Kansas, Lawrence, KS, USA,,Center for Financial Stability, New York, NY, USA,School of Finance, Central University of Finance and Economics, Beijing, China,Institute of Chinese Financial Studies, Southwestern University of Finance and Economics, Chengdu, China,Guanghua School of Management, Peking University, Beijing, China","Received 25 May 2018, Revised 19 November 2018, Accepted 22 November 2018, Available online 22 November 2018, Version of Record 7 December 2018.",https://doi.org/10.1016/j.jmacro.2018.11.006,Cited by (2),"What is the appropriate ==== should be followed when a negative energy endowment shock hits the economy. In our extended model in which a representative energy producer in the home country inputs domestic non-energy consumption goods to produce energy to meet the union-wide demands, we also find that the core inflation Taylor rule should be followed when a negative energy productivity shock occurs. In addition, whichever monetary policy rule the monetary policymaker chooses, the welfare level of home households is always lower than that of foreign households following a negative energy productivity shock.","Recently, substantial volatility in energy prices has inspired debate about how monetary policymakers should conduct monetary policy.====As is well known, substantial energy price hikes produce a stark divergence between headline and core inflation rates.====Which inflation rate is the appropriate target for monetary policymaking? Opinions among researchers and policymakers are divided on this subject.====Researchers have long considered core inflation to be the most appropriate target for monetary policymakers. In a New Keynesian monetary model, Aoki (2001) demonstrates that targeting sticky-price inflation can achieve stabilization of relative price around its efficient value, thus targeting the core inflation rate can be taken as one of the appropriate goals of monetary policy. In a multi-sector model, Mankiw and Reis (2003) show that the monetary policymaker should put more weight on sectors that have sluggish price adjustment. Bodenstein et al. (2008) also find that, for both the operational conduct of monetary policymakers and their communication strategies, monetary policy responding to a forecast of core inflation performs better to stabilize the economy. By contrast, Anand et al. (2015) find that, for developing countries with incomplete financial markets, targeting the core inflation rate cannot stabilize output and improve welfare. By comparison, targeting the headline inflation rate can achieve a higher welfare level. In practice, when framing the objectives and conducting monetary policy, both the Bank of England and the European Central Bank target the headline inflation rate. However, other central banks, including the U.S. Federal Reserve, pay more attention to the core inflation rate, at least in describing their operational decisions.====Since the European Central Bank targets the headline inflation rate, one question naturally arises: is it always an optimal choice for the European Central Bank to target the headline inflation rate?==== In a two-country currency union model, Benigno (2004) finds that, when two countries have the same degree of price stickiness, it is optimal for the monetary policymaker to target a weighted average of the inflation rates in the union, with the weights being equal to the sizes of the countries. Though he makes substantial progress towards answering the question facing the European Central Bank, he makes no distinction between the core inflation rate and the headline inflation rate. Thus, the conclusion obtained in Benigno (2004) cannot act as a clear rationale to guide the monetary policy choice facing the European Central Bank.====To fill the gap, building on Benigno (2004), we distinguish the core inflation rate from the headline inflation rate by introducing the energy into consumption and production. We then investigate which inflation rate performs better from the perspective of welfare comparisons. We find that, when the monetary policymaker chooses to target zero inflation rates for the whole union, the ""divine coincidence"" breaks down. The reason is that the terms of trade cannot adjust efficiently in response to asymmetric shocks.====Following the literature, we derive the welfare loss function of the monetary policymaker and find that it depends upon the inflation rates of the non-energy consumption goods, the union-wide output gap, the terms of trade gap, and the real marginal cost gaps. It is well known that the output gap and the inflation rates of the sticky-price sectors are the sources of distortion in a New Keynesian monetary model. As mentioned previously, in a two-country currency union model, following asymmetric shocks, the terms of trade cannot adjust efficiently and thus cause the welfare loss. In addition, when energy price is flexible, the price stickiness in the other sector prevents energy from being allocated efficiently not only between households and firms but also between firms in different countries. Accordingly, the real marginal cost gaps appear in the welfare loss function.====We use the welfare loss function to evaluate two different Taylor-type monetary policy rules to answer the following question. In a currency union, what is the appropriate target for the monetary policymaker, the core inflation rate or the headline inflation rate? Under the first regime, the interest rate is used to target the headline inflation rate. We call it the headline inflation Taylor rule (HIT). Under the second regime, the interest rate is used to target the core inflation rate. We call it the core inflation Taylor rule (CIT).====We find that which monetary policy rule performs better depends on the source of the shocks. Specifically, when the whole union is buffeted by productivity shocks, CIT regime, compared with HIT regime, amplifies the shocks thus causes a greater welfare loss. In this circumstance, the European Central Bank should target the headline inflation rate. However, when a negative energy endowment shock hits the economy, a greater increase in the interest rate under HIT regime amplifies the shock and results in a greater welfare loss. Thus the European Central Bank should target the core inflation rate.====The above conclusions are drawn in a model representing the European Monetary Union in which most countries import energy thus they face a common energy endowment shock. However, since some states in the U.S. produce energy while others do not, the readers may wonder whether our conclusions can be carried over to a model of regional heterogeneity in the sense that one country produces energy while the other does not. Therefore, we follow Nakov and Pescatori (2010) and extend the baseline model by introducing a representative energy producer in the home country which inputs domestic non-energy consumption goods to produce energy to meet the union-wide demands.====When energy price goes up following a negative energy productivity shock occurring, the non-energy consumption goods producers in the home country curtail outputs and employ fewer workers than before no matter which monetary policy rule the monetary policymaker chooses. By comparison, the non-energy consumption goods producers in the foreign country expand outputs and employment under CIT regime while reduce outputs and employment under HIT regime. It implies that the welfare level under CIT regime is higher than that under HIT regime and the monetary policymaker should follow the CIT. In addition, under HIT regime, the degrees of reduction in home non-energy consumption goods outputs and employment is larger than their foreign counterparts, thus home country's welfare level falls short of that of the foreign country under both monetary policy regimes.====The rest of the paper is organized as follows. Section 2 provides some empirical motivation. Section 3 lays out the model. Section 4 derives the steady state, the flexible-price equilibrium, and the sticky-price equilibrium. Section 5 performs the monetary policy analysis. Section 6 extends the model to introduce regional heterogeneity. Section 7 concludes.",What inflation measure should a currency union target?,https://www.sciencedirect.com/science/article/pii/S0164070418302325,March 2019,2019,Research Article,293.0
Saibene Giacomo,"Universitá Bocconi, Milano, Via Sarfatti 25 20136, Italy","Received 21 December 2017, Revised 2 November 2018, Accepted 19 November 2018, Available online 20 November 2018, Version of Record 27 November 2019.",https://doi.org/10.1016/j.jmacro.2018.11.004,Cited by (3),"Since the 2000s, the non-financial corporate sector moved from net borrower to net lender in many advanced economies - what has been labelled the corporate saving glut. Using data on U.S. listed firms, I document that the firms behind this widespread pattern are the largest corporations. The glut is the consequence of a constant profit share, relative to total corporate assets, which is larger than the sum of a decreasing investment share and a small and constant dividend share. In addition, I find no explanation able to empirically account for this pervasive phenomenon: neither deleveraging, nor increased uncertainty, nor increased market power are meaningfully correlated with the emergence of the glut at the firm-level.","Net lending/borrowing is a ==== measure that is defined as saving less investment/consumption. In the corporate sector, net lending corresponds to undistributed profits less dividends (which results in ====) less investment (which results in ====).====Corporations are often ==== considered as net borrowers in the economy. In principle, a firm should raise funds on the market, install productive capital, and obtain a stream of future cash flows, which will allow them to pay back the funds initially borrowed. At an aggregate level, the presumption is that there are always many more firms raising funds than paying them back. However, in the recent years, the corporate sector turned from net borrower to net lender, in many advanced economies. This macroeconomic fact has been labeled the “corporate saving glut”; see Loeys et al. (2005) and Gruber and Kamin (2015).====First of all, this article defines and identifies net lending/borrowing from firm-level data, using Compustat, showing that what we see on aggregate corresponds to what we see among publicly listed companies. Second, it identifies which are the firms that drive the glut: the largest corporations. Indeed, it is an aggregate phenomenon, with no peculiar observable variable that accounts for it, and widespread across all firms, encompassing all economic sectors. Finally, this article focuses on three different explanations that could account for the glut, yet without finding any positive answer.====The glut has emerged in the U.S. corporate sector during the 1990s and, most prominently, in the 2000s. Its rise can be linked, by construction, to the different long-term evolution of three corporate shares: a profit share that is broadly constant and larger than the sum of a decreasing investment share and a small and constant dividend share, resulting in the corporate sector moving from net borrower to net lender. Accordingly, the glut might be viewed as the consequence of the joint behavior of these three different shares.====On the one hand, the profit share has been remarkably stable. This is reminiscent of the capital share of an economy in which aggregate production is Cobb-Douglas. In fact, if anything, both the capital share and the capital stock increased over time - which happens if production is CES with an elasticity larger than one; see e.g. Barkai (2017), Karabarbounis and Neiman (2014), or Piketty and Zucman (2014). On the other hand, the investment share has shown an impressive decline, whose causes are still debated; see e.g. Gutiérrez and Philippon (2017). For instance, it might be that, as in many growth models, the representative firm has accumulated enough capital so that investment has decreased to a level just enough to replace depreciating capital. Finally, the dividend share is instead both relatively small and stable over time, which is in accordance with a reluctance of corporations to change their pay back policies. This behavior is referred to as dividend stickiness or smoothing, which is one of the most well-known documented phenomenon in corporate financial policy, although there is not yet a consensus on the causes, for which there is a number of different explanations; see Leary and Michaely (2011) and references therein. In particular, there has been even an increase in the degree of dividend smoothing over the last decades, even after accounting the increasing use of share issuances and buybacks.====Of course, there might be a very specific reason behind the glut, but we must bring evidence about it - and I strongly believe that even providing negative evidence about something is a useful contribution. This article aims to do exactly that, focus on three major potential motives of the glut:====The first motive presumes that a positive saving flow is the mechanism through which firms reduce their debt outstanding. This might be caused by an increasing reliance on intangible capital, which, together with financial constraints, depresses borrowing and increases saving as firms cannot pledge it as collateral to fund outstanding or new debt; see e.g. Falato et al. (2013). In recent years, we have even witnessed an increasing number of zero-leverage firms; see Strebulaev and Yang (2013) and also Graham et al. (2015). However, the set of firms relying on intangible capital is in fact different from the set of firms responsible for the glut. Indeed, it is the relatively small firms that rely more on R&D expenditures and, plausibly, on intangible capital; firm size is indeed a widely-used proxy for borrowing capacity. Yet, these same small firms are in fact net borrowers, whereas net lending is associated to the largest listed firms - for which it is difficult to believe that they are financially constrained in any meaningful way. This is apparent also from aggregate data behavior: aggregate corporate leverage has remained quite constant over time, which is inconsistent with the emergence of the saving glut for a deleveraging motive.====The second motive is about another conventional perspective: saving as a way to postpone investment or consumption. In particular, this links to the long-term decline in investment rates; see e.g. Furman (2015) or Gutiérrez and Philippon (2017). This also opens the questions on the investment determinants: for instance, Kothari et al. (2015) discuss the predominant role of profits growth and stock returns in predicting investment, while many other variables, such as interest rates, seem to be quite irrelevant, or Gennaioli et al. (2016) discuss the role of expectations in determining investment, which seem to be more adaptive than rational. Moreover, a precautionary motive can also be related to the long-term accumulation of cash and liquid assets by corporations, on which many articles focused; see Bates et al. (2009), Riddick and Whited (2009), or Falato et al. (2013). However, notice that “savings” (the stock) is conceptually different from “saving” (the flow): for example, corporations willing to strengthen their liquidity position could issue long-term liabilities and acquire liquid assets, without any change in their net lending positions; or corporations willing to deleverage could decrease investment relative to saving and use these resources to repay debt, but with no change in their cash holdings. Indeed, the accumulation of cash holdings seems to be related to risk and/or non-collateralizable assets in small firms that pay no dividends; on the other hand, the increase in net lending is about large corporations that pay dividends with no much firm-risk.====The third motive focuses instead on a more uncommon perspective: firms with enough market power can potentially invest less to maintain their dominant position, while accumulating assets with their saving flows.==== This links to recent articles that point to a broad rise in market power in the U.S. economy; see e.g. Council of Economic Advisers (2016), Ohlhausen (2016), Grullon et al. (2017), Barkai (2017), or Autor et al. (2017). Furthermore, there is also evidence about predatory behavior and the accumulation of liquid assets; see Bolton and Scharfstein (1990), Fresard (2010), and Frésard and Valta (2016).",The corporate saving glut,https://www.sciencedirect.com/science/article/pii/S0164070417304950,20 November 2018,2018,Research Article,294.0
"Jump Robert Calvert,Levine Paul","Department of Accounting, Economics and Finance, University of the West of England, United Kingdom,School of Economics, University of Surrey, United Kingdom","Received 31 May 2018, Revised 14 November 2018, Accepted 15 November 2018, Available online 20 November 2018, Version of Record 6 December 2018.",https://doi.org/10.1016/j.jmacro.2018.11.002,Cited by (22),"This paper provides a bird’s eye view of the behavioural New Keynesian literature. We discuss three key empirical regularities in macroeconomic data which are not accounted for by the standard ====, namely, excess ====, stochastic volatility, and departures from rational expectations. We then present a simple behavioural ==== that accounts for these empirical regularities in a straightforward manner. We discuss elaborations and extensions of the basic model, and suggest areas for future research.","The New Keynesian three equation model has defined monetary policy orthodoxy since the 1990s. Important contributions to the literature include Goodfriend and King (1997), McCallum and Nelson (1997), Clarida et al. (1999), and Woodford (2003). Large scale New Keynesian models, building on the contributions of Smets and Wouters (2003) and others, became increasingly popular at the turn of the millennium. These models are now used in a variety of central banks and policy institutions, guiding monetary policy and shaping forecasts around the world. Among other things, the New Keynesian orthodoxy is closely associated with inflation targeting and central bank independence.====Around the same time that the New Keynesian three equation model was emerging, economists working in various fields began to explore formal models of bounded rationality and heterogeneous expectations. An important early paper is Evans and Ramey (1992), which embeds costly expectation technologies into a simple macroeconomic model. The authors demonstrate that positive costs of prediction can prevent the emergence of rational expectations equilibria, as the increase in utility associated with accurate prediction may not be worth the costs of accessing those predictions. Their model builds on the earlier theories of consistent and inconsistent learning pioneered in the 1980s.====Evans and Ramey (1992) argue that the optimality of rational expectations is suspect if the costs of their formulation are not taken into account. Essentially, the imposition of rational expectations on macroeconomic models becomes the ultimate free lunch, as agents are endowed with a considerable amount of information at zero cost. Instead, the authors propose that, “any model of expectation formation must begin with an appropriate specification of expectational preferences and technology, if the model is to be regarded as representing rational, optimizing behavior” (ibid., 208). This approach to expectations clearly has the potential to affect theories regarding the conduct of monetary policy, and the authors show in their simple model that long run non-neutrality and hysteresis effects can occur when forecasting is costly.====A considerable advance in the literature on bounded rationality came in Brock and Hommes (1997), which embeds a simple heterogeneous expectations mechanism into a cobweb model of partial equilibrium. As in the standard cobweb model, firms have to forecast the equilibrium price before they set their output level. To do so, they have the choice of using a simple adaptive expectations predictor at zero cost, or perfect foresight at positive cost. The authors argue that firms will choose predictors that result in higher net profits, where the probability of choosing a given predictor is determined by a logit model. This choice is justified by an appeal to the discrete choice model described in Manski and McFadden (1981), which is widely used in microeconomics and econometrics. A similar approach is used in the reinforcement learning literature described in Young (2004).====The insights of Brock and Hommes (1997) were slowly incorporated into the New Keynesian literature in the early 2000s, mainly in the work of William Branch and his co-authors. Branch and McGough (2004) studied the impact of heterogeneous expectations on the existence of sunspot equilibria in rational expectations models, and Branch and Evans (2007) examined discrete choice dynamics of the form considered in Brock and Hommes (1997) in the context of a simple macroeconomic model. Heterogeneous expectations in a New Keynesian framework, albeit without discrete choice, were then examined in considerable detail in Branch and McGough (2009). Heterogeneous expectations and discrete choice were fully incorporated into the New Keynesian three equation model around the time that the USA and Europe were recovering from the effects of the 2008 financial crisis, in the papers of Branch and McGough (2010) and De Grauwe (2011).====The basic framework set out in Branch and McGough (2010) and De Grauwe (2011) has become known as the behavioural New Keynesian model. This terminology is largely the result of De Grauwe’s book length treatment of the subject, ==== (De Grauwe, 2012). Via the expectational mechanism of Brock and Hommes (1997), the behavioural New Keynesian (BNK) approach incorporates bounded rationality and heterogeneity into the standard New Keynesian (NK) three equation model. This embeds an intuitive form of complexity into the standard approach, where strategy switching generates recurring bouts of instability. Specifically, households and firms have the choice between two (or more) predictors of output and inflation. Around the steady state of a BNK model, both predictors are equally accurate, but one predictor becomes increasingly accurate relative to the other as the economy moves away from the steady state. Any exogenous shock that moves the economy away from the steady state can then lead to agents rapidly switching from one predictor to the other. This creates an endogenous amplification effect which can explain the existence of excess kurtosis and stochastic volatility observed in macroeconomic data.====The purpose of the present paper is to provide the reader with an accessible introduction to the BNK approach, by,====Our approach is therefore to emphasise and explain the connections between existing papers in the BNK literature, and draw together the dispersed empirical results which currently exist. Thus the paper serves a synthetic purpose, amalgamating and comparing existing results, which allows us to identify future research priorities. As argued in Dilaver et al. (2018), we are of the opinion that the BNK model successfully addresses a number of the criticisms levelled at the standard NK model since the crisis, making it an important contender for a new consensus. We hope that the present paper will increase its visibility, and encourage further research in the area.====In Section 2 we discuss the main empirical facts that BNK models have sought to explain. We then present the simple BNK model in Section 3, which illustrates the basic mechanism described above in a straightforward manner and accounts for the empirical facts discussed in Section 2. We then present a bird’s eye view of the BNK literature in Section 4, including a detailed discussion of the basic model and a survey of the various extensions. Finally, Section 5 suggests a number of areas that require further research.",Behavioural New Keynesian models,https://www.sciencedirect.com/science/article/pii/S016407041830243X,March 2019,2019,Research Article,295.0
Maruta Admasu Asfaw,"Centre for Applied Finance and Economics, Business School, University of South Australia, G.P.O. Box 2471, Adelaide, South Australia 5001, Australia","Received 23 March 2018, Revised 4 November 2018, Accepted 19 November 2018, Available online 20 November 2018, Version of Record 27 November 2019.",https://doi.org/10.1016/j.jmacro.2018.11.003,Cited by (19)," based on the fact that donors provide large amount of aid targeted to financial sector to countries which have similar voting positions in the United Nations General Assembly. The analysis suggests that aid targeted to financial sector has a significantly positive effect on financial development. This result is robust with different sensitivity checks such as using alternative measures of financial development, measures of aid, estimation methods and dropping outliers. The finding has strong policy implications for donor countries and international aid organisations.","The amount of aid targeted to financial sector has increased significantly in the recent years, from US$ 57.9 million in 2000 to 115 million in 2010.==== The main aim of this type of aid is to help developing countries in fostering their financial development. Despite the clarity of this aim, the evidence on the effectiveness of aid provided to financial sector in promoting financial development is still insufficient. The effectiveness of foreign aid in developing countries is a topic that has kept researchers in economics busy in the recent years. The literature on aid effectiveness has produced mixed results as to whether foreign aid has really been effective in achieving its desired goals. Some of the studies (such as Collier and Dollar, 2003, Basnet, 2013, Headey, 2008, Ekanayake and Dasha, 2010) find that aggregated aid has a significantly positive effect on economic growth of the recipient countries. On the other hand, some of the studies such as Djankov et al. (2008) show that aggregated aid causes various social, political and economic instabilities in the recipient countries. For example, it aggravates corruption, civil conflicts, dependency syndrome and it reduces the level of domestic production. In addition, Easterly (2003) argues that aid is sometimes helpful, however in general, it causes deterioration in business and financial sectors. Moyo (2009) suggests that aid is not the solution for problems associated with economic, social and political problems in developing countries; rather it is a burden for the recipient countries.====What both camps tend to ignore is that different types of aid are unlikely to have the same economic effects on recipient countries. In much of the literature, it is still common to run panel regressions with aggregate aid flows as the explanatory variable. Recently, however, the question of aid effectiveness has shifted from “Is aggregated aid helpful?” to “Which type of aid is more helpful?” For example, work by Clemens et al. (2004) on short-impact aid has started a shift toward using disaggregated aid data. It is open to debate whether a decision on the effectiveness of aid can be reached at all if the focus is restricted to examine the effect of aggregated-aid on economic growth. Donors have repeatedly stressed that they pursue multiple aims when delivering aid (see, for example, Isenman and Ehrenpreis, 2003). Specific-purpose aid intended to support donors’ policy statements, including the improvement of financial sector in developing countries, tends to deviate from the analyses narrowly focused on aggregate-aid-growth nexus.====For empirical support, almost all researchers (both from opponents and proponents camps) often turn to the aid-growth nexus literature. This literature, however, is exposed to the daunting challenge of finding a significant impact of aid due to both endogenous relationship between aid and growth and the complexity of the effect of aid on economic growth of the recipient countries. In general, aid and economic growth are too distantly linked (with several channels in between) to be able to detect any significant relationship in the data. Against this backdrop, it seems appropriate to pursue a different avenue for assessing the effectiveness of aid. Since aid targeted to financial sector and financial development are more closely linked, the relationship between the two may be easier to detect statistically. Thus, this paper focuses on more specific outcome variables than growth. It uses disaggregated aid data to investigate the link between aid granted to the financial sector and financial sector outcome variables.====Financial sector development in developing countries is part of the private sector development strategy to stimulate economic growth and reduce poverty. Donors have committed themselves to helping developing countries achieve sound financial systems. To this end, donors have allocated an increasing amount of aid to the financial sector particularly after 1990s (see Fig. 1).====Given the suggestive evidence of significant impact of sector-specific aid on outcomes the sectors for which aid is being allocated, and the ongoing heated debate over aggregated-aid effectiveness, it is important to understand the relationship between aid targeted to financial sector and financial development. Despite the burgeoning empirical literature considering the effect of aggerated-aid on economic growth, there is no (to my knowledge) systematic empirical evidence on how aid targeted to financial sector affects financial development. This is surprising, given donors provide an increasing amount of aid targeted to financial sector of developing countries. The main objective of this analysis is to examine the effect of aid committed for financial sector on developing countries using comprehensive datasets from 70 aid-receiving countries covering an extending period: 1980–2016. To the best of my knowledge, this study presents the first analysis delving the relationship between aid targeted to financial sector and financial sector.====Private credit provided by the banking sector is the primary financial development indicator for the following reasons. First, data on private credit provided by the banking sector are available for many countries and extended period. Second, the extant literature considers private credit as the most important banking development indicator because it shows the presence of good opportunities for new firms to obtain bank finance (see Baltagi et al., 2009, Rajan and Zingales, 2003). Hence, private credit is a proper indicator of financial development of an economy.====This paper considers three identification strategies to probe the effect of aid targeted to financial sector on financial development. First, it uses ordinary least squares (OLS) with a set of control variables that pools all country–year observations. The second strategy is two stages least square (2SLS) in which the aid variables is instrumented by an index measuring the voting similarity between donors and recipient countries in the United Nations General Assembly. Finally, the third strategy is based on a dynamic panel data model Generalized Method of Moments (GMM) proposed by Blundell and Bond (1998). In this study, all predetermined and endogenous variables are instrumented by their appropriate lags. This method potentially eliminates spurious correlation between these variables and the error term. GMM needs a sufficiently large number of observations to generate consistent. This study, therefore, depends on significantly broader datasets obtained from 70 countries spanning the period 1980–2016.====The main finding is that aid targeted to financial sector significantly promotes financial development in the recipient countries. An increase of US$1 per capita financial sector aid contributes an average enhancement of 3.22% in financial development of the recipient countries. This result is robust to the battery of robustness checks including alternative measures of financial development, alternative measures of aid, alternative method of estimation and to the inclusion of a set of control variables.====The remainder of the paper is organized as follows. Section 2 presents the review of related literature. Section 3 shows the expected pathways through which aid targeted to financial sector affects financial development. Section 4 presents data description. Section 5 discusses empirical strategies. Section 6 presents the results on the relationship between aid targeted to financial sector and private credit. Section 7 shows several alternative specifications that show the robustness of the benchmark findings. Section 8 concludes.",Can aid for financial sector buy financial development?,https://www.sciencedirect.com/science/article/pii/S016407041830123X,20 November 2018,2018,Research Article,296.0
Mavrodimitrakis Christos,"King's College London, King's Business School, Bush House, 30 Aldwych, Strand Campus, WC2B 4BG, UK,Copenhagen Business School, Denmark","Received 9 April 2018, Revised 21 July 2018, Accepted 11 November 2018, Available online 14 November 2018, Version of Record 6 December 2018.",https://doi.org/10.1016/j.jmacro.2018.11.001,Cited by (2),"This paper utilises a static representation of a reduced-form Dynamic Stochastic General Equilibrium (DSGE) model to deal with strategic fiscal/monetary policy interactions in a core-periphery monetary union, in which the periphery member's fiscal authority is always the follower (except in the EMU benchmark where all policymakers move simultaneously). In such a set-up, we examine the policy mix of the alternative institutional arrangements of (i) non-cooperation between the two leading authorities, the lead fiscal authority and the monetary authority; and (ii) in a regime of cooperation between the leaders; and (iii) in a regime of fiscal leadership in which the monetary authority moves between the two fiscal authorities. We explore the welfare implications of these alternative institutional arrangements for the monetary authority, the two fiscal authorities, the social planner, and provide a ranking. Our main results are: (i) the lead fiscal authority's ability to contribute less counter-cyclically increases with cooperation at the core, while it is unchanged for explicit or implicit cooperation system wide; (ii) monetary leadership provides no advantage over no cooperation in the core; (iii) the ranking for the core member's fiscal authority is shock independent and favours a cooperative strategic regime, and then fiscal leadership; (iv) the ranking for the other authorities and the social planner is shock dependent, and can either coincide with or be the exact opposite of the core member's ranking; (v) the ranking for the peripheral fiscal authority always coincides with the social planner's; and (vi) for common supply shocks, all the policy authorities and the social planner are in favour of the cooperative strategic regime.","This paper deals with the interaction of fiscal and monetary policies in a monetary union, with a clear reference to the Economic and Monetary Union (EMU) in Europe. The main characteristic of this setup is that monetary policy is centralised, conducted by a single monetary authority, the European Central Bank (ECB), while fiscal policy is decentralised in that there are many fiscal authorities, each responsible for fiscal policy at the country-specific level. However each fiscal authority is subject to the deficit constraint imposed by the European Union (EU)’s Stability and Growth Pact (SGP), refined by the Fiscal Compact==== (FC) agreement, and to a debt constraint (European Council, 2012). In this setup, various coordination problems and conflicts of interest arise. Typically, the theoretical literature assumes strategic behaviour on the part of the authorities and investigates the policy interactions in a game-theoretic context; so, through policy games (Hughes Hallett, 1986a, Hughes Hallett, 1986b; Beetsma and Bovenberg (1998); Dixit and Lambertini, 2001, Dixit and Lambertini, 2003a, Dixit and Lambertini, 2003b).==== But, as the Eurozone sovereign debt crisis that followed the Great Recession clearly showed, there is an imperfect institutional structure in EMU for dealing with the various shocks that occurred. This has created renewed interest in the theoretical literature on strategic policy interactions in monetary unions (Foresti, 2018).====Beetsma and Giuliodori (2010) point out that the ‘traditional’ Optimum Currency Area (OCA) theory of Mundell (1961) and McKinnon (1962) largely ignored the implications of monetary unification for fiscal policymaking; while the fiscal authorities, acting as decentralized players, may employ their fiscal policies in a strategic way. This requires a framework to model the strategic interactions of fiscal/monetary policies in a monetary union, mainly in terms of authorities’ objectives, their ability to commit, and the sequencing in the game. According to Fragetta and Kirsanova (2010, p. 856), ====. These authors further point out that an analysis of the potential strategic interactions between fiscal and monetary authorities is crucial, since the assumption of complete cooperation is seldom realistic==== and different objectives or priorities can lead to conflicts between the authorities.====In this paper, we investigate the policy mix in a monetary union in a strategic context, under alternative institutional arrangements and various shocks in the absence of wage-price flexibility, labour mobility and fiscal transfers. The focus is on macroeconomic stabilisation and the welfare implications of fiscal-monetary policy interactions at both national and union levels. A policy-mix problem appears when policies, in particular the common monetary policy relative to fiscal policy, conflict over how best to resolve the business cycle (Andersen, 2008). The institutional arrangements will of course determine how monetary and fiscal policies interact. And that will depend on the sequencing of decisions in the game (simultaneous moves vs. policy leadership or cooperation), the degree of cooperation among monetary and fiscal authorities, their objectives, and the constraints imposed by the economy.====Typically, the theoretical literature that studied strategic policy interactions in a monetary union, following the creation of the EMU, embraces the fiscal leadership strategic regime (see Beetsma and Bovenberg (1998), Uhlig (2003), Andersen (2005, 2008), Ferre (2008, 2012)). The national fiscal authorities move in a non-cooperative simultaneous-move manner at the beginning of the game, while the monetary authority follows. According to Andersen (2008), this works because monetary strategy is made credible and clear, and fiscal decisions precede the monetary decisions. Thus, if the monetary policy objectives are well-known to the fiscal authorities and the policies predictable, then fiscal policymakers can consider the possible monetary threats to their fiscal decisions on fiscal policy. In that way, monetary policy acting as follower can discipline fiscal policies (Libich and Stehlík, 2012). Put differently, the fiscal authorities have first-mover advantage since once fiscal policy is decided, they cannot react to monetary policy.==== This rigidity assumes a degree of stickiness or persistence; and applies when debt is a target or constraint since debt is a stock, but the policies a flow. The authority creating the greater persistence necessarily moves first since there will come a point when it cannot react to retaliations from the low persistence player to the ==== decision. In contrast, Kirsanova et al. (2005) assume a simultaneous-move regime on the argument that, coalitions apart, the fiscal authorities are too many to be considered as leaders in their game with the ECB. But the core may be a coalition, especially if it cooperates with the ECB.====There is little firm empirical evidence on strategic interactions and the sequence of moves between monetary and fiscal authorities. Fragetta and Kirsanova (2010) provide an empirical analysis of policy interactions in the US, UK and Sweden to identify potential leadership regimes. The authors specify a small-scale structural general equilibrium model, like ours, of an open economy and estimate it using Bayesian methods. Their main finding is that fiscal leadership can best describe the actual regime for the UK and Sweden, instead of a simultaneous-move game for the US. Hughes Hallett and Lewis (2015) estimate Taylor rules for anticipated fiscal decisions to condition ECB monetary decisions using real-time data. They find that the fiscal and monetary policies appear to conflict ex-ante, but switch to accommodative ex-post – which may imply monetary leadership or cooperation to ensure long-run sustainability.====In what follows, we consider an alternative approach with a formalised core-periphery union. This allows us to consider a core leadership game in which there are two leading authorities, the core's fiscal authority and a single monetary authority, and a follower fiscal authority in the periphery. In this world, we examine the outcomes from cooperation between core and monetary authority. We compare the results in terms of policy mix and overall welfare to four alternative regimes: (i) a simultaneous-move strategic regime that assumes no cooperation at the core; (ii) a simultaneous-move regime where all three policy authorities move together (the benchmark for comparisons); (iii) a cooperative regime at the core, where the core member's fiscal authority cooperates with the monetary authority explicitly; and (iv) a fiscal leadership regime, in which we assume that the core member's fiscal authority leads vs. the monetary authority, and hence both lead the periphery – this amounts to implicit cooperation.==== We further note that monetary leadership delivers the same results as no cooperation among the leaders (regime (i)).====The asymmetry in the sequence of moves between the two fiscal authorities rests on the core-periphery assumption, where the lead fiscal authority represents a core member-state. In such a setting, the core member's fiscal authority retains an informational advantage vs. the periphery which might reflect an unequal distribution of power in the monetary union (Chortareas and Mavrodimitrakis, 2016).==== This means that the peripheral member-state chooses its fiscal stance based on prior observation of the choices made by the core member fiscal authority and the monetary authority. There are various ways this assumption can be justified. One explanation might be that countries in the core provide some form of ‘political hegemony’ in the union, in that they, together with the ECB, act pre-emptively to provide economic leadership to the rest of the Eurozone. Another explanation might be that this fiscal sequential asymmetry captures the ability to commit to policy actions. A third is that core economies have an information advantage in terms of knowing what the other policymakers will do and/or what bail out plans may exist. In any event, it seems reasonable for the peripheral member's fiscal authority to react to the choices made at the core of the monetary union; especially when those member-states are small relative to the core==== (so they need to adapt or free ride), or have high levels of past debt.====Our point of departure is to examine fiscal vs. monetary leadership as implicit coordination devices in an incomplete economic union like the Eurozone, starting from the ideas in Dixit and Lambertini, 2003a, Dixit and Lambertini, 2003b, Hughes Hallett and Weymark, 2007, and the idea that coordination might be counterproductive as a result of de facto institutionalised coalitions (Rogoff, 1985). Rogoff (1985) shows that the cooperation of two monetary authorities can be counterproductive if there is a third party (the private sector) that does not cooperate. We then investigate whether giving the lead to the core member's fiscal authority relative to the monetary authority could be welfare-improving. Beetsma and Debrun (2004) argue that coordination in a currency union can be horizontal, across governments, and vertical, between fiscal and monetary authorities. In our core-periphery monetary union, vertical coordination is explored at the core where it can be either implicit or explicit (fiscal leadership represents implicit coordination). Foresti (2018) stresses that vertical coordination should be of no interest in EMU since the ECB is not allowed to coordinate its policies with the fiscal authorities. However, the author points out that ==== (Foresti, 2018, p. 235). Hence, we explore the conditions under which the monetary authority might find it beneficial to cooperate with the core member's fiscal authority, and vice versa.====Fragetta and Kirsanova (2010) argue that fiscal authorities in European countries might be in different leadership relationships with the ECB, since they are bigger and more independent than fiscal authorities in the US. Knowing that fiscal authorities can affect economic performance we need to explore institutional designs that accommodate that fact. Our paper is a first attempt to provide the necessary theoretical analysis since, in each policy game, the fiscal authorities are set in a different strategic relationship with the monetary authority. The peripheral member's fiscal authority always plays follower to the monetary authority; while the lead fiscal authority is either (i) in a simultaneous-moves non-cooperative game, or (ii) in a cooperative game, or (iii) has leadership over the monetary authority.====We use a standard two-country monetary-union model based on a static representation of a reduced-form DSGE model where the two countries are interconnected through trade, a free flow of investment, and a terms-of-trade effect. Attention is paid on the terms-of-trade effect, since that represents an important transmission channel within a monetary union. In our setting, supply shock asymmetries induce real exchange rate differences in the monetary union which affect aggregate demand and lead to a great source of imbalance. Hence the impact of supply shock asymmetries on the main macroeconomic variables at both the national and union levels is explored. We then consider cyclical stabilisation policies. The common central bank pursues a flexible inflation-targeting policy (Svensson, 1997), while national fiscal authorities care about the output gap and sustainable fiscal balances. These assumptions are common in various papers in the literature (see Uhlig (2003), Andersen (2008), Chortareas and Mavrodimitrakis (2016, 2017)).====Our main results can be summarised as follows: (i) that fiscal strategic advantage increases with cooperation in the core, while it is unchanged for either explicit or implicit cooperation; (ii) the ranking for the core member's fiscal authority is shock independent and favours a cooperative strategic regime and failing that fiscal leadership; (iii) the ranking for the other authorities and the social planner is shock dependent, and it can either coincide or be the exact opposite of the core member's ranking; (iv) the ranking for the peripheral fiscal authority always coincides with the social planner's; and (v) under common supply shocks, all policy authorities and the social planner favour the cooperative strategic regime.====To establish these results, the paper is structured as follows: Section 2 presents the benchmark model; Section 3 solves for the alternative institutional arrangements and provides equilibrium solutions for the main macroeconomic variables at both union and country-specific levels. Section 4 provides a welfare comparison of the alternative regimes with respect to the two fiscal authorities, while Section 5 provides a macroeconomic stabilisation and welfare analysis at the union level. Section 6 concludes.",Cooperation vs. leadership in a core-periphery monetary union: Inter-country vs. inter-institutional policy coordination,https://www.sciencedirect.com/science/article/pii/S0164070418301472,March 2019,2019,Research Article,297.0
Mand Matthias,"Department of Economics, Universität Mannheim, L7 3-5, Mannheim 68131, Germany","Received 19 July 2017, Revised 12 October 2018, Accepted 31 October 2018, Available online 5 November 2018, Version of Record 6 December 2018.",https://doi.org/10.1016/j.jmacro.2018.10.008,Cited by (8),", including the procyclicality of R&D activities. An implication of this particular specification of the R&D process is that productivity shocks amplify the business cycle by stimulating more ideas in good times.","Research and development are important sources of long-run growth. Cyclical swings in R&D result in fluctuating productivity improvements. Empirical evidence for the post-war U.S. economy shows that R&D activities are procyclical. Aggregate measures of real R&D expenditures and the employment of scientists and engineers tend to rise during booms and fall during recessions, according to most recent studies.==== This evidence suggests that recessions can hamper economic growth and depress productivity, even beyond the time span usually associated with the cycle.====Basic knowledge-driven models of R&D-based growth are not able to replicate the observed procyclicality of R&D. Rather, the theoretical work carried out by Aghion and Saint-Paul (1998) suggests that R&D activities are countercyclical when labor is the sole R&D input. The reason behind this is that business cycle fluctuations induce workers to move between goods production and R&D. During recessions, labor productivity within goods production is lower than during booms, whereas it is acyclical in R&D. This means that the opportunity cost of R&D, expressed in terms of foregone output, decreases in cyclical downturns. Consequently, workers move from production to R&D activities during recessions and vice versa during booms. This opportunity cost argument predicts that high levels of R&D during recessions foster growth. This is, however, at odds with the empirical evidence.====This paper studies the sources of cyclical fluctuations in R&D activities and their implications for economic growth. It focuses on the role of various R&D inputs. To this purpose, the analysis considers different R&D production technologies and also takes procyclical employment into account. The main objective is to test which R&D technology is able to account for the fact that R&D activities are procyclical and to discriminate them accordingly. More specifically, I ask whether a knowledge-driven model with endogenous labor supply is able to replicate the observed procyclicality of R&D. This contributes to settling the dispute about the role of labor supply. As my answer is negative, I consider an alternative specification for innovation that allows for multiple R&D inputs, including scientists, staff, and final goods.==== This specification contributes a new explanation for the procylicality of R&D, namely complementarity among R&D inputs. Then, I employ a calibrated version of this model economy to shed some light on the quantitative importance of the different mechanisms that determine the cyclicality of R&D. In particular, I investigate the range of the labor share in innovation for which R&D activities are pro- or countercyclical. The analysis also considers the consequences of business cycle shocks for the economy’s productivity trend. Finally, this paper makes an empirical contribution by providing new business cycle facts on R&D activities.====To address these issues, I develop a plain real business cycle model with endogenous innovation. The model economy is hit by shocks to productivity and government spending which cause business cycle fluctuations (cf. Christiano, Eichenbaum, 1992, Hansen, Wright, 1992). In particular, these shocks give rise to cyclical swings in R&D activities and affect, in turn, productivity growth. More specifically, the model generates R&D-based growth through horizontal innovations along the lines of Romer (1990). However, I extend the basic knowledge-driven growth framework in two respects. First, I consider endogenous labor supply, as suggested by Fatás (2000a). As a consequence, total employment expands in booms and contracts in recessions. This mitigates the opportunity cost effect. Second, and most importantly, I propose a specification for the R&D process that allows for multiple inputs which complement each other.==== Specifically, I not only consider labor and goods inputs, but I also distinguish two types of labor: scientists and supporting workers. As goods are scarcer in recessions than in booms, the complementarity of R&D inputs dampens the reallocation of labor implied by the opportunity cost argument. Similarly, employment of scientists, which tends to be procyclical in the data, weakens the opportunity cost channel. Thus, the multi-input specification introduces complementarity of R&D inputs as a new mechanism to explain the procyclicality of R&D. Moreover, this description of R&D is more realistic than the knowledge-driven or lab-equipment models studied in the literature. In fact, data provided by the National Science Foundation shows that labor and goods inputs are approximately equally important for R&D. As both extensions work against the opportunity cost effect, which effect dominates is ultimately a quantitative question.====The quantitative analysis shows first that introducing endogenous labor supply into the popular knowledge-driven model alone is not sufficient to replicate the cyclical patterns of R&D observed in the data. In particular, the knowledge-driven model struggles to generate procyclical R&D investment and procyclical R&D labor simultaneously. Rather, R&D labor tends to be countercyclical due to the opportunity cost effect. Thus, R&D output is typically countercyclical such that economic downturns would foster endogenous TFP. Moreover, due to its simplicity, the knowledge-driven specification of the R&D process cannot match the empirical values of the share of R&D investment in GDP and the number of scientists and engineers simultaneously.====The second key finding is that the calibrated multi-input specification is able to capture important business cycle patterns, including the cyclical properties of R&D activities. More precisely, both R&D investment and employment of scientists are procyclical. The reason for this is that the intersectoral reallocation of workers is much less relevant when the R&D process involves multiple inputs that complement each other. Further to this, the endogenous growth mechanism amplifies business cycle shocks. Therefore, the calibrated multi-input model implies that booms promote productivity growth. In sum, my findings indicate that the specification of the R&D process is crucial. It requires a careful modeling of the R&D inputs to replicate the cyclical properties of R&D activities. Specifically, under the baseline calibration, endogenous labor supply and R&D complementarities reinforce each other and overturn the intersectoral reallocation of workers caused by the opportunity cost effect.====This paper relates to both empirical and theoretical work on R&D activities. Most recent empirical studies for the U.S. find that real R&D expenditures are procyclical.==== This procyclicality is documented at both the aggregate level (Nuño, 2011, Barlevy, 2007, Comin, Gertler, 2006, Fatás, 2000) and the industry level (Ouyang, 2011, Barlevy, 2007), for total as well as industry-funded R&D, and for both National Science Foundation (NSF) and S&P Compustat data. The current paper documents a set of new business cycle facts related to R&D activities for the U.S. post-war economy. Using NSF data, it extends the well-known procyclicality result for R&D expenditures to several distinct R&D inputs. Unlike previous studies, the present paper employs the classical RBC approach, pioneered by Kydland and Prescott (1982).==== Furthermore, the RBC analysis is built on national accounts that are conceptually in line with the model. This requires R&D to be an investment rather than an expense so that R&D investment constitutes a component of GDP. To this end, I construct corresponding aggregate macroeconomic data, based on the BEA’s R&D Satellite Account (see Lee and Schmidt, 2010). Previous studies, however, are based on NIPA national accounting standards which do not count business spending on R&D for GDP.====The paper also adds to a growing theoretical literature on growth and business cycles which views cyclical fluctuations and productivity growth as closely interrelated. Many economists have recently studied the growth effects of business cycle shocks.==== Comin and Gertler (2006), Nuño (2011), Queralto (2015), for example, generate procyclical R&D by counterfactually considering only final goods as inputs into R&D. Therefore, short-run economic fluctuations are amplified by construction. Aghion and Saint-Paul (1998) propose a model of optimal productivity growth under demand fluctuations. Their favorite model specification makes use of an opportunity cost argument and, hence, predicts productivity-enhancing activities, such as R&D, to be countercyclical. This implication is, however, at odds with the data.====This paper is most closely related to studies which seek to resolve this discrepancy. Aghion et al. (2010) propose binding credit constraints. Barlevy (2007) employs fixed costs of production to generate procyclical profits that drive R&D. In contrast to this integral view of R&D, Comin and Gertler (2006) and Francois and Lloyd-Ellis (2009) consider multi-stage models of innovation in which the creation of knowledge is procyclical but its adaption to productive use may not be. Unlike previous studies which neglect distinct inputs into R&D, this paper models the R&D process in line with detailed evidence on R&D inputs from the National Science Foundation. My results suggest that complementarity among R&D inputs provides a new explanation for their observed procylicality. One merit of this approach is that the calibration procedure allows us to impose discipline on the mechanism, because the cyclical properties of R&D inputs are directly observable. Furthermore, the quantitative analysis contributes to the discussion about the role of labor supply (Barlevy, 2007, Fatás, 2000, Nuño, 2011).====The rest of this paper is organized as follows. Section 2 presents empirical evidence on the cyclicality of R&D activities. Section 3 sets up the model. Section 4 calibrates the model, discusses impulse responses, and reports the findings on the business cycle properties of R&D activities. Section 5 summarizes the paper and discusses the empirical relevance of the model’s main mechanism, as well as directions for future research.",On the cyclicality of R&D activities,https://www.sciencedirect.com/science/article/pii/S0164070417302914,March 2019,2019,Research Article,298.0
"Gobbi Lucio,Mazzocchi Ronny,Tamborini Roberto","Doctoral Programme in Economics and Management, School of Social Sciences, University of Trento, Trento, Italy,European Parliament, Brussels, Belgium,Department of Economics and Management, University of Trento, Trento, Italy","Received 5 May 2018, Revised 18 October 2018, Accepted 29 October 2018, Available online 31 October 2018, Version of Record 17 August 2019.",https://doi.org/10.1016/j.jmacro.2018.10.006,Cited by (19)," it, when the shock is large and/or when the reactivity of inflation expectations is sufficiently high. This latter finding seems to support the necessity, in those conditions, to abandon conventional monetary policy and to switch to an aggressive reflationary policy package that prevents the entrenchment of deflationary expectations.","In the aftermath of the Great Recession inflation dynamics in advanced economies has surprised many economists. On the one hand, just after the outbreak of the crisis there was a period of ====: based on historical data and given the depth of the recession, inflation should have declined much more than it actually did (International Monetary Fund 2013, Murphy, 2014, Blanchard et al., 2015). On the other hand, subsequently inflation has surprised for the opposite reason. Three bodies of evidence are relevant here.====First, since the end of 2011, the Euro Area, the United Kingdom and the United States have been experiencing a ==== away from the central bank's target. This has become more pronounced since 2013. Even greater surprise has been provoked by the fact that this process has been taking place in parallel with an unprecedented easing of monetary conditions to the limit of the zero lower bound (ZLB) of policy interest rates. Second, this concomitance has challenged the consensus on the worldwide “flattening” of the Phillips Curves pointing to their “steepening” (for the Euro Area see e.g. Riggi and Venditti, 2014, Bank of Ireland 2014, Oinonen and Palovitta, 2014). Third, direct investigation of expectation forecasts has shown both their downward drift and a significant under-estimation of actual inflation (Riggi and Venditti, 2014, Miccoli and Neri, 2015).====Overall, though episodes of strict deflation (negative inflation rate) have been limited (e.g. December 2014–March 2015, and February–May 2016 in the Euro Area) these phenomena have raised concern among central bankers about their ability to govern the dynamics of the price level by credibly anchoring expectations to the medium-term inflation target pursued by central banks. (Iakova, 2007, Kuttner and Robinson, 2010, European Central Bank 2014). In the words of the President of the European Central Bank (ECB) Mario Draghi,====“[...] The most fundamental question facing all major central banks today is this: can our price stability mandate still be delivered? Across advanced economies inflation is low and has been low for some time. And in several of those economies, long-term inflation expectations, based on market prices, remain below our numerical definitions of price stability. That has led some to question whether it makes sense for central banks to pursue expansionary policies to meet their inflation objectives. Are they fighting a futile battle against forces beyond their control?” (Draghi, 2016, p. 1).====This question has come to be known as the ==== of inflation expectations, and this problem plays a key role in the communication strategy of today's conventional and non-conventional monetary policies (Draghi, 2014a, b, Draghi, 2016, Yellen, 2014, Eggertsson and Woodford, 2004).====Independent empirical analyses support concern with the de-anchoring problem in various countries. Though common across advanced economies, the problem seems particularly pronounced in the Euro Area, confirming Ehrmann et al. (2015) claim that de-anchoring is more likely the more persistent is low inflation. In the empirical literature, the more common criterion used to test the anchoring of inflation expectations is based on the idea that firmly anchored expectations should be insensitive to the announcement of macroeconomic news (Gürkaynak et al., 2010, Fracasso and Probo, 2017). Survey-based analyses suggest that in recent years inflation expectations have shown some signs of de-anchoring, moving away from the ECB target even at the longer horizons.==== Lyziak and Paloviita (2016) report evidence of increased sensitivity of longer-term inflation forecasts to shorter-term forecasts and to actual HICP inflation, and find that the role of ECB inflation targets for those expectations has diminished in recent years. Buono and Formai (2016) show that the short-term inflation expectations of the professional forecasters interviewed by Consensus Economics started falling in 2012 and that its reduction implies a reduction by 0.3 percentage points of long-run expectations.====Market-based analyses==== largely confirm these results. By applying the multiple endogenous break point tests of Bai, 1997, Nautz et al., 2017 find that expectations in the Euro Area became less well anchored after September 2011. Using a news-regression approach to assess the sensitivity (or lack thereof) of expectations to the release of unexpected macroeconomic news, Fracasso and Probo (2017) find evidence that the de-anchoring of expectations started in December 2011 and never reversed. Symptoms have also been detected by Natoli and Sigalotti (2018): studying the pass-through of inflation expectations, they find that, since mid-2014, negative tail events affecting short-term inflation expectations have been increasingly channelled into long-term ones, and that this phenomenon has generated both downward revisions in expectations and upward shifts in uncertainty.====This evidence of de-anchoring of inflation expectations challenges standard macro-theory of conventional monetary policy based on agents' long-run rational expectation (RE) that actual inflation will remain anchored to the central bank's target up to short-run deviations (Woodford, 2003, ch. 2).==== In parallel, it supports the concern of central banks about their ability to fulfil their inflation target once expectations are de-anchored.====Benhabib et al. (2001) first warned that a feedback rule ==== Taylor may generate multiple RE equilibria, one of which at the ZLB with a perpetual “liquidity trap” with low inflation (below target) and output (below potential).==== Woodford (2003, Ch. 2, Section 4) discusses “self-fulfilling inflations and deflations”. He also finds that under an interest-rate feedback rule a multiplicity of equilibria may arise and that the RE equilibrium that fulfils the central bank's inflation target is locally stable, but not globally unique. Moreover he highlights that the drift towards self-fulfilling inflations and deflations has to be driven by de-anchored inflation expectations that, in the case of deflations, ====. In simple words, expected inflation should “fall faster” than the interest rate.====In light of this account, the de-anchoring problem consists of ==== that we investigate in this paper. The first is why agents' expectations that inflation will remain in line with the central bank's target begin to falter. The second is how the de-anchoring of expectations interacts with monetary policy determining whether the central bank is still able to achieve the inflation target, and hence re-anchor inflation expectations, or whether the system drifts towards negative inflation and output gaps.====Insights into the de-anchoring of inflation expectations can be found in different strands of literature that drop the RE hypothesis. In the first place, one may find studies which question the validity of the RE hypothesis empirically. For the Euro Area, using the European Commission survey, Forsells and Kenny (2004) find that inflation expectations appear unbiased, but they reject the hypothesis of orthogonality with respect to the available information. Several studies provide evidence in favour of adaptive learning in inflation expectations Pfajfar and Santoro, 2010, Slobodyan and Wouters, 2012, Molnar and Ormeno, 2015, Carvalho et al., 2017). A more theoretical literature investigates processes of expectation formation and whether they converge to the unique RE equilibrium. Generally, this literature posits agents lacking full information or knowledge about the data generating process, and examines some form of learning mechanism thereof. Multiple equilibria typically emerge, among which self-fulfilling liquidity traps in conjunction with the ZLB on the nominal interest rate. Some authors (e.g. Bullard and Mitra, 2002, Evans et al., 2008, Evans and McGough, 2017) show that, under least-square learning introduced by Marcet and Sargent (1989) and Evans and Honkapohja (2001), the liquidity trap exists, but it fails to be stochastically stable. Others, who employ different learning mechanisms, instead show that it can be stable (Arifovic et al., 2012, Arifovic et al., 2017, Busetti et al., 2014, De Grauwe and Ji, 2016).====In this paper we present a model consisting of the standard New-Keynesian three equations used in this literature (e.g. De Grauwe and Ji, 2016, Arifovic et al., 2017),==== augmented with an inflation-expectation mechanism which endogenises the de-anchoring process. It is both consistent with a broad notion of “rational beliefs” (e.g. Kurz, 2011), and amenable to simple parameterisation and control for comparative simulations. We draw on the key idea of “regime switch” in Arifovic et al. (2017). Forming their expectation of the future path of inflation, agents elaborate a probabilistic belief that, whenever the economy has negative inflation and output gaps, it may shift from the “normal regime”, where gaps will always return to zero, to a “depression regime” where the gaps will be permanent.==== The regime-switch probability ==== ∈ [0, 1] held by agents affects their inflation expectations. In fact, the belief-consistent expectation for each point in time is the (====; 1 − ====) mean value of the inflation (gaps) in the two regimes: as long as agents believe in the normal regime (==== = 0), their expected inflation gap is zero at all times; as ==== rises, the expectation is tilted towards the existing inflation gap. Hence, ==== > 0 can be regarded as an indicator of de-anchoring.====The key element in our model is that the regime-switch probability is formed and updated upon observing the actual state of the economy, namely it increases ==== worse output and inflation conditions, in accordance with the empirical methodology that identifies de-anchoring with the reactivity of expectations to macroeconomic news (see above). As we shall see, this makes the regime switch − the transition from a negative episode in the normal regime to the depression regime − dependent on agents' beliefs themselves. In this sense, we are also in the field of self-fulfilling beliefs.====With respect to the largest part of the literature, we extend our analysis beyond the single issue of the existence and stability of depressed states of the economy at the ZLB. Starting from the zero-gaps equilibrium, we introduce a shock to the natural rate of interest and, by means of simulations, we investigate the global dynamic behaviour of the system. Key to whether the system tends to return to the zero-gaps steady state, to settle down into a negative-gaps steady state or to display global instability is the interaction between the inflation-expectation mechanism, conditioned by agents' belief about a regime switch, and monetary policy, together with contour conditions given by structural parameters and the extent of the initial shock. Therefore, our study also offers some insights for both empirical analysis and policy conduct.====More in detail, Section 2 introduces the model. The New Keynesian block of the model consists of the standard equations of the output gap relative to potential, of the inflation gap relative to the central bank's inflation target, and of the nominal interest rate (policy rate for short) set by the central bank following a Taylor Rule. Then we introduce a fourth equation which, under suitable conditions that will be discussed, determines the regime-shift probability ==== elaborated by agents as a function of the observed state of the economy. The single key parameter of this function is its reactivity (====) to output gaps: larger ==== induces larger increase of ==== for a given output gap. This conditions their inflation expectations as described above, which in turn affect the actual output and inflation gaps and hence the monetary policy response, normally a cut in the interest rate aimed at closing the output and inflation gaps. This induces a revision of agent's beliefs and inflation expectations, and so on and so forth. Hence, we have precise identification and full control of the conditions that determine the extent of de-anchoring of inflation expectations, and whether they “fall faster” than the interest rate.====In Section 3 we analyse the properties of the system and present simulations of its long-run evolution. In the normal regime (==== = 0), the system displays the usual RE equilibrium with zero gaps. With ==== endogenous, however, the system becomes nonlinear, and dependent on the values of six parameters plus the initial natural-rate shock. Hence, we examine its global dynamics by means of simulations. These are organised in a baseline case with parameter values consistent with the empirical literature, and then pairwise interactions of the reactivity parameter ==== with three other ones: the extent of the initial shock, the output gap elasticity of inflation gaps (κ) and the inflation gap parameter (====) in the central bank's rule.====Our main findings, summarised and discussed in Section 4, are the following. Depending on the parameter values and the initial shock, the system's dynamic behaviour is more complex and richer than in similar studies on the de-anchoring problem. The system displays ==== possible regions: (1) ==== = 0, i.e. the system remains in the normal regime; (2) convergence to a steady state with ==== > 0, that we call a “new normal”, i.e. a depressed state of the economy to which there corresponds a permanent, though finite in size, de-anchoring of inflation expectations from the central bank's target; (3) ====, i.e. divergence from any steady state.====Key to the destiny of the economy is the interaction between the magnitude of the shock and of the sensitivity parameter ====. The economy remains in the first region as long as the policy rate can accommodate the shock above the ZLB; yet this basin of attraction shrinks as ==== increases. In the second and third region we have a clear representation of the phenomenon of inflation expectations that “fall faster” than the policy rate. The economy is pushed from the second to the third region for combinations of high ==== and a large shock.====Controlling for structural price stickiness (κ), simulations show that, ==== as κ increases (prices are more flexible and the Phillips Curve is steeper) the convergence regions shrink, the reason being that a steeper Phillips Curve amplifies the output gaps. With regard to the inflation parameter in the central banks' policy rule (====), simulations show that a more aggressive (conventional) monetary policy, i.e. with larger ====, is obviously ineffective at the ZLB, but it is also ineffective above the ZLB when adverse conditions push the system into the divergence region. This finding seems to support the necessity, in those conditions, to abandon conventional monetary policy and to switch to an aggressive reflationary policy that prevents the entrenchment of deflationary expectations.","Monetary policy, de-anchoring of inflation expectations, and the “new normal”",https://www.sciencedirect.com/science/article/pii/S0164070418301939,31 October 2018,2018,Research Article,299.0
Kim Jiseob,"School of Economics, Yonsei University, 50 Yonsei-ro, Seodaemun-gu, Seoul, 03722, Korea","Received 1 June 2018, Revised 12 October 2018, Accepted 30 October 2018, Available online 31 October 2018, Version of Record 13 November 2018.",https://doi.org/10.1016/j.jmacro.2018.10.007,Cited by (0)," with foreclosure delays that mirror the financial crisis triple the mortgage delinquency rate, while it temporarily reducing the foreclosure rate by half. An increase in mortgage defaults motivates financial intermediaries to voluntarily modify loan terms after initiating the foreclosure process to mitigate their losses.","The US housing market suffered a severe crash after the financial crisis. In turn, many households with negative home equity had no choice but to default. The mortgage foreclosure rate increased to 4.2% after from 0.5% before the crisis. As the number of foreclosures increased, financial intermediaries and state courts could not manage the foreclosure process in a timely fashion, leading to foreclosure delays. Though the average time to complete the foreclosure process is different depending on state laws, the national average processing time significantly increased from nine months to 15 months after the financial crisis (Herkenhoff and Ohanian (2015)). The prolonged process makes it possible for defaulted households to stay in their homes without paying either mortgage- or rent-related fees.====Initially, I analyze how the prolonged foreclosure process affects mortgage defaults. As the time needed for the process is extended, financially troubled households are more likely to default on their mortgages, staying in their homes and not making periodic mortgage payments or paying rent until the foreclosure process is completed. This in turn leads to an increase in households that are not repaying their mortgages, or delinquent debtors. However, as the foreclosure timeline is extended, the number of completed foreclosures may temporarily decrease.====Next, I examine the impact of foreclosure delays on post-foreclosure modifications. After the financial crisis, the US government introduced several foreclosure prevention policies (Robinson (2009) and Gerardi and Li (2010)). Since foreclosure delays influence households’ mortgage default decisions, financial intermediaries’ incentive to implement post-foreclosure modification is also affected by the foreclosure timeline. A sudden increase in mortgage defaults leads financial intermediaries to voluntarily modify mortgage terms to mitigate their losses, even when the cost of modifying is prohibitively high. Hence, the prolonged foreclosure process can lead to an increase in the post-foreclosure modification rate.====In order to analyze the impact of the foreclosure delay on households’ and financial intermediaries’ optimal behavior, I introduce a quantitative model where households make savings, housing tenure, down payment, and default decisions. Each household in the model faces two types of idiosyncratic shocks: income and house price shocks. When a mortgage holder faces either low income or low house price shocks, it is highly probable that the debtor will choose to default on the mortgage, even though doing so incurs significant post-default costs. Once a household defaults, a financial intermediary can foreclose the house immediately with a certain probability. However, the defaulted house is not foreclosed with the other probability, and the debtor becomes delinquent. The delinquent household can then stay in its own house for free. If the foreclosure process does not proceed immediately, the financial intermediary offers to reschedule payments which maximizes the financial intermediary’s expected profit. The delinquent loan becomes current if the household accepts the offer and continues repaying.====When the foreclosure process is initiated, the financial intermediary takes over the defaulted house and sells it on the market. However, the intermediary cannot fully recover the market value of the house. They incur a deadweight cost, called the foreclosure cost, which is around 20%–30% of the house value (Pennington-Cross (2006) and Posner and Zingales (2009)). Since completing the foreclosure involves significant costs to both households and financial intermediaries, this motivates the latter party to voluntarily modify loan contracts after initiating the foreclosure process. In my model, financial intermediaries marginally reduce the mortgage burden up to the point where the value of foreclosure and repayment is the same for the household. The financial intermediary initiates modification if the expected cash inflow after modification is higher than the value of the house net of the foreclosure cost.====After calibrating the benchmark steady-state model to match the pre-crisis US economy, I analyze the impact of foreclosure delays on mortgage defaults and household behavior in an environment mirroring the crisis. Specifically, I model an unexpected decline of 30% in house prices during the crisis, as in the Case-Shiller index. At the same time, the foreclosure process is extended from nine months to 15 months. Given this benchmark transition, I consider a counter-factual scenario where the foreclosure process is not extended, even after the financial crisis. By comparing these two transition paths, I can evaluate the effects of foreclosure delays on major household finance moments.====My quantitative exercise shows that an increase in the length of the foreclosure process temporarily reduces the mortgage foreclosure rate during the financial crisis. As the foreclosure process becomes prolonged, defaulted houses cannot be foreclosed swiftly, leading to a decrease in the foreclosure rate. On the other hand, foreclosure delays increase the value of defaults because defaulted households can stay in their homes for free, leading to an increase in the foreclosure rate. In sum, the prolonged foreclosure process slightly increased the foreclosure rate by 0.8 percentage points in 2008 and decreased it by almost half in 2009.====A delinquent household can stay in its own house without paying rent or making a mortgage payment. In addition, the delinquent household has a second chance to rehabilitate its bad credit by accepting an offer to reschedule payments. Hence, the mortgage delinquency rate significantly increases as the foreclosure process is extended. According to the quantitative exercise, the delinquency rate tripled in 2008 and doubled in 2009 because of the foreclosure delay.====The post-foreclosure modification rate increases as the foreclosure timeline is extended. The foreclosure delay increases mortgage defaults because of the increased benefits of being delinquent. Then, financial intermediaries are more likely to initiate mortgage loan modifications to decrease their losses, thereby increasing the post-foreclosure modification rate. At the same time, as the mortgage default rate increases, mortgage interest rates increase, leading to a decrease in the value of homeownership for renters who want to own homes. This in turn makes the value of homeownership for foreclosed households, which then become renters, decrease. Then, financial intermediaries can mitigate their losses and recover more cash after writing off small amounts of debt, leading to an increase in the voluntary modification rate.====The rest of the paper is organized as follows. I review related literature in Section 2. Then, I introduce a quantitative model in Section 3 and calibrate it in Section 4. Section 5 analyzes the benchmark steady-state economy. Section 6 examines households’ short-run responses to a sudden drop in house prices with different foreclosure timelines. I conclude the paper in Section 7.",How foreclosure delays impact mortgage defaults and mortgage modifications,https://www.sciencedirect.com/science/article/pii/S0164070418302489,March 2019,2019,Research Article,300.0
Reed Jason R.,"232 Mendoza College of Business, Department of Finance, University of Notre Dame, Notre Dame, IN 46556, USA","Received 13 March 2018, Revised 19 October 2018, Accepted 22 October 2018, Available online 25 October 2018, Version of Record 12 November 2018.",https://doi.org/10.1016/j.jmacro.2018.10.005,Cited by (3),", explain the forward premium bias found empirically. This is different from most research, which focuses on persistent monetary model fundamentals to explain the bias. This result holds under robust parametrizations of constant gain learning, monetary fundamental persistence, and Markov-switching ====, which govern the model. Thus, this paper offers a novel solution to the forward premium puzzle.","Since its initial treatment, the forward premium puzzle has been a longstanding paradox within macroeconomic research. The puzzle originates from an empirical analysis, which finds the forward exchange rate to be a poor predictor of the spot exchange rate over short time horizons – even though it is a good predictor over longer horizons. Fama (1984) finds that under rational expectations (RE), the OLS estimate of the forward premium regression coefficient consistently underestimates the theoretical value of unity and can estimate negative values of the forward premium coefficient. Additionally, the goodness-of-fit measurement, ====, for the forward premium regression is generally low. Furthermore, Meese and Rogoff (1983) shows that a random walk model outperforms both structural and free-floating monetary models when forecasting exchange rates. In their treatment of the puzzle, Chakraborty and Evans (2008) contends that econometric learning, or adaptive learning, plays a large part in abolishing the poor statistical outcomes attributed to the forward premium puzzle. They find that learning not only generates the forward premium puzzle but also contributes to the stylized empirical results often reported. The results from adaptive learning papers Chakraborty (2009), Chakraborty and Evans (2008), and Kim (2009), however, do not account for the effects of monetary policy regime changes on the forward premium estimates. To account for the monetary regime changes, this paper incorporates state-dependent parameters, in conjunction with adaptive learning to explain the forward premium puzzle first presented by Fama (1984).====Sargent (1993), and Evans and Honkapohja (2001, 2012) argue that adaptive learning and the idea of bounded rationality has grown into a plausible and tractable substitute for rational expectations. The adaptive learning assumption held by this paper, requires economic agents, who forecast future spot exchange rates, to update their information set with their own forecast error. Since the timing of monetary policy changes are unknown to economic agents, if a change occurs, the agents’ information set suddenly becomes less reliable and agents should discount past observations to account for the change. Otherwise, agents face the possibility of creating a self-referential outcome. To account properly for what Farmer et al. (2009) refers to as regime spillover, this paper introduces new stability conditions to work alongside adaptive learning.====As Stone and Bhundia (2004) indicates, monetary policy regimes will change as conditions and circumstances change for the monetary authority. For example, during the mid to late 1980s, the US, Germany, Japan, Italy, and France decided on monetary policy measures to appreciate the US dollar relative to each bi-lateral currency. The 1985 Plaza accord was too successful and overshot the target appreciation level of the US dollar.==== In response, the countries created the 1987 Louvre accord. The Louvre accord committed central banks to depreciate the US dollar against each bi-lateral currency. Furthermore, Stone and Bhutani (2004) identifies that the US, UK, and Switzerland changed monetary regimes in the decade following the Plaza and Louvre accords, unlike Japan. Switzerland's central bank provides further anecdotal evidence from their 2011 and 2015 monetary regime decision to peg and then unpeg the Franc to the Euro, creating confusion, and uncertainty in an otherwise stable economic environment. To model the circumstances stemming from monetary regime changes, this paper uses data post Plaza and Louvre accords with state-dependent, Markov-switching, constant gain adaptive learning model.====Using Markov-switching parameters and adaptive learning techniques, this paper simulates the forward premium regression coefficients found for the US, UK, Japan, and Switzerland. I find that persistent monetary fundamentals combined with persistent regimes achieve the theoretical value of the forward premium regression coefficient predicted by Fama (1984).",The forward premium puzzle and Markov-switching adaptive learning,https://www.sciencedirect.com/science/article/pii/S0164070418300909,March 2019,2019,Research Article,301.0
"Marszk Adam,Lechman Ewa","Faculty of Management and Economics, Gdansk University of Technology, Narutowicza 11/12, 80-233 Gdansk, Poland,Faculty of Management and Economics, Gdansk University of Technology, Poland","Received 27 December 2017, Revised 18 September 2018, Accepted 1 October 2018, Available online 2 October 2018, Version of Record 27 November 2019.",https://doi.org/10.1016/j.jmacro.2018.10.001,Cited by (12)," models, which are applied to characterize the key features of the process of diffusion of ETFs and ICT, with dynamic panel models, panel VAR models with exogenous variables and VAR models with exogenous variables (for country-specific analysis), which are used to examine the relationships between ETFs, ICTs, and other selected factors. Our major findings confirm that adoption of ICTs constitutes an important prerequisite for the diffusion of ETFs due to potential demand- and supply-side linkages. Among the other factors that potentially influence the diffusion of ETFs, we found three variables that demonstrate positive and statistically significant impacts: stock market turnover, financial development, and financial markets. Country-wise VAR models with selected exogenous variables confirm the influence of ICTs in most of the countries analyzed.","Exchange-traded funds (ETFs) are one of the most rapidly-expanding categories of innovative financial products. The popularity of ETFs has been spurred by the benefits they offer to their users in comparison to those offered by traditional investment companies (Gastineau, 2010, Agapova, 2011, Hill et al., 2015, Lechman and Marszk, 2015). Their spread has been observed on many financial markets, not only in the developed economies but also in some emerging countries. Despite the growth of the ETF markets in many countries, factors influencing the diffusion of these financial innovations (i.e., the growth of the turnover of their shares or assets under management) remain a largely neglected topic in scientific research.====It is claimed that information and communication technologies (ICTs) constitute an important factor contributing to the strengthening of financial systems and financial development [see, for example, Wurgler (2000) and Yartey (2008)]. ICTs may affect financial markets and the spread of financial innovations (including ETFs) in various ways. Undeniably, the unbounded spread of ICTs may effectively facilitate both the emergence and the diffusion of financial innovations and enhance the dynamic development of financial markets as a whole. Broad adoption of ICTs gives rise to a new network, the members of which gain access to financial markets, especially to innovative financial products, on-line banking systems, and many other financial services that enhance their financial inclusion. Arguably, a kind of ‘domino effect’ (Economides, 1996, Cabral, 2006) occurs as new potential users wish to join the emergent ‘financial network’, hoping to fulfill their expectations of potential gains. Apparently, the role of ICTs in the diffusion of ETFs can be observed in both the demand and the supply sides of the ETF market. On the one hand, broad usage of ICTs eliminates information asymmetries, enhances in-time trading, and makes trading mechanisms more effective, which boosts demand for transactions on stock markets. Meanwhile, on the other hand, adoption of new communication methods (especially using a broadband connection) allows an increasing supply of various innovative financial products. Bearing the latter in mind, both sides of the financial market develop continuously. In our research, to more accurately evaluate the impact of ICTs on the diffusion of ETFs, we consider also the impact of other segments of the financial system, such as the banking sector. Undeniably, diffusion of ETFs seems to be dependent on a wide variety of other factors, such as the rate of economic growth and capital and labor productivity.====The main aim of this paper is to provide empirical evidence on the relationships between the penetration of ICTs and the diffusion of innovative financial products—ETFs— using a sample of 32 emerging and developed economies. This means that, with some minor exceptions, we analyze all countries for which it is possible to acquire data on the turnover of ETFs on the local stock exchanges. More specifically, our research contributes to the present state of knowledge by:====Our methodological framework involves adoption of a combination of innovation diffusion models (Geroski, 2000, Lechman, 2015), which are used to briefly characterize the key features of diffusion of ETFs, with dynamic panel models, panel vector autoregression models with exogenous variables and country-wise VAR models with exogenous variables. These are applied to examine the relationships between ETFs, ICTs, and other factors. We use annual data for 2004–2014 on the turnover of ETFs, the adoption of ICTs, the development of financial markets/banking sector, and economic growth (as well as its components). These are derived from the following databases: World Telecommunication/ICT Indicators Database 2016, World Federation of Exchanges, World Bank's Global Financial Development Database, the International Monetary Fund (IMF) Financial Development Index Database, and The Conference Board's Total Economy Database.",New technologies and diffusion of innovative financial products: Evidence on exchange-traded funds in selected emerging and developed economies,https://www.sciencedirect.com/science/article/pii/S0164070417305736,2 October 2018,2018,Research Article,302.0
"Bellettini Giorgio,Delbono Flavio,Karlström Peter,Pastorello Sergio","Department of Economics, University of Bologna, Piazza Scaravilli, 2, Bologna 40126, Italy","Received 24 December 2017, Revised 13 August 2018, Accepted 16 August 2018, Available online 22 August 2018, Version of Record 27 November 2019.",https://doi.org/10.1016/j.jmacro.2018.08.007,Cited by (4),"We perform an empirical analysis to investigate the relationship between income inequality and the occurrence of ==== crises on a panel of 33 advanced countries in the period 1970–2011. Differently from other empirical studies, we focus on levels rather than growth rates of income inequality. We find a statistically significant and positive relationship between the value of the ==== and the probability of ==== crises. This result is confirmed when income distribution is summarized by the top 1% income share.","In recent years, it has been claimed by some prominent scholars that income inequality may be responsible for financial crises. This point of view is well summarized in the influential book by Rajan (2010, p. 43), according to whom “growing economic inequality in the United States led to political pressures for more housing credit. This pressure created a serious fault line that distorted lending in the financial sector”. While the argument is not fully new (see Galbraith, 1954, for instance), other researchers (e.g., Krugman, 2007, Fitoussi, Saraceno and Stiglitz, 2012, among others) have endorsed what is now referred to as the “Rajan hypothesis”, linking income inequality to the surge in household indebtedness which has been found to be a major predictor of banking crises.====The hypothesis put forward by Rajan and other scholars has triggered a vigorous debate (see the exhaustive and updated surveys by Van Treeck, 2014 and Bazillier and Hericourt, 2017), and a relatively small group of studies have tried to investigate empirically the relationship between income inequality and the occurrence of banking or financial crises. We aim at participating to this growing debate by means of an econometric analysis that tests the link between levels of income inequality and the probability of banking crises. Differently from other scholars (see in particular Bordo and Meissner, 2012 and Perugini et al., 2016) we test this link directly. Our study encompasses a panel of 33 advanced countries in the period 1970–2011. The main finding of our paper is that the level of gross income inequality (measured by either the Gini index or the top 1% income share) is positively associated with the occurrence of banking crises. Furthermore, we find that the effect of an increase in the level of income inequality is sizable. In addition, the effect seems to be larger for countries that already have an elevated level of income inequality. Finally, the results hold for a series of robustness checks, such as, among the others, the rare events small sample bias, the inclusion of regional and time fixed effects, the exclusion of various country groups.====There are several reasons to claim that the relevant measure of income inequality is the level and not the growth rate when investigating the association between income inequality and the likelihood of banking crises. First of all, the theoretical literature by Iacoviello (2008) and Kumhof et al. (2015) emphasizes the existence of a long-run relation between household debt and income inequality. As a consequence, the use of the growth rate of income inequality that removes the long-run trend as in the studies by Bordo and Meissner (2012) and Kirschenmann et al. (2016) seems problematic. In fact, the use of the growth rate of income inequality will lead to biased estimates if a long-run relation exists between income inequality and household debt as emphasized by Klein (2015). Iacoviello (2008) argues that short-term movements in household debt can be explained by business cycle fluctuations while the connection between household debt and income inequality is a long-term one. Klein (2015) uses panel cointegration techniques and shows that a long-run relation exists between income inequality and household debt. This result is consistent with Malinen (2016), who also finds a long-run steady-state relationship between the top 1% income share and domestic bank credit. In short, both the theoretical and empirical literature emphasize that income inequality is related to household debt only in the long-run, which suggests that the level of income inequality (rather than the growth rate) is the suitable measure in this study.====This paper is organized as follows. Section 2 reviews the related literature and places our analysis in perspective with respect to other empirical studies. Section 3 describes the dataset and variables. In Section 4 we provide a description of our empirical approach and the statistical model (4.1) followed by a discussion of our findings (4.2) and their robustness (4.3). Section 5 concludes.",Income inequality and banking crises: Testing the level hypothesis directly,https://www.sciencedirect.com/science/article/pii/S0164070417305682,22 August 2018,2018,Research Article,303.0
"Mao Sheng-Zhi,Huang Chien-Yu,Chang Juin-Jen","Research Institute of Economics and Management, Southwestern University of Finance and Economics,Institute of Urban Development, School of Economics, Nanjing Audit University,Institute of Economics, Academia Sinica","Received 26 December 2017, Revised 5 July 2018, Accepted 1 August 2018, Available online 10 August 2018, Version of Record 27 November 2019.",https://doi.org/10.1016/j.jmacro.2018.08.002,Cited by (2),"This paper analyzes the effects of three distinctive monetary instruments, namely, the money growth rate, the required reserve ratio, and the leverage ratio, on growth and welfare in an R&D growth model with an active ","Over the past few decades, the issue of the growth and welfare effects of monetary policy has remained at the heart of macroeconomics (see Jones, Manuelli, 1995, Chari, Jones, Manuelli, 1996). Meanwhile, financial development has been also essential to derive monetary policy recommendations (Pagano, 1993, Levine, 2005). A vast literature has accumulated valuable insights toward understanding how monetary policy affects growth and welfare through either capital accumulation (Jones, Manuelli, 1995, Chari, Jones, Manuelli, 1996, Wu, Zhang, 1998, Jha, Wang, Yip, 2002, Chang, Chang, Lai, Wang, 2007) or the innovation process (Marquis, Reffett, 1994, Funk, Kromen, 2010, Chu, Lai, 2013, Chu, Cozzi, 2014, He, 2015, Huang, Chang, Ji, 2016).====The nexus between financial development and economic performance (in long-run growth) has been also studied extensively. The degree of financial development (or depth) of a country can enhance economic growth through promoting human capital accumulation (De Gregorio, 1996, De Gregorio, Kim, 2000), stimulating R&D and innovation (Morales, 2003, Trew, 2008), and channeling savings to the most efficient uses (Trew, 2014). Recent works refer to a non-monotonic relationship between financial development and economic growth (see Bucci and Marsiglio, 2018 and Bucci et al., 2018).====Undoubtedly, the banking system plays an important role in the effects of monetary policy and financial development. On the one hand, the intermediary role of banks will govern the monetary transmission mechanism and financial intermediary function and hence the effects of monetary policy (see Bernanke, Gertler, 1995, Kashyap, Stein, 2000, Levine, 2005, Disyatat, 2011). On the other hand, the efficiency of a banking system will govern the extent to which the assets of households can be converted into capital accumulation and/or innovation which can enhance economic growth and raise social welfare (see Chang, Chang, Lai, Wang, 2007, Cetorelli, Peretto, 2012).====In spite of its importance, however, the role of an active banking sector is abstracted from most studies, as it is hard to provide a comprehensive understanding of how different monetary instruments affect the long-run growth and welfare through the banking sector. Instead, in this paper, we will thoughtfully discuss the role of banks in the transmission mechanism of distinctive monetary policies. By shedding light on the role of banking, we will show that different monetary instruments at different levels of financial development end up with different growth and welfare effects.====The model we build has several salient features. First, we model money and banking in a dynamic general equilibrium framework. Money is introduced into the model by using a conventional transactions-based approach, i.e., money is used to reduce transactions costs by providing liquidity services (see Wang and Yip  (1992) for a survey). Banks receive deposits from households and loan out non-reserved parts of the deposits, converting them into productive capital and/or innovation.====Second, and more importantly, an active banking sector enables us to introduce two banking- and financial-related monetary instruments in addition to the money growth rate, namely, the required reserve ratio and the leverage ratio requirement. The required reserve ratio governs the proportion of banks’ deposits to be kept within the bank as required reserve (see, for example, Chari, Jones, Manuelli, 1996, Chang, Chang, Lai, Wang, 2007), while the leverage ratio requirement imposes a low bound for the ratio of capital to assets, which is used to control the systematic risk (see, for example, Kishan and Opiela, 2000). The leverage ratio requirement is particularly crucial in the 2010 Basel III agreement in regard to bank asset regulation that was introduced after the 2008 financial crisis.==== In a way that differs from the conventional monetary models, we simultaneously analyze three distinctive monetary instruments, namely, the money growth rate, the required reserve ratio, and the leverage ratio requirement, in a unified R&D-driven growth model. Such a study will provide the monetary authority with a guideline for choosing appropriate policy instruments for balancing out the short-run economic stabilization and the long-run welfare costs.====Third, including a banking sector in an R&D-driven growth model allows us to shed light on the importance of financial frictions. The financial frictions occur as banks engage in paying real resources (the operational costs) to manage their loan services. These costs, in general, are characterized by the conversion rate from banking deposits to productive capital where a higher conversion rate denotes a more efficient management and thus a lower operational cost. We capture these costs through two channels. The first occurs when banks finance intermediate goods production, converting loans to capital for intermediate goods production, and the second occurs when banks finance R&D investment, converting loans to R&D capital. Moreover, we distinguish these two operational costs in terms of size because financing R&D investment requires the payment of additional resources for assessing projects and monitoring the innovation process (i.e., the assessment and monitoring costs) and for facing the risk of failing the project and the uncertain future profitability (i.e., the cost of risk and uncertainty) (see Hall and Lerner, 2010).====Our study consists of both analytical and numerical analysis. The analytical results show that increases in the money growth rate, the required reserve ratio and the leverage ratio requirement all lead to decreases in the growth rate.==== The three instruments, however, have different effects in terms of magnitude on economic growth, with these effects crucially depending on the components of the banking capital structure that each monetary instrument can regulate or has influence on. Specifically, the magnitude of the growth effects depends on how the financial frictions of a banking system affect the transmission mechanism under various monetary instruments.====In the numerical analysis, we calibrate the model to match the U.S. economy and quantify the growth effects and welfare costs under the three distinct monetary instruments. Our calibration results show that the required reserve ratio slows down economic growth and enhances welfare costs by the largest magnitude and the corresponding effects of the money growth rate are the smallest. Although the required reserve ratio is commonly viewed as an effective short-run instrument, it gives rise to the largest welfare cost in the long run.====In the extended analysis, we further show that the growth and welfare impacts are more responsive to the change in the operational cost from converting loans to intermediate goods production than that from converting loans to R&D. Of particular importance, we find that the growth effects and welfare costs of the leverage ratio requirement may become the greatest if the cost arising from converting loans to production is substantially large. The main reason is that the cost resulting from converting loans to intermediate goods production largely affects the spread between the capital return rate and real loan interest rate, which is the key determinant of the size of the growth effect of the leverage ratio. These results give rise to an important implication in the sense that the choice of a better monetary instrument crucially relies on banking efficiency. In particular, the leverage ratio requirement may be beneficial to developed countries with more efficient banking systems, but may be unfavorable to developing countries with less efficient banking systems. To the best of our knowledge, this is the first study that carefully compares the growth and welfare effects of the three different monetary instruments in a unified R&D-driven growth model.====By focusing on the money growth rate, our model generates relatively large welfare costs compared with those obtained in previous studies. We compare the welfare costs of the money growth rate with those obtained from similar R&D growth models. Marquis and Reffett (1994) use the extended Romer (1990) model with the cash-in-advance (CIA) constraint on consumption and refer to a 2.13 percentage point increase in welfare costs in response to an increase in the inflation rate from the Friedman rule to zero inflation. The figure shows a 6.8 percentage point increase in welfare costs in the study by Chu and Lai (2013) which uses a Schumpeterian growth model with the specification of money in the utility function.==== Our results show that, with the presence of financial frictions, the welfare costs from the Friedman rule to zero inflation increase from 5.16 percentage points to 9.28 percentage points. This large gap indicates that financial frictions in the banking sector play a key role in governing the magnitude of welfare costs, which have not been evaluated in the aforementioned studies.====As a notable exception, Chang et al. (2007) introduce a banking sector in a capital-driven growth model and analytically evaluate how monetary policy affects macroeconomic performance without specifically focusing on welfare. In contrast to Chang et al. (2007), the present paper incorporates a banking sector into an R&D growth model. In particular, by using both analytical and numerical methods, we discuss why different monetary instruments end up with different growth and welfare effects in the presence of various financial frictions. Chen (2015) incorporates a banking system and a reserve market along with a CIA constraint to discuss the effects of active and inactive monetary policies on stabilizing the macro economy in the short run. In contrast to Chen (2015), our study focuses on the long-run growth effects and welfare costs, instead of the short-run impacts.====The remainder of the paper is organized as follows. Section 2 presents the model economy. Section 3 characterizes the competitive equilibrium and the balanced growth path. Section 4 analyzes the growth effects and welfare costs of monetary policies. Section 5 quantitatively analyzes both the growth effects and welfare costs. Section 6 concludes.",Growth effects and welfare costs in an innovation-driven growth model of money and banking,https://www.sciencedirect.com/science/article/pii/S0164070417305724,10 August 2018,2018,Research Article,304.0
"Bucci Alberto,Eraydın Levent,Müller Moritz","Department of Economics, Management and Quantitative Methods (DEMM), University of Milan, Italy,CEIS (Centre for Economic and International Studies), University of Rome-Tor Vergata, Italy,RCEA (Rimini Centre for Economic Analysis), Wilfrid Laurier University, Waterloo, Canada,Chair in Economic Policy, Institute of Economics (ECON), Karlsruhe Institute of Technology, Germany,Faculty of Economics and Management, University of Strasbourg, France","Received 29 December 2017, Revised 23 July 2018, Accepted 1 August 2018, Available online 10 August 2018, Version of Record 27 November 2019.",https://doi.org/10.1016/j.jmacro.2018.08.003,Cited by (17),"This paper answers the following two questions: (1) In the data, do we find a ==== of population growth with regards to the process of per-capita human capital formation. And we observe that, depending on the ==== observed in a given country, population growth may be relevant — either positively or negatively — for economic growth. In the second part of the paper we use these results to build a multi-sector growth model which, by allowing for a country specific ==== of population growth on per-capita human capital formation, is capable of accounting for the non-monotonous correlation between demographic and economic growth rates in the long-run.","Because the existence of a causal relationship and, potentially, the sign of such a relation are still controversial issues in the literature, it remains very important — not only for demographers and economists, but also for policy-makers — to investigate the impact that population growth may have on long-term economic growth, i.e. the growth rate of real per-capita income.====Many studies (Solow, 1956, Coale, Hoover, 1958, Ehrlich, 1968, Li, Zhang, 2007, Herzer, Strulik, Vollmer, 2012, just to mention a few) find a negative influence of population growth on economic growth. In exogenous growth models, for example, this result is ultimately explained by the so-called physical capital ==== of population growth: an increase in population, by diluting the stock of physical capital held by each individual, lowers ==== the long-run (or steady-state) level and the short-run (or transitional) growth rate of physical capital per-capita. Other studies (Kuznets, 1967, Boserup, 1981, Simon, 1981, Romer, 1987, Romer, 1990; Kremer, 1993, Jones, 1995), instead, conclude that, once endogenous technological change is explicitly taken into account, the impact of population growth on economic growth is definitely positive as larger populations stimulate the advancement of technical progress, enjoy greater economies of scale, and are likely to have a greater number of geniuses. Finally, there are also strong arguments advocating that better economic performance, i.e. a higher economic growth rate, causes an increase of the population growth rate.==== Blanchet (1988) is among those who years ago have already used this claim to explain the presence of an insignificant correlation between economic and population growth rates.====In the light of all this, it seems that the main conclusion coming from the path-breaking paper by Kelley and Schmidt (1995, p. 554) still continues to hold today: “====”.====The present paper is motivated on empirical as well as theoretical grounds. More concretely, this paper deals with the following two questions: In the data, is the so-called ==== of population growth, that the literature often relates solely to the process of per-capita physical capital accumulation, also present in the development of per-capita human capital? If so, how can we use this observation to explain theoretically the existence of a differential cross-country impact of population change on economic growth?====In the first part of our paper we investigate and document empirically the existence of a ==== of population growth in regard to the process of per-capita human capital formation that is remarkably heterogenous across countries. We further observe that, for an individual country, population growth may be relevant (either positively or negatively) for economic growth depending on the specific way population growth affects the process of schooling-acquisition by agents.====As a consequence of these empirical results, in the second part of the paper we turn to the theory and present a generalization of the very well-known Uzawa’s (1965) and Lucas’ (1988) settings. The analysis of the theoretical model we propose here is limited to the characteristics of its balanced growth path (BGP hereafter) equilibrium. Hence the model’s transitional dynamics are not studied. Our framework introduces two important differences with respect to Uzawa (1965) and Lucas (1988). The first consists in assuming that firms may undertake horizontal R&D activity. This means that disembodied technological progress (in terms of the growth rate of available variety of intermediate inputs) is endogenous in our economy, as in the path-breaking papers by Romer (1990) and Jones (1995). Moreover, consistent with our empirical findings, we postulate that individual human capital investment is subject to some sort of ==== related to an increase in population. This effect captures the idea that, since newborns enter the world completely uneducated, they reduce the stock of human capital per-capita available in the population. Hence, population growth operates like a depreciation of human capital per-capita and, thus, may contribute to deter human capital accumulation at an individual’s level. In the original Lucas (1988, Eq. 13, p. 19) model such a ==== in per-capita human capital investment is missing, based on the belief that skill acquisition is a sort of “====” and, as such, is completely different from physical capital investment.====Our extended model shows that the magnitude of the ==== of population change on per-capita human capital investment can be used as an argument to explain the differential (i.e., positive, negative, or neutral) impact that population growth may have on long-run economic growth across countries. In other words, our theoretical model proposes a new, alternative explanation for the non-uniform effect of population growth on long-run per-capita income growth. This new explanation is based on the recognition that population growth brings about two simultaneous and different (in sign and size) effects on economic growth. The first is precisely the already-mentioned ====. This effect is always negative because when newborns enter the world, they reduce the existing per-capita stock of any reproducible factor-input (human capital in our case). So, in order to equip every single member of the growing population with an even amount of such input, some further resources need to be explicitly devoted to this aim (as opposed to other uses), which dampens productivity growth. The second effect, instead, describes the possibly positive impact that population growth may have on the economy’s growth rate of ideas, and hence on per-capita income growth. Indeed, in R&D-based growth theory it is now well-known that ideas introduce scale effects (see, Jones and Romer, 2010). The scale effect of ideas follows immediately from their nonrival nature: ideas are shareable==== and, unlike private goods, can be used at any time by any number of people without congestion or depletion. Therefore, a higher population growth rate does not only mean that more ‘Isaac Newtons’ will be born, but also that the ideas produced by this larger pool of ‘Isaac Newtons’ will be shared among a greater number of individuals, which, in turn, fosters the creation of even more new ideas to be spread around. In a world where ideas are an engine of economic growth such a positive spiral of idea generation and diffusion ultimately fosters per-capita income growth. Incorporating both, the negative ==== as well as the positive ====, our theoretical model suggests that there exists a country-specific threshold level of the per-capita human capital accumulation ====: for those countries in which the ==== of population growth is below (above) the threshold, the model predicts a positive (negative) impact of population change on long-run economic growth.====Although one contribution of the present paper is empirical,==== we believe that the theoretical model we present in the second part of this article has features that make it new and original within the existing literature. Unlike the canonical Romer’s (1990) and Jones’ (1995) settings, we consider the possibility that the intermediate sector uses human capital as an input. Hence, in our model human capital is employed to produce new human capital, final and intermediate goods, and to invent new ideas. Despite the fact that in Mierau and Turnovsky (2014) — who use a one-sector endogenous growth framework ==== Romer (1986) — the link between the whole population growth rate and the economic growth rate is monotonic, their model is still able to account for a hump-shaped relation between the two variables since population growth can be explained by either an increase in fertility or a reduction in mortality. Thus, in Mierau and Turnovsky (2014) the way in which population growth occurs (i.e. through an increase in the birth rate or a decrease in the mortality rate) is important in making the relationship between demographic change and economic growth eventually non-uniform in sign. Our paper differs from Mierau and Turnovsky (2014) in that we consider a multi-sector growth model with R&D activity and human capital accumulation. Moreover, we do not split the population growth rate into its birth- and mortality-rate components because we are interested in uncovering another potential source of non-monotonicity in the overall relationship between demographic change and economic growth. Using an endogenous growth model with human capital investment and two types of R&D activities (horizontal and vertical, respectively), Strulik (2005) concludes that the sign of the correlation between population growth and the growth rate of per-capita income crucially depends on the form of households’ preferences (‘Millian’, ‘Benthamite’, or an intermediate type between the two, respectively). By considering an economy where R&D activity is purely horizontal, Bucci (2008), instead, postulates that the investment in skill-acquisition by agents is directly influenced by technological progress. In his framework, the sign of the long-run correlation between population growth and economic growth is finally found to depend not only on the form of households’ preferences, but also on the nature of technical change (whether ‘skill-biased’, ‘eroding’, or ‘neutral’). Unlike Bucci (2013), it is not an objective of this article to highlight the role of the so-called ‘returns-to-specialization’ in shaping the link between population growth and economic growth. Moreover, contrary to Bucci (Bucci, 2008, Bucci, 2013) and Strulik (2005), we pay no attention here to how the type of households’ preferences might contribute to affect the correlation between population and economic growth rates. Lastly, and differently from Bucci (2015), in the present article we are not interested in highlighting the possible tension between ==== (due to specialization) and ==== (due to production-complexity) that arises from an expansion of input-variety as a major determinant of the sign of the long-run relation between economic and population growth rates. Instead, as mentioned above, we are interested here in emphasizing a completely new mechanism (based on the role played by the ==== of population change on human capital formation by agents) through which the relation between population and economic growth rates might be non-uniform in sign in the long-term.====In a recent paper, Prettner (2014) has proposed another way through which a higher population growth can have a non-monotonous effect on economic growth within a Romer’s (1990)—Jones’ (1995) economy augmented with an education sector: In his model, an increase in population growth, while positively influencing aggregate human capital accumulation, decreases simultaneously schooling intensity (defined as the productivity of teachers times the resources spent on educating each child). The fall of schooling intensity has, in turn, a negative impact on the future evolution of aggregate human capital. This model allows for a long-run non-uniform impact of population change on economic growth, depending on whether the negative effect of population growth dominates the positive effect. Prettner (2014), however, is not interested in emphasizing the role of the ==== in per-capita human capital investment, which is instead the main focus of our article.====The remainder of the paper is organized as follows. Section 2 empirically investigates the effect of population growth on economic growth through individuals’ human capital formation. Section 3 lays out the theoretical model, and Section 4 analyzes its predictions along a BGP equilibrium. Section 5 focuses on the correlation between population growth and per-capita income growth in the long-run, and explains the possibly non-monotonous sign of this correlation in terms of the extent of a ==== of population growth on per-capita human capital accumulation. The last section concludes and provides suggestions for future research.","Dilution effects, population growth and economic growth under human capital accumulation and endogenous technological change",https://www.sciencedirect.com/science/article/pii/S0164070417305785,10 August 2018,2018,Research Article,305.0
"Batini Nicoletta,Melina Giovanni,Villa Stefania","International Monetary Fund, 700 19th Street N.W., Washington, D.C. 20431, United States,International Monetary Fund, 700 19th Street N.W., Washington, D.C. 20431, United States,Fellow at Banca d’Italia, Via Nazionale, 91, Rome 00184, Italy","Received 19 December 2017, Revised 23 April 2018, Accepted 26 June 2018, Available online 30 June 2018, Version of Record 27 November 2019.",https://doi.org/10.1016/j.jmacro.2018.06.012,Cited by (26),"Focusing on Euro Area countries we show empirically that higher private debt leads deeper recessions, while higher public debt does not, unless the level of public debt is especially high. We then build a ==== model that replicates these dynamics, and use it to design a policy that can mitigate the recessionary consequences of private deleveraging. In the model, following financial shocks, recessions are milder and public debt is more contained when the government lends directly to those households and firms that face binding borrowing constraints. Accordingly, the resulting gains from this policy increase with more bountiful fiscal buffers.","The global financial crisis followed an extraordinary upward swing in the leverage cycle (Geanakoplos et al., 2012). When the bubble burst, the massive debt accumulation in the private sector sparked a typical debt deflation (Fisher, 1933, Minsky, 1982) that propelled the ratio of public debt-to-GDP very rapidly. This reflected, on one side, the recession-induced decline in government revenues and prices, including those of assets; and, on the other side, governments directly taking over private debt gone sour.====Spurred by such economic developments, late empirical studies have started to focus increasingly more on the relationship between private debt and the macroeconomy. Part of this literature documents the links between rapid credit growth–especially credit to households–and financial crises in the advanced world (Glick, Lansing, 2010, Dell’Ariccia, Igan, Laeven, Tong, Bakker, Vandenbussche, 2012, IMF, 2012, Schularick, Taylor, 2012, Taylor, 2012, Jordà, Schularick, Taylor, 2013, Mian, Sufi, 2014, Jordà, Schularick, Taylor, 2014). The key messages from this body of research are that credit growth predicts financial crises and that, conditional on having a recession, stronger credit growth predicts deeper recessions. Mian et al. (2017) take these results a level further, finding unconditional negative correlations between household debt changes and future growth in a panel of advanced and emerging market economies. In addition, they demonstrate that rises in public debt are not associated with lower future output growth.====Theoretical economic modeling has flanked the empirical research, at least up to a certain point. Building upon the modern macroeconomic model-based literature on collateral and leverage cycles (pioneered by Bernanke, Gertler, Gilchrist, 1999, Kiyotaki, Moore, 1997, Iacoviello, 2005) a number of recent papers in macro-finance have focused on how to reproduce mechanisms through which excessive indebtedness in the private sector can harm the economy (e.g. Eggertsson, Krugman, 2012, Farhi, Werning, 2016, Korinek, Simsek, 2016, Guerrieri, Lorenzoni, 2017, Martin, Philippon, 2017).====However, there are important gaps in the literature. First, none of these contributions studies the macro-financial interlinkages between private and public balance sheets that so distinctly characterized both the evolution and the recovery phases of the recent crisis in the advanced world. Second, models featuring a fully-fledged public sector facing fiscal limits (such as Cantore et al., 2018) do not factor in the role of the government as a lender of last resort during protracted phases of financial stress. And third, research so far–starting with Gertler and Karadi (2011)–has focused exclusively on the impact of central bank lending to banks during a financial crisis, abstracting from other equally effective policy tools, like the possibility–observed during the financial crisis–that the government lends directly to financially-constrained agents.====In this paper we move forward by making positive and normative additions to the existing literature to fill these gaps. Focusing on the Euro Area, an economy in which the loop between private and public debt has played a particularly important role during the Great Recession, we begin by highlighting three stylized facts. The first two reaffirm those unveiled by Mian et al. (2017) for a broader set of countries, i.e. a negative correlation between increases in private debt and future levels of output, and a lack of correlation between a rise in public debt and the severity of future recessions. Furthermore, we show that in those Euro-Area countries where the level of public debt is especially elevated, increases in public debt lead more severe recessions.====We then build a dynamic stochastic general equilibrium (DSGE) model calibrated on the Euro Area that reproduces these stylized facts and can help us both examine the macroeconomic effects of a financial-crisis-style event leading to private deleveraging and conduct policy experiments.==== The basic structure of the model embeds Iacoviello (2005)’s features of borrowing constraints in the housing market within a New-Keynesian setting. The model is enriched with a fiscal bloc that tracks changes in the stock of government debt and captures the links between this and the sovereign risk premium, as in Cantore et al. (2018). In this set up, through the financial accelerator, private deleveraging depresses output and prices, which in turn depresses government revenues, accelerating the accumulation of public debt. Crucially, we assume that the government can alleviate private borrowing constraints via targeted lending operations (dubbed henceforth “targeted interventions”), which impinge on debt-to-GDP dynamics by affecting output and deficits. By virtue of these features, the model can mimic well the key links between debt and output that characterized the recent financial crisis in the Euro Area.====Model simulations suggest that, running into a financial shock with high private debt is potentially more hazardous for output than confronting such a shock with high public debt. They also suggest that government loans to borrowing-constrained agents during deleveraging phases mitigates the adverse recessionary effects of financial shocks on output, whilst offering net gains for the government medium-run fiscal position. At the same time, these results do not call for limitless financial assistance to the private sector. In fact, we unveil clear trade-offs between costs and benefits of targeted interventions. For countries closer to their fiscal limit, the benefits are more muted because they are undone by subsequent tax hikes necessary to preserve fiscal sustainability. It follows that one of the key benefits of having fiscal buffers is the greater macroeconomic resilience to financial shocks.====The remainder of the paper is organized as follows. Section 2 presents stylized facts on the links between debt, both private and public, and future output in the Euro Area. Section 3 describes the model. Section 4 discusses the model calibration and validation. Section 5 studies a deleveraging episode and government targeted intervention. Finally, Section 6 concludes. Technical details and robustness checks are appended to the paper.","Fiscal buffers, private debt, and recession: The good, the bad and the ugly",https://www.sciencedirect.com/science/article/pii/S0164070417305621,30 June 2018,2018,Research Article,306.0
"Neri Stefano,Gerali Andrea","Banca d'Italia, Italy","Received 13 November 2017, Revised 3 April 2018, Accepted 19 April 2018, Available online 24 April 2018, Version of Record 27 November 2019.",https://doi.org/10.1016/j.jmacro.2018.04.007,Cited by (23),"A closed-economy medium-scale model is estimated for the United States and the euro-area to assess the current level of the natural rate of interest and shed light on its drivers over the last decades. The model features a rich set of shocks that bear some connection with the explanations put forward in the literature to explain the secular downward trend in ====. The analysis shows that the natural rate has declined over the past decades, contributing to lowering nominal and real rates. Risk premium shocks, short-cut for changes in agents’ preference for safe assets, have been the main driver in the euro area. In the United States, shocks to the efficiency of investment, which may capture the functioning of the financial sector, and to the risk premium have played a major role. These differences between the two economies underscore the importance of adopting a structural model with a rich stochastic structure.","Nominal and real short- and long-term interest rates have been decreasing since the eighties. This trend accelerated with the outbreak of the global financial crisis in 2008. Albeit falling inflation expectations and risk premia have contributed to these developments – in an environment characterized by increased attention of central banks to price stability – the fall in the nominal rates has been associated primarily with a decline in the real component. These developments have led policymakers and academics to a greater focus on the natural rate of interest and on the effectiveness of monetary policy in a low interest rate environment.====The concept of the natural rate of interest – defined by Knutt Wicksell (1898, p. 102) as the real interest rate that “====” – has regained importance after the speech by Summers (2014) on secular stagnation in the US. According to this view, the natural rate has been pushed downward by secular factors such as the slowdown in the technological innovation and adverse demographic developments, which have affected both the demand and supply of savings. A lower natural rate can limit the effectiveness of conventional monetary policy by increasing the probability of hitting the lower bound to the policy rates. According to an alternative and partly competing view, the current low interest rate environment can be rationalized by relying on financial factors and the legacy of the global financial crisis (Borio, 2014 and Lo and Rogoff, 2015).====As the policy implications from these two views can be different, it is essential for academics and policymakers to identify the factors behind the world-wide decline in real interest rates (Bean et al., 2015). This is particularly true for those central banks that have embarked in unconventional measures. The pace and timing of the normalization of monetary policy may depend on where the natural rate is and how it is expected to evolve in the medium-term.====In this paper we estimate a medium-scale DSGE model to answer two related questions: (====) what is the current level of the natural rate in the US and the euro area; (====) what are the main factors underlying its developments over the last decades, and, in particular, since the outbreak of the global financial crisis. Within the DSGE literature, the natural rate is defined as the real rate which arises in an economy in which output is at potential and inflation is at the central bank's target. We adopt this definition and compute the natural rate by shutting down nominal rigidities (Woodford, 2003). The model is endowed with a rich set of shocks that allow us to gauge which of the views put forward in the literature for the decline in interest rates gets empirical support.====There are several papers providing estimates of the natural rate to which we connect. This growing literature can be broadly divided in three strands, following Giammarioli and Valla (2004). The first one relies on pure time-series methods – based on multivariate time series models (Hamilton et al., 2016 and Christensen and Rudebusch, 2017). A second approach uses semi-structural econometric models in which the natural rate is a latent variable that depend on the trend growth rate of potential output and a unit root process which captures other determinants (Laubach and Williams, 2003, Holston et al., 2017 for the US; Mésonnier and Renne, 2007 and Fries et al., 2016 for the euro area). The third approach relies on structural models, which can be either overlapping generation (OLG) (Gagnon et al., 2016, Kara and von Thadden, 2016, Eggertsson et al., 2017) or DSGE models of the New Keynesian tradition (Edge et al., 2008, Justiniano and Primiceri, 2010, Barsky et al., 2014, Cúrdia et al., 2015, Del Negro et al., 2015, Del Negro et al., 2017 all for the US; Hristov, 2016 for the euro area).====We use a structural (DSGE) model since in our view this approach has the decisive advantage of providing economic intuition for the underling drivers of the natural rate (Neiss and Nelson, 2003). However, we do not go to the extreme of developing an OLG or a heterogeneous agents model whose results will be less easy to digest for policymakers given their complexity. Our approach is somewhat reminiscent of a growth accounting exercise: we take a standard workhorse model routinely used in central banks (e.g. Smets and Wouters, 2007) and include two set of shocks: permanent shocks that affect the ==== of the economy and transitory shocks that drive its ==== (i.e. around the trend) component. The shocks play the role of “wedges” the model requires to explain the data. As these wedges are chosen to “speak” to the theories for the low interest rate environment, the estimation tells us which theories are more consistent with the data. We consider this to be our contribution to the literature, in addition to comparing the estimates of the natural rates in two large advanced economies and highlighting the role of common factors.====Our results point to qualitatively similar developments in the two economies. The natural rate reached its maximum level in early eighties in the US and early nineties in the euro area. Since then, it has declined persistently until the first years of this century, when it reached essentially zero per cent. After a surge in 2007 and 2008, the natural rate started declining again and reached negative values after the outbreak of the global financial crisis. Our results confirm the finding in recent studies on the US – using either DSGE models or the Laubach and Williams (2003) approach – of a decline in the natural rate since the beginning of the 21st century.====With regards to the drivers, the natural rate in the US and the euro area is driven primarily by shocks to the cyclical component, with those affecting the growth rate of output in the long-run playing a more muted role. Risk premium shocks, possibly capturing changes in the preference for safe assets, explain the bulk of the decline since early nineties in the euro area. In the US, an important role is also played by shocks to the efficiency of investment, which in a model without a financial sector may capture inefficiencies in financial intermediation that reduce the efficiency in channeling resources from savers to productive borrowers (Justiniano et al., 2011). The importance of the risk premium shock for the natural rate in the US has been documented also by Del Negro et al. (2017). The differences in the contribution of the shocks between the US and the euro area underscore the importance of adopting a structural approach with a rich stochastic structure to the estimation of the natural rate. The results show that the downward trend in the natural rates is due to unfavorable transitory but persistent shocks and suggest the need for developing models featuring financial factors, including the presence of scarce safe assets.====Our results have important implications for monetary policy. A lower natural rate implies that under stable inflation and well-anchored expectations, the policy rate consistent with unemployment and output at their natural levels and inflation on target is closer to its lower bound, limiting the room for manoeuvre of central banks (Kiley and Roberts, 2017). To the extent that the natural rate remains at current low levels, changes to monetary policy frameworks, including raising the inflation target or adopting price-level targeting, may be required (Bernanke, 2017 and Williams, 2017).====The remainder of the paper is organized as follows. Section 2 presents the model and Section 3 describes its estimation; Section 4 presents the estimates of the natural rate and its historical decomposition. Section 5 discusses selected robustness checks. Section 6 draws the conclusions.",Natural rates across the Atlantic,https://www.sciencedirect.com/science/article/pii/S0164070417304652,24 April 2018,2018,Research Article,307.0
Shami Labib,"University of Haifa, 199 Aba Khoushy Ave., Mount Carmel, Haifa, Israel","Received 3 December 2017, Revised 23 March 2018, Accepted 15 April 2018, Available online 16 April 2018, Version of Record 27 November 2019.",https://doi.org/10.1016/j.jmacro.2018.04.006,Cited by (5),"We build a ==== model with inside and outside money, heterogeneous tax-evading households, a government and a central bank to demonstrate the dynamic nature of the relationship between ==== rates as a ","The Great Recession of 2008, followed by the ongoing European Debt Crisis, left many governments in the Euro-zone with mounting public debts, emphasizing the need for additional governmental revenue sources (Anand et al., 2012). One of the unexploited sources for these revenues is the Non-Observed Economy (NOE).====Recent estimates of the NOE imply that it may account for up to 12% of the total economic activity in Anglo–Saxon countries, between 20%–30% in southern European nations (Schneider and Williams, 2013), and about 40% in developing and transition economies (Schneider, 2008). Moreover, as Fig. 1 illustrates, the size of the NOE approaches that of the government for some countries, even within the OECD.==== These figures imply that a great deal of economic activity remains undeclared, and thus untaxed, producing negative effects on government revenues and distorting official indicators, such as the rate of growth and income distribution.====However, despite the fact that the NOE is a widespread phenomenon that poses serious social, economic, cultural and political challenges across the world, many issues about its nature and consequences still remain largely under-explored or unresolved (Elgin and Oztunali, 2012). Our study focuses on one of these issues: What is the relationship between the change in the rate of inflation and the size of the Non-Observed Economy?====The existing answers to this question are mixed. A common empirical finding, based on cross sectional evidence, is that inflation rates and the size of the NOE appear to be positively correlated (Koreshkova, 2006, Aruoba, 2010, Mazhar, Méon, 2017). However, this finding is at odds with the predictions of some theoretical models (Gillman, Kejak, 2006, Gomis-Porqueras, Peralta-Alva, Waller, 2014).====To address the question, we first introduce a general equilibrium model with heterogeneous households, a government and a central bank but ==== the NOE. It would be senseless to analyze monetary policy in a model where (fiat) money has no value or where monetary policy has no real effects. Nevertheless, constructing such a general equilibrium model has been a challenge for decades (Kiyotaki, Wright, 1989, Dubey, Geanakoplos, 1992, Dubey, Geanakoplos, 2003, Dubey, Geanakoplos, 2006, Lagos, Wright, 2005, Lagos, Rocheteau, Wright, 2017).====Throughout our analysis we use several basic assumptions. First, and to ensure that money has a value, we adopt Dubey and Geanakoplos (2003) approach of inside and outside money that appear naturally in a setting where agents bid cash to buy goods in the market.==== This setting is consistent with Shapley and Shubik (1977) game, and also used in Starr (2008). Furthermore, to focus on the demand side, the total supply of the consumption good is fixed in each period. Thus, the price in the (official) market is the ratio of total bids to the quantity offered in that market. In other words, prices are not rigid.====In our benchmark economy, without the NOE, money has a value, however, monetary policy has no real effects. Adding the NOE into the model creates the “effect of friction”, a feature that creates the real effect of monetary policy in the economy. The NOE is based on income tax evasion by heterogeneous agents who use the hidden cash to purchase goods in an ==== market. This view of the NOE is in-line with its commonly used definition as ==== (Schneider, Dell’Anno, 2003, Feige, 2007).====To address the issue of the interactions between monetary policy, enforcement institutions and the size of the NOE, we introduce an endogenous tax evasion that depends on the enforcement mechanism in the economy. Knowing the official tax auditing policy, meaning, the punishment for each level of tax evasion, and given an income tax, each agent optimally chooses the amount of nominal income to under-report.====Other things being equal, an increase in price reduces the real size of the NOE. However, to assess the full effect of a change in the inflation rate, the whole path of inflation has to be known. The model can produce both positive and negative relationships between the change in the inflation rate and the size of the NOE, both over time and across countries.====Another key feature of the model is inflation targeting, which is consistent with a ==== on the part of the government: its debt has to be eventually returned. Although the latter might be unrealistic, it is to be understood as an extreme version of a common situation in which a government is subject to external pressure to reduce its debt (cf. e.g., Blyth, 2013). This feature forces the non-zero inflation to change sign (at least once), which, in turn, affects the size of the NOE and that of the observed GDP.====As is expected from forward-looking agents with diminishing marginal utility of consumption, the total consumption (coming from the formal and informal markets) is smoothed over time. However, the two components of consumption vary with the inflation rate. To avoid additional frictions, we assume that the informal market provides any amount of goods demanded at the prevailing (formal) market price. Therefore, a change (increase or decrease) in the inflation rate, that leads to an increase in price, will reduce the quantity purchased in the informal market. As the total amount of consumption goods is fixed, this will increase the amount sold in the formal market. This is the force that might generate both negative and positive relationships between the inflation rate and the size of the NOE. Hence, by the inclusion of the NOE we get a real effect of monetary policy: The distribution of the goods between government and private consumption is affected by the dynamics of the price changes, meaning, by the inflation path. We perform a series of computational exercises to illustrate this non-trivial dependence.",Dynamic monetary equilibrium with a Non-Observed Economy and Shapley and Shubik’s price mechanism,https://www.sciencedirect.com/science/article/pii/S0164070417304949,16 April 2018,2018,Research Article,308.0
"Azariadis Costas,Bucci Alberto","Washington University, St. Louis, USA,Federal Reserve Bank of St. Louis, USA,University of Milan, Department of Economics, Management, and Quantitative Methods, Italy,CEIS (,), University of Rome–‘Tor Vergata’, Italy,RCEA (,), Wilfrid Laurier University, Waterloo, Canada","Available online 6 April 2019, Version of Record 27 November 2019.",https://doi.org/10.1016/j.jmacro.2019.04.003,Cited by (2),None,None,On ‘Finance and economic growth in the aftermath of the crisis’,https://www.sciencedirect.com/science/article/pii/S0164070419301466,6 April 2019,2019,Research Article,313.0
"Ash Colin,Easaw Joshy Z.","University of Buffalo, Buffalo, NY, United States,University of Reading, Reading, United Kingdom,Cardiff University, Cardiff, Wales, United Kingdom","Available online 4 April 2019, Version of Record 16 May 2019.",https://doi.org/10.1016/j.jmacro.2019.03.005,Cited by (0),None,"David J. Smyth (1936–2014), the founding editor of the ==== (hereafter ====), had a remarkable publication record prior to the birth of ==== for a relatively young researcher. He published sixty-eight refereed articles and two substantial research monographs between 1967 and 1978. Over his academic lifetime he published a total of 169 refereed journal articles as well the two books. David's publication record, expertise and interest were varied and stretched from forecasting, macroeconomics and political economy. Having established his reputation in the areas of forecasting and macroeconomics, in the last decade of his academic research, David began to forge a reputation and expertise in the area of political economy.====David also published in the fields of industrial economics, labor economics, public finance and applied econometrics. While an extensive appreciation of David's enormous body of work would be of interest, the present paper is a more modest undertaking. We consider David's key contributions, together with his research collaborators, to the academic economics community. To this end, we will focus on his achievements; notably the founding of the ==== and his main contributions to the areas of macroeconomics, forecasting and political economy.====David founded ==== in 1979 and continued to edit the journal until 1998. Since its founding, ==== has published some of the more innovative contributions to the broad area of macroeconomics. In the twenty years under David's editorship, the journal was established as a leading field journal in that area.====David was a valued colleague, productive research collaborator and generous mentor to us. Jim (James Holmes) was a long-term research collaborator of David, beginning with his time at Buffalo; Jim was also at the inception of ====, serving on the original Editorial Board. Together, they published several influential papers and contributions in the area of macroeconomics, for example the ‘Holmes-Smyth’ effect. Colin (Colin Ash) was a graduate student of David's and his research assistant during his time at the University of Birmingham, UK. They went on to be research collaborators and, together with others, made a number of important contributions in the field of forecasting. Joshy (Joshy Easaw) got to know David during his last academic appointment as Research Professor at Middlesex University Business School in London, UK. They shared a common research interest in political economy, notably political business cycle, incumbent popularity and voter preference functions.====In what follows, we assess David's motivation and objectives for starting ==== We also evaluate some of the more innovative contributions David jointly made in the area of macroeconomics, especially around this period. Subsequently in Section III, we evaluate the notable contributions David has made in the field of macro forecasting and, finally, we consider the extensions and contributions David and his research collaborators have made in the understanding of voter preferences and incumbent popularity functions.",David J Smyth: An appreciation of his work,https://www.sciencedirect.com/science/article/pii/S0164070419301259,4 April 2019,2019,Research Article,314.0
